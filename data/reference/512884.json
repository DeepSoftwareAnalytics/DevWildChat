[
    {
        "link": "https://geeksforgeeks.org/cpp-program-to-implement-avl-tree",
        "document": "AVL Tree, named after its inventors Adelson-Velsky and Landis, is a self-balancing binary search tree. In an AVL tree, the heights of the two child subtrees of any node differ by at most one, which ensures that the tree remains approximately balanced, providing efficient search, insertion, and deletion operations.\n\nIn this article, we will learn about the implementation of AVL Tree in C++, its basic operation and its applications.\n\nWhat is an AVL Tree?\n\nAn AVL tree maintains balance by performing rotations during insertions and deletions to ensure the height difference between the left and right subtrees of any node is no more than one. This property guarantees that the tree's height remains O(logn), ensuring efficient operations.\n\nAn AVL Tree is a binary search tree with the following properties:\n• None The heights of the left and right subtrees of every node differ by at most one.\n• None Every subtree is an AVL tree.\n• None For every node, its balance factor (height of left subtree - height of right subtree) is -1, 0, or 1.\n\nImplementation of AVL Tree in C++\n\nAn AVL tree can be implemented using a binary tree structure where each node will have left and right pointers and key values to store the data but along with that, we will store the height for each node so that the balance factor can be calculated easily. The balance factor of a node will be calculated for each node as the difference between the heights of its left and right subtrees.\n\nWhen the balance factor for any node is not in the allowed limits, rotations are preformed to balance it.\n\nRotations are the most important part of the working of the AVL tree. They are responsible for maintaining the balance in the AVL tree. There are 4 types of rotations based on the 4 possible cases:\n\nThe Right Rotation (RR) is applied in an AVL tree when a node becomes unbalanced due to an insertion into the right subtree of its right child, leading to a Left Imbalance. To correct this imbalance, the unbalanced node is rotated 90° to the right (clockwise) along the top edge connected to its parent.\n\nThe Left Rotation (LL) is used to balance a node that becomes unbalanced due to an insertion into the left subtree of its left child, also resulting in a Left Imbalance. The solution is to rotate the unbalanced node 90° to the left (anti-clockwise) along the top edge connected to its parent.\n\nThe Left-Right Rotation (LR) is necessary when the left child of a node is right-heavy, creating a double imbalance. This situation is resolved by performing a left rotation on the left child, followed by a right rotation on the original node.\n\nThe Right-Left Rotation (RL) is used when the right child of a node is left-heavy. This imbalance is corrected by performing a right rotation on the right child, followed by a left rotation on the original node.\n\nRepresentation of AVL Tree in C++\n\nThe following diagram represents the structure of an AVL Tree where the balance factor of each node is 0 and the tree is balanced:\n\nTo represent an AVL Tree in C++, we will use a class AVLNode to define the node structure and a class AVLTree to implement the AVL tree operations. We will use the in order to keep the AVL tree generic so that it can store multiple data types.\n• key: represents the value stored inside the node.\n• left &right: are pointers to the left and right node.\n• height: represents the height of each subtree starting from each node.\n\nImplementation of Basic Operations of an AVL Tree in C++\n\nFollowing are the basic operations that are required to work with an AVL tree:\n\nThe following program demonstrates the implementation of AVL tree in C++:\n\n// Pointer to the right child // Height of the node in the tree // Constructor to initialize a node with a given key // Pointer to the root of the tree // function to get the height of a node // function to get the balance factor of a node // function to perform a right rotation on a subtree // function to insert a new key into the subtree rooted // Get the balance factor of this ancestor node // If this node becomes unbalanced, then there are 4 // function to find the node with the minimum key value // function to delete a key from the subtree rooted with // Node with only one child or no child // Get the balance factor of this node // If this node becomes unbalanced, then there are 4 // function to perform inorder traversal of the tree // function to search for a key in the subtree rooted // Function to insert a key into the AVL tree // Function to remove a key from the AVL tree // Function to search for a key in the AVL tree // Function to print the inorder traversal of the AVL // Print the inorder traversal of the AVL tree // Search for nodes in the AVL tree\n\nFollowing are some of the common applications of an AVL Tree:\n• Database Indexing: AVL trees are used in database systems for efficient indexing and quick data retrieval.\n• File Systems: Some file systems use AVL trees to maintain directory structures for fast file lookups.\n• Spatial Indexing: In computer graphics and computational geometry, AVL trees are used for spatial indexing of objects.\n• Network Routing Tables: AVL trees can be used to implement routing tables in network routers for efficient IP lookup.\n• Priority Queues: While not as common as heaps, AVL trees can be used to implement priority queues with efficient operations.\n• Symbol Tables in Compilers: Compilers often use AVL trees to implement symbol tables for fast insertion, deletion, and lookup of identifiers."
    },
    {
        "link": "https://runestone.academy/ns/books/published/cppds/Trees/AVLTreeImplementation.html",
        "document": "Now that we have demonstrated that keeping an AVL tree in balance is going to be a big performance improvement, let us look at how we will augment the procedure to insert a new key into the tree. Since all new keys are inserted into the tree as leaf nodes and we know that the balance factor for a new leaf is zero, there are no new requirements for the node that was just inserted. But once the new leaf is added we must update the balance factor of its parent. How this new leaf affects the parent’s balance factor depends on whether the leaf node is a left child or a right child. If the new node is a right child the balance factor of the parent will be reduced by one. If the new node is a left child then the balance factor of the parent will be increased by one. This relation can be applied recursively to the grandparent of the new node, and possibly to every ancestor all the way up to the root of the tree. Since this is a recursive procedure let us examine the two base cases for updating balance factors:\n• None The recursive call has reached the root of the tree.\n• None The balance factor of the parent has been adjusted to zero. You should convince yourself that once a subtree has a balance factor of zero, then the balance of its ancestor nodes does not change.\n\nWe will implement the AVL tree as a subclass of . To begin, we will override the method and write a new helper method. These methods are shown in Listing 1. You will notice that the definition for is exactly the same as in simple binary search trees except for the additions of the calls to on lines 8 and 17.\n\nThe new method is where most of the work is done. The method first checks to see if the current node is out of balance enough to require rebalancing (line 16). If that is the case then the rebalancing is done and no further updating to parents is required. If the current node does not require rebalancing then the balance factor of the parent is adjusted. If the balance factor of the parent is non-zero then the algorithm continues to work its way up the tree toward the root by recursively calling on the parent.\n\nWhen a rebalancing of the tree is necessary, how do we do it? Efficient rebalancing is the key to making the AVL Tree work well without sacrificing performance. In order to bring an AVL Tree back into balance we will perform one or more rotations on the tree.\n\nTo understand what a rotation is let us look at a very simple example. Consider the tree in the left half of Figure 1. This tree is out of balance with a balance factor of -2. To bring this tree into balance we will use a left rotation around the subtree rooted at node A.\n\nTo perform a left rotation we essentially do the following:\n• None Promote the right child (B) to be the root of the subtree.\n• None Move the old root (A) to be the left child of the new root.\n• None If new root (B) already had a left child then make it the right child of the new left child (A). Note: Since the new root (B) was the right child of A the right child of A is guaranteed to be empty at this point. This allows us to add a new node as the right child without any further consideration.\n\nWhile this procedure is fairly easy in concept, the details of the code are a bit tricky since we need to move things around in just the right order so that all properties of a Binary Search Tree are preserved. Furthermore we need to make sure to update all of the parent pointers appropriately.\n\nLet’s look at a slightly more complicated tree to illustrate the right rotation. The left side of Figure 2 shows a tree that is left-heavy and with a balance factor of 2 at the root. To perform a right rotation we essentially do the following:\n• None Promote the left child (C) to be the root of the subtree.\n• None Move the old root (E) to be the right child of the new root.\n• None If the new root(C) already had a right child (D) then make it the left child of the new right child (E). Note: Since the new root (C) was the left child of E, the left child of E is guaranteed to be empty at this point. This allows us to add a new node as the left child without any further consideration.\n\nNow that you have seen the rotations and have the basic idea of how a rotation works let us look at the code. Listing 2 shows the code for both the right and the left rotations. In line 2 we create a temporary variable to keep track of the new root of the subtree. As we said before the new root is the right child of the previous root. Now that a reference to the right child has been stored in this temporary variable we replace the right child of the old root with the left child of the new.\n\nThe next step is to adjust the parent pointers of the two nodes. If has a left child then the new parent of the left child becomes the old root. The parent of the new root is set to the parent of the old root. If the old root was the root of the entire tree then we must set the root of the tree to point to this new root. Otherwise, if the old root is a left child then we change the parent of the left child to point to the new root; otherwise we change the parent of the right child to point to the new root. (lines 10-13). Finally we set the parent of the old root to be the new root. This is a lot of complicated bookkeeping, so we encourage you to trace through this function while looking at Figure 1. The method is symmetrical to so we will leave it to you to study the code for .\n\nFinally, lines 21-22 require some explanation. In these two lines we update the balance factors of the old and the new root. Since all the other moves are moving entire subtrees around the balance factors of all other nodes are unaffected by the rotation. But how can we update the balance factors without completely recalculating the heights of the new subtrees? The following derivation should convince you that these lines are correct.\n\nFigure 3 shows a left rotation. B and D are the pivotal nodes and A, C, E are their subtrees. Let \\(h_x\\) denote the height of a particular subtree rooted at node \\(x\\). By definition we know the following:\n\nBut we know that the old height of D can also be given by \\(1 + max(h_C,h_E)\\), that is, the height of D is one more than the maximum height of its two children. Remember that \\(h_c\\) and \\(h_E\\) hav not changed. So, let us substitute that in to the second equation, which gives us\n\nand then subtract the two equations. The following steps do the subtraction and use some algebra to simplify the equation for \\(newBal(B)\\).\n\nNext we will move \\(oldBal(B)\\) to the right hand side of the equation and make use of the fact that \\(max(a,b)-c = max(a-c, b-c)\\).\n\nBut, \\(h_E - h_C\\) is the same as \\(-oldBal(D)\\). So we can use another identity that says \\(max(-a,-b) = -min(a,b)\\). So we can finish our derivation of \\(newBal(B)\\) with the following steps:\n\nNow we have all of the parts in terms that we readily know. If we remember that B is and D is then we can see this corresponds exactly to the statement on line 21, or:\n\nA similar derivation gives us the equation for the updated node D, as well as the balance factors after a right rotation. We leave these as exercises for you.\n\nNow you might think that we are done. We know how to do our left and right rotations, and we know when we should do a left or right rotation, but take a look at Figure 4. Since node A has a balance factor of -2 we should do a left rotation. But, what happens when we do the left rotation around A?\n\nFigure 5 shows us that after the left rotation we are now out of balance the other way. If we do a right rotation to correct the situation we are right back where we started.\n\nTo correct this problem we must use the following set of rules:\n• None If a subtree needs a left rotation to bring it into balance, first check the balance factor of the right child. If the right child is left heavy then do a right rotation on right child, followed by the original left rotation.\n• None If a subtree needs a right rotation to bring it into balance, first check the balance factor of the left child. If the left child is right heavy then do a left rotation on the left child, followed by the original right rotation.\n\nFigure 6 shows how these rules solve the dilemma we encountered in Figure 4 and Figure 5. Starting with a right rotation around node C puts the tree in a position where the left rotation around A brings the entire subtree back into balance.\n\nThe code that implements these rules can be found in our method, which is shown in Listing 3. Rule number 1 from above is implemented by the statement starting on line 2. Rule number 2 is implemented by the statement starting on line 11.\n\nThe discussion questions provide you the opportunity to rebalance a tree that requires a left rotation followed by a right. In addition the discussion questions provide you with the opportunity to rebalance some trees that are a little more complex than the tree in Figure 6.\n\nBy keeping the tree in balance at all times, we can ensure that the method will run in order \\(O(log_2(n))\\) time. But the question is at what cost to our method? Let us break this down into the operations performed by . Since a new node is inserted as a leaf, updating the balance factors of all the parents will require a maximum of \\(log_2(n)\\) operations, one for each level of the tree. If a subtree is found to be out of balance a maximum of two rotations are required to bring the tree back into balance. But, each of the rotations works in \\(O(1)\\) time, so even our operation remains \\(O(log_2(n))\\).\n\nAt this point we have implemented a functional AVL-Tree, unless you need the ability to delete a node. We leave the deletion of the node and subsequent updating and rebalancing as an exercise for you."
    },
    {
        "link": "https://geeksforgeeks.org/introduction-to-avl-tree",
        "document": "An AVL tree defined as a self-balancing Binary Search Tree (BST) where the difference between heights of left and right subtrees for any node cannot be more than one.\n• None The absolute difference between the heights of the left subtree and the right subtree for any node is known as the balance factor of the node. The balance factor for all nodes must be less than or equal to 1.\n• None Every AVL tree is also a Binary Search Tree (Left subtree values Smaller and Right subtree values grater for every node), but every BST is not AVL Tree. For example, the second diagram below is not an AVL Tree.\n• None The main advantage of an AVL Tree is, the time complexities of all operations (search, insert and delete, max, min, floor and ceiling) become O(Log n). This happens because height of an AVL tree is bounded by O(Log n). In case of a normal BST, the height can go up to O(n).\n• None An AVL tree maintains its height by doing some extra work during insert and delete operations. It mainly uses rotations to maintain both BST properties and height balance.\n• None There exist other self-balancing BSTs also like . Red Black tree is more complex, but used more in practice as it is less restrictive in terms of left and right subtree height differences.\n\nExample of an AVL Tree:\n\nThe balance factors for different nodes are : 12 :1, 8:1, 18:1, 5:1, 11:0, 17:0 and 4:0. Since all differences are less than or equal to 1, the tree is an AVL tree.\n\nExample of a BST which is NOT AVL:\n\nThe Below Tree is NOT an AVL Tree as the balance factor for nodes 8, 4 and 7 is more than 1.\n• Searching : It is same as normal Binary Search Tree (BST) as an AVL Tree is always a BST. So we can use the same implementation as BST. The advantage here is time complexity is O(Log n)\n• Insertion : It does rotations along with normal BST insertion to make sure that the balance factor of the impacted nodes is less than or equal to 1 after insertion\n• Deletion : It also does rotations along with normal BST deltion to make sure that the balance factor of the impacted nodes is less than or equal to 1 after deletion.\n\nPlease refer Insertion in AVL Tree and Deletion in AVL Tree for details.\n\nRotating the subtrees (Used in Insertion and Deletion)\n\nAn AVL tree may rotate in one of the following four ways to keep itself balanced while making sure that the BST properties are maintained.\n\nWhen a node is added into the right subtree of the right subtree, if the tree gets out of balance, we do a single left rotation.\n\nIf a node is added to the left subtree of the left subtree, the AVL tree may get out of balance, we do a single right rotation.\n\nA left-right rotation is a combination in which first left rotation takes place after that right rotation executes.\n\nA right-left rotation is a combination in which first right rotation takes place after that left rotation executes.\n• None AVL trees can self-balance themselves and therefore provides time complexity as O(Log n) for search, insert and delete.\n• None It is a BST only (with balancing), so items can be traversed in sorted order.\n• None Since the balancing rules are strict compared to , AVL trees in general have relatively less height and hence the search is faster.\n• None AVL tree is relatively less complex to understand and implement compared to Red Black Trees.\n• None It is difficult to implement compared to normal BST and easier compared to Red Black\n• None Less used compared to Red-Black trees. Due to its rather strict balance, AVL trees provide complicated insertion and removal operations as more rotations are performed.\n• None AVL Tree is used as a first example self balancing BST in teaching DSA as it is easier to understand and implement compared to Red Black\n• None Applications, where insertions and deletions are less common but frequent data lookups along with other operations of BST like sorted traversal, floor, ceil, min and max.\n• None Red Black tree is more commonly implemented in language libraries like\n• None AVL Trees can be used in a real time environment where predictable and consistent performance is required."
    },
    {
        "link": "https://stackoverflow.com/questions/10796509/implementation-of-priority-queue-by-avl-tree-data-structure",
        "document": "Complexity isn't everything, there are other considerations for actual performance.\n\nFor most purposes, most people don't even use an AVL tree as a balanced tree (Red-Black trees are more common as far as I've seen), let alone as a priority queue.\n\nThis is not to say that AVL trees are useless, I quite like them. But they do have a relatively expensive insert. What AVL trees are good for (beating even Red-Black trees) is doing lots and lots of lookups without modification. This is not what you need for a priority queue.\n\nAs a separate consideration -- never mind your O(log n) insert for a binary heap, a fibonacci heap has O(1) insert and O(log N) delete-minimum. There are a lot of data structures to choose from with slightly different trade-offs, so you wouldn't expect to see everyone just pick the first thing that satisfies your (quite brief) criteria."
    },
    {
        "link": "https://akcoding.com/dsa/non-linear-data-structures/tree-data-structure/avl-tree",
        "document": "An AVL Tree is a type of self-balancing binary search tree (BST) named after its inventors, Georgy Adelson-Velsky and Evgenii Landis, who introduced it in 1962. The primary goal of an AVL Tree is to maintain balance as elements are inserted or deleted, ensuring that the tree remains balanced at all times. This balancing ensures that operations like search, insertion, and deletion are efficient, typically maintaining a time complexity of O(log n).\n• Self-Balancing: An AVL Tree automatically balances itself after every insertion and deletion to maintain the height difference (balance factor) between the left and right subtrees of any node to be no more than 1.\n• Binary Search Tree (BST) Properties: Like any BST, an AVL Tree has nodes where the left child contains a value less than its parent node, and the right child contains a value greater than its parent node.\n• Height Balance: The height of the AVL Tree is kept logarithmic, ensuring faster access and modification operations compared to an unbalanced BST.\n\nImportance in Computer Science:\n\nAVL Trees are crucial in scenarios where data needs to be retrieved quickly and operations like insertion and deletion must remain efficient. They are widely used in databases and memory management systems where balanced trees optimize performance.\n\nThe AVL Tree is a specialized binary search tree (BST) that ensures the tree remains balanced, optimizing the efficiency of search, insertion, and deletion operations. Here are the key properties of an AVL Tree:\n• The AVL Tree is known for its height-balanced property. For every node in the tree, the height difference between the left and right subtrees (known as the balance factor) is at most 1. This balance ensures that the tree remains approximately balanced, preventing it from becoming skewed like a standard BST.\n• The Balance Factor (BF) of a node is defined as the difference in height between its left and right subtrees:\n\n[\n\n\\text{Balance Factor (BF)} = \\text{Height of Left Subtree} – \\text{Height of Right Subtree}\n\n]\n• BF = 0: The left and right subtrees have the same height.\n• BF = 1: The left subtree is taller by one level.\n• BF = -1: The right subtree is taller by one level.\n• |BF| > 1: The tree is unbalanced and requires rotations to restore balance.\n• AVL Trees use rotations to restore balance when the balance factor condition is violated after an insertion or deletion. There are four types of rotations:\n• Left Rotation (LL Rotation): Applied when a node’s right subtree is heavier (BF = -2).\n• Right Rotation (RR Rotation): Applied when a node’s left subtree is heavier (BF = 2).\n• Left-Right Rotation (LR Rotation): A double rotation applied when the left subtree of a node’s right child is heavier.\n• Right-Left Rotation (RL Rotation): A double rotation applied when the right subtree of a node’s left child is heavier.\n• Due to its balanced nature, AVL Trees provide efficient time complexities for operations:\n• These time complexities make AVL Trees suitable for applications requiring frequent data access and updates.\n• The space complexity of an AVL Tree is O(n), where n is the number of nodes in the tree. The additional space is primarily used for storing the balance factor at each node.\n• Binary Search Tree Properties: As an AVL Tree is a type of BST, it adheres to the BST property where each node’s left child has a value less than the node itself, and the right child has a value greater than the node.\n• Logarithmic Height: The height of an AVL Tree is kept logarithmic concerning the number of nodes, ensuring optimal performance for operations.\n\nSummary:\n\nThe key properties of an AVL Tree, including the height-balanced property, balance factor, and rotation mechanisms, make it a highly efficient and reliable data structure for scenarios requiring quick and consistent access to data.\n\nRotations in an AVL Tree are operations used to restore balance when the tree becomes unbalanced after an insertion or deletion. Rotations adjust the positions of nodes within the tree, ensuring that the balance factor of each node remains within the acceptable range of -1 to 1. There are four types of rotations in an AVL Tree: Left Rotation (LL Rotation), Right Rotation (RR Rotation), Left-Right Rotation (LR Rotation), and Right-Left Rotation (RL Rotation).\n• A Left Rotation is needed when the right subtree of a node is taller than the left subtree by more than one level (Balance Factor = -2), and the imbalance is caused by an insertion in the right subtree of the right child.\n• The node’s right child becomes the new root of the subtree, and the original root becomes the left child of the new root. The left child of the new root is then reassigned as the right child of the original root.\n• A Right Rotation is necessary when the left subtree of a node is taller than the right subtree by more than one level (Balance Factor = 2), and the imbalance is caused by an insertion in the left subtree of the left child.\n• The node’s left child becomes the new root of the subtree, and the original root becomes the right child of the new root. The right child of the new root is then reassigned as the left child of the original root.\n• A Left-Right Rotation is required when the left subtree of a node is taller, but the imbalance is caused by an insertion in the right subtree of the left child.\n• First, a Left Rotation is performed on the left child of the unbalanced node, converting it into an LL situation. Then, a Right Rotation is performed on the original unbalanced node.\n• A Right-Left Rotation is needed when the right subtree of a node is taller, but the imbalance is caused by an insertion in the left subtree of the right child.\n• First, a Right Rotation is performed on the right child of the unbalanced node, converting it into an RR situation. Then, a Left Rotation is performed on the original unbalanced node.\n• Maintaining Balance: Rotations ensure that the AVL Tree maintains its balanced structure, keeping the height difference between subtrees minimal.\n• Efficiency: By performing rotations, the AVL Tree operations (insertion, deletion, and search) remain efficient with a time complexity of O(log n).\n• LL Example: Insert a node into the right subtree of the right child.\n• RR Example: Insert a node into the left subtree of the left child.\n• LR Example: Insert a node into the right subtree of the left child.\n• RL Example: Insert a node into the left subtree of the right child.\n\nRotations are essential in AVL Trees to preserve the balance and ensure the tree remains efficient for search, insertion, and deletion operations. Understanding how and when to apply each rotation is crucial for correctly implementing an AVL Tree.\n\nInsertion in an AVL Tree involves adding a new node while ensuring that the tree remains balanced according to the AVL property. This process includes standard binary search tree (BST) insertion followed by necessary rotations to restore balance if the tree becomes unbalanced.\n• Insert the new node in the appropriate position according to the rules of a binary search tree (BST), where the left child is smaller than the parent node, and the right child is larger.\n• After insertion, the new node is initially added as a leaf node.\n• After the insertion, traverse back up the tree to the root, updating the balance factor of each node along the path.\n• The balance factor is calculated as the height difference between the left and right subtrees of each node.\n• As you update the balance factors, check if any node becomes unbalanced (i.e., the balance factor is greater than 1 or less than -1).\n• If an imbalance is detected, identify the type of rotation needed to restore balance.\n• Depending on the location of the imbalance and the type of insertion, perform one of the following rotations:\n• LL Rotation: Used when the imbalance is caused by an insertion in the left subtree of the left child.\n• RR Rotation: Used when the imbalance is caused by an insertion in the right subtree of the right child.\n• LR Rotation: Used when the imbalance is caused by an insertion in the right subtree of the left child.\n• RL Rotation: Used when the imbalance is caused by an insertion in the left subtree of the right child.\n• Suppose you insert a node in the left subtree of the left child of a node. The resulting imbalance requires a single right rotation (RR Rotation) at the unbalanced node.\n• Suppose you insert a node in the right subtree of the right child of a node. The resulting imbalance requires a single left rotation (LL Rotation) at the unbalanced node.\n• Suppose you insert a node in the right subtree of the left child. The resulting imbalance requires a left rotation at the left child followed by a right rotation at the unbalanced node.\n• Suppose you insert a node in the left subtree of the right child. The resulting imbalance requires a right rotation at the right child followed by a left rotation at the unbalanced node.\n• The time complexity of inserting a node in an AVL Tree is O(log n). This is because:\n• The height of the AVL Tree is kept logarithmic concerning the number of nodes, ensuring efficient searching for the correct insertion point.\n• Even after insertion, balancing the tree through rotations is also performed in O(log n) time.\n\n4. Visual Example of Insertion and Rotation\n\nLet’s consider an example where we insert nodes into an AVL Tree:\n• Node 10 is inserted as the left child of 20.\n• The balance factor of node 20 becomes 2, indicating the tree is unbalanced.\n\nThis process illustrates how the insertion operation, followed by the necessary rotations, maintains the AVL Tree’s balance, ensuring the tree remains optimized for future operations.\n\nBy maintaining balance through rotations, the AVL Tree ensures that all operations, including insertion, remain efficient with logarithmic time complexity, making it a powerful data structure for dynamic sets.\n\nDeletion in an AVL Tree involves removing a node while ensuring that the tree remains balanced. Like insertion, deletion in an AVL Tree follows the standard binary search tree (BST) deletion process, followed by necessary rotations to maintain the AVL property if the tree becomes unbalanced.\n• Find the node to be deleted using the properties of a binary search tree (BST).\n• There are three cases to consider when deleting a node:\n• Case 1: Node has no children (Leaf Node): Simply remove the node from the tree.\n• Case 2: Node has one child: Remove the node and replace it with its child.\n• Case 3: Node has two children: Find the in-order predecessor (maximum value in the left subtree) or in-order successor (minimum value in the right subtree), replace the node’s value with it, and then delete the in-order predecessor or successor.\n• After deletion, traverse back up to the root, updating the balance factors of the nodes along the path.\n• The balance factor for each node is recalculated based on the new heights of its subtrees.\n• As you update the balance factors, check if any node becomes unbalanced (i.e., the balance factor is greater than 1 or less than -1).\n• If an imbalance is detected, identify the type of rotation needed to restore balance.\n• Depending on the location of the imbalance and the structure of the subtrees, perform one of the following rotations:\n• LL Rotation: Used when the left subtree is heavier by two levels, and the left child’s left subtree is taller.\n• RR Rotation: Used when the right subtree is heavier by two levels, and the right child’s right subtree is taller.\n• LR Rotation: Used when the left subtree is heavier by two levels, and the left child’s right subtree is taller.\n• RL Rotation: Used when the right subtree is heavier by two levels, and the right child’s left subtree is taller.\n• Suppose you delete a node from the right subtree, causing an imbalance where the left subtree is heavier. An RR rotation is required to balance the tree.\n• Suppose you delete a node from the left subtree, causing an imbalance where the right subtree is heavier. An LL rotation is required to balance the tree.\n• Suppose you delete a node from the right subtree of the left child, causing an imbalance. A left rotation at the left child followed by a right rotation at the unbalanced node restores balance.\n• Suppose you delete a node from the left subtree of the right child, causing an imbalance. A right rotation at the right child followed by a left rotation at the unbalanced node restores balance.\n• The time complexity of deleting a node in an AVL Tree is O(log n). This is because:\n• The search for the node to be deleted takes O(log n) time due to the balanced nature of the AVL Tree.\n• The deletion operation itself, followed by necessary rotations, also takes O(log n) time, ensuring the tree remains balanced.\n\n4. Visual Example of Deletion and Rotation\n\nLet’s consider an example where we delete nodes from an AVL Tree:\n• Node 20 is a leaf node and can be removed directly.\n• Update balance factors and check for any imbalances. No rotation is needed as the tree remains balanced.\n• Node 30 has one child, so replace it with its child, node 40.\n• The balance factor of node 50 becomes -2, indicating that the right subtree is heavier.\n• Multiple Rotations: In some cases, multiple rotations may be needed as the tree may become unbalanced at multiple levels after a deletion.\n• Imbalance Propagation: Deleting a node might cause an imbalance to propagate upwards, requiring careful attention as you move back up the tree.\n\nBy following these steps, you can efficiently delete nodes from an AVL Tree while ensuring that the tree remains balanced, preserving the logarithmic time complexity for future operations.\n\nSearching in an AVL Tree is similar to searching in a standard binary search tree (BST). However, the key advantage of using an AVL Tree is that it remains balanced, ensuring that the search operation is efficient, with a time complexity of O(log n).\n• Begin the search at the root node of the AVL Tree.\n• Compare the target value with the value of the current node:\n• If the target value is equal to the current node’s value: The search is successful, and the node is found.\n• If the target value is less than the current node’s value: Move to the left child and repeat the comparison.\n• If the target value is greater than the current node’s value: Move to the right child and repeat the comparison.\n• Continue traversing the tree by moving left or right, depending on the comparison, until you either find the target value or reach a leaf node.\n• If you reach a leaf node (a node with no children) and haven’t found the target value, it means the value is not present in the tree.\n• The time complexity of searching in an AVL Tree is O(log n) due to the tree’s balanced nature. This ensures that the height of the tree is kept logarithmic concerning the number of nodes, making the search process efficient.\n\n3. Example of Searching in an AVL Tree\n\nLet’s consider an example of searching in an AVL Tree:\n• Step 2: 50 > 40, so move to the right child (60).\n• Step 3: 50 < 60, so move to the left child (50).\n• Step 4: 50 == 50, so the search is successful, and the node is found.\n• Step 2: 25 < 40, so move to the left child (20).\n• Step 3: 25 > 20, so move to the right child (30).\n• Step 4: 25 < 30, and the left child of 30 is null, meaning 25 is not in the tree.\n• Efficiency: The balanced structure of the AVL Tree ensures that the height is minimized, preventing the worst-case scenario of O(n) seen in unbalanced BSTs.\n• Consistent Performance: Unlike unbalanced trees, which can degrade into a linked list in the worst case, AVL Trees consistently provide logarithmic search time, making them highly reliable for search operations.\n• Databases: AVL Trees are used in databases and other applications where quick lookups are essential.\n• Memory Management: They are also employed in memory management systems where efficient search operations are crucial for performance.\n\nBy ensuring that the tree remains balanced, AVL Trees offer a consistent and efficient search operation, making them a powerful data structure for scenarios where quick access to data is required.\n\nWhen studying AVL Trees, it’s important to understand how they compare with other types of trees, such as Binary Search Trees (BSTs), Red-Black Trees, and B-Trees. Each of these trees has its own characteristics, advantages, and disadvantages, making them suitable for different use cases.\n• AVL Tree: Always balanced, ensuring the height difference between the left and right subtrees of any node is at most 1.\n• BST: Can become unbalanced, especially in the case of sequential insertions, where it may degrade into a linked list, leading to O(n) time complexity for search, insertion, and deletion.\n• BST: The height can be O(n) in the worst case, reducing efficiency.\n• AVL Tree: Better for scenarios where frequent searches are performed because it guarantees logarithmic height.\n• BST: May perform better with fewer insertions and deletions, but its performance can degrade if the tree becomes unbalanced.\n• AVL Tree: Ideal for applications where search time is critical and frequent operations are performed on the tree.\n• BST: Suitable for simpler implementations or when insertions are expected to be random, maintaining a balanced structure naturally.\n• AVL Tree: Stricter balancing (balance factor between -1 and 1), leading to a more rigid structure.\n• Red-Black Tree: Less strict balancing (balance is maintained by ensuring no path is more than twice as long as any other), allowing for fewer rotations and faster insertions and deletions in some cases.\n• AVL Tree: More rotations may be needed during insertion and deletion to maintain balance.\n• Red-Black Tree: Fewer rotations, as it allows for a more relaxed balance, potentially leading to faster insertions and deletions.\n• AVL Tree: Shorter height due to stricter balancing, ensuring better performance in searches.\n• Red-Black Tree: Height is slightly taller but within a constant factor of the AVL Tree’s height.\n• AVL Tree: Better for search-intensive applications due to stricter balancing.\n• Red-Black Tree: May perform better in scenarios with frequent insertions and deletions, where the overhead of rotations in an AVL Tree would be a disadvantage.\n• AVL Tree: Used in applications requiring fast lookups and consistent performance, like databases and indexing.\n• Red-Black Tree: Often used in programming languages (e.g., Java’s TreeMap, C++ STL map) and other system-level implementations where a balance between insertion/deletion speed and lookup efficiency is needed.\n• B-Tree: Multi-way tree that stores multiple keys in each node and is used primarily in databases and file systems.\n• AVL Tree: Each node contains only one key and two children.\n• B-Tree: Each node can contain multiple keys and multiple children, reducing the tree’s height and minimizing disk access in databases.\n• AVL Tree: Height is O(log n) with respect to the number of nodes.\n• B-Tree: Height is also O(log n) but is typically much shorter due to the node’s capacity to hold multiple keys.\n• AVL Tree: Optimized for in-memory operations with fast search, insertion, and deletion times.\n• B-Tree: Optimized for disk-based storage, where minimizing disk access is more important than minimizing in-memory operations.\n• AVL Tree: Suitable for in-memory data structures where quick lookups and modifications are needed.\n• B-Tree: Ideal for databases and file systems where large amounts of data are stored on disk, and minimizing disk I/O is critical.\n\nAVL Trees excel in scenarios where consistent and efficient search times are crucial, thanks to their strict balancing rules. However, they may require more rotations compared to Red-Black Trees, making the latter a better choice for applications with frequent insertions and deletions. B-Trees, with their multi-way node structure, are preferred in database systems where minimizing disk access is essential.\n\nUnderstanding these differences helps in selecting the appropriate tree structure based on the specific requirements of the application, ensuring optimal performance and resource utilization.\n\nAVL Trees are widely used in various applications that require efficient and reliable data structures for managing sorted data. Their strict balancing property ensures that operations such as search, insertion, and deletion remain efficient, even as the tree grows.\n• Efficient Search Operations: AVL Trees are commonly used in database indexing where fast search operations are crucial. The balanced nature of AVL Trees ensures that queries can be executed quickly, even with large datasets.\n• Maintaining Order: They are used to maintain sorted order of records, enabling efficient range queries and ordered traversals.\n• Dynamic Memory Allocation: In memory management systems, AVL Trees are used to manage free blocks of memory. The tree helps in quickly finding the best-fit block of memory for allocation, reducing fragmentation.\n• Garbage Collection: AVL Trees can be used in garbage collection algorithms to keep track of memory addresses and efficiently manage the allocation and deallocation of memory.\n• Directory Management: AVL Trees can be used to manage file directories where quick access to files and directories is required. The balanced structure ensures that searching for a file or directory remains efficient, even as the number of files increases.\n• File Indexing: In some file systems, AVL Trees are used to index file data, allowing quick access and retrieval of information.\n• Routing Table Management: AVL Trees can be used in network routers to manage routing tables. The balanced structure allows for efficient lookups and updates, ensuring that packets are routed quickly and correctly.\n• IP Address Management: AVL Trees can also be used to manage IP addresses in a network, ensuring efficient allocation and retrieval of IP addresses.\n• Syntax Trees: AVL Trees can be used to represent abstract syntax trees in compilers, ensuring efficient traversal and manipulation of the tree during code analysis and optimization.\n• Symbol Table Management: AVL Trees are used to implement symbol tables, which store variable names and other identifiers. The balanced nature of the AVL Tree ensures that lookups, insertions, and deletions in the symbol table are efficient.\n• Real-Time Data Processing: AVL Trees are used in in-memory databases where data is stored in memory for fast access and processing. The balanced structure of AVL Trees ensures that operations remain efficient, even as the data grows.\n• Caching Mechanisms: AVL Trees can be used in caching mechanisms to maintain the order of cached items, enabling quick retrieval and efficient cache management.\n• Range Queries: AVL Trees are used in geospatial applications to manage spatial data. They allow for efficient range queries, such as finding all points within a certain distance from a given location.\n• Spatial Indexing: AVL Trees can be used to index spatial data, ensuring quick access to geographical information.\n• Event Scheduling: AVL Trees are used in event-driven systems to manage events that need to be processed in a specific order. The tree helps in efficiently scheduling and retrieving events based on their priority or timestamp.\n• Priority Queues: AVL Trees can be used to implement priority queues where tasks or events are prioritized. The balanced structure ensures that the highest-priority task can be quickly accessed and processed.\n• Order Book Management: In financial trading systems, AVL Trees can be used to manage order books, where buy and sell orders are stored in a sorted manner. The tree ensures that the best available price can be quickly matched.\n• Risk Management: AVL Trees can be used to track and manage financial data in risk management systems, ensuring efficient access to relevant data for quick decision-making.\n• Collision Detection: AVL Trees can be used in game development to manage objects in a game world. The balanced structure ensures that collision detection and other spatial queries are performed efficiently.\n• State Management: In complex games, AVL Trees can be used to manage the state of various game elements, allowing for quick updates and retrievals.\n\nAVL Trees are a versatile data structure with a wide range of applications across different domains. Their ability to maintain balance ensures that operations remain efficient, making them ideal for use in systems where performance and reliability are critical.\n\nImplementing an AVL Tree involves creating the tree structure, managing nodes, and ensuring that the tree remains balanced after each insertion and deletion. The implementation includes defining the tree node structure, handling rotations, and performing insertions and deletions.\n\nIn an AVL Tree, each node typically contains the following components:\n• Key/Value: The value stored in the node.\n• Right Child: A reference to the right child node.\n• Height: The height of the node (or alternatively, the balance factor can be used).\n\nTo maintain balance in an AVL Tree, rotations are used. There are four types of rotations:\n• Right Rotation (LL Rotation): Used when the left subtree of the left child is too tall.\n• Left Rotation (RR Rotation): Used when the right subtree of the right child is too tall.\n• Left-Right Rotation (LR Rotation): Used when the left subtree of the right child is too tall.\n• Right-Left Rotation (RL Rotation): Used when the right subtree of the left child is too tall.\n\nExample of Rotations in Python:\n\nInsertion in an AVL Tree involves inserting the node as in a regular BST, followed by updating heights and balancing the tree using rotations if necessary.\n\nExample of Insertion in Python:\n\nDeletion involves removing a node and then balancing the tree. The deletion process has to handle three cases, similar to standard BST deletion, followed by rotations to maintain balance.\n\nExample of Deletion in Python:\n\nSearching in an AVL Tree follows the same principles as searching in a standard BST.\n\nExample of Searching in Python:\n\nTraversal methods are used to visit all nodes in the tree in a specific order: In-order, Pre-order, and Post-order.\n\nExample of Traversals in Python:\n• Node Structure: Defines the structure of nodes in the AVL Tree.\n• Rotations: Implemented to maintain the balance of the tree.\n• Insertion: Adds a new node and ensures the tree remains balanced.\n\nImplementing an AVL Tree requires careful management of rotations and balancing to maintain its efficiency. The strict balancing ensures that operations remain efficient, making AVL Trees suitable for applications requiring fast and reliable data management.\n\nAn AVL Tree is a self-balancing binary search tree (BST) where the height difference between the left and right subtrees of any node is no more than one. This strict balancing criterion ensures that operations such as search, insertion, and deletion are efficient, maintaining a time complexity of O(log n) for these operations.\n• AVL Tree: A type of binary search tree with an added constraint to ensure that the tree remains balanced. This balance guarantees logarithmic time complexity for basic operations, which is crucial for performance in various applications.\n• Balance Factor: The difference in heights between the left and right subtrees of any node, which must be -1, 0, or 1.\n• Height: The height of an AVL Tree is kept at O(log n), where n is the number of nodes, ensuring efficient operations.\n• Left-Right Rotation (LR Rotation): Corrects imbalance where a left subtree’s right child is taller.\n• Right-Left Rotation (RL Rotation): Corrects imbalance where a right subtree’s left child is taller.\n• Process: Insert as in a standard BST, then perform necessary rotations to maintain balance. The insertion process ensures that the AVL Tree remains balanced after adding a new node.\n• Process: Remove the node as in a standard BST, then adjust the tree and perform rotations if necessary to restore balance. The deletion process ensures the tree remains balanced after a node is removed.\n• Process: Searching in an AVL Tree follows the same principles as a standard BST, but with the assurance that the tree’s balance provides efficient search operations.\n• BST: AVL Trees offer better balance compared to traditional BSTs, leading to more efficient operations.\n• Red-Black Trees: AVL Trees are stricter in balance, resulting in faster lookups but potentially more rotations during insertions and deletions.\n• B-Trees: AVL Trees are suited for in-memory operations, whereas B-Trees are used for disk-based storage and manage larger data sets.\n• Memory Management: Used for dynamic allocation and deallocation of memory blocks.\n• Compiler Design: Used in syntax trees and symbol table management.\n• Game Development: Used for collision detection and game state management.\n• Node Structure: Defines the components of each node in the AVL Tree.\n• Rotations: Critical for maintaining balance during insertion and deletion.\n• Insertion and Deletion: Ensure that the AVL Tree remains balanced after modifications.\n• Searching and Traversal: Efficient methods for accessing and manipulating tree data.\n\nAVL Trees are a powerful and versatile data structure that provide efficient and balanced operations for dynamic data management. Understanding their properties, implementation details, and applications helps in choosing the right data structure for specific use cases and ensures optimal performance in various computational scenarios.\n\nPracticing problems related to AVL Trees can help solidify your understanding of their structure and operations. Here are some practice problems that cover various aspects of AVL Trees:\n• Insert the following sequence of keys into an empty AVL Tree: . Draw the AVL Tree after each insertion and show the rotations performed to maintain balance.\n• Given an AVL Tree, delete the node with key . Show the AVL Tree after deletion and any necessary rotations to restore balance.\n• Implement a search function for an AVL Tree and use it to find nodes with keys and in a given AVL Tree.\n• Write a function to calculate the height of an AVL Tree and use it to find the height of a sample AVL Tree.\n• Given an AVL Tree with the root node , left child , and left-left child , show the result of performing a right rotation.\n• Given an AVL Tree with the root node , right child , and right-right child , show the result of performing a left rotation.\n• Given an AVL Tree with the root node , left child , and left-right child , show the result of performing a left-right rotation.\n• Given an AVL Tree with the root node , right child , and right-left child , show the result of performing a right-left rotation.\n• Given an AVL Tree with nodes , calculate the balance factor for each node and determine if any rotations are needed to maintain balance.\n• Given an AVL Tree where inserting a node causes an imbalance, identify the type of imbalance and determine the necessary rotations to restore balance.\n• Construct an AVL Tree from the following sequence of keys: . Show the tree after each insertion and any rotations performed.\n• Given an AVL Tree, implement in-order, pre-order, and post-order traversal functions. Additionally, write a function to find the in-order successor of a node with a given key.\n• Given an AVL Tree with nodes , delete the node with key . Show the AVL Tree after deletion and any necessary rotations to restore balance.\n• Given a sorted array , construct an AVL Tree. Show the steps involved in the construction and the final balanced AVL Tree.\n• Compare the performance of AVL Trees with other balanced trees like Red-Black Trees and B-Trees for insertion, deletion, and search operations. Discuss the advantages and disadvantages in different scenarios.\n• Consider a scenario where an AVL Tree is used to manage dynamic data in a real-world application (e.g., a file system, a database index). Discuss how AVL Trees would handle various operations and the impact on performance.\n• Implement an AVL Tree class with methods for insertion, deletion, searching, and traversal. Write test cases to validate the functionality of the AVL Tree.\n• After implementing the AVL Tree, write test cases to verify that the tree remains balanced after various operations, including insertions and deletions.\n\nBy working through these practice problems, you will gain a deeper understanding of AVL Trees and how to manage their balance and operations effectively.\n\nRead other awesome articles in Medium.com or in akcoding’s posts."
    },
    {
        "link": "https://stackoverflow.com/questions/29000192/pointers-structures-and-dynamic-memory-allocation-in-c",
        "document": "I have two structures in which the second structure has the first structure nested inside of it:\n\nNow the problem is that the second structure has to be dynamically allocated through a pointer. Also, the nested first structure has to be a dynamically allocated array through a pointer, whose array size has to be read in through an input file.\n\nI know how to read it in but I don't know how to access it. For example lets suppose the size is 8. How would I go about specifying the values for the second structure given the pointer's format?\n\nI tried assuming points to first structure and points to second structure where we can process it through a loop. This doesn't work. So I was wondering how you would initialize the values the second structure whose member includes all the n structures in the n element array."
    },
    {
        "link": "https://stackoverflow.com/questions/68243749/can-someone-explain-me-how-to-dynamically-allocate-memory-for-a-vectors-of-point",
        "document": "It looks like incomplete code. It should be something like\n\nbecause is a template type with type parameter and must be template to allow template parameter . I also question sanity of this code. If it describes a node of tree containing data of type T (which may be ), then\n\nA in context of declaration is a vector of pointers to .\n\nNow consider following code. I added something useful to to show off how you can allocate memory. To avoid confusion, a new type would serve as T instead of .\n\nWe can use a smart pointer instead, avoid usage of new\n\nCan you figure what it would do?\n\nSome Data died there. That's because this setup creates some temporaries of T = Data, Data was copied or moved and we didn't saw it. It's ok, if of treenode is an . For a complex type it can be ab issue, if our Data didn't have proper copy\\move operations. Let's add constructors at least.\n\nNow we can see copy operation:\n\nFOr simple, \"flat\" types this can be fine, copy and move operation are equal for them. But for some containers creation of temporary object which we never will use again and then making a copy of allocated memory is expensive. It's agreed that \"move\" operation would transfer ownership of allocated resources to target object and modify the source so it will not deallocate them. Source object would become invalid after that.We must have add support for move and copy of T to our class.\n\nNow everything adds up:\n\nHaving two implementations is excessive. In some case, where more than one argument is involved, it may lead to exponential explosion in amount of declarations. All we need is to use single perfect forwarding.\n\nThis would lead to operation like this:\n\nAs a last touch would need to define a default constructor for a tree node, a move constructors as well, to finish its design to the Rule of 5. A copy constructor may or maybe not correct thing to do, it depends if we want allow to create a copy of tree, just of the node's value, or disallow it at all.\n\nData needs its special members too:"
    },
    {
        "link": "https://geeksforgeeks.org/cpp-pointers",
        "document": "A pointer is a variable that stores the address of another variable. Pointers can be used with any data type, including basic types (e.g., int, char), arrays, and even user-defined types like classes and structures.\n\nA pointer can be declared in the same way as any other variable but with an asterisk symbol (*) as shown:\n\nHere, data_type is the type of data that the pointer is pointing to, and name is the name assigned to the pointer. The * symbol is also called dereference operator.\n\nIn the above statement, we create a pointer ptr that can store the address of an integer data. It is pronounced as “Pointer to Integer” or “Integer Pointer”\n\nThe addressof operator (&) determines the address of any variable in C++. This address can be assigned to the pointer variable to initialize it.\n\nIn the above statement, pointer ptr store the address of variable val using address-of operator (&). The pointer and the variable should be of same type, othewise type mismatch error occurs.\n\nThe process of accessing the value present at the memory address pointed by the pointer is called dereferencing. This is done with the help of dereferencing operator as shown:\n\nDirectly accessing the pointer will just give us the address that is stored in the pointer.\n\nThe below image demonstrates how pointer works.\n\nThe pointer can be modified to point to another memory address if its is of the same type.\n\nThe size of pointer in a system is equal for every pointer no matter what type of data it is pointing to. It does not depend on the type, but on operating system and CPU architecture. The size of pointers in C++ is\n\nThe logic is simple: pointers store the addresses of the memory and in a computer, the maximum width of a memory address is determined by the CPU architecture. For example, for a 64-bit CPU, the address will always be 64-bit wide. This can be verified using sizeof operator.\n\nAs we can see, both the pointers have same size. It’s a 64-bit system, so the size is 8 bytes.\n\nThere are 4 special types of pointers that used or referred to in different contexts:\n\nWhen a pointer is created, it just contains a random address that may or may not be valid. This type of pointer is called wild pointer.\n\nDereferencing this pointer may lead to errors such as segmentation faults. So, it is always recommended to initialize a pointer.\n\nA NULL pointer is a pointer that does not point to any valid memory location but NULL. It is often used to initialize a pointer when you do not want it to point to any object.\n\nThis is a special type of pointer available in C++ which represents the absence of type. Void pointers are pointers that point to a value that has no particular type. It also means that they can point to any data type.\n\nBut as the datatype is not known, the compiler also does not know how many bytes to access when the void pointer is dereferenced, so these pointers cannot be directly dereferenced. They have to be first converted into some other pointer type that points to a concrete data type before being dereferenced.\n\nA dangling pointer is a pointer that was pointing to the valid memory location earlier, but that memory has been taken from the program, and it is still pointing to it. This pointer behaves like a wild pointer and may lead to errors.\n\nIn C++, pointers and arrays are closely related. An array name acts like a constant pointer to the first element of the array. If we assign this value to a non-constant pointer of the same type, then we can access the elements of the array with this pointer using pointer arithmetic.\n\nPointer arithmetic refers to the operations that C++ allows on the pointers. They include:\n• Subtraction of Two Pointers of the Same Type\n\nWe can also have pointers that point to other pointers. This is called a double pointer, and it is useful when working with multi-level data structures like arrays of pointers or dynamic memory management.\n\nIn C++, a function pointer is used to point to functions, similar to how pointers are used to point to variables. It allows you to save the address of a function. Function pointers can be used to call a function indirectly, or they can be passed as arguments to another function, enabling dynamic function invocation and flexibility in function handling.\n\nA smart pointer is a wrapper class around a pointer that overloads operators like * and ->. Smart pointer objects resemble normal pointers, they have the added functionality of automatically deallocating and freeing the memory of the object when it is no longer needed, unlike regular pointers.\n\nUnderstanding the differences between pointers and references in C++. Both are used to refer to other variables, but they operate in different ways and have different behavior.\n\nPointers are a useful concept in C++ that allow direct access to memory addresses, providing greater control over memory and data manipulation. Below are some primary uses of pointers in C++:\n• Dynamic Memory Allocation: Pointers allow memory to be allocated dynamically at runtime using operators like new and delete. This enables the creation of objects or arrays whose sizes are determined during execution.\n• Implementing Data Structures: Pointers are used to implementing complex data structures such as linked lists, trees, and graphs, where elements are dynamically allocated and linked together.\n• Pass Arguments by Pointer: Pass the argument with their address and make changes in their value using pointer. So that the values get changed into the original argument.\n\nWhy are pointers important in C++?\n\nWhat is the size of a pointer in C++?\n\nWhy the type of the pointer is needed when all the pointers are of same size?\n\nWhat is a dangling pointer in C++?"
    },
    {
        "link": "https://learn.saylor.org/mod/page/view.php?id=18971",
        "document": "Pointers are a powerful and dangerous feature of C.\n\nA pointer value is the address of a variable or a chunk of dynamically-allocated memory. Pointer values are very much like references in Java.\n\nA pointer variable is a variable that contains a pointer value.\n\nA pointer type in C is a datatype that can be used to declare a pointer variable.\n\nExample: the type \"pointer to int\" is\n\nTo declare a variable p as a pointer to int:\n\nWe can get a pointer value specifying the address of any variable by using &, the address-of operator. For example:\n\nWe can retrieve the value stored in the memory location to which a pointer points using *, the dereferenceoperator.\n\nIn many contexts, C treats the types \"array of X\" and \"pointer to X\" interchangeably. Consider the variable declaration\n\nwhich declares the variable p to have type \"pointer to int\". We can treat p as pointing to a single intvariable, or as pointing to an array of int variables.\n\nDynamic memory allocation is the allocation of a chunk of memory of a specified size. The malloc function performs dynamic memory allocation in C. The function call\n\nallocates a chunk of memory of size numBytes, and returns a pointer to it. You can think of malloc as being like the new operator in Java and C++.\n\nPointer/array duality is often used to allow the allocation of arrays with an arbitrary number of elements. For example, let's say that we have an integer variable n, and we need an array of n double elements. We can use malloc to allocate the array, and a variable of type \"pointer to double\" to refer to it:\n\nThe sizeof operator computes the number of bytes required to store a single instance of a given datatype, so sizeof(double) yields the number of bytes needed to store a single double value.\n\nNote that malloc returns a \"generic\" pointer value that is safe to assign to a variable declared as any pointer type. So, we could use malloc to create a pointer of double elements, int elements, etc.\n\nUnlike Java, C does not use garbage collection to automatically reclaim unused dynamically-allocated memory. Your program must manually free such memory when it is no longer needed, using the free function:\n\nThis would free the chunk of memory to which the variable arr points.\n\nInstances of structs are often allocated dynamically. For example:\n\nIn this example, the variable c points to a dynamically-allocated instance of the Coord struct.\n\nIn theory, we could use the dereference operator (*) and the member selection operator (.) together to access the fields of a struct via a pointer:\n\nThe parentheses are needed because the member selection operator has higher precedence than the dereference operator.\n\nBecause access fields of struct instances through pointers is so important in C, there is a special operator to do it: the arrow, ->. So, the code above could be written as\n\nYou should always use the arrow operator to access fields of a struct instance through a pointer.\n\nComplex C programs are often \"object-oriented\": they represent information as instances of struct types, and access those instances via pointers. To draw an analogy with Java:\n\nBecause a C struct type can't contain functions (methods), instead each C struct type will have functions that take a pointer to an instance of that struct type as the first parameter: these functions are essentially methods.\n\nExample: Grid datatype from assignment 1. In the header file:\n\nHere, we are declaring a datatype called Grid. \"typedef\" is used so that this type can be referred to as Gridinstead of struct Grid.\n\nAn instance of Grid represents a two-dimensional grid of uint8_t values: see assignment 1 for details.\n\nThe Grid datatype has functions that create, destroy, and do operations on instances of Grid:\n\nAside from grid_alloc, which creates an instance of Grid and returns a pointer to it, each function takes a pointer to a Grid instance as its first parameter.\n\nSource: David Hovemeyer, https://ycpcs.github.io/cs365-spring2017/lectures/lecture01.html\n\n This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 License."
    },
    {
        "link": "https://icarus.cs.weber.edu/~dab/cs1410/textbook/4.Pointers/uses.html",
        "document": "The sections of this chapter introduced the concept of pointers and all the operators needed to work with them and dynamic memory. Pointers and their operators are a recurring theme we encounter throughout most of the remaining chapters of the text. But, it is often easier to understand abstract concepts like pointers with a concrete use. So, before we end the chapter, let's look at two detailed, albeit simplified, examples to help solidify the pointer concept and syntax.\n\nProgrammers use pointers and dynamic memory allocation to create dynamic or linked data structures. The names describe a data structure constructed with dynamic memory allocated on the heap and linked with pointers, making both appropriate. Figures 1 and 2 illustrate two examples of these structures: linked lists and binary trees. Programmers call the large boxes in the examples nodes, and the arrows represent pointers called links. C++ programs implement nodes as objects instantiated from structures (the next chapter) or classes. Nodes contain the data we wish to store in the structure and one or more pointers linking them together. The \"data\" may span a broad spectrum of complexity - anything from a single data element like a character to thousands of complex data elements.\n\nProgrammers can implement dynamic data structures and their nodes in many ways. For example, a node can embed the data or have a pointer to a separate structure containing the data. Lists can be sorted (e.g., alphabetically) or unsorted. The link in the last list node may be or point to the first node in the list. A list may have links pointing in one direction or two. Binary trees can be balanced (keeping the left and right sides about the same height) or unbalanced. Sometimes, binary trees are back-threaded with pointers from the nodes back up the tree. When using dynamic structures, programmers always match the structure's behaviors with the problem they are solving.\n\nWe conclude with a few simplified code fragments implementing a linked list append operation. The code illustrates some of the pointer operators introduced in the chapter. The code is a brief introduction and may seem mysterious now, but it will become understandable as you continue your studies. The following statements assume the program has a class named that has:"
    }
]