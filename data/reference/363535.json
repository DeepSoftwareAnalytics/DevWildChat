[
    {
        "link": "https://stackoverflow.com/questions/12806386/is-there-any-standard-for-json-api-response-format",
        "document": "Do standards or best practices exist for structuring JSON responses from an API? Obviously, every application's data is different, so that much I'm not concerned with, but rather the \"response boilerplate\", if you will. An example of what I mean:\n\nAssuming you question is about REST webservices design and more precisely concerning success/error. I think there are 3 different types of design.\n• None Use only HTTP Status code to indicate if there was an error and try to limit yourself to the standard ones (usually it should suffice).\n• Pros: It is a standard independent of your api.\n• Cons: Less information on what really happened.\n• None Use HTTP Status + json body (even if it is an error). Define a uniform structure for errors (ex: code, message, reason, type, etc) and use it for errors, if it is a success then just return the expected json response.\n• Pros: Still standard as you use the existing HTTP status codes and you return a json describing the error (you provide more information on what happened).\n• Cons: The output json will vary depending if it is a error or success.\n• None Forget the http status (ex: always status 200), always use json and add at the root of the response a boolean responseValid and a error object (code,message,etc) that will be populated if it is an error otherwise the other fields (success) are populated.\n• None Pros: The client deals only with the body of the response that is a json string and ignores the status(?). It's up to you to choose :) Depending on the API I would choose 2 or 3 (I prefer 2 for json rest apis). Another thing I have experienced in designing REST Api is the importance of documentation for each resource (url): the parameters, the body, the response, the headers etc + examples. I would also recommend you to use jersey (jax-rs implementation) + genson (java/json databinding library). You only have to drop genson + jersey in your classpath and json is automatically supported.\n• None Solution 2 is the hardest to implement but the advantage is that you can nicely handle exceptions and not only business errors, initial effort is more important but you win on the long term.\n• None Solution 3 is the easy to implement on both, server side and client but it's not so nice as you will have to encapsulate the objects you want to return in a response object containing also the responseValid + error.\n\nI will not be as arrogant to claim that this is a standard so I will use the \"I prefer\" form. I prefer terse response (when requesting a list of /articles I want a JSON array of articles). In my designs I use HTTP for status report, a 200 returns just the payload. 400 returns a message of what was wrong with request: If there was error with processing on my side, I return 501 with a message: {\"message\" : \"Could not connect to data store.\"} From what I've seen quite a few REST-ish frameworks tend to be along these lines. JSON is supposed to be a payload format, it's not a session protocol. The whole idea of verbose session-ish payloads comes from the XML/SOAP world and various misguided choices that created those bloated designs. After we realized all of it was a massive headache, the whole point of REST/JSON was to KISS it, and adhere to HTTP. I don't think that there is anything remotely standard in either JSend and especially not with the more verbose among them. XHR will react to HTTP response, if you use jQuery for your AJAX (like most do) you can use / and / callbacks to capture errors. I can't see how encapsulating status reports in JSON is any more useful than that.\n\nFor what it's worth I do this differently. A successful call just has the JSON objects. I don't need a higher level JSON object that contains a success field indicating true and a payload field that has the JSON object. I just return the appropriate JSON object with a 200 or whatever is appropriate in the 200 range for the HTTP status in the header. However, if there is an error (something in the 400 family) I return a well-formed JSON error object. For example, if the client is POSTing a User with an email address and phone number and one of these is malformed (i.e. I cannot insert it into my underlying database) I will return something like this: Important bits here are that the \"field\" property must match the JSON field exactly that could not be validated. This allows clients to know exactly what went wrong with their request. Also, \"message\" is in the locale of the request. If both the \"emailAddress\" and \"phoneNumber\" were invalid then the \"errors\" array would contain entries for both. A 409 (Conflict) JSON response body might look like this: { \"description\" : \"Already Exists\" \"errors\" : [ { \"field\" : \"phoneNumber\", \"message\" : \"Phone number already exists for another user.\" } ], } With the HTTP status code and this JSON the client has all they need to respond to errors in a deterministic way and it does not create a new error standard that tries to complete replace HTTP status codes. Note, these only happen for the range of 400 errors. For anything in the 200 range I can just return whatever is appropriate. For me it is often a HAL-like JSON object but that doesn't really matter here. The one thing I thought about adding was a numeric error code either in the the \"errors\" array entries or the root of the JSON object itself. But so far we haven't needed it.\n\nThe point of JSON is that it is completely dynamic and flexible. Bend it to whatever whim you would like, because it's just a set of serialized JavaScript objects and arrays, rooted in a single node. What the type of the rootnode is is up to you, what it contains is up to you, whether you send metadata along with the response is up to you, whether you set the mime-type to or leave it as is up to you (as long as you know how to handle the edge cases). Build a lightweight schema that you like.\n\n Personally, I've found that analytics-tracking and mp3/ogg serving and image-gallery serving and text-messaging and network-packets for online gaming, and blog-posts and blog-comments all have very different requirements in terms of what is sent and what is received and how they should be consumed. So the last thing I'd want, when doing all of that, is to try to make each one conform to the same boilerplate standard, which is based on XML2.0 or somesuch. That said, there's a lot to be said for using schemas which make sense to you and are well thought out.\n\n Just read some API responses, note what you like, criticize what you don't, write those criticisms down and understand why they rub you the wrong way, and then think about how to apply what you learned to what you need.\n\nI used to follow this standard, was pretty good, easy, and clean on the client layer. Normally, the HTTP status 200, so that's a standard check which I use at the top. and I normally use the following JSON I also use a template for the API's dynamic response; try { // query and what not. response.payload = new { data = new { pagination = new Pagination(), customer = new Customer(), notifications = 5 } } // again something here if we get here success has to be true // I follow an exit first strategy, instead of building a pyramid // of doom. response.success = true; } catch(Exception exception){ response.success = false; response.message = exception.GetStackTrace(); _logger.Fatal(exception, this.GetFacadeName()) } return response; { \"success\": boolean, \"message\": \"some message\", \"payload\": { \"data\" : [] \"message\": \"\" ... // put whatever you want to here. } } on the client layer I would use the following: if(response.code != 200) { // woops something went wrong. return; } if(!response.success){ console.debug ( response.message ); return; } // if we are here then success has to be true. if(response.payload) { .... } notice how I break early avoiding the pyramid of doom.\n\nThere is no lawbreaking or outlaw standard other than common sense. If we abstract this like two people talking, the standard is the best way they can accurately understand each other in minimum words in minimum time. In our case, 'minimum words' is optimizing bandwidth for transport efficiency and 'accurately understand' is the structure for parser efficiency; which ultimately ends up with the less the data, and the common the structure; so that it can go through a pin hole and can be parsed through a common scope (at least initially). Almost in every cases suggested, I see separate responses for 'Success' and 'Error' scenario, which is kind of ambiguity to me. If responses are different in these two cases, then why do we really need to put a 'Success' flag there? Is it not obvious that the absence of 'Error' is a 'Success'? Is it possible to have a response where 'Success' is TRUE with an 'Error' set? Or the way, 'Success' is FALSE with no 'Error' set? Just one flag is not enough? I would prefer to have the 'Error' flag only, because I believe there will be less 'Error' than 'Success'. Also, should we really make the 'Error' a flag? What about if I want to respond with multiple validation errors? So, I find it more efficient to have an 'Error' node with each error as child to that node; where an empty (counts to zero) 'Error' node would denote a 'Success'."
    },
    {
        "link": "https://stackoverflow.blog/2020/03/02/best-practices-for-rest-api-design",
        "document": "REST APIs are one of the most common kinds of web interfaces available today. They allow various clients including browser apps to communicate with services via the REST API. Therefore, it's very important to design REST APIs properly so that we won't run into problems down the road. We have to take into account security, performance, and ease of use for API consumers.\n\nOtherwise, we create problems for clients that use our APIs, which isn’t pleasant and detracts people from using our API. If we don’t follow commonly accepted conventions, then we confuse the maintainers of the API and the clients that use them since it’s different from what everyone expects.\n\nIn this article, we'll look at how to design REST APIs to be easy to understand for anyone consuming them, future-proof, and secure and fast since they serve data to clients that may be confidential.\n• Use nouns instead of verbs in endpoint paths\n\nA REST API is an application programming interface architecture style that conforms to specific architectural constraints, like stateless communication and cacheable data. It is not a protocol or standard. While REST APIs can be accessed through a number of communication protocols, most commonly, they are called over HTTPS, so the guidelines below apply to REST API endpoints that will be called over the internet.\n\nNote: For REST APIs called over the internet, you'll like want to follow the best practices for REST API authentication.\n\nEven though some people think REST should only return hypertext (including Roy Fielding who created the term) REST APIs should accept JSON for request payload and also send responses to JSON. JSON is the standard for transferring data. Almost every networked technology can use it: JavaScript has built-in methods to encode and decode JSON either through the Fetch API or another HTTP client. Server-side technologies have libraries that can decode JSON without doing much work.\n\nThere are other ways to transfer data. XML isn’t widely supported by frameworks without transforming the data ourselves to something that can be used, and that’s usually JSON. We can’t manipulate this data as easily on the client-side, especially in browsers. It ends up being a lot of extra work just to do normal data transfer.\n\nForm data is good for sending data, especially if we want to send files. But for text and numbers, we don’t need form data to transfer those since—with most frameworks—we can transfer JSON by just getting the data from it directly on the client side. It’s by far the most straightforward to do so.\n\nTo make sure that when our REST API app responds with JSON that clients interpret it as such, we should set Content-Type in the response header to application/json after the request is made. Many server-side app frameworks set the response header automatically. Some HTTP clients look at the Content-Type response header and parse the data according to that format.\n\nThe only exception is if we’re trying to send and receive files between client and server. Then we need to handle file responses and send form data from client to server. But that is a topic for another time.\n\nWe should also make sure that our endpoints return JSON as a response. Many server-side frameworks have this as a built-in feature.\n\nLet’s take a look at an example API that accepts JSON payloads. This example will use the Express back end framework for Node.js. We can use the body-parser middleware to parse the JSON request body, and then we can call the res.json method with the object that we want to return as the JSON response as follows:\n\nbodyParser.json() parses the JSON request body string into a JavaScript object and then assigns it to the req.body object.\n\nSet the Content-Type header in the response to application/json; charset=utf-8 without any changes. The method above applies to most other back end frameworks.\n\nUse nouns instead of verbs in endpoint paths\n\nWe shouldn't use verbs in our endpoint paths. Instead, we should use the nouns which represent the entity that the endpoint that we're retrieving or manipulating as the pathname.\n\nThis is because our HTTP request method already has the verb. Having verbs in our API endpoint paths isn’t useful and it makes it unnecessarily long since it doesn’t convey any new information. The chosen verbs could vary by the developer’s whim. For instance, some like ‘get’ and some like ‘retrieve’, so it’s just better to let the HTTP GET verb tell us what and endpoint does.\n\nThe action should be indicated by the HTTP request method that we're making. The most common methods include GET, POST, PUT, and DELETE.\n• POST submits new data to the server.\n\nWith the two principles we discussed above in mind, we should create routes like GET /articles/ for getting news articles. Likewise, POST /articles/ is for adding a new article , PUT /articles/:id is for updating the article with the given id. DELETE /articles/:id is for deleting an existing article with the given ID.\n\n/articles represents a REST API resource. For instance, we can use Express to add the following endpoints for manipulate articles as follows:\n\nIn the code above, we defined the endpoints to manipulate articles. As we can see, the path names do not have any verbs in them. All we have are nouns. The verbs are in the HTTP verbs.\n\nThe POST, PUT, and DELETE endpoints all take JSON as the request body, and they all return JSON as the response, including the GET endpoint.\n\nWhen designing endpoints, it makes sense to group those that contain associated information. That is, if one object can contain another object, you should design the endpoint to reflect that. This is good practice regardless of whether your data is structured like this in your database. In fact, it may be advisable to avoid mirroring your database structure in your endpoints to avoid giving attackers unnecessary information.\n\nFor example, if we want an endpoint to get the comments for a news article, we should append the /comments path to the end of the /articles path. We can do that with the following code in Express:\n\nIn the code above, we can use the GET method on the path '/articles/:articleId/comments'. We get comments on the article identified by articleId and then return it in the response. We add 'comments' after the '/articles/:articleId' path segment to indicate that it's a child resource of /articles.\n\nThis makes sense since comments are the children objects of the articles, assuming each article has its own comments. Otherwise, it’s confusing to the user since this structure is generally accepted to be for accessing child objects. The same principle also applies to the POST, PUT, and DELETE endpoints. They can all use the same kind of nesting structure for the path names.\n\nHowever, nesting can go too far. After about the second or third level, nested endpoints can get unwieldy. Consider, instead, returning the URL to those resources instead, especially if that data is not necessarily contained within the top level object.\n\nFor example, suppose you wanted to return the author of particular comments. You could use /articles/:articleId/comments/:commentId/author. But that's getting out of hand. Instead, return the URI for that particular user within the JSON response instead:\n\nTo eliminate confusion for API users when an error occurs, we should handle errors gracefully and return HTTP response codes that indicate what kind of error occurred. This gives maintainers of the API enough information to understand the problem that’s occurred. We don’t want errors to bring down our system, so we can leave them unhandled, which means that the API consumer has to handle them.\n• 401 Unauthorized - This means the user isn't not authorized to access a resource. It usually returns when the user isn't authenticated.\n• 403 Forbidden - This means the user is authenticated, but it's not allowed to access a resource.\n• 404 Not Found - This indicates that a resource is not found.\n• 500 Internal server error - This is a generic server error. It probably shouldn't be thrown explicitly.\n• 502 Bad Gateway - This indicates an invalid response from an upstream server.\n• 503 Service Unavailable - This indicates that something unexpected happened on server side (It can be anything like server overload, some parts of the system failed, etc.).\n\nWe should be throwing errors that correspond to the problem that our app has encountered. For example, if we want to reject the data from the request payload, then we should return a 400 response as follows in an Express API:\n\nIn the code above, we have a list of existing users in the users array with the given email.\n\nThen if we try to submit the payload with the email value that already exists in users, we'll get a 400 response status code with a 'User already exists' message to let users know that the user already exists. With that information, the user can correct the action by changing the email to something that doesn't exist.\n\nError codes need to have messages accompanied with them so that the maintainers have enough information to troubleshoot the issue, but attackers can’t use the error content to carry our attacks like stealing information or bringing down the system.\n\nWhenever our API does not successfully complete, we should fail gracefully by sending an error with information to help users make corrective action.\n\nThe databases behind a REST API can get very large. Sometimes, there's so much data that it shouldn’t be returned all at once because it’s way too slow or will bring down our systems. Therefore, we need ways to filter items.\n\nWe also need ways to paginate data so that we only return a few results at a time. We don't want to tie up resources for too long by trying to get all the requested data at once.\n\nFiltering and pagination both increase performance by reducing the usage of server resources. As more data accumulates in the database, the more important these features become.\n\nHere’s a small example where an API can accept a query string with various query parameters to let us filter out items by their fields:\n\nIn the code above, we have the req.query variable to get the query parameters. We then extract the property values by destructuring the individual query parameters into variables using the JavaScript destructuring syntax. Finally, we run filter on with each query parameter value to locate the items that we want to return.\n\nOnce we have done that, we return the results as the response. Therefore, when we make a GET request to the following path with the query string:\n\nas the returned response since we filtered by lastName and age.\n\nLikewise, we can accept the page query parameter and return a group of entries in the position from (page - 1) * 20 to page * 20.\n\nWe can also specify the fields to sort by in the query string. For instance, we can get the parameter from a query string with the fields we want to sort the data for. Then we can sort them by those individual fields.\n\nFor instance, we may want to extract the query string from a URL like:\n\nWhere + means ascending and - means descending. So we sort by author’s name in alphabetical order and datepublished from most recent to least recent.\n\nMost communication between client and server should be private since we often send and receive private information. Therefore, using SSL/TLS for security is a must.\n\nA SSL certificate isn't too difficult to load onto a server and the cost is free or very low. There's no reason not to make our REST APIs communicate over secure channels instead of in the open.\n\nPeople shouldn't be able to access more information that they requested. For example, a normal user shouldn't be able to access information of another user. They also shouldn't be able to access data of admins.\n\nTo enforce the principle of least privilege, we need to add role checks either for a single role, or have more granular roles for each user.\n\nIf we choose to group users into a few roles, then the roles should have the permissions that cover all they need and no more. If we have more granular permissions for each feature that users have access to, then we have to make sure that admins can add and remove those features from each user accordingly. Also, we need to add some preset roles that can be applied to a group users so that we don’t have to do that for every user manually.\n\nWe can add caching to return data from the local memory cache instead of querying the database to get the data every time we want to retrieve some data that users request. The good thing about caching is that users can get data faster. However, the data that users get may be outdated. This may also lead to issues when debugging in production environments when something goes wrong as we keep seeing old data.\n\nThere are many kinds of caching solutions like Redis, in-memory caching, and more. We can change the way data is cached as our needs change.\n\nFor instance, Express has the apicache middleware to add caching to our app without much configuration. We can add a simple in-memory cache into our server like so:\n\nThe code above just references the apicache middleware with apicache.middleware and then we have:\n\nto apply the caching to the whole app. We cache the results for five minutes, for example. We can adjust this for our needs.\n\nIf you are using caching, you should also include Cache-Control information in your headers. This will help users effectively use your caching system.\n\nWe should have different versions of API if we're making any changes to them that may break clients. The versioning can be done according to semantic version (for example, 2.0.6 to indicate major version 2 and the sixth patch) like most apps do nowadays.\n\nThis way, we can gradually phase out old endpoints instead of forcing everyone to move to the new API at the same time. The v1 endpoint can stay active for people who don’t want to change, while the v2, with its shiny new features, can serve those who are ready to upgrade. This is especially important if our API is public. We should version them so that we won't break third party apps that use our APIs.\n\nVersioning is usually done with /v1/, /v2/, etc. added at the start of the API path.\n\nFor example, we can do that with Express as follows:\n\nWe just add the version number to the start of the endpoint URL path to version them.\n\nThe most important takeaways for designing high-quality REST APIs is to have consistency by following web standards and conventions. JSON, SSL/TLS, and HTTP status codes are all standard building blocks of the modern web.\n\nPerformance is also an important consideration. We can increase it by not returning too much data at once. Also, we can use caching so that we don't have to query for data all the time.\n\nPaths of endpoints should be consistent, we use nouns only since the HTTP methods indicate the action we want to take. Paths of nested resources should come after the path of the parent resource. They should tell us what we’re getting or manipulating without the need to read extra documentation to understand what it’s doing."
    },
    {
        "link": "https://jsonapi.org/recommendations",
        "document": "This section contains recommendations for JSON:API implementations. These recommendations are intended to establish a level of consistency in areas that are beyond the scope of the base JSON:API specification.\n\nThe specification places certain hard restrictions on how members (i.e., keys) in a JSON:API document may be named. To further standardize member names, which is especially important when mixing profiles authored by different parties, the following rules are also recommended:\n• Member names SHOULD start and end with a character “a-z” (U+0061 to U+007A)\n\nWhen determining an API’s URL structure, it is helpful to consider that all of its resources exist in a single “reference document” in which each resource is addressable at a unique path. Resources are grouped by type at the top level of this document. Individual resources are keyed by ID within these typed collections. Attributes and links within individual resources are uniquely addressable according to the resource object structure described above.\n\nThis concept of a reference document is used to determine appropriate URLs for resources as well as their relationships. It is important to understand that this reference document differs slightly in structure from documents used to transport resources due to different goals and constraints. For instance, collections in the reference document are represented as sets because members must be addressable by ID, while collections are represented as arrays in transport documents because order is significant.\n\nIt is recommended that the URL for a collection of resources be formed from the resource type.\n\nFor example, a collection of resources of type will have the URL:\n\nTreat collections of resources as sets keyed by resource ID. The URL for an individual resource can be formed by appending the resource’s ID to the collection URL.\n\nFor example, a photo with an ID of will have the URL:\n\nAs described in the base specification, there are two URLs that can be exposed for each relationship:\n• the “relationship URL” - a URL for the relationship itself, which is identified with the key in a relationship’s object. This URL allows the client to directly manipulate the relationship. For example, it would allow a client to remove an from a without deleting the resource itself.\n• the “related resource URL” - a URL for the related resource(s), which is identified with the key within a relationship’s object. When fetched, it returns the related resource object(s) as the response’s primary data.\n\nIt is recommended that a relationship URL be formed by appending and the name of the relationship to the resource’s URL.\n\nFor example, a photo’s relationship will have the URL:\n\nAnd a photo’s relationship will have the URL:\n\nIt is recommended that a related resource URL be formed by appending the name of the relationship to the resource’s URL.\n\nFor example, the URL for a photo’s will be:\n\nAnd the URL for a photo’s will be:\n\nBecause these URLs represent resources in relationships, they should not be used as links for the resources themselves. Instead the recommendations for individual resource URLs should still apply when forming links.\n\nThe base specification is agnostic about filtering strategies supported by a server. The query parameter family is reserved to be used as the basis for any filtering strategy.\n\nIt’s recommended that servers that wish to support filtering of a resource collection based upon associations do so by allowing query parameters that combine with the association name.\n\nFor example, the following is a request for all comments associated with a particular post:\n\nMultiple filter values can be combined in a comma-separated list. For example:\n\nFurthermore, multiple filters can be applied to a single request:\n\nThe base specification is agnostic about including links with a resource response. However, it is recommended that the following links be included within response documents:\n• Top-level links like a self-link (for the whole response) as well as relative pagination links (if appropriate).\n• Resource-level links like a self-link for each resource (which differs from the top-level, if the resource is part of a collection).\n• Relationship links for all available relationships of a resource.\n\nFor example, a request for a collection of comments could prompt the following response:\n\nSome clients, like IE8, lack support for HTTP’s method. API servers that wish to support these clients are recommended to treat requests as requests if the client includes the header. This allows clients that lack support to have their update requests honored, simply by adding the header.\n\nAlthough JSON:API does not specify the format of date and time fields, it is recommended that servers align with ISO 8601. This W3C NOTE provides an overview of the recommended formats.\n\nConsider a situation when you need to create a resource and the operation takes a long time to complete.\n\nThe request SHOULD return a status with a link in the header.\n\nTo check the status of the job process, a client can send a request to the location given earlier.\n\nRequests for still-pending jobs SHOULD return a status , as the server is reporting the status successfully. Optionally, the server can return a header to provide guidance to the client as to how long it should wait before checking again. Recommendations to retry sooner than 1 second can be accomplished with .\n\nWhen job process is done, the request SHOULD return a status with a link in header.\n\nProfiles are a mechanism that can be used by the sender of a document to make promises about its content, without adding to or altering the basic semantics of the JSON:API specification. For example, a profile may indicate that all resource objects will have a attribute field and that the members of the object will be formatted using the ISO 8601 date time format.\n\nA profile is an independent specification of those promises. The following example illustrates how the aforementioned profile might be authored:"
    },
    {
        "link": "https://stackoverflow.com/questions/74725942/best-api-response-format-in-json",
        "document": "An open source CRM with more than 18K start on github uses Laravel-default api resource Project Link Code Example link\n\nThis one is super important. If there's one thing you need to remember from this article, this is probably it:\n\nIt's simply bad semantics. Instead, return a meaningful status code that correctly describes the type of error.\n\nStill, you might wonder, \"But I'm sending error details in the response body as you recommended, so what's wrong with that?\"\n\nLet me tell you a story.\n\nI once had to use an API that returned 200 OK for every response and indicated whether the request had succeeded via a status field:\n\nSo even though the status was 200 OK, I could not be absolutely sure it didn't fail to process my request.\n\nThis kind of design is a real no-no because it breaks the trust between the API and their users. You come to fear that the API could be lying to you.\n\nAll of this is extremely un-RESTful. What should you do instead?\n\nMake use of the status code and only use the response body to provide error details."
    },
    {
        "link": "https://jsonapi.org",
        "document": "If you’ve ever argued with your team about the way your JSON responses should be formatted, JSON:API can help you stop the bikeshedding and focus on what matters: your application.\n\nBy following shared conventions, you can increase productivity, take advantage of generalized tooling and best practices. Clients built around JSON:API are able to take advantage of its features around efficiently caching responses, sometimes eliminating network requests entirely.\n\nHere’s an example response from a blog that implements JSON:API:\n\nThe response above contains the first in a collection of “articles”, as well as links to subsequent members in that collection. It also contains resources linked to the article, including its author and comments. Last but not least, links are provided that can be used to fetch or update any of these resources.\n\nJSON:API covers creating and updating resources as well, not just responses.\n\nJSON:API has been properly registered with the IANA. Its media type designation is .\n\nTo get started with JSON:API, check out documentation for the base specification.\n\nThe JSON:API community has created a collection of extensions that APIs can use to provide clients with information or functionality beyond that described in the base JSON:API specification. These extensions are called profiles.\n\nYou can browse existing profiles or create a new one.\n\nA more thorough history is available here.\n\nYou can subscribe to an RSS feed of individual changes here."
    },
    {
        "link": "https://w3schools.com/sql/sql_like.asp",
        "document": "The operator is used in a clause to search for a specified pattern in a column.\n\nThere are two wildcards often used in conjunction with the operator:\n• The percent sign represents zero, one, or multiple characters\n\nYou will learn more about wildcards in the next chapter.\n\nBelow is a selection from the Customers table used in the examples:\n\nIt can be any character or number, but each represents one, and only one, character.\n\nReturn all customers from a city that starts with 'L' followed by one wildcard character, then 'nd' and then two wildcard characters: SELECT * FROM Customers\n\n WHERE city LIKE 'L_nd__'; Try it Yourself »\n\nThe wildcard represents any number of characters, even zero characters.\n\nTo return records that starts with a specific letter or phrase, add the at the end of the letter or phrase.\n\nTo return records that ends with a specific letter or phrase, add the at the beginning of the letter or phrase.\n\nTo return records that contains a specific letter or phrase, add the both before and after the letter or phrase.\n\nAny wildcard, like and , can be used in combination with other wildcards.\n\nReturn all customers that starts with \"a\" and are at least 3 characters in length: SELECT * FROM Customers\n\n WHERE CustomerName LIKE 'a__%'; Try it Yourself »\n\nIf no wildcard is specified, the phrase has to have an exact match to return a result."
    },
    {
        "link": "https://geeksforgeeks.org/sql-like",
        "document": "The SQL LIKE operator is used for performing pattern-based searches in a database. It is used in combination with the WHERE clause to filter records based on specified patterns, making it essential for any database-driven application that requires flexible search functionality.\n\nIn this article, we will explain the SQL LIKE operator, its syntax, uses, and practical examples. It also dives into advanced concepts like case sensitivity and wildcard characters, helping you optimize your queries for better performance and relevance.\n\nWhat is the SQL LIKE Operator?\n\nSQL LIKE operator is used with the WHERE clause to search for a specified pattern in a column. LIKE operator finds and returns the rows that fit in the given pattern.\n\nLIKE operator is case-insensitive by default in most database systems. This means that if you search for “apple” using the LIKE operator, it will return results that include “Apple”, “APPLE”, “aPpLe”, and so on.\n• None column_name: The column to be searched.\n• None pattern: The pattern to search for, which can include wildcard characters.\n\nFor making the LIKE operator case-sensitive, you can use the “BINARY” keyword in MySQL or the “COLLATE” keyword in other database systems.\n\nThis following query will only return products whose name starts with “apple” and is spelled exactly like that, without capital letters.\n\nWildcard Characters with the SQL LIKE Operator\n\nWildcards are used with the LIKE operator to search for specific patterns in strings. Wildcard characters substitute one or more characters in the string. There are four wildcard characters in SQL:\n• % (Percent): Represents zero or more characters.\n\nThe below table shows some examples on how wild card can be written and what do they mean:\n\nIn this tutorial on SQL LIKE Operator, we will use the following table in the examples.\n\nRetrieve SupplierID, Name, and Address from suppliers table, where supplier name starts form k.\n\nExample 3: Match Names Where ‘ango’ Appears in the Second Position\n\nRetrieve SupplierID, Name and Address of supplier whose name contains “ango” in second substring.\n\nExample 4: Using LIKE with AND for Complex Conditions\n\nRetrieve suppliers from Delhi with names starting with “C”:\n\nExample 5: Using NOT LIKE for Exclusion\n\nTo retrieve all suppliers whose name does not contain “Mango”\n\nThe LIKE operator is extremely resourceful in situations such as address filtering wherein we know only a segment or a portion of the entire address (such as locality or city) and would like to retrieve results based on that. The wildcards can be resourcefully exploited to yield even better and more filtered tuples based on the requirement.\n\nThe SQL LIKE operator is a powerful tool for pattern matching, allowing you to perform flexible searches within columns. By combining wildcards and logical operators, you can craft complex queries to find the data you need with precision. Understanding how to optimize the use of LIKE with indexes and case sensitivity will help improve query performance."
    },
    {
        "link": "https://stackoverflow.com/questions/19338321/how-to-make-a-data-manipulation-language-like-sql",
        "document": "How to make a data manipulation language like SQL, and implement the basic functions like\n\nI tried to search online, but I didn't get any proper link, where I can start from. Most of the results go towards making a SQL parser.\n\nSo I wanted to ask\n• What is the basic idea behind making a DML?\n• How should I be manipulating the data?\n• Which language or platform should I be using to implement it?\n\nIf possible, please post any links of past works in this field."
    },
    {
        "link": "https://opentextbc.ca/dbdesign01/chapter/chapter-sql-dml",
        "document": "The SQL data manipulation language (DML) is used to query and modify database data. In this chapter, we will describe how to use the SELECT, INSERT, UPDATE, and DELETE SQL DML command statements, defined below.\n• SELECT – to query data in the database\n• Each clause in a statement should begin on a new line.\n• The beginning of each clause should line up with the beginning of other clauses.\n• If a clause has several parts, they should appear on separate lines and be indented under the start of the clause to show the relationship.\n• Upper case letters are used to represent reserved words.\n• Lower case letters are used to represent user-defined words.\n\nThe SELECT statement, or command, allows the user to extract data from tables, based on specific criteria. It is processed according to the following sequence:\n\nSELECT DISTINCT item(s)\n\n FROM table(s)\n\n WHERE predicate\n\n GROUP BY field(s)\n\n ORDER BY fields\n\nWe can use the SELECT statement to generate an employee phone list from the Employees table as follows:\n\nThis action will display employee’s last name, first name, and phone number from the Employees table, seen in Table 16.1.\n\nIn this next example, we will use a Publishers table (Table 16.2). (You will notice that Canada is mispelled in the Publisher Country field for Example Publishing and ABC Publishing. To correct mispelling, use the UPDATE statement to standardize the country field to Canada – see UPDATE statement later in this chapter.)\n\nIf you add the publisher’s name and city, you would use the SELECT statement followed by the fields name separated by a comma:\n\nThis action will display the publisher’s name and city from the Publishers table.\n\nIf you just want the publisher’s name under the display name city, you would use the SELECT statement with no comma separating pub_name and city:\n\nPerforming this action will display only the pub_name from the Publishers table with a “city” heading. If you do not include the comma, SQL Server assumes you want a new column name for pub_name.\n\nSometimes you might want to focus on a portion of the Publishers table, such as only publishers that are in Vancouver. In this situation, you would use the SELECT statement with the WHERE criterion, i.e., WHERE city = ‘Vancouver’.\n\nThese first two examples illustrate how to limit record selection with the WHERE criterion using BETWEEN. Each of these examples give the same results for store items with between 20 and 50 items in stock.\n\nExample #1 uses the quantity, qty BETWEEN 20 and 50.\n\nSELECT StorID, qty, TitleID\n\n FROM Sales\n\n WHERE qty BETWEEN 20 and 50 (includes the 20 and 50)\n\nExample #2, on the other hand, uses qty >=20 and qty <=50 .\n\nSELECT \n\n FROM Sales\n\n WHERE qty >= 20 and qty <= 50\n\nExample #3 illustrates how to limit record selection with the WHERE criterion using NOT BETWEEN.\n\nSELECT \n\n FROM Sales\n\n WHERE qty NOT BETWEEN 20 and 50\n\nThe next two examples show two different ways to limit record selection with the WHERE criterion using IN, with each yielding the same results.\n\nExample #4 shows how to select records using province= as part of the WHERE statement.\n\nSELECT *\n\n FROM Publishers\n\n WHERE province = ‘BC’ OR province = ‘AB’ OR province = ‘ON’\n\nExample #5 select records using province IN as part of the WHERE statement.\n\nSELECT *\n\n FROM Publishers\n\n WHERE province IN (‘BC’, ‘AB’, ‘ON’)\n\nThe final two examples illustrate how NULL and NOT NULL can be used to select records. For these examples, a Books table (not shown) would be used that contains fields called Title, Quantity, and Price (of book). Each publisher has a Books table that lists all of its books.\n\nSELECT price, title\n\n FROM Books\n\n WHERE price IS NULL\n\nExample #7 uses NOT NULL.\n\nSELECT price, title\n\n FROM Books\n\n WHERE price IS NOT NULL\n\nUsing wildcards in the LIKE clause\n\nThe LIKE keyword selects rows containing fields that match specified portions of character strings. LIKE is used with char, varchar, text, datetime and smalldatetime data. A wildcard allows the user to match fields that contain certain letters. For example, the wildcard province = ‘N%’ would give all provinces that start with the letter ‘N’. Table 16.3 shows four ways to specify wildcards in the SELECT statement in regular express format.\n\nTable 16.3. How to specify wildcards in the SELECT statement.\n\nIn example #1, LIKE ‘Mc%’ searches for all last names that begin with the letters “Mc” (e.g., McBadden).\n\nSELECT LastName\n\n FROM Employees\n\n WHERE LIKE ‘Mc%’\n\nFor example #2: LIKE ‘%inger’ searches for all last names that end with the letters “inger” (e.g., Ringer, Stringer).\n\nSELECT \n\n FROM Employees\n\n WHERE LIKE ‘%inger’\n\nIn, example #3: LIKE ‘%en%’ searches for all last names that have the letters “en” (e.g., Bennett, Green, McBadden).\n\nSELECT \n\n FROM Employees\n\n WHERE LIKE ‘%en%’\n\nYou use the ORDER BY clause to sort the records in the resulting list. Use ASC to sort the results in ascending order and DESC to sort the results in descending order.\n\nFor example, with ASC:\n\nThe GROUP BY clause is used to create one output row per each group and produces summary values for the selected columns, as shown below.\n\nHere is an example using the above statement.\n\nSELECT type AS ‘Type’, MIN(price) AS ‘Minimum Price’\n\n FROM Books\n\n WHERE royalty > 10\n\n GROUP BY type\n\nIf the SELECT statement includes a WHERE criterion where price is not null,\n\nSELECT type, price\n\n FROM Books\n\n WHERE price is not null\n\nthen a statement with the GROUP BY clause would look like this:\n\nSELECT type AS ‘Type’, MIN(price) AS ‘Minimum Price’\n\n FROM Books\n\n WHERE price is not null\n\n GROUP BY type\n\nUsing COUNT with GROUP BY\n\nWe can use COUNT to tally how many items are in a container. However, if we want to count different items into separate groups, such as marbles of varying colours, then we would use the COUNT function with the GROUP BY command.\n\nThe below SELECT statement illustrates how to count groups of data using the COUNT function with the GROUP BY clause.\n\nUsing AVG and SUM with GROUP BY\n\nWe can use the AVG function to give us the average of any group, and SUM to give the total.\n\nExample #1 uses the AVG FUNCTION with the GROUP BY type.\n\nExample #2 uses the SUM function with the GROUP BY type.\n\nExample #3 uses both the AVG and SUM functions with the GROUP BY type in the SELECT statement.\n\nSELECT ‘Total Sales’ = SUM(qty), ‘Average Sales’ = AVG(qty), stor_id\n\n FROM Sales\n\n GROUP BY StorID ORDER BY ‘Total Sales’\n\nThe HAVING clause can be used to restrict rows. It is similar to the WHERE condition except HAVING can include the aggregate function; the WHERE cannot do this.\n\nThe HAVING clause behaves like the WHERE clause, but is applicable to groups. In this example, we use the HAVING clause to exclude the groups with the province ‘BC’.\n\nSELECT au_fname AS ‘Author”s First Name’, province as ‘Province’\n\n FROM Authors\n\n GROUP BY au_fname, province\n\n HAVING province <> ‘BC’\n\nThe INSERT statement adds rows to a table. In addition,\n• INSERT specifies the table or view that data will be inserted into.\n• Column_list lists columns that will be affected by the INSERT.\n• If a column is omitted, each value must be provided.\n• If you are including columns, they can be listed in any order.\n• VALUES specifies the data that you want to insert into the table. VALUES is required.\n• Columns with the IDENTITY property should not be explicitly listed in the column_list or values_clause.\n\nThe syntax for the INSERT statement is:\n\nWhen inserting rows with the INSERT statement, these rules apply:\n• Inserting an empty string (‘ ‘) into a varchar or text column inserts a single space.\n• All char columns are right-padded to the defined length.\n• All trailing spaces are removed from data inserted into varchar columns, except in strings that contain only spaces. These strings are truncated to a single space.\n• If an INSERT statement violates a constraint, default or rule, or if it is the wrong data type, the statement fails and SQL Server displays an error message.\n\nWhen you specify values for only some of the columns in the column_list, one of three things can happen to the columns that have no values:\n• A default value is entered if the column has a DEFAULT constraint, if a default is bound to the column, or if a default is bound to the underlying user-defined data type.\n• NULL is entered if the column allows NULLs and no default value exists for the column.\n• An error message is displayed and the row is rejected if the column is defined as NOT NULL and no default exists.\n\nThis example uses INSERT to add a record to the publisher’s Authors table.\n\nThis following example illustrates how to insert a partial row into the Publishers table with a column list. The country column had a default value of Canada so it does not require that you include it in your values.\n\nTo insert rows into a table with an IDENTITY column, follow the below example. Do not supply the value for the IDENTITY nor the name of the column in the column list.\n\nBy default, data cannot be inserted directly into an IDENTITY column; however, if a row is accidentally deleted, or there are gaps in the IDENTITY column values, you can insert a row and specify the IDENTITY column value.\n\nTo allow an insert with a specific identity value, the IDENTITY_INSERT option can be used as follows.\n\nWe can sometimes create a small temporary table from a large table. For this, we can insert rows with a SELECT statement. When using this command, there is no validation for uniqueness. Consequently, there may be many rows with the same pub_id in the example below.\n\nThis example creates a smaller temporary Publishers table using the CREATE TABLE statement. Then the INSERT with a SELECT statement is used to add records to this temporary Publishers table from the publis table.\n\nIn this example, we’re copying a subset of data.\n\nIn this example, the publishers’ data are copied to the tmpPublishers table and the country column is set to Canada.\n\nThe UPDATE statement changes data in existing rows either by adding new data or modifying existing data.\n\nThis example uses the UPDATE statement to standardize the country field to be Canada for all records in the Publishers table.\n\nThis example increases the royalty amount by 10% for those royalty amounts between 10 and 20.\n\nUPDATE roysched\n\n SET royalty = royalty + (royalty * .10)\n\n WHERE royalty BETWEEN 10 and 20\n\nThe employees from the Employees table who were hired by the publisher in 2010 are given a promotion to the highest job level for their job type. This is what the UPDATE statement would look like.\n\nUPDATE Employees\n\n SET job_lvl =\n\n (SELECT max_lvl FROM jobs\n\n WHERE employee.job_id = jobs.job_id)\n\n WHERE DATEPART(year, employee.hire_date) = 2010\n\nThe DELETE statement removes rows from a record set. DELETE names the table or view that holds the rows that will be deleted and only one table or row may be listed at a time. WHERE is a standard WHERE clause that limits the deletion to select records.\n\nThe DELETE syntax looks like this.\n\nThe rules for the DELETE statement are:\n• If you omit a WHERE clause, all rows in the table are removed (except for indexes, the table, constraints).\n• DELETE cannot be used with a view that has a FROM clause naming more than one table. (Delete can affect only one base table at a time.)\n\nWhat follows are three different DELETE statements that can be used.\n\n3. Deleting rows based on a value in a subquery:\n\nDELETE FROM Sales\n\n WHERE title_id IN\n\n (SELECT title_id FROM Books WHERE type = ‘mod_cook’)\n\nThere are many built-in functions in SQL Server such as:\n• Conversion: transforms one data type to another\n• System: returns a special piece of information from the database\n• Text and image: performs operations on text and image data\n\nBelow you will find detailed descriptions and examples for the first four functions.\n\nAggregate functions perform a calculation on a set of values and return a single, or summary, value. Table 16.4 lists these functions.\n\nBelow are examples of each of the aggregate functions listed in Table 16.4.\n\nSELECT COUNT(PubID) AS ‘Number of Publishers’\n\n FROM Publishers\n\nSELECT COUNT(province) AS ‘Number of Publishers’\n\n FROM Publishers\n\nThe conversion function transforms one data type to another.\n\nIn the example below, a price that contains two 9s is converted into five characters. The syntax for this statement is SELECT ‘The date is ‘ + CONVERT(varchar(12), getdate()).\n\nSELECT CONVERT(int, 10.6496)\n\n SELECT title_id, price\n\n FROM Books\n\n WHERE CONVERT(char(5), price) LIKE ‘%99%’\n\nIn this second example, the conversion function changes data to a data type with a different size.\n\nSELECT title_id, CONVERT(char(4), ytd_sales) as ‘Sales’\n\n FROM Books\n\n WHERE type LIKE ‘%cook’\n\nThe date function produces a date by adding an interval to a specified date. The result is a datetime value equal to the date plus the number of date parts. If the date parameter is a smalldatetime value, the result is also a smalldatetime value.\n\nThe DATEADD function is used to add and increment date values. The syntax for this function is DATEADD(datepart, number, date).\n\nIn this example, the function DATEDIFF(datepart, date1, date2) is used.\n\nThis command returns the number of datepart “boundaries” crossed between two specified dates. The method of counting crossed boundaries makes the result given by DATEDIFF consistent across all data types such as minutes, seconds, and milliseconds.\n\nFor any particular date, we can examine any part of that date from the year to the millisecond.\n\nThe date parts (DATEPART) and abbreviations recognized by SQL Server, and the acceptable values are listed in Table 16.5.\n\nMathematical functions perform operations on numeric data. The following example lists the current price for each book sold by the publisher and what they would be if all prices increased by 10%.\n\nJoining two or more tables is the process of comparing the data in specified columns and using the comparison results to form a new table from the rows that qualify. A join statement:\n• Compares the values in those columns row by row\n• Combines rows with qualifying values into a new row\n\nAlthough the comparison is usually for equality – values that match exactly – other types of joins can also be specified. All the different joins such as inner, left (outer), right (outer), and cross join will be described below.\n\nAn inner join connects two tables on a column with the same data type. Only the rows where the column values match are returned; unmatched rows are discarded.\n\nSELECT jobs.job_id, job_desc\n\n FROM jobs\n\n INNER JOIN Employees ON employee.job_id = jobs.job_id\n\n WHERE jobs.job_id < 7\n\nSELECT authors.au_fname, authors.au_lname, books.royalty, title\n\n FROM authorsINNER JOIN titleauthor ON authors.au_id=titleauthor.au_id\n\n INNER JOIN books ON titleauthor.title_id=books.title_id\n\n GROUP BY authors.au_lname, authors.au_fname, title, title.royalty\n\n ORDER BY authors.au_lname\n\nA left outer join specifies that all left outer rows be returned. All rows from the left table that did not meet the condition specified are included in the results set, and output columns from the other table are set to NULL.\n\nThis first example uses the new syntax for a left outer join.\n\nThis is an example of a left outer join using the old syntax.\n\nA right outer join includes, in its result set, all rows from the right table that did not meet the condition specified. Output columns that correspond to the other table are set to NULL.\n\nBelow is an example using the new syntax for a right outer join.\n\nSELECT titleauthor.title_id, authors.au_lname, authors.au_fname\n\n FROM titleauthor\n\n RIGHT OUTER JOIN authors ON titleauthor.au_id = authors.au_id\n\n ORDERY BY au_lname\n\nThis second example show the old syntax used for a right outer join.\n\nSELECT titleauthor.title_id, authors.au_lname, authors.au_fname\n\n FROM titleauthor, authors\n\n WHERE titleauthor.au_id =* authors.au_id\n\n ORDERY BY au_lname\n\nA full outer join specifies that if a row from either table does not match the selection criteria, the row is included in the result set, and its output columns that correspond to the other table are set to NULL.\n\nHere is an example of a full outer join.\n\nSELECT books.title, publishers.pub_name, publishers.province\n\n FROM Publishers\n\n FULL OUTER JOIN Books ON books.pub_id = publishers.pub_id\n\n WHERE (publishers.province <> “BC” and publishers.province <> “ON”)\n\n ORDER BY books.title_id\n\nA cross join is a product combining two tables. This join returns the same rows as if no WHERE clause were specified. For example:"
    },
    {
        "link": "https://quora.com/Is-there-a-general-purpose-programming-language-with-SQL-like-syntax",
        "document": "Something went wrong. Wait a moment and try again."
    }
]