[
    {
        "link": "https://duckdb.org/docs/stable/extensions/working_with_extensions.html",
        "document": ""
    },
    {
        "link": "https://duckdb.org/docs/stable/extensions/installing_extensions.html",
        "document": "To install core DuckDB extensions, use the command. For example:\n\nThis installs the extension from the default repository ( ).\n\nBy default, DuckDB extensions are installed from a single repository containing extensions built and signed by the core DuckDB team. This ensures the stability and security of the core set of extensions. These extensions live in the default repository, which points to .\n\nBesides the core repository, DuckDB also supports installing extensions from other repositories. For example, the repository contains nightly builds for core extensions that are built for the latest stable release of DuckDB. This allows users to try out new features in extensions before they are officially published.\n\nTo install extensions from the default repository ( ), run:\n\nTo explicitly install an extension from the core repository, run:\n\nTo install an extension from the core nightly repository:\n\nTo install an extension from a custom repository:\n\nAlternatively, the setting can be used to change the default repository used by DuckDB:\n\nDuckDB contains the following predefined repositories:\n\nWhen working with extensions from different repositories, especially mixing and , it is important to know the origins and version of the different extensions. For this reason, DuckDB keeps track of this in the extension installation metadata. For example:\n\nWhen DuckDB installs an extension, it is copied to a local directory to be cached and avoid future network traffic. Any subsequent calls to will use the local version instead of downloading the extension again. To force re-downloading the extension, run:\n\nForce installing can also be used to overwrite an extension with an extension of the same name from another repository,\n\nFor example, first, is installed from the core repository:\n\nThen, to overwrite this installation with the extension from the repository:\n\nTo switch repositories for an extension, use the command. For example, if you have installed from the repository but would like to switch back to using , run:\n\nFor many clients, using SQL to load and install extensions is the preferred method. However, some clients have a dedicated API to install and load extensions. For example, the Python client, has dedicated and methods. For more details on a specific client API, refer to the Client API documentation\n\nBy default, extensions are installed under the user's home directory:\n\nNote that setting the value of the configuration option has no effect on the location of the extensions.\n\nCurrently, DuckDB does not provide a command to uninstall extensions. To uninstall an extension, navigate to the extension's Installation Location and remove its binary file: For example:\n\nThe shared installation location allows extensions to be shared between the client APIs of the same DuckDB version, as long as they share the same or ABI. For example, if an extension is installed with version 1.2.1 of the CLI client on macOS, it is available from the Python, R, etc. client libraries provided that they have access to the user's home directory and use DuckDB version 1.2.1.\n\nDuckDB's extension mechanism has the following limitations:\n• Extensions cannot be reloaded. If you update extensions, restart the DuckDB process to use newer extensions."
    },
    {
        "link": "https://duckdb.org/docs/extensions/working_with_extensions.html",
        "document": ""
    },
    {
        "link": "https://duckdb.org/docs/installation",
        "document": "This page contains installation options for DuckDB. For production use, we recommend the stable release.\n\nBinaries are available for major programming languages and platforms. If there are no pre-packaged binaries available, consider building DuckDB from source."
    },
    {
        "link": "https://duckdb.org/docs/stable/extensions/installing_extensions",
        "document": "To install core DuckDB extensions, use the command. For example:\n\nThis installs the extension from the default repository ( ).\n\nBy default, DuckDB extensions are installed from a single repository containing extensions built and signed by the core DuckDB team. This ensures the stability and security of the core set of extensions. These extensions live in the default repository, which points to .\n\nBesides the core repository, DuckDB also supports installing extensions from other repositories. For example, the repository contains nightly builds for core extensions that are built for the latest stable release of DuckDB. This allows users to try out new features in extensions before they are officially published.\n\nTo install extensions from the default repository ( ), run:\n\nTo explicitly install an extension from the core repository, run:\n\nTo install an extension from the core nightly repository:\n\nTo install an extension from a custom repository:\n\nAlternatively, the setting can be used to change the default repository used by DuckDB:\n\nDuckDB contains the following predefined repositories:\n\nWhen working with extensions from different repositories, especially mixing and , it is important to know the origins and version of the different extensions. For this reason, DuckDB keeps track of this in the extension installation metadata. For example:\n\nWhen DuckDB installs an extension, it is copied to a local directory to be cached and avoid future network traffic. Any subsequent calls to will use the local version instead of downloading the extension again. To force re-downloading the extension, run:\n\nForce installing can also be used to overwrite an extension with an extension of the same name from another repository,\n\nFor example, first, is installed from the core repository:\n\nThen, to overwrite this installation with the extension from the repository:\n\nTo switch repositories for an extension, use the command. For example, if you have installed from the repository but would like to switch back to using , run:\n\nFor many clients, using SQL to load and install extensions is the preferred method. However, some clients have a dedicated API to install and load extensions. For example, the Python client, has dedicated and methods. For more details on a specific client API, refer to the Client API documentation\n\nBy default, extensions are installed under the user's home directory:\n\nNote that setting the value of the configuration option has no effect on the location of the extensions.\n\nCurrently, DuckDB does not provide a command to uninstall extensions. To uninstall an extension, navigate to the extension's Installation Location and remove its binary file: For example:\n\nThe shared installation location allows extensions to be shared between the client APIs of the same DuckDB version, as long as they share the same or ABI. For example, if an extension is installed with version 1.2.1 of the CLI client on macOS, it is available from the Python, R, etc. client libraries provided that they have access to the user's home directory and use DuckDB version 1.2.1.\n\nDuckDB's extension mechanism has the following limitations:\n• Extensions cannot be reloaded. If you update extensions, restart the DuckDB process to use newer extensions."
    },
    {
        "link": "https://blog.min.io/s3-security-access-control",
        "document": "Today we want to cover the MinIO best practices with respect to S3 security and access controls.\n\nThe least privilege access principle is fundamental to the security of your MinIO deployment. You control which applications access which S3 resource on MinIO. Therefore, you should only grant the permissions that are required for a particular task.\n\nAccess to S3 resources is controlled by IAM policies that are attached to the application credentials, in particular the S3 access key. For example, when creating a new access key through the web UI, you can restrict the permissions of the access key by defining an attached IAM policy:\n\nThe attached policy shown above only grants read access to the bucket. So, the application using these access credentials can only read but not write to MinIO - and only from this one specific bucket.\n\nS3 Object Locking enables you to store objects on MinIO using the write-once-read-many (WORM) model. You can either attach a retention period or legal hold to objects. The former prevents any overwrites until the retention period has expired, while the later disallows overwrites until the legal hold has been removed from the object.\n\nAs per the S3 specification, you can enable object locking on a bucket that has versioning turned on.\n\nYou can set a retention configuration on the bucket that controls how long the WORM model applies to newly uploaded objects. You can of course always set specific retention policies or legal holds on individual objects. However, a bucket-level default is recommended to ensure that all objects within the bucket are protected from accidental or malicious overwrites.\n\nVersioning and Object Locking are particularly useful to meet the business-critical threats imposed by ransomware attacks. The WORM model prevents ransomware from deleting or encrypting your data and is a important line of defence against cybercrime groups trying to hold your data hostage.\n\nEncrypting your data before saving it prevents data breaches from low-level access to your MinIO storage infrastructure. MinIO supports S3 server-side encryption with externally managed keys.\n\nYou can connect the leading on-prem and cloud KMS providers, like Hashicorp Vault or AWS, to your MinIO cluster via our KES project. Your encryption keys remain secure inside the KMS while MinIO can leverage the KMS to encrypt data before it gets persisted.\n\nTherefore, you set up KES to connect to your KMS provider of choice. Then you connect MinIO to KES. For example like shown in your Hashicorp Vault guide.\n\nIn your MinIO web UI, you can also set a bucket-level encryption configuration such that any newly uploaded object to the bucket gets encrypted with the specified key at the KMS.\n\nWhen there is no KMS solution in place, MinIO has you covered. We support a simple built-in KMS that you can use to have encryption enabled from the get go while putting the KMS in place.\n\nYou can generate your own random encryption key using basic UNIX commands:\n\nMinIO can be configured to encrypt data with your key by setting just one environment variable.\n\nOnce your KMS solution is available, you can import this key easily into the KMS.\n\nIn addition to encryption at rest, you can enable HTTPS on your MinIO deployment to prevent potential attackers from eavesdropping on your network traffic. MinIO supports only modern TLS 1.2 and 1.3 ciphersuites with zero configuration. All you have to provide is the TLS private key and certificate. Learn more by visiting your TLS and network security documentation.\n\nMinIO has built-in identity and user management. However, when managing a large storage infrastructure for many different application teams, a centralized identity management system like Okta, or LDAP/Active Directory may give a better control over all users and applications.\n\nMinIO has built-in support for LDAP and OpenID Connect identity providers.\n\nWhen configured, MinIO uses the user identities provided by the identity provider for S3 request authentication. While external identity providers add some complexity to the storage system, they provide the advantage of centralized identity management that reduces the overall complexity for large deployments and help reduce the risk of not deactivating stale accounts.\n\nCross-site or cross-region data replication is essential to reduce the risk and duration of operational and security incidents. MinIO makes it easy to set up synchronous and asynchronous replication between sites across data centers.\n\nWhen a site goes down or is under attack, you can fall back to your replicated site(s), either via load balancer rules or DNS configuration. Hence, your storage infrastructure can handle failures of entire sites without impacting client applications.\n\nLike with our openly available source code, we at MinIO handle security incidents transparently. When you run MinIO deployments you should keep an eye on our security advisories. As soon as a new release with a security fix is available we publish a security advisory explaining the impact and mitigations. This helps you identify and patch security risks within your storage infrastructure quickly. When subscribed to our SUBNET support system, you get notifications about new security fixes and you have direct engineering support when assessing the impact of a security issue on your deployments.\n\nIn this post we have outlined the security best practices for MinIO deployments. The two main topics you should focus on the most are access control management via IAM policies and enabling encryption at rest as well as inflight. In addition, you should leverage features for data protection against malicious or accidental deletes - provided by the S3 object locking and cross-site data replication.\n\nBy following these few best practices, you will put your storage infrastructure and all the applications consuming it on a solid and secure foundation.\n\nDownload MinIO and build your cloud-native foundation today. Join our public slack channel to share tips and tricks."
    },
    {
        "link": "https://stackoverflow.com/questions/78980555/proper-way-to-create-a-duckdb-table-from-a-minio-s3-bucket",
        "document": "I ran minio as a docker container using the latest image from https://hub.docker.com/r/minio/minio:\n\nThe container spins up successfully with no error messages in the logs. When I go to I get redirected to the webUI and I'm able to login and view my object storage with no issues.\n\nWhen trying to create a duckdb table with the following query:\n\nwhere my bucketname is . I keep getting a:\n\nhowever when I go to in the browser I just get back xml response\n\nWhat am I missing? Am I setting up the url or the secret credentials wrong with duckdb?"
    },
    {
        "link": "https://duckdb.org/docs/stable/extensions/httpfs/s3api.html",
        "document": "The extension supports reading/writing/globbing files on object storage servers using the S3 API. S3 offers a standard API to read and write to remote files (while regular http servers, predating S3, do not offer a common write API). DuckDB conforms to the S3 API, that is now common among industry storage providers.\n\nThe filesystem is tested with AWS S3, Minio, Google Cloud, and lakeFS. Other services that implement the S3 API (such as Cloudflare R2) should also work, but not all features may be supported.\n\nThe following table shows which parts of the S3 API are required for each feature.\n\nThe preferred way to configure and authenticate to S3 endpoints is to use secrets. Multiple secret providers are available.\n\nThe default provider, (i.e., user-configured), allows access to the S3 bucket by manually providing a key. For example:\n\nNow, to query using the above secret, simply query any prefixed file:\n\nThe provider allows automatically fetching credentials using mechanisms provided by the AWS SDK. For example, to use the AWS SDK default provider:\n\nAgain, to query a file using the above secret, simply query any prefixed file.\n\nDuckDB also allows specifying a specific chain using the keyword. This takes a semicolon-separated list ( ) of providers that will be tried in order. For example:\n\nThe possible values for are the following:\n\nThe provider also allows overriding the automatically fetched config. For example, to automatically load credentials, and then override the region, run:\n\nBelow is a complete list of the supported parameters that can be used for both the and providers:\n\nThe httpfs extension supports Server Side Encryption via the AWS Key Management Service (KMS) on S3 using the option:\n\nWhile Cloudflare R2 uses the regular S3 API, DuckDB has a special Secret type, , to make configuring it a bit simpler:\n\nNote the addition of the which is used to generate to correct endpoint URL for you. Also note that for Secrets can also use both the and providers. Finally, secrets are only available when using URLs starting with , for example:\n\nWhile Google Cloud Storage is accessed by DuckDB using the S3 API, DuckDB has a special Secret type, , to make configuring it a bit simpler:\n\nNote that the above secret, will automatically have the correct Google Cloud Storage endpoint configured. Also note that for Secrets can also use both the and providers. Finally, secrets are only available when using URLs starting with or , for example:\n\nReading files from S3 is now as simple as:\n\nMultiple files are also possible, for example:\n\nFile globbing is implemented using the ListObjectsV2 API call and allows to use filesystem-like glob patterns to match multiple files, for example:\n\nThis query matches all files in the root of the bucket with the Parquet extension.\n\nSeveral features for matching are supported, such as to match any number of any character, for any single character or for a single character in a range of characters:\n\nA useful feature when using globs is the option, which adds a column named that encodes the file that a particular row originated from:\n\ncould for example result in:\n\nDuckDB also offers support for the Hive partitioning scheme, which is available when using HTTP(S) and S3 endpoints.\n\nWriting to S3 uses the multipart upload API. This allows DuckDB to robustly upload files at high speed. Writing to S3 works for both CSV and Parquet:\n\nAn automatic check is performed for existing files/directories, which is currently quite conservative (and on S3 will add a bit of latency). To disable this check and force writing, an flag is added:\n\nThe naming scheme of the written files looks like this:\n\nSome additional configuration options exist for the S3 upload, though the default values should suffice for most use cases."
    },
    {
        "link": "https://duckdb.org/docs/extensions/httpfs/s3api.html",
        "document": ""
    },
    {
        "link": "https://github.com/duckdb/duckdb/issues/4038",
        "document": "To facilitate making system specific settings in duckdb CLI, a feature allowing to make settings using values from system environment variables would be convenient.\n\nFor example in a CLI session when issuing initial PRAGMA statements for setting threads and memory limit or location of temp files, these settings could be made from system environment variables.\n\nAnother similar use case is connection strings, such as when using duckdb CLI with local object storage and setting connection information for an S3 backend.\n\nConsider running with these contents:\n\nIf direct variable substitution is out of scope, could the environment variables for $MINIO_* that are set on the host be set through a or similar function for picking up and using these in the duckdb CLI?\n\nWhen packaging duckdb CLI, for example in a container, a mechanism to allow configuring settings through environment variables would be very useful."
    }
]