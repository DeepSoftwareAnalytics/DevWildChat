[
    {
        "link": "https://huggingface.co/docs/transformers/en/model_doc/gpt2",
        "document": "and get access to the augmented documentation experience\n\nOpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from OpenAI. It’s a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nWrite With Transformer is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five different sizes: small, medium, large, xl and a distilled version of the small checkpoint: distilgpt-2.\n\nThis model was contributed by thomwolf. The original code can be found here.\n• GPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n• GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n• The model can take the past_key_values (for PyTorch) or past (for TF) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the GPT2Model.forward() method, or for TF the past argument of the TFGPT2Model.call() method for more information on its usage.\n• Enabling the scale_attn_by_inverse_layer_idx and reorder_and_upcast_attn flags will apply the training stability improvements from Mistral (for PyTorch only).\n\nThe method can be used to generate text using GPT2 model.\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on kernels.\n\nFirst, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the official documentation. If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered above.\n\nNext, install the latest version of Flash Attention 2:\n\nTo load a model using Flash Attention 2, we can pass the argument to . We’ll also load the model in half-precision (e.g. ), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using checkpoint and the Flash Attention 2 version of the model using a sequence length of 512.\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of . This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the official documentation or the GPU Inference page for more information.\n\nSDPA is used by default for when an implementation is available, but you may also set in to explicitly request SDPA to be used.\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. or ).\n\nOn a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using with gpt2-large, we saw the following speedups during training and inference.\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GPT2. If you’re interested in submitting a resource to be included here, please feel free to open a Pull Request and we’ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n• A blog on how to Finetune a non-English GPT-2 Model with Hugging Face.\n• A blog on How to generate text: using different decoding methods for language generation with Transformers with GPT-2.\n• A blog on Faster Text Generation with TensorFlow and XLA with GPT-2.\n• A blog on How to train a Language Model with Megatron-LM with a GPT-2 model.\n• A notebook on how to finetune GPT2 to generate lyrics in the style of your favorite artist. 🌎\n• A notebook on how to finetune GPT2 to generate tweets in the style of your favorite Twitter user. 🌎\n• Causal language modeling chapter of the 🤗 Hugging Face Course.\n• GPT2LMHeadModel is supported by this causal language modeling example script, text generation example script, and notebook.\n• TFGPT2LMHeadModel is supported by this causal language modeling example script and notebook.\n• FlaxGPT2LMHeadModel is supported by this causal language modeling example script and notebook."
    },
    {
        "link": "https://github.com/openai/gpt-2",
        "document": "Code and models from the paper \"Language Models are Unsupervised Multitask Learners\".\n\nYou can read about GPT-2 and its staged release in our original blog post, 6 month follow-up post, and final post.\n\nWe have also released a dataset for researchers to study their behaviors.\n\n* Note that our original parameter counts were wrong due to an error (in our previous blog posts and paper). Thus you may have seen small referred to as 117M and medium referred to as 345M.\n\nThis repository is meant to be a starting point for researchers and engineers to experiment with GPT-2.\n\nFor basic information, see our model card.\n• GPT-2 models' robustness and worst case behaviors are not well-understood. As with any machine-learned model, carefully evaluate GPT-2 for your use case, especially if used without fine-tuning or in safety-critical applications where reliability is important.\n• The dataset our GPT-2 models were trained on contains many texts with biases and factual inaccuracies, and thus GPT-2 models are likely to be biased and inaccurate as well.\n• To avoid having samples mistaken as human-written, we recommend clearly labeling samples as synthetic before wide dissemination. Our models are often incoherent or inaccurate in subtle ways, which takes more than a quick read for a human to notice.\n\nPlease let us know if you’re doing interesting research with or working on applications of GPT-2! We’re especially interested in hearing from and potentially working with those who are studying\n• Potential malicious use cases and defenses against them (e.g. the detectability of synthetic text)\n• The extent of problematic content (e.g. bias) being baked into the models and effective mitigations\n\nPlease use the following bibtex entry:\n\nWe may release code for evaluating the models on various benchmarks.\n\nWe are still considering release of the larger models."
    },
    {
        "link": "https://huggingface.co/docs/transformers/main/en/model_doc/gpt2",
        "document": "and get access to the augmented documentation experience\n\nOpenAI GPT-2 model was proposed in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei and Ilya Sutskever from OpenAI. It’s a causal (unidirectional) transformer pretrained using language modeling on a very large corpus of ~40 GB of text data.\n\nThe abstract from the paper is the following:\n\nGPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nWrite With Transformer is a webapp created and hosted by Hugging Face showcasing the generative capabilities of several models. GPT-2 is one of them and is available in five different sizes: small, medium, large, xl and a distilled version of the small checkpoint: distilgpt-2.\n\nThis model was contributed by thomwolf. The original code can be found here.\n• GPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n• GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n• The model can take the past_key_values (for PyTorch) or past (for TF) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the GPT2Model.forward() method, or for TF the past argument of the TFGPT2Model.call() method for more information on its usage.\n• Enabling the scale_attn_by_inverse_layer_idx and reorder_and_upcast_attn flags will apply the training stability improvements from Mistral (for PyTorch only).\n\nThe method can be used to generate text using GPT2 model.\n\nFlash Attention 2 is a faster, optimized version of the attention scores computation which relies on kernels.\n\nFirst, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the official documentation. If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered above.\n\nNext, install the latest version of Flash Attention 2:\n\nTo load a model using Flash Attention 2, we can pass the argument to . We’ll also load the model in half-precision (e.g. ), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:\n\nBelow is an expected speedup diagram that compares pure inference time between the native implementation in transformers using checkpoint and the Flash Attention 2 version of the model using a sequence length of 512.\n\nPyTorch includes a native scaled dot-product attention (SDPA) operator as part of . This function encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the official documentation or the GPU Inference page for more information.\n\nSDPA is used by default for when an implementation is available, but you may also set in to explicitly request SDPA to be used.\n\nFor the best speedups, we recommend loading the model in half-precision (e.g. or ).\n\nOn a local benchmark (rtx3080ti-16GB, PyTorch 2.2.1, OS Ubuntu 22.04) using with gpt2-large, we saw the following speedups during training and inference.\n\nA list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GPT2. If you’re interested in submitting a resource to be included here, please feel free to open a Pull Request and we’ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n• A blog on how to Finetune a non-English GPT-2 Model with Hugging Face.\n• A blog on How to generate text: using different decoding methods for language generation with Transformers with GPT-2.\n• A blog on Faster Text Generation with TensorFlow and XLA with GPT-2.\n• A blog on How to train a Language Model with Megatron-LM with a GPT-2 model.\n• A notebook on how to finetune GPT2 to generate lyrics in the style of your favorite artist. 🌎\n• A notebook on how to finetune GPT2 to generate tweets in the style of your favorite Twitter user. 🌎\n• Causal language modeling chapter of the 🤗 Hugging Face Course.\n• GPT2LMHeadModel is supported by this causal language modeling example script, text generation example script, and notebook.\n• TFGPT2LMHeadModel is supported by this causal language modeling example script and notebook.\n• FlaxGPT2LMHeadModel is supported by this causal language modeling example script and notebook."
    },
    {
        "link": "https://github.com/Jayveersinh-Raj/code_generation_gpt2",
        "document": "This Python script trains a GPT-2 language model for code generation. It uses the Hugging Face's Transformers library for the GPT-2 model and tokenizer, and the Datasets library for handling the dataset.\n• It starts by training a ByteLevelBPETokenizer on the provided text data file and saves the tokenizer model to disk. CUDA device order and visible devices are set according to your environment configuration.\n• The trained tokenizer is then assigned to the GPT2Tokenizer and special tokens are added.\n• A GPT-2 model is initialized with the GPT2Config class, using the vocab size and special tokens from the tokenizer.\n• The dataset is loaded from the provided paths and transformed using the tokenizer.\n• The data collator is set to DataCollatorForLanguageModeling from the Transformers library with masked language modeling enabled.\n• Training arguments are set using the TrainingArguments class from Transformers, including output directory, number of epochs, batch size, save steps, etc.\n• Finally, a Trainer is initialized with the model, training arguments, data collator, and dataset, and is ready to be trained.\n\nThis script uses PyTorch through the Transformers library. You need to have a compatible CUDA version installed if you wish to train the model on a GPU.\n\nThe signifies new line. On given import the model suggested the most viable imports it learned during training.\n\nIt is used as a proof of concept, and has been trained by checkpoints for multiple times, on multiple GPUs, and has been checked time to time before moving it on large scale. It is advised to do the same. The fine tuning on gpt2 itself is done to prototype with smaller models. But, as it can be seen that the results are very good, and improvement has been shown over time by the model during training.\n\nThe dataset is provided, and scarpped by Advanced Engineering School (AES) of Innopolis University with whom the project is assosiated. Hence, data, and model/checkpoints are not open source. One can still scrap the data from github to prototype for learning purposes. The file is provided as an example of how to do so."
    },
    {
        "link": "https://flowygo.com/en/blog/gpt-2-automatic-text-generation-with-python",
        "document": "Artificial intelligence has made huge strides in recent years. With it, large amounts of data can be analyzed in order to generate models capable of performing even very complex tasks. A branch of artificial intelligence that fascinates a lot is the ability to generate text from a few words of input: Natural Language Generation. In this article we will see how it is possible to generate articles on Elastisearch whose features we have already discussed in the articles ELK Stack: what it is and what it is used for, What is Kibana used for?, Kibana: let’s explore data, and Python starting from a simple question and a few lines of Python code. We will use, in particular, the GPT-2 Large Pre-trained model and HuggingFace Transformers. However, let’s start by giving two broad facts about the technologies we will be using. Announced in 2019 by OpenAI, GPT-2 is the successor to GPT, and was theorized in the article Language Models are Unsupervised Multitask Learners. This language model is composed of transformers with 1.5 billion parameters and trained on a dataset of 8 million web pages. The training goal of GPT-2 is very simple: predict the next word by knowing all previous words within a text. The heterogeneity of the training dataset implies that the generated model can be adapted to different application domains. Compared to its predecessor, GPT-2 is a direct scale-up with more than 10 times the parameters and trained on a 10 times larger dataset. However, several models of GPT2 have been released as shown in the figure. These differ by size: small (124M parameters), medium (355M parameters), large (774M parameters), and extra large (1.5BM parameters, i.e., the full implementation).\n\nIn May 2020, OpenAI announced GPT-3. Unlike its predecessors, this model can only be used through paid APIs which makes it less usable as a starting point for creating new models. If you want more information you can read an interesting article directly from the OpenAI website. HuggingFace Transformers (formerly known as PyTorch-transformers and PyTorch-pretrained-bert) provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for natural language understanding (NLU) and natural language generation (NLG) with over 32 pre-trained models in more than 100 languages and deep interoperability between TensorFlow 2.0 and PyTorch. Basically Hugging Face Transformers is a python package that has some pre-defined or pre-trained functions, pipelines and templates that we can use for our natural language processing tasks. GPT-2 Tokenizer and templates are also included in HuggingFace Transformers. For more information about the available Transformers and their use we refer you to the official documentation. For this introductory tutorial to text generation with GPT we will use a Jupyter Notebook. As we have seen in the article Jupyter Notebook: user’s guide, it is possible to install it simply through Anaconda. To take advantage of the HuggingFace transformers it is necessary to install, in addition to them, Pytorch. To do this, simply go to the Anaconda site and search for the packages that interest you. For each package is indicated the architecture for which it is released. For example, if you have a Mac with an M1 chip you’ll need to look at all packages compatible with the osx-arm64 platform.\n\nSince we are using the large model, it will take a while for it to download. Make sure you have more than 3GB free before running this command. Otherwise you can use the smaller versions of the GTP-2 templates. For text generation, we need to provide some initial text to our model. In order for the input text to be correctly recognized by the model, we need to preprocess it. We then define the text to start with as shown below.\n\nBefore showing you the result, let’s analyze in detail the various parameters used to generate the text.\n• max_length: Maximum number of words in the generated text.\n• num_beams: Beam search reduces the risk of missing high probability hidden word sequences by keeping the most likely number of hypotheses at each time step and ultimately choosing the hypothesis that has the highest overall probability. Beam search will always find an output sequence with a higher probability than greedy search, even if it is not guaranteed to find the most likely output.\n• no_repeat_ngram_size: While the output is arguably more fluent, the output still includes repetitions of the same word sequences. A simple workaround is to introduce n-gram penalties (i.e., sequences of n words) as introduced by Paulus et al. (2017) and Klein et al. (2017). Setting the penalty to 0 ensures that no n-gram appears twice.\n• early_stopping: Set it to the value True, generation will end when all beam hypotheses have reached the EOS token. More details about the arguments of the generation function can be found in the official documentation. The generate function will return the ids of the tokens in our new text. By simply decoding the result we can print our article. The text generation time varies depending on parameters, including the length of the text to be generated, and the power of your machine.\n\nWhat is Elasticsearch? Elasticsearch is a full-text search engine that can be used to search and index large amounts of data. It is used by companies like Google, Microsoft, Yahoo, and many others. ElasticSearch is also used in many open source projects, such as Apache Hadoop, Apache Spark, HBase, MongoDB, Cassandra, Redis, Memcached, etc. In this post, we will be looking at how to install and set up a simple elasticsearch cluster on Ubuntu 16.04 LTS (Xenial Xerus) using Docker. We will also be using the Docker Compose tool to automate the installation and configuration of the cluster. This post assumes that you have a basic understanding of Docker and how it works. If you are not familiar with Docker, you can read more about it here: https://docs.docker.com/en/latest/userguide/getting-started/installation-and-configuration-of-docker-compose.html. For the purposes of this tutorial, let's assume you already have Docker installed and running on your system. You can find more information about Docker on the official Docker website: http://docker.io/docs/tutorials/how-to-build-a-containerized-application-on-ubuntu-16-04-lts-with-dockery-1.2.0-beta-2-amd64-linux-x86_64. The following Dockerfile will create a Docker container that will act as the master node of our cluster and will serve as a data store for the data that we are going to index. Note that this is not a production-ready cluster, but it should give you a good idea of how Docker works and what it can do for you. #!/bin/bash export DOCKER_OPTS=\"-v\" docker run -d -p 8080:8080 \\ -v /var/lib/docker/data:/data \\ --name my-elastic-search-cluster \\ my_docker_data_dir /data/my_search_file.txt -it --restart=always --volume-driver=dynamodb \\ /usr/share/nginx/html/index.php:8000:9000 \\ nginx -t http.server:5000 -c \"server { listen 80; listen [::]:80; server_name localhost; location / { try_files $uri @api; fastcgi_split_path_info ^(.+\\.php)(/.+)$; } location ~ \\.(jpg|jpeg|gif|png|ico|css|js|swf)$ { add_header Cache-Control \"public, max-age=0, must-revalidate\" ; } }\" /etc/init.d/nginx start # Create a container for our index file docker create -f /path/to/your/search.yml.example /dev/null # Start the container docker exec -ti -e \"rm -rf /tmp/*\" -i /home/ubuntu/dynamic-index-file-name-server.sh /root/node_modules/mysql-5.6.1-0ubuntu1_all.deb # Run the indexing service docker start --net=host --memory-size=512M --volumes-from=my-data-dir --start-stop-daemon --log-level=info --user=root --group=ubuntu --system-type=linux --vhosts=localhost.localdomain --tls-verify=yes --ssl-cert-path=/etc/.ssl/certs.pem docker stop --rm-if-not-found --recursive-service docker rm --ignore-errors --remove-empty-files --force-removal --no-startup-id docker restart --recheck-interval=1s --privileged --service=mysqldd # Check that everything is working docker ps docker inspect --format '{{.}}' --color=auto | grep -E 'index' | awk -F: '{print $1}' > /proc/sys/kernel/random/entropy_avail docker logs --tail -n 1 --pretty=format: \"%d\" | sort | uniq | head -1 | xargs -I {} \\ | sed's/.*\\(.*\\)$/\\1/g' \\; docker status # List all the containers that are running docker info # This will show you the name of each container and the number of processes # in each of them. Each process is represented by a number in the range 1-65535 # (e.g. 1 is"
    },
    {
        "link": "https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras",
        "document": "Recurrent neural networks can also be used as generative models.\n\nThis means that in addition to being used for predictive models (making predictions), they can learn the sequences of a problem and then generate entirely new plausible sequences for the problem domain.\n\nGenerative models like this are useful not only to study how well a model has learned a problem but also to learn more about the problem domain itself.\n\nIn this post, you will discover how to create a generative model for text, character-by-character using LSTM recurrent neural networks in Python with Keras.\n\nAfter reading this post, you will know:\n• Where to download a free corpus of text that you can use to train text generative models\n• How to frame the problem of text sequences to a recurrent neural network generative model\n• How to develop an LSTM to generate plausible text sequences for a given problem\n\nKick-start your project with my new book Deep Learning for Natural Language Processing, including step-by-step tutorials and the Python source code files for all examples.\n\nNote: LSTM recurrent neural networks can be slow to train, and it is highly recommended that you train them on GPU hardware. You can access GPU hardware in the cloud very cheaply using Amazon Web Services. See the tutorial here.\n• Update Oct/2016: Fixed a few minor comment typos in the code\n\nMany of the classical texts are no longer protected under copyright.\n\nThis means you can download all the text for these books for free and use them in experiments, like creating generative models. Perhaps the best place to get access to free books that are no longer protected by copyright is Project Gutenberg.\n\nIn this tutorial, you will use a favorite book from childhood as the dataset: Alice’s Adventures in Wonderland by Lewis Carroll.\n\nYou will learn the dependencies between characters and the conditional probabilities of characters in sequences so that you can, in turn, generate wholly new and original sequences of characters.\n\nThis is a lot of fun, and repeating these experiments with other books from Project Gutenberg is recommended. Here is a list of the most popular books on the site.\n\nThese experiments are not limited to text; you can also experiment with other ASCII data, such as computer source code, marked-up documents in LaTeX, HTML or Markdown, and more.\n\nYou can download the complete text in ASCII format (Plain Text UTF-8) for this book for free and place it in your working directory with the filename wonderland.txt.\n\nNow, you need to prepare the dataset ready for modeling.\n\nProject Gutenberg adds a standard header and footer to each book, which is not part of the original text. Open the file in a text editor and delete the header and footer.\n\nThe header is obvious and ends with the text:\n\nThe footer is all the text after the line of text that says:\n\nYou should be left with a text file that has about 3,330 lines of text.\n\nIn this section, you will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section, you will use this model to generate new sequences of characters.\n\nLet’s start by importing the classes and functions you will use to train your model.\n\nNext, you need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary the network must learn.\n\nNow that the book is loaded, you must prepare the data for modeling by the neural network. You cannot model the characters directly; instead, you must convert the characters to integers.\n\nYou can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer.\n\nFor example, the list of unique sorted lowercase characters in the book is as follows:\n\nYou can see that there may be some characters that we could remove to further clean up the dataset to reduce the vocabulary, which may improve the modeling process.\n\nNow that the book has been loaded and the mapping prepared, you can summarize the dataset.\n\nRunning the code to this point produces the following output.\n\nYou can see the book has just under 150,000 characters, and when converted to lowercase, there are only 47 distinct characters in the vocabulary for the network to learn—much more than the 26 in the alphabet.\n\nYou now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n\nIn this tutorial, you will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. You could just as easily split the data by sentences, padding the shorter sequences and truncating the longer ones.\n\nEach training pattern of the network comprises 100 time steps of one character (X) followed by one character output (y). When creating these sequences, you slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters, of course).\n\nFor example, if the sequence length is 5 (for simplicity), then the first two training patterns would be as follows:\n\nAs you split the book into these sequences, you convert the characters to integers using the lookup table you prepared earlier.\n\nRunning the code to this point shows that when you split up the dataset into training data for the network to learn that you have just under 150,000 training patterns. This makes sense as, excluding the first 100 characters, you have one training pattern to predict each of the remaining characters.\n\nNow that you have prepared your training data, you need to transform it to be suitable for use with Keras.\n\nFirst, you must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n\nNext, you need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network using the sigmoid activation function by default.\n\nFinally, you need to convert the output patterns (single characters converted to integers) into a one-hot encoding. This is so that you can configure the network to predict the probability of each of the 47 different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector with a length of 47, full of zeros, except with a 1 in the column for the letter (integer) that the pattern represents.\n\nFor example, when “n” (integer value 31) is one-hot encoded, it looks as follows:\n\nYou can implement these steps as below:\n\nYou can now define your LSTM model. Here, you define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the 47 characters between 0 and 1.\n\nThe problem is really a single character classification problem with 47 classes and, as such, is defined as optimizing the log loss (cross entropy) using the ADAM optimization algorithm for speed.\n\nThere is no test dataset. You are modeling the entire training dataset to learn the probability of each character in a sequence.\n\nYou are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead, you are interested in a generalization of the dataset that minimizes the chosen loss function. You are seeking a balance between generalization and overfitting but short of memorization.\n\nThe network is slow to train (about 300 seconds per epoch on an Nvidia K520 GPU). Because of the slowness and because of the optimization requirements, use model checkpointing to record all the network weights to file each time an improvement in loss is observed at the end of the epoch. You will use the best set of weights (lowest loss) to instantiate your generative model in the next section.\n\nYou can now fit your model to the data. Here, you use a modest number of 20 epochs and a large batch size of 128 patterns.\n\nThe full code listing is provided below for completeness.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nAfter running the example, you should have a number of weight checkpoint files in the local directory.\n\nYou can delete them all except the one with the smallest loss value. For example, when this example was run, you can see below the checkpoint with the smallest loss that was achieved.\n\nThe network loss decreased almost every epoch, so the network could likely benefit from training for many more epochs.\n\nIn the next section, you will look at using this model to generate new text sequences.\n\nGenerating text using the trained LSTM network is relatively straightforward.\n\nFirst, you will load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file, and the network does not need to be trained.\n\nAlso, when preparing the mapping of unique characters to integers, you must also create a reverse mapping that you can use to convert the integers back to characters so that you can understand the predictions.\n\nFinally, you need to actually make predictions.\n\nThe simplest way to use the Keras LSTM model to make predictions is to first start with a seed sequence as input, generate the next character, then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as you want to predict new characters (e.g., a sequence of 1,000 characters in length).\n\nYou can pick a random input pattern as your seed sequence, then print generated characters as you generate them.\n\nThe full code example for generating text using the loaded LSTM model is listed below for completeness.\n\nRunning this example first outputs the selected random seed, then each character as it is generated.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nFor example, below are the results from one run of this text generator. The random seed was:\n\nThe generated text with the random seed (cleaned up for presentation) was:\n\nbe no mistake about it: it was neither more nor less than a pig, and she felt that it would be quit e aelin that she was a little want oe toiet ano a grtpersent to the tas a little war th tee the tase oa teettee the had been tinhgtt a little toiee at the cadl in a long tuiee aedun thet sheer was a little tare gereen to be a gentle of the tabdit soenee the gad ouw ie the tay a tirt of toiet at the was a little anonersen, and thiu had been woite io a lott of tueh a tiie and taede bot her aeain she cere thth the bene tith the tere bane to tee toaete to tee the harter was a little tire the same oare cade an anl ano the garee and the was so seat the was a little gareen and the sabdit, and the white rabbit wese tilel an the caoe and the sabbit se teeteer, and the white rabbit wese tilel an the cade in a lonk tfne the sabdi ano aroing to tea the was sf teet whitg the was a little tane oo thete the sabeit she was a little tartig to the tar tf tee the tame of the cagd, and the white rabbit was a little toiee to be anle tite thete ofs and the tabdit was the wiite rabbit, and be no mistake about it: it was neither more nor less than a pig, and she felt that it would be quit e aelin that she was a little want oe toiet ano a grtpersent to the tas a little war th tee the tase oa teettee the had been tinhgtt a little toiee at the cadl in a long tuiee aedun thet sheer was a little tare gereen to be a gentle of the tabdit soenee the gad ouw ie the tay a tirt of toiet at the was a little anonersen, and thiu had been woite io a lott of tueh a tiie and taede bot her aeain she cere thth the bene tith the tere bane to tee toaete to tee the harter was a little tire the same oare cade an anl ano the garee and the was so seat the was a little gareen and the sabdit, and the white rabbit wese tilel an the caoe and the sabbit se teeteer, and the white rabbit wese tilel an the cade in a lonk tfne the sabdi ano aroing to tea the was sf teet whitg the was a little tane oo thete the sabeit she was a little tartig to the tar tf tee the tame of the cagd, and the white rabbit was a little toiee to be anle tite thete ofs and the tabdit was the wiite rabbit, and\n\nLet’s note some observations about the generated text.\n• It generally conforms to the line format observed in the original text of fewer than 80 characters before a new line.\n• The characters are separated into word-like groups, and most groups are actual English words (e.g., “the,” “little,” and “was”), but many are not (e.g., “lott,” “tiie,” and “taede”).\n• Some of the words in sequence make sense(e.g., “and the white rabbit“), but many do not (e.g., “wese tilel“).\n\nThe fact that this character-based model of the book produces output like this is very impressive. It gives you a sense of the learning capabilities of LSTM networks.\n\nHowever, the results are not perfect.\n\nIn the next section, you will look at improving the quality of results by developing a much larger LSTM network.\n\nYou got results, but not excellent results in the previous section. Now, you can try to improve the quality of the generated text by creating a much larger network.\n\nYou will keep the number of memory units the same at 256 but add a second layer.\n\nYou will also change the filename of the checkpointed weights so that you can tell the difference between weights for this network and the previous (by appending the word “bigger” in the filename).\n\nFinally, you will increase the number of training epochs from 20 to 50 and decrease the batch size from 128 to 64 to give the network more of an opportunity to be updated and learn.\n\nThe full code listing is presented below for completeness.\n\nRunning this example takes some time, at least 700 seconds per epoch.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nAfter running this example, you may achieve a loss of about 1.2. For example, the best result achieved from running this model was stored in a checkpoint file with the name:\n\nThis achieved a loss of 1.2219 at epoch 47.\n\nAs in the previous section, you can use this best model from the run to generate text.\n\nThe only change you need to make to the text generation script from the previous section is in the specification of the network topology and from which file to seed the network weights.\n\nThe full code listing is provided below for completeness.\n\nOne example of running this text generation script produces the output below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nThe generated text with the seed (cleaned up for presentation) was :\n\nherself lying on the bank, with her head in the lap of her sister, who was gently brushing away so siee, and she sabbit said to herself and the sabbit said to herself and the sood way of the was a little that she was a little lad good to the garden, and the sood of the mock turtle said to herself, 'it was a little that the mock turtle said to see it said to sea it said to sea it say it the marge hard sat hn a little that she was so sereated to herself, and she sabbit said to herself, 'it was a little little shated of the sooe of the coomouse it was a little lad good to the little gooder head. and said to herself, 'it was a little little shated of the mouse of the good of the courte, and it was a little little shated in a little that the was a little little shated of the thmee said to see it was a little book of the was a little that she was so sereated to hare a little the began sitee of the was of the was a little that she was so seally and the sabbit was a little lad good to the little gooder head of the gad seared to see it was a little lad good to the little good herself lying on the bank, with her head in the lap of her sister, who was gently brushing away so siee, and she sabbit said to herself and the sabbit said to herself and the sood way of the was a little that she was a little lad good to the garden, and the sood of the mock turtle said to herself, 'it was a little that the mock turtle said to see it said to sea it said to sea it say it the marge hard sat hn a little that she was so sereated to herself, and she sabbit said to herself, 'it was a little little shated of the sooe of the coomouse it was a little lad good to the little gooder head. and said to herself, 'it was a little little shated of the mouse of the good of the courte, and it was a little little shated in a little that the was a little little shated of the thmee said to see it was a little book of the was a little that she was so sereated to hare a little the began sitee of the was of the was a little that she was so seally and the sabbit was a little lad good to the little gooder head of the gad seared to see it was a little lad good to the little good\n\nYou can see that there are generally fewer spelling mistakes, and the text looks more realistic but is still quite nonsensical.\n\nFor example, the same phrases get repeated again and again, like “said to herself” and “little.” Quotes are opened but not closed.\n\nThese are better results, but there is still a lot of room for improvement.\n\nBelow are ten ideas that may further improve the model that you could experiment with are:\n• Predict fewer than 1,000 characters as output for a given seed\n• Remove all punctuation from the source text and, therefore, from the models’ vocabulary\n• Try a one-hot encoding for the input sequences\n• Train the model on padded sentences rather than random sequences of characters\n• Increase the number of training epochs to 100 or many hundreds\n• Add dropout to the visible input layer and consider tuning the dropout percentage\n• Tune the batch size; try a batch size of 1 as a (very slow) baseline and larger sizes from there\n• Add more memory units to the layers and/or more layers\n• Experiment with scale factors (temperature) when interpreting the prediction probabilities\n• Change the LSTM layers to be “stateful” to maintain state across batches\n\nDid you try any of these extensions? Share your results in the comments.\n\nThis character text model is a popular way of generating text using recurrent neural networks.\n\nBelow are some more resources and tutorials on the topic if you are interested in going deeper. Perhaps the most popular is the tutorial by Andrej Karpathy titled “The Unreasonable Effectiveness of Recurrent Neural Networks.”\n• Keras code example of LSTM for text generation\n• Lasagne code example of LSTM for text generation\n• MXNet tutorial for using an LSTM for text generation\n\nIn this post, you discovered how you can develop an LSTM recurrent neural network for text generation in Python with the Keras deep learning library.\n\nAfter reading this post, you know:\n• Where to download the ASCII text for classical books for free that you can use for training\n• How to train an LSTM network on text sequences and how to use the trained network to generate new sequences\n• How to develop stacked LSTM networks and lift the performance of the model\n\nDo you have any questions about text generation with LSTM networks or this post? Ask your questions in the comments below, and I will do my best to answer them."
    },
    {
        "link": "https://realpython.com/python-ai-neural-network",
        "document": "If you’re just starting out in the artificial intelligence (AI) world, then Python is a great language to learn since most of the tools are built using it. Deep learning is a technique used to make predictions using data, and it heavily relies on neural networks. Today, you’ll learn how to build a neural network from scratch.\n\nIn a production setting, you would use a deep learning framework like TensorFlow or PyTorch instead of building your own neural network. That said, having some knowledge of how neural networks work is helpful because you can use it to better architect your deep learning models.\n• How both machine learning and deep learning play a role in AI\n• How to build a neural network from scratch using Python\n\nIn basic terms, the goal of using AI is to make computers think as humans do. This may seem like something new, but the field was born in the 1950s. Imagine that you need to write a Python program that uses AI to solve a sudoku problem. A way to accomplish that is to write conditional statements and check the constraints to see if you can place a number in each position. Well, this Python script is already an application of AI because you programmed a computer to solve a problem! Machine learning (ML) and deep learning (DL) are also approaches to solving problems. The difference between these techniques and a Python script is that ML and DL use training data instead of hard-coded rules, but all of them can be used to solve problems using AI. In the next sections, you’ll learn more about what differentiates these two techniques. Machine learning is a technique in which you train the system to solve a problem instead of explicitly programming the rules. Getting back to the sudoku example in the previous section, to solve the problem using machine learning, you would gather data from solved sudoku games and train a statistical model. Statistical models are mathematically formalized ways to approximate the behavior of a phenomenon. A common machine learning task is supervised learning, in which you have a dataset with inputs and known outputs. The task is to use this dataset to train a model that predicts the correct outputs based on the inputs. The image below presents the workflow to train a model using supervised learning: The combination of the training data with the machine learning algorithm creates the model. Then, with this model, you can make predictions for new data. Note: scikit-learn is a popular Python machine learning library that provides many supervised and unsupervised learning algorithms. To learn more about it, check out Split Your Dataset With scikit-learn’s train_test_split(). The goal of supervised learning tasks is to make predictions for new, unseen data. To do that, you assume that this unseen data follows a probability distribution similar to the distribution of the training dataset. If in the future this distribution changes, then you need to train your model again using the new training dataset. Prediction problems become harder when you use different kinds of data as inputs. The sudoku problem is relatively straightforward because you’re dealing directly with numbers. What if you want to train a model to predict the sentiment in a sentence? Or what if you have an image, and you want to know whether it depicts a cat? Another name for input data is feature, and feature engineering is the process of extracting features from raw data. When dealing with different kinds of data, you need to figure out ways to represent this data in order to extract meaningful information from it. An example of a feature engineering technique is lemmatization, in which you remove the inflection from words in a sentence. For example, inflected forms of the verb “watch,” like “watches,” “watching,” and “watched,” would be reduced to their lemma, or base form: “watch.” If you’re using arrays to store each word of a corpus, then by applying lemmatization, you end up with a less-sparse matrix. This can increase the performance of some machine learning algorithms. The following image presents the process of lemmatization and representation using a bag-of-words model: First, the inflected form of every word is reduced to its lemma. Then, the number of occurrences of that word is computed. The result is an array containing the number of occurrences of every word in the text. Deep learning is a technique in which you let the neural network figure out by itself which features are important instead of applying feature engineering techniques. This means that, with deep learning, you can bypass the feature engineering process. Not having to deal with feature engineering is good because the process gets harder as the datasets become more complex. For example, how would you extract the data to predict the mood of a person given a picture of her face? With neural networks, you don’t need to worry about it because the networks can learn the features by themselves. In the next sections, you’ll dive deep into neural networks to better understand how they work.\n\nA neural network is a system that learns how to make predictions by following these steps:\n• Comparing the prediction to the desired output\n• Adjusting its internal state to predict correctly the next time Vectors, layers, and linear regression are some of the building blocks of neural networks. The data is stored as vectors, and with Python you store these vectors in arrays. Each layer transforms the data that comes from the previous layer. You can think of each layer as a feature engineering step, because each layer extracts some representation of the data that came previously. One cool thing about neural network layers is that the same computations can extract information from any kind of data. This means that it doesn’t matter if you’re using image data or text data. The process to extract meaningful information and train the deep learning model is the same for both scenarios. In the image below, you can see an example of a network architecture with two layers: Each layer transforms the data that came from the previous layer by applying some mathematical operations. Training a neural network is similar to the process of trial and error. Imagine you’re playing darts for the first time. In your first throw, you try to hit the central point of the dartboard. Usually, the first shot is just to get a sense of how the height and speed of your hand affect the result. If you see the dart is higher than the central point, then you adjust your hand to throw it a little lower, and so on. These are the steps for trying to hit the center of a dartboard: Steps to hit the center of a dartboard Notice that you keep assessing the error by observing where the dart landed (step 2). You go on until you finally hit the center of the dartboard. With neural networks, the process is very similar: you start with some random weights and bias vectors, make a prediction, compare it to the desired output, and adjust the vectors to predict more accurately the next time. The process continues until the difference between the prediction and the correct targets is minimal. Knowing when to stop the training and what accuracy target to set is an important aspect of training neural networks, mainly because of overfitting and underfitting scenarios. Working with neural networks consists of doing operations with vectors. You represent the vectors as multidimensional arrays. Vectors are useful in deep learning mainly because of one particular operation: the dot product. The dot product of two vectors tells you how similar they are in terms of direction and is scaled by the magnitude of the two vectors. The main vectors inside a neural network are the weights and bias vectors. Loosely, what you want your neural network to do is to check if an input is similar to other inputs it’s already seen. If the new input is similar to previously seen inputs, then the outputs will also be similar. That’s how you get the result of a prediction. Regression is used when you need to estimate the relationship between a dependent variable and two or more independent variables. Linear regression is a method applied when you approximate the relationship between the variables as linear. The method dates back to the nineteenth century and is the most popular regression method. Note: A linear relationship is one where there’s a direct relationship between an independent variable and a dependent variable. By modeling the relationship between the variables as linear, you can express the dependent variable as a weighted sum of the independent variables. So, each independent variable will be multiplied by a vector called . Besides the weights and the independent variables, you also add another vector: the bias. It sets the result when all the other independent variables are equal to zero. As a real-world example of how to build a linear regression model, imagine you want to train a model to predict the price of houses based on the area and how old the house is. You decide to model this relationship using linear regression. The following code block shows how you can write a linear regression model for the stated problem in pseudocode: In the above example, there are two weights: and . The training process consists of adjusting the weights and the bias so the model can predict the correct price value. To accomplish that, you’ll need to compute the prediction error and update the weights accordingly. These are the basics of how the neural network mechanism works. Now it’s time to see how to apply these concepts using Python.\n\nPython AI: Starting to Build Your First Neural Network The first step in building a neural network is generating an output from input data. You’ll do that by creating a weighted sum of the variables. The first thing you’ll need to do is represent the inputs with Python and NumPy. Wrapping the Inputs of the Neural Network With NumPy You’ll use NumPy to represent the input vectors of the network as arrays. But before you use NumPy, it’s a good idea to play with the vectors in pure Python to better understand what’s going on. In this first example, you have an input vector and the other two weight vectors. The goal is to find which of the weights is more similar to the input, taking into account the direction and the magnitude. This is how the vectors look if you plot them: is more similar to the input vector since it’s pointing in the same direction and the magnitude is also similar. So how do you figure out which vectors are similar using Python? First, you define the three vectors, one for the input and the other two for the weights. Then you compute how similar and are. To do that, you’ll apply the dot product. Since all the vectors are two-dimensional vectors, these are the steps to do it:\n• Multiply the first index of by the first index of .\n• Multiply the second index of by the second index of .\n• Sum the results of both multiplications. You can use an IPython console or a Jupyter Notebook to follow along. It’s a good practice to create a new virtual environment every time you start a new Python project, so you should do that first. ships with Python versions 3.3 and above, and it’s handy for creating a virtual environment: Using the above commands, you first create the virtual environment, then you activate it. Now it’s time to install the IPython console using . Since you’ll also need NumPy and Matplotlib, it’s a good idea install them too: Now you’re ready to start coding. This is the code for computing the dot product of and : # Computing the dot product of input_vector and weights_1 The result of the dot product is . Now that you know how to compute the dot product, it’s time to use from NumPy. Here’s how to compute using : does the same thing you did before, but now you just need to specify the two arrays as arguments. Now let’s compute the dot product of and : This time, the result is . As a different way of thinking about the dot product, you can treat the similarity between the vector coordinates as an on-off switch. If the multiplication result is , then you’ll say that the coordinates are not similar. If the result is something other than , then you’ll say that they are similar. This way, you can view the dot product as a loose measurement of similarity between the vectors. Every time the multiplication result is , the final dot product will have a lower result. Getting back to the vectors of the example, since the dot product of and is , and is greater than , it means that is more similar to . You’ll use this same mechanism in your neural network. Note: Click the prompt (>>>) at the top right of each code block if you need to copy and paste it. In this tutorial, you’ll train a model to make predictions that have only two possible outcomes. The output result can be either or . This is a classification problem, a subset of supervised learning problems in which you have a dataset with the inputs and the known targets. These are the inputs and the outputs of the dataset: The target is the variable you want to predict. In this example, you’re dealing with a dataset that consists of numbers. This isn’t common in a real production scenario. Usually, when there’s a need for a deep learning model, the data is presented in files, such as images or text. Since this is your very first neural network, you’ll keep things straightforward and build a network with only two layers. So far, you’ve seen that the only two operations used inside the neural network were the dot product and a sum. Both are linear operations. If you add more layers but keep using only linear operations, then adding more layers would have no effect because each layer will always have some correlation with the input of the previous layer. This implies that, for a network with multiple layers, there would always be a network with fewer layers that predicts the same results. What you want is to find an operation that makes the middle layers sometimes correlate with an input and sometimes not correlate. You can achieve this behavior by using nonlinear functions. These nonlinear functions are called activation functions. There are many types of activation functions. The ReLU (rectified linear unit), for example, is a function that converts all negative numbers to zero. This means that the network can “turn off” a weight if it’s negative, adding nonlinearity. The network you’re building will use the sigmoid activation function. You’ll use it in the last layer, . The only two possible outputs in the dataset are and , and the sigmoid function limits the output to a range between and . This is the formula to express the sigmoid function: The e is a mathematical constant called Euler’s number, and you can use to calculate eˣ. Probability functions give you the probability of occurrence for possible outcomes of an event. The only two possible outputs of the dataset are and , and the Bernoulli distribution is a distribution that has two possible outcomes as well. The sigmoid function is a good choice if your problem follows the Bernoulli distribution, so that’s why you’re using it in the last layer of your neural network. Since the function limits the output to a range of to , you’ll use it to predict probabilities. If the output is greater than , then you’ll say the prediction is . If it’s below , then you’ll say the prediction is . This is the flow of the computations inside the network you’re building: The flow of computations inside your neural network The yellow hexagons represent the functions, and the blue rectangles represent the intermediate results. Now it’s time to turn all this knowledge into code. You’ll also need to wrap the vectors with NumPy arrays. This is the code that applies the functions presented in the image above: The raw prediction result is , which is higher than , so the output is . The network made a correct prediction. Now try it with another input vector, . The correct result for this input is . You’ll only need to change the variable since all the other parameters remain the same: # Changing the value of input_vector This time, the network made a wrong prediction. The result should be less than since the target for this input is , but the raw result was . It made a wrong guess, but how bad was the mistake? The next step is to find a way to assess that.\n\nIn the process of training the neural network, you first assess the error and then adjust the weights accordingly. To adjust the weights, you’ll use the gradient descent and backpropagation algorithms. Gradient descent is applied to find the direction and the rate to update the parameters. Before making any changes in the network, you need to compute the error. That’s what you’ll do in the next section. To understand the magnitude of the error, you need to choose a way to measure it. The function used to measure the error is called the cost function, or loss function. In this tutorial, you’ll use the mean squared error (MSE) as your cost function. You compute the MSE in two steps:\n• Compute the difference between the prediction and the target.\n• Multiply the result by itself. The network can make a mistake by outputting a value that’s higher or lower than the correct value. Since the MSE is the squared difference between the prediction and the correct result, with this metric you’ll always end up with a positive value. This is the complete expression to compute the error for the last previous prediction: In the example above, the error is . One implication of multiplying the difference by itself is that bigger errors have an even larger impact, and smaller errors keep getting smaller as they decrease. Understanding How to Reduce the Error The goal is to change the weights and bias variables so you can reduce the error. To understand how this works, you’ll change only the weights variable and leave the bias fixed for now. You can also get rid of the sigmoid function and use only the result of . All that’s left is to figure out how you can modify the weights so that the error goes down. You compute the MSE by doing . If you treat as a single variable , then you have , which is a quadratic function. Here’s how the function looks if you plot it: The error is given by the y-axis. If you’re in point and want to reduce the error toward 0, then you need to bring the value down. On the other hand, if you’re in point and want to reduce the error, then you need to bring the value up. To know which direction you should go to reduce the error, you’ll use the derivative. A derivative explains exactly how a pattern will change. Another word for the derivative is gradient. Gradient descent is the name of the algorithm used to find the direction and the rate to update the network parameters. Note: To learn more about the math behind gradient descent, check out Stochastic Gradient Descent Algorithm With Python and NumPy. In this tutorial, you won’t focus on the theory behind derivatives, so you’ll simply apply the derivative rules for each function you’ll encounter. The power rule states that the derivative of xⁿ is nx⁽ⁿ⁻¹⁾. So the derivative of is , and the derivative of is . Remember that the error expression is . When you treat as a single variable , the derivative of the error is . By taking the derivative of this function, you want to know in what direction should you change to bring the result of to zero, thereby reducing the error. When it comes to your neural network, the derivative will tell you the direction you should take to update the weights variable. If it’s a positive number, then you predicted too high, and you need to decrease the weights. If it’s a negative number, then you predicted too low, and you need to increase the weights. Now it’s time to write the code to figure out how to update for the previous wrong prediction. If the mean squared error is , then should you increase or decrease the weights? Since the derivative is , you just need to multiply the difference between the prediction and the target by : The result is , a positive number, so you need to decrease the weights. You do that by subtracting the derivative result of the weights vector. Now you can update accordingly and predict again to see how it affects the prediction result: The error dropped down to almost ! Beautiful, right? In this example, the derivative result was small, but there are some cases where the derivative result is too high. Take the image of the quadratic function as an example. High increments aren’t ideal because you could keep going from point straight to point , never getting close to zero. To cope with that, you update the weights with a fraction of the derivative result. To define a fraction for updating the weights, you use the alpha parameter, also called the learning rate. If you decrease the learning rate, then the increments are smaller. If you increase it, then the steps are higher. How do you know what’s the best learning rate value? By making a guess and experimenting with it. If you take the new weights and make a prediction with the first input vector, then you’ll see that now it makes a wrong prediction for that one. If your neural network makes a correct prediction for every instance in your training set, then you probably have an overfitted model, where the model simply remembers how to classify the examples instead of learning to notice features in the data. There are techniques to avoid that, including regularization the stochastic gradient descent. In this tutorial you’ll use the online stochastic gradient descent. Now that you know how to compute the error and how to adjust the weights accordingly, it’s time to get back continue building your neural network. In your neural network, you need to update both the weights and the bias vectors. The function you’re using to measure the error depends on two independent variables, the weights and the bias. Since the weights and the bias are independent variables, you can change and adjust them to get the result you want. The network you’re building has two layers, and since each layer has its own functions, you’re dealing with a function composition. This means that the error function is still , but now is the result of another function. To restate the problem, now you want to know how to change and to reduce the error. You already saw that you can use derivatives for this, but instead of a function with only a sum inside, now you have a function that produces its result using other functions. Since now you have this function composition, to take the derivative of the error concerning the parameters, you’ll need to use the chain rule from calculus. With the chain rule, you take the partial derivatives of each function, evaluate them, and multiply all the partial derivatives to get the derivative you want. Now you can start updating the weights. You want to know how to change the weights to decrease the error. This implies that you need to compute the derivative of the error with respect to weights. Since the error is computed by combining different functions, you need to take the partial derivatives of these functions. Here’s a visual representation of how you apply the chain rule to find the derivative of the error with respect to the weights: The bold red arrow shows the derivative you want, . You’ll start from the red hexagon, taking the inverse path of making a prediction and computing the partial derivatives at each function. In the image above, each function is represented by the yellow hexagons, and the partial derivatives are represented by the gray arrows on the left. Applying the chain rule, the value of will be the following: To calculate the derivative, you multiply all the partial derivatives that follow the path from the error hexagon (the red one) to the hexagon where you find the weights (the leftmost green one). You can say that the derivative of is the derivative of with respect to . Using this nomenclature, for , you want to know the derivative of the function that computes the error with respect to the prediction value. This reverse path is called a backward pass. In each backward pass, you compute the partial derivatives of each function, substitute the variables by their values, and finally multiply everything. This “take the partial derivatives, evaluate, and multiply” part is how you apply the chain rule. This algorithm to update the neural network parameters is called backpropagation. In this section, you’ll walk through the backpropagation process step by step, starting with how you update the bias. You want to take the derivative of the error function with respect to the bias, . Then you’ll keep going backward, taking the partial derivatives until you find the variable. Since you are starting from the end and going backward, you first need to take the partial derivative of the error with respect to the prediction. That’s the in the image below: A diagram showing the partial derivatives to compute the bias gradient The function that produces the error is a square function, and the derivative of this function is , as you saw earlier. You applied the first partial derivative ( ) and still didn’t get to the bias, so you need to take another step back and take the derivative of the prediction with respect to the previous layer, . The prediction is the result of the sigmoid function. You can take the derivative of the sigmoid function by multiplying and . This derivative formula is very handy because you can use the sigmoid result that has already been computed to compute the derivative of it. You then take this partial derivative and continue going backward. Now you’ll take the derivative of with respect to the bias. There it is—you finally got to it! The variable is an independent variable, so the result after applying the power rule is . Cool, now that you’ve completed this backward pass, you can put everything together and compute : To update the weights, you follow the same process, going backward and taking the partial derivatives until you get to the weights variable. Since you’ve already computed some of the partial derivatives, you’ll just need to compute . The derivative of the dot product is the derivative of the first vector multiplied by the second vector, plus the derivative of the second vector multiplied by the first vector. Now you know how to write the expressions to update both the weights and the bias. It’s time to create a class for the neural network. Classes are the main building blocks of object-oriented programming (OOP). The class generates random start values for the weights and bias variables. When instantiating a object, you need to pass the parameter. You’ll use to make a prediction. The methods and have the computations you learned in this section. This is the final class: There you have it: That’s the code of your first neural network. Congratulations! This code just puts together all the pieces you’ve seen so far. If you want to make a prediction, first you create an instance of , and then you call : The above code makes a prediction, but now you need to learn how to train the network. The goal is to make the network generalize over the training dataset. This means that you want it to adapt to new, unseen data that follow the same probability distribution as the training dataset. That’s what you’ll do in the next section. Training the Network With More Data You’ve already adjusted the weights and the bias for one data instance, but the goal is to make the network generalize over an entire dataset. Stochastic gradient descent is a technique in which, at every iteration, the model makes a prediction based on a randomly selected piece of training data, calculates the error, and updates the parameters. Now it’s time to create the method of your class. You’ll save the error over all data points every 100 iterations because you want to plot a chart showing how this metric changes as the number of iterations increases. This is the final method of your neural network: # Compute the gradients and update the weights # Measure the cumulative error for all the instances # Loop through all the instances to measure the error There’s a lot going on in the above code block, so here’s a line-by-line breakdown:\n• Lines 14 to 16 calculate the partial derivatives and return the derivatives for the bias and the weights. They use , which you defined earlier.\n• Line 18 updates the bias and the weights using , which you defined in the previous code block.\n• Line 21 checks if the current iteration index is a multiple of . You do this to observe how the error changes every iterations.\n• Line 24 starts the loop that goes through all the data instances.\n• Line 29 computes the for every instance.\n• Line 31 is where you accumulate the sum of the errors using the variable. You do this because you want to plot a point with the error for all the data instances. Then, on line 32, you append the to , the array that stores the errors. You’ll use this array to plot the graph. In short, you pick a random instance from the dataset, compute the gradients, and update the weights and the bias. You also compute the cumulative error every 100 iterations and save those results in an array. You’ll plot this array to visualize how the error changes during the training process. Note: If you’re running the code in a Jupyter Notebook, then you need to restart the kernel after adding to the class. To keep things less complicated, you’ll use a dataset with just eight instances, the array. Now you can call and use Matplotlib to plot the cumulative error for each iteration: # (and don't forget to add the train method to the class) You instantiate the class again and call using the and the values. You specify that it should run times. This is the graph showing the error for an instance of a neural network: The overall error is decreasing, which is what you want. The image is generated in the same directory where you’re running IPython. After the largest decrease, the error keeps going up and down quickly from one interaction to another. That’s because the dataset is random and very small, so it’s hard for the neural network to extract any features. But it’s not a good idea to evaluate the performance using this metric because you’re evaluating it using data instances that the network already saw. This can lead to overfitting, when the model fits the training dataset so well that it doesn’t generalize to new data. Adding More Layers to the Neural Network The dataset in this tutorial was kept small for learning purposes. Usually, deep learning models need a large amount of data because the datasets are more complex and have a lot of nuances. Since these datasets have more complex information, using only one or two layers isn’t enough. That’s why deep learning models are called “deep.” They usually have a large number of layers. By adding more layers and using activation functions, you increase the network’s expressive power and can make very high-level predictions. An example of these types of predictions is face recognition, such as when you take a photo of your face with your phone, and the phone unlocks if it recognizes the image as you."
    },
    {
        "link": "https://tensorflow.org/text/tutorials/text_generation",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nThis tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's The Unreasonable Effectiveness of Recurrent Neural Networks. Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n\nThis tutorial includes runnable code implemented using tf.keras and eager execution. The following is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":\n\nWhile some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n• None The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n• None The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n• None As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure.\n\nChange the following line to run this code on your own data.\n\nFirst, look in the text:\n\nBefore training, you need to convert the strings to a numerical representation.\n\nThe layer can convert each character into a numeric ID. It just needs the text to be split into tokens first.\n\nIt converts from tokens to character IDs:\n\nSince the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use .\n\nThis layer recovers the characters from the vectors of IDs, and returns them as a of characters:\n\nYou can to join the characters back into strings.\n\nGiven a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n\nSince RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n\nNext divide the text into example sequences. Each input sequence will contain characters from the text.\n\nFor each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n\nSo break the text into chunks of . For example, say is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n\nTo do this first use the function to convert the text vector into a stream of character indices.\n\nThe method lets you easily convert these individual characters to sequences of the desired size.\n\nIt's easier to see what this is doing if you join the tokens back into strings:\n\nFor training you'll need a dataset of pairs. Where and are sequences. At each time step the input is the current character and the label is the next character.\n\nHere's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:\n\nYou used to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches.\n\nThis section defines the model as a subclass (For details see Making new Layers and Models via subclassing).\n\nThis model has three layers:\n• : The input layer. A trainable lookup table that will map each character-ID to a vector with dimensions;\n• : A type of RNN with size (You can also use an LSTM layer here.)\n• : The output layer, with outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n\nFor each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n\nNow run the model to see that it behaves as expected.\n\nFirst check the shape of the output:\n\nIn the above example the sequence length of the input is but the model can be run on inputs of any length:\n\nTo get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n\nTry it for the first example in the batch:\n\nThis gives us, at each timestep, a prediction of the next character index:\n\nDecode these to see the text predicted by this untrained model:\n\nAt this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character.\n\nThe standard loss function works in this case because it is applied across the last dimension of the predictions.\n\nBecause your model returns logits, you need to set the flag.\n\nA newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:\n\nConfigure the training procedure using the method. Use with default arguments and the loss function.\n\nUse a to ensure that checkpoints are saved during training:\n\nTo keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training.\n\nThe simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n\nEach time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n\nRun it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences.\n\nThe easiest thing you can do to improve the results is to train it for longer (try ).\n\nYou can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions.\n\nIf you want the model to generate text faster the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above.\n\nThis single-step model can easily be saved and restored, allowing you to use it anywhere a is accepted.\n\nThe above training procedure is simple, but does not give you much control. It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n\nSo now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement curriculum learning to help stabilize the model's open-loop output.\n\nThe most important part of a custom training loop is the train step function.\n\nUse to track the gradients. You can learn more about this approach by reading the eager execution guide.\n• Execute the model and calculate the loss under a .\n• Calculate the updates and apply them to the model using the optimizer.\n\nThe above implementation of the method follows Keras' conventions. This is optional, but it allows you to change the behavior of the train step and still use keras' and methods.\n\nOr if you need more control, you can write your own complete custom training loop:"
    },
    {
        "link": "https://medium.com/@annikabrundyn1/the-beginners-guide-to-recurrent-neural-networks-and-text-generation-44a70c34067f",
        "document": "RNNs generalise feedforward networks (FFNs) to be able to model sequential data. FFNs take an input (e.g. an image) and immediately produce an output (e.g. probabilities of different classes). RNNs, on the other hand, consider the data sequentially, and can remember what they have seen earlier in the sequence to help interpret elements from later in the sequence.\n\nTo better demonstrate the distinction between FFNs and RNNs, imagine we want to label words as the part-of-speech categories that they belong to: E.g. for the input sentence “I would like the duck” and “He had to duck”. Our model should predict that duck is a in the first sentence and a in the second. To do this successfully, the model needs to be aware of the surrounding context. However, if we feed a FFN model only one word at a time, how could it know the difference? If we want to feed it all the words at once, how do we deal with the fact that sentences are of different lengths?\n\nWe have seen that text consists of words or characters that are required to be in a specific sequence in order to understand their intended meaning. But sequence data comes in many other forms. Audio is a natural sequence of audiograms. Stock market prices are numerical time series. Genomes are sequences. Videos are sequences of images. RNNs can operate over sequences of vectors in both the input and the output.The many forms of sequence prediction problems are probably best described by the types of inputs and outputs supported.\n\nEach rectangle is a vector and arrows represent functions (e.g. matrix multiply). Input vectors are in red, output vectors are in blue and green vectors hold the RNN’s state (more on this soon). From left to right:\n• One-to-one: Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification).\n• One-to-many: Sequence output (e.g. image captioning takes an image and outputs a sentence of words).\n• Many-to-one: Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment or given some text predict the next character)\n• Many-to-many: Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French).\n• Many-to-many: Synced sequence input and output (e.g. video classification where we wish to label each frame of the video).\n\nAlright so, we’ve established that RNNs are good at processing sequence data for predictions and that there are many examples of where this may be useful — but why exactly are they good at doing this?\n\nRNNs have the ability to capture “sequential memory” by maintaining an internal state summarising what has been seen so far. Thereby, making it significantly easier for them to recognise sequential patterns.\n\nNow, how exactly does the structure of an RNN manage to incorporate this abstract concept of sequential memory?\n\nRemember what a traditional feed-forward neural network looks like? It has an input layer, hidden layer, and the output layer.\n\nRNNs allow information to persist in the network by introducing loops that can pass prior information forward. Essentially, an RNN introduces a looping mechanism that acts as a highway to allow information to flow from one step to the next.\n\nYou can think of the hidden state as the memory of the network capturing information about what happened in all the previous time steps. While processing the hidden state is continuously updated and passed to the next step of the sequence.\n\nLet’s run through how an RNN works in full:\n\nFirst words get transformed into machine-readable vectors. Then the RNN processes the sequence of vectors one by one.\n\nWhile processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks internal memory. It holds information on previous data the network has seen before.\n\nLet’s zoom in on a cell of the RNN to see how you would calculate the hidden state. First, the input and previous hidden state are combined to form a vector. That vector now has information on the current input and previous inputs. The vector goes through the tanh activation, and the output is the new hidden state, or the memory of the network.\n\nThe tanh activation function (indicated by the blue circle above) is commonly used in many neural networks to ensure that values in a network don’t explode given the many mathematical operations by forcing them to stay between -1 and 1. This regulates the flow of output in the network. For more information on tanh functions, take a look at this article.\n\nAnd there we have the control flow of doing a forward pass of a recurrent neural network.\n\nThe problem: short-term memory and the vanishing gradient\n\nImagine we are trying to predict the next word in the following piece of text:\n\nIn theory, RNNs can make use of information in arbitrarily long sequences but in practice they are limited to looking back only a few steps. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So, in the above text the RNN may be able to detect that it should predict the name of a language but it may also leave out important information from the beginning of the text like the fact that the speaker grew up in France.\n\nAnd so, RNNs suffer from what is known as short-term memory. This phenomenon is caused by the infamous vanishing gradient problem, occurring in many other neural network architectures. And the vanishing gradient problem is ultimately driven by the nature of back-propagation: an algorithm used to train and optimise neural networks.\n\nRNNs are trained using an application of back-propagation known as back-propagation through time (BPTT). Gradients are values used to update a neural networks weights, allowing them to learn. The bigger the gradient, the bigger the adjustments to the weights and vice versa. Here is where the problem lies: when doing back propagation, each gradient is calculated with respect to the effects of the gradients, in the previous time step. So if the adjustments to the previous time-step are small, then adjustments at the current time step will be even smaller. Small gradients mean small adjustments, which means that the early layers will not learn. This post will not cover how BPTT works in detail (this article contains a great explanation).\n\nBecause of vanishing gradients, the vanilla RNN suffers from short-term memory and the decay of information over time.\n\nLSTMs and GRUs to the rescue\n\nIn combatting the curse of short-term memory, two specialised RNNs were created that make use of an internal mechanism called “gates” that can regulate the flow of information. Gating is a technique that helps the network learn which data in a sequence is important to keep (by adding it to the hidden state) and which should be thrown away. By doing so it makes the network more capable of learning long-term dependencies. Two of the most popular gating types today are called Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU).\n\nAlmost all state of the art results based on recurrent neural networks are achieved with these two networks. LSTM’s and GRU’s can be found in speech recognition, speech synthesis, and text generation (as we’ll see shortly). You can even use them to generate captions for videos.\n\nNeither LSTMs or GRUs have fundamentally different architectures from the vanilla RNNs. The control flow remains similar: it processes data by passing on information as it propagates forward. The differences lie in how the hidden state is computed and the operations within the LSTM or GRU cells.\n\nI’m hoping to write a follow up post exclusively on the gritty mechanisms underlying LSTM and GRU cells but if you are interested in going deeper — I think this article is brilliantly intuitive and very visual in explaining the working of these networks."
    },
    {
        "link": "https://stackoverflow.com/questions/60170063/correctly-structuring-text-data-for-text-generation-with-tensorflow-model",
        "document": "There are a few things that must be changed on the first sight.\n• At first (didn't analyse it deeply), I can't imagine why you're flattening your dataset and getting slices if it's probably correctly structured\n• You cannot use if you don't want that \"batch 2 is a sequel of batch 1\", you have individual sentences, so . (Unless you are training correctly with manual training loops and resetting states for each batch, which is unnecessary trouble in the training phase)\n\nWhat you need to check visually:\n\nInput data must have format like:\n\nOutput data must then be:\n\nPrint a few rows of them to check if they're correctly preprocessed as intended.\n\nTraining will then be easy:\n\nYes, your model will predict zeros, as you have zeros in your data, that's not weird: you did a .\n\n You can interpret a zero as a \"sentence end\" in this case, since you did a pading. When your model gives you a zero, it decided that the sentence it's generating should end at that point - if it was well trained, it will probably continue outputting zeros for that sentence from this point on.\n\nThis part is more complex and you need to rewrite the model, now being , and transfer the weights from the trained model to this new model.\n\nYou will need to manually feed a batch with shape . This will be the \"first character\" of each of the sentences it will generate. The output will be the \"second character\" of each sentence.\n\nGet this output and feed the model with it. It will generate the \"third character\" of each sentence. And so on.\n\nWhen all outputs are zero, all sentences are fully generated and you can stop the loop.\n\nCall again before trying to generate a new batch of sentences.\n\nYou can find examples of this kind of predicting here: https://stackoverflow.com/a/50235563/2097240"
    }
]