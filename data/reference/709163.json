[
    {
        "link": "https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html",
        "document": "\n• We will use the cv::CascadeClassifier class to detect objects in a video stream. Particularly, we will use the functions:\n• cv::CascadeClassifier::load to load a .xml classifier file. It can be either a Haar or a LBP classifier\n\nObject Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.\n\nHere we will work with face detection. Initially, the algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier. Then we need to extract features from it. For this, Haar features shown in the below image are used. They are just like our convolutional kernel. Each feature is a single value obtained by subtracting sum of pixels under the white rectangle from sum of pixels under the black rectangle.\n\nNow, all possible sizes and locations of each kernel are used to calculate lots of features. (Just imagine how much computation it needs? Even a 24x24 window results over 160000 features). For each feature calculation, we need to find the sum of the pixels under white and black rectangles. To solve this, they introduced the integral image. However large your image, it reduces the calculations for a given pixel to an operation involving just four pixels. Nice, isn't it? It makes things super-fast.\n\nFor this, we apply each and every feature on all the training images. For each feature, it finds the best threshold which will classify the faces to positive and negative. Obviously, there will be errors or misclassifications. We select the features with minimum error rate, which means they are the features that most accurately classify the face and non-face images. (The process is not as simple as this. Each image is given an equal weight in the beginning. After each classification, weights of misclassified images are increased. Then the same process is done. New error rates are calculated. Also new weights. The process is continued until the required accuracy or error rate is achieved or the required number of features are found).\n\nThe final classifier is a weighted sum of these weak classifiers. It is called weak because it alone can't classify the image, but together with others forms a strong classifier. The paper says even 200 features provide detection with 95% accuracy. Their final setup had around 6000 features. (Imagine a reduction from 160000+ features to 6000 features. That is a big gain).\n\nSo now you take an image. Take each 24x24 window. Apply 6000 features to it. Check if it is face or not. Wow.. Isn't it a little inefficient and time consuming? Yes, it is. The authors have a good solution for that.\n\nIn an image, most of the image is non-face region. So it is a better idea to have a simple method to check if a window is not a face region. If it is not, discard it in a single shot, and don't process it again. Instead, focus on regions where there can be a face. This way, we spend more time checking possible face regions.\n\nFor this they introduced the concept of Cascade of Classifiers. Instead of applying all 6000 features on a window, the features are grouped into different stages of classifiers and applied one-by-one. (Normally the first few stages will contain very many fewer features). If a window fails the first stage, discard it. We don't consider the remaining features on it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region. How is that plan!"
    },
    {
        "link": "https://docs.opencv.org/4.x/db/d28/tutorial_cascade_classifier.html",
        "document": "\n• We will learn how the Haar cascade object detection works.\n• We will see the basics of face detection and eye detection using the Haar Feature-based Cascade Classifiers\n• We will use the cv::CascadeClassifier class to detect objects in a video stream. Particularly, we will use the functions:\n• cv::CascadeClassifier::load to load a .xml classifier file. It can be either a Haar or a LBP classifier\n\nObject Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.\n\nHere we will work with face detection. Initially, the algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier. Then we need to extract features from it. For this, Haar features shown in the below image are used. They are just like our convolutional kernel. Each feature is a single value obtained by subtracting sum of pixels under the white rectangle from sum of pixels under the black rectangle.\n\nNow, all possible sizes and locations of each kernel are used to calculate lots of features. (Just imagine how much computation it needs? Even a 24x24 window results over 160000 features). For each feature calculation, we need to find the sum of the pixels under white and black rectangles. To solve this, they introduced the integral image. However large your image, it reduces the calculations for a given pixel to an operation involving just four pixels. Nice, isn't it? It makes things super-fast.\n\nBut among all these features we calculated, most of them are irrelevant. For example, consider the image below. The top row shows two good features. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks. The second feature selected relies on the property that the eyes are darker than the bridge of the nose. But the same windows applied to cheeks or any other place is irrelevant. So how do we select the best features out of 160000+ features? It is achieved by Adaboost.\n\nFor this, we apply each and every feature on all the training images. For each feature, it finds the best threshold which will classify the faces to positive and negative. Obviously, there will be errors or misclassifications. We select the features with minimum error rate, which means they are the features that most accurately classify the face and non-face images. (The process is not as simple as this. Each image is given an equal weight in the beginning. After each classification, weights of misclassified images are increased. Then the same process is done. New error rates are calculated. Also new weights. The process is continued until the required accuracy or error rate is achieved or the required number of features are found).\n\nThe final classifier is a weighted sum of these weak classifiers. It is called weak because it alone can't classify the image, but together with others forms a strong classifier. The paper says even 200 features provide detection with 95% accuracy. Their final setup had around 6000 features. (Imagine a reduction from 160000+ features to 6000 features. That is a big gain).\n\nSo now you take an image. Take each 24x24 window. Apply 6000 features to it. Check if it is face or not. Wow.. Isn't it a little inefficient and time consuming? Yes, it is. The authors have a good solution for that.\n\nIn an image, most of the image is non-face region. So it is a better idea to have a simple method to check if a window is not a face region. If it is not, discard it in a single shot, and don't process it again. Instead, focus on regions where there can be a face. This way, we spend more time checking possible face regions.\n\nFor this they introduced the concept of Cascade of Classifiers. Instead of applying all 6000 features on a window, the features are grouped into different stages of classifiers and applied one-by-one. (Normally the first few stages will contain very many fewer features). If a window fails the first stage, discard it. We don't consider the remaining features on it. If it passes, apply the second stage of features and continue the process. The window which passes all stages is a face region. How is that plan!\n\nThe authors' detector had 6000+ features with 38 stages with 1, 10, 25, 25 and 50 features in the first five stages. (The two features in the above image are actually obtained as the best two features from Adaboost). According to the authors, on average 10 features out of 6000+ are evaluated per sub-window.\n\nSo this is a simple intuitive explanation of how Viola-Jones face detection works. Read the paper for more details or check out the references in the Additional Resources section.\n\nOpenCV provides a training method (see Cascade Classifier Training) or pretrained models, that can be read using the cv::CascadeClassifier::load method. The pretrained models are located in the data folder in the OpenCV installation or can be found here.\n\nThe following code example will use pretrained Haar cascade models to detect faces and eyes in an image. First, a cv::CascadeClassifier is created and the necessary XML file is loaded using the cv::CascadeClassifier::load method. Afterwards, the detection is done using the cv::CascadeClassifier::detectMultiScale method, which returns boundary rectangles for the detected faces or eyes.\n• Here is the result of running the code above and using as input the video stream of a built-in webcam:\n\nBe sure the program will find the path of files haarcascade_frontalface_alt.xml and haarcascade_eye_tree_eyeglasses.xml. They are located in opencv/data/haarcascades\n• This is the result of using the file lbpcascade_frontalface.xml (LBP trained) for the face detection. For the eyes we keep using the file used in the tutorial.\n• Rainer Lienhart and Jochen Maydt. An extended set of haar-like features for rapid object detection. In Image Processing. 2002. Proceedings. 2002 International Conference on, volume 1, pages I–900. IEEE, 2002. [168]\n• An interesting interview regarding Face Detection by Adam Harvey"
    },
    {
        "link": "https://geeksforgeeks.org/face-detection-using-cascade-classifier-using-opencv-python",
        "document": "In this article, we are going to see how to detect faces using a cascade classifier in OpenCV Python. Face detection has much significance in different fields of today’s world. It is a significant step in several applications, face recognition (also used as biometrics), photography (for auto-focus on the face), face analysis (age, gender, emotion recognition), video surveillance, etc.\n\nOne of the popular algorithms for facial detection is “haarcascade”. It is computationally less expensive, a fast algorithm, and gives high accuracy.\n\nHaarcascade file can be download from here: haarcascade_frontalface_default.xml\n\nIt works in four stages:\n• Haar-feature selection: A Haar-like feature consists of dark regions and light regions. It produces a single value by taking the difference of the sum of the intensities of the dark regions and the sum of the intensities of light regions. It is done to extract useful elements necessary for identifying an object. The features proposed by viola and jones are:\n• Creation of Integral Images: A given pixel in the integral image is the sum of all the pixels on the left and all the pixels above it. Since the process of extracting Haar-like features involves calculating the difference of dark and light rectangular regions, the introduction of Integral Images reduces the time needed to complete this task significantly.\n• AdaBoost Training: This algorithm selects the best features from all features. It combines multiple “weak classifiers” (best features) into one “strong classifier”. The generated “strong classifier” is basically the linear combination of all “weak classifiers”.\n• Cascade Classifier: It is a method for combining increasingly more complex classifiers like AdaBoost in a cascade which allows negative input (non-face) to be quickly discarded while spending more computation on promising or positive face-like regions. It significantly reduces the computation time and makes the process more efficient.\n\nOpenCV comes with lots of pre-trained classifiers. Those XML files can be loaded by cascadeClassifier method of the cv2 module. Here we are going to use haarcascade_frontalface_default.xml for detecting faces.\n\nInitially, the image is a three-layer image (i.e., RGB), So It is converted to a one-layer image (i.e., grayscale).\n\nCascadeClassifier method in cv2 module supports the loading of haar-cascade XML files. Here, we need “haarcascade_frontalface_default.xml” for face detection.\n\nStep 4: Applying the face detection method on the grayscale image\n\nThis is done using the cv2::CascadeClassifier::detectMultiScale method, which returns boundary rectangles for the detected faces (i.e., x, y, w, h). It takes two parameters namely, scaleFactor and minNeighbors. ScaleFactor determines the factor of increase in window size which initially starts at size “minSize”, and after testing all windows of that size, the window is scaled up by the “scaleFactor”, and the window size goes up to “maxSize”. If the “scaleFactor” is large, (e.g., 2.0), there will be fewer steps, so detection will be faster, but we may miss objects whose size is between two tested scales. (default scale factor is 1.3). Higher the values of the “minNeighbors”, less will be the number of false positives, and less error will be in terms of false detection of faces. However, there is a chance of missing some unclear face traces as well.\n\nRectangles are drawn around the detected faces by the rectangle method of the cv2 module by iterating over all detected faces.\n\nBelow is the implementation:"
    },
    {
        "link": "https://datacamp.com/tutorial/face-detection-python-opencv",
        "document": "Discover how to begin responsibly leveraging generative AI. Learn how generative AI models are developed and how they will impact society moving forward."
    },
    {
        "link": "https://medium.com/@lfoster49203/using-opencv-with-python-for-computer-vision-face-detection-edge-detection-more-49f99210c23b",
        "document": "In this tutorial, I will go over the basics of using OpenCV with Python for image and video processing.\n\nI’ll cover how to install OpenCV (it’s easier than teaching your grandparents how to use Facebook), import it into Python, read and display images and videos, and perform tasks such as grayscale conversion, edge detection, and face detection (Real Secret Agent Type Stuff).\n\nWith OpenCV, the possibilities for image and video processing are endless!\n\nBefore we get started, you need to install OpenCV on your machine. There are several ways to do this, but the easiest way is to use pip. Open a terminal and run the following command:\n\nThis will install the latest version of OpenCV on your machine.\n\nOnce you have installed OpenCV, you can import it into your Python code using the following command:\n\nTo read an image using OpenCV, you can use the function. This function takes the filename of the image as an argument and returns a NumPy array representing the image. Here's an example:\n\nIn this example, we load an image called using . We then display the image using , which opens a window showing the image. Finally, we use to wait for a key press, and to close the window.\n\nReading and displaying videos is similar to reading and displaying images. To read a video, you can use the function. This function takes the filename of the video as an argument and returns a object. You can then use the method of the object to read frames from the video.\n\nIn this example, we load a video called using . We then loop over frames from the video using a loop. Inside the loop, we read a frame from the video using the method of the object. We display the frame using , and we check if the user pressed the 'q' key using . If the user presses 'q', we break out of the loop. Finally, we release the object and close the window.\n\nOpenCV provides a wide range of image and video processing functions. Here are a few examples:\n\nIn this example, we load an image called using . We convert the image to grayscale using , and then detect edges using . The function takes three arguments: the input image, a threshold for the lower bound of the edges, and a threshold for the upper bound of the edges. We then display the edges using .\n\nIn this example, we load a pre-trained face detection classifier using . We then load an image called using , convert it to grayscale using , and detect faces using . The function takes three arguments: the input image, a scale factor, and a minimum number of neighboring rectangles that need to be present for a rectangle to be accepted as a face. We then draw rectangles around the faces using , and display the image with the faces detected using .\n\nPretty cool, right? This can be considered a nice introduction to OpenCV. But understand that OpenCV provides many more functions for image and video processing, so be sure to check out the official documentation for more information."
    },
    {
        "link": "https://face-recognition.readthedocs.io/en/latest/face_recognition.html",
        "document": "Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector If you are using a GPU, this can give you much faster results since the GPU can process batches of images at once. If you aren’t using a GPU, you don’t need this function.\n• images – A list of images (each as a numpy array)\n• number_of_times_to_upsample – How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n• batch_size – How many images to include in each GPU processing batch. A list of tuples of found face locations in css (top, right, bottom, left) order\n\nCompare a list of face encodings against a candidate encoding to see if they match.\n• face_encoding_to_check – A single face encoding to compare against the list\n• tolerance – How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance. A list of True/False values indicating which known_face_encodings match the face encoding to check\n\nGiven a list of face encodings, compare them to a known face encoding and get a euclidean distance for each comparison face. The distance tells you how similar the faces are. A numpy ndarray with the distance for each face in the same order as the ‘faces’ array\n\nGiven an image, return the 128-dimension face encoding for each face in the image.\n• face_image – The image that contains one or more faces\n• known_face_locations – Optional - the bounding boxes of each face if you already know them.\n• num_jitters – How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n• model – Optional - which model to use. “large” or “small” (default) which only returns 5 points but is faster. A list of 128-dimensional face encodings (one for each face in the image)\n\nGiven an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n• model – Optional - which model to use. “large” (default) or “small” which only returns 5 points but is faster. A list of dicts of face feature locations (eyes, nose, etc)"
    },
    {
        "link": "https://face-recognition.readthedocs.io/en/latest/readme.html",
        "document": "Find all the faces that appear in a picture: Get the locations and outlines of each person’s eyes, nose, mouth and chin. Finding facial features is super useful for lots of important stuff. But you can also use for really stupid stuff Recognize who appears in each photo. You can even use this library with other Python libraries to do real-time face recognition: See this example for the code.\n\nWhen you install , you get a simple command-line program called that you can use to recognize faces in a First, you need to provide a folder with one picture of each person you already know. There should be one image file for each person with the files named according to who is in the picture: Next, you need a second folder with the files you want to identify: Then in you simply run the command , passing in the folder of known people and the folder (or single image) with unknown people and it tells you who is in each image: There’s one line in the output for each face. The data is comma-separated with the filename and the name of the person found. An is a face in the image that didn’t match anyone in your folder of known people. If you are getting multiple matches for the same person, it might be that the people in your photos look very similar and a lower tolerance value is needed to make face comparisons more strict. You can do that with the parameter. The default tolerance value is 0.6 and lower numbers make face comparisons more strict: If you want to see the face distance calculated for each match in order to adjust the tolerance setting, you can use : If you simply want to know the names of the people in each photograph but don’t care about file names, you could do this: Face recognition can be done in parallel if you have a computer with multiple CPU cores. For example if your system has 4 CPU cores, you can process about 4 times as many images in the same amount of time by using all your CPU cores in parallel. If you are using Python 3.4 or newer, pass in a parameter: You can also pass in to use all CPU cores in your system. You can import the module and then easily manipulate faces with just a couple of lines of code. It’s super easy! Automatically find all the faces in an image¶ # face_locations is now an array listing the co-ordinates of each face! You can also opt-in to a somewhat more accurate deep-learning-based face detection model. Note: GPU acceleration (via nvidia’s CUDA library) is required for good performance with this model. You’ll also want to enable CUDA support # face_locations is now an array listing the co-ordinates of each face! If you have a lot of images and a GPU, you can also Automatically locate the facial features of a person in an image¶ # face_landmarks_list is now an array with the locations of each facial feature in each face. # face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye. Recognize faces in images and identify who they are¶ # my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face! # Now we can see the two face encodings are of the same person with `compare_faces`! \"It's not a picture of me!\"\n\nIssue: when using face_recognition or running examples. Solution: is compiled with SSE4 or AVX support, but your CPU is too old and doesn’t support that. after You’ll need to recompileafter making the code change outlined here Issue: RuntimeError: Unsupported image type, must be 8bit gray or RGB image. when running the webcam examples. Solution: Your webcam probably isn’t set up correctly with OpenCV. Look here for more. Solution: The face_recognition_models file is too big for your available pip cache memory. Instead, try to avoid the issue. Solution: The version of you have installed is too old. You need version 19.7 or newer. Upgrade . Solution: The version of you have installed is too old. You need version 19.7 or newer. Upgrade . Solution: The version of you have installed is too old. You need version 0.17 or newer. Upgrade ."
    },
    {
        "link": "https://pypi.org/project/face-recognition",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://github.com/ageitgey/face_recognition",
        "document": "You can also read a translated version of this file in Chinese 简体中文版 or in Korean 한국어 or in Japanese 日本語.\n\nRecognize and manipulate faces from Python or from the command line with the world's simplest face recognition library.\n\nBuilt using dlib's state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the Labeled Faces in the Wild benchmark.\n\nThis also provides a simple command line tool that lets you do face recognition on a folder of images from the command line!\n\nFind all the faces that appear in a picture:\n\nGet the locations and outlines of each person's eyes, nose, mouth and chin.\n\nFinding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff like applying digital make-up (think 'Meitu'):\n\nRecognize who appears in each photo.\n\nYou can even use this library with other Python libraries to do real-time face recognition:\n\nSee this example for the code.\n• macOS or Linux (Windows not officially supported, but might work)\n\nFirst, make sure you have dlib already installed with Python bindings:\n• How to install dlib from source on macOS or Ubuntu\n\nThen, make sure you have cmake installed:\n\nFinally, install this module from pypi using (or for Python 2):\n\nAlternatively, you can try this library with Docker, see this section.\n\nIf you are having trouble with installation, you can also try out a pre-configured VM.\n• Jetson Nano installation instructions\n• Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don't follow the instructions in the article to comment out a line in dlib and recompile it.\n\nWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:\n• Download the pre-configured VM image (for VMware Player or VirtualBox).\n\nWhen you install , you get two simple command-line programs:\n• - Recognize faces in a photograph or folder full for photographs.\n• - Find faces in a photograph or folder full for photographs.\n\nThe command lets you recognize faces in a photograph or folder full for photographs.\n\nFirst, you need to provide a folder with one picture of each person you already know. There should be one image file for each person with the files named according to who is in the picture:\n\nNext, you need a second folder with the files you want to identify:\n\nThen in you simply run the command , passing in the folder of known people and the folder (or single image) with unknown people and it tells you who is in each image:\n\nThere's one line in the output for each face. The data is comma-separated with the filename and the name of the person found.\n\nAn is a face in the image that didn't match anyone in your folder of known people.\n\nThe command lets you find the location (pixel coordinatates) of any faces in an image.\n\nJust run the command , passing in a folder of images to check (or a single image):\n\nIt prints one line for each face that was detected. The coordinates reported are the top, right, bottom and left coordinates of the face (in pixels).\n\nIf you are getting multiple matches for the same person, it might be that the people in your photos look very similar and a lower tolerance value is needed to make face comparisons more strict.\n\nYou can do that with the parameter. The default tolerance value is 0.6 and lower numbers make face comparisons more strict:\n\nIf you want to see the face distance calculated for each match in order to adjust the tolerance setting, you can use :\n\nIf you simply want to know the names of the people in each photograph but don't care about file names, you could do this:\n\nFace recognition can be done in parallel if you have a computer with multiple CPU cores. For example, if your system has 4 CPU cores, you can process about 4 times as many images in the same amount of time by using all your CPU cores in parallel.\n\nIf you are using Python 3.4 or newer, pass in a parameter:\n\nYou can also pass in to use all CPU cores in your system.\n\nYou can import the module and then easily manipulate faces with just a couple of lines of code. It's super easy!\n\nSee this example to try it out.\n\nYou can also opt-in to a somewhat more accurate deep-learning-based face detection model.\n\nNote: GPU acceleration (via NVidia's CUDA library) is required for good performance with this model. You'll also want to enable CUDA support when compliling .\n\nSee this example to try it out.\n\nIf you have a lot of images and a GPU, you can also find faces in batches.\n\nSee this example to try it out.\n\nSee this example to try it out.\n\nAll the examples are available here.\n• Find faces in batches of images w/ GPU (using deep learning)\n• Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)\n• Find and recognize unknown faces in a photograph based on photographs of known people\n• Identify and draw boxes around each person in a photo\n• Compare faces by numeric face distance instead of only True/False matches\n• Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)\n• Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)\n• Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)\n• Run a web service to recognize faces via HTTP (Requires Flask to be installed)\n• Train multiple images per person then recognize faces using a SVM\n\nIf you want to create a standalone executable that can run without the need to install or , you can use PyInstaller. However, it requires some custom configuration to work with this library. See this issue for how to do it.\n• My article on how Face Recognition works: Modern Face Recognition with Deep Learning\n• Covers the algorithms and how they generally work\n• Face recognition with OpenCV, Python, and deep learning by Adrian Rosebrock\n• Covers how to use face recognition in practice\n• Raspberry Pi Face Recognition by Adrian Rosebrock\n• Covers how to use this on a Raspberry Pi\n• Face clustering with Python by Adrian Rosebrock\n• Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning\n\nIf you want to learn how face location and recognition work instead of depending on a black box library, read my article.\n• The face recognition model is trained on adults and does not work very well on children. It tends to mix up children quite easy using the default comparison threshold of 0.6.\n• Accuracy may vary between ethnic groups. Please see this wiki page for more details.\n\nSince depends on which is written in C++, it can be tricky to deploy an app using it to a cloud hosting provider like Heroku or AWS.\n\nTo make things easier, there's an example Dockerfile in this repo that shows how to run an app built with in a Docker container. With that, you should be able to deploy to any service that supports Docker images.\n\nYou can try the Docker image locally by running:\n\nThere are also several prebuilt Docker images.\n\nLinux users with a GPU (drivers >= 384.81) and Nvidia-Docker installed can run the example on the GPU: Open the docker-compose.yml file and uncomment the and lines.\n\nIf you run into problems, please read the Common Errors section of the wiki before filing a github issue.\n• Many, many thanks to Davis King (@nulhom) for creating dlib and for providing the trained facial feature detection and face encoding models used in this library. For more information on the ResNet that powers the face encodings, check out his blog post.\n• Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image, pillow, etc, etc that makes this kind of stuff so easy and fun in Python.\n• Thanks to Cookiecutter and the audreyr/cookiecutter-pypackage project template for making Python project packaging way more tolerable."
    },
    {
        "link": "https://stackoverflow.com/questions/54846337/how-to-do-face-recognition-on-images-and-determining-if-the-faces-in-it-is-known",
        "document": "Currently, I'm making a face recognition program where I have a KNOWN_FACES folder that is accessed by program to recognise known faces in an unidentified image. I test the program on a image that should return 'Unknown' on all faces, but I'm having trouble with getting rectangles on all the faces, and then saving the image in an image_output folder.\n\nAnd I have a faces.jpg in the directory, where my program is located. In my code, I'm pretty sure that there's something wrong with how I try to draw rectangles around each face and display their name right under the rectangle. (Almost at the bottom of the program.)\n\nI only have this one program, which is a modification of THIS piece of code from the maker of the face_recognition module."
    },
    {
        "link": "https://geeksforgeeks.org/create-first-gui-application-using-python-tkinter",
        "document": "We are now stepping into making applications with graphical elements, we will learn how to make cool apps and focus more on its GUI(Graphical User Interface) using Tkinter.\n\nTkinter is a Python Package for creating GUI applications. Python has a lot of GUI frameworks, but Tkinter is the only framework that’s built into the Python standard library.\n\nTkinter has several strengths; it’s cross-platform, so the same code works on Windows, macOS, and Linux.\n\nTkinter is lightweight and relatively painless to use compared to other frameworks. This makes it a compelling choice for building GUI applications in Python, especially for applications where a modern shine is unnecessary, and the top priority is to build something functional and cross-platform quickly.\n\n1. Creating windows and dialog boxes: Tkinter can be used to create windows and dialog boxes that allow users to interact with your program. These can be used to display information, gather input, or present options to the user.\n\nTo create a window or dialog box, you can use the Tk() function to create a root window, and then use functions like Label, Button, and Entry to add widgets to the window.\n\n2. Building a GUI for a desktop application: Tkinter can be used to create the interface for a desktop application, including buttons, menus, and other interactive elements.\n\nTo build a GUI for a desktop application, you can use functions like Menu, Checkbutton, and RadioButton to create menus and interactive elements and use layout managers like pack and grid to arrange the widgets on the window.\n\n3. Adding a GUI to a command-line program: Tkinter can be used to add a GUI to a command-line program, making it easier for users to interact with the program and input arguments.\n\nTo add a GUI to a command-line program, you can use functions like Entry and Button to create input fields and buttons, and use event handlers like command and bind to handle user input.\n\n4. Creating custom widgets: Tkinter includes a variety of built-in widgets, such as buttons, labels, and text boxes, but it also allows you to create your own custom widgets.\n\nTo create a custom widget, you can define a class that inherits from the Widget class and overrides its methods to define the behavior and appearance of the widget.\n\n5. Prototyping a GUI: Tkinter can be used to quickly prototype a GUI, allowing you to test and iterate on different design ideas before committing to a final implementation.\n\nTo prototype a GUI with Tkinter, you can use the Tk() function to create a root window, and then use functions like Label, Button, and Entry to add widgets to the window and test different layouts and design ideas.\n\nThere are several libraries that are similar to Tkinter and can be used for creating graphical user interfaces (GUIs) in Python. Some examples include:\n• PyQt : PyQt is a GUI library that allows you to create GUI applications using the Qt framework. It is a comprehensive library with a large number of widgets and features.\n• wxPython : wxPython is a library that allows you to create GUI applications using the wxWidgets framework. It includes a wide range of widgets in it’s GUI toolkit and is cross-platform, meaning it can run on multiple operating systems.\n• PyGTK : PyGTK is a GUI library that allows you to create GUI applications using the GTK+ framework. It is a cross-platform library with a wide range of widgets and features.\n• Kivy : Kivy is a library that allows you to create GUI applications using a modern, responsive design. It is particularly well-suited for building mobile apps and games.\n• PyForms : PyForms is a library that allows you to create GUI applications using a simple, declarative syntax. It is designed to be easy to use and has a small footprint.\n• Pygame : PyForms is a library that is popular because you can develop video games using it. It is a free, open source, and cross-platform wrapper for the Simple DirectMedia Library (SDL). You can check if you are interested in video game development.\n\nIn summary, there are several libraries available for creating GUI applications in Python, each with its own set of features and capabilities. Tkinter is a popular choice, but you may want to consider other options depending on your specific needs and requirements.\n\nTo understand Tkinter better, we will create a simple GUI.\n\n1. Import tkinter package and all of its modules.\n\n2. Create a root window. Give the root window a title(using title()) and dimension(using geometry()). All other widgets will be inside the root window. \n\n3. Use mainloop() to call the endless loop of the window. If you forget to call this nothing will appear to the user. The window will wait for any user interaction till we close it.\n\n4. We’ll add a label using the Label Class and change its text configuration as desired. The grid() function is a geometry manager which keeps the label in the desired location inside the window. If no parameters are mentioned by default it will place it in the empty cell; that is 0,0 as that is the first location.\n\n5. Now add a button to the root window. Changing the button configurations gives us a lot of options. In this example we will make the button display a text once it is clicked and also change the color of the text inside the button.\n\n6. Using the Entry() class we will create a text box for user input. To display the user input text, we’ll make changes to the function clicked(). We can get the user entered text using the get() function. When the Button after entering of the text, a default text concatenated with the user text. Also change button grid location to column 2 as Entry() will be column 1.\n\n7. To add a menu bar, you can use Menu class. First, we create a menu, then we add our first label, and finally, we assign the menu to our window. We can add menu items under any menu by using add_cascade().\n\nOutput\n\nThis simple GUI covers the basics of Tkinter package. Similarly, you can add more widgets and change their configurations as desired.\n\nTkinter is the GUI library of Python, it provides various controls, such as buttons, labels and text boxes used in a GUI application. These controls are commonly called Widgets. The list of commonly used Widgets are mentioned below –\n\nThe Label widget is used to provide a single-line caption for other widgets. It can also contain images. The Button widget is used to display buttons in your application. The Entry widget is used to display a single-line text field for accepting values from a user. The Menu widget is used to provide various commands to a user. These commands are contained inside Menubutton. The Canvas widget is used to draw shapes, such as lines, ovals, polygons and rectangles, in your application. The Checkbutton widget is used to display a number of options as checkboxes. The user can select multiple options at a time. The Frame widget is used as a container widget to organize other widgets. The Listbox widget is used to provide a list of options to a user. The Menubutton widget is used to display menus in your application. The Message widget is used to display multiline text fields for accepting values from a user. The Radiobutton widget is used to display a number of options as radio buttons. The user can select only one option at a time. The Scale widget is used to provide a slider widget. The Scrollbar widget is used to add scrolling capability to various widgets, such as list boxes. The Text widget is used to display text in multiple lines. The Toplevel widget is used to provide a separate window container. A labelframe is a simple container widget. Its primary purpose is to act as a spacer or container for complex window layouts. This module is used to display message boxes in your applications. The Spinbox widget is a variant of the standard Tkinter Entry widget, which can be used to select from a fixed number of values. A PanedWindow is a container widget that may contain any number of panes, arranged horizontally or vertically.\n\nAll Tkinter widgets have access to specific geometry management methods, which have the purpose of organizing widgets throughout the parent widget area. Tkinter exposes the following geometry manager classes: pack, grid, and place. Their description is mentioned below –\n\nIn this article, we have learned about GUI programming in Python and how to make GUI in Python. GUI is a very demanded skill so you must know how to develop GUI using Python. Hope this article helped you in creating GUI using Python.\n\nHow to Make a GUI in Python Tkinter?\n\nHow Do I Create My First Application in Python?\n\nHow to Create an App Using Tkinter?\n\nWhich GUI is Best for Python?\n\nWhat is the Simplest Python GUI?"
    },
    {
        "link": "https://docs.python.org/3/library/tkinter.html",
        "document": "The package (“Tk interface”) is the standard Python interface to the Tcl/Tk GUI toolkit. Both Tk and are available on most Unix platforms, including macOS, as well as on Windows systems.\n\nRunning from the command line should open a window demonstrating a simple Tk interface, letting you know that is properly installed on your system, and also showing what version of Tcl/Tk is installed, so you can read the Tcl/Tk documentation specific to that version.\n\nTkinter supports a range of Tcl/Tk versions, built either with or without thread support. The official Python binary release bundles Tcl/Tk 8.6 threaded. See the source code for the module for more information about supported versions.\n\nTkinter is not a thin wrapper, but adds a fair amount of its own logic to make the experience more pythonic. This documentation will concentrate on these additions and changes, and refer to the official Tcl/Tk documentation for details that are unchanged.\n\nTcl/Tk is not a single library but rather consists of a few distinct modules, each with separate functionality and its own official documentation. Python’s binary releases also ship an add-on module together with it. Tcl is a dynamic interpreted programming language, just like Python. Though it can be used on its own as a general-purpose programming language, it is most commonly embedded into C applications as a scripting engine or an interface to the Tk toolkit. The Tcl library has a C interface to create and manage one or more instances of a Tcl interpreter, run Tcl commands and scripts in those instances, and add custom commands implemented in either Tcl or C. Each interpreter has an event queue, and there are facilities to send events to it and process them. Unlike Python, Tcl’s execution model is designed around cooperative multitasking, and Tkinter bridges this difference (see Threading model for details). Tk is a Tcl package implemented in C that adds custom commands to create and manipulate GUI widgets. Each object embeds its own Tcl interpreter instance with Tk loaded into it. Tk’s widgets are very customizable, though at the cost of a dated appearance. Tk uses Tcl’s event queue to generate and process GUI events. Themed Tk (Ttk) is a newer family of Tk widgets that provide a much better appearance on different platforms than many of the classic Tk widgets. Ttk is distributed as part of Tk, starting with Tk version 8.5. Python bindings are provided in a separate module, . Internally, Tk and Ttk use facilities of the underlying operating system, i.e., Xlib on Unix/X11, Cocoa on macOS, GDI on Windows. When your Python application uses a class in Tkinter, e.g., to create a widget, the module first assembles a Tcl/Tk command string. It passes that Tcl command string to an internal binary module, which then calls the Tcl interpreter to evaluate it. The Tcl interpreter will then call into the Tk and/or Ttk packages, which will in turn make calls to Xlib, Cocoa, or GDI.\n\nSupport for Tkinter is spread across several modules. Most applications will need the main module, as well as the module, which provides the modern themed widget set and API: Construct a toplevel Tk widget, which is usually the main window of an application, and initialize a Tcl interpreter for this widget. Each instance has its own associated Tcl interpreter. The class is typically instantiated using all default values. However, the following keyword arguments are currently recognized: When given (as a string), sets the environment variable. (X11 only) Name of the profile file. By default, baseName is derived from the program name ( ). Name of the widget class. Used as a profile file and also as the name with which Tcl is invoked (argv0 in interp). If , initialize the Tk subsystem. The function sets this to . If , execute all X server commands synchronously, so that errors are reported immediately. Can be used for debugging. (X11 only) Specifies the id of the window in which to embed the application, instead of it being created as an independent toplevel window. id must be specified in the same way as the value for the -use option for toplevel widgets (that is, it has a form like that returned by ). Note that on some platforms this will only work correctly if id refers to a Tk frame or toplevel that has its -container option enabled. reads and interprets profile files, named and , into the Tcl interpreter and calls on the contents of and . The path for the profile files is the environment variable or, if that isn’t defined, then . The Tk application object created by instantiating . This provides access to the Tcl interpreter. Each widget that is attached the same instance of has the same value for its attribute. The widget object that contains this widget. For , the master is because it is the main window. The terms master and parent are similar and sometimes used interchangeably as argument names; however, calling returns a string of the widget name whereas returns the object. parent/child reflects the tree-like relationship while master/slave reflects the container structure. The immediate descendants of this widget as a with the child widget names as the keys and the child instance objects as the values. The function is a factory function which creates an object much like that created by the class, except that it does not initialize the Tk subsystem. This is most often useful when driving the Tcl interpreter in an environment where one doesn’t want to create extraneous toplevel windows, or where one cannot (such as Unix/Linux systems without an X server). An object created by the object can have a Toplevel window created (and the Tk subsystem initialized) by calling its method. Dialog to let the user choose a color. Base class for the dialogs defined in the other modules listed here. Common dialogs to allow the user to specify a file to open or save. Utilities to help work with fonts. Themed widget set introduced in Tk 8.5, providing modern alternatives for many of the classic widgets in the main module. A binary module that contains the low-level interface to Tcl/Tk. It is automatically imported by the main module, and should never be used directly by application programmers. It is usually a shared library (or DLL), but might in some cases be statically linked with the Python interpreter. Symbolic constants that can be used in place of strings when passing various parameters to Tkinter calls. Automatically imported by the main module. (experimental) Drag-and-drop support for . This will become deprecated when it is replaced with the Tk DND.\n\nThis section is not designed to be an exhaustive tutorial on either Tk or Tkinter. For that, refer to one of the external resources noted earlier. Instead, this section provides a very quick orientation to what a Tkinter application looks like, identifies foundational Tk concepts, and explains how the Tkinter wrapper is structured. The remainder of this section will help you to identify the classes, methods, and options you’ll need in your Tkinter application, and where to find more detailed documentation on them, including in the official Tcl/Tk reference manual. We’ll start by walking through a “Hello World” application in Tkinter. This isn’t the smallest one we could write, but has enough to illustrate some key concepts you’ll need to know. After the imports, the next line creates an instance of the class, which initializes Tk and creates its associated Tcl interpreter. It also creates a toplevel window, known as the root window, which serves as the main window of the application. The following line creates a frame widget, which in this case will contain a label and a button we’ll create next. The frame is fit inside the root window. The next line creates a label widget holding a static text string. The method is used to specify the relative layout (position) of the label within its containing frame widget, similar to how tables in HTML work. A button widget is then created, and placed to the right of the label. When pressed, it will call the method of the root window. Finally, the method puts everything on the display, and responds to user input until the program terminates. Even this simple program illustrates the following key Tk concepts: A Tkinter user interface is made up of individual widgets. Each widget is represented as a Python object, instantiated from classes like , , and . Widgets are arranged in a hierarchy. The label and button were contained within a frame, which in turn was contained within the root window. When creating each child widget, its parent widget is passed as the first argument to the widget constructor. Widgets have configuration options, which modify their appearance and behavior, such as the text to display in a label or button. Different classes of widgets will have different sets of options. Widgets aren’t automatically added to the user interface when they are created. A geometry manager like controls where in the user interface they are placed. Tkinter reacts to user input, changes from your program, and even refreshes the display only when actively running an event loop. If your program isn’t running the event loop, your user interface won’t update. When your application uses Tkinter’s classes and methods, internally Tkinter is assembling strings representing Tcl/Tk commands, and executing those commands in the Tcl interpreter attached to your application’s instance. Whether it’s trying to navigate reference documentation, trying to find the right method or option, adapting some existing code, or debugging your Tkinter application, there are times that it will be useful to understand what those underlying Tcl/Tk commands look like. To illustrate, here is the Tcl/Tk equivalent of the main part of the Tkinter script above. Tcl’s syntax is similar to many shell languages, where the first word is the command to be executed, with arguments to that command following it, separated by spaces. Without getting into too many details, notice the following:\n• None The commands used to create widgets (like ) correspond to widget classes in Tkinter.\n• None Tcl widget options (like ) correspond to keyword arguments in Tkinter.\n• None Widgets are referred to by a pathname in Tcl (like ), whereas Tkinter doesn’t use names but object references.\n• None A widget’s place in the widget hierarchy is encoded in its (hierarchical) pathname, which uses a (dot) as a path separator. The pathname for the root window is just (dot). In Tkinter, the hierarchy is defined not by pathname but by specifying the parent widget when creating each child widget.\n• None Operations which are implemented as separate commands in Tcl (like or ) are represented as methods on Tkinter widget objects. As you’ll see shortly, at other times Tcl uses what appear to be method calls on widget objects, which more closely mirror what would is used in Tkinter. How do I…? What option does…?¶ If you’re not sure how to do something in Tkinter, and you can’t immediately find it in the tutorial or reference documentation you’re using, there are a few strategies that can be helpful. First, remember that the details of how individual widgets work may vary across different versions of both Tkinter and Tcl/Tk. If you’re searching documentation, make sure it corresponds to the Python and Tcl/Tk versions installed on your system. When searching for how to use an API, it helps to know the exact name of the class, option, or method that you’re using. Introspection, either in an interactive Python shell or with , can help you identify what you need. To find out what configuration options are available on any widget, call its method, which returns a dictionary containing a variety of information about each object, including its default and current values. Use to get just the names of each option. As most widgets have many configuration options in common, it can be useful to find out which are specific to a particular widget class. Comparing the list of options to that of a simpler widget, like a frame, is one way to do that. Similarly, you can find the available methods for a widget object using the standard function. If you try it, you’ll see there are over 200 common widget methods, so again identifying those specific to a widget class is helpful. As noted, the official Tk commands reference manual (man pages) is often the most accurate description of what specific operations on widgets do. Even when you know the name of the option or method that you need, you may still have a few places to look. While all operations in Tkinter are implemented as method calls on widget objects, you’ve seen that many Tcl/Tk operations appear as commands that take a widget pathname as its first parameter, followed by optional parameters, e.g. Others, however, look more like methods called on a widget object (in fact, when you create a widget in Tcl/Tk, it creates a Tcl command with the name of the widget pathname, with the first parameter to that command being the name of a method to call). In the official Tcl/Tk reference documentation, you’ll find most operations that look like method calls on the man page for a specific widget (e.g., you’ll find the method on the ttk::button man page), while functions that take a widget as a parameter often have their own man page (e.g., grid). You’ll find many common options and methods in the options or ttk::widget man pages, while others are found in the man page for a specific widget class. You’ll also find that many Tkinter methods have compound names, e.g., , , . You’d find documentation for all of these in the winfo man page. Somewhat confusingly, there are also methods on all Tkinter widgets that don’t actually operate on the widget, but operate at a global scope, independent of any widget. Examples are methods for accessing the clipboard or the system bell. (They happen to be implemented as methods in the base class that all Tkinter widgets inherit from).\n\nPython and Tcl/Tk have very different threading models, which tries to bridge. If you use threads, you may need to be aware of this. A Python interpreter may have many threads associated with it. In Tcl, multiple threads can be created, but each thread has a separate Tcl interpreter instance associated with it. Threads can also create more than one interpreter instance, though each interpreter instance can be used only by the one thread that created it. Each object created by contains a Tcl interpreter. It also keeps track of which thread created that interpreter. Calls to can be made from any Python thread. Internally, if a call comes from a thread other than the one that created the object, an event is posted to the interpreter’s event queue, and when executed, the result is returned to the calling Python thread. Tcl/Tk applications are normally event-driven, meaning that after initialization, the interpreter runs an event loop (i.e. ) and responds to events. Because it is single-threaded, event handlers must respond quickly, otherwise they will block other events from being processed. To avoid this, any long-running computations should not run in an event handler, but are either broken into smaller pieces using timers, or run in another thread. This is different from many GUI toolkits where the GUI runs in a completely separate thread from all application code including event handlers. If the Tcl interpreter is not running the event loop and processing events, any calls made from threads other than the one running the Tcl interpreter will fail.\n• None Tcl/Tk libraries can be built so they are not thread-aware. In this case, calls the library from the originating Python thread, even if this is different than the thread that created the Tcl interpreter. A global lock ensures only one call occurs at a time.\n• None While allows you to create more than one instance of a object (with its own interpreter), all interpreters that are part of the same thread share a common event queue, which gets ugly fast. In practice, don’t create more than one instance of at a time. Otherwise, it’s best to create them in separate threads and ensure you’re running a thread-aware Tcl/Tk build.\n• None Blocking event handlers are not the only way to prevent the Tcl interpreter from reentering the event loop. It is even possible to run multiple nested event loops or abandon the event loop entirely. If you’re doing anything tricky when it comes to events or threads, be aware of these possibilities.\n• None There are a few select functions that presently work only when called from the thread that created the Tcl interpreter.\n\nOptions control things like the color and border width of a widget. Options can be set in three ways: After object creation, treating the option name like a dictionary index Use the config() method to update multiple attrs subsequent to object creation For a complete explanation of a given option and its behavior, see the Tk man pages for the widget in question. Note that the man pages list “STANDARD OPTIONS” and “WIDGET SPECIFIC OPTIONS” for each widget. The former is a list of options that are common to many widgets, the latter are the options that are idiosyncratic to that particular widget. The Standard Options are documented on the options(3) man page. No distinction between standard and widget-specific options is made in this document. Some options don’t apply to some kinds of widgets. Whether a given widget responds to a particular option depends on the class of the widget; buttons have a option, labels do not. The options supported by a given widget are listed in that widget’s man page, or can be queried at runtime by calling the method without arguments, or by calling the method on that widget. The return value of these calls is a dictionary whose key is the name of the option as a string (for example, ) and whose values are 5-tuples. Some options, like are synonyms for common options with long names ( is shorthand for “background”). Passing the method the name of a shorthand option will return a 2-tuple, not 5-tuple. The 2-tuple passed back will contain the name of the synonym and the “real” option (such as ). Of course, the dictionary printed will include all the options available and their values. This is meant only as an example. The packer is one of Tk’s geometry-management mechanisms. Geometry managers are used to specify the relative positioning of widgets within their container - their mutual master. In contrast to the more cumbersome placer (which is used less commonly, and we do not cover here), the packer takes qualitative relationship specification - above, to the left of, filling, etc - and works everything out to determine the exact placement coordinates for you. The size of any master widget is determined by the size of the “slave widgets” inside. The packer is used to control where slave widgets appear inside the master into which they are packed. You can pack widgets into frames, and frames into other frames, in order to achieve the kind of layout you desire. Additionally, the arrangement is dynamically adjusted to accommodate incremental changes to the configuration, once it is packed. Note that widgets do not appear until they have had their geometry specified with a geometry manager. It’s a common early mistake to leave out the geometry specification, and then be surprised when the widget is created but nothing appears. A widget will appear only after it has had, for example, the packer’s method applied to it. The pack() method can be called with keyword-option/value pairs that control where the widget is to appear within its container, and how it is to behave when the main application window is resized. Here are some examples: For more extensive information on the packer and the options that it can take, see the man pages and page 183 of John Ousterhout’s book. Anchor type. Denotes where the packer is to place each slave in its parcel. A distance - designating internal padding on each side of the slave widget. A distance - designating external padding on each side of the slave widget. The current-value setting of some widgets (like text entry widgets) can be connected directly to application variables by using special options. These options are , , , , and . This connection works both ways: if the variable changes for any reason, the widget it’s connected to will be updated to reflect the new value. Unfortunately, in the current implementation of it is not possible to hand over an arbitrary Python variable to a widget through a or option. The only kinds of variables for which this works are variables that are subclassed from a class called Variable, defined in . There are many useful subclasses of Variable already defined: , , , and . To read the current value of such a variable, call the method on it, and to change its value you call the method. If you follow this protocol, the widget will always track the value of the variable, with no further intervention on your part. # Tell the entry widget to watch this variable. # Define a callback for when the user hits return. # It prints the current value of the variable. \"Hi. The current entry content is:\" In Tk, there is a utility command, , for interacting with the window manager. Options to the command allow you to control things like titles, placement, icon bitmaps, and the like. In , these commands have been implemented as methods on the class. Toplevel widgets are subclassed from the class, and so can call the methods directly. To get at the toplevel window that contains a given widget, you can often just refer to the widget’s master. Of course if the widget has been packed inside of a frame, the master won’t represent a toplevel window. To get at the toplevel window that contains an arbitrary widget, you can call the method. This method begins with an underscore to denote the fact that this function is part of the implementation, and not an interface to Tk functionality. Here are some examples of typical usage: # here are method calls to the window manager class Legal values are points of the compass: , , , , , , , , and also . There are eight built-in, named bitmaps: , , , , , , , . To specify an X bitmap filename, give the full path to the file, preceded with an , as in . You can pass integers 0 or 1 or the strings or . This is any Python function that takes no arguments. For example: Colors can be given as the names of X colors in the rgb.txt file, or as strings representing RGB values in 4 bit: , 8 bit: , 12 bit: , or 16 bit: ranges, where R,G,B here represent any legal hex digit. See page 160 of Ousterhout’s book for details. The standard X cursor names from can be used, without the prefix. For example to get a hand cursor ( ), use the string . You can also specify a bitmap and mask file of your own. See page 179 of Ousterhout’s book. Screen distances can be specified in either pixels or absolute distances. Pixels are given as numbers and absolute distances as strings, with the trailing character denoting units: for centimetres, for inches, for millimetres, for printer’s points. For example, 3.5 inches is expressed as . Tk uses a list font name format, such as . Font sizes with positive numbers are measured in points; sizes with negative numbers are measured in pixels. This is a string of the form , where width and height are measured in pixels for most widgets (in characters for widgets displaying text). For example: . Legal values are the strings: , , , and . This is a string with four space-delimited elements, each of which is a legal distance (see above). For example: and and are all legal regions. Determines what the border style of a widget will be. Legal values are: , , , , and . This is almost always the method of some scrollbar widget, but can be any widget method that takes a single argument. Must be one of: , , or . The bind method from the widget command allows you to watch for certain events and to have a callback function trigger when that event type occurs. The form of the bind method is: is a string that denotes the target kind of event. (See the bind(3tk) man page, and page 201 of John Ousterhout’s book, , for details). is a Python function, taking one argument, to be invoked when the event occurs. An Event instance will be passed as the argument. (Functions deployed this way are commonly known as callbacks.) is optional, either or . Passing an empty string denotes that this binding is to replace any other bindings that this event is associated with. Passing a means that this function is to be added to the list of functions bound to this event type. Notice how the widget field of the event is being accessed in the callback. This field contains the widget that caught the X event. The following table lists the other event fields you can access, and how they are denoted in Tk, which can be useful when referring to the Tk man pages. A number of widgets require “index” parameters to be passed. These are used to point at a specific place in a Text widget, or to particular characters in an Entry widget, or to particular menu items in a Menu widget. Entry widgets have options that refer to character positions in the text being displayed. You can use these functions to access these special points in text widgets: The index notation for Text widgets is very rich and is best described in the Tk man pages. Some options and methods for menus manipulate specific menu entries. Anytime a menu index is needed for an option or a parameter, you may pass in:\n• None an integer which refers to the numeric position of the entry in the widget, counted from the top, starting with 0;\n• None the string , which refers to the menu position that is currently under the cursor;\n• None the string which refers to the last menu item;\n• None An integer preceded by , as in , where the integer is interpreted as a y pixel coordinate in the menu’s coordinate system;\n• None the string , which indicates no menu entry at all, most often used with menu.activate() to deactivate all entries, and finally,\n• None a text string that is pattern matched against the label of the menu entry, as scanned from the top of the menu to the bottom. Note that this index type is considered after all the others, which means that matches for menu items labelled , , or may be interpreted as the above literals, instead. Images of different formats can be created through the corresponding subclass of :\n• None for images in PGM, PPM, GIF and PNG formats. The latter is supported starting with Tk 8.6. Either type of image is created through either the or the option (other options are available as well). Changed in version 3.13: Added the method to copy a region from one image to other image, possibly with pixel zooming and/or subsampling. Add from_coords parameter to methods , and . Add zoom and subsample parameters to method . The image object can then be used wherever an option is supported by some widget (e.g. labels, buttons, menus). In these cases, Tk will not keep a reference to the image. When the last Python reference to the image object is deleted, the image data is deleted as well, and Tk will display an empty box wherever the image was used. The Pillow package adds support for formats such as BMP, JPEG, TIFF, and WebP, among others."
    },
    {
        "link": "https://stackoverflow.com/questions/78872682/creating-a-toplevel-window-using-tkinterdnd2",
        "document": "I'm using for my python GUI. I'm quite familiar with and, if I want to create a Toplevel window with another windoe, as a parent, then I'd use the code\n\nHowever, if I try using for the same task\n\nThe documentation doesn't help that much on this regard and I was hoping that perhaps someone here knew how to create a toplevel window using (or if it was possible). I'm trying to create a window (different from my main window), that has drag and drop capabilities."
    },
    {
        "link": "https://geeksforgeeks.org/python-tkinter-toplevel-widget",
        "document": "Tkinter is a GUI toolkit used in python to make user-friendly GUIs.Tkinter is the most commonly used and the most basic GUI framework available in Python. Tkinter uses an object-oriented approach to make GUIs.\n\nNote: For more information, refer to Python GUI – tkinter\n\nA Toplevel widget is used to create a window on top of all other windows. The Toplevel widget is used to provide some extra information to the user and also when our program deals with more than one application. These windows are directly organized and managed by the Window Manager and do not need to have any parent window associated with them every time.\n• cursor = cursor that appears on the widget which can be an arrow, a dot etc.\n• withdraw removes the window from the screen.\n\nExample 2: Creating Multiple toplevels over one another"
    },
    {
        "link": "https://stackoverflow.com/questions/68417979/how-do-i-use-multiple-windows-in-python-tkinter",
        "document": "You can use to create new window in tkinter.\n\nMore information is available here\n\nNote: if you destroy the main all of the attached to that main window will also be destroyed."
    }
]