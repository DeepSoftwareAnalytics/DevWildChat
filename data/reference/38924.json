[
    {
        "link": "https://docs.opencv.org/3.4/d8/dfe/classcv_1_1VideoCapture.html",
        "document": ""
    },
    {
        "link": "https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html",
        "document": "\n• Learn to capture video from a camera and display it.\n• You will learn these functions : cv.VideoCapture(), cv.VideoWriter()\n\nOften, we have to capture live stream with a camera. OpenCV provides a very simple interface to do this. Let's capture a video from the camera (I am using the built-in webcam on my laptop), convert it into grayscale video and display it. Just a simple task to get started.\n\nTo capture a video, you need to create a VideoCapture object. Its argument can be either the device index or the name of a video file. A device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). You can select the second camera by passing 1 and so on. After that, you can capture frame-by-frame. But at the end, don't forget to release the capture.\n\nreturns a bool ( / ). If the frame is read correctly, it will be . So you can check for the end of the video by checking this returned value.\n\nSometimes, cap may not have initialized the capture. In that case, this code shows an error. You can check whether it is initialized or not by the method cap.isOpened(). If it is , OK. Otherwise open it using cap.open().\n\nYou can also access some of the features of this video using cap.get(propId) method where propId is a number from 0 to 18. Each number denotes a property of the video (if it is applicable to that video). Full details can be seen here: cv::VideoCapture::get(). Some of these values can be modified using cap.set(propId, value). Value is the new value you want.\n\nFor example, I can check the frame width and height by and . It gives me 640x480 by default. But I want to modify it to 320x240. Just use and .\n\nPlaying video from file is the same as capturing it from camera, just change the camera index to a video file name. Also while displaying the frame, use appropriate time for . If it is too less, video will be very fast and if it is too high, video will be slow (Well, that is how you can display videos in slow motion). 25 milliseconds will be OK in normal cases.\n\nSo we capture a video and process it frame-by-frame, and we want to save that video. For images, it is very simple: just use . Here, a little more work is required.\n\nThis time we create a VideoWriter object. We should specify the output file name (eg: output.avi). Then we should specify the FourCC code (details in next paragraph). Then number of frames per second (fps) and frame size should be passed. And the last one is the isColor flag. If it is , the encoder expect color frame, otherwise it works with grayscale frame.\n\nFourCC is a 4-byte code used to specify the video codec. The list of available codes can be found in fourcc.org. It is platform dependent. The following codecs work fine for me.\n• In Fedora: DIVX, XVID, MJPG, X264, WMV1, WMV2. (XVID is more preferable. MJPG results in high size video. X264 gives very small size video)\n• In Windows: DIVX (More to be tested and added)\n\nFourCC code is passed as ‘cv.VideoWriter_fourcc('M’,'J','P','G') cv.VideoWriter_fourcc(*'MJPG')` for MJPG.\n\nThe below code captures from a camera, flips every frame in the vertical direction, and saves the video."
    },
    {
        "link": "https://geeksforgeeks.org/python-opencv-capture-video-from-camera",
        "document": "Python provides various libraries for image and video processing. One of them is OpenCV. OpenCV is a vast library that helps in providing various functions for image and video operations. With OpenCV, we can capture a video from the camera. It lets you create a video capture object which is helpful to capture videos through webcam and then you may perform desired operations on that video.\n• None ) to create a video capture object for the camera.\n• None Create a VideoWriter object to save captured frames as a video in the computer.\n• None Set up an infinite while loop and use the method to read the frames using the above created object.\n• None method to show the frames in the video.\n• None Breaks the loop when the user clicks a specific key.\n\nBelow is the implementation.\n\nFirst, we import the OpenCV library for python i.e. cv2. We use the VideoCapture() class of OpenCV to create the camera object which is used to capture photos or videos. The constructor of this class takes an integer argument which denotes the hardware camera index. Suppose the devices has two cameras, then we can capture photos from first camera by using VideoCapture(0) and if we want to use second camera, we should use VideoCapture(1).\n\nThe dimensions of the frame and the recorded video must be specified. For this purpose, we use the camera object method which is cam.get() to get cv2.CAP_PROP_FRAME_WIDTH and cv2.CAP_PROP_FRAME_HEIGHT which are the fields of cv2 package. The default values of these fields depend upon the actual hardware camera and drivers which are used.\n\nWe are creating a VideoWriter object to write the captured frames to a video file which is stored in the local machine. The constructor of this class takes the following parameters:\n• Filepath/Filename: The file would be saved by specified name and at the specified path. If we mention only the file name, then the video file would be saved at the same folder where the python script for capturing video is running.\n• Four Character Code: A four-letter word to denote the specific encoding and decoding technique or format which must be used in order to create the video. Some popular codecs include ‘MJPG’ = Motion JPEG codec, ‘X264’ = H.264 codec, ‘MP4V’ =\n• Frame rate: Frame rate determines the number of frames or individual images which are shown in a second. Higher frame rates produce smoother video, but it will also take more storage space as compared to lower frame rates.\n• Video Dimension: The height and width of the video which are passed as a tuple.\n\nInside an infinite while loop, we use the read() method of the camera object which returns two values, the first value is boolean and describes whether the frame was successfully captured or not. The second value is a 3D numpy array which represents the actual photo capture. In order to represent an image, we need two-dimensional data structure and for representing color information, we must add another dimension to it making it three dimensional.\n\nThen, we are writing the individual frames to the video file using the VideoWriter object which was previously created using the write() method which takes the captured frame as the argument. To display the captured frame, we are using cv2.imshow() function, which takes window name and the captured frame as arguments. This process is repeated infinitely to produce continuous video stream which consists of these frames.\n\nIn order to exit the application, we have break out of this infinite while loop. Hence we are assigning a key which can be used to quit the application. Here the cv2.waitKey() function takes and integer argument which denotes the number of milliseconds the program waits for to check whether the user has pressed any button. We can use this function to adjust the frame rate of the captured video by providing certain delay. We have assigned “q” as the quit button and we are checking every millisecond whether it is pressed in order to break out of the while loop. The ord() function returns the ASCII value of the character which is passed to it as an argument."
    },
    {
        "link": "https://note.nkmk.me/en/python-opencv-videocapture-file-camera",
        "document": "In Python, you can read, capture, and display video files and camera stream using the class with OpenCV. Note that must be enabled in OpenCV to process video.\n\nThis article describes the following contents.\n• Get and set video properties (size, FPS, number of frames, etc.)\n\nThe tutorial on video in the official documentation is below.\n\nThe OpenCV version of the sample code is .\n\nSee the following article for information on reading and writing still images, not videos.\n\nmust be enabled to handle video in OpenCV. If an error occurs and the video cannot be loaded, you should first check to see if is enabled.\n\nYou can check OpenCV build information with .\n\nHow to build with enabled varies depending on the environment, and is not discussed here.\n\nFor macOS, if you install OpenCV with Homebrew, ffmpeg will be installed at the same time and will be enabled (as of January 27, 2019).\n\nTo read a video file, specify the path to the file in . It can be an absolute path or a relative path.\n\nYou can check if it is successfully read with the method. If there is no problem, is returned.\n\nNote that does not raise an error if a wrong path is specified. You should check with .\n\nTo capture video from a PC's built-in camera or a USB camera/webcam, specify the device index in .\n\nIn most cases, the device index is assigned from , such as for the built-in camera and for additional cameras connected via USB.\n\nAccording to the tutorial in the official documentation, there are cases where it is .\n\nAnyway, you can try them in order.\n\nAn API to get a list of available cameras is not yet provided (as of January 27, 2019).\n\nYou can check with the method to see if the camera is successfully specified, just as you would with a video file.\n\nNote that, as with video files, does not raise an error if an incorrect device index is specified.\n\nYou can close video files and camera devices with the method.\n\nGet and set video properties (size, FPS, number of frames, etc.)\n\nYou can get the properties of the opened video with the method of the object.\n\nThe following sample video is used as an example. Sorry for the Japanese site, but if you check the checkbox in the lower right corner, you can download the file from the blue button.\n\nSpecify the property as an argument of the method.\n\nFor example, you can get the size (width and height), FPS (frames per second), and total number of frames as follows:\n\nYou can calculate the playback time of the video in seconds with .\n\nSee the official documentation for a list of property identifiers.\n\nNote that if you use OpenCV2, you need to prefix it with .\n\nThe example above uses a video file, but the same is true for a camera stream. However, there are some differences, such as the total number of frames is not set (always ) because the camera is real-time video.\n\nSome properties can be set to a value with the method. Specify the property as the first argument and the value as the second argument.\n\nreturns or . If it cannot be changed, is returned. In this case, the value remains the same.\n\nIf the value can be changed, is returned.\n\nproperty represents the current position of the video. Details are described below.\n\nNote that even if returns , the value may not be changed. For example, is used to change the FPS of a camera, but if the camera itself does not support that FPS, the value may not be changed even if returns .\n\nYou can get the frames of the video as a NumPy array with the method of the object.\n\nFor video files, you need to consider the current position.\n\nYou can get the current number of frames and the elapsed time in milliseconds with the method. Both are when the file is opened.\n\nreturns a tuple consisting of indicating if the frame was successfully read or not and , the array of the image. You can assign to each variable with unpacking.\n\nIf you want to save a frame image or do something, you can do it to this .\n\nWhen the method is executed, the current position is advanced by one frame. The elapsed time of one frame is equal to the reciprocal of FPS.\n\nYou can move the current position to any frame with the method.\n\nIf is called here, the current position is advanced by one frame.\n\nIf the current position is set to the total number of frames, returns and since no more frames exist. In this case, the current position does not advance and remains at the end.\n\nIf a value greater than the total number of frames is set, the current position is still at the end.\n\nIn a live stream from a camera, new frames are constantly input. Unlike files, the camera has no current position, and always returns .\n\nWhen is called, the image of the newly input frame is read.\n\nOpenCV does not provide any method to play (display) videos.\n\nYou can display images loaded with sequentially with . Since this is only a display of images, audio is ignored.\n\nAn example of repeated playback of a video file is as follows. Press to exit.\n\nUsing an infinite loop with a statement, the image will continue to be read and displayed until on the keyboard is pressed.\n• while loop in Python (infinite loop, etc.)\n\nWhen the video is played to the end, the method sets the current position to (the start position of the video).\n\nIf you want to play the video only once without looping, set after .\n\nstops operation for a specified time (in milliseconds) and waits for keyboard input. If is set to , it waits until keyboard input, so the displayed image is not updated until some key is pressed. The image is updated when a key other than is pressed.\n\ncloses the window with the specified name. , which closes all windows, is also provided.\n\nThe method is automatically called in the destructor of the object, so it is omitted here. If the code continues after this, it may be safer to close it explicitly with .\n\nAn example of real-time video playback from a camera is as follows. Press to exit.\n\nIf the camera is unstable, some frames cannot be read by and may be returned. In such a case, use to check if the frame is read correctly.\n\nTo process images from a camera in real-time, you can add operations between and .\n\nAn example of grayscale conversion and blurring processing is as follows."
    },
    {
        "link": "https://geeksforgeeks.org/opencv-python-tutorial",
        "document": ""
    },
    {
        "link": "https://github.com/timesler/facenet-pytorch",
        "document": "You can also read a translated version of this file in Chinese 简体中文版.\n\nThis is a repository for Inception Resnet (V1) models in pytorch, pretrained on VGGFace2 and CASIA-Webface.\n\nPytorch model weights were initialized using parameters ported from David Sandberg's tensorflow facenet repo.\n\nAlso included in this repo is an efficient pytorch implementation of MTCNN for face detection prior to inference. These models are also pretrained. To our knowledge, this is the fastest MTCNN implementation available.\n• Use this repo in your own git project\n• Conversion of parameters from Tensorflow to Pytorch\n• pip install facenet-pytorch or clone this repo, removing the '-' to allow python imports: git clone https://github.com/timesler/facenet-pytorch.git facenet_pytorch or use a docker container (see https://github.com/timesler/docker-jupyter-dl-gpu): docker run -it --rm timesler/jupyter-dl-gpu pip install facenet-pytorch ipython\n• . ( ) ( , ) ( . ( )) # Or, if using for VGGFace2 classification . ( . ( ))\n\nSee and for usage and implementation details.\n\nThe following models have been ported to pytorch (with links to download pytorch state_dict's):\n\nThere is no need to manually download the pretrained state_dict's; they are downloaded automatically on model instantiation and cached for future use in the torch cache. To use an Inception Resnet (V1) model for facial recognition/identification in pytorch, use:\n\nBoth pretrained models were trained on 160x160 px images, so will perform best if applied to images resized to this shape. For best results, images should also be cropped to the face using MTCNN (see below).\n\nBy default, the above models will return 512-dimensional embeddings of images. To enable classification instead, either pass to the model constructor, or you can set the object attribute afterwards with . For VGGFace2, the pretrained model will output logit vectors of length 8631, and for CASIA-Webface logit vectors of length 10575.\n\nFace recognition can be easily applied to raw images by first detecting faces using MTCNN before calculating embedding or probabilities using an Inception Resnet model. The example code at examples/infer.ipynb provides a complete example pipeline utilizing datasets, dataloaders, and optional GPU processing.\n\nMTCNN can be used to build a face tracking system (using the method). A full face tracking example can be found at examples/face_tracking.ipynb.\n\nIn most situations, the best way to implement face recognition is to use the pretrained models directly, with either a clustering algorithm or a simple distance metrics to determine the identity of a face. However, if finetuning is required (i.e., if you want to select identity based on the model's output logits), an example can be found at examples/finetune.ipynb.\n\nThis guide demonstrates the functionality of the MTCNN module. Topics covered are:\n\nSee the notebook on kaggle.\n\nThis notebook demonstrates the use of three face detection packages:\n\nEach package is tested for its speed in detecting the faces in a set of 300 images (all frames from one video), with GPU support enabled. Performance is based on Kaggle's P100 notebook kernel. Results are summarized below.\n\nSee the notebook on kaggle.\n\nThis algorithm demonstrates how to achieve extremely efficient face detection specifically in videos, by taking advantage of similarities between adjacent frames.\n\nSee the notebook on kaggle.\n\nThe package and any of the example notebooks can be run with docker (or nvidia-docker) using:\n\nNavigate to the examples/ directory and run any of the ipython notebooks.\n\nTo use this code in your own git repo, I recommend first adding this repo as a submodule. Note that the dash ('-') in the repo name should be removed when cloning as a submodule as it will break python when importing:\n\nAlternatively, the code can be installed as a package using pip:\n\nNote that this functionality is not needed to use the models in this repo, which depend only on the saved pytorch 's.\n\nFollowing instantiation of the pytorch model, each layer's weights were loaded from equivalent layers in the pretrained tensorflow models from davidsandberg/facenet.\n\nThe equivalence of the outputs from the original tensorflow models and the pytorch-ported models have been tested and are identical:\n\nIn order to re-run the conversion of tensorflow parameters into the pytorch model, ensure you clone this repo with submodules, as the davidsandberg/facenet repo is included as a submodule and parts of it are required for the conversion.\n• Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman. VGGFace2: A dataset for recognising face across pose and age, International Conference on Automatic Face and Gesture Recognition, 2018. PDF\n• K. Zhang, Z. Zhang, Z. Li and Y. Qiao. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks, IEEE Signal Processing Letters, 2016. PDF"
    },
    {
        "link": "https://medium.com/@danushidk507/facenet-pytorch-pretrained-pytorch-face-detection-mtcnn-and-facial-recognition-b20af8771144",
        "document": "Facenet-Pytorch\n\nFaceNet is a deep learning model for face recognition that was introduced by Google researchers in a paper titled “FaceNet: A Unified Embedding for Face Recognition and Clustering” by Schroff et al. The “facenet_pytorch” library is a PyTorch implementation of the FaceNet model, which allows you to utilize FaceNet for face recognition tasks in your own projects.\n\nFaceNet is a deep convolutional neural network (CNN) architecture designed for face recognition tasks. It learns to map facial images into a high-dimensional feature space where the Euclidean distance between face embeddings corresponds to their similarity. FaceNet uses a triplet loss function during training to ensure that the embeddings of the same person’s face are close in the feature space, while embeddings of different people are far apart.\n\nfacenet_pytorch is a Python library that provides a PyTorch implementation of the FaceNet model, making it easy to use FaceNet for face recognition tasks in PyTorch-based projects.\n• Face Recognition: The primary use case of FaceNet and facenet_pytorch is face recognition. You can use this library to recognize and verify faces in images or videos.\n• Face Clustering: FaceNet embeddings can also be used for clustering faces. Similar faces will have embeddings that are close to each other, making it possible to group similar faces together.\n• Face Verification: You can use FaceNet embeddings for face verification tasks, where you want to determine if two face images belong to the same person.\n• The MTCNN (Multi-task Cascaded Convolutional Networks) is used for face detection.\n• The InceptionResnetV1 model is loaded for extracting face embeddings.\n• The code detects faces in an input image, aligns them, and then computes embeddings for each face.\n• These embeddings can be used to compare and recognize faces.\n\nMTCNN is a deep learning architecture used for face detection. It’s known for its ability to efficiently detect faces in images with varying scales and angles. MTCNN is a cascaded network that consists of three stages: face detection, facial landmark localization, and face alignment.\n\nIt has three stages (P-Net, R-Net, and O-Net) that work together to detect faces and facial landmarks in an image. Each stage has a specific role in the face detection process, and they progressively refine the results.\n\nThe P-Net is the first stage of MTCNN and is responsible for proposing potential face regions in the input image.\n• It consists of a series of convolutional layers that scan the input image at multiple scales to identify potential face candidates.\n• P-Net generates bounding box proposals and scores for each candidate face region, indicating how likely it is to contain a face.\n• It applies non-maximum suppression (NMS) to filter out overlapping bounding boxes and selects the most confident face candidates.\n\nThe output of the P-Net is a list of bounding boxes and corresponding confidence scores for potential faces.\n\nThe R-Net is the second stage of MTCNN and is responsible for refining the face candidates proposed by the P-Net.\n• It takes the bounding box candidates produced by the P-Net and further evaluates them to reduce false positives.\n• R-Net applies more sophisticated feature extraction and classification to accurately classify each bounding box as containing a face or not.\n• It also regresses the bounding box coordinates to improve the accuracy of the bounding boxes.\n\nThe output of the R-Net is a refined list of bounding boxes, confidence scores, and refined coordinates.\n\nThe O-Net is the third and final stage of MTCNN, which further refines the face candidates obtained from the previous stages.\n• Similar to the R-Net, the O-Net evaluates each remaining bounding box candidate in greater detail.\n• It performs more accurate facial landmark localization by detecting specific facial points, such as eyes, nose, and mouth.\n• It also refines the bounding box coordinates and provides more precise facial feature information.\n\nThe output of the O-Net includes the final bounding boxes, confidence scores, refined coordinates, and facial landmarks (points).\n\nIn summary, MTCNN is a cascaded architecture consisting of three stages: P-Net, R-Net, and O-Net. These stages work sequentially to detect faces in an image, refine the bounding box candidates, and localize facial landmarks. The result is a set of accurately detected faces with their corresponding bounding boxes and facial landmarks, making it a powerful tool for face detection and alignment in various applications such as face recognition and facial analysis.\n\nFace Detection: The first stage of MTCNN is responsible for detecting candidate face regions in an image. It uses a series of convolutional layers to identify potential face locations.\n\nFacial Landmark Localization: The second stage further refines the candidate regions by locating facial landmarks such as the eyes, nose, and mouth. This information helps in accurately aligning the detected faces.\n\nFace Alignment: The final stage aligns the detected faces based on the positions of the facial landmarks. This results in well-aligned face regions suitable for feature extraction or recognition.\n• We initialize the MTCNN model using .\n• We load an image and use MTCNN to detect faces and obtain their bounding box coordinates.\n• If faces are detected, we draw bounding boxes around them on the image.\n\nInception ResNet is a deep convolutional neural network architecture used for feature extraction in various computer vision tasks. It combines elements from both the Inception architecture and residual connections (ResNets) to create a deep and efficient model.\n\nHigh-Level Feature Extraction: Inception ResNet is designed to extract high-level features from images, making it suitable for tasks like face recognition.\n\nDeep Network: It is a deep architecture with many layers, allowing it to capture complex patterns and representations.\n\nEfficiency: Inception ResNet is known for its efficiency in terms of both computation and memory usage.\n• We load an image containing a face and preprocess it.\n• We pass the preprocessed face through the Inception ResNet model to obtain a feature vector (embedding) for the face.\n• Verify if two face images belong to the same person.\n• Group similar faces together in a collection of images.\n• Predict the age and gender of faces in images.\n\nThus,these embeddings can be used for face recognition, face verification, or other facial analysis tasks. They represent the unique characteristics of the detected face for subsequent processing or comparison with other face embeddings."
    },
    {
        "link": "https://github.com/timesler/facenet-pytorch/blob/master/examples/infer.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://kaggle.com/code/timesler/guide-to-mtcnn-in-facenet-pytorch/notebook?scriptVersionId=26165086",
        "document": ""
    },
    {
        "link": "https://kaggle.com/code/haoxuanjade/guide-to-mtcnn-in-facenet-pytorch/notebook",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/python-tkinter-canvas-widget",
        "document": ""
    },
    {
        "link": "https://dafarry.github.io/tkinterbook/canvas.htm",
        "document": "The Canvas widget provides structured graphics facilities for Tkinter. This is a highly versatile widget which can be used to draw graphs and plots, create graphics editors, and implement various kinds of custom widgets.\n\nWhen to use the Canvas Widget #\n\nThe canvas is a general purpose widget, which is typically used to display and edit graphs and other drawings.\n\nAnother common use for this widget is to implement various kinds of custom widgets. For example, you can use a canvas as a completion bar, by drawing and updating a rectangle on the canvas.\n\nTo draw things in the canvas, use the create methods to add new items.\n\nNote that items added to the canvas are kept until you remove them. If you want to change the drawing, you can either use methods like coords, itemconfig, and move to modify the items, or use delete to remove them.\n\nTo display things on the canvas, you create one or more canvas items, which are placed in a stack. By default, new items are drawn on top of items already on the canvas.\n\nTkinter provides lots of methods allowing you to manipulate the items in various ways. Among other things, you can attach (bind) event callbacks to individual canvas items.\n\nThe Canvas widget supports the following standard items:\n\nChords, pieslices, ovals, polygons, and rectangles consist of both an outline and an interior area, either of which can be made transparent (and if you insist, you can make both transparent).\n\nWindow items are used to place other Tkinter widgets on top of the canvas; for these items, the Canvas widget simply acts like a geometry manager.\n\nYou can also write your own item types in C or C++ and plug them into Tkinter via Python extension modules.\n\nThe Canvas widget uses two coordinate systems; the window coordinate system (with (0, 0) in the upper left corner), and a canvas coordinate system which specify where the items are drawn. By scrolling the canvas, you can specify which part of the canvas coordinate system to show in the window.\n\nThe scrollregion option is used to limit scrolling operations for the canvas. To set this, you can usually use something like:\n\nTo convert from window coordinates to canvas coordinates, use the canvasx and canvasy methods:\n\nThe Canvas widget allows you to identify items in several ways. Everywhere a method expects an item specifier, you can use one of the following:\n\nItem handles are integer values used to identify a specific item on the canvas. Tkinter automatically assigns a new handle to each new item created on the canvas. Item handles can be passed to the various canvas methods either as integers or as strings.\n\nTags are symbolic names attached to items. Tags are ordinary strings, and they can contain anything except whitespace (as long as they don’t look like item handles).\n\nAn item can have zero or more tags associated with it, and the same tag can be used for more than one item. However, unlike the Text widget, the Canvas widget doesn’t allow you to create bindings or otherwise configure tags for which there are no existing items. Tags are owned by the items, not the widget itself. All such operations are ignored.\n\nYou can either specify the tags via an option when you create the item, set them via the itemconfig method, or add them using the addtag_withtag method. The tags option takes either a single tag string, or a tuple of strings.\n\nTo get all tags associated with a specific item, use gettags. To get the handles for all items having a given tag, use find_withtag.\n\nThe Canvas widget also provides two predefined tags:\n\nALL (or the string “all”) matches all items on the canvas.\n\nCURRENT (or “current”) matches the item under the mouse pointer, if any. This can be used inside mouse event bindings to refer to the item that triggered the callback.\n\nThe Canvas widget implements a straight-forward damage/repair display model. Changes to the canvas, and external events such as Expose, are all treated as “damage” to the screen. The widget maintains a dirty rectangle to keep track of the damaged area.\n\nWhen the first damage event arrives, the canvas registers an idle task (using after_idle) which is used to “repair” the canvas when the program gets back to the Tkinter main loop. You can force updates by calling the update_idletasks method.\n\nWhen it’s time to redraw the canvas, the widget starts by allocating a pixmap (on X windows, this is an image memory stored on the display) with the same size as the dirty rectangle.\n\nIt then loops over the canvas items, and redraws all items for which the bounding box touch the dirty rectangle (this means that diagonal lines may be redrawn also if they don’t actually cover the rectangle, but this is usually no big deal).\n\nFinally, the widget copies the pixmap to the display, and releases the pixmap. The copy operation is a very fast operation on most modern hardware.\n\nSince the canvas uses a single dirty rectangle, you can sometimes get better performance by forcing updates. For example, if you’re changing things in different parts of the canvas without returning to the main loop, adding explicit calls to update_idletasks() allows the canvas to update a few small rectangles, instead of a large one with many more objects."
    },
    {
        "link": "https://i-programmer.info/programming/195-python/5105-creating-the-python-ui-with-tkinter-the-canvas-widget.html?start=2",
        "document": "In mastering any graphics system you first have to discover what sort of system it is - bitmapped or vector and retained or immediate. Next you need to understand its co-ordinate system.\n\nIf you simply specify a co-ordinate as a numeric value then its units are assumed to be pixels. If you specify a string with a trailing m then the units are millimeters, c for centimeters, i for inches and p for points (1/72 inch).\n\nWe already have the idea that the co-ordinate system in the canvas is slightly different from that in the root window. The canvas has 0,0 in the top left-hand corner and x increases to the right and y down the canvas. This is the same as the windows coordinate system but it doesn't stop where the window stops. That is you can draw things that are outside of the window. Things that are drawn off the edge so to speak are in the display list and you can work with then in the usual way but they are not drawn to the window.\n\nThere are no co-ordinate transformation methods for canvas - i.e. no translate, rotate or scale. It is possible to implement these using add-hoc methods but the only real co-ordinate methods provided are designed to implement scrolling.\n\nTo see the area of the drawing outside of the window area you can either increase the size of the window or you can define a scroll area.The scroll area is the area of the canvas that you could scroll into the window.\n\nFor example if you specify that the canvas is 200x200 within the window then you have a 200x200 window onto the entire canvas drawing space. If you set the scroll region to 400x400 then you could scroll the 200x200 region smoothly across the bigger region so that a x scroll of 0 placed the window area with top left-hand corner at 0.0 and a x scroll of 1 placed the window with its top left-hand corner at 200,0. Scrolling in the y direction works in the same way. The only complication in this is that the scroll factor varies from 0 to 1 with 0 placing the viewing window either hard left or at the top and a value of 1 sending the window to the far right or bottom of the viewing window.\n\nIn the case of a 200 by 200 window\n\nwe can draw some text at 300,20\n\nand it will be invisible because it is outside of the canvas area i.e. 0 to 199 in x and 0 to 199 in y.\n\nTo make it visible we can simply move the viewing window to the far right, i.e. a scroll factor of 1:\n\nNow the viewing window is positioned with its top right-hand corner at 200,0 and the text is visible.\n\nYou can do the same scrolling action in the y direction. You can scroll to any faction of the scroll area from 0.0 to 1.0 but it can be difficult to work out the scroll factor needed to place the viewing window where you want it.\n\nIn general if the window i.e. canvas size is wx by wy and the scroll area is sx by sy then to place the top left-hand corner at x, y you need scroll factors of:\n\nYou can also use the\n\nmethod to scroll n steps in the x direction (similar method for y) and the steps can be \"units\" or \"pages\" the \"little\" and \"big\" steps that you can scroll by when using a scrollbar.\n\nThere is also the xview and yview methods that can be used for either purpose, to scroll or to moveto, but these are mostly used by a scrollbar widget interface.\n\nThe scrollbar widget is an interesting topic in its own right and it will be covered in a future article but it is worth showing how to add a scrollbar to a canvas widget.\n\nTo use a scrollbar you have to hook up its command attribute to the canvas xview and yview methods and, if you want the scrollbar to set the size of its \"button\" to indicate the ratio of the scroll you need to set the canvas' xscrollcommand and yscrollcommand to the appropriate scrollbar set method.\n\nLet's do this job for just the x direction to make things easier to follow. First the canvas is set up at 200 by 200:\n\nNext we need to create the scrollbar:\n\nNow we need to connect the two widgets together:\n\nPutting all this together and runing it gives you a windows with a scroll bar and you can scroll the canvas horizontally.\n\nBeing able to scroll the canvas brings a new problem. It doesn't matter how the scroll is set events always report position in terms of the window co-ordinates. If you want to work with event locations in terms of the canvas co-ordinates you need to convert them using the w.canvasx and w.canvasy methods. These return the co-ordinates of the event location in terms of the canvas no matter how it has been scrolled.\n\nIn general if you want to use mouse clicks etc to manipulate graphics objects then you always need to perform the conversion to canvas co-ordinates first.\n\nMore about how to work with graphics objects and events in the next article.\n\nComing soon: Graphics objects, events, tags and using canvas to create widgets.\n• Mike James is the author of Programmer's Python: Everything is an Object published by I/O Press as part of the I Programmer Library. With the subtitle \"Something Completely Different\" this is for those who want to understand the deeper logic in the approach that Python 3 takes to classes and objects.\n\nTo be informed about new articles on I Programmer, sign up for our weekly newsletter, subscribe to the RSS feed and follow us on Twitter, Facebook or Linkedin.\n\nor email your comment to: comments@i-programmer.info"
    },
    {
        "link": "https://geeksforgeeks.org/python-gui-tkinter",
        "document": "Python Tkinter is a standard GUI (Graphical User Interface) library for Python which provides a fast and easy way to create desktop applications. Tkinter provides a variety of widgets like buttons, labels, text boxes, menus and more that can be used to create interactive user interfaces. Tkinter supports event-driven programming, where actions are taken in response to user events like clicks or keypresses.\n• Import the tkinter module : Import the tkinter module, which is necessary for creating the GUI components.\n• Create the main window (container) : Initialize the main application window using the Tk() class.\n• Set Window Properties : We can set properties like the title and size of the window.\n• Add widgets to the main window : We can add any number of widgets like buttons, labels, entry fields, etc., to the main window to design the interface.\n• Pack Widgets: Use geometry managers like pack(), grid() or place() to arrange the widgets within the window.\n• Apply event triggers to the widgets : We can attach event triggers to the widgets to define how they respond to user interactions.\n\nThere are two main methods used which the user needs to remember while creating the Python application with GUI.\n\nTo create a main window in Tkinter, we use the Tk() class. The syntax for creating a main window is as follows:\n• screenName: This parameter is used to specify the display name.\n• baseName: This parameter can be used to set the base name of the application.\n• className: We can change the name of the window by setting this parameter to the desired name.\n• useTk: This parameter indicates whether to use Tk or not.\n\nThe mainloop() method is used to run application once it is ready. It is an infinite loop that keeps the application running, waits for events to occur (such as button clicks) and processes these events as long as the window is not closed.\n\nThere are a number of tkinter widgets which we can put in our tkinter application. Some of the major widgets are explained below:\n\nIt refers to the display box where we display text or image. It can have various options like font, background, foreground, etc. The general syntax is:\n• master is the parameter used to represent the parent window.\n\nNote: We have a number of options and parameters that we can pass to widgets, only some them are used in the examples given in this article.\n\nA clickable button that can trigger an action. The general syntax is:\n\nIt is used to input the single line text entry from the user. For multi-line text input, Text widget is used. The general syntax is:\n\nA checkbox can be toggled on or off. It can be linked to a variable to store its state. The general syntax is:\n\nIt allows the user to select one option from a set of choices. They are grouped by sharing the same variable. The general syntax is:\n\nIt displays a list of items from which a user can select one or more. The general syntax is:\n\nIt refers to the slide controller which will be used to implement listed widgets. The general syntax is:\n\nIt is used to create all kinds of menus used by the application. The general syntax is:\n\nCombobox widget is created using the ttk.Combobox class from the tkinter.ttk module. The values for the Combobox are specified using the values parameter. The default value is set using the set method. An event handler function on_select is bound to the Combobox using the bind method, which updates a label with the selected item whenever an item is selected.\n\nIt is used to provide a graphical slider that allows to select any value from that scale. The general syntax is:\n\nThis widget is directly controlled by the window manager. It don’t need any parent window to work on.The general syntax is:\n\nIt is a widget to display text messages with word wrapping. The general syntax is:\n\nIt is a part of top-down menu which stays on the window all the time. Every menubutton has its own functionality. The general syntax is:\n\nprogressbar indicates the progress of a long-running task. When the button is clicked, the progressbar fills up to 100% over a short period, simulating a task that takes time to complete.\n\nIt is an entry of ‘Entry’ widget. Here, value can be input by selecting a fixed value of numbers. The general syntax is:\n\nTo edit a multi-line text and format the way it has to be displayed. The general syntax is:\n\nIt is used to draw pictures and other complex layout like graphics, text and widgets. The general syntax is:\n\nIt is a container widget which is used to handle number of panes arranged in it. The general syntax is:\n\nThis example demonstrates the usage of various color options in Tkinter widgets, including active background and foreground colors, background and foreground colors, disabled state colors, and selection colors. Each widget in the example showcases a different color option, providing a visual representation of how these options affect the appearance of the widgets.\n\nTkinter also offers access to the geometric configuration of the widgets which can organize the widgets in the parent windows. There are mainly three geometry manager classes class.\n\nIt organizes the widgets in blocks before placing in the parent widget. Widgets can be packed from the top, bottom, left or right. It can expand widgets to fill the available space or place them in a fixed size.\n\nIt organizes the widgets in grid (table-like structure) before placing in the parent widget. Each widget is assigned a row and column. Widgets can span multiple rows or columns using rowspan and columnspan.\n\nIt organizes the widgets by placing them on specific positions directed by the programmer. Widgets are placed at specific x and y coordinates. Sizes and positions can be specified in absolute or relative terms.\n\nIn Tkinter, events are actions that occur when a user interacts with the GUI, such as pressing a key, clicking a mouse button or resizing a window. Event handling allows us to define how our application should respond to these interactions.\n\nEvents in Tkinter are captured and managed using a mechanism called bindings. A binding links an event to a callback function (also known as an event handler) that is called when the event occurs.\n• widget: The Tkinter widget you want to bind the event to.\n• event: A string that specifies the type of event (e.g., <Button-1> for a left mouse click).\n• handler: The callback function that will be executed when the event occurs.\n\nKey events are triggered when a user presses a key on the keyboard. Mouse events are triggered by mouse actions, such as clicking or moving the mouse.\n\nIn this advanced example, multiple event types are handled simultaneously. The on_mouse_motion function is called whenever the mouse is moved within the window, demonstrating how we can track and respond to continuous events.\n\nThe event object is passed to the callback function when an event occurs. It contains useful information about the event, such as:\n• event.x and event.y: The x and y coordinates of the mouse event.\n• event.widget: The widget that triggered the event.\n\nWhat is Tkinter in Python used for?\n\nWhat does TK() mean in Python?\n\nIs Tkinter the only GUI for Python?\n\nWhat is a Tkinter window in Python?\n\nWhich GUI is best for Python?"
    },
    {
        "link": "https://stackoverflow.com/questions/19870735/how-to-use-tkinters-widget-config-to-modify-separate-widgets-that-were-create",
        "document": "I'm pretty new to Python. I can MAKE things work, but I'm never really sure if I'm actually using best practices, so forgive me if I'm not explaining things correctly, or if my code isn't written using best practices. I'm working on it!\n\nI'm trying to figure out how to use tkinter's widget.config() to modify separate widgets that were created by the same method? For example, in the code below, how can I use widget.config() to modify the text in results section 2, r2??\n\nThanks so much in advance for your help!"
    }
]