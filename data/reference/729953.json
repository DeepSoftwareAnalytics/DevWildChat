[
    {
        "link": "https://digitalocean.com/community/tutorials/how-to-work-with-web-data-using-requests-and-beautiful-soup-with-python-3",
        "document": "The web provides us with more data than any of us can read and understand, so we often want to work with that information programmatically in order to make sense of it. Sometimes, that data is provided to us by website creators via or comma-separated values files, or through an API (Application Programming Interface). Other times, we need to collect text from the web ourselves.\n\nThis tutorial will go over how to work with the Requests and Beautiful Soup Python packages in order to make use of data from web pages. The Requests module lets you integrate your Python programs with web services, while the Beautiful Soup module is designed to make screen-scraping get done quickly. Using the Python interactive console and these two libraries, we’ll go through how to collect a web page and work with the textual information available there.\n\nTo complete this tutorial, you’ll need a development environment for Python 3. You can follow the appropriate guide for your operating system available from the series How To Install and Set Up a Local Programming Environment for Python 3 or How To Install Python 3 and Set Up a Programming Environment on an Ubuntu 20.04 Server to configure everything you need.\n\nAdditionally, you should be familiar with:\n\nWith your development environment set up and these Python programming concepts in mind, let’s start working with Requests and Beautiful Soup.\n\nLet’s begin by activating our Python 3 programming environment. Make sure you’re in the directory where your environment is located, and run the following command:\n\nIn order to work with web pages, we’re going to need to request the page. The Requests library allows you to make use of HTTP within your Python programs in a human readable way.\n\nWith our programming environment activated, we’ll install Requests with pip:\n\nWhile the Requests library is being installed, you’ll receive the following output:\n\nIf Requests was previously installed, you would have received feedback similar to the following from your terminal window:\n\nWith Requests installed into our programming environment, we can go on to install the next module.\n\nJust as we did with Requests, we’ll install Beautiful Soup with pip. The current version of Beautiful Soup 4 can be installed with the following command:\n\nOnce you run this command, you should see output that looks similar to the following:\n\nNow that both Beautiful Soup and Requests are installed, we can move on to understanding how to work with the libraries to scrape websites.\n\nWith the two Python libraries we’ll be using now installed, we’re can familiarize ourselves with stepping through a basic web page.\n\nLet’s first move into the Python Interactive Console:\n\nFrom here, we’ll import the Requests module so that we can collect a sample web page:\n\nWe’ll assign the URL (below) of the sample web page, to the variable :\n\nNext, we can assign the result of a request of that page to the variable with the method. We pass the page’s URL (that was assigned to the variable) to that method.\n\nThe Response object above tells us the property in square brackets (in this case ). This attribute can be called explicitly:\n\nThe returned code of tells us that the page downloaded successfully. Codes that begin with the number generally indicate success, while codes that begin with a or indicate that an error occurred. You can read more about HTTP status codes from the W3C’s Status Code Definitions.\n\nIn order to work with web data, we’re going to want to access the text-based content of web files. We can read the content of the server’s response with (or if we would like to access the response in bytes).\n\nOnce we press , we’ll receive the following output:\n\nHere we see that the full text of the page was printed out, with all of its HTML tags. However, it is difficult to read because there is not much spacing.\n\nIn the next section, we can leverage the Beautiful Soup module to work with this textual data in a more human-friendly manner.\n\nThe Beautiful Soup library creates a parse tree from parsed HTML and XML documents (including documents with non-closed tags or tag soup and other malformed markup). This functionality will make the web page text more readable than what we saw coming from the Requests module.\n\nTo start, we’ll import Beautiful Soup into the Python console:\n\nNext, we’ll run the document through the module to give us a object — that is, a parse tree from this parsed page that we’ll get from running Python’s built-in over the HTML. The constructed object represents the document as a nested data structure. This is assigned to the variable .\n\nTo show the contents of the page on the terminal, we can print it with the method in order to turn the Beautiful Soup parse tree into a nicely formatted Unicode string.\n\nThis will render each HTML tag on its own line:\n\nIn the output above, we can see that there is one tag per line and also that the tags are nested because of the tree schema used by Beautiful Soup.\n\nWe can extract a single tag from a page by using Beautiful Soup’s method. This will return all instances of a given tag within a document.\n\nRunning that method on our object returns the full text of the song along with the relevant tags and any tags contained within that requested tag, which here includes the line break tags :\n\nYou will notice in the output above that the data is contained in square brackets . This means it is a Python list data type.\n\nBecause it is a list, we can call a particular item within it (for example, the third element), and use the method to extract all the text from inside that tag:\n\nThe output that we receive will be what is in the third element in this case:\n\nNote that line breaks are also shown in the returned string above.\n\nHTML elements that refer to CSS selectors like class and ID can be helpful to look at when working with web data using Beautiful Soup. We can target specific classes and IDs by using the method and passing the class and ID strings as arguments.\n\nFirst, let’s find all of the instances of the class . In Beautiful Soup we will assign the string for the class to the keyword argument :\n\nWhen we run the above line, we’ll receive the following list as output:\n\nThe two -tagged sections with the class of were printed out to the terminal.\n\nWe can also specify that we want to search for the class only within tags, in case it is used for more than one tag:\n\nRunning the line above will produce the same output as before.\n\nWe can also use Beautiful Soup to target IDs associated with HTML tags. In this case we will assign the string to the keyword argument :\n\nOnce we run the line above, we’ll receive the following output:\n\nThe text associated with the tag with the id of is printed out to the terminal along with the relevant tags.\n\nThis tutorial took you through retrieving a web page with the Requests module in Python and doing some preliminary scraping of that web page’s textual data in order to gain an understanding of Beautiful Soup.\n\nFrom here, you can go on to creating a web scraping program that will create a CSV file out of data collected from the web by following the tutorial How To Scrape Web Pages with Beautiful Soup and Python 3."
    },
    {
        "link": "https://kirenz.com/blog/posts/2022-05-02-web-scraping-in-python-with-beautiful-soup-requests-and-pandas",
        "document": ""
    },
    {
        "link": "https://realpython.com/beautiful-soup-web-scraper-python",
        "document": "Beautiful Soup is a Python library designed for parsing HTML and XML documents. It creates parse trees that make it straightforward to extract data from HTML documents you’ve scraped from the internet. Beautiful Soup is a useful tool in your web scraping toolkit, allowing you to conveniently extract specific information from HTML, even from complex static websites.\n\nIn this tutorial, you’ll learn how to build a web scraper using Beautiful Soup along with the Requests library to scrape and parse job listings from a static website. Static websites provide consistent HTML content, while dynamic sites may require handling JavaScript. For dynamic websites, you’ll need to incorporate additional tools that can execute JavaScript, such as Scrapy or Selenium.\n\nBy the end of this tutorial, you’ll understand that:\n• You can use Beautiful Soup for parsing HTML and XML documents to extract data from web pages.\n• Beautiful Soup is named after a song in Alice’s Adventures in Wonderland by Lewis Carroll, based on its ability to tackle poorly structured HTML known as tag soup.\n• You’ll often use Beautiful Soup in your web scraping pipeline when scraping static content, while you’ll need additional tools such as Selenium to handle dynamic, JavaScript-rendered pages.\n• Using Beautiful Soup is legal because you only use it for parsing documents. Web scraping in general is also legal if you respect a website’s terms of service and copyright laws.\n\nWorking through this project will give you the knowledge and tools that you need to scrape any static website out there on the World Wide Web. If you like learning with hands-on examples and have a basic understanding of Python and HTML, then this tutorial is for you! You can download the project source code by clicking on the link below:\n\nWeb scraping is the process of gathering information from the internet. Even copying and pasting the lyrics of your favorite song can be considered a form of web scraping! However, the term “web scraping” usually refers to a process that involves automation. While some websites don’t like it when automatic scrapers gather their data, which can lead to legal issues, others don’t mind it. If you’re scraping a page respectfully for educational purposes, then you’re unlikely to have any problems. Still, it’s a good idea to do some research on your own to make sure you’re not violating any Terms of Service before you start a large-scale web scraping project. Say that you like to surf—both in the ocean and online—and you’re looking for employment. It’s clear that you’re not interested in just any job. With a surfer’s mindset, you’re waiting for the perfect opportunity to roll your way! You know about a job site that offers precisely the kinds of jobs you want. Unfortunately, a new position only pops up once in a blue moon, and the site doesn’t provide an email notification service. You consider checking up on it every day, but that doesn’t sound like the most fun and productive way to spend your time. You’d rather be outside surfing real-life waves! Thankfully, Python offers a way to apply your surfer’s mindset. Instead of having to check the job site every day, you can use Python to help automate the repetitive parts of your job search. With automated web scraping, you can write the code once, and it’ll get the information that you need many times and from many pages. Note: In contrast, when you try to get information manually, you might spend a lot of time clicking, scrolling, and searching, especially if you need large amounts of data from websites that are regularly updated with new content. Manual web scraping can take a lot of time and be highly repetitive and error-prone. There’s so much information on the internet, with new information constantly being added. You’ll probably be interested in some of that data, and much of it is out there for the taking. Whether you’re actually on the job hunt or just want to automatically download all the lyrics of your favorite artist, automated web scraping can help you accomplish your goals. The internet has grown organically out of many sources. It combines many different technologies, styles, and personalities, and it continues to grow every day. In other words, the internet is a hot mess! Because of this, you’ll run into some challenges when scraping the web:\n• Variety: Every website is different. While you’ll encounter general structures that repeat themselves, each website is unique and will need personal treatment if you want to extract the relevant information.\n• Durability: Websites constantly change. Say you’ve built a shiny new web scraper that automatically cherry-picks what you want from your resource of interest. The first time you run your script, it works flawlessly. But when you run the same script a while later, you run into a discouraging and lengthy stack of tracebacks! Unstable scripts are a realistic scenario because many websites are in active development. If a site’s structure changes, then your scraper might not be able to navigate the sitemap correctly or find the relevant information. The good news is that changes to websites are often small and incremental, so you’ll likely be able to update your scraper with minimal adjustments. Still, keep in mind that the internet is dynamic and keeps on changing. Therefore, the scrapers you build will probably require maintenance. You can set up continuous integration to run scraping tests periodically to ensure that your main script doesn’t break without your knowledge. Some website providers offer application programming interfaces (APIs) that allow you to access their data in a predefined manner. With APIs, you can avoid parsing HTML. Instead, you can access the data directly using formats like JSON and XML. HTML is primarily a way to visually present content to users. When you use an API, the data collection process is generally more stable than it is through web scraping. That’s because developers create APIs to be consumed by programs rather than by human eyes. The front-end presentation of a site might change often, but a change in the website’s design doesn’t affect its API structure. The structure of an API is usually more permanent, which means it’s a more reliable source of the site’s data. However, APIs can change as well. The challenges of both variety and durability apply to APIs just as they do to websites. Additionally, it’s much harder to inspect the structure of an API by yourself if the provided documentation lacks quality. The approach and tools you need to gather information using APIs is outside the scope of this tutorial. To learn more about it, check out API Integration in Python.\n\nBefore you write any Python code, you need to get to know the website that you want to scrape. Getting to know the website should be your first step for any web scraping project that you want to tackle. You’ll need to understand the site structure to extract the information relevant for you. Start by opening the site that you want to scrape with your favorite browser. Click through the site and interact with it just like any typical job searcher would. For example, you can scroll through the main page of the website: On that page, you can see many job postings in a card format. Each of them has two buttons. If you click on Learn, then you’ll visit Real Python’s home page. If you click on Apply, then you’ll see a new page that contains more detailed descriptions of the job on that card. You might also notice that the URL in your browser’s address bar changes when you navigate to one of those pages. You can encode a lot of information in a URL. Becoming familiar with how URLs work and what they’re made of will help you on your web scraping journey. For example, you might find yourself on a details page that has the following URL: You can deconstruct the above URL into two main parts:\n• The base URL points to the main location of the web resource. In the example above, the base URL is .\n• The path to a specific resource location points to a unique job description. In the example above, the path is . Any job posted on this website will share the same base URL. However, the location of the unique resources will be different depending on the job posting that you view. Usually, similar resources on a website will share a similar location, such as the folder structure . However, the final part of the path points to a specific resource and will be different for each job posting. In this case, it’s a static HTML file named . URLs can hold more information than just the location of a file. Some websites use query parameters to encode values that you submit when performing a search. You can think of them as query strings that you send to the database to retrieve specific records. You’ll find query parameters at the end of a URL. For example, if you go to Indeed and search for “software developer” in “Australia” through the site’s search bar, you’ll see that the URL changes to include these values as query parameters: The query parameters in this URL are . Query parameters consist of three parts:\n• Start: You can identify the beginning of the query parameters by looking for the question mark ( ).\n• Information: You’ll find the pieces of information that constitute one query parameter encoded in key-value pairs, where related keys and values are joined together by an equal sign ( ).\n• Separator: You’ll see an ampersand symbol ( ) separating multiple query parameters if there are more than one. Equipped with this information, you can separate the URL’s query parameters into two key-value pairs:\n• selects the location of the job. Try to change the search parameters and observe how that affects your URL. Go ahead and enter new values in the search bar of the Indeed job board: Change these values to observe the changes in the URL. Next, try to change the values directly in your URL. See what happens when you paste the following URL into your browser’s address bar: If you change and submit the values in the website’s search box, then it’ll be directly reflected in the URL’s query parameters and vice versa. If you change either of them, then you’ll see different results on the website. As you can see, exploring the URLs of a site can give you insight into how to retrieve data from the website’s server. Head back to Fake Python jobs and continue to explore it. This site is a static website containing hardcoded information. It doesn’t operate on top of a database, which is why you won’t have to work with query parameters in this scraping tutorial. Next, you’ll want to learn more about how the data is structured for display. You’ll need to understand the page structure to pick what you want from the HTML response that you’ll collect in one of the upcoming steps. Developer tools can help you understand the structure of a website. All modern browsers come with developer tools installed. In this section, you’ll learn how to work with the developer tools in Chrome. The process will be very similar on other modern browsers. In Chrome on macOS, you can open up the developer tools through the menu by selecting View → Developer → Developer Tools. On Windows and Linux, you can access them by clicking the top-right menu button ( ) and selecting More Tools → Developer Tools. You can also access your developer tools by right-clicking on the page and selecting the Inspect option or using a keyboard shortcut: Developer tools allow you to interactively explore the site’s document object model (DOM) to better understand your source. To dig into your page’s DOM, select the Elements tab in developer tools. You’ll see a structure with clickable HTML elements. You can expand, collapse, and even edit elements right in your browser: The HTML on the right represents the structure of the page you can see on the left. You can think of the text displayed in your browser as the HTML structure of the page. If you’re interested, then you can read more about the difference between the DOM and HTML. When you right-click elements on the page, you can select Inspect to zoom to their location in the DOM. You can also hover over the HTML text on your right and see the corresponding elements light up on the page. Click to expand the exercise block for a specific task to practice using your developer tools: Find a single job posting. What HTML element is it wrapped in, and what other HTML elements does it contain? Play around and explore! The more you get to know the page you’re working with, the easier it’ll be to scrape. But don’t get too overwhelmed with all that HTML text. You’ll use the power of programming to step through this maze and cherry-pick the information that’s relevant to you.\n\nNow that you have an idea of what you’re working with, it’s time to start using Python. First, you’ll want to get the site’s HTML code into your Python script so that you can interact with it. For this task, you’ll use Python’s Requests library. Before you install any external package, you’ll need to create a virtual environment for your project. Activate your new virtual environment, then type the following command in your terminal to install the Requests library: Then open up a new file in your favorite text editor and call it . You only need a few lines of code to retrieve the HTML: When you run this code, it issues an HTTP request to the given URL. It retrieves the HTML data that the server sends back and stores that data in a Python object you called . If you print the attribute of , then you’ll notice that it looks just like the HTML you inspected earlier with your browser’s developer tools. You’ve successfully fetched the static site content from the internet! You now have access to the site’s HTML from within your Python script. The website that you’re scraping in this tutorial serves static HTML content. In this scenario, the server that hosts the site sends back HTML documents that already contain all the data a user gets to see. When you inspected the page with developer tools earlier on, you discovered that a single job posting consists of the following long and messy-looking HTML: It can be challenging to wrap your head around a long block of HTML code. To make it easier to read, you can use an HTML formatter to clean up the HTML automatically. Good readability can help you better understand the structure of any block of code. While improved HTML formatting may or may not help, it’s always worth a try. Note: Keep in mind that every website looks different. That’s why it’s necessary to inspect and understand the structure of the site you’re working with before moving forward. The HTML you’ll encounter will sometimes be confusing. Luckily, the HTML of this job board has descriptive class names on the elements that you’re interested in:\n• contains the title of the job posting.\n• contains the name of the company that offers the position.\n• contains the location where you’d be working. If you ever get lost in a large pile of HTML, remember that you can always go back to your browser and use the developer tools to further explore the HTML structure interactively. By now, you’ve successfully harnessed the power and user-friendly design of Python’s Requests library. With only a few lines of code, you managed to scrape static HTML content from the web and make it available for further processing. While this was a breeze, you may encounter more challenging situations when working on your own web scraping projects. Before you learn how to select the relevant information from the HTML that you just scraped, you’ll take a quick look at two more challenging situations. Some pages contain information that’s hidden behind a login. This means you’ll need an account to be able to scrape anything from the page. Just like you need to log in on your browser when you want to access content on such a page, you’ll also need to log in from your Python script. The Requests library comes with the built-in capacity to handle authentication. With these techniques, you can log in to websites when making the HTTP request from your Python script and then scrape information that’s hidden behind a login. You won’t need to log in to access the job board information, so this tutorial won’t cover authentication. Many modern websites don’t send back static HTML content like this practice site does. If you’re dealing with a dynamic website, then you could receive JavaScript code as a response. This code will look completely different from what you see when you inspect the same page with your browser’s developer tools. Note: In this tutorial, the term dynamic website refers to a website that doesn’t return the same HTML that you see when viewing the page in your browser. Dynamic websites are designed to provide their functionality in collaboration with the clients’ browsers. Instead of sending HTML pages, these apps send JavaScript code that instructs your browser to create the desired HTML. Web apps deliver dynamic content this way to offload work from the server to the clients’ machines, as well as to avoid page reloads and improve the overall user experience. Your browser will diligently execute the JavaScript code it receives from a server and create the DOM and HTML for you locally. However, if you request a dynamic website in your Python script, then you won’t get the HTML page content. When you use Requests, you receive only what the server sends back. In the case of a dynamic website, you’ll end up with JavaScript code without the relevant data. The only way to go from that code to the content that you’re interested in is to execute the code, just like your browser does. The Requests library can’t do that for you, but there are other solutions that can:\n• Requests-HTML is a project created by the author of the Requests library that allows you to render JavaScript using syntax that’s similar to the syntax in Requests. It also includes capabilities for parsing the data by using Beautiful Soup under the hood.\n• Selenium is another popular choice for scraping dynamic content. Selenium automates a full browser and can execute JavaScript, allowing you to interact with and retrieve the fully rendered HTML response for your script. You won’t go deeper into scraping dynamically-generated content in this tutorial. If you need to scrape a dynamic website, then you can look into one of the options mentioned above.\n\nYou’ve successfully scraped some HTML from the internet, but when you look at it, it looks like a mess. There are tons of HTML elements here and there, thousands of attributes scattered around—and maybe there’s some JavaScript mixed in as well? It’s time to parse this lengthy code response with the help of Python to make it more accessible so you can pick out the data that you want. Beautiful Soup is a Python library for parsing structured data. It allows you to interact with HTML in a similar way to how you interact with a web page using developer tools. The library exposes intuitive methods that you can use to explore the HTML you received. Note: The name Beautiful Soup originates from the Lewis Carroll song Beautiful Soup in Alice’s Adventures in Wonderland, where a character sings about beautiful soup. This name reflects the library’s ability to parse poorly formed HTML that’s also known as tag soup. To get started, use your terminal to install Beautiful Soup into your virtual environment: Then, import the library in your Python script and create a object: When you add the two highlighted lines of code, then you create a object that takes as input, which is the HTML content that you scraped earlier. Note: You’ll want to pass instead of to avoid problems with character encoding. The attribute holds raw bytes, which Python’s built-in HTML parser can decode better than the text representation you printed earlier using the attribute. The second argument that you pass to the class constructor, , makes sure that you use an appropriate parser for HTML content. At this point, you’re set up with a object that you named . You can now run your script using Python’s interactive mode: When you use the command-option to run a script, then Python executes the code and drops you into a REPL environment. This can be a good way to continue exploring the scraped HTML through the user-friendly lens of Beautiful Soup. In an HTML web page, every element can have an attribute assigned. As the name already suggests, that attribute makes the element uniquely identifiable on the page. You can begin to parse your page by selecting a specific element by its ID. Switch back to developer tools and identify the HTML object that contains all the job postings. Explore by hovering over parts of the page and using right-click to Inspect. Note: It helps to periodically switch back to your browser and explore the page interactively using developer tools. You’ll get a better idea of where and how to find the exact elements that you’re looking for. In this case, the element that you’re looking for is a with an attribute that has the value . It has some other attributes as well, but below is the gist of what you’re looking for: Beautiful Soup allows you to find that specific HTML element by its ID: For easier viewing, you can prettify any object when you print it out. If you call on the variable that you assigned above, then you’ll see all the HTML contained within the neatly structured: When you find an element by its ID, you can pick out one specific element from among the rest of the HTML, no matter how large the source code of the website is. Now you can focus on working with only this part of the page’s HTML. It looks like your soup just got a little thinner! Nevertheless, it’s still quite dense. You’ve seen that every job posting is wrapped in a element with the class . Now you can work with your new object called and select only the job postings in it. These are, after all, the parts of the HTML that you’re interested in! You can pick out all job cards in a single line of code: Here, you call on , which is a object. It returns an iterable containing all the HTML for all the job listings displayed on that page. Take a look at all of them: That’s pretty neat already, but there’s still a lot of HTML! You saw earlier that your page has descriptive class names on some elements. You can pick out those child elements from each job posting with : Each is another object. Therefore, you can use the same methods on it as you did on its parent element, . With this code snippet, you’re getting closer and closer to the data that you’re actually interested in. Still, there’s a lot going on with all those HTML tags and attributes floating around: Next, you’ll learn how to narrow down this output to access only the text content that you’re interested in. You only want to see the title, company, and location of each job posting. And behold! Beautiful Soup has got you covered. You can add to a object to return only the text content of the HTML elements that the object contains: Run the above code snippet, and you’ll see the text of each element displayed. However, you’ll also get some extra whitespace. But no worries, because you’re working with Python strings so you can the superfluous whitespace. You can also apply any other familiar Python string methods to further clean up your text: The results finally look much better! You’ve now got a readable list of jobs, associated company names, and each job’s location. However, you’re specifically looking for a position as a software developer, and these results contain job postings in many other fields as well. Find Elements by Class Name and Text Content Not all of the job listings are developer jobs. Instead of printing out all the jobs listed on the website, you’ll first filter them using keywords. You know that job titles in the page are kept within elements. To filter for only specific jobs, you can use the argument: This code finds all elements where the contained string matches exactly. Note that you’re directly calling the method on your first variable. If you go ahead and the output of the above code snippet to your console, then you might be disappointed because it’ll be empty: There was a Python job in the search results, so why isn’t it showing up? When you use as you did above, your program looks for that string exactly. Any variations in the spelling, capitalization, or whitespace will prevent the element from matching. In the next section, you’ll find a way to make your search string more general. In addition to strings, you can sometimes pass functions as arguments to Beautiful Soup methods. You can change the previous line of code to use a function instead: Now you’re passing an anonymous function to the argument. The lambda function looks at the text of each element, converts it to lowercase, and checks whether the substring is found anywhere. You can check whether you managed to identify all the Python jobs with this approach: Your program has found ten matching job posts that include the word in their job title! Finding elements based on their text content is a powerful way to filter your HTML response for specific information. Beautiful Soup allows you to use exact strings or functions as arguments for filtering text in objects. However, when you try to print the information of the filtered Python jobs like you’ve done before, you run into an error: This traceback message is a common error that you’ll run into a lot when you’re scraping information from the internet. Inspect the HTML of an element in your list. What does it look like? Where do you think the error is coming from? When you look at a single element in , you’ll see that it consists of only the element that contains the job title: When you revisit the code you used to select the items, you’ll notice that’s what you targeted. You filtered for only the title elements of the job postings that contain the word . As you can see, these elements don’t include the rest of the information about the job. The error message you received earlier was related to this: You tried to find the job title, the company name, and the job’s location in each element in , but each element contains only the job title text. Your diligent parsing library still looks for the other ones, too, and returns because it can’t find them. Then, fails with the shown error message when you try to extract the attribute from one of these objects. The text you’re looking for is nested in sibling elements of the elements that your filter returns. Beautiful Soup can help you select sibling, child, and parent elements of each object. One way to get access to all the information for a job is to step up in the hierarchy of the DOM starting from the elements that you identified. Take another look at the HTML of a single job posting, for example, using your developer tools. Then, find the element that contains the job title and its closest parent element that contains the information you’re interested in: The element with the class contains all the information you want. It’s a third-level parent of the title element that you found using your filter. With this information in mind, you can now use the elements in and fetch their great-grandparent elements to get access to all the information you want: You added a list comprehension that operates on each of the title elements in that you got by filtering with the lambda expression. You’re selecting the parent element of the parent element of the parent element of each title element. That’s three generations up! When you were looking at the HTML of a single job posting, you identified that this specific parent element with the class name contains all the information you need. Now you can adapt the code in your loop to iterate over the parent elements instead: When you run your script another time, you’ll see that your code once again has access to all the relevant information. That’s because you’re now looping over the elements instead of just the title elements. Using the attribute that each object comes with gives you an intuitive way to step through your DOM structure and address the elements you need. You can also access child elements and sibling elements in a similar manner. Read up on navigating the tree for more information. At this point, you’ve already written code that scrapes the site and filters its HTML for relevant job postings. Well done! However, what’s still missing is fetching the link to apply for a job. While inspecting the page, you found two links at the bottom of each card. If you use on the link elements in the same way you did for the other elements, then you won’t get the URLs that you’re interested in: If you execute the code shown above, then you’ll get the link text for and instead of the associated URLs. That’s because the attribute leaves only the visible content of an HTML element. It strips away all HTML tags, including the HTML attributes containing the URL, and leaves you with just the link text. To get the URL instead, you need to extract the value of one of the HTML attributes instead of discarding it. The URL of a link element is associated with the HTML attribute. The specific URL that you’re looking for is the value of the attribute of the second tag at the bottom of the HTML for a single job posting: Start by fetching all the elements in a job card. Then, extract the value of their attributes using square-bracket notation: In this code snippet, you first fetch all the links from each of the filtered job postings. Then, you extract the attribute, which contains the URL, using and print it to your console. Each job card has two links associated with it. However, you’re only looking for the second link, so you’ll apply a small edit to the code: In the updated code snippet, you use indexing to pick the second link element from the results of using its index ( ). Then, you directly extract the URL using the square-bracket notation with the key, thereby fetching the value of the attribute. You can use the same square-bracket notation to extract other HTML attributes as well."
    },
    {
        "link": "https://geeksforgeeks.org/implementing-web-scraping-python-beautiful-soup",
        "document": "There are mainly two ways to extract data from a website:\n• None Use the API of the website (if it exists). For example, Facebook has the Facebook Graph API which allows retrieval of data posted on Facebook.\n• None Access the HTML of the webpage and extract useful information/data from it. This technique is called web scraping or web harvesting or web data extraction.\n\nThis article discusses the steps involved in web scraping using the implementation of a Web Scraping framework of Python called Beautiful Soup. Steps involved in web scraping:\n• None Send an HTTP request to the URL of the webpage you want to access. The server responds to the request by returning the HTML content of the webpage. For this task, we will use a third-party HTTP library for python-requests.\n• None Once we have accessed the HTML content, we are left with the task of parsing the data. Since most of the HTML data is nested, we cannot extract data simply through string processing. One needs a parser which can create a nested/tree structure of the HTML data. There are many HTML parser libraries available but the most advanced one is html5lib.\n• None Now, all we need to do is navigating and searching the parse tree that we created, i.e. tree traversal. For this task, we will be using another third-party python library, . It is a Python library for pulling data out of HTML and XML files.\n• None Easiest way to install external libraries in python is to use pip. pip is a package management system used to install and manage software packages written in Python. All you need to do is:\n• None Another way is to download them manually from these links:\n\nLet us try to understand this piece of code.\n• None First of all import the requests library.\n• None Then, specify the URL of the webpage you want to scrape.\n• None Send a HTTP request to the specified URL and save the response from server in a response object called r.\n• None Now, as print r.content to get the raw HTML content of the webpage. It is of ‘string’ type.\n\nNote: Sometimes you may get error “Not accepted” so try adding a browser user agent like below. Find your user agent based on device and browser from here https://deviceatlas.com/blog/list-of-user-agent-strings\n\nA really nice thing about the BeautifulSoup library is that it is built on the top of the HTML parsing libraries like html5lib, lxml, html.parser, etc. So BeautifulSoup object and specify the parser library can be created at the same time. In the example above,\n\nWe create a BeautifulSoup object by passing two arguments:\n• r.content : It is the raw HTML content.\n• html5lib : Specifying the HTML parser we want to use.\n\nNow soup.prettify() is printed, it gives the visual representation of the parse tree created from the raw HTML content. Step 4: Searching and navigating through the parse tree Now, we would like to extract some useful data from the HTML content. The soup object contains all the data in the nested structure which could be programmatically extracted. In our example, we are scraping a webpage consisting of some quotes. So, we would like to create a program to save those quotes (and all relevant information about them).\n\nBefore moving on, we recommend you to go through the HTML content of the webpage which we printed using soup.prettify() method and try to find a pattern or a way to navigate to the quotes.\n• None It is noticed that all the quotes are inside a div container whose id is ‘all_quotes’. So, we find that div element (termed as table in above code) using find()\n• None The first argument is the HTML tag you want to search and second argument is a dictionary type element to specify the additional attributes associated with that tag. find() method returns the first matching element. You can try to print table.prettify() to get a sense of what this piece of code does.\n• None Now, in the table element, one can notice that each quote is inside a div container whose class is quote. So, we iterate through each div container whose class is quote. Here, we use findAll() method which is similar to find method in terms of arguments but it returns a list of all matching elements. Each quote is now iterated using a variable called row. Here is one sample row HTML content for better understanding: Now consider this piece of code:\n• None We create a dictionary to save all information about a quote. The nested structure can be accessed using dot notation. To access the text inside an HTML element, we use .text :\n• None We can add, remove, modify and access a tag’s attributes. This is done by treating the tag as a dictionary:\n• None Lastly, all the quotes are appended to the list called quotes.\n• None Finally, we would like to save all our data in some CSV file.\n• None Here we create a CSV file called inspirational_quotes.csv and save all the quotes in it for any further use.\n\nSo, this was a simple example of how to create a web scraper in Python. From here, you can try to scrap any other website of your choice. In case of any queries, post them below in comments section.\n\nNote : Web Scraping is considered as illegal in many cases. It may also cause your IP to be blocked permanently by a website. This blog is contributed by Nikhil Kumar.\n\nHow to Use BeautifulSoup for Web Scraping in Python?\n\nWhat is the Use of BeautifulSoup?\n\nWhy is it Called BeautifulSoup?\n\nWhich Tool is Best for Web Scraping?"
    },
    {
        "link": "https://stackoverflow.com/questions/31754456/efficient-web-page-scraping-with-python-requests-beautifulsoup",
        "document": "I am trying to grab information from the Chicago Transit Authority bustracker website. In particular, I would like to quickly output the arrival ETAs for the top two buses. I can do this rather easily with Splinter; however I am running this script on a headless Raspberry Pi model B and Splinter plus pyvirtualdisplay results in a significant amount of overhead.\n\nSomething along the lines of\n\ndoes not do the trick. All of the data fields are empty (well, have ). For example, when the page looks like this:\n\nThis code snippet gives me instead of \"12 MINUTES\" when I perform the analogous search with Splinter.\n\nI'm not wedded to BeautifulSoup/requests; I just want something that doesn't require the overhead of Splinter/pyvirtualdisplay since the project requires that I obtain a short list of strings (e.g. for the image above, ) and then exits."
    },
    {
        "link": "https://docs.scrapy.org",
        "document": "Scrapy is a fast high-level web crawling and web scraping framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.\n\nUnderstand what Scrapy is and how it can help you. Get Scrapy installed on your computer. Learn more by playing with a pre-made Scrapy project.\n\nSee what has changed in recent Scrapy versions. Learn how to contribute to the Scrapy project."
    },
    {
        "link": "https://docs.scrapy.org/en/latest/intro/overview.html",
        "document": "Scrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\n\nEven though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.\n\nWalk-through of an example spider¶ In order to show you what Scrapy brings to the table, we’ll walk you through an example of a Scrapy Spider using the simplest way to run a spider. Here’s the code for a spider that scrapes famous quotes from website https://quotes.toscrape.com, following the pagination: Put this in a text file, name it something like and run the spider using the command: When this finishes you will have in the file a list of the quotes in JSON Lines format, containing the text and author, which will look like this: The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid. A day without sunshine is like, you know, night. Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car. When you ran the command , Scrapy looked for a Spider definition inside it and ran it through its crawler engine. The crawl started by making requests to the URLs defined in the attribute (in this case, only the URL for quotes in the humor category) and called the default callback method , passing the response object as an argument. In the callback, we loop through the quote elements using a CSS Selector, yield a Python dict with the extracted quote text and author, look for a link to the next page and schedule another request using the same method as callback. Here you will notice one of the main advantages of Scrapy: requests are scheduled and processed asynchronously. This means that Scrapy doesn’t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime. This also means that other requests can keep going even if a request fails or an error happens while handling it. While this enables you to do very fast crawls (sending multiple concurrent requests at the same time, in a fault-tolerant way) Scrapy also gives you control over the politeness of the crawl through a few settings. You can do things like setting a download delay between each request, limiting the amount of concurrent requests per domain or per IP, and even using an auto-throttling extension that tries to figure these settings out automatically. This is using feed exports to generate the JSON file, you can easily change the export format (XML or CSV, for example) or the storage backend (FTP or Amazon S3, for example). You can also write an item pipeline to store the items in a database.\n\nYou’ve seen how to extract and store items from a website using Scrapy, but this is just the surface. Scrapy provides a lot of powerful features for making scraping easy and efficient, such as:\n• None Built-in support for selecting and extracting data from HTML/XML sources using extended CSS selectors and XPath expressions, with helper methods for extraction using regular expressions.\n• None An interactive shell console (IPython aware) for trying out the CSS and XPath expressions to scrape data, which is very useful when writing or debugging your spiders.\n• None Built-in support for generating feed exports in multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP, S3, local filesystem)\n• None Robust encoding support and auto-detection, for dealing with foreign, non-standard and broken encoding declarations.\n• None Strong extensibility support, allowing you to plug in your own functionality using signals and a well-defined API (middlewares, extensions, and pipelines).\n• None A wide range of built-in extensions and middlewares for handling:\n• None A Telnet console for hooking into a Python console running inside your Scrapy process, to introspect and debug your crawler\n• None Plus other goodies like reusable spiders to crawl sites from Sitemaps and XML/CSV feeds, a media pipeline for automatically downloading images (or any other media) associated with the scraped items, a caching DNS resolver, and much more!"
    },
    {
        "link": "https://docs.scrapy.org/en/latest/intro/tutorial.html",
        "document": "In this tutorial, we’ll assume that Scrapy is already installed on your system. If that’s not the case, see Installation guide.\n\nWe are going to scrape quotes.toscrape.com, a website that lists quotes from famous authors.\n\nThis tutorial will walk you through these tasks:\n• None Exporting the scraped data using the command line\n\nScrapy is written in Python. The more you learn about Python, the more you can get out of Scrapy.\n\nIf you’re already familiar with other languages and want to learn Python quickly, the Python Tutorial is a good resource.\n\nIf you’re new to programming and want to start with Python, the following books may be useful to you:\n• None How To Think Like a Computer Scientist\n\nYou can also take a look at this list of Python resources for non-programmers, as well as the suggested resources in the learnpython-subreddit.\n\nSpiders are classes that you define and that Scrapy uses to scrape information from a website (or a group of websites). They must subclass and define the initial requests to be made, and optionally, how to follow links in pages and parse the downloaded page content to extract data. This is the code for our first Spider. Save it in a file named under the directory in your project: As you can see, our Spider subclasses and defines some attributes and methods:\n• None : identifies the Spider. It must be unique within a project, that is, you can’t set the same name for different Spiders.\n• None : must return an iterable of Requests (you can return a list of requests or write a generator function) which the Spider will begin to crawl from. Subsequent requests will be generated successively from these initial requests.\n• None : a method that will be called to handle the response downloaded for each of the requests made. The response parameter is an instance of that holds the page content and has further helpful methods to handle it. The method usually parses the response, extracting the scraped data as dicts and also finding new URLs to follow and creating new requests ( ) from them. How to run our spider¶ To put our spider to work, go to the project’s top level directory and run: This command runs the spider named that we’ve just added, that will send some requests for the domain. You will get an output similar to this: Now, check the files in the current directory. You should notice that two new files have been created: quotes-1.html and quotes-2.html, with the content for the respective URLs, as our method instructs. If you are wondering why we haven’t parsed the HTML yet, hold on, we will cover that soon. What just happened under the hood?¶ Scrapy schedules the objects returned by the method of the Spider. Upon receiving a response for each one, it instantiates objects and calls the callback method associated with the request (in this case, the method) passing the response as an argument. Instead of implementing a method that generates objects from URLs, you can just define a class attribute with a list of URLs. This list will then be used by the default implementation of to create the initial requests for your spider. The method will be called to handle each of the requests for those URLs, even though we haven’t explicitly told Scrapy to do so. This happens because is Scrapy’s default callback method, which is called for requests without an explicitly assigned callback. The best way to learn how to extract data with Scrapy is trying selectors using the Scrapy shell. Run: Remember to always enclose URLs in quotes when running Scrapy shell from the command line, otherwise URLs containing arguments (i.e. character) will not work. On Windows, use double quotes instead: You will see something like: Using the shell, you can try selecting elements using CSS with the response object: The result of running is a list-like object called , which represents a list of objects that wrap around XML/HTML elements and allow you to run further queries to refine the selection or extract the data. To extract the text from the title above, you can do: There are two things to note here: one is that we’ve added to the CSS query, to mean we want to select only the text elements directly inside element. If we don’t specify , we’d get the full title element, including its tags: The other thing is that the result of calling is a list: it is possible that a selector returns more than one result, so we extract them all. When you know you just want the first result, as in this case, you can do: As an alternative, you could’ve written: Accessing an index on a instance will raise an exception if there are no results: You might want to use directly on the instance instead, which returns if there are no results: There’s a lesson here: for most scraping code, you want it to be resilient to errors due to things not being found on a page, so that even if some parts fail to be scraped, you can at least get some data. Besides the and methods, you can also use the method to extract using regular expressions: In order to find the proper CSS selectors to use, you might find it useful to open the response page from the shell in your web browser using . You can use your browser’s developer tools to inspect the HTML and come up with a selector (see Using your browser’s Developer Tools for scraping). Selector Gadget is also a nice tool to quickly find CSS selector for visually selected elements, which works in many browsers. Besides CSS, Scrapy selectors also support using XPath expressions: XPath expressions are very powerful, and are the foundation of Scrapy Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You can see that if you read the text representation of the selector objects in the shell closely. While perhaps not as popular as CSS selectors, XPath expressions offer more power because besides navigating the structure, it can also look at the content. Using XPath, you’re able to select things like: the link that contains the text “Next Page”. This makes XPath very fitting to the task of scraping, and we encourage you to learn XPath even if you already know how to construct CSS selectors, it will make scraping much easier. We won’t cover much of XPath here, but you can read more about using XPath with Scrapy Selectors here. To learn more about XPath, we recommend this tutorial to learn XPath through examples, and this tutorial to learn “how to think in XPath”. Now that you know a bit about selection and extraction, let’s complete our spider by writing the code to extract the quotes from the web page. Each quote in https://quotes.toscrape.com is represented by HTML elements that look like this: “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.” by Albert Einstein (about) Tags: change deep-thoughts thinking world Let’s open up scrapy shell and play a bit to find out how to extract the data we want: We get a list of selectors for the quote HTML elements with: Each of the selectors returned by the query above allows us to run further queries over their sub-elements. Let’s assign the first selector to a variable, so that we can run our CSS selectors directly on a particular quote: Now, let’s extract the , and from that quote using the object we just created: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”' Given that the tags are a list of strings, we can use the method to get all of them: Having figured out how to extract each bit, we can now iterate over all the quote elements and put them together into a Python dictionary: {'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']} {'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']} Let’s get back to our spider. Until now, it hasn’t extracted any data in particular, just saving the whole HTML page to a local file. Let’s integrate the extraction logic above into our spider. A Scrapy spider typically generates many dictionaries containing the data extracted from the page. To do that, we use the Python keyword in the callback, as you can see below: To run this spider, exit the scrapy shell by entering: Now, it should output the extracted data with the log: '“It is better to be hated for what you are than to be loved for what you are not.”' \"“I have not failed. I've just found 10,000 ways that won't work.”\"\n\nThe simplest way to store the scraped data is by using Feed exports, with the following command: That will generate a file containing all scraped items, serialized in JSON. The command-line switch overwrites any existing file; use instead to append new content to any existing file. However, appending to a JSON file makes the file contents invalid JSON. When appending to a file, consider using a different serialization format, such as JSON Lines: The JSON Lines format is useful because it’s stream-like, so you can easily append new records to it. It doesn’t have the same problem as JSON when you run twice. Also, as each record is a separate line, you can process big files without having to fit everything in memory, there are tools like JQ to help do that at the command-line. In small projects (like the one in this tutorial), that should be enough. However, if you want to perform more complex things with the scraped items, you can write an Item Pipeline. A placeholder file for Item Pipelines has been set up for you when the project is created, in . Though you don’t need to implement any item pipelines if you just want to store the scraped items.\n\nLet’s say, instead of just scraping the stuff from the first two pages from https://quotes.toscrape.com, you want quotes from all the pages in the website. Now that you know how to extract data from pages, let’s see how to follow links from them. The first thing to do is extract the link to the page we want to follow. Examining our page, we can see there is a link to the next page with the following markup: We can try extracting it in the shell: This gets the anchor element, but we want the attribute . For that, Scrapy supports a CSS extension that lets you select the attribute contents, like this: There is also an property available (see Selecting element attributes for more): Now let’s see our spider, modified to recursively follow the link to the next page, extracting data from it: Now, after extracting the data, the method looks for the link to the next page, builds a full absolute URL using the method (since the links can be relative) and yields a new request to the next page, registering itself as callback to handle the data extraction for the next page and to keep the crawling going through all the pages. What you see here is Scrapy’s mechanism of following links: when you yield a Request in a callback method, Scrapy will schedule that request to be sent and register a callback method to be executed when that request finishes. Using this, you can build complex crawlers that follow links according to rules you define, and extract different kinds of data depending on the page it’s visiting. In our example, it creates a sort of loop, following all the links to the next page until it doesn’t find one – handy for crawling blogs, forums and other sites with pagination. As a shortcut for creating Request objects you can use : Unlike scrapy.Request, supports relative URLs directly - no need to call urljoin. Note that just returns a Request instance; you still have to yield this Request. You can also pass a selector to instead of a string; this selector should extract necessary attributes: For elements there is a shortcut: uses their href attribute automatically. So the code can be shortened further: To create multiple requests from an iterable, you can use instead: or, shortening it further: Here is another spider that illustrates callbacks and following links, this time for scraping author information: This spider will start from the main page, it will follow all the links to the authors pages calling the callback for each of them, and also the pagination links with the callback as we saw before. Here we’re passing callbacks to as positional arguments to make the code shorter; it also works for . The callback defines a helper function to extract and cleanup the data from a CSS query and yields the Python dict with the author data. Another interesting thing this spider demonstrates is that, even if there are many quotes from the same author, we don’t need to worry about visiting the same author page multiple times. By default, Scrapy filters out duplicated requests to URLs already visited, avoiding the problem of hitting servers too much because of a programming mistake. This can be configured in the setting. Hopefully by now you have a good understanding of how to use the mechanism of following links and callbacks with Scrapy. As yet another example spider that leverages the mechanism of following links, check out the class for a generic spider that implements a small rules engine that you can use to write your crawlers on top of it. Also, a common pattern is to build an item with data from more than one page, using a trick to pass additional data to the callbacks."
    },
    {
        "link": "https://scrapy.org",
        "document": "An open source and collaborative framework for extracting the data you need from websites.\n\nIn a fast, simple, yet extensible way.\n\nMaintained by Zyte and many other contributors"
    },
    {
        "link": "https://scrapfly.io/blog/web-scraping-with-scrapy",
        "document": "How to Find All URLs on a Domain\n\nLearn how to efficiently find all URLs on a domain using Python and web crawling. Guide on how to crawl entire domain to collect all website data"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes",
        "document": "Text preprocessing is an essential step in natural language processing (NLP) that involves cleaning and transforming unstructured text data to prepare it for analysis. It includes tokenization, stemming, lemmatization, stop-word removal, and part-of-speech tagging. In this article, we will introduce the basics of text preprocessing and provide Python code examples to illustrate how to implement these tasks using the NLTK library. By the end of the article, readers will better understand how to prepare text data for NLP tasks.\n• Learn about the essential steps in text preprocessing using Python, including tokenization, stemming, lemmatization, and stop-word removal.\n• Discover the importance of text preprocessing in improving data quality and reducing noise for effective NLP analysis.\n• With practical code examples, you can learn how to clean and prepare text data using Python and the NLTK library.\n• Explore the differences between stemming and lemmatization and their impact on word meaning and context.\n• Understand the application of preprocessing techniques on SMS spam data to prepare it for model building.\n\nThis article was published as a part of the Data Science Blogathon\n\nWhat is Text Preprocessing in NLP?\n\nNatural Language Processing (NLP) is a branch of Data Science that deals with text data. Apart from numerical data, text data is available to a great extent and is used to analyze and solve business problems. However, before using the data for analysis or prediction, processing the data is important.\n\nWe perform text preprocessing to prepare the text data for the model building. It is the very first step of NLP projects. Some of the preprocessing steps are:\n\nText preprocessing is crucial in natural language processing (NLP) for several reasons:\n\nText preprocessing is crucial in preparing text data for NLP tasks. It improves data quality, reduces noise, and facilitates effective analysis and modelling.\n\nWe need to use the required steps based on our dataset. This article will use SMS spam data to understand the steps in text preprocessing in NLP using Python’s Pandas library.\n\nLet’s start by importing the Pandas library and reading the data for text processing in NLP.\n\nThe data has 5572 rows and 2 columns. You can check the shape of data using data.shape function. Let’s check the dependent variable distribution between spam and ham.\n\nNow, let us learn the steps for cleaning the data.\n\nThis step in text processing in NLP involves removing all the punctuation from the text. String library of Python contains some pre-defined list of punctuations such as ‘!”#$%&'()*+,-./:;?@[\\]^_`{|}~’\n\nWe remove all the punctuations from v2 and store them in the column, as shown in the above output.\n\nConverting the text into the same case, preferably lowercase, is one of Python’s most common text preprocessing steps. However, doing this step for text processing every time you work on an NLP problem is unnecessary, as lower casing can lead to a loss of information for some problems.\n\nFor example, when dealing with a person’s emotions in any project, words written in upper case can signify frustration or excitement.\n\nOutput: All the text of clean_msg column is converted into lowercase and stored in the msg_lower column\n\nIn this step, the text is split into smaller units. Based on our problem statement, we can use sentence or word tokenization.\n\nWe remove commonly used stopwords from the text because they do not add value to the analysis and carry little or no meaning.\n\nNLTK library consists of a list of stopwords considered stopwords in the English language. Some of them are : [i, me, my, myself, we, our, ours, ourselves, you, you’re, you’ve, you’ll, you’d, your, yours, yourself, yourselves, he, most, other, some, such, no, nor, not, only, own, same, so, then, too, very, s, t, can, will, just, don, don’t, should, should’ve, now, d, ll, m, o, re, ve, y, ain, aren’t, could, couldn’t, didn’t, didn’t]\n\nHowever, using the provided list of stopwords is unnecessary, as they should be chosen wisely based on the project. For example, ‘How’ can be a stopword for a model but can be important for some other problem where we are working on customers’ queries. We can create a customized list of stopwords for different problems.\n\nOutput: Stop words in the nltk library, such as in, until, to, I, and here, are removed from the tokenized text, and the rest are stored in the no_stopwords column.\n\nThis step, known as text standardization, stems or reduces words to their root or base form. For example, we stem words like ‘programmer,’ ‘programming,’ and ‘program’ to ‘program.’\n\nHowever, stemming can cause the root form to lose its meaning or not be reduced to a proper English word. We will see this in the steps below.\n\nOutput: In the below image, we can see how some words stem from their base.\n\nNow, let’s see how Lemmatization is different from Stemming.\n\nAlso Read: Stemming vs Lemmatization in NLP: Must-Know Differences\n\nIt stems from the word lemmatize, which means reducing the different forms of a word to one single form. However, one must ensure that it does not lose meaning. Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary while diminishing.\n\nLet us now understand the difference between after stemming and after lemmatization:\n\nOutput: The difference between stemming and lemmatization can be seen in the output below.\n\nIn the first row- crazy has been changed to crazi which has no meaning, but for lemmatization, it remained the same, i.e. crazy.\n\nIn the last row- goes has changed to goe while stemming, but for lemmatization, it has converted into go, which is meaningful.\n\nAfter performing all the text processing steps, we convert the final acquired data into numeric form using Bag of Words or TF-IDF.\n\nApart from the steps in this article, many other steps are a part of text processing NLP . Some of them are URL removal, HTML tags removal, Rare words removal, Frequent words removal, Spelling checking, and many more. You must choose the steps based on the dataset you are working on and what is necessary for the project."
    },
    {
        "link": "https://realpython.com/natural-language-processing-spacy-python",
        "document": "spaCy is a robust open-source library for Python, ideal for natural language processing (NLP) tasks. It offers built-in capabilities for tokenization, dependency parsing, and named-entity recognition, making it a popular choice for processing and analyzing text. With spaCy, you can efficiently represent unstructured text in a computer-readable format, enabling automation of text analysis and extraction of meaningful insights.\n\nBy the end of this tutorial, you’ll understand that:\n• You can use spaCy for natural language processing tasks such as part-of-speech tagging, and named-entity recognition.\n• spaCy is often preferred over NLTK for production environments due to its performance and modern design.\n• spaCy provides integration with transformer models, such as BERT.\n• You handle tokenization in spaCy by breaking text into tokens using its efficient built-in tokenizer.\n• Dependency parsing in spaCy helps you understand grammatical structures by identifying relationships between headwords and dependents.\n\nUnstructured text is produced by companies, governments, and the general population at an incredible scale. It’s often important to automate the processing and analysis of text that would be impossible for humans to process. To automate the processing and analysis of text, you need to represent the text in a format that can be understood by computers. spaCy can help you do that.\n\nIf you’re new to NLP, don’t worry! Before you start using spaCy, you’ll first learn about the foundational terms and concepts in NLP. You should be familiar with the basics in Python, though. The code in this tutorial contains dictionaries, lists, tuples, loops, comprehensions, object oriented programming, and lambda functions, among other fundamental Python concepts.\n\nIn this section, you’ll install spaCy into a virtual environment and then download data and models for the English language. You can install spaCy using , a Python package manager. It’s a good idea to use a virtual environment to avoid depending on system-wide packages. To learn more about virtual environments and , check out Using Python’s pip to Manage Your Projects’ Dependencies and Python Virtual Environments: A Primer. First, you’ll create a new virtual environment, activate it, and install spaCy. Select your operating system below to learn how: With spaCy installed in your virtual environment, you’re almost ready to get started with NLP. But there’s one more thing you’ll have to install: There are various spaCy models for different languages. The default model for the English language is designated as . Since the models are quite large, it’s best to install them separately—including all languages in one package would make the download too massive. Once the model has finished downloading, open up a Python REPL and verify that the installation has been successful: If these lines run without any errors, then it means that spaCy was installed and that the models and data were successfully downloaded. You’re now ready to dive into NLP with spaCy!\n\nIn this section, you’ll use spaCy to deconstruct a given input string, and you’ll also read the same text from a file. First, you need to load the language model instance in spaCy: The function returns a callable object, which is commonly assigned to a variable called . To start processing your input, you construct a object. A object is a sequence of objects representing a lexical token. Each object has information about a particular piece—typically one word—of text. You can instantiate a object by calling the object with the input string as an argument: \"This tutorial is about Natural Language Processing in spaCy.\" ['This', 'tutorial', 'is', 'about', 'Natural', 'Language', In the above example, the text is used to instantiate a object. From there, you can access a whole bunch of information about the processed text. For instance, you iterated over the object with a list comprehension that produces a series of objects. On each object, you called the attribute to get the text contained within that token. You won’t typically be copying and pasting text directly into the constructor, though. Instead, you’ll likely be reading it from a file: ['This', 'tutorial', 'is', 'about', 'Natural', 'Language', In this example, you read the contents of the file with the method of the object. Since the file contains the same information as the previous example, you’ll get the same result.\n\nSentence detection is the process of locating where sentences start and end in a given text. This allows you to you divide a text into linguistically meaningful units. You’ll use these units when you’re processing your text to perform tasks such as part-of-speech (POS) tagging and named-entity recognition, which you’ll come to later in the tutorial. In spaCy, the property is used to extract sentences from the object. Here’s how you would extract the total number of sentences and the sentences themselves for a given input: \" company. He is interested in learning\" He is interested in learning... In the above example, spaCy is correctly able to identify the input’s sentences. With , you get a list of objects representing individual sentences. You can also slice the objects to produce sections of a sentence. You can also customize sentence detection behavior by using custom delimiters. Here’s an example where an ellipsis ( ) is used as a delimiter, in addition to the full stop, or period ( ): \"Gus, can you, ... never mind, I forgot\" \" what I was saying. So, do you think\" \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\" never mind, I forgot what I was saying. So, do you think we should ... For this example, you used the decorator to define a new function that takes a object as an argument. The job of this function is to identify tokens in that are the beginning of sentences and mark their attribute to . Once done, the function must return the object again. Then, you can add the custom boundary function to the object by using the method. Parsing text with this modified object will now treat the word after an ellipse as the start of a new sentence.\n\nBuilding the container involves tokenizing the text. The process of tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as objects. As you’ve already seen, with spaCy, you can print the tokens by iterating over the object. But objects also have other attributes available for exploration. For instance, the token’s original index position in the string is still available as an attribute on : \" company. He is interested in learning\" In this example, you iterate over , printing both and the attribute, which represents the starting position of the token in the original text. Keeping this information could be useful for in-place word replacement down the line, for example. spaCy provides various other attributes for the class: Text with Whitespace Is Alphanum? Is Punctuation? Is Stop Word? In this example, you use f-string formatting to output a table accessing some common attributes from each in :\n• prints the token text along with any trailing space, if present.\n• indicates whether the token consists of alphabetic characters or not.\n• indicates whether the token is a punctuation symbol or not.\n• indicates whether the token is a stop word or not. You’ll be covering stop words a bit later in this tutorial. As with many aspects of spaCy, you can also customize the tokenization process to detect tokens on custom characters. This is often used for hyphenated words such as London-based. To customize tokenization, you need to update the property on the callable object with a new object. To see what’s involved, imagine you had some text that used the symbol instead of the usual hyphen ( ) as an infix to link words together. So, instead of London-based, you had London@based: \" company. He is interested in learning\" In this example, the default parsing read the London@based text as a single token, but if you used a hyphen instead of the symbol, then you’d get three tokens. To include the symbol as a custom infix, you need to build your own object: In this example, you first instantiate a new object. To build a new , you generally provide it with:\n• : A storage container for special cases, which is used to handle cases like contractions and emoticons.\n• : A function that handles preceding punctuation, such as opening parentheses.\n• : A function that handles succeeding punctuation, such as closing parentheses.\n• : A function that handles non-whitespace separators, such as hyphens.\n• : An optional Boolean function that matches strings that should never be split. It overrides the previous rules and is useful for entities like URLs or numbers. The functions involved are typically regex functions that you can access from compiled regex objects. To build the regex objects for the prefixes and suffixes—which you don’t want to customize—you can generate them with the defaults, shown on lines 5 to 10. To make a custom infix function, first you define a new list on line 12 with any regex patterns that you want to include. Then, you join your custom list with the object’s attribute, which needs to be cast to a list before joining. You want to do this to include all the existing infixes. Then you pass the extended tuple as an argument to to obtain your new regex object for infixes. When you call the constructor, you pass the method on the prefix and suffix regex objects, and the function on the infix regex object. Now you can replace the tokenizer on the object. After that’s done, you’ll see that the symbol is now tokenized separately.\n\nLemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form, or root word, is called a lemma. For example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma. The inflection of a word allows you to express different grammatical categories, like tense (organized vs organize), number (trains vs train), and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text. spaCy puts a attribute on the class. This attribute has the lemmatized form of the token: \" and several internal talks at his workplace.\" In this example, you check to see if the original word is different from the lemma, and if it is, you print both the original word and its lemma. You’ll note, for instance, that reduces to its lemma form, . If you don’t lemmatize the text, then and will be counted as different tokens, even though they both refer to the same concept. Lemmatization helps you avoid duplicate words that may overlap conceptually.\n\nYou can now convert a given text into tokens and perform statistical analysis on it. This analysis can give you various insights, such as common words or unique words in the text: \" working for a London-based Fintech company. He is\" \" There is a developer conference happening on 21 July\" ' 2019 in London. It is titled \"Applications of Natural' \" available at +44-1234567891. Gus is helping organize it.\" \" He keeps organizing local Python meetups and several\" \" internal talks at his workplace. Gus is also presenting\" ' a talk. The talk will introduce the reader about \"Use' \" Apart from his work, he is very passionate about music.\" \" Gus is learning to play the Piano. He has enrolled\" \" himself in the weekend batch of Great Piano Academy.\" \" Great Piano Academy is situated in Mayfair or the City\" \" of London and has world-class piano instructors.\" By looking just at the common words, you can probably assume that the text is about , , and . That’s a significant finding! If you can just look at the most common words, that may save you a lot of reading, because you can immediately tell if the text is about something that interests you or not. That’s not to say this process is guaranteed to give you good results. You are losing some information along the way, after all. That said, to illustrate why removing stop words can be useful, here’s another example of the same text including stop words: [('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)] Four out of five of the most common words are stop words that don’t really tell you much about the summarized text. This is why stop words are often considered noise for many applications.\n\nRule-based matching is one of the steps in extracting information from unstructured text. It’s used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech). While you can use regular expressions to extract entities (such as phone numbers), rule-based matching in spaCy is more powerful than regex alone, because you can include semantic or grammatical filters. For example, with rule-based matching, you can extract a first name and a last name, which are always proper nouns: \" company. He is interested in learning\" In this example, is a list of objects that defines the combination of tokens to be matched. Both POS tags in it are (proper noun). So, the consists of two objects in which the POS tags for both tokens should be . This pattern is then added to with the method, which takes a identifier and a list of patterns. Finally, matches are obtained with their starting and end indexes. You can also use rule-based matching to extract phone numbers: \" happening on 21 July 2019 in London. It is titled\" \" There is a helpline number available\" In this example, the pattern is updated in order to match phone numbers. Here, some attributes of the token are also used:\n• matches the exact text of the token.\n• transforms the token string to show orthographic features, standing for digit.\n• defines operators. Using as a value means that the pattern is optional, meaning it can match 0 or 1 times. Chaining together these dictionaries gives you a lot of flexibility to choose your matching criteria. Note: For simplicity, in the example, phone numbers are assumed to be of a particular format: . You can change this depending on your use case. Again, rule-based matching helps you identify and extract tokens and phrases by matching according to lexical patterns and grammatical features. This can be useful when you’re looking for a particular entity."
    },
    {
        "link": "https://sunscrapers.com/blog/9-best-python-natural-language-processing-nlp",
        "document": "Natural language processing (NLP) is a field located at the intersection of data science and Artificial Intelligence (AI) that – when boiled down to the basics – is all about teaching machines how to understand human languages and extract meaning from text. This is also why machine learning is often part of NLP projects.\n\nBut why are so many organizations interested in NLP these days? Primarily, these technologies can provide them with a broad range of valuable insights and solutions that address language-related problems consumers might experience when interacting with a product.\n\nThere’s a reason why tech giants like Google, Amazon, or Facebook are pouring millions of dollars into this line of research to power their chatbots, virtual assistants, recommendation engines, and other solutions powered by machine learning.\n\nSince NLP relies on advanced computational skills, developers need the best available tools that help to make the most of NLP approaches and algorithms for creating services that can handle natural languages.\n\nWhat is an NLP library?\n\nIn the past, only experts could be part of natural language processing projects that required superior mathematics, machine learning, and linguistics knowledge. Today, the scenario has changed. Developers can access ready-made tools that simplify text preprocessing, allowing them to focus more on building robust machine-learning models.\n\nThese tools and libraries are created to address and solve various NLP problems. Over the years, many such libraries have come to the forefront, especially in the Python ecosystem, assisting developers in delivering quality projects efficiently.\n\nWhy Use Python for Natural Language Processing (NLP)?\n\nMany things about Python make it a perfect programming language choice for an NLP project. For example, it has a simple syntax and clear semantics.\n\nMoreover, developers can enjoy great support for integrating other languages and tools that come in handy for techniques like machine learning.\n\nBut something else about this versatile language makes it an ideal technology for helping machines process natural languages. It provides developers with an extensive collection of NLP tools and libraries that enable them to handle many NLP-related tasks, such as document classification, topic modeling, part-of-speech (POS) tagging, word vectors, and sentiment analysis.\n\nHow to Leverage Python's Power for NLP?\n\nTo truly harness the capabilities of Python for NLP, it's crucial to delve into the vast array of libraries it offers.\n\nPython boasts a rich assortment of NLP libraries, from NLTK and spaCy to TextBlob. Familiarizing yourself with these resources and selecting one that aligns seamlessly with your project's objectives is paramount. Furthermore, becoming an active member of Python-NLP communities can be invaluable. Engaging in regular discussions, attending workshops, and participating in webinars can help you stay abreast of the latest developments and serve as a platform to address any queries. But the journey doesn't stop at knowing the tools; it’s about mastering them.\n\nNLTK is an essential library that supports tasks such as classification, stemming, tagging, parsing, semantic reasoning, and tokenization in Python. It's your primary tool for natural language processing and machine learning. Today it serves as an educational foundation for Python developers who are dipping their toes in this field (and machine learning).\n\nThe library was developed by Steven Bird and Edward Loper at the University of Pennsylvania and played a crucial role in breakthrough NLP research. Many universities around the globe now use NLTK, Python libraries, and other tools in their courses.\n\nThis library is pretty versatile, but I must admit that it’s also quite challenging to use for Natural Language Processing with Python. NLTK can be relatively slow and doesn’t match the demands of quick-paced production usage. The learning curve is steep, but developers can take advantage of resources like this helpful NLTK book to learn more about the concepts behind the language processing tasks this toolkit supports.\n\nTextBlob is a must for developers who are starting their journey with NLP in Python and want to make the most of their first encounter with NLTK. It provides beginners with an easy interface to help them learn the most basic NLP tasks like sentiment analysis, pos-tagging, or noun phrase extraction.\n\nWhile it streamlines many of NLTK's complexities, it does inherit its slower processing speed. But TextBlob doesn't rest on NLTK's laurels. Features like spelling correction and translation allow developers to perform NLP tasks without wading deep into intricate processes.\n\nThis library was developed at Stanford University and written in Java. It has been pivotal in academic and research settings due to its accurate natural language parsing and rich linguistic annotations.\n\nWhat is the most significant advantage of CoreNLP? The library is high-speed and works well in product development environments. It's renowned for its robustness and supports various tasks, including named entity recognition and coreference resolution.\n\nMoreover, some CoreNLP components can be integrated with NLTK, which is bound to boost the latter's efficiency.\n\nGensim is a Python library that identifies semantic similarity between two documents through vector space modeling and topic modeling toolkits. It can handle large text corpora with the help of efficient data streaming and incremental algorithms, which is more than can be said about other packages that only target batch and in-memory processing.\n\nWhat I love about it are its small memory footprint, usage optimization, and processing speed. These were achieved with the help of another Python library, NumPy. The tool's vector space modeling capabilities are also top-notch.\n\nspaCy is a relatively young library designed for production usage. That’s why it’s much more accessible than other Python NLP libraries like NLTK. spaCy offers the fastest syntactic parser available on the market today.\n\nMoreover, since the toolkit is written in Cython, it’s also really speedy and efficient.\n\nHowever, no tool is perfect. Compared to the libraries we have covered so far, spaCy supports the smallest number of languages (seven). However, the growing popularity of machine learning, NLP, and spaCy, as key libraries, means that the tool might start supporting more programming languages soon.\n\nThis slightly lesser-known library is one of my favorites because it offers a broad range of analysis and impressive language coverage. Thanks to NumPy, it also works fast.\n\nPolyglot is similar to spaCy – it’s very efficient, straightforward, and an excellent choice for projects involving a language spaCy doesn’t support. The library also stands out from the crowd because it requests a dedicated command in the command line through the pipeline mechanisms—worth a try.\n\nPolyglot is more than just an efficient library; it's a multilingual NLP library. It offers word embeddings for over 130 languages and supports tasks like named entity recognition and morphological analysis in multiple languages, making it a versatile choice for multilingual projects.\n\nThis handy NLP library provides developers with a wide range of algorithms for building machine-learning models. It offers many functions for the bag-of-words method of creating features to tackle text classification problems. The strength of this library is the intuitive class methods.\n\nHowever, the library doesn't use neural networks for text preprocessing. So if you'd like to carry out more complex preprocessing tasks like POS tagging for your text corpora, it's better to use other NLP libraries and then return to scikit-learn for building your models.\n\nWith strong community backing and extensive documentation, it remains a favorite among many developers.\n\nAnother gem in the NLP libraries Python developers use to handle natural languages. The Pattern allows part-of-speech tagging, sentiment analysis, vector space modeling, SVM, clustering, n-gram search, and WordNet. You can use a DOM parser, a web crawler, and helpful APIs like Twitter or Facebook. Still, the tool is a web miner and might not be enough to complete other natural language processing tasks.\n\nHuggingFace has been gaining prominence in Natural Language Processing (NLP) since transformers' inception. It is an AI community and Machine Learning platform created in 2016 by Julien Chaumond, Clément Delangue, and Thomas Wolf.\n\nIts goal is to provide Data Scientists, AI practitioners, and Engineers with immediate access to over 20,000 pre-trained models based on the state-of-the-art pre-trained models available from the Hugging Face hub.\n\nThese models can be applied to:\n• Text in over 100 languages for classification, information extraction, question answering, generation, generation, and translation.\n• Speech, for tasks such as object audio classification and speech recognition.\n\nHugging Face Transformers also provides almost 2000 data sets and layered APIs. Thanks to nearly 31 libraries, programmers can efficiently work with those models. Most are deep learning, such as PyTorch, TensorFlow, JAX, ONNX, Fastai, Stable-Baseline 3, etc.\n\nNote: Each of these code snippets is a simplification of real-world use cases. Before using them in actual projects, you may need to install the necessary libraries, handle edge cases, and adapt them to the specific requirements of your task.\n\nTake Advantage of Python for NLP\n\nIn the universe of natural language processing (NLP), Python shines as a gleaming star. Imagine crafting intelligent software that gracefully dances with the complexities of human languages—it's no easy feat. Yet, Python rolls out a red carpet, armed with an arsenal of powerful NLP libraries, ensuring developers are well-equipped and inspired.\n\nDive into these nine remarkable libraries; you'll see why Python is the maestro orchestrating the symphony of machine understanding of human dialects. With its seamless versatility and the dynamism of these tools, Python beckons any NLP enthusiast to create, innovate, and mesmerize. Welcome to the enthralling world of Python-powered NLP.\n\nUnleash the Power of Expertise with Sunscrapers!\n\nAt Sunscrapers, we aren't just developers; we're your dream team, a blend of unparalleled expertise and passion. Got an idea? Let's elevate it! With our adept hands-on Python, Django, and an array of technologies, your vision transforms into reality, not just any reality but one that stands out!\n\nReady to collaborate with the best? Dive into a conversation with us and discover how we can push boundaries and redefine excellence together.\n\nReach Out to Sunscrapers Now!"
    },
    {
        "link": "https://medium.com/@maleeshadesilva21/preprocessing-steps-for-natural-language-processing-nlp-a-beginners-guide-d6d9bf7689c9",
        "document": "Machine Learning heavily relies on the quality of the data fed into it, and thus, data preprocessing plays a crucial role in ensuring the accuracy and efficiency of the model. In this article, we will discuss the main text preprocessing techniques used in NLP.\n\nIn this step, we will perform fundamental actions to clean the text. These actions involve transforming all the text to lowercase, eliminating characters that do not qualify as words or whitespace, as well as removing any numerical digits present.\n\nPython is a case sensitive programming language. Therefore, to avoid any issues and ensure consistency in the processing of the text, we convert all the text to lowercase.\n\nThis way, “Free” and “free” will be treated as the same word, and our data analysis will be more accurate and reliable.\n\nWhen building a model, URLs are typically not relevant and can be removed from the text data.\n\nFor removing URLs we can use ‘regex’ library.\n\nIt is essential to remove any characters that are not considered as words or whitespace from the text dataset.\n\nThese non-word and non-whitespace characters can include punctuation marks, symbols, and other special characters that do not provide any meaningful information for our analysis.\n\nIt is important to remove all numerical digits from the text dataset. This is because, in most cases, numerical values do not provide any significant meaning to the text analysis process.\n\nMoreover, they can interfere with natural language processing algorithms, which are designed to understand and process text-based information.\n\nTokenization is the process of breaking down large blocks of text such as paragraphs and sentences into smaller, more manageable units.\n\nIn this step, we will be applying word tokenization to split the data in the ‘Message’ column into words.\n\nBy performing word tokenization, we can obtain a more accurate representation of the underlying patterns and trends present in the text data.\n\nStopwords refer to the most commonly occurring words in any natural language.\n\nFor the purpose of analyzing text data and building NLP models, these stopwords might not add much value to the meaning of the document. Therefore, removing stopwords can help us to focus on the most important information in the text and improve the accuracy of our analysis.\n\nOne of the advantages of removing stopwords is that it can reduce the size of the dataset, which in turn reduces the training time required for natural language processing models.\n\nVarious libraries such as ‘Natural Language Toolkit’ (NLTK), ‘spaCy’, and ‘Scikit-Learn’ can be used to remove stopwords.\n\nIn this example, we will use the NLTK library to remove stopwords in the ‘Message’ column of our dataset.\n\nWhat’s the difference between Stemming and Lemmatization?\n\nThere are various algorithms that can be used for stemming,\n\nLet’s take a look at how we can use ‘Porter Stemmer’ algorithm on our dataset.\n\nSome basic rules defined under the Porter Stemmer algorithm are,\n\nNext, let’s take a look at how we can implement Lemmatization for the same dataset.\n\nThe above code segments will produce outputs as shown below.\n\nNote that, we only use either Stemming or Lemmatization on our dataset based on the requirement.\n\nIn this article we discussed main preprocessing steps in building an NLP model, which include text cleaning, tokenization, stopword removal, and stemming/lemmatization. Implementing these steps can help improve model accuracy by reducing the noise in the text data and converting it into a structured format that can be easily analyzed by the model."
    },
    {
        "link": "https://spotintelligence.com/2022/12/21/nltk-preprocessing-pipeline",
        "document": "This is a complete guide on utilising NLTK to build a whole preprocessing pipeline. Take the time to read through the different components so you know how to start building your pipeline.\n\nWhat is an NLTK preprocessing pipeline?\n\nPreprocessing in Natural Language Processing (NLP) is a means to get text data ready for further processing or analysis. Most of the time, preprocessing is a mix of cleaning and normalising techniques that make the text easier to use for the task at hand.\n\nA useful library for processing text in Python is the Natural Language Toolkit (NLTK). This guide will go into 14 of the most commonly used pre-processing steps and provide code examples so you can start using the techniques immediately.\n\nBuilding an NLP pipeline might seem intimidating at first but it doesn’t have to be.\n\nAfter this we will build an NLP preprocessing pipeline completely in NLTK so that you can see how these techniques can be used together, to create a whole system.\n\nNote that every application is different and would require a different pre-processing pipeline. The key here is to understand the different building blocks so that you can put them together to build your own pipeline.\n\nWhy use NLTK for preprocessing?\n\nThe Natural Language Toolkit (NLTK) is a popular Python library for working with human language data (text). There are plenty of good reasons to use NLTK.\n• Ease of use: NLTK provides a simple and intuitive interface for performing common NLP tasks such as tokenization, stemming, and part-of-speech tagging.\n• Large collection of data and resources: NLTK includes a wide range of corpora (large collections of text data) and resources for working with them, such as lexicons, grammars, and corpora annotated with linguistic information.\n• Support for various languages: NLTK supports several languages and provides tools for working with them, including tokenizers, stemmers, and other language-specific resources e.g. Arabic.\n• Active development and community: NLTK is an actively developed library with a large and supportive community of users and contributors.\n• Compatibility with other libraries: NLTK is compatible with other popular Python libraries for data manipulation and machine learning, such as NumPy and scikit-learn, making it easy to incorporate into larger projects.\n\nIt’s useful to dig into the different components that can be used for preprocessing first. Once we understand these, we can build an entire pipeline. Some common NLP preprocessing steps include:\n\nSplitting the text into individual words or subwords (tokens).\n\nHere is how to implement tokenization in NLTK:\n\nThis will output the following list of tokens:\n\nThe function uses the Punkt tokenization algorithm, which is a widely used method for tokenizing text in multiple languages. You can also use other tokenization methods, such as splitting the text on whitespace or punctuation, but these may not be as reliable for handling complex text structures and languages.\n\nConverting all text to lowercase to make it case-insensitive. To lowercase the tokens in a list using NLTK, you can simply use the built-in method for strings:\n\nThis will output the following list of lowercased tokens:\n\nYou can also use the function to create a Text object from the tokens, which provides additional methods for text processing, such as concordancing and collocation analysis.\n\nRemoving punctuation marks simplifies the text and make it easier to process.\n\nTo remove punctuation from a list of tokens using NLTK, you can use the module to check if each token is a punctuation character. Here is an example of how to do this:\n\nThis will output the following list of tokens without punctuation:\n\nRemoving common words that do not add significant meaning to the text, such as “a,” “an,” and “the.”\n\nTo remove common stop words from a list of tokens using NLTK, you can use the function to get a list of stopwords in a specific language and filter the tokens using this list. Here is an example of how to do this:\n\nThis will output the following list of tokens without stopwords:\n\nWhite space could be spaces, tabs and newlines that don’t add value to further analysis.\n\nTo remove extra white space from a string of text using NLTK you can use the function to remove leading and trailing white space, and the function to replace multiple consecutive white space characters with a single space. Here is an example of how to do this:\n\nThis will output the following cleaned text:\n\nTo remove URLs from a string of text using NLTK, you can use a regular expression pattern to identify URLs and replace them with an empty string. Here is an example of how to do this:\n\nThis will output the following text without URLs:\n\nTo remove HTML code from a string of text using NLTK, you can use a regular expression pattern to identify HTML tags and replace them with an empty string. Here is an example of how to do this:\n\nThis will output the following text without HTML code:\n\nTo remove frequent words (also known as “high-frequency words”) from a list of tokens using NLTK, you can use the function to calculate the frequency of each word and filter out the most common ones. Here is an example of how to do this:"
    }
]