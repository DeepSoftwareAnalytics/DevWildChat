[
    {
        "link": "https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html",
        "document": "In the constructor we instantiate four parameters and assign them as In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. Just like any class in Python, you can also define custom method on PyTorch modules # Construct our model by instantiating the class defined above # Construct our loss function and an Optimizer. The call to model.parameters() # in the SGD constructor will contain the learnable parameters (defined # with torch.nn.Parameter) which are members of the model. # Forward pass: Compute predicted y by passing x to the model # Zero gradients, perform a backward pass, and update the weights."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.Module.html",
        "document": "Your models should also subclass this class.\n\nModules can also contain other Modules, allowing them to be nested in a tree structure. You can assign the submodules as regular attributes:\n\nSubmodules assigned in this way will be registered, and will also have their parameters converted when you call , etc.\n\nThe hook will be called every time before is invoked. If is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature: If is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature:\n• None hook (Callable) – The user defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. Default:\n• None with_kwargs (bool) – If true, the will be passed the kwargs given to the forward function. Default: a handle that can be used to remove the added hook by calling\n\nThe hook will be called every time the gradients with respect to a module are computed, i.e. the hook will execute if and only if the gradients with respect to module outputs are computed. The hook should have the following signature: The and are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of in subsequent computations. will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in and will be for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module’s forward function. Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.\n• None hook (Callable) – The user-defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. a handle that can be used to remove the added hook by calling\n\nThe hook will be called every time the gradients for the module are computed. The hook should have the following signature: The is a tuple. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the output that will be used in place of in subsequent computations. Entries in will be for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module’s forward function. Modifying inputs inplace is not allowed when using backward hooks and will raise an error.\n• None hook (Callable) – The user-defined hook to be registered.\n• None prepend (bool) – If true, the provided will be fired before all existing hooks on this . Otherwise, the provided will be fired after all existing hooks on this . Note that global hooks registered with will fire before all hooks registered by this method. a handle that can be used to remove the added hook by calling"
    },
    {
        "link": "https://pytorch.org/docs/stable/notes/modules.html",
        "document": "PyTorch uses modules to represent neural networks. Modules are:\n• None Building blocks of stateful computation. PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for easy construction of elaborate, multi-layer neural networks.\n• None Tightly integrated with PyTorch’s autograd system. Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update.\n• None Easy to work with and transform. Modules are straightforward to save and restore, transfer between CPU / GPU / TPU devices, prune, quantize, and more.\n\nThis note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch, many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents are provided here as well.\n\nTo get started, let’s look at a simpler, custom version of PyTorch’s module. This module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules:\n• None It inherits from the base Module class. All modules should subclass for composability with other modules.\n• None It defines some “state” that is used in computation. Here, the state consists of randomly-initialized and tensors that define the affine transformation. Because each of these is defined as a , they are registered for the module and will automatically be tracked and returned from calls to . Parameters can be considered the “learnable” aspects of the module’s computation (more on this later). Note that modules are not required to have state, and can also be stateless.\n• None It defines a forward() function that performs the computation. For this affine transformation module, the input is matrix-multiplied with the parameter (using the short-hand notation) and added to the parameter to produce the output. More generally, the implementation for a module can perform arbitrary computation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be constructed and called: Note that the module itself is callable, and that calling it invokes its function. This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module. The “forward pass” is responsible for applying the computation represented by the module to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of module outputs with respect to its inputs, which can be used for “training” parameters through gradient descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it is not required to manually implement a function for each module. The process of training module parameters through successive forward / backward passes is covered in detail in Neural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call to or , where the latter includes each parameter’s name: In general, the parameters registered by a module are aspects of the module’s computation that should be “learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers. Before we get to that, however, let’s first examine how modules can be composed with one another.\n\nModules can contain other modules, making them useful building blocks for developing more elaborate functionality. The simplest way to do this is using the module. It allows us to chain together multiple modules: Note that automatically feeds the output of the first module as input into the , and the output of that as input into the second module. As shown, it is limited to in-order chaining of modules with a single input and output. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives full flexibility on how submodules are used for a module’s computation. For example, here’s a simple neural network implemented as a custom module: This module is composed of two “children” or “submodules” ( and ) that define the layers of the neural network and are utilized for computation within the module’s method. Immediate children of a module can be iterated through via a call to or : To go deeper than just the immediate children, and recursively iterate through a module and its child modules: Sometimes, it’s necessary for a module to dynamically define submodules. The and modules are useful here; they register submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules. This means that calls to and will recursively include child parameters, allowing for convenient optimization of all parameters within the network: It’s also easy to move all parameters to a different device or change their precision using : More generally, an arbitrary function can be applied to a module and its submodules recursively by using the function. For example, to apply custom initialization to parameters of a module and its submodules: # Note that no_grad() is used here to avoid tracking this computation in the autograd graph. # Apply the function recursively on the module and its submodules. These examples show how elaborate neural networks can be formed through module composition and conveniently manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of performant modules within the namespace that perform common neural network operations like pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out:\n\nOnce a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s Optimizers from : # Create the network (from previous section) and optimizer # to output the constant zero function # After training, switch the module to eval mode to do inference, compute performance metrics, etc. # (see discussion below for a description of training and evaluation modes) In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according to its absolute value by employing as a loss function. While this is not a very interesting task, the key parts of training are present:\n• None An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s parameters are associated with it.\n• None\n• None calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the value of ‘s parameter shows that its values are now much closer to 0 (as may be expected): Note that the above process is done entirely while the network module is in “training mode”. Modules default to training mode and can be switched between training and evaluation modes using and . They can behave differently depending on which mode they are in. For example, the module maintains a running mean and variance during training that are not updated when the module is in evaluation mode. In general, modules should be in training mode during training and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module that behaves differently between the two modes: Training neural networks can often be tricky. For more information, check out:\n\nIn the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation. Now, if we want to save the trained model to disk, we can do so by saving its (i.e. “state dictionary”): # Load the module later on A module’s contains state that affects its computation. This includes, but is not limited to, the module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent” and “non-persistent”. Following is an overview of the various types of state a module can have:\n• None Parameters: learnable aspects of computation; contained within the\n• \n• None Persistent buffers: contained within the (i.e. serialized when saving & loading)\n• None Non-persistent buffers: not contained within the (i.e. left out of serialization) As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want the current value of the running mean to be considered part of the module’s so that it will be restored when loading a serialized form of the module, but we don’t want it to be learnable. This snippet shows how to use to accomplish this: Now, the current value of the running mean is considered part of the module’s and will be properly restored when loading the module from disk: # Serialized form will contain the 'mean' tensor As mentioned previously, buffers can be left out of the module’s by marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with : # Moves all module parameters and buffers to the specified device / dtype Buffers of a module can be iterated over using or . The following class demonstrates the various ways of registering parameters and buffers within a module: # Setting a nn.Parameter as an attribute of the module automatically registers the tensor # as a parameter of the module. # Reserves the \"param3\" attribute as a parameter, preventing it from being set to anything # except a parameter. \"None\" entries like this will not be present in the module's state_dict. # Registers a persistent buffer (one that appears in the module's state_dict). # Registers a non-persistent buffer (one that does not appear in the module's state_dict). # Reserves the \"buffer3\" attribute as a buffer, preventing it from being set to anything # except a buffer. \"None\" entries like this will not be present in the module's state_dict. # Adding a submodule registers its parameters as parameters of the module. # Note that non-persistent buffer \"buffer2\" and reserved attributes \"param3\" and \"buffer3\" do # not appear in the state_dict. For more information, check out:\n\nBy default, parameters and floating-point buffers for modules provided by are initialized during module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to perform well historically for the module type. For certain use cases, it may be desired to initialize with a different dtype, device (e.g. GPU), or initialization technique. Note that the device and dtype options demonstrated above also apply to any floating-point buffers registered for the module: While module writers can use any device or dtype to initialize parameters in their custom modules, good practice is to use and by default as well. Optionally, you can provide full flexibility in these areas for your custom module by conforming to the convention demonstrated above that all modules follow:\n• None Provide a constructor kwarg that applies to any parameters / buffers registered by the module.\n• None Provide a constructor kwarg that applies to any parameters / floating-point buffers registered by the module.\n• None Only use initialization functions (i.e. functions from ) on parameters and buffers within the module’s constructor. Note that this is only required to use ; see this page for an explanation. For more information, check out:\n\nIn Neural Network Training with Modules, we demonstrated the training process for a module, which iteratively performs forward and backward passes, updating module parameters each iteration. For more control over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward pass, even modifying how the pass is done if desired. Some useful examples for this functionality include debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules. PyTorch provides two types of hooks for modules:\n• None Forward hooks are called during the forward pass. They can be installed for a given module with and . These hooks will be called respectively just before the forward function is called and just after it is called. Alternatively, these hooks can be installed globally for all modules with the analogous and functions.\n• None Backward hooks are called during the backward pass. They can be installed with and . These hooks will be called when the backward for this Module has been computed. will allow the user to access the gradients for outputs while will allow the user to access the gradients both the inputs and outputs. Alternatively, they can be installed globally for all modules with and . All hooks allow the user to return an updated value that will be used throughout the remaining computation. Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or modify some inputs/outputs without having to change the module’s function. Below is an example demonstrating usage of forward and backward hooks: # Allows for examination and modification of the input before the forward pass. # Note that inputs are always wrapped in a tuple. # Allows for examination of inputs / outputs and modification of the outputs # after the forward pass. Note that inputs are always wrapped in a tuple while outputs # Allows for examination of grad_inputs / grad_outputs and modification of # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and # grad_outputs are always wrapped in tuples. # Run input through module before and after adding hooks. # Note that the modified input results in a different output. # Remove hooks; note that the output here matches the output before adding hooks."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html?highlight=nn%20parameter",
        "document": "In the constructor we instantiate four parameters and assign them as In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. Just like any class in Python, you can also define custom method on PyTorch modules # Construct our model by instantiating the class defined above # Construct our loss function and an Optimizer. The call to model.parameters() # in the SGD constructor will contain the learnable parameters (defined # with torch.nn.Parameter) which are members of the model. # Forward pass: Compute predicted y by passing x to the model # Zero gradients, perform a backward pass, and update the weights."
    },
    {
        "link": "https://geeksforgeeks.org/create-model-using-custom-module-in-pytorch",
        "document": "A custom module in PyTorch is a user-defined module that is built using the PyTorch library’s built-in neural network module, torch.nn.Module. It’s a way of creating new modules by combining and extending the functionality provided by existing PyTorch modules.\n\nThe torch.nn.Module class provides a convenient way to create custom modules because it includes some key features that are important for building neural networks, such as the ability to keep track of learnable parameters and the ability to perform automatic differentiation (for computing gradients during training).\n\nBy creating a new class that inherits from torch.nn.Module, and defining an __init__ method to initialize the module’s parameters, and forward method that perform the computation, we can create our own custom module. These custom modules can be used just like any of the built-in PyTorch modules, such as torch.nn.Module or torch.nn.Conv2d, and can be included in a larger model architecture.\n\nCreating a custom module can be useful in many situations. For example, we might create a custom module to implement a novel layer or activation function that is not included in PyTorch’s built-in modules. Or we could create a custom module that represents a more complex model, such as a sequence-to-sequence model, composed of multiple layers and other modules.\n\nWhen creating a custom data model using a custom module in PyTorch, we will need to define a subclass of the torch.nn.Module class and define the __init__() and forward() methods.\n• __init__(): The __init__ method is used to initialize the module’s parameters. This method is called when the module is created, and it allows we to set up any internal state that the module needs. For example, we might use this method to initialize the weights of a neural network or to create other modules that the module needs in order to function.\n• forward(): The forward method is used to perform the computation that the module represents. This method takes in one or more input tensors, performs computations on them, and returns the output tensors. It is a forward pass of the module.\n\nOnce defined the custom module, we can create an instance of the module and use it to train a model by defining the loss function and optimizer, and then iterating through the training data to perform the forward and backward passes and optimize the model parameters.\n\nBefore going forward with creating a custom module in Pytorch, we have to install the torch library using the following command:\n\nHere is a step-by-step example of creating a custom module in PyTorch and training it on a dataset from torchvision.datasets:\n\nIn this step, we define a custom module called MyModule by creating a new class that inherits from the nn.Module base class. In the __init__ method, define the architecture of the model by creating the necessary layers. Here, we create two linear layers, one with num_inputs and hidden_size, and the other one with hidden_size and num_outputs.\n\nDefining the forward pass: Here we define the forward pass of the model within the class by implementing the forward method. In this example, the input is passed through the first linear layer, then a relu activation function is applied to it, and then it is passed through the second linear layer.\n\nStep 4: Define the transformations for the dataset\n\nIn this step, we load the MNIST dataset with pytorch dataset from torchvision.datasets. This will download the datasets and save them in the data folder. Here we use transform to transform the dataset into pytorch tensor.\n\nThis will convert the dataset into batch size of 64.\n\nOnce we have trained our custom model on a dataset, we can use it to make predictions and evaluate its performance using a classification report. A classification report is a summary of the performance of a classification model and includes several metrics such as precision, recall, f1-score, and support."
    },
    {
        "link": "https://neptune.ai/blog/pytorch-loss-functions",
        "document": "Your neural networks can do a lot of different tasks. Whether it’s classifying data, like grouping pictures of animals into cats and dogs, regression tasks, like predicting monthly revenues, or anything else. Every task has a different output and needs a different type of loss function.\n\nThe way you configure your loss functions can make or break the performance of your algorithm. By correctly configuring the loss function, you can make sure your model will work how you want it to.\n\nLuckily for us, there are loss functions we can use to make the most of machine learning tasks.\n\nIn this article, we’ll talk about popular loss functions in PyTorch, and about building custom loss functions. Once you’re done reading, you should know which one to choose for your project.\n\nWhat are the loss functions?\n\nBefore we jump into PyTorch specifics, let’s refresh our memory of what loss functions are.\n\nLoss functions are used to gauge the error between the prediction output and the provided target value. A loss function tells us how far the algorithm model is from realizing the expected outcome. The word ‘loss’ means the penalty that the model gets for failing to yield the desired results.\n\nFor example, a loss function (let’s call it J) can take the following two parameters:\n\nThis function will determine your model’s performance by comparing its predicted output with the expected output. If the deviation between y_pred and y is very large, the loss value will be very high.\n\nIf the deviation is small or the values are nearly identical, it’ll output a very low loss value. Therefore, you need to use a loss function that can penalize a model properly when it is training on the provided dataset.\n\nLoss functions change based on the problem statement that your algorithm is trying to solve.\n\nPyTorch’s torch.nn module has multiple standard loss functions that you can use in your project.\n\nTo add them, you need to first import the libraries:\n\nNext, define the type of loss you want to use. Here’s how to define the mean absolute error loss function:\n\nAfter adding a function, you can use it to accomplish your specific task.\n\nWhich loss functions are available in PyTorch?\n\nBroadly speaking, loss functions in PyTorch are divided into two main categories: regression losses and classification losses.\n\nRegression loss functions are used when the model is predicting a continuous value, like the age of a person.\n\nClassification loss functions are used when the model is predicting a discrete value, such as whether an email is spam or not.\n\nRanking loss functions are used when the model is predicting the relative distances between inputs, such as ranking products according to their relevance on an e-commerce search page.\n\nNow we’ll explore the different types of loss functions in PyTorch, and how to use them:\n\nThe Mean Absolute Error (MAE), also called L1 Loss, computes the average of the sum of absolute differences between actual values and predicted values.\n\nIt checks the size of errors in a set of predicted values, without caring about their positive or negative direction. If the absolute values of the errors are not used, then negative values could cancel out the positive values.\n\nThe Pytorch L1 Loss is expressed as:\n\nx represents the actual value and y the predicted value.\n\nWhen could it be used?\n• Regression problems, especially when the distribution of the target variable has outliers, such as small or big values that are a great distance from the mean value. It is considered to be more robust to outliers.\n\nThe Mean Squared Error (MSE), also called L2 Loss, computes the average of the squared differences between actual values and predicted values.\n\nPytorch MSE Loss always outputs a positive result, regardless of the sign of actual and predicted values. To enhance the accuracy of the model, you should try to reduce the L2 Loss—a perfect value is 0.0.\n\nThe squaring implies that larger mistakes produce even larger errors than smaller ones. If the classifier is off by 100, the error is 10,000. If it’s off by 0.1, the error is 0.01. This punishes the model for making big mistakes and encourages small mistakes.\n\nThe Pytorch L2 Loss is expressed as:\n\nx represents the actual value and y the predicted value.\n\nWhen could it be used?\n• MSE is the default loss function for most Pytorch regression problems.\n\nThe Negative Log-Likelihood Loss function (NLL) is applied only on models with the softmax function as an output activation layer. Softmax refers to an activation function that calculates the normalized exponential function of every unit in the layer.\n\nThe Softmax function is expressed as:\n\nThe function takes an input vector of size N, and then modifies the values such that every one of them falls between 0 and 1. Furthermore, it normalizes the output such that the sum of the N values of the vector equals to 1.\n\nNLL uses a negative connotation since the probabilities (or likelihoods) vary between zero and one, and the logarithms of values in this range are negative. In the end, the loss value becomes positive.\n\nIn NLL, minimizing the loss function assists us get a better output. The negative log likelihood is retrieved from approximating the maximum likelihood estimation (MLE). This means that we try to maximize the model’s log likelihood, and as a result, minimize the NLL.\n\nIn NLL, the model is punished for making the correct prediction with smaller probabilities and encouraged for making the prediction with higher probabilities. The logarithm does the punishment.\n\nNLL does not only care about the prediction being correct but also about the model being certain about the prediction with a high score.\n\nThe Pytorch NLL Loss is expressed as:\n\nwhere x is the input, y is the target, w is the weight, and N is the batch size.\n\nWhen could it be used?\n\nThis loss function computes the difference between two probability distributions for a provided set of occurrences or random variables.\n\nIt is used to work out a score that summarizes the average difference between the predicted values and the actual values. To enhance the accuracy of the model, you should try to minimize the score—the cross-entropy score is between 0 and 1, and a perfect value is 0.\n\nOther loss functions, like the squared loss, punish incorrect predictions. Cross-Entropy penalizes greatly for being very confident and wrong.\n\nUnlike the Negative Log-Likelihood Loss, which doesn’t punish based on prediction confidence, Cross-Entropy punishes incorrect but confident predictions, as well as correct but less confident predictions.\n\nThe Cross-Entropy function has a wide range of variants, of which the most common type is the Binary Cross-Entropy (BCE). The BCE Loss is mainly used for binary classification models; that is, models having only 2 classes.\n\nThe Pytorch Cross-Entropy Loss is expressed as:\n\nWhere x is the input, y is the target, w is the weight, C is the number of classes, and N spans the mini-batch dimension.\n\nWhen could it be used?\n• Binary classification tasks, for which it’s the default loss function in Pytorch.\n• Creating confident models—the prediction will be accurate and with a higher probability.\n\nThe Hinge Embedding Loss is used for computing the loss when there is an input tensor, x, and a labels tensor, y. Target values are between {1, -1}, which makes it good for binary classification tasks.\n\nWith the Hinge Loss function, you can give more error whenever a difference exists in the sign between the actual class values and the predicted class values. This motivates examples to have the right sign.\n\nThe Hinge Embedding Loss is expressed as:\n\nWhen could it be used?\n• Classification problems, especially when determining if two inputs are dissimilar or similar.\n\nThe Margin Ranking Loss computes a criterion to predict the relative distances between inputs. This is different from other loss functions, like MSE or Cross-Entropy, which learn to predict directly from a given set of inputs.\n\nWith the Margin Ranking Loss, you can calculate the loss provided there are inputs x1, x2, as well as a label tensor, y (containing 1 or -1).\n\nWhen y == 1, the first input will be assumed as a larger value. It’ll be ranked higher than the second input. If y == -1, the second input will be ranked higher.\n\nThe Pytorch Margin Ranking Loss is expressed as:\n\nWhen could it be used?\n\nThe Triplet Margin Loss computes a criterion for measuring the triplet loss in models. With this loss function, you can calculate the loss provided there are input tensors, x1, x2, x3, as well as margin with a value greater than zero.\n\nThe Pytorch Triplet Margin Loss is expressed as:\n\nWhen could it be used?\n• It is used in content-based retrieval problems\n\nThe Kullback-Leibler Divergence, shortened to KL Divergence, computes the difference between two probability distributions.\n\nWith this loss function, you can compute the amount of lost information (expressed in bits) in case the predicted probability distribution is utilized to estimate the expected target probability distribution.\n\nIts output tells you the proximity of two probability distributions. If the predicted probability distribution is very far from the true probability distribution, it’ll lead to a big loss. If the value of KL Divergence is zero, it implies that the probability distributions are the same.\n\nKL Divergence behaves just like Cross-Entropy Loss, with a key difference in how they handle predicted and actual probability. Cross-Entropy punishes the model according to the confidence of predictions, and KL Divergence doesn’t. KL Divergence only assesses how the probability distribution prediction is different from the distribution of ground truth.\n\nThe KL Divergence Loss is expressed as:\n\nx represents the true label’s probability and y represents the predicted label’s probability.\n\nWhen could it be used?\n• If you want to make sure that the distribution of predictions is similar to that of training data\n\nHow to create a custom loss function in PyTorch?\n\nPyTorch lets you create your own custom loss functions to implement in your projects.\n\nHere’s how you can create your own simple Cross-Entropy Loss function.\n\nYou can also create other advanced PyTorch custom loss functions.\n\nLet’s modify the Dice coefficient, which computes the similarity between two samples, to act as a loss function for binary classification problems:\n\nIt is quite obvious that while training a model, one needs to keep an eye on the loss function values to track the model’s performance. As the loss value keeps decreasing, the model keeps getting better. There are a number of ways that we can do this. Let’s take a look at them.\n\nFor this, we will be training a simple Neural Network created in PyTorch which will perform classification on the famous Iris dataset.\n\nMaking the required imports for getting the dataset.\n\nScaling the dataset to have mean=0 and variance=1, gives quick model convergence.\n\nSplitting the dataset into train and test in an 80-20 ratio.\n\nMaking the necessary imports for our Neural Network and its training.\n\nDefining functions for getting accuracy and training the network.\n\nAbove, we have used print statements in the train_network function to monitor the loss as well as accuracy. Let’s see this in action:\n\nWe get an output like this:\n\nSince we’ve stored the intermediate values in lists, we can also plot the metrics using Matplotlib:\n\nThis will give us a graph that helps us analyze the correlation between loss and accuracy:\n\nThis method gets the job done. But if we train several model versions with different parameters, or have to analyze the model’s performance over time, we need a more capable experiment tracking solution.\n\nA simpler way to monitor your metrics would be to log them in a service like neptune.ai and focus on more important tasks, such as building and training the model.\n\nTo get started with Neptune, we need to install a couple of libraries:\n\nneptune is a Python SDK to authorize communication between your scripts and Neptune. We will need python-dotenv for managing environment variables, such as Neptune project name and your API token.\n\nFirst, you need to retrieve those variables from your Neptune account:\n\nHere’s how it looks in the UI.\n\n3. Copy your API key and the project name which is in the format of workspace-name/project-name.\n\n4. Return to your code editor and create a file named .env in your working directory:\n\n5. Paste the following contents into the file:\n\nNext, we need to initialize a new Neptune run using the init_run() method. The method requires your API token and the project name, which we retrieve using the os and python-dotenv libraries:\n\nrun is an instance of a Run object which we can use to log any metadata related to our ML experiments including:\n\nThe syntax to log metadata is very intuitive as the Run object can be thought of as a special dictionary (don’t run the below snippet just yet):\n\nIf you need them later on, you can retrieve the logged details using a similar syntax:\n\nIn the next sections, we will use this pattern of code to capture our model’s training process to Neptune.\n\nTo log the loss of our model, we need to add a couple of lines to the train_network function (notice the three lines where we use the run object):\n\nLet’s rerun our model training and inspect the data that ends up in our Neptune project:\n\nUsing the PyTorch integration for advanced logging\n\nFor more sophisticated logging features such as automated capture of model parameters, logging frequency configuration, and model checkpointing, you can use neptune_pytorch library:\n\nThe NeptuneLogger class requires both the run and model objects to enable logging. Then, it can automatically capture model parameters and gradients at a frequency specified by log_freq.\n\nUsing the neptune_callback object requires us to change the lines of code where the run object is used. The PyTorch integration collects all the data under a specific key of the run object defined by neptune_callback.base_namespace, so we replace run[‘key’] by run[neptune_callback.base_namespace][‘key’]. For example:\n\nWith those lines changed, we can retrain the model:\n\nYou can view the result in the Neptune app.\n\nTo stop the connection to Neptune and sync all data, call the stop() method:\n\nUsing the `neptune_pytorch` integration is the recommended method for logging PyTorch models. It gives you finer control over metadata generated during training and allows you to log more challenging artifacts such as model checkpoints and predictions in a formatted syntax.\n\nThis article covered the most common loss functions in machine learning and how to use them in PyTorch. Choosing a loss function depends on the problem type like regression, classification or ranking. If none of the functions in today’s list don’t meet your requirements, PyTorch allows creating custom loss functions as well.\n\nLoss functions are critical in informing you about the performance of your model. Therefore, you will spend a lot of time monitoring the loss and changing your model training strategy accordingly. And in our view, the best way to monitor loss is by using an experiment tracking tool such as Neptune."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html",
        "document": "For tensors of the same shape ypred​, ytrue​, where ypred​ is the and ytrue​ is the , we define the pointwise KL-divergence as\n\nTo avoid underflow issues when computing this quantity, this loss expects the argument in the log-space. The argument may also be provided in the log-space if .\n\nTo summarise, this function is roughly equivalent to computing\n\nand then reducing this result depending on the argument as\n\nAs all the other losses in PyTorch, this function expects the first argument, , to be the output of the model (e.g. the neural network) and the second, , to be the observations in the dataset. This differs from the standard mathematical notation KL(P ∣∣ Q) where P denotes the distribution of the observations and Q denotes the model.\n• None size_average (bool, optional) – Deprecated (see ). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field is set to , the losses are instead summed for each minibatch. Ignored when is . Default:\n• None reduce (bool, optional) – Deprecated (see ). By default, the losses are averaged or summed over observations for each minibatch depending on . When is , returns a loss per batch element instead and ignores . Default:\n• None reduction (str, optional) – Specifies the reduction to apply to the output. Default:\n• None log_target (bool, optional) – Specifies whether is the log space. Default:"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/pytorch-loss-functions",
        "document": "Loss functions are fundamental in ML model training, and, in most machine learning projects, there is no way to drive your model into making correct predictions without a loss function. In layman terms, a loss function is a mathematical function or expression used to measure how well a model is doing on some dataset. Knowing how well a model is doing on a particular dataset gives the developer insights into making a lot of decisions during training such as using a new, more powerful model or even changing the loss function itself to a different type. Speaking of types of loss functions, there are several of these loss functions which have been developed over the years, each suited to be used for a particular training task.\n\nThis article requires understanding neural networks. At a high level, neural networks are composed of interconnected nodes (“neurons”) organized into layers. They learn and make predicitions through a process called “training” which adjusts the weights and biases of the connections between neurons. An understanding of neural networks includes knowledge of their different layers (input layer, hidden layers, output layer), activation functions, optimization algorithms (variants of gradient descent), loss functions, etc.\n\nAdditionally, familiarity with Python syntax and the PyTorch library is essential for understanding the code snippets presented in this article.\n\nIn this article, we are going to explore different loss functions which are part of the PyTorch nn module. We will further take a deep dive into how PyTorch exposes these loss functions to users as part of its nn module API by building a custom one.\n\nNow that we have a high-level understanding of what loss functions are, let’s explore some more technical details about how loss functions work.\n\nWe stated earlier that loss functions tell us how well a model does on a particular dataset. Technically, how it does this is by measuring how close a predicted value is close to the actual value. When our model is making predictions that are very close to the actual values on both our training and testing dataset, it means we have a quite robust model.\n\nAlthough loss functions give us critical information about the performance of our model, that is not the primary function of loss function, as there are more robust techniques to assess our models such as accuracy and F-scores. The importance of loss functions is mostly realized during training, where we nudge the weights of our model in the direction that minimizes the loss. By doing so, we increase the probability of our model making correct predictions, something which probably would not have been possible without a loss function.\n\nDifferent loss functions suit different problems, each carefully crafted by researchers to ensure stable gradient flow during training.\n\nSometimes, the mathematical expressions of loss functions can be a bit daunting, and this has led to some developers treating them as black boxes. We are going to uncover some of PyTorch’s most used loss functions later, but before that, let us take a look at how we use loss functions in the world of PyTorch.\n\nPyTorch comes out of the box with a lot of canonical loss functions with simplistic design patterns that allow developers to easily iterate over these different loss functions very quickly during training. All PyTorch’s loss functions are packaged in the nn module, PyTorch’s base class for all neural networks. This makes adding a loss function into your project as easy as just adding a single line of code. Let’s look at how to add a mean squared error loss function in PyTorch.\n\nThe function returned from the code above can be used to calculate how far a prediction is from the actual value using the format below.\n\nNow that we have an idea of how to use loss functions in PyTorch, let’s dive deep into the behind the scenes of several of the loss functions PyTorch offers.\n\nWhich loss functions are available in PyTorch?\n\nA lot of these loss functions PyTorch comes with are broadly categorised into 3 groups - regression loss, classification loss and ranking loss.\n\nRegression losses are mostly concerned with continuous values which can take any value between two limits. One example of this would be predictions of the house prices of a community.\n\nClassification loss functions deal with discrete values, like the task of classifying an object as a box, pen or bottle.\n\nRanking losses predict the relative distances between values. An example of this would be face verification, where we want to know which face images belong to a particular face, and can do so by ranking which faces do and do not belong to the original face-holder via their degree of relative approximation to the target face scan.\n\nThe L1 loss function computes the mean absolute error between each value in the predicted tensor and that of the target. It first calculates the absolute difference between each value in the predicted tensor and that of the target, and computes the sum of all the values returned from each absolute difference computation. Finally, it computes the average of this sum value to obtain the mean absolute error (MAE). The L1 loss function is very robust for handling noise.\n\nThe single value returned is the computed loss between two tensors with dimension 3 by 5.\n\nThe mean squared error shares some striking similarities with MAE. Instead of computing the absolute difference between values in the prediction tensor and target, as is the case with mean absolute error, it computes the squared difference between values in the prediction tensor and that of the target tensor. By doing so, relatively large differences are penalized more, while relatively small differences are penalized less. MSE is considered less robust at handling outliers and noise than MAE, however.\n\nCross-entropy loss is used in classification problems involving a number of discrete classes. It measures the difference between two probability distributions for a given set of random variables. Usually, when using cross-entropy loss, the output of our network is a softmax layer, which ensures that the output of the neural network is a probability value (value between 0-1).\n\nThe softmax layer consists of two parts - the exponent of the prediction for a particular class.\n\nyi is the output of the neural network for a particular class. The output of this function is a number close to zero, but never zero, if yi is large and negative, and closer to 1 if yi is positive and very large.\n\nThe second part is a normalization value and is used to ensure that the output of the softmax layer is always a probability value.\n\nThis is obtained by summing all the exponents of each class value. The final equation of softmax looks like this:\n\nIn PyTorch’s nn module, cross-entropy loss combines log-softmax and negative log-likelihood (NLL) loss into a single loss function.\n\nNotice how the gradient function in the printed output is an NLL loss. This actually reveals that cross-entropy loss combines NLL loss under the hood with a log-softmax layer.\n\nThe NLL loss function works quite similarly to the cross-entropy loss function. Cross-entropy loss combines a log-softmax layer and NLL loss to obtain the value of the cross-entropy loss. This means that NLL loss can be used to obtain the cross-entropy loss value by having the last layer of the neural network be a log-softmax layer instead of a normal softmax layer.\n\nBinary cross-entropy loss is a special class of cross-entropy losses used for the special problem of classifying data points into only two classes. Labels for this type of problem are usually binary, and our goal is therefore to push the model to predict a number close to zero for a zero label and a number close to one for a one label. Usually when using BCE loss for binary classification, the output of the neural network is a sigmoid layer to ensure that the output is either a value close to zero or a value close to one.\n\nWe mentioned in the previous section that a binary cross-entropy loss is usually output as a sigmoid layer to ensure that output is between 0 and 1. A binary cross-entropy loss with logits combines these two layers into just one layer. According to the PyTorch documentation, this is a more numerically stable version as it takes advantage of the log-sum exp trick.\n\nThe smooth L1 loss function combines the benefits of MSE loss and MAE loss through a heuristic value beta. This criterion was introduced in the Fast R-CNN paper. When the absolute difference between the ground truth value and the predicted value is below beta, the criterion uses a squared difference, much like MSE loss. The graph of MSE loss is a continuous curve, which means the gradient at each loss value varies and can be derived everywhere. Moreover, as the loss value reduces the gradient diminishes, which is convenient during gradient descent. However for very large loss values the gradient explodes, hence the criterion for switching to MAE, for which the gradient is almost constant for every loss value, when the absolute difference becomes larger than beta and the potential gradient explosion is eliminated.\n\nHinge embedding loss is mostly used in semi-supervised learning tasks to measure the similarity between two inputs. It’s used when there is an input tensor and a label tensor containing values of 1 or -1. It is mostly used in problems involving non-linear embeddings and semi-supervised learning.\n\nMargin ranking loss belongs to the ranking losses whose main objective, unlike other loss functions, is to measure the relative distance between a set of inputs in a dataset. The margin ranking loss function takes two inputs and a label containing only 1 or -1. If the label is 1, then it is assumed that the first input should have a higher ranking than the second input and if the label is -1, it is assumed that the second input should have a higher ranking than the first input. This relationship is shown by the equation and code below.\n\nThis criterion measures similarity between data points by using triplets of the training data sample. The triplets involved are an anchor sample, a positive sample and a negative sample. The objective is 1) to get the distance between the positive sample and the anchor as minimal as possible, and 2) to get the distance between the anchor and the negative sample to have greater than a margin value plus the distance between the positive sample and the anchor. Usually, the positive sample belongs to the same class as the anchor, but the negative sample does not. Hence, by using this loss function, we aim to use triplet margin loss to predict a high similarity value between the anchor and the positive sample and a low similarity value between the anchor and the negative sample.\n\nCosine embedding loss measures the loss given inputs x1, x2, and a label tensor y containing values 1 or -1. It is used for measuring the degree to which two inputs are similar or dissimilar.\n\nThe criterion measures similarity by computing the cosine distance between the two data points in space. The cosine distance correlates to the angle between the two points which means that the smaller the angle, the closer the inputs and hence the more similar they are.\n\nGiven two distributions, P and Q, Kullback-Leibler (KL) divergence loss measures how much information is lost when P (assumed to be the true distributions) is replaced with Q. By measuring how much information is lost when we use Q to approximate P, we are able to obtain the similarity between P and Q and hence drive our algorithm to produce a distribution very close to the true distribution, P. The information loss when Q is used to approximate P is not the same when P is used to approximate Q, and thus KL divergence is not symmetric.\n\nPyTorch provides us with two popular ways to build our own loss function to suit our problem; these are namely using a class implementation and using a function implementation. Let’s see how we can implement both methods starting with the function implementation.\n\nThis is easily the simplest way to write your own custom loss function. It’s just as easy as creating a function, passing into it the required inputs and other parameters, performing some operation using PyTorch’s core API or Functional API, and returning a value. Let’s see a demonstration with custom mean squared error.\n\nIn the code above, we define a custom loss function to calculate the mean squared error given a prediction tensor and a target tensor\n\nWe can compute the loss using our custom loss function and PyTorch’s MSE loss function to observe that we have obtained the same results.\n\nThis approach is probably the standard and recommended method of defining custom losses in PyTorch. The loss function is created as a node in the neural network graph by subclassing the nn module. This means that our custom loss function is a PyTorch layer exactly the same way a convolutional layer is. Let’s see a demonstration of how this works with a custom MSE loss.\n\nWe have discussed a lot about loss functions available in PyTorch and also taken a deep dive into the inner workings of most of these loss functions. Choosing the right loss function for a particular problem can be an overwhelming task. Hopefully, this tutorial alongside the official PyTorch documentation serves as a guideline when trying to understand which loss function suits your problem well."
    },
    {
        "link": "https://pytorch.org/docs/stable/nn.html",
        "document": "These are the basic building blocks for graphs:\n\nApplies a 1D convolution over an input signal composed of several input planes. Applies a 2D convolution over an input signal composed of several input planes. Applies a 3D convolution over an input signal composed of several input planes. Applies a 1D transposed convolution operator over an input image composed of several input planes. Applies a 2D transposed convolution operator over an input image composed of several input planes. Applies a 3D transposed convolution operator over an input image composed of several input planes. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. Combines an array of sliding local blocks into a large containing tensor.\n\nApplies a 1D max pooling over an input signal composed of several input planes. Applies a 2D max pooling over an input signal composed of several input planes. Applies a 3D max pooling over an input signal composed of several input planes. Applies a 1D average pooling over an input signal composed of several input planes. Applies a 2D average pooling over an input signal composed of several input planes. Applies a 3D average pooling over an input signal composed of several input planes. Applies a 2D fractional max pooling over an input signal composed of several input planes. Applies a 3D fractional max pooling over an input signal composed of several input planes. Applies a 1D power-average pooling over an input signal composed of several input planes. Applies a 2D power-average pooling over an input signal composed of several input planes. Applies a 3D power-average pooling over an input signal composed of several input planes. Applies a 1D adaptive max pooling over an input signal composed of several input planes. Applies a 2D adaptive max pooling over an input signal composed of several input planes. Applies a 3D adaptive max pooling over an input signal composed of several input planes. Applies a 1D adaptive average pooling over an input signal composed of several input planes. Applies a 2D adaptive average pooling over an input signal composed of several input planes. Applies a 3D adaptive average pooling over an input signal composed of several input planes.\n\nPads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary.\n\nCreates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y. Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y. This criterion computes the cross entropy loss between input logits and target. Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities: This loss combines a layer and the in one single class. Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch or 0D , and a label 1D mini-batch or 0D y (containing 1 or -1). Measures the loss given an input tensor x and a labels tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 2D of target class indices). Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x and target y of size (N,C). Creates a criterion that measures the loss given input tensors x1​, x2​ and a label y with values 1 or -1. Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 1D tensor of target class indices, 0≤y≤x.size(1)−1): Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. Creates a criterion that measures the triplet loss given input tensors a, p, and n (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\nClip the gradient norm of an iterable of parameters. Clip the gradient norm of an iterable of parameters. Clip the gradients of an iterable of parameters at specified value. Compute the norm of an iterable of tensors. Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm. Utility functions to flatten and unflatten Module parameters to and from a single vector. Flatten an iterable of parameters into a single vector. Copy slices of a vector into an iterable of parameters. Fuse a convolutional module and a BatchNorm module into a single, new convolutional module. Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters. Fuse a linear module and a BatchNorm module into a single, new linear module. Fuse linear module parameters and BatchNorm module parameters into new linear module parameters. Convert of to The conversion recursively applies to nested , including . Utility functions to apply and remove weight normalization from Module parameters. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers. Abstract base class for creation of new pruning techniques. Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. Prune (currently unpruned) units in a tensor at random. Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. Prune entire (currently unpruned) channels in a tensor at random. Prune entire (currently unpruned) channels in a tensor based on their L -norm. Prune tensor by removing units with the lowest L1-norm. Prune tensor by removing random channels along the specified dimension. Prune tensor by removing channels with the lowest L -norm along the specified dimension. Globally prunes tensors corresponding to all parameters in by applying the specified . Prune tensor corresponding to parameter called in by applying the pre-computed mask in . Remove the pruning reparameterization from a module and the pruning method from the forward hook. Check if a module is pruned by looking for pruning pre-hooks. Parametrizations implemented using the new parametrization functionality in . Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Remove the parametrizations on a tensor in a module. Context manager that enables the caching system within parametrizations registered with . A sequential container that holds and manages the original parameters or buffers of a parametrized . Utility functions to call a given Module in a stateless manner. Perform a functional call on the module by replacing the module parameters and buffers with the provided ones. Holds the data and list of of a packed sequence."
    },
    {
        "link": "https://medium.com/we-talk-data/mastering-kl-divergence-in-pytorch-4d0be6d7b6e3",
        "document": "You’ve probably encountered KL divergence countless times in your deep learning journey — its central role in model training, especially in fields like generative models and policy optimization, makes it indispensable. But while the concept is familiar, the nitty-gritty of implementing and customizing it in PyTorch might still hold some surprises. This guide is built to change that. Here, we’ll go straight to the heart of applying KL divergence in PyTorch with practical, hands-on code. We’ll skip over definitions and unnecessary theory — this is a high-level, practical approach meant for readers like you who already know their way around deep learning. In the following sections, you’ll find:\n• Clear, advanced examples of using in real-world settings.\n• Step-by-step instructions on building custom distributions and calculating KL divergence when standard methods fall short.\n• Expert techniques for optimizing with KL divergence in models like Variational Autoencoders (VAEs) and reinforcement learning setups. By the end, you’ll be equipped with everything you need to make KL divergence work exactly as you want it to in PyTorch. Let’s get started.\n\nNow, why does KL divergence play such a vital role in deep learning? Here’s the deal: KL divergence measures how one probability distribution diverges from another. This concept is critical in models where you need to enforce similarity or dissimilarity between two distributions, like in VAEs or reinforcement learning. In Variational Autoencoders, for instance, KL divergence acts as a guiding force in the loss function, ensuring the learned distribution doesn’t stray too far from a target (typically Gaussian) distribution. Without KL divergence here, a VAE would lack the structure needed to generate new data points that resemble the training data. You can think of it as the “compass” keeping the encoder’s learned distribution aligned with the prior. Another fascinating application is in policy learning for reinforcement learning. Take Proximal Policy Optimization (PPO): here, KL divergence acts as a constraint, limiting how much the policy can change in a single update. Without this, your policy might oscillate wildly, becoming unstable and hard to train. Let’s look at some code to solidify these ideas.\n\nIn PyTorch, provides a straightforward way to compute KL divergence between different probability distributions. This built-in function is both versatile and efficient, capable of handling various distribution types, including Gaussian, Bernoulli, and Categorical. To get you started, here’s a practical example of calculating the KL divergence between two Gaussian distributions — common in tasks like VAEs: import torch\n\nimport torch.distributions as dist\n\n\n\n# Define two Gaussian distributions\n\np = dist.Normal(loc=0.0, scale=1.0) # Target distribution\n\nq = dist.Normal(loc=1.0, scale=1.5) # Learned distribution\n\n\n\n# Compute KL divergence\n\nkl_divergence = dist.kl_divergence(p, q)\n\nprint(\"KL Divergence between two Gaussian distributions:\", kl_divergence.item()) In this example, represents the target distribution, while is the distribution our model has learned. The KL divergence here provides a measure of how much deviates from . This value is key in guiding the model to adjust closer to , especially useful in generative models. PyTorch also supports KL divergence calculations between other distribution types, like Bernoulli or Categorical. Here’s an example of calculating KL divergence between two Categorical distributions — a common task in discrete latent variable models. # Define two categorical distributions\n\np_cat = dist.Categorical(probs=torch.tensor([0.1, 0.3, 0.6]))\n\nq_cat = dist.Categorical(probs=torch.tensor([0.25, 0.25, 0.5]))\n\n\n\n# Compute KL divergence\n\nkl_divergence_cat = dist.kl_divergence(p_cat, q_cat)\n\nprint(\"KL Divergence between two Categorical distributions:\", kl_divergence_cat.item()) This might surprise you: KL divergence values can be quite sensitive to even minor shifts in distribution probabilities, especially in discrete distributions. Always consider these nuances when applying KL divergence in models, as small tweaks in distribution parameters can lead to large changes in divergence values.\n\nThere are times when the built-in function won’t quite cut it—perhaps your model involves a complex, custom distribution that isn’t covered by PyTorch’s native options. In such cases, you’ll want to define a custom distribution class and implement KL divergence manually. Here’s how you can define a custom distribution and calculate its KL divergence with another distribution in PyTorch: class CustomDistribution(dist.Distribution):\n\n def __init__(self, mean, stddev):\n\n self.mean = mean\n\n self.stddev = stddev\n\n\n\n def sample(self, sample_shape=torch.Size()):\n\n return self.mean + self.stddev * torch.randn(sample_shape)\n\n\n\n def log_prob(self, value):\n\n var = self.stddev ** 2\n\n log_scale = self.stddev.log()\n\n return -((value - self.mean) ** 2) / (2 * var) - log_scale - torch.log(torch.sqrt(2 * torch.pi))\n\n\n\n# Instantiate two custom distributions\n\np_custom = CustomDistribution(mean=torch.tensor(0.0), stddev=torch.tensor(1.0))\n\nq_custom = CustomDistribution(mean=torch.tensor(1.0), stddev=torch.tensor(1.5))\n\n\n\n# Manually compute KL divergence\n\nkl_divergence_custom = (p_custom.mean - q_custom.mean) ** 2 / (2 * q_custom.stddev ** 2) + \\\n\n torch.log(q_custom.stddev / p_custom.stddev) - 0.5\n\n\n\nprint(\"Custom KL Divergence:\", kl_divergence_custom.item()) In this example, we’ve defined a custom Gaussian-like distribution, calculating KL divergence based on the difference in means and variances. By integrating this into your PyTorch pipeline, you gain fine-grained control over the behavior of KL divergence, which is invaluable in advanced model architectures. In your models, you can now apply this custom KL divergence as part of your loss function or as a regularizer to keep the learned distribution in check. This level of customization is particularly useful in scenarios where standard distributions aren’t sufficient or when you need an additional level of precision. With this foundation, you’re now well-equipped to handle both standard and custom KL divergence calculations in PyTorch. Let’s continue exploring how this applies to specific model types and advanced use cases in the next sections.\n\nVAEs bring something unique to the table, blending probabilistic models with deep learning. At the heart of this structure lies KL divergence, which shapes the latent space by ensuring that the learned distribution doesn’t stray too far from a prior distribution (usually Gaussian). Let’s look at how to incorporate KL divergence effectively in the VAE loss function and introduce KL annealing to optimize model performance over time. In a VAE, the objective is to maximize the Evidence Lower Bound (ELBO), where KL divergence acts as a regularizer in the loss function. By enforcing this term, we encourage the latent representation to stay close to a Gaussian prior, facilitating coherent generation and smoother latent spaces. Here’s an advanced implementation of the VAE loss function, incorporating both the reconstruction loss and KL divergence:\n• The reconstruction loss measures how close the reconstructed data is to the original input, often with binary cross-entropy.\n• The KL divergence term penalizes deviation from the prior, calculated using the mean and log variance from the encoder. Now, let’s integrate this loss into a VAE training loop: # Training loop for a VAE\n\nfor epoch in range(num_epochs):\n\n for x in data_loader:\n\n x = x.to(device)\n\n \n\n # Forward pass through the VAE\n\n reconstructed_x, mean, log_var = model(x)\n\n \n\n # Calculate loss\n\n loss = vae_loss_function(reconstructed_x, x, mean, log_var)\n\n \n\n # Backpropagation and optimization step\n\n optimizer.zero_grad()\n\n loss.backward()\n\n optimizer.step()\n\n \n\n print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\") This setup gives you a solid framework for training a VAE. By backpropagating through both the reconstruction and KL divergence terms, the model gradually learns a latent space that’s both useful and well-aligned with the prior. KL annealing is a clever trick used to gradually “turn up” the effect of KL divergence over epochs. This prevents the model from getting overwhelmed by the KL penalty early on, allowing it to first focus on learning meaningful representations before enforcing the prior alignment more strongly. Let’s incorporate KL annealing by adding a coefficient that increases over time in the training loop: # KL annealing parameters\n\nkl_start_weight = 0.01 # Initial KL weight\n\nkl_end_weight = 1.0 # Final KL weight\n\nanneal_rate = 0.001 # Rate of increase per epoch\n\n\n\nfor epoch in range(num_epochs):\n\n kl_weight = min(kl_end_weight, kl_start_weight + anneal_rate * epoch) # Annealing logic\n\n \n\n for x in data_loader:\n\n x = x.to(device)\n\n \n\n # Forward pass through the VAE\n\n reconstructed_x, mean, log_var = model(x)\n\n \n\n # Calculate loss with annealed KL weight\n\n recon_loss = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')\n\n kl_divergence = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n\n \n\n # Adjusted loss function\n\n loss = recon_loss + kl_weight * kl_divergence\n\n \n\n # Backpropagation and optimization step\n\n optimizer.zero_grad()\n\n loss.backward()\n\n optimizer.step()\n\n \n\n print(f\"Epoch [{epoch+1}/{num_epochs}], KL Weight: {kl_weight:.3f}, Loss: {loss.item()}\") This setup dynamically scales the KL divergence term, starting small and progressively enforcing the prior alignment as training advances. This gradual increase allows the model to capture useful representations initially, then aligns with the prior to improve generation quality.\n\nIn reinforcement learning, KL divergence is indispensable for maintaining stability in policy updates, particularly in policy optimization algorithms like Proximal Policy Optimization (PPO). You might wonder: why introduce a regularization term here? The answer lies in controlling how much the policy changes between steps, avoiding drastic shifts that could destabilize training. In actor-critic and policy gradient methods, KL divergence measures the divergence between an older policy and a new, updated one. By limiting this divergence, you can prevent the policy from making overly aggressive updates. Here’s how you might calculate KL divergence between an old and new policy in PyTorch: import torch.distributions as dist\n\n\n\n# Sample actions from old and new policies\n\nold_policy = dist.Categorical(logits=old_logits)\n\nnew_policy = dist.Categorical(logits=new_logits)\n\n\n\n# Calculate KL divergence\n\nkl_divergence = torch.sum(old_policy.probs * (old_policy.logits - new_policy.logits), dim=-1)\n\nprint(\"KL Divergence between old and new policy:\", kl_divergence.mean().item()) This KL divergence acts as a penalty if the updated policy shifts too far from the original. By averaging this value across a batch, you get a reliable indicator of policy stability, essential for actor-critic algorithms where drastic policy updates can be counterproductive. PPO famously incorporates KL divergence to limit the “distance” between the old and new policies in a way that avoids dramatic swings. Here’s a code snippet illustrating how to use KL divergence as a penalty in PPO: # PPO training loop with KL penalty\n\nclip_range = 0.2\n\nkl_target = 0.01 # Desired KL target\n\nkl_penalty_coeff = 1.0 # Initial KL penalty coefficient\n\n\n\nfor epoch in range(num_epochs):\n\n for batch in data_loader:\n\n old_logits, action_taken, advantages = batch['old_logits'], batch['action_taken'], batch['advantages']\n\n old_policy = dist.Categorical(logits=old_logits)\n\n \n\n # Forward pass through the policy network to get new logits\n\n new_logits = policy_net(batch['states'])\n\n new_policy = dist.Categorical(logits=new_logits)\n\n \n\n # Calculate ratio and clipped objective\n\n ratio = torch.exp(new_policy.log_prob(action_taken) - old_policy.log_prob(action_taken))\n\n clipped_ratio = torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n\n policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n\n \n\n # KL divergence between old and new policy\n\n kl_divergence = torch.sum(old_policy.probs * (old_policy.logits - new_policy.logits), dim=-1).mean()\n\n \n\n # Adjust KL penalty coefficient based on target\n\n if kl_divergence > kl_target:\n\n kl_penalty_coeff *= 1.5 # Increase penalty if KL is too high\n\n elif kl_divergence < kl_target / 2:\n\n kl_penalty_coeff *= 0.5 # Decrease penalty if KL is too low\n\n \n\n # Total loss with KL penalty\n\n loss = policy_loss + kl_penalty_coeff * kl_divergence\n\n \n\n # Backpropagation and optimization step\n\n optimizer.zero_grad()\n\n loss.backward()\n\n optimizer.step()\n\n \n\n print(f\"Epoch [{epoch+1}/{num_epochs}], KL Penalty Coeff: {kl_penalty_coeff:.3f}, Loss: {loss.item()}\")\n• We start by calculating the KL divergence between the old and new policy, which measures how far the updated policy deviates from the original.\n• A dynamic KL penalty coefficient adjusts based on whether the divergence exceeds a target value, providing finer control over policy stability.\n• The total loss includes both the PPO objective and the KL penalty, ensuring that policy updates remain within safe bounds while maximizing reward. With these techniques, KL divergence becomes not only a metric but a guiding force for stability and control in policy optimization. Implementing it thoughtfully in RL settings allows for efficient, stable training even in complex environments.\n\nOptimizing KL divergence computations can improve model efficiency and accuracy, especially when training at scale or handling high-dimensional data. Let’s explore two best practices to ensure smooth, efficient KL divergence calculations. Choosing the Right Precision for KL Divergence Calculations Floating-point precision is often an overlooked aspect in KL divergence calculations. When distributions are closely aligned, small errors can cause large discrepancies in the divergence measure. Precision issues can particularly arise when:\n• Using very small or very large variance values, making exponential terms challenging to compute accurately.\n• Use Mixed Precision: In PyTorch, mixed precision (combining 16-bit and 32-bit floats) can balance computational speed and accuracy. This small addition ensures you avoid division by zero or extremely small values, stabilizing calculations without affecting model performance. Efficient batch-wise computation is key when training large-scale models or using GPUs. Calculating KL divergence individually for each sample can quickly become a bottleneck. Instead, let’s see how to compute KL divergence across batches for better performance: # Mean and log variance for the batch\n\nmean1_batch, log_var1_batch = model.get_latent_params(batch_x)\n\nmean2_batch, log_var2_batch = torch.zeros_like(mean1_batch), torch.zeros_like(log_var1_batch) # prior distribution\n\n\n\n# KL divergence across the entire batch\n\nkl_divergence_batch = 0.5 * torch.sum(\n\n log_var2_batch - log_var1_batch + (torch.exp(log_var1_batch) + (mean1_batch - mean2_batch).pow(2)) / torch.exp(log_var2_batch) - 1,\n\n dim=1\n\n)\n\n\n\n# Sum over batch for total KL loss\n\ntotal_kl_loss = kl_divergence_batch.sum() Batch-wise operations are optimized for GPUs and reduce memory overhead, which is particularly useful for models processing high-dimensional data or large batches. By summing over the batch dimension, you retain efficiency without compromising accuracy."
    }
]