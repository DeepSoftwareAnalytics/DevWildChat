[
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/applications/ResNet50",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nUsed in the notebooks\n\nFor image classification use cases, see this page for detailed examples.\n\nFor transfer learning use cases, make sure to read the guide to transfer learning & fine-tuning.\n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call on your inputs before passing them to the model. will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling."
    },
    {
        "link": "https://keras.io/api/applications/resnet",
        "document": "For image classification use cases, see this page for detailed examples.\n\nFor transfer learning use cases, make sure to read the guide to transfer learning & fine-tuning.\n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call on your inputs before passing them to the model. will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n• include_top: whether to include the fully-connected layer at the top of the network.\n• weights: one of (random initialization), (pre-training on ImageNet), or the path to the weights file to be loaded.\n• input_tensor: optional Keras tensor (i.e. output of ) to use as image input for the model.\n• input_shape: optional shape tuple, only to be specified if is (otherwise the input shape has to be (with data format) or (with data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. would be one valid value.\n• pooling: Optional pooling mode for feature extraction when is .\n• means that the output of the model will be the 4D tensor output of the last convolutional block.\n• means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor.\n• means that global max pooling will be applied.\n• classes: optional number of classes to classify images into, only to be specified if is , and if no argument is specified. Defaults to .\n• classifier_activation: A or callable. The activation function to use on the \"top\" layer. Ignored unless . Set to return the logits of the \"top\" layer. When loading pretrained weights, can only be or .\n• name: The name of the model (string).\n\nFor image classification use cases, see this page for detailed examples.\n\nFor transfer learning use cases, make sure to read the guide to transfer learning & fine-tuning.\n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call on your inputs before passing them to the model. will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n• include_top: whether to include the fully-connected layer at the top of the network.\n• weights: one of (random initialization), (pre-training on ImageNet), or the path to the weights file to be loaded.\n• input_tensor: optional Keras tensor (i.e. output of ) to use as image input for the model.\n• input_shape: optional shape tuple, only to be specified if is (otherwise the input shape has to be (with data format) or (with data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. would be one valid value.\n• pooling: Optional pooling mode for feature extraction when is .\n• means that the output of the model will be the 4D tensor output of the last convolutional block.\n• means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor.\n• means that global max pooling will be applied.\n• classes: optional number of classes to classify images into, only to be specified if is , and if no argument is specified. Defaults to .\n• classifier_activation: A or callable. The activation function to use on the \"top\" layer. Ignored unless . Set to return the logits of the \"top\" layer. When loading pretrained weights, can only be or .\n• name: The name of the model (string).\n\nFor image classification use cases, see this page for detailed examples.\n\nFor transfer learning use cases, make sure to read the guide to transfer learning & fine-tuning.\n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call on your inputs before passing them to the model. will convert the input images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling.\n• include_top: whether to include the fully-connected layer at the top of the network.\n• weights: one of (random initialization), (pre-training on ImageNet), or the path to the weights file to be loaded.\n• input_tensor: optional Keras tensor (i.e. output of ) to use as image input for the model.\n• input_shape: optional shape tuple, only to be specified if is (otherwise the input shape has to be (with data format) or (with data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. would be one valid value.\n• pooling: Optional pooling mode for feature extraction when is .\n• means that the output of the model will be the 4D tensor output of the last convolutional block.\n• means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor.\n• means that global max pooling will be applied.\n• classes: optional number of classes to classify images into, only to be specified if is , and if no argument is specified. Defaults to .\n• classifier_activation: A or callable. The activation function to use on the \"top\" layer. Ignored unless . Set to return the logits of the \"top\" layer. When loading pretrained weights, can only be or .\n• name: The name of the model (string).\n\nFor image classification use cases, see this page for detailed examples.\n\nFor transfer learning use cases, make sure to read the guide to transfer learning & fine-tuning.\n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call on your inputs before passing them to the model. will scale input pixels between -1 and 1.\n• include_top: whether to include the fully-connected layer at the top of the network.\n• weights: one of (random initialization), (pre-training on ImageNet), or the path to the weights file to be loaded.\n• input_tensor: optional Keras tensor (i.e. output of ) to use as image input for the model.\n• input_shape: optional shape tuple, only to be specified if is (otherwise the input shape has to be (with data format) or (with data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. would be one valid value.\n• pooling: Optional pooling mode for feature extraction when is .\n• means that the output of the model will be the 4D tensor output of the last convolutional block.\n• means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor.\n• means that global max pooling will be applied.\n• classes: optional number of classes to classify images into, only to be specified if is , and if no argument is specified.\n• classifier_activation: A or callable. The activation function to use on the \"top\" layer. Ignored unless . Set to return the logits of the \"top\" layer. When loading pretrained weights, can only be or .\n• name: The name of the model (string).\n\nFor image classification use cases, see this page for detailed examples.\n\nFor transfer learning use cases, make sure to read the guide to transfer learning & fine-tuning.\n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call on your inputs before passing them to the model. will scale input pixels between -1 and 1.\n• include_top: whether to include the fully-connected layer at the top of the network.\n• weights: one of (random initialization), (pre-training on ImageNet), or the path to the weights file to be loaded.\n• input_tensor: optional Keras tensor (i.e. output of ) to use as image input for the model.\n• input_shape: optional shape tuple, only to be specified if is (otherwise the input shape has to be (with data format) or (with data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. would be one valid value.\n• pooling: Optional pooling mode for feature extraction when is .\n• means that the output of the model will be the 4D tensor output of the last convolutional block.\n• means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor.\n• means that global max pooling will be applied.\n• classes: optional number of classes to classify images into, only to be specified if is , and if no argument is specified.\n• classifier_activation: A or callable. The activation function to use on the \"top\" layer. Ignored unless . Set to return the logits of the \"top\" layer. When loading pretrained weights, can only be or .\n• name: The name of the model (string).\n\nFor image classification use cases, see this page for detailed examples.\n\nFor transfer learning use cases, make sure to read the guide to transfer learning & fine-tuning.\n\nNote: each Keras Application expects a specific kind of input preprocessing. For ResNet, call on your inputs before passing them to the model. will scale input pixels between -1 and 1.\n• include_top: whether to include the fully-connected layer at the top of the network.\n• weights: one of (random initialization), (pre-training on ImageNet), or the path to the weights file to be loaded.\n• input_tensor: optional Keras tensor (i.e. output of ) to use as image input for the model.\n• input_shape: optional shape tuple, only to be specified if is (otherwise the input shape has to be (with data format) or (with data format). It should have exactly 3 inputs channels, and width and height should be no smaller than 32. E.g. would be one valid value.\n• pooling: Optional pooling mode for feature extraction when is .\n• means that the output of the model will be the 4D tensor output of the last convolutional block.\n• means that global average pooling will be applied to the output of the last convolutional block, and thus the output of the model will be a 2D tensor.\n• means that global max pooling will be applied.\n• classes: optional number of classes to classify images into, only to be specified if is , and if no argument is specified.\n• classifier_activation: A or callable. The activation function to use on the \"top\" layer. Ignored unless . Set to return the logits of the \"top\" layer. When loading pretrained weights, can only be or .\n• name: The name of the model (string)."
    },
    {
        "link": "https://keras.io/api/applications",
        "document": "Keras Applications are deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning.\n\nWeights are downloaded automatically when instantiating a model. They are stored at .\n\nUpon instantiation, the models will be built according to the image data format set in your Keras configuration file at . For instance, if you have set , then any model loaded from this repository will get built according to the data format convention \"Height-Width-Depth\".\n\nThe top-1 and top-5 accuracy refers to the model's performance on the ImageNet validation dataset.\n\nDepth refers to the topological depth of the network. This includes activation layers, batch normalization layers etc.\n\nTime per inference step is the average of 30 batches and 10 repetitions.\n\nDepth counts the number of layers with parameters.\n\nExtract features from an arbitrary intermediate layer with VGG19\n\nFine-tune InceptionV3 on a new set of classes\n\n# and a logistic layer -- let's say we have 200 classes # this is the model we will train # first: train only the top layers (which were randomly initialized) # compile the model (should be done *after* setting layers to non-trainable) # train the model on the new data for a few epochs # at this point, the top layers are well trained and we can start fine-tuning # convolutional layers from inception V3. We will freeze the bottom N layers # let's visualize layer names and layer indices to see how many layers # we chose to train the top 2 inception blocks, i.e. we will freeze # the first 249 layers and unfreeze the rest: # we need to recompile the model for these modifications to take effect # we use SGD with a low learning rate # we train our model again (this time fine-tuning the top 2 inception blocks"
    },
    {
        "link": "https://stackoverflow.com/questions/68082626/resnet50-transfer-learning-with-tensorflow-keras-version-2-4",
        "document": "I have a model architecture based on a resnet50 that needs to be retrained regularly. It worked for years. It is running on tensorflow version 1.9 and keras 2.3.1. Now I bought a new computer with a RTX 3070 - which means I have to use tensorflow 2.4 or higher in order to make use of the GPU. I installed tensorflow 2.5 together with the relevant Cuda 11.2 and cudnn 8.1, manually copied some files - and the model is indeed running on GPU. However, when I freeze layers of the base model, I get completely different results as compared to when I run it on my old computer. For example: For two warm-up epochs with all layers of the resnet50 frozen, I get more than 50 percent accuracy on my old computer - but only 7.5 on the new one. I am aware of the problems with BatchNormalization layers and followed the tutorial (or instruction) here:\n\nhow to solve the issue (as you can see in the code below). I also tried to downgrade tensorflow to 2.4, reinstalled Anaconda and set everything up from scratch, etc. - but nothing works. To compare the two architectures and in order make sure that no other reason could be responsible for the discrepancy, I copied the entire data on an external hard drive - and only adjusted the imports from keras to tensorflow.keras (together with some other small alterations necessary to use tensorflow.keras, i.e. using fit instead of fit_generator, etc.). Could someone look at the code for the tensorflow.keras model (second code block from top) - and tell me where I go wrong.\n\nHere is the code for the model in keras (which works perfectly):\n\nAnd here is the code for the tensorflow.keras model (which does NOT work):\n\nthe striking thing is: When I do NOT freeze layer, the performance of the tensorflow.keras model is comparable to the one of the keras model. As I said, I do not know where I gor wrong here. Any help is appreciated, Thank you very much for your answers!"
    },
    {
        "link": "https://restack.io/p/transfer-learning-answer-resnet50-tensorflow-example-cat-ai",
        "document": "To set up ResNet50 with TensorFlow, you can leverage the Keras API, which simplifies the process of building and training deep learning models. Below is a step-by-step guide to get you started.\n\nFirst, ensure you have TensorFlow installed. You can install it using pip:\n\nNext, import the necessary libraries:\n\nYou can load the ResNet50 model with pre-trained weights from ImageNet. This is done using the following code:\n\nThis command initializes the model with weights that have been trained on the ImageNet dataset, which contains over a million images.\n\nTo train or fine-tune the model, you need to prepare your dataset. Use the class to augment your images and create batches:\n\nBefore training, compile the model with an optimizer and loss function. For example:\n\nNow, you can train the model using the method:\n\nAfter training, evaluate the model's performance on a validation dataset:\n\nBy following these steps, you can effectively set up and train the ResNet50 model using TensorFlow. This model is powerful for various image classification tasks and can be fine-tuned for specific datasets to improve accuracy. For more detailed information, refer to the official TensorFlow documentation on ResNet50."
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nUsed in the notebooks\n\nApplies a transformation to an image according to given parameters.\n\nFits the data generator to some sample data.\n\nThis computes the internal data stats related to the data-dependent transformations, based on an array of sample data.\n\nOnly required if or or are set to .\n\nWhen is set to a value, rescaling is applied to sample data before computing the internal data stats.\n\nApplies the normalization configuration in-place to a batch of inputs.\n\nis changed in-place since the function is mainly used internally to standardize images and feed them to your network. If a copy of would be created instead it would have a significant performance cost. If you want to apply this method without changing the input in-place you can call the method creating a copy before:"
    },
    {
        "link": "https://stackoverflow.com/questions/70080062/how-to-correctly-use-imagedatagenerator-in-keras",
        "document": "I am playing with augmentation of data in Keras lately and I am using basic ImageDataGenerator. I learned the hard way it is actually a generator, not iterator (because gives I thought it is an iterator). I also checked few blogs about using it, but they don't answer all my questions.\n\nSo, I loaded my data like this:\n\nAnd to train my model I did the following:\n\nAnd it worked. I am a bit confused how it works, because is generator, so it should give infinitely big dataset. And documentation says:\n\nWhich I didn't do, yet, it works. Does it somehow infer number of steps? Also, does it use only augmented data, or it also uses non-augmented images in batch?\n\nSo basically, my question is how to use this generator correctly with function to have all data in my training set, including original, non-augmented images and augmented images, and to cycle through it several times/steps (right now it seems it does only one step per epoch)?"
    },
    {
        "link": "https://tensorflow.org/tutorials/images/data_augmentation",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nThis tutorial demonstrates data augmentation: a technique to increase the diversity of your training set by applying random (but realistic) transformations, such as image rotation.\n\nYou will learn how to apply data augmentation in two ways:\n• Use the Keras preprocessing layers, such as , , , and .\n• Use the methods, such as , , , , and .\n\nThis tutorial uses the tf_flowers dataset. For convenience, download the dataset using TensorFlow Datasets. If you would like to learn about other ways of importing data, check out the load images tutorial.\n\nThe flowers dataset has five classes.\n\nLet's retrieve an image from the dataset and use it to demonstrate data augmentation.\n\nYou can use the Keras preprocessing layers to resize your images to a consistent shape (with ), and to rescale pixel values (with ).\n\nYou can visualize the result of applying these layers to an image.\n\nVerify that the pixels are in the range:\n\nYou can use the Keras preprocessing layers for data augmentation as well, such as and .\n\nLet's create a few preprocessing layers and apply them repeatedly to the same image.\n\nThere are a variety of preprocessing layers you can use for data augmentation including , , , and others.\n\nTwo options to use the Keras preprocessing layers\n\nThere are two ways you can use these preprocessing layers, with important trade-offs.\n\nOption 1: Make the preprocessing layers part of your model\n\nThere are two important points to be aware of in this case:\n• None Data augmentation will run on-device, synchronously with the rest of your layers, and benefit from GPU acceleration.\n• None When you export your model using , the preprocessing layers will be saved along with the rest of your model. If you later deploy this model, it will automatically standardize images (according to the configuration of your layers). This can save you from the effort of having to reimplement that logic server-side.\n\nOption 2: Apply the preprocessing layers to your dataset\n\nWith this approach, you use to create a dataset that yields batches of augmented images. In this case:\n• Data augmentation will happen asynchronously on the CPU, and is non-blocking. You can overlap the training of your model on the GPU with data preprocessing, using , shown below.\n• In this case the preprocessing layers will not be exported with the model when you call . You will need to attach them to your model before saving it or reimplement them server-side. After training, you can attach the preprocessing layers before export.\n\nYou can find an example of the first option in the Image classification tutorial. Let's demonstrate the second option here.\n\nApply the preprocessing layers to the datasets\n\nConfigure the training, validation, and test datasets with the Keras preprocessing layers you created earlier. You will also configure the datasets for performance, using parallel reads and buffered prefetching to yield batches from disk without I/O become blocking. (Learn more dataset performance in the Better performance with the tf.data API guide.)\n\nFor completeness, you will now train a model using the datasets you have just prepared.\n\nThe Sequential model consists of three convolution blocks ( ) with a max pooling layer ( ) in each of them. There's a fully-connected layer ( ) with 128 units on top of it that is activated by a ReLU activation function ( ). This model has not been tuned for accuracy (the goal is to show you the mechanics).\n\nChoose the optimizer and loss function. To view training and validation accuracy for each training epoch, pass the argument to .\n\nYou can also create custom data augmentation layers.\n\nThis section of the tutorial shows two ways of doing so:\n• First, you will create a layer. This is a good way to write concise code.\n• Next, you will write a new layer via subclassing, which gives you more control.\n\nBoth layers will randomly invert the colors in an image, according to some probability.\n\nBoth of these layers can be used as described in options 1 and 2 above.\n\nThe above Keras preprocessing utilities are convenient. But, for finer control, you can write your own data augmentation pipelines or layers using and . (You may also want to check out TensorFlow Addons Image: Operations and TensorFlow I/O: Color Space Conversions.)\n\nSince the flowers dataset was previously configured with data augmentation, let's reimport it to start fresh:\n\nRetrieve an image to work with:\n\nLet's use the following function to visualize and compare the original and augmented images side-by-side:\n\nFlip an image either vertically or horizontally with :\n\nYou can grayscale an image with :\n\nSaturate an image with by providing a saturation factor:\n\nChange the brightness of image with by providing a brightness factor:\n\nCrop the image from center up to the image part you desire using :\n\nRotate an image by 90 degrees with :\n\nApplying random transformations to the images can further help generalize and expand the dataset. The current API provides eight such random image operations (ops):\n\nThese random image ops are purely functional: the output only depends on the input. This makes them simple to use in high performance, deterministic input pipelines. They require a value be input each step. Given the same , they return the same results independent of how many times they are called.\n\nIn the following sections, you will:\n• Go over examples of using random image operations to transform an image.\n• Demonstrate how to apply random transformations to a training dataset.\n\nRandomly change the brightness of using by providing a brightness factor and . The brightness factor is chosen randomly in the range and is associated with the given .\n\nRandomly change the contrast of using by providing a contrast range and . The contrast range is chosen randomly in the interval and is associated with the given .\n\nRandomly crop using by providing target and . The portion that gets cropped out of is at a randomly chosen offset and is associated with the given .\n\nLet's first download the image dataset again in case they are modified in the previous sections.\n\nNext, define a utility function for resizing and rescaling the images. This function will be used in unifying the size and scale of images in the dataset:\n\nLet's also define the function that can apply the random transformations to the images. This function will be used on the dataset in the next step.\n\nCreate a object (let's call it ) and the dataset with . This will ensure that each image in the dataset gets associated with a unique value (of shape ) based on which later can get passed into the function as the value for random transformations.\n\nMap the function to the training dataset:\n• Create a object with an initial value. Calling the function on the same generator object always returns a new, unique value.\n• Define a wrapper function that: 1) calls the function; and 2) passes the newly generated value into the function for random transformations.\n\nMap the wrapper function to the training dataset, and the function—to the validation and test sets:\n\nThese datasets can now be used to train a model as shown previously.\n\nThis tutorial demonstrated data augmentation using Keras preprocessing layers and .\n• To learn how to include preprocessing layers inside your model, refer to the Image classification tutorial.\n• You may also be interested in learning how preprocessing layers can help you classify text, as shown in the Basic text classification tutorial.\n• You can learn more about in this guide, and you can learn how to configure your input pipelines for performance here."
    },
    {
        "link": "https://datumo.com/en/blog/tech/preprocessing-and-augmenting-images-for-classification-in-tensorflow-keras",
        "document": "Image preprocessing and augmentation are important steps before you train statistical algorithms e.g machine learning models on images. Image preprocessing and augmentation can help increase dataset size by generating similar copies of images. It can also improve data versatility which helps improve model generalisation and reduces overfitting. In this article, you will see how to preprocess and augment input images using TensorFlow keras library. The article also demonstrates how image preprocessing can help improve a deep learning model performance in TensorFlow keras.\n\nThe sample dataset for this article can be downloaded from this Kaggle Link: The dataset consists of images of cats and dogs. Once you download the dataset, you will see that it contains three folders: test, train, and validation. Each of these three folders further consist of two sub-folders: cats and dogs, which contain unique images of cats, and dogs, respectively. The directory structure for your dataset looks like this:\n\nIn this section, you will see how to preprocess and apply augmentation to in-memory images using TensorFlow keras. To preprocess and augment images with TensorFlow keras, you can use the Image submodule from the https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ module. The ImageDataGenerator class from the image module preprocesses and augments images on the fly as you will see in the upcoming sections. The following script imports this module along with the other libraries required to run codes in this section. import numpy as np\n\n\n\nimport matplotlib.pyplot as plt from keras.preprocessing import image The script below imports an image of a dog from the train dataset. The image is then displayed using the “imshow()” method. To apply preprocessing via the Keras.preprocessing.image module, you need to convert an input image into a numpy array. You can do so using the “img_to_array()” method as shown below: The script above plots the image shape. You can see the height, width, and the colour channels from the above output. Most of the keras.preprocessing.image module functionalities including the ImageDataGenerator, expect the images in the format (number of samples, height, width, number of colour channels). In our case, we have only one image and hence one sample. The following script adds the number of samples dimension to the input image. Let’s now see some of the most common preprocessing and image augmentation features of the ImageDataGenerator class.\n\nIn this section, you will train a convolutional neural network for the classification of cats and dog images in your dataset. We will be using the default images without any preprocessing and augmentation to see what is the maximum prediction accuracy that our model can achieve when we do not apply any preprocessing or augmentation to our images. In the previous section you saw how to preprocess and augment images that reside in the computer’s memory in the form of numpy arrays. In the case of huge datasets, it is not convenient to keep all your images in the computer’s memory. Rather, the images are stored in directories on the system’s harddrive. The images are then retrieved in batches and subsequent operations are performed on it. The process of preprocessing images residing in directories is similar to preprocessing in-memory images that you saw in the previous section. However the process of loading images will be different, which you will see in this section. The following script defines variables that store the paths to train, test and validation folders in our dataset. train_images = '/content/Cats vs Dogs/train' test_images = '/content/Cats vs Dogs/test' val_images = '/content/Cats vs Dogs/validation' The script below displays the total number of images in train, test and validation folders. training_files = glob(train_images + '//.jp*g') test_files = glob(test_images + '//.jp*g') val_files = glob(val_images + '//.jp*g') print(\"Number of training images: \",len(training_files)) print(\"Number of test images: \",len(test_files)) print(\"Number of val images: \",len(val_files)) The following script creates ImageDataGenerator objects for the images in train, test, and validation folders. The only preprocessing step applied is dividing the pixel values by 255 in order to normalise the dataset. In the previous section you used the “flow()” method to create an iterator that returns images. To create an iterator that loads images from directories, you need to call the “flow_from_directory()” method as shown in the script below. You need to pass the path to the directory and the batch size as parameter values. The iterator in the script below, resizes the input images into 400 x 400 pixel images. A batch size of 16 images is processed at a time for training. And finally, the “class_mode” attribute is set to “binary” since there are only two output classes in our dataset. Finally, the script below defines our convolutional neural network model for classification. The model consists of an input layer, three convolutional and subsequent maxpool layers, followed by a dense layer and an output layer. import numpy as np import matplotlib.pyplot as plt from keras.layers import Input, Conv2D, Dense, Flatten, Dropout, MaxPool2D from keras.models import Model input = Input(shape = (400, 400, 3) ) convol1 = Conv2D(64, (3,3), strides = 2, activation= 'relu')(input) maxpool1 = MaxPool2D(2, 2)(convol1) convol2 = Conv2D(32, (3,3), strides = 2, activation= 'relu')(maxpool1) maxpool2 = MaxPool2D(2, 2)(convol2) convol3 = Conv2D(16, (3,3), strides = 2, activation= 'relu')(maxpool2) maxpool3 = MaxPool2D(pool_size = (2, 2))(convol3) flat_layer1 = Flatten()(maxpool3) dense_layer1 = Dense(256, activation = 'relu')(flat_layer1) output = Dense(1, activation= 'sigmoid')(dense_layer1) model = Model(input, output) The following script compiles the model: And the script below trains our model on the images from the training set, while the trained model is validated on the validation. The output below shows that after 20 epochs, the model achieves an accuracy of 65.80% on the validation set. The following script plots the training and validation accuracies: The plot below shows that the training accuracy reaches upto 95% while the validation accuracy is around 65%. This shows that our model is clearly overfitting. The output below shows that we get an accuracy of 66.90 % on the unseen test set.\n\nNow let’s perform classification on cats and dog images again. However, this time you will be applying preprocessing and data augmentation on the training images. It is important to mention that you should only apply image preprocessing and augmentation on your training data. As discussed earlier, preprocessing and augmenting images has two main advantages for machine learning algorithms:\n• It can help increase the dataset size by generating similar copies of existing images.\n• It increases versatility of images in a dataset which helps reduce overfitting for classification tasks. Let’s see how image preprocessing and augmentation will help improve model performance and remove overfitting. The script below creates an ImageDataGenerator object that applies horizontal flip, rotation, zooming, and shear preprocessing steps to the training images. Notice, we did not apply any preprocessing to the test and validation images. The rest of the process is similar to what you have seen in the previous section. The script below creates image iterators for train, test, and validation sets. The script below defines your convolutional neural network model. import numpy as np import matplotlib.pyplot as plt from keras.layers import Input, Conv2D, Dense, Flatten, Dropout, MaxPool2D from keras.models import Model input = Input(shape = (400, 400, 3) ) convol1 = Conv2D(64, (3,3), strides = 2, activation= 'relu')(input) maxpool1 = MaxPool2D(2, 2)(convol1) convol2 = Conv2D(32, (3,3), strides = 2, activation= 'relu')(maxpool1) maxpool2 = MaxPool2D(2, 2)(convol2) convol3 = Conv2D(16, (3,3), strides = 2, activation= 'relu')(maxpool2) maxpool3 = MaxPool2D(pool_size = (2, 2))(convol3) flat_layer1 = Flatten()(maxpool3) dense_layer1 = Dense(256, activation = 'relu')(flat_layer1) output = Dense(1, activation= 'sigmoid')(dense_layer1) model = Model(input, output) The following script compiles the model: Finally, the script below trains the model: The output shows that at the end of 20 epochs, our model achieves the validation accuracy of 74.50% which is much better than 65.80% which was achieved by the model trained on non-preprocessed images. The output below shows that the overfitting in this case is much lower compared to the previous model. At the end of 20 epochs, the training accuracy is 76.60 while the validation accuracy is 74.50%. The following script evaluates the model performance on the test set. The output above shows that a model returns an accuracy of 71.39% on the test set which is better than 66.90% achieved via model trained on images that are not preprocessed."
    },
    {
        "link": "https://pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation",
        "document": "In today’s tutorial, you will learn how to use Keras’ ImageDataGenerator class to perform data augmentation. I’ll also dispel common confusions surrounding what data augmentation is, why we use data augmentation, and what it does/does not do.\n\nHaving a dataset to practice Keras ImageDataGenerator and data augmentation is beneficial in machine learning, as it allows us to artificially increase the size and diversity of our dataset, improving the ability of models to generalize.\n\nRoboflow has free tools for each stage of the computer vision pipeline that will streamline your workflows and supercharge your productivity.\n\nSign up or Log in to your Roboflow account to access state of the art dataset libaries and revolutionize your computer vision pipeline.\n\nYou can start by choosing your own datasets or using our PyimageSearch’s assorted library of useful datasets.\n\nBring data in any of 40+ formats to Roboflow, train using any state-of-the-art model architectures, deploy across multiple platforms (API, NVIDIA, browser, iOS, etc), and connect to applications or 3rd party tools.\n\nKnowing that I was going to write a tutorial on data augmentation, two weekends ago I decided to have some fun and purposely post a semi-trick question on my Twitter feed.\n\nThe question was simple — data augmentation does which of the following?\n\nHere are the results:\n\nOnly 5% of respondents answered this trick question “correctly” (at least if you’re using Keras’ ImageDataGenerator class).\n\nAgain, it’s a trick question so that’s not exactly a fair assessment, but here’s the deal:\n\nWhile the word “augment” means to make something “greater” or “increase” something (in this case, data), the Keras ImageDataGenerator class actually works by:\n• Accepting a batch of images used for training.\n• Taking this batch and applying a series of random transformations to each image in the batch (including random rotation, resizing, shearing, etc.).\n• Replacing the original batch with the new, randomly transformed batch.\n• Training the CNN on this randomly transformed batch (i.e., the original data itself is not used for training).\n\nThat’s right — the Keras ImageDataGenerator class is not an “additive” operation. It’s not taking the original data, randomly transforming it, and then returning both the original data and transformed data.\n\nInstead, the ImageDataGenerator accepts the original data, randomly transforms it, and returns only the new, transformed data.\n\nBut remember how I said this was a trick question?\n\nTechnically, all the answers are correct — but the only way you know if a given definition of data augmentation is correct is via the context of its application.\n\nI’ll help you clear up some of the confusion regarding data augmentation (and give you the context you need to successfully apply it).\n\nInside the rest of today’s tutorial you will:\n• Learn about three types of data augmentation.\n• Dispel any confusion you have surrounding data augmentation.\n• Learn how to apply data augmentation with Keras and the class.\n\nTo learn more about data augmentation, including using Keras’ ImageDataGenerator class, just keep reading!\n\n2020-06-04 Update: This blog post is now TensorFlow 2+ compatible!\n\nWe’ll start this tutorial with a discussion of data augmentation and why we use it.\n\nI’ll then cover the three types of data augmentation you’ll see when training deep neural networks:\n• Dataset generation and data expansion via data augmentation (less common)\n\nFrom there I’ll teach you how to apply data augmentation to your own datasets (using all three methods) using Keras’ class.\n\nData augmentation encompasses a wide range of techniques used to generate “new” training samples from the original ones by applying random jitters and perturbations (but at the same time ensuring that the class labels of the data are not changed).\n\nOur goal when applying data augmentation is to increase the generalizability of the model.\n\nGiven that our network is constantly seeing new, slightly modified versions of the input data, the network is able to learn more robust features.\n\nAt testing time we do not apply data augmentation and simply evaluate our trained network on the unmodified testing data — in most cases, you’ll see an increase in testing accuracy, perhaps at the expense of a slight dip in training accuracy.\n\nLet’s consider Figure 2 (left) of a normal distribution with zero mean and unit variance.\n\nTraining a machine learning model on this data may result in us modeling the distribution exactly — however, in real-world applications, data rarely follows such a nice, neat distribution.\n\nInstead, to increase the generalizability of our classifier, we may first randomly jitter points along the distribution by adding some random values drawn from a random distribution (right).\n\nOur plot still follows an approximately normal distribution, but it’s not a perfect distribution as on the left.\n\nA model trained on this modified, augmented data is more likely to generalize to example data points not included in the training set.\n\nIn the context of computer vision, data augmentation lends itself naturally.\n\nFor example, we can obtain augmented data from the original images by applying simple geometric transforms, such as random:\n• Horizontal (and in some cases, vertical) flips\n\nApplying a (small) amount of the transformations to an input image will change its appearance slightly, but it does not change the class label — thereby making data augmentation a very natural, easy method to apply for computer vision tasks.\n\nThere are three types of data augmentation you will likely encounter when applying deep learning in the context of computer vision applications.\n\nExactly which definition of data augmentation is “correct” is entirely dependent on the context of your project/set of experiments.\n\nTake the time to read this section carefully as I see many deep learning practitioners confuse what data augmentation does and does not do.\n\nType #1: Dataset generation and expanding an existing dataset (less common)\n\nThe first type of data augmentation is what I call dataset generation or dataset expansion.\n\nAs you know machine learning models, and especially neural networks, can require quite a bit of training data — but what if you don’t have very much training data in the first place?\n\nLet’s examine the most trivial case where you only have one image and you want to apply data augmentation to create an entire dataset of images, all based on that one image.\n\nTo accomplish this task, you would:\n• Randomly transform the original image via a series of random translations, rotations, etc.\n• Take the transformed image and write it back out to disk.\n\nAfter performing this process you would have a directory full of randomly transformed “new” images that you could use for training, all based on that single input image.\n\nThis is, of course, an incredibly simplified example.\n\nYou more than likely have more than a single image — you probably have 10s or 100s of images and now your goal is to turn that smaller set into 1000s of images for training.\n\nIn those situations, dataset expansion and dataset generation may be worth exploring.\n\nBut there’s a problem with this approach — we haven’t exactly increased the ability of our model to generalize.\n\nYes, we have increased our training data by generating additional examples, but all of these examples are based on a super small dataset.\n\nKeep in mind that our neural network is only as good as the data it was trained on.\n\nWe cannot expect to train a NN on a small amount of data and then expect it to generalize to data it was never trained on and has never seen before.\n\nIf you find yourself seriously considering dataset generation and dataset expansion, you should take a step back and instead invest your time gathering additional data or looking into methods of behavioral cloning (and then applying the type of data augmentation covered in the “Combining dataset generation and in-place augmentation” section below).\n\nThe second type of data augmentation is called in-place data augmentation or on-the-fly data augmentation. This type of data augmentation is what Keras’ class implements.\n\nUsing this type of data augmentation we want to ensure that our network, when trained, sees new variations of our data at each and every epoch.\n• Step #1: An input batch of images is presented to the .\n• Step #2: The transforms each image in the batch by a series of random translations, rotations, etc.\n• Step #3: The randomly transformed batch is then returned to the calling function.\n\nThere are two important points that I want to draw your attention to:\n• The is not returning both the original data and the transformed data — the class only returns the randomly transformed data.\n• We call this “in-place” and “on-the-fly” data augmentation because this augmentation is done at training time (i.e., we are not generating these examples ahead of time/prior to training).\n\nWhen our model is being trained, we can think of our class as “intercepting” the original data, randomly transforming it, and then returning it to the neural network for training, all the while the NN has no idea the data was modified!\n\nI’ve written previous tutorials on the PyImageSearch blog where readers think that Keras’ ImageDateGenerator class is an “additive operation”, similar to the following (incorrect) figure:\n\nIn the above illustration the accepts an input batch of images, randomly transforms the batch, and then returns both the original batch and modified data — again, this is not what the Keras does. Instead, the class will return just the randomly transformed data.\n\nWhen I explain this concept to readers the next question is often:\n\nKeep in mind that the entire point of the data augmentation technique described in this section is to ensure that the network sees “new” images that it has never “seen” before at each and every epoch.\n\nIf we included the original training data along with the augmented data in each batch, then the network would “see” the original training data multiple times, effectively defeating the purpose. Secondly, recall that the overall goal of data augmentation is to increase the generalizability of the model.\n\nTo accomplish this goal we “replace” the training data with randomly transformed, augmented data.\n\nIn practice, this leads to a model that performs better on our validation/testing data but perhaps performs slightly worse on our training data (to due to the variations in data caused by the random transforms).\n\nYou’ll learn how to use the Keras class later in this tutorial.\n\nThe final type of data augmentation seeks to combine both dataset generation and in-place augmentation — you may see this type of data augmentation when performing behavioral cloning.\n\nA great example of behavioral cloning can be seen in self-driving car applications.\n\nCreating self-driving car datasets can be extremely time consuming and expensive — a way around the issue is to instead use video games and car driving simulators.\n\nVideo game graphics have become so life-like that it’s now possible to use them as training data.\n\nTherefore, instead of driving an actual vehicle, you can instead:\n• Use the underlying rendering engine of the video game\n\n…all to generate actual data that can be used for training.\n\nOnce you have your training data you can go back and apply Type #2 data augmentation (i.e., in-place/on-the-fly data augmentation) to the data you gathered via your simulation.\n\nTo configure your system for this tutorial, I first recommend following either of these tutorials:\n• How to install TensorFlow 2.0 on Ubuntu\n• How to install TensorFlow 2.0 on macOS\n\nEither tutorial will help you configure you system with all the necessary software for this blog post in a convenient Python virtual environment.\n\nPlease note that PyImageSearch does not recommend or support Windows for CV/DL projects.\n\nBefore we dive into the code let’s first review our directory structure for the project:\n\nFirst, there are two dataset directories which are not to be confused:\n• : A subset of the popular Kaggle Dogs vs. Cats competition dataset. In my curated subset, only 2,000 images (1,000 per class) are present (as opposed to the 25,000 images for the challenge).\n• : We’ll create this generated dataset using the and images which are in the parent directory. We’ll utilize data augmentation Type #1 to generate this dataset automatically and fill this directory with images.\n\nNext, we have our module which contains our implementation of the ResNet CNN classifier.\n• : Used to train models for both Type #1 and Type #2 (and optionally Type #3 if the user so wishes) data augmentation techniques. We’ll perform three training experiments resulting in each of the three files in the project folder.\n• : Used to generate a dataset from a single image using Type #1.\n\nIn the remainder of this tutorial we’ll be performing three experiments:\n• Experiment #1: Generate a dataset via dataset expansion and train a CNN on it.\n• Experiment #2: Use a subset of the Kaggle Dogs vs. Cats dataset and train a CNN without data augmentation.\n• Experiment #3: Repeat the second experiment, but this time with data augmentation.\n\nAll of these experiments will be accomplished using the same Python script.\n\nOpen up the script and let’s get started:\n\nOn Lines 2-18 our necessary packages are imported. Line 10 is our import from the Keras library — a class for data augmentation.\n\nLet’s go ahead and parse our command line arguments:\n\nOur script accepts three command line arguments via the terminal:\n• : The path to the input dataset.\n• : Whether “on-the-fly” data augmentation should be used (refer to type #2 above). By default, this method is not performed.\n• : The path to the output training history plot.\n\nLet’s proceed to initialize hyperparameters and load our image data:\n\nTraining hyperparameters, including initial learning rate, batch size, and number of epochs to train for, are initialized on Lines 32-34.\n\nFrom there Lines 39-53 grab , load images, and populate our and lists. The only image preprocessing we perform at this point is to resize each image to 64×64px.\n\nNext, let’s finish preprocessing, encode our labels, and partition our data:\n\nOn Line 57, we convert data to a NumPy array as well as scale all pixel intensities to the range [0, 1]. This completes our preprocessing.\n\nFrom there we perform “one-hot encoding” of our (Lines 61-63). This method of encoding our results in an array that may look like this:\n\nFor this sample of data, there are two cats ( ) and five dogs ( ) where the label corresponding to the image is marked as “hot”.\n\nFrom there we partition our into training and testing splits marking 75% of our data for training and the remaining 25% for testing (Lines 67 and 68).\n\nNow, we are ready to initialize our data augmentation object:\n\nLine 71 initializes our empty data augmentation object (i.e., no augmentation will be performed). This is the default operation of this script.\n\nLet’s check if we’re going to override the default with the command line argument:\n\nLine 75 checks to see if we are performing data augmentation. If so, we re-initialize the data augmentation object with random transformation parameters (Lines 77-84). As the parameters indicate, random rotations, zooms, shifts, shears, and flips will be performed during in-place/on-the-fly data augmentation.\n\n2020-06-04 Update: Formerly, TensorFlow/Keras required use of a method called in order to accomplish data augmentation. Now, the method can handle data augmentation as well, making for more-consistent code. This also applies to the migration from to . Be sure to check out my other article fit and fit_generator after you’re done reading this tutorial.\n\nLines 88-92 construct our model using Stochastic Gradient Descent optimization and learning rate decay. We use loss for this 2-class problem. If you have more than two classes, be sure to use .\n\nLines 96-100 then train our model. The object handles data augmentation in batches (although be sure to recall that the object will only perform data augmentation if the command line argument was set).\n\n2020-06-04 Update: In order for this plotting snippet to be TensorFlow 2+ compatible the dictionary keys are updated to fully spell out “accuracy” sans “acc” (i.e., and ). It is semi-confusing that “val” is not spelled out as “validation”; we have to learn to love and live with the API and always remember that it is a work in progress that many developers around the world contribute to.\n\nLine 104 makes predictions on the test set for evaluation purposes. A classification report is printed via Lines 105 and 106.\n\nFrom there, Lines 109-120 generate and save an accuracy/loss training plot.\n\nIn our first experiment, we will perform dataset expansion via data augmentation with Keras.\n\nOur dataset will contain 2 classes and initially, the dataset will trivially contain only 1 image per class:\n\nWe’ll utilize Type #1 data augmentation (see the “Type #1: Dataset generation and expanding an existing dataset” section above) to generate a new dataset with 100 images per class:\n\nAgain, this meant to be an example — in a real-world application you would have 100s of example images, but we’re keeping it simple here so you can learn the concept.\n\nBefore we can train our CNN we first need to generate an example dataset.\n\nFrom our “Project Structure” section above you know that we have two example images in our root directory: and . We will use these example images to generate 100 new training images per class (200 images in total).\n\nTo see how we can use data augmentation to generate new examples, open up the file and follow along:\n\nLines 2-6 import our necessary packages. Our is imported on Line 2 and will handle our data augmentation with Keras.\n\nFrom there, we’ll parse three command line arguments:\n• : The path to the input image. We’ll generate additional random, mutated versions of this image.\n• : The path to the output directory to store the data augmentation examples.\n• : The number of sample images to generate.\n\nLet’s go ahead and load our and initialize our data augmentation object:\n\nOur is loaded and prepared for data augmentation via Lines 21-23. Image loading and processing is handled via Keras functionality (i.e. we aren’t using OpenCV).\n\nFrom there, we initialize the object. This object will facilitate performing random rotations, zooms, shifts, shears, and flips on our input image.\n\nNext, we’ll construct a Python generator and put it to work until all of our images have been produced:\n\nWe will use the to randomly transform the input image (Lines 39 and 40). This generator saves images as .jpg files to the specified output directory contained within .\n\nFinally, we’ll loop over examples from our image data generator and count them until we’ve reached the required number of images.\n\nTo run the script make sure you have used the “Downloads” section of the tutorial to download the source code and example images.\n\nFrom there open up a terminal and execute the following command:\n\nCheck the output of the directory you will now see 100 images:\n\nLet’s do the same now for the “dogs” class:\n\nAnd now check for the dog images:\n\nA visualization of the dataset generation via data augmentation can be seen in Figure 6 at the top of this section — notice how we have accepted a single input image (of me — not of a dog or cat) and then created 100 new training examples (48 of which are visualized) from that single image.\n\nWe are now ready to perform our first experiment:\n\nOur results show that we were able to obtain 100% accuracy with little effort.\n\nOf course, this is a trivial, contrived example. In practice, you would not be taking only a single image and then building a dataset of 100s or 1000s of images via data augmentation. Instead, you would have a dataset of 100s of images and then you would apply dataset generation to that dataset — but again, the point of this section was to demonstrate on a simple example so you could understand the process.\n\nThe more popular form of (image-based) data augmentation is called in-place data augmentation (see the “Type #2: In-place/on-the-fly data augmentation” section of this post for more details).\n\nWhen performing in-place augmentation our Keras will:\n• Return the transformed batch to the network for training.\n\nWe’ll explore how data augmentation can reduce overfitting and increase the ability of our model to generalize via two experiments.\n\nTo accomplish this task we’ll be using a subset of the Kaggle Dogs vs. Cats dataset:\n\nWe’ll then train a variation of ResNet, from scratch, on this dataset with and without data augmentation.\n\nIn our first experiment we’ll perform no data augmentation:\n\nLooking at the raw classification report you’ll see that we’re obtaining 64% accuracy — but there’s a problem!\n\nTake a look at the plot associated with our training:\n\nThere is dramatic overfitting occurring — at approximately epoch 15 we see our validation loss start to rise while training loss continues to fall. By epoch 20 the rise in validation loss is especially pronounced.\n\nThis type of behavior is indicative of overfitting.\n\nThe solution is to (1) reduce model capacity, and/or (2) perform regularization.\n\nLet’s now investigate how data augmentation can act as a form of regularization:\n\nWe’re now up to 69% accuracy, an increase from our previous 64% accuracy.\n\nBut more importantly, we are no longer overfitting:\n\nNote how validation and training loss are falling together with little divergence. Similarly, classification accuracy for both the training and validation splits are growing together as well.\n\nBy using data augmentation we were able to combat overfitting!\n\nIn nearly all situations, unless you have very good reason not to, you should be performing data augmentation when training your own neural networks.\n\nIn this tutorial, you learned about data augmentation and how to apply data augmentation via Keras’ ImageDataGenerator class.\n\nYou also learned about three types of data augmentation, including:\n• Dataset generation and data expansion via data augmentation (less common).\n\nBy default, Keras’ class performs in-place/on-the-fly data augmentation, meaning that the class:\n• Accepts a batch of images used for training.\n• Takes this batch and applies a series of random transformations to each image in the batch.\n• Replaces the original batch with the new, randomly transformed batch\n• 4. Trains the CNN on this randomly transformed batch (i.e., the original data itself is not used for training).\n\nAll that said, we actually can take the class and use it for dataset generation/expansion as well — we just need to use it to generate our dataset before training.\n\nThe final method of data augmentation, combining both in-place and dataset expansion, is rarely used. In those situations, you likely have a small dataset, need to generate additional examples via data augmentation, and then have an additional augmentation/preprocessing at training time.\n\nWe wrapped up the guide by performing a number of experiments with data augmentation, noting that data augmentation is a form of regularization, enabling our network to generalize better to our testing/validation set.\n\nThis claim of data augmentation as regularization was verified in our experiments when we found that:\n• While apply data augmentation allowed for smooth training, no overfitting, and higher accuracy/lower loss\n\nYou should apply data augmentation in all of your experiments unless you have a very good reason not to.\n\nTo learn more about data augmentation, including my best practices, tips, and suggestions, be sure to take a look at my book, Deep Learning for Computer Vision with Python.\n\nTo download the source code to this post (and receive email updates when future tutorials are published here on PyImageSearch), just enter your email address in the form below!"
    }
]