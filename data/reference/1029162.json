[
    {
        "link": "https://github.com/milesial/Pytorch-UNet",
        "document": "Customized implementation of the U-Net in PyTorch for Kaggle's Carvana Image Masking Challenge from high definition images.\n\nThis model was trained from scratch with 5k images and scored a Dice coefficient of 0.988423 on over 100k test images.\n\nIt can be easily used for multiclass segmentation, portrait segmentation, medical segmentation, ...\n\nA docker image containing the code and the dependencies is available on DockerHub. You can download and jump in the container with (docker >=19.03):\n\nBy default, the is 0.5, so if you wish to obtain better results (but use more memory), set it to 1.\n\nAutomatic mixed precision is also available with the flag. Mixed precision allows the model to use less memory and to be faster on recent GPUs by using FP16 arithmetic. Enabling AMP is recommended.\n\nAfter training your model and saving it to , you can easily test the output masks on your images via the CLI.\n\nTo predict a single image and save it:\n\nTo predict a multiple images and show them without saving them:\n\nYou can specify which model file to use with .\n\nThe training progress can be visualized in real-time using Weights & Biases. Loss curves, validation curves, weights and gradient histograms, as well as predicted masks are logged to the platform.\n\nWhen launching a training, a link will be printed in the console. Click on it to go to your dashboard. If you have an existing W&B account, you can link it by setting the environment variable. If not, it will create an anonymous run which is automatically deleted after 7 days.\n\nA pretrained model is available for the Carvana dataset. It can also be loaded from torch.hub:\n\nAvailable scales are 0.5 and 1.0.\n\nThe Carvana data is available on the Kaggle website.\n\nYou can also download it using the helper script:\n\nThe input images and target masks should be in the and folders respectively (note that the and folder should not contain any sub-folder or any other files, due to the greedy data-loader). For Carvana, images are RGB and masks are black and white.\n\nYou can use your own dataset as long as you make sure it is loaded properly in ."
    },
    {
        "link": "https://medium.com/@ashishbisht0307/swin-transformer-based-unet-architecture-for-semantic-segmentation-with-pytorch-code-91e779334e8e",
        "document": "\n• The Swin-U-Net is a version of the widely used U-Net architecture that combines the windowed self-attention mechanism with the U-Net framework.\n• The Swin Transformer, which has demonstrated outstanding performance in a variety of vision applications, particularly when handling huge images, and U-Net, which is frequently used for image segmentation tasks, are combined in this architecture to provide a powerful combination.\n• The Swin-Transformer builds on the Vision-Transformer by calculating the attention limited to a local window and making use of shifted windows for providing connections between windows that significantly enhance the modeling power of the architecture.\n• The attention limitation to a local window allows it to have a linear computation complexity to input image size against the quadratic of Vision Transformer.\n• Stacking Swin-Transformer blocks allows hierarchical feature maps by merging image patches in deeper layers.\n• The shift of the window between consecutive transformer blocks allows for cross-window connection thus enabling learning of finer details required in dense prediction tasks such as object detection and semantic segmentation.\n• The Swin-Transformer can be hence pugged into the existing convolution-based U-Net architecture to improve the segmentation results.\n• In the model architecture, at any stage, we have two consecutive Swin-Transformer blocks using localized window attention instead of multiheaded attention, with the window shifting technique being utilized by the second block.\n• The encoder-decoder structure is the same as in the conventional U-Net design in the Swin-UNet.\n• In the Swin-Unet, the downsampling (max pool2X2) is replaced with patch merging.\n• Also, the upsampling (up-conv 2x2 ) is replaced with patch expansion.\n• Skip connections are used to concatenate the multi-scale features from the encoder with the up-sampled features in the decoder.\n• The encoder-decoder structure can be written as follows:\n\nclass SwinBlock(nn.Module):\n\n def __init__(self, dims, ip_res, ss_size = 3):\n\n super().__init__()\n\n self.swtb1 = SwinTransformerBlock(dim=dims, input_resolution=ip_res)\n\n self.swtb2 = SwinTransformerBlock(dim=dims, input_resolution=ip_res, shift_size=ss_size)\n\n\n\n def forward(self, x):\n\n return self.swtb2(self.swtb1(x))\n\n\n\n\n\nclass Encoder(nn.Module):\n\n def __init__(self, C, partioned_ip_res, num_blocks=3):\n\n super().__init__()\n\n H,W = partioned_ip_res[0], partioned_ip_res[1]\n\n self.enc_swin_blocks = nn.ModuleList([\n\n SwinBlock(C, (H, W)),\n\n SwinBlock(2*C, (H//2, W//2)),\n\n SwinBlock(4*C, (H//4, W//4))\n\n ])\n\n self.enc_patch_merge_blocks = nn.ModuleList([\n\n PatchMerging(C),\n\n PatchMerging(2*C),\n\n PatchMerging(4*C)\n\n ])\n\n\n\n def forward(self, x):\n\n skip_conn_ftrs = []\n\n for swin_block,patch_merger in zip(self.enc_swin_blocks, self.enc_patch_merge_blocks):\n\n x = swin_block(x)\n\n skip_conn_ftrs.append(x)\n\n x = patch_merger(x)\n\n return x, skip_conn_ftrs\n\n\n\n\n\nclass Decoder(nn.Module):\n\n def __init__(self, C, partioned_ip_res, num_blocks=3):\n\n super().__init__()\n\n H,W = partioned_ip_res[0], partioned_ip_res[1]\n\n self.dec_swin_blocks = nn.ModuleList([\n\n SwinBlock(4*C, (H//4, W//4)),\n\n SwinBlock(2*C, (H//2, W//2)),\n\n SwinBlock(C, (H, W))\n\n ])\n\n self.dec_patch_expand_blocks = nn.ModuleList([\n\n PatchExpansion(8*C),\n\n PatchExpansion(4*C),\n\n PatchExpansion(2*C)\n\n ])\n\n self.skip_conn_concat = nn.ModuleList([\n\n nn.Linear(8*C, 4*C),\n\n nn.Linear(4*C, 2*C),\n\n nn.Linear(2*C, 1*C)\n\n ])\n\n\n\n def forward(self, x, encoder_features):\n\n for patch_expand,swin_block, enc_ftr, linear_concatter in zip(self.dec_patch_expand_blocks, self.dec_swin_blocks, encoder_features,self.skip_conn_concat):\n\n x = patch_expand(x)\n\n x = torch.cat([x, enc_ftr], dim=-1)\n\n x = linear_concatter(x)\n\n x = swin_block(x)\n\n return x\n\n\n\n\n\nclass SwinUNet(nn.Module):\n\n def __init__(self, H, W, ch, C, num_class, num_blocks=3, patch_size = 4):\n\n super().__init__()\n\n self.patch_embed = PatchEmbedding(ch, C, patch_size)\n\n self.encoder = Encoder(C, (H//patch_size, W//patch_size),num_blocks)\n\n self.bottleneck = SwinBlock(C*(2**num_blocks), (H//(patch_size* (2**num_blocks)), W//(patch_size* (2**num_blocks))))\n\n self.decoder = Decoder(C, (H//patch_size, W//patch_size),num_blocks)\n\n self.final_expansion = FinalPatchExpansion(C)\n\n self.head = nn.Conv2d(C, num_class, 1,padding='same')\n\n\n\n def forward(self, x):\n\n x = self.patch_embed(x)\n\n\n\n x,skip_ftrs = self.encoder(x)\n\n\n\n x = self.bottleneck(x)\n\n\n\n x = self.decoder(x, skip_ftrs[::-1])\n\n\n\n x = self.final_expansion(x)\n\n\n\n x = self.head(x.permute(0,3,1,2))\n\n\n\n return x\n\nFor an End-to-end example of a binary semantic segmentation on benign breast ultrasound images, you can refer to the following notebook."
    },
    {
        "link": "https://github.com/qubvel-org/segmentation_models.pytorch",
        "document": "The main features of the library are:\n• Super simple high-level API (just two lines to create a neural network)\n\nVisit Read The Docs Project Page or read the following README to know more about Segmentation Models Pytorch (SMP for short) library\n\nThe segmentation model is just a PyTorch , which can be created as easy as:\n• see table with available model architectures\n• see table with available encoders and their corresponding weights\n\nAll encoders have pretrained weights. Preparing your data the same way as during weights pre-training may give you better results (higher metric score and faster convergence). It is not necessary in case you train the whole model, not only the decoder.\n\nCongratulations! You are done! Now you can train your model with your favorite framework!\n\nThe library provides a wide range of pretrained encoders (also known as backbones) for segmentation models. Instead of using features from the final layer of a classification model, we extract intermediate features and feed them into the decoder for segmentation tasks.\n\nAll encoders come with pretrained weights, which help achieve faster and more stable convergence when training segmentation models.\n\nGiven the extensive selection of supported encoders, you can choose the best one for your specific use case, for example:\n• Lightweight encoders for low-latency applications or real-time inference on edge devices (mobilenet/mobileone).\n\nBy selecting the right encoder, you can balance efficiency, performance, and model complexity to suit your project needs.\n\nAll encoders and corresponding pretrained weight are listed in the documentation:\n\nThe input channels parameter allows you to create a model that can process a tensor with an arbitrary number of channels. If you use pretrained weights from ImageNet, the weights of the first convolution will be reused:\n• For the 1-channel case, it would be a sum of the weights of the first convolution layer.\n• Otherwise, channels would be populated with weights like , and then scaled with .\n\nAll models support parameters, which is default set to . If then classification auxiliary output is not created, else model produce not only , but also output with shape . Classification head consists of GlobalPooling->Dropout(optional)->Linear->Activation(optional) layers, which can be configured by as follows:\n\nDepth parameter specify a number of downsampling operations in encoder, so you can make your model lighter if specify smaller .\n\npackage is widely used in image segmentation competitions. Here you can find competitions, names of the winners and links to their solutions.\n• Update a table (in case you added an encoder)\n\nThe project is primarily distributed under MIT License, while some files are subject to other licenses. Please refer to LICENSES and license statements in each file for careful check, especially for commercial use."
    },
    {
        "link": "https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch",
        "document": "In today’s tutorial, we will be looking at image segmentation and building our own segmentation model from scratch, based on the popular U-Net architecture.\n\nThis lesson is the last of a 3-part series on Advanced PyTorch Techniques:\n• Training an Object Detector from Scratch in PyTorch (last week’s lesson)\n\nThe computer vision community has devised various tasks, such as image classification, object detection, localization, etc., for understanding images and their content. These tasks give us a high-level understanding of the object class and its location in the image.\n\nIn Image Segmentation, we go a step further and ask our model to classify each pixel in our image to the object category it represents. This can be viewed as pixel-level image classification and is a much harder task than simple image classification, detection, or localization. Our model must automatically determine all objects and their precise location and boundaries at a pixel level in the image.\n\nThus image segmentation provides an intricate understanding of the image and is widely used in medical imaging, autonomous driving, robotic manipulation, etc.\n\nTo learn how to train a U-Net-based segmentation model in PyTorch, just keep reading.\n\nThroughout this tutorial, we will be looking at image segmentation and building and training a segmentation model in PyTorch. We will focus on a very successful architecture, U-Net, which was originally proposed for medical image segmentation. Furthermore, we will understand the salient features of the U-Net model, which make it an apt choice for the task of image segmentation.\n\nSpecifically, we will discuss the following, in detail, in this tutorial:\n• The architectural details of U-Net that make it a powerful segmentation model\n• Making predictions on novel images with our trained U-Net model\n\nThe U-Net architecture (see Figure 1) follows an encoder-decoder cascade structure, where the encoder gradually compresses information into a lower-dimensional representation. Then the decoder decodes this information back to the original image dimension. Owing to this, the architecture gets an overall U-shape, which leads to the name U-Net.\n\nIn addition to this, one of the salient features of the U-Net architecture is the skip connections (shown with grey arrows in Figure 1), which enable the flow of information from the encoder side to the decoder side, enabling the model to make better predictions.\n\nSpecifically, as we go deeper, the encoder processes information at higher levels of abstraction. This simply means that at the initial layers, the feature maps of the encoder capture low-level details about object texture and edges, and as we gradually go deeper, the features capture high-level information about object shapes and categories.\n\nIt is worth noting that to segment objects in an image, both low-level and high-level information is important. For example, a change in texture between objects and edge information can help determine the boundaries of various objects. On the other hand, high-level information about the class to which an object shape belongs can help segment corresponding pixels to correct object classes they represent.\n\nThus, to use both these pieces of information during predictions, the U-Net architecture implements skip connections between the encoder and decoder. This enables us to take intermediate feature map information from various depths on the encoder side and concatenate it at the decoder side to process and facilitate better predictions.\n\nWe will look at the U-Net model in further detail and build it from scratch in PyTorch later in this tutorial.\n\nFor this tutorial, we will use the TGS Salt Segmentation dataset. The dataset was introduced as part of the TGS Salt Identification Challenge on Kaggle.\n\nPractically, it is difficult to accurately identify the location of salt deposits from images even with the help of human experts. Therefore, the challenge required participants to help experts precisely identify the locations of salt deposits from seismic images of the earth sub-surface. This is practically important since incorrect estimates of salt presence can lead companies to set up drillers at the wrong locations for mining, leading to a waste of time and resources.\n\nWe use a sub-part of this dataset which comprises 4000 images of size pixels, taken from various locations on earth. Here, each pixel corresponds to either salt deposit or sediment. In addition to images, we are also provided with the ground-truth pixel-level segmentation masks of the same dimension as the image (see Figure 2).\n\nThe white pixels in the masks represent salt deposits, and the black pixels represent sediment. We aim to correctly predict the pixels that correspond to salt deposits in the images. Thus, we have a binary classification problem where we have to classify each pixel into one of the two classes, Class 1: Salt or Class 2: Not Salt (or, in other words, sediment).\n\nTo follow this guide, you need to have the PyTorch deep learning library, matplotlib, OpenCV, imutils, scikit-learn, and tqdm packages installed on your system.\n\nLuckily, these packages are extremely easy to install using pip:\n\nIf you need help configuring your development environment for PyTorch, I highly recommend that you read the PyTorch documentation — PyTorch’s documentation is comprehensive and will have you up and running quickly.\n\nAll that said, are you:\n• Wanting to skip the hassle of fighting with the command line, package managers, and virtual environments?\n• Ready to run the code right now on your Windows, macOS, or Linux system?\n\nGain access to Jupyter Notebooks for this tutorial and other PyImageSearch guides that are pre-configured to run on Google Colab’s ecosystem right in your web browser! No installation required.\n\nAnd best of all, these Jupyter Notebooks will run on Windows, macOS, and Linux!\n\nWe first need to review our project directory structure.\n\nStart by accessing the “Downloads” section of this tutorial to retrieve the source code and example images.\n\nFrom there, take a look at the directory structure:\n\nThe folder stores the TGS Salt Segmentation dataset we will use for training our segmentation model.\n\nFurthermore, we will be storing our trained model and training loss plots in the folder.\n\nThe file in the folder stores our code’s parameters, initial settings, and configurations.\n\nOn the other hand, the file consists of our custom segmentation dataset class, and the file contains the definition of our U-Net model.\n\nFinally, our model training and prediction codes are defined in and files, respectively.\n\nWe start by discussing the file, which stores configurations and parameter settings used in the tutorial.\n\nWe start by importing the necessary packages on Lines 2 and 3. Then, we define the path for our dataset (i.e., ) on Line 6 and the paths for images and masks within the dataset folder (i.e., and ) on Lines 9 and 10.\n\nOn Line 13, we define the fraction of the dataset we will keep aside for the test set. Then, on Line 16, we define the parameter, which determines based on availability, whether we will be using a GPU or CPU for training our segmentation model. In this case, we are using a CUDA-enabled GPU device, and we set the parameter to on Line 19.\n\nNext, we define the , , and parameters on Lines 23-25, which we will discuss in more detail later in the tutorial. Finally, on Lines 29-31, we define the training parameters such as initial learning rate (i.e., ), the total number of epochs (i.e., ), and batch size (i.e., ).\n\nOn Lines 34 and 35, we also define input image dimensions to which our images should be resized for our model to process them. We further define a threshold parameter on Line 38, which will later help us classify the pixels into one of the two classes in our binary classification-based segmentation task.\n\nFinally, we define the path to our output folder (i.e., ) on Line 41 and the corresponding paths to the trained model weights, training plots, and test images within the output folder on Lines 45-47.\n\nNow that we have defined our initial configurations and parameters, we are ready to understand the custom dataset class we will be using for our segmentation dataset.\n\nLet’s open the file from the folder in our project directory.\n\nWe begin by importing the class from the module on Line 2. This is important since all PyTorch datasets must inherit from this base dataset class. Furthermore, on Line 3, we import the OpenCV package, which will enable us to use its image handling functionalities.\n\nWe are now ready to define our own custom segmentation dataset. Each PyTorch dataset is required to inherit from class (Line 5) and should have a (Lines 13-15) and a (Lines 17-34) method. We discuss each of these methods below.\n\nWe start by defining our initializer constructor, that is, the method on Lines 6-11. The method takes as input the list of image paths (i.e., ) of our dataset, the corresponding ground-truth masks (i.e., ), and the set of transformations (i.e., ) we want to apply to our input images (Line 6).\n\nOn Lines 9-11, we initialize the attributes of our class with the parameters input to the constructor.\n\nNext, we define the method, which returns the total number of image paths in our dataset, as shown on Line 15.\n\nThe task of the method is to take an index as input (Line 17) and returns the corresponding sample from the dataset. On Line 19, we simply grab the image path at the index in our list of input image paths. Then, we load the image using OpenCV (Line 23). By default, OpenCV loads an image in the BGR format, which we convert to the RGB format as shown on Line 24. We also load the corresponding ground-truth segmentation mask in grayscale mode on Line 25.\n\nFinally, we check for input transformations that we want to apply to our dataset images (Line 28) and transform both the image and mask with the required transforms on Lines 30 and 31, respectively. This is important since we want our image and ground-truth mask to correspond and have the same dimension. On Line 34, we return the tuple containing the image and its corresponding mask (i.e., ) as shown.\n\nThis completes the definition of our custom Segmentation dataset. Next, we will discuss the implementation of the U-Net architecture.\n\nIt is time to look at our U-Net model architecture in detail and build it from scratch in PyTorch.\n\nWe open our file from the folder in our project directory and get started.\n\nOn Lines 2-11, we import the necessary layers, modules, and activation functions from PyTorch, which we will use to build our model.\n\nOverall, our U-Net model will consist of an class and a class. The encoder will gradually reduce the spatial dimension to compress information. Furthermore, it will increase the number of channels, that is, the number of feature maps at each stage, enabling our model to capture different details or features in our image. On the other hand, the decoder will take the final encoder representation and gradually increase the spatial dimension and reduce the number of channels to finally output a segmentation mask of the same spatial dimension as the input image.\n\nNext, we define a module as the building unit of our encoder and decoder architecture. It is worth noting that all models or model sub-parts that we define are required to inherit from the PyTorch class, which is the parent class in PyTorch for all neural network modules.\n\nWe start by defining our class on Lines 13-23. The function of this module is to take an input feature map with the number of channels, apply two convolution operations with a ReLU activation between them and return the output feature map with the channels.\n\nThe constructor takes as input two parameters, and (Line 14), which determine the number of channels in the input feature map and the output feature map, respectively.\n\nWe initialize the two convolution layers (i.e., and ) and a ReLU activation on Lines 17-19. On Lines 21-23, we define the function which takes as input our feature map , applies sequence of operations and returns the output feature map.\n\nNext, we define our class on Lines 25-47. The class constructor (i.e., the method) takes as input a tuple (i.e., ) of channel dimensions (Line 26). Note that the first value denotes the number of channels in our input image, and the subsequent numbers gradually double the channel dimension.\n\nWe start by initializing a list of blocks for the encoder (i.e., ) with the help of PyTorch’s functionality on Lines 29-31. Each takes the input channels of the previous block and doubles the channels in the output feature map. We also initialize a layer, which reduces the spatial dimension (i.e., height and width) of the feature maps by a factor of 2.\n\nFinally, we define the function for our encoder on Lines 34-47. The function takes as input an image as shown on Line 34. On Line 36, we initialize an empty list, storing the intermediate outputs from the blocks of our encoder. Note that this will enable us to later pass these outputs to that decoder where they can be processed with the decoder feature maps.\n\nOn Lines 39-44, we loop through each block in our encoder, process the input feature map through the block (Line 42), and add the output of the block to our list. We then apply the max pool operation on our block output (Line 44). This is done for each block in the encoder.\n\nFinally, we return our list on Line 47.\n\nNow we define our class (Lines 50-87). Similar to the encoder definition, the decoder method takes as input a tuple (i.e., ) of channel dimensions (Line 51). Note that the difference here, when compared with the encoder side, is that the channels gradually decrease by a factor of 2 instead of increasing.\n\nWe initialize the number of channels on Line 55. Furthermore, on Lines 56-58, we define a list of upsampling blocks (i.e., ) that use the layer to upsample the spatial dimension (i.e., height and width) of the feature maps by a factor of 2. In addition, the layer also reduces the number of channels by a factor of 2.\n\nFinally, we initialize a list of blocks for the decoder (i.e., ) similar to that on the encoder side.\n\nOn Lines 63-75, we define the function, which takes as input our feature map and the list of intermediate outputs from the encoder (i.e., ). Starting on Line 65, we loop through the number of channels and perform the following operations:\n• First, we upsample the input to our decoder (i.e., ) by passing it through our i-th upsampling block (Line 67)\n• Since we have to concatenate (along the channel dimension) the i-th intermediate feature map from the encoder (i.e., ) with our current output from the upsampling block, we need to ensure that the spatial dimensions of and match. To accomplish this, we use our function on Line 73.\n• Next, we concatenate our cropped encoder feature maps (i.e., ) with our current upsampled feature map , along the channel dimension on Line 74\n• Finally, we pass the concatenated output through our i-th decoder block (Line 75)\n\nAfter the completion of the loop, we return the final decoder output on Line 78.\n\nOn Lines 80-87, we define our crop function which takes an intermediate feature map from the encoder (i.e., ) and a feature map output from the decoder (i.e., ) and spatially crops the former to the dimension of the latter.\n\nTo do this, we first grab the spatial dimensions of (i.e., height and width ) on Line 83. Then, we crop to the spatial dimension using the function (Line 84) and finally return the cropped output on Line 87.\n\nNow that we have defined the sub-modules that make up our U-Net model, we are ready to build our U-Net model class.\n\nWe start by defining the constructor method (Lines 91-103). It takes the following parameters as input:\n• : The tuple defines the gradual increase in channel dimension as our input passes through the encoder. We start with 3 channels (i.e., RGB) and subsequently double the number of channels.\n• : The tuple defines the gradual decrease in channel dimension as our input passes through the decoder. We reduce the channels by a factor of 2 at every step.\n• : This defines the number of segmentation classes where we have to classify each pixel. This usually corresponds to the number of channels in our output segmentation map, where we have one channel for each class.\n• Since we are working with two classes (i.e., binary classification), we keep a single channel and use thresholding for classification, as we will discuss later.\n• : This indicates whether we want to retain the original output dimension.\n• : This determines the spatial dimensions of the output segmentation map. We set this to the same dimension as our input image (i.e., ( )).\n\nOn Lines 97 and 98, we initialize our encoder and decoder networks. Furthermore, we initialize a convolution head through which will later take our decoder output as input and output our segmentation map with number of channels (Line 101).\n\nWe also initialize the and attributes on Lines 102 and 103.\n\nFinally, we are ready to discuss our U-Net model’s function (Lines 105-124).\n\nWe begin by passing our input through the encoder. This outputs the list of encoder feature maps (i.e., ) as shown on Line 107. Note that the list contains all the feature maps starting from the first encoder block output to the last, as discussed previously. Therefore, we can reverse the order of feature maps in this list: .\n\nNow the list contains the feature map outputs in reverse order (i.e., from the last to the first encoder block). Note that this is important since, on the decoder side, we will be utilizing the encoder feature maps starting from the last encoder block output to the first.\n\nNext, we pass the output of the final encoder block (i.e., ) and the feature map outputs of all intermediate encoder blocks (i.e., ) to the decoder on Line 111. The output of the decoder is stored as .\n\nWe pass the decoder output to our convolution head (Line 116) to obtain the segmentation mask.\n\nFinally, we check if the attribute is (Line 120). If yes, we interpolate the final segmentation map to the output size defined by (Line 121). We return our final segmentation map on Line 124.\n\nThis completes the implementation of our U-Net model. Next, we will look at the training procedure for our segmentation pipeline.\n\nNow that we have implemented our dataset class and model architecture, we are ready to construct and train our segmentation pipeline in PyTorch. Let’s open the file from our project directory.\n\nSpecifically, we will be looking at the following in detail:\n\nWe begin by importing our custom-defined class and the model on Lines 5 and 6. Next, we import our config file on Line 7.\n\nSince our salt segmentation task is a pixel-level binary classification problem, we will be using binary cross-entropy loss to train our model. On Line 8, we import the binary cross-entropy loss function (i.e., ) from the PyTorch module. In addition to this, we import the optimizer from the PyTorch module, which we will be using to train our network (Line 9).\n\nNext, on Line 11, we import the in-built function from the library, enabling us to split our dataset into training and testing sets. Furthermore, we import the module from on Line 12 to apply image transformations on our input images.\n\nFinally, we import other useful packages for handling our file system, keeping track of progress during training, timing our training process, and plotting loss curves on Lines 13-18.\n\nOnce we have imported all necessary packages, we will load our data and structure the data loading pipeline.\n\nOn Lines 21 and 22, we first define two lists (i.e., and ) that store the paths of all images and their corresponding segmentation masks, respectively.\n\nWe then partition our dataset into a training and test set with the help of scikit-learn’s on Line 26. Note that this function takes as input a sequence of lists (here, and ) and simultaneously returns the training and test set images and corresponding training and test set masks which we unpack on Lines 30 and 31.\n\nWe store the paths in the list in the test folder path defined by on Line 36.\n\nNow, we are ready to set up our data loading pipeline.\n\nWe first define the transformations that we want to apply while loading our input images and consolidate them with the help of the function on Lines 41-44. Our transformations include:\n• : it enables us to convert our input images to PIL image format. Note that this is necessary since we used OpenCV to load images in our custom dataset, but PyTorch expects the input image samples to be in PIL format.\n• : allows us to resize our images to a particular input dimension (i.e., , ) that our model can accept\n• : enables us to convert input images to PyTorch tensors and convert the input PIL Image, which is originally in the range from , to .\n\nFinally, we pass the train and test images and corresponding masks to our custom to create the training dataset (i.e., ) and test dataset (i.e., ) on Lines 47-50. Note that we can simply pass the transforms defined on Line 41 to our custom PyTorch dataset to apply these transformations while loading the images automatically.\n\nWe can now print the number of samples in and with the help of the method, as shown in Lines 51 and 52.\n\nOn Lines 55-60, we create our training dataloader (i.e., ) and test dataloader (i.e., ) directly by passing our train dataset and test dataset to the Pytorch DataLoader class. We keep the parameter in the train dataloader since we want samples from all classes to be uniformly present in a batch which is important for optimal learning and convergence of batch gradient-based optimization approaches.\n\nNow that we have structured and defined our data loading pipeline, we will initialize our U-Net model and the training parameters.\n\nWe start by defining our model on Line 63. Note that the function takes as input our and registers our model and its parameters on the device mentioned.\n\nOn Lines 66 and 67, we define our loss function and optimizer, which we will use to train our segmentation model. The optimizer class takes as input the parameters of our model (i.e., ) and the learning rate (i.e., ) we will be using to train our model.\n\nWe then define the number of steps required to iterate over our entire train and test set, that is, and , on Lines 70 and 71. Given that the dataloader provides our model number of samples to process at a time, the number of steps required to iterate over the entire dataset (i.e., train or test set) can be calculated by dividing the total samples in the dataset by the batch size.\n\nWe also create an empty dictionary, , on Line 74, that we will use to keep track of our training and test loss history.\n\nFinally, we are in good shape to start understanding our training loop.\n\nTo time our training process, we use the function on Line 78. This function outputs the time when it is called. Thus, we can call it once at the start and once at the end of our training process and subtract the two outputs to get the time elapsed.\n\nWe iterate for in the training loop, as shown on Line 79. Before we start training, it is important to set our model to train mode, as we see on Line 81. This directs the PyTorch engine to track our computations and gradients and build a computational graph to backpropagate later.\n\nWe initialize variables and on Lines 84 and 85 to track our losses in the given epoch. Next, on Line 88, we iterate over our dataloader, which provides a batch of samples at a time. The training loop, as shown on Lines 88-103, comprises of the following steps:\n• First, on Line 90, we move our data samples (i.e., and ) to the device we are training our model on, defined by\n• We then pass our input image sample through our model on Line 93 and get the output prediction (i.e., )\n• On Line 94, we compute the loss between the model prediction, and our ground-truth label\n• On Lines 98-100, we backpropagate our loss through the model and update the parameters\n• This is executed with the help of three simple steps; we start by clearing all accumulated gradients from previous steps on Line 98. Next, we call the method on our computed loss function as shown on Line 99. This directs PyTorch to compute gradients of our loss w.r.t. all variables involved in the computation graph. Finally, we call to update our model parameters as shown on Line 100.\n• In the end, Line 103 enables us to keep track of our training loss by adding the loss for the step to the variable, which accumulates the training loss for all samples.\n\nThis process is repeated until iterated through all dataset samples once (i.e., completed one epoch).\n\nOnce we have processed our entire training set, we would want to evaluate our model on the test set. This is helpful since it allows us to monitor the test loss and ensure that our model is not overfitting to the training set.\n\nWhile evaluating our model on the test set, we do not track gradients since we will not be learning or backpropagating. Thus we can switch off the gradient computation with the help of and freeze the model weights, as shown on Line 106. This directs the PyTorch engine not to calculate and save gradients, saving memory and compute during evaluation.\n\nWe set our model to evaluation mode by calling the function on Line 108. Then, we iterate through the test set samples and compute the predictions of our model on test data (Line 116). The test loss is then added to the , which accumulates the test loss for the entire test set.\n\nWe then obtain the average training loss and test loss over all steps, that is, and on Lines 120 and 121, and store them on Lines 124 and 125, to our dictionary, , which we had created in the beginning to keep track of our losses.\n\nFinally, we print the current epoch statistics, including train and test losses on Lines 128-130. This brings us to the end of one epoch, consisting of one full cycle of training on our train set and evaluation on our test set. This entire process is repeated times until our model converges.\n\nOn Lines 133 and 134, we note the end time of our training loop and subtract from (which we had initialized at the beginning of training) to get the total time elapsed during our network training.\n\nNext, we use the pyplot package of matplotlib to visualize and save our training and test loss curves on Lines 138-146. We can do this by simply passing the and keys of our loss history dictionary, , to the function as shown on Lines 140 and 141. Finally, we set the title and legends of our plots (Lines 142-145) and save our visualizations on Line 146.\n\nFinally, on Lines 149, we save the weights of our trained U-Net model with the help of the function, which takes our trained model and the as input where we want our model to be saved.\n\nOnce our model is trained, we will see a loss trajectory plot similar to the one shown in Figure 4. Notice that gradually reduces over epochs and slowly converges. Furthermore, we see that also consistently reduces with following similar trends and values, implying our model generalizes well and is not overfitting to the training set.\n\nUsing Our Trained U-Net Model for Prediction\n\nOnce we have trained and saved our segmentation model, we are ready to see it in action and use it for segmentation tasks.\n\nOpen the file from our project directory.\n\nWe import the necessary packages and modules as always on Lines 5-10.\n\nTo use our segmentation model for prediction, we will need a function that can take our trained model and test images, predict the output segmentation mask and finally, visualize the output predictions.\n\nTo this end, we start by defining the function to help us to visualize our model predictions.\n\nThis function takes as input an image, its ground-truth mask, and the segmentation output predicted by our model, that is, , , and (Line 12) and creates a grid with a single row and three columns (Line 14) to display them (Lines 17-19).\n\nFinally, Lines 22-24 set titles for our plots, displaying them on Lines 27 and 28.\n\nNext, we define our function (Lines 31-77), which will take as input the path to a test image and our trained segmentation model and plot the predicted output.\n\nSince we are only using our trained model for prediction, we start by setting our model to mode and switching off PyTorch gradient computation on Line 33 and Line 36, respectively.\n\nOn Lines 39-41, we load the test image (i.e., ) from using OpenCV (Line 39), convert it to RGB format (Line 40), and normalize its pixel values from the standard to the range , which our model is trained to process (Line 41).\n\nThe image is then resized to the standard image dimension that our model can accept on Line 44. Since we will have to modify and process the variable before passing it through the model, we make an additional copy of it on Line 45 and store it in the variable, which we will use later.\n\nOn Lines 49-51, we get the path to the ground-truth mask for our test image and load the mask on Line 55. Note that we resize the mask to the same dimensions as the input image (Lines 56 and 57).\n\nNow we process our to a format that our model can process. Note that currently, our has the shape . However, our segmentation model accepts four-dimensional inputs of the format .\n\nOn Line 62, we transpose the image to convert it to channel-first format, that is, , and on Line 63, we add an extra dimension using the function of numpy to convert our image into a four-dimensional array (i.e., ). Note that the first dimension here represents the batch dimension equal to one since we are processing one test image at a time. We then convert our image to a PyTorch tensor with the help of the function and move it to the device our model is on with the help of Line 64.\n\nFinally, on Lines 68-70, we process our test image by passing it through our model and saving the output prediction as . We then apply the sigmoid activation to get our predictions in the range . As discussed earlier, the segmentation task is a classification problem where we have to classify the pixels in one of the two discrete classes. Since sigmoid outputs continuous values in the range , we use our on Line 73 to binarize our output and assign the pixels, values equal to or . This implies that anything greater than the threshold will be assigned the value , and others will be assigned .\n\nSince the thresholded output (i.e., ), now comprises of values or , multiplying it with makes the final pixel values in our either (i.e., pixel value for black color) or (i.e., pixel value for white color). As discussed earlier, the white pixels will correspond to the region where our model has detected salt deposits, and the black pixels correspond to regions where salt is not present.\n\nWe plot our original image (i.e., ), ground-truth mask (i.e., ), and our predicted output (i.e., ) with the help of our function on Line 77. This completes the definition of our function.\n\nWe are ready to see our model in action now.\n\nOn Lines 82 and 83, we open the folder where our test image paths are stored and randomly grab 10 image paths. Line 87 loads the trained weights of our U-Net from the saved checkpoint at .\n\nWe finally iterate over our randomly chosen test and predict the outputs with the help of our function on Lines 90-92.\n\nFigure 5 shows sample visualization outputs from our function. The yellow region represents Class 1: Salt and the dark blue region represents Class 2: Not Salt (sediment).\n\nWe see that in case 1 and case 2 (i.e., row 1 and row 2, respectively), our model correctly identified most of the locations containing salt deposits. However, some regions where the salt deposit exists are not identified.\n\nHowever, in case 3 (i.e., row 3), our model has identified some regions as salt deposits where there is no salt (the yellow blob in the middle). This is a false positive, where our model has incorrectly predicted the positive class, that is, the presence of salt, in a region where it does not exist in the ground truth.\n\nIt is worth noting that, practically, from an application point of view, the prediction in case 3 is misleading and riskier than that in the other two cases. This is likely because for the first two cases if experts set up drillers for mining salt deposits at the predicted yellow marked locations, they will successfully find salt deposits. However, if they do the same at the location of false-positive predictions (as seen in case 3), it will waste time and resources since salt deposits do not exist at that location.\n\nAman Arora’s amazing article inspires our implementation of the U-Net model in the file.\n\nIn this tutorial, we learned about image segmentation and built a U-Net-based image segmentation pipeline from scratch in PyTorch.\n\nSpecifically, we discussed the architectural details and salient features of the U-Net model that make it the de-facto choice for image segmentation.\n\nIn addition, we learned how we can define our own custom dataset in PyTorch for the segmentation task at hand.\n\nFinally, we saw how we can train our U-Net based-segmentation pipeline in PyTorch and use the trained model to make predictions on test images in real-time.\n\nAfter following the tutorial, you will be able to understand the internal working of any image segmentation pipeline and build your own segmentation models from scratch in PyTorch.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!"
    },
    {
        "link": "https://pytorch.org/vision/stable/models.html",
        "document": "The subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection, video classification, and optical flow.\n\nTorchVision offers pre-trained weights for every provided architecture, using the PyTorch . Instancing a pre-trained model will download its weights to a cache directory. This directory can be set using the environment variable. See for details. The pre-trained models provided in this library may have their own licenses or terms and conditions derived from the dataset used for training. It is your responsibility to determine whether you have permission to use the models for your use case. Backward compatibility is guaranteed for loading a serialized to the model created using old PyTorch version. On the contrary, loading entire saved models or serialized (serialized using older versions of PyTorch) may not preserve the historic behaviour. Refer to the following documentation As of v0.13, TorchVision offers a new Multi-weight support API for loading different weights to the existing model builder methods: # Best available weights (currently alias for IMAGENET1K_V2) # Note that these weights may change across versions Migrating to the new API is very straightforward. The following method calls between the 2 APIs are all equivalent: Note that the parameter is now deprecated, using it will emit warnings and will be removed on v0.15. Before using the pre-trained models, one must preprocess the image (resize with right resolution/interpolation, apply inference transforms, rescale the values etc). There is no standard way to do this as it depends on how a given model was trained. It can vary across model families, variants or even weight versions. Using the correct preprocessing method is critical and failing to do so may lead to decreased accuracy or incorrect outputs. All the necessary information for the inference transforms of each pre-trained model is provided on its weights documentation. To simplify inference, TorchVision bundles the necessary preprocessing transforms into each model weight. These are accessible via the attribute: # Apply it to the input image Some models use modules which have different training and evaluation behavior, such as batch normalization. To switch between these modes, use or as appropriate. See or for details. As of v0.14, TorchVision offers a new mechanism which allows listing and retrieving models and weights by their names. Here are a few examples on how to use them: Here are the available public functions to retrieve models and their corresponding weights: Gets the model name and configuration and returns an instantiated model. Returns the weights enum class associated to the given model. Gets the weights enum value by its full name. Returns a list with the names of registered models. Most pre-trained models can be accessed directly via PyTorch Hub without having TorchVision installed: You can also retrieve all the available weights of a specific model via PyTorch Hub by doing: The only exception to the above are the detection models included on . These models require TorchVision to be installed because they depend on custom C++ operators.\n\nThe segmentation module is in Beta stage, and backward compatibility is not guaranteed. The following semantic segmentation models are available, with or without pre-trained weights: Here is an example of how to use the pre-trained semantic segmentation models: # Step 1: Initialize model with the best available weights # Step 4: Use the model and visualize the prediction The classes of the pre-trained model outputs can be found at . The output format of the models is illustrated in Semantic segmentation models. Table of all available semantic segmentation weights¶ All models are evaluated a subset of COCO val2017, on the 20 categories that are present in the Pascal VOC dataset:\n\nThe pre-trained models for detection, instance segmentation and keypoint detection are initialized with the classification models in torchvision. The models expect a list of . Check the constructor of the models for more information. The detection module is in Beta stage, and backward compatibility is not guaranteed. The following object detection models are available, with or without pre-trained weights: Here is an example of how to use the pre-trained object detection models: # Step 1: Initialize model with the best available weights # Step 4: Use the model and visualize the prediction The classes of the pre-trained model outputs can be found at . For details on how to plot the bounding boxes of the models, you may refer to Instance segmentation models. Table of all available Object detection weights¶ The following instance segmentation models are available, with or without pre-trained weights: For details on how to plot the masks of the models, you may refer to Instance segmentation models. Table of all available Instance segmentation weights¶ Box and Mask MAPs are reported on COCO val2017: The following person keypoint detection models are available, with or without pre-trained weights: The classes of the pre-trained model outputs can be found at . For details on how to plot the bounding boxes of the models, you may refer to Visualizing keypoints. Table of all available Keypoint detection weights¶ Box and Keypoint MAPs are reported on COCO val2017:\n\nThe video module is in Beta stage, and backward compatibility is not guaranteed. The following video classification models are available, with or without pre-trained weights: Here is an example of how to use the pre-trained video classification models: # Step 1: Initialize model with the best available weights # Step 4: Use the model and print the predicted category The classes of the pre-trained model outputs can be found at . Table of all available video classification weights¶ Accuracies are reported on Kinetics-400 using single crops for clip length 16:"
    },
    {
        "link": "https://medium.com/@maahip1304/the-complete-guide-to-image-preprocessing-techniques-in-python-dca30804550c",
        "document": "Have you ever struggled with poor quality images in your machine learning or computer vision projects? Images are the lifeblood of many Al systems today, but not all images are created equal. Before you can train a model or run an algorithm, you often need to do some preprocessing on your images to get the best results. Image preprocessing in Python is your new best friend.\n\nIn this guide, you’ll learn all the tips and tricks for preparing your images for analysis using Python. We’ll cover everything from resizing and cropping to reducing noise and normalizing. By the time you’re done, your images will be ready for their closeup. With the help of libraries like OpenCV, Pillow, and scikit-image, you’ll be enhancing images in no time. So get ready to roll up your sleeves — it’s time to dive into the complete guide to image preprocessing techniques in Python!\n\nWhat Is Image Preprocessing and Why Is It Important?\n\nImage preprocessing is the process of manipulating raw image data into a usable and meaningful format. It allows you to eliminate unwanted distortions and enhance specific qualities essential for computer vision applications. Preprocessing is a crucial first step to prepare your image data before feeding it into machine learning models.\n\nThere are several techniques used in image preprocessing:\n• Resizing: Resizing images to a uniform size is important for machine learning algorithms to function properly. We can use OpenCV’s resize() method to resize images.\n• Grayscaling: Converting color images to grayscale can simplify your image data and reduce computational needs for some algorithms. The cvtColor() method can be used to convert RGB to grayscale.\n• Noise reduction: Smoothing, blurring, and filtering techniques can be applied to remove unwanted noise from images. The GaussianBlur () and medianBlur () methods are commonly used for this.\n• Normalization: Normalization adjusts the intensity values of pixels to a desired range, often between 0 to 1. This can improve the performance of machine learning models. Normalize () from scikit-image can be used for this.\n• Binarization: Binarization converts grayscale images to black and white by thresholding. The threshold () method is used to binarize images in OpenCV.\n• Contrast enhancement: The contrast of images can be adjusted using histogram equalization. The equalizeHist () method enhances the contrast of images.\n\nWith the right combination of these techniques, you can significantly improve your image data and build better computer vision applications. Image preprocessing allows you to refine raw images into a format suitable for the problem you want to solve.\n\nTo get started with image processing in Python, you’ll need to load and convert your images into a format the libraries can work with. The two most popular options for this are OpenCV and Pillow.\n\nLoading images with OpenCV: OpenCV can load images in formats like PNG, JPG, TIFF, and BMP. You can load an image with:\n\nThis will load the image as a NumPy array. The image is in the BGR color space, so you may want to convert it to RGB.\n\nLoading images with Pillow: Pillow is a friendly PIL (Python Image Library) fork. It supports even more formats than OpenCV, including PSD, ICO, and WEBP. You can load an image with:\n\nThe image will be in RGB color space.\n\nConverting between color spaces: You may need to convert images between color spaces like RGB, BGR, HSV, and Grayscale. This can be done with OpenCV or Pillow. For example, to convert BGR to Grayscale in OpenCV, use:\n\nOr to convert RGB to HSV in Pillow:\n\nWith these foundational skills, you’ll be ready to move on to more advanced techniques like resizing, filtering, edge detection, and beyond. The possibilities are endless! What image processing project will you build?\n\nResizing and cropping your images is an important first step in image preprocessing.\n\nImages come in all shapes and sizes, but machine learning algorithms typically require a standard size. You’ll want to resize and crop your images to square dimensions, often 224x224 or 256x256 pixels.\n\nIn Python, you can use the OpenCY or Pillow library for resizing and cropping. With OpenCV, use the resize() function. For example:\n\nThis will resize the image to 224x224 pixels.\n\nTo crop an image to a square, you can calculate the center square crop size and use crop() in OpenCV with the center coordinates. For example:\n\nWith Pillow, you can use the Image. open () and resize() functions. For example:\n\nTo crop the image, use img. crop(). For example:\n\nResizing and cropping your images to a standard size is a crucial first step. It will allow your machine learning model to process the images efficiently and improve the accuracy of your results. Take the time to resize and crop your images carefully, and your model will thank you!\n\nWhen working with image data, it’s important to normalize the pixel values to have a consistent brightness and improve contrast. This makes the images more suitable for analysis and allows machine learning models to learn patterns independent of lighting conditions.\n\nRescaling Pixel Values: The most common normalization technique is rescaling the pixel values to range from 0 to 1. This is done by dividing all pixels by the maximum pixel value (typically 255 for RGB images). For example:\n\nThis will scale all pixels between 0 and 1, with 0 being black and 1 being white.\n\nHistogram Equalization: Another useful technique is histogram equalization. This spreads out pixel intensities over the whole range to improve contrast. It can be applied with OpenCV using:\n\nThis works well for images with low contrast where pixel values are concentrated in a narrow range.\n\nFor some algorithms, normalizing to have zero mean and unit variance is useful. This can be done by subtracting the mean and scaling to unit variance:\n\nThis will center the image around zero with a standard deviation of 1.\n\nThere are a few other more complex normalization techniques, but these three methods-rescaling to the 0–1 range, histogram equalization, and standardization — cover the basics and will prepare your image data for most machine learning applications. Be sure to apply the same normalization to both your training and testing data for the best results.\n\nOnce you have your images loaded in Python, it’s time to start enhancing them. Image filters are used to reduce noise, sharpen details, and overall improve the quality of your images before analysis. Here are some of the main filters you’ll want to know about:\n\nGaussian Blur:\n\nThe Gaussian blur filter reduces detail and noise in an image. It “blurs” the image by applying a Gaussian function to each pixel and its surrounding pixels. This can help smooth edges and details in preparation for edge detection or other processing techniques.\n\nMedian Blur:\n\nThe median blur filter is useful for removing salt and pepper noise from an image. It works by replacing each pixel with the median value of its neighboring pixels. This can help smooth out isolated noisy pixels while preserving edges.\n\nLaplacian Filter:\n\nThe Laplacian filter is used to detect edges in an image. It works by detecting areas of rapid intensity change. The output will be an image with edges highlighted, which can then be used for edge detection. This helps identify and extract features in an image.\n\nUnsharp Masking:\n\nUnsharp masking is a technique used to sharpen details and enhance edges in an image. It works by subtracting a blurred version of the image from the original image. This amplifies edges and details, making the image appear sharper. Unsharp masking can be used to sharpen details before feature extraction or object detection.\n\nBilateral Filter:\n\nThe bilateral filter smooths images while preserving edges. It does this by considering both the spatial closeness and color similarity of pixels. Pixels that are close together spatially and similar in color are smoothed together. Pixels that are distant or very different in color are not smoothed. This results in a smoothed image with sharp edges.\n\nThe bilateral filter can be useful for noise reduction before edge detection.\n\nBy applying these filters, you’ll have high-quality, enhanced images ready for in-depth analysis and computer vision tasks. Give them a try and see how they improve your image processing results!\n\nDetecting and removing backgrounds from images is an important preprocessing step for many computer vision tasks. Segmentation separates the foreground subject from the background, leaving you with a clean image containing just the subject.\n\nThere are a few common ways to perform image segmentation in Python using OpenCV and scikit-image:\n\nThresholding:\n\nThresholding converts a grayscale image into a binary image (black and white) by choosing a threshold value. Pixels darker than the threshold become black, and pixels lighter become white. This works well for images with high contrast and uniform lighting. You can use OpenCV’s threshold() method to apply thresholding.\n\nEdge Detection:\n\nEdge detection finds the edges of objects in an image. By connecting edges, you can isolate the foreground subject. The Canny edge detector is a popular algorithm implemented in scikit-image’s canny() method. Adjust the low_threshold and high_threshold parameters to detect edges.\n\nRegion Growing:\n\nRegion growing starts with a group of seed points and grows outward to detect contiguous regions in an image. You provide the seed points, and the algorithm examines neighboring pixels to determine if they should be added to the region. This continues until no more pixels can be added. The skimage. segmentation. region_growing () method implements this technique.\n\nWatershed:\n\nThe watershed algorithm treats an image like a topographic map, with high intensity pixels representing peaks and valleys representing borders between regions. It starts at the peaks and floods down, creating barriers when different regions meet. The skimage. segmentation. watershed() method performs watershed segmentation.\n\nBy experimenting with these techniques, you can isolate subjects from the background in your images. Segmentation is a key first step, allowing you to focus your computer vision models on the most important part of the image-the foreground subject.\n\nUsing Data Augmentation to Expand Your Dataset\n\nData augmentation is a technique used to artificially expand the size of your dataset by generating new images from existing ones. This helps reduce overfitting and improves the generalization of your model. Some common augmentation techniques for image data include:\n\nFlipping and rotating:\n\nSimply flipping (horizontally or vertically) or rotating (90, 180, 270 degrees) images can generate new data points. For example, if you have 1,000 images of cats, flipping and rotating them can give you 4,000 total images (1,000 original + 1,000 flipped horizontally + 1,000 flipped vertically + 1,000 rotated 90 degrees).\n\nCropping:\n\nCropping images to different sizes and ratios creates new images from the same original. This exposes your model to different framings and compositions of the same content. You can create random crops of varying size, or target more specific crop ratios like squares.\n\nColor manipulation:\n\nAdjusting brightness, contrast, hue, and saturation are easy ways to create new augmented images. For example, you can randomly adjust the brightness and contrast of images by up to 30% to generate new data points. Be careful not to distort the images too much, or you risk confusing your model.\n\nImage overlays:\n\nOverlaying transparent images, textures or noise onto existing images is another simple augmentation technique. Adding things like watermarks, logos, dirt/scratches or Gaussian noise can create realistic variations of your original data. Start with subtle overlays and see how your model responds.\n\nCombining techniques:\n\nFor the biggest increase in data, you can combine multiple augmentation techniques on the same images. For example, you can flip, rotate, crop and adjust the color of images, generating many new data points from a single original image. But be careful not to overaugment, or you risk distorting the images beyond recognition!\n\nUsing data augmentation, you can easily multiply the size of your image dataset by 4x, 10x or more, all without collecting any new images. This helps combat overfitting and improves model accuracy, all while keeping training time and cost the same.\n\nChoosing the Right Preprocessing Steps for Your Application\n\nChoosing the right preprocessing techniques for your image analysis project depends on your data and goals. Some common steps include:\n\nResizing:\n\nResizing images to a consistent size is important for machine learning algorithms to work properly. You’ll want all your images to be the same height and width, usually a small size like 28x28 or 64x64 pixels. The resize() method in OpenCV or Pillow libraries make this easy to do programmatically.\n\nColor conversion:\n\nConverting images to grayscale or black and white can simplify your analysis and reduce noise. The cvtColor() method in OpenCV converts images from RGB to grayscale. For black and white, use thresholding.\n\nNoise reduction:\n\nTechniques like Gaussian blurring, median blurring, and bilateral filtering can reduce noise and smooth images. OpenCV’s GaussianBlur(), medianBlur(), and bilateralFilter() methods apply these filters.\n\nNormalization\n\nNormalizing pixel values to a standard range like 0 to 1 or -1 to 1 helps algorithms work better. You can normalize images with the normalize() method in scikit-image.\n\nContrast enhancement:\n\nFor low contrast images, histogram equalization improves contrast. The equaliseHist() method in OpenCV performs this task.\n\nEdge detection:\n\nFinding the edges or contours in an image is useful for many computer vision tasks. The Canny edge detector in OpenCV’s Canny() method is a popular choice.\n\nThe key is choosing techniques that will prepare your images to suit your particular needs. Start with basic steps like resizing, then try different methods to improve quality and see which ones optimize your results. With some experimenting, you’ll find an ideal preprocessing workflow.\n\nNow that you have a good grasp of the various image preprocessing techniques in Python, you probably have a few lingering questions. Here are some of the most frequently asked questions about image preprocessing and their answers:\n\nWhat image formats does Python support?\n\nPython supports a wide range of image formats through libraries like OpenCV and Pillow.\n\nSome of the major formats include:\n• JPEG — Common lossy image format\n• PNG — Lossless image format good for images with transparency\n• TIFF — Lossless image format good for high color depth images\n• BMP — Uncompressed raster image format\n\nWhen should I resize an image?\n\nYou should resize an image when:\n• The image is too large to process efficiently. Reducing size can speed up processing.\n• The image needs to match the input size of a machine learning model.\n• The image needs to be displayed pn a screen or webpage at a specific. size.\n\nWhat are some common noise reduction techniques?\n\nSome popular noise reduction techniques include:\n• Gaussian blur — Uses a Gaussian filter to blur the image and reduce high frequency noise.\n• Median blur — Replaces each pixel with the median of neighboring pixels. Effective at removing salt and pepper noise.\n• Bilateral filter — Blurs images while preserving edges. It can remove noise while retaining sharp edges.\n\nWhat color spaces are supported in OpenCV and how do I convert between them?\n\nOpenCV supports RGB, HSV, LAB, and Grayscale color spaces. You can convert between color spaces using the cvtColor function. For example:\n\nConverting to different color spaces is useful for certain computer vision tasks like thresholding, edge detection, and object tracking.\n\nSo there you have it, a complete guide to getting your images ready for analysis in Python. With the power of OpenCV and other libraries, you now have all the tools you need to resize, enhance, filter, and transform your images. Go ahead and play around with the different techniques, tweak the parameters, and find what works best for your specific dataset and computer vision task. Image preprocessing may not be the sexiest part of building an Al system, but it’s absolutely critical to get right. Put in the work upfront, and you’ll have clean, optimized images ready to feed into your machine learning models. Your computer vision system will thank you, and you’ll achieve better results faster. Happy image processing!"
    },
    {
        "link": "https://medium.com/@feitgemel/u-net-image-segmentation-how-to-segment-persons-in-images-2fd282d1005a",
        "document": "This tutorial provides a step-by-step guide on how to implement and train a U-Net model for persons segmentation using TensorFlow/Keras.\n\nThe tutorial is divided into four parts:\n\nIn this part, you load and preprocess the persons dataset, including resizing images and masks, converting masks to binary format, and splitting the data into training, validation, and testing sets.\n\nThis part defines the U-Net model architecture using Keras. It includes building blocks for convolutional layers, constructing the encoder and decoder parts of the U-Net, and defining the final output layer.\n\nHere, you load the preprocessed data and train the U-Net model. You compile the model, define training parameters like learning rate and batch size, and use callbacks for model checkpointing, learning rate reduction, and early stopping.\n\nThe final part demonstrates how to load the trained model, perform inference on test data, and visualize the predicted segmentation masks."
    },
    {
        "link": "https://kaggle.com/code/hassanikram/swin-unet-liver-tumor-segmentation",
        "document": ""
    },
    {
        "link": "https://keras.io/examples/vision/swin_transformers",
        "document": "Author: Rishit Dagli\n\n Date created: 2021/09/08\n\n Last modified: 2021/09/08\n\n Description: Image classification using Swin Transformers, a general-purpose backbone for computer vision.\n\nⓘ This example uses Keras 3\n\nThis example implements Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. for image classification, and demonstrates it on the CIFAR-100 dataset.\n\nSwin Transformer (Shifted Window Transformer) can serve as a general-purpose backbone for computer vision. Swin Transformer is a hierarchical Transformer whose representations are computed with shifted windows. The shifted window scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connections. This architecture has the flexibility to model information at various scales and has a linear computational complexity with respect to image size.\n\nThis example requires TensorFlow 2.5 or higher.\n\nA key parameter to pick is the , the size of the input patches. In order to use each pixel as an individual input, you can set to . Below, we take inspiration from the original paper settings for training on ImageNet-1K, keeping most of the original settings for this example.\n\nWe load the CIFAR-100 dataset through , normalize the images, and convert the integer labels to one-hot encoded vectors.\n\nWe create two helper functions to help us get a sequence of patches from the image, merge patches, and apply dropout.\n\nUsually Transformers perform global self-attention, where the relationships between a token and all other tokens are computed. The global computation leads to quadratic complexity with respect to the number of tokens. Here, as the original paper suggests, we compute self-attention within local windows, in a non-overlapping manner. Global self-attention leads to quadratic computational complexity in the number of patches, whereas window-based self-attention leads to linear complexity and is easily scalable.\n\nFinally, we put together the complete Swin Transformer by replacing the standard multi-head attention (MHA) with shifted windows attention. As suggested in the original paper, we create a model comprising of a shifted window-based MHA layer, followed by a 2-layer MLP with GELU nonlinearity in between, applying before each MSA layer and each MLP, and a residual connection after each of these layers.\n\nNotice that we only create a simple MLP with 2 Dense and 2 Dropout layers. Often you will see models using ResNet-50 as the MLP which is quite standard in the literature. However in this paper the authors use a 2-layer MLP with GELU nonlinearity in between.\n\nWe first create 3 layers to help us extract, embed and merge patches from the images on top of which we will later use the Swin Transformer class we built.\n\nWe do all the steps, which do not have trainable weights with tf.data. Prepare the training, validation and testing sets.\n\nWe put together the Swin Transformer model.\n\nWe train the model on CIFAR-100. Here, we only train the model for 40 epochs to keep the training time short in this example. In practice, you should train for 150 epochs to reach convergence.\n\nLet's visualize the training progress of the model.\n\nLet's display the final results of the training on CIFAR-100.\n\nThe Swin Transformer model we just trained has just 152K parameters, and it gets us to ~75% test top-5 accuracy within just 40 epochs without any signs of overfitting as well as seen in above graph. This means we can train this network for longer (perhaps with a bit more regularization) and obtain even better performance. This performance can further be improved by additional techniques like cosine decay learning rate schedule, other data augmentation techniques. While experimenting, I tried training the model for 150 epochs with a slightly higher dropout and greater embedding dimensions which pushes the performance to ~72% test accuracy on CIFAR-100 as you can see in the screenshot.\n\nThe authors present a top-1 accuracy of 87.3% on ImageNet. The authors also present a number of experiments to study how input sizes, optimizers etc. affect the final performance of this model. The authors further present using this model for object detection, semantic segmentation and instance segmentation as well and report competitive results for these. You are strongly advised to also check out the original paper.\n\nThis example takes inspiration from the official PyTorch and TensorFlow implementations."
    },
    {
        "link": "https://pyimagesearch.com/2022/02/21/u-net-image-segmentation-in-keras",
        "document": "In this tutorial, you will learn how to create U-Net, an image segmentation model in TensorFlow 2 / Keras. We will first present a brief introduction on image segmentation, U-Net architecture, and then walk through the code implementation with a Colab notebook.\n\nTo learn how to implement a U-Net with TensorFlow 2 / Keras, just keep reading.\n\nImage segmentation is a computer vision task that segments an image into multiple areas by assigning a label to every pixel of the image. It provides much more information about an image than object detection, which draws a bounding box around the detected object, or image classification, which assigns a label to the object.\n\nSegmentation is useful and can be used in real-world applications such as medical imaging, clothes segmentation, flooding maps, self-driving cars, etc.\n\nThere are two types of image segmentation:\n• Instance segmentation: classify each pixel and differentiate each object instance.\n\nU-Net is a semantic segmentation technique originally proposed for medical imaging segmentation. It’s one of the earlier deep learning segmentation models, and the U-Net architecture is also used in many GAN variants such as the Pix2Pix generator.\n\nU-Net was introduced in the paper, U-Net: Convolutional Networks for Biomedical Image Segmentation. The model architecture is fairly simple: an encoder (for downsampling) and a decoder (for upsampling) with skip connections. As Figure 1 shows, it shapes like the letter U hence the name U-Net.\n\nThe gray arrows indicate the skip connections that concatenate the encoder feature map with the decoder, which helps the backward flow of gradients for improved training.\n\nNow that we have a basic understanding of semantic segmentation and the U-Net architecture, let’s implement a U-Net with TensorFlow 2 / Keras. Please follow the tutorial below with this Colab notebook.\n\nWe will be using Colab for model training, so make sure you set “Hardware accelerator” to “GPU under Runtime / change runtime type.” Then import the libraries and packages this project depends on:\n\nWe will use the Oxford-IIIT pet dataset, available as part of the TensorFlow Datasets (TFDS). It can be easily loaded with TFDS, and then with a bit of data preprocessing, ready for training segmentation models.\n\nWith just one line of code, we can use to load the dataset by specifying the name of the dataset, and get the dataset info by setting :\n\nPrint the dataset info with , and we will see all kinds of detailed information about the Oxford pet dataset. For example, in Figure 2, we can see there are a total of 7349 images with a built-in test/train split.\n\nLet’s first make a few changes to the downloaded data before we start training U-Net with it.\n\nFirst, we need to resize the images and masks to :\n\nWe then create a function to augment the dataset by flipping them horizontally:\n\nWe create a function to normalize the dataset by scaling the images to the range of and decreasing the image mask by :\n\nWe create two functions to preprocess the training and test datasets with a slight difference between the two – we only perform image augmentation on the training dataset.\n\nNow we are ready to build an input pipeline with by using the function:\n\nIf we execute , we will notice that the image is in the shape of of while the image mask is in the shape of with the data type of .\n\nWe define a batch size of and a buffer size of for creating batches of training and test datasets. With the original TFDS dataset, there are 3680 training samples and 3669 test samples, which are further split into validation/test sets. We will use the and the for training the U-Net model. After the training finishes, we will then use the to test the model predictions.\n\nNow the datasets are ready for training. Let’s visualize a random sample image and its mask from the training dataset, to get an idea of how the data looks.​​\n\nThe sample input image of a cat is in the shape of . The true mask has three segments: the green background; the purple foreground object, in this case, a cat; and the yellow outline. Figure 3 shows both the original input image and the true mask image.\n\nNow that we have the data ready for training, let’s define the U-Net model architecture. As mentioned earlier, the U-Net is shaped like a letter U with an encoder, decoder, and the skip connections between them. So we will create a few building blocks to make the U-Net model.\n\nFirst, we create a function with layers , which we will use in both the encoder (or the contracting path) and the bottleneck of the U-Net.\n\nThen we define a function for downsampling or feature extraction to be used in the encoder.\n\nFinally, we define an upsampling function for the decoder (or expanding path) of the U-Net.\n\nThere are three options for making a Keras model, as well explained in Adrian’s blog and the Keras documentation:\n• Functional API: more flexible and allows non-linear topology, shared layers, and multiple inputs or multi-outputs.\n• subclassing: most flexible and best for complex models that need custom training loops.\n\nU-Net has a fairly simple architecture; however, to create the skip connections between the encoder and decoder, we will need to concatenate some layers. So the Keras Functional API is most appropriate for this purpose.\n\nFirst, we create a function, specify the inputs, encoder layers, bottleneck, decoder layers, and finally the output layer with with activation of . Note the input image shape is . The output has three channels corresponding to the three classes that the model will classify each pixel for: background, foreground object, and object outline.\n\nWe call the function to create the model :\n\nAnd we can visualize the model architecture with to see each detail of the model. And we can use a Keras utils function called to generate a more visual diagram, including the skip connections. The generated image generated in Colab is rotated 90 degrees so that you can see U-shaped architecture in Figure 4 (see the details better in the Colab notebook):\n\nTo compile , we specify the optimizer, the loss function, and the accuracy metrics to track during training:\n\n\n\nWe train the by calling and training it for 20 epochs.\n\nAfter training for 20 epochs, we get a training accuracy and a validation accuracy of . The learning curve during training indicates that the model is doing well on both the training dataset and validation set, which indicates the model is generalizing well without much overfitting (Figure 5).\n\nNow that we have completed training the , let’s use it to make predictions on a few sample images of the test dataset.\n\nSee Figure 6 for the input images, the true masks, and the masks predicted by the U-Net model we trained.\n\nIn this post, you have learned how to load the Oxford-IIIT pet data with the TensorFlow dataset and how to train an image segmentation U-Net model from scratch. We created the U-Net with Keras Functional API and visualized the U-shaped architecture with skip connections. This post has been inspired by the official TensorFlow.org image segmentation tutorial and the U-Net tutorial on Keras.io, which uses for loading the data and has an Xception-style U-Net architecture. U-Net is a great start for learning semantic segmentation on images. To learn more about this topic, read segmentation papers on modern models such as DeepLab V3, HRNet, U2-Net, etc., among many other papers.\n\nTo download the source code to this post (and be notified when future tutorials are published here on PyImageSearch), simply enter your email address in the form below!"
    }
]