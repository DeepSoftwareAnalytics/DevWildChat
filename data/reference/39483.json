[
    {
        "link": "https://geeksforgeeks.org/huffman-coding-greedy-algo-3",
        "document": "Huffman coding is a lossless data compression algorithm. The idea is to assign variable-length codes to input characters, lengths of the assigned codes are based on the frequencies of corresponding characters. \n\nThe variable-length codes assigned to input characters are Prefix Codes, means the codes (bit sequences) are assigned in such a way that the code assigned to one character is not the prefix of code assigned to any other character. This is how Huffman Coding makes sure that there is no ambiguity when decoding the generated bitstream. \n\nLet us understand prefix codes with a counter example. Let there be four characters a, b, c and d, and their corresponding variable length codes be 00, 01, 0 and 1. This coding leads to ambiguity because code assigned to c is the prefix of codes assigned to a and b. If the compressed bit stream is 0001, the de-compressed output may be ‚Äúcccd‚Äù or ‚Äúccb‚Äù or ‚Äúacd‚Äù or ‚Äúab‚Äù.\n\nSee this for applications of Huffman Coding. \n\nThere are mainly two major parts in Huffman Coding\n‚Ä¢ Traverse the Huffman Tree and assign codes to characters.\n\nThe method which is used to construct optimal prefix code is called Huffman coding.\n\nThis algorithm builds a tree in bottom up manner. We can denote this tree by\n\nLet, |c| be number of leaves\n\n|c| -1 are number of operations required to merge the nodes. Q be the priority queue which can be used while constructing binary heap.\n\nSteps to build Huffman Tree\n\nInput is an array of unique characters along with their frequency of occurrences and output is Huffman Tree.\n‚Ä¢ Create a leaf node for each unique character and build a min heap of all leaf nodes (Min Heap is used as a priority queue. The value of frequency field is used to compare two nodes in min heap. Initially, the least frequent character is at root)\n‚Ä¢ Extract two nodes with the minimum frequency from the min heap.\n‚Ä¢ Create a new internal node with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap.\n‚Ä¢ Repeat steps#2 and #3 until the heap contains only one node. The remaining node is the root node and the tree is complete.\n\nLet us understand the algorithm with an example:\n\nStep 1. Build a min heap that contains 6 nodes where each node represents root of a tree with single node.\n\nStep 2 Extract two minimum frequency nodes from min heap. Add a new internal node with frequency 5 + 9 = 14. \n\n\n\nNow min heap contains 5 nodes where 4 nodes are roots of trees with single element each, and one heap node is root of tree with 3 elements\n\nStep 3: Extract two minimum frequency nodes from heap. Add a new internal node with frequency 12 + 13 = 25\n\n\n\nNow min heap contains 4 nodes where 2 nodes are roots of trees with single element each, and two heap nodes are root of tree with more than one nodes\n\nStep 4: Extract two minimum frequency nodes. Add a new internal node with frequency 14 + 16 = 30\n\n\n\nStep 5: Extract two minimum frequency nodes. Add a new internal node with frequency 25 + 30 = 55\n\n\n\nStep 6: Extract two minimum frequency nodes. Add a new internal node with frequency 45 + 55 = 100\n\n\n\nNow min heap contains only one node.\n\nSince the heap contains only one node, the algorithm stops here.\n\nSteps to print codes from Huffman Tree:\n\nTraverse the tree formed starting from the root. Maintain an auxiliary array. While moving to the left child, write 0 to the array. While moving to the right child, write 1 to the array. Print the array when a leaf node is encountered.\n\n\n\nThe codes are as follows:\n\nBelow is the implementation of above approach:\n\nTime complexity: O(nlogn) where n is the number of unique characters. If there are n nodes, extractMin() is called 2*(n ‚Äì 1) times. extractMin() takes O(logn) time as it calls minHeapify(). So, the overall complexity is O(nlogn).\n\nIf the input array is sorted, there exists a linear time algorithm. We will soon be discussing this in our next post.\n‚Ä¢ They are used for transmitting fax and text.\n‚Ä¢ They are used by conventional compression formats like PKZIP, GZIP, etc.\n‚Ä¢ Multimedia codecs like JPEG, PNG, and MP3 use Huffman encoding(to be more precise the prefix codes).\n\nIt is useful in cases where there is a series of frequently occurring characters.\n\nReference:\n\nhttp://en.wikipedia.org/wiki/Huffman_coding\n\nThis article is compiled by Aashish Barnwal and reviewed by GeeksforGeeks team."
    },
    {
        "link": "https://geeksforgeeks.org/huffman-coding-in-python",
        "document": "Huffman Coding is one of the most popular lossless data compression techniques. This article aims at diving deep into the Huffman Coding and its implementation in Python.\n\nHuffman Coding is an approach used in lossless data compression with the primary objective of delivering reduced transit size without any loss of meaningful data content. There is a key rule that lies in Huffman coding: use shorter codes for frequent letters and longer ones for uncommon letters. Through the employment of the binary tree, called the Huffman tree, the frequency of each character is illustrated, where each leaf node represents the character and its frequency. Shorter codes which are arrived at by traveling from the tree root to the leaf node representing the character, are the ones that are applied.\n\nFirst of all, we define a class to instantiate Huffman nodes. Every node includes the details such as character frequency, rank of its left and right children.\n\nNow, we design a function to construct our Huffman tree. We apply priority queue (heap) to link the nodes according to the lowest frequencies, and when the only one node is left there, it roots the Huffman tree.Ôªø\n\nFor that, next we move across the Huffman tree to produce the Huffman codes for each character. Raw binary code is built from the left (with no preceding bit) to the right (bit is one), until occurring upon the character's leaf.\n\nBelow is the implementation of above approach:\n\nThe Huffman Coding is an effective algorithm for data compression because it saved both storage space and transmission time. Through the effective assignment of symbol codes of variable length to the symbols by following the rule of higher frequency to lower code, Huffman coding optimizes the compression ratio and maintain the soundness of the data."
    },
    {
        "link": "https://w3schools.com/dsa/dsa_ref_huffman_coding.php",
        "document": "Huffman Coding is an algorithm used for lossless data compression.\n\nHuffman Coding is also used as a component in many different compression algorithms. It is used as a component in lossless compressions such as zip, gzip, and png, and even as part of lossy compression algorithms like mp3 and jpeg.\n\nUse the animation below to see how a text can be compressed using Huffman Coding.\n\nThe animation shows how the letters in a text are normally stored using UTF-8, and how Huffman Coding makes it possible to store the same text with fewer bits.\n\nHuffman Coding uses a variable length of bits to represent each piece of data, with a shorter bit representation for the pieces of data that occurs more often.\n\nFurthermore, Huffman Coding ensures that no code is the prefix of another code, which makes the compressed data easy to decode.\n\nData compression is when the original data size is reduced, but the information is mostly, or fully, kept. Sound or music files are for example usually stored in a compressed format, roughly just 10% of the original data size, but with most of the information kept. Lossless means that even after the data is compressed, all the information is still there. This means that for example a compressed text still has all the same letters and characters as the original. Lossy is the other variant of data compression, where some of the original information is lost, or sacrificed, so that the data can be compressed even more. Music, images, and video is normally stored and streamed with lossy compression like mp3, jpeg, and mp4.\n\nTo get a better understanding of how Huffman Coding works, let's create a Huffman code manually, using the same text as in the animation: 'lossless'.\n\nA text is normally stored in the computer using UTF-8, which means that each letter is stored using 8 bits for normal latin letters, like we have in 'lossless'. Other letters or symbols such as '‚Ç¨' or 'ü¶Ñ' are stored using more bits.\n\nTo compress the text 'lossless' using Huffman Coding, we start by counting each letter.\n\nAs you can see in the nodes above, 's' occurs 4 times, 'l' occurs 2 times, and 'o' and 'e' occurs just 1 time each.\n\nWe start building the tree with the least occurring letters 'o' and 'e', and their parent node gets count '2', because the counts for letter 'o' and 'e' are summarized.\n\nThe next nodes that get a new parent node, are the nodes with the lowest count: 'l', and the parent node of 'o' and 'e'.\n\nNow, the last node 's' must be added to the binary tree. Letter node 's' and the parent node with count '4' get a new parent node with count '8'.\n\nFollowing the edges from the root node, we can now determine the Huffman code for each letter in the word 'lossless'.\n\nThe Huffman code for each letter can now be found under each letter node in the image above. A good thing about Huffman coding is that the most used data pieces get the shortest code, so just '0' is the code for the letter 's'.\n\nAs mentioned earlier, such normal latin letters are usually stored with UTF-8, which means they take up 8 bits each. So for example the letter 'o' is stored as '01101111' with UTF-8, but it is stored as '110' with our Huffman code for the word 'lossless'.\n\nTo summarize, we have now compressed the word 'lossless' from its UTF-8 code\n\nusing Huffman Coding, which is a huge improvement.\n\nBut if data is stored with Huffman Coding as , or the code is sent to us, how can it be decoded so that we see what information the Huffman code contains?\n\nFurthermore, the binary code is really , without the spaces, and with variable bit lengths for each piece of data, so how can the computer understand where the binary code for each piece of data starts and ends?\n\nJust like with code stored as UTF-8, which our computers can already decode to the correct letters, the computer needs to know which bits represent which piece of data in the Huffman code.\n\nSo along with a Huffman code, there must also be a conversion table with information about what the Huffman binary code is for each piece of data, so that it can be decoded.\n\nSo, for this Huffman code:\n\nAre you able to decode the Huffman code?\n\nWe start with the first bit:\n\nThere is no letter in the table with just as the Huffman code, so we continue and include the next bit as well.\n\nWe can see from the table that is 'b', so now we have the first letter. We check the next bit:\n\nWe find that is 'a', so now we have the two first letters 'ba' stored in the Huffman code.\n\nWe continue looking up Huffman codes in the table:\n\nThe Huffman code is now decoded, and the word is 'banana'!\n\nAn interesting and very useful part of the Huffman coding algorithm is that it ensures that there is no code that is the prefix of another code.\n\nThis property, that no code is the prefix of another code, makes it possible to decode. And it is especially important in Huffman Coding because of the variable bit lengths.\n\nThe correct word for creating Huffman code based on data or text is \"encoding\", and the opposite would be \"decoding\", when the original data or text is recreated based on the code.\n\nThe code example below takes a word, or any text really, and compress it using Huffman Coding.\n\nIn addition to encode data using Huffman coding, we should also have a way to decode it, to recreate the original information.\n\nThe implementation below is basically the same as the previous code example, but with an additional function for decoding the Huffman code.\n\nThe function takes the Huffman code, and the Python dictionary (a hashmap) with the characters and their corresponding binary codes. The Function then reverse the mapping, and checks the Huffman code bit-by-bit to recreate the original text.\n\nYou have now seen how a text can be compressed using Huffman coding, and how a Huffman code can be decoded to recreate the original text."
    },
    {
        "link": "https://reintech.io/blog/python-huffman-coding-problem",
        "document": "Python, an immensely versatile and powerful programming language, is widely utilized by software developers to tackle complex problems, including the Huffman Coding Problem. This tutorial provides a comprehensive guide on how to implement Huffman Coding using Python.\n\nHuffman Coding is a lossless data compression algorithm. The core idea of the algorithm is to assign variable-length codes to input characters, lengths are based on the frequencies of corresponding characters. The most frequent character gets the smallest code and the least frequent character gets the largest code.\n\nFirst, Huffman Coding starts with constructing a Huffman Tree. This is a type of binary tree where each node represents a character and its frequency.\n\nNext, we convert the frequency table into a heap. Python's heapq module provides an implementation of the heap queue algorithm, also known as the priority queue algorithm.\n\nStep 3: Constructing the Huffman Tree from the Heap\n\nWe then convert the heap into a Huffman tree. This is done by removing two nodes from the heap at a time and replacing them with a new node with the two removed nodes as children.\n\nFinally, we generate the Huffman codes. This is done by traversing the Huffman tree, appending a '0' every time we take a left child, and appending a '1' every time we take a right child.\n\nThis tutorial has provided a comprehensive guide on implementing Huffman Coding using Python. Whether you're a seasoned developer or looking to hire Python developers, understanding such algorithms can add immense value."
    },
    {
        "link": "https://stackoverflow.com/questions/25700602/huffman-encoding-with-variable-length-symbols",
        "document": "I'm thinking of using a Huffman code to compress text, but with symbols of variable length (strings). For example (using an underscore as a space):\n\nHow can I construct the frequency table? Obviously there are some overlapping issues, the sequence would appear neary as often as , but would be useless in the table (both and have short huffman code).\n\nDoes such an algorithm exists? Does it have a special name? What would be the tricks to generate the frequency table? Do I need to tokenize the input? I did not found anything in the litterature / web. (All this make me think also of radix trees).\n\nI was thinking of using an iterative process:\n‚Ä¢ Generate an huffman tree for all symbols of length 1 to N\n‚Ä¢ Remove from the tree all symbols with N>1 and below a certain count threshold\n‚Ä¢ Regenerate a second huffman tree, but this time tokenizing the input with the previous one (probably using a radix tree for lookup)\n‚Ä¢ Repeat to 1 until we converge (or for a few times)\n\nBut I can't figure out how can I prevent the problem of overlaps ( vs ) with this."
    },
    {
        "link": "https://algs4.cs.princeton.edu/55compression",
        "document": "Data compression: reduces the size of a file to save space when storing it and to save time when transmitting it. Moore's law: # transistor on a chip doubles every 18-24 months. Parkinson's law: data expands to fill available space. Text, images, sound, video, etc. Wikipedia provides public dumps of all content for academic research and republishing. Uses bzip and SevenZip's LZMA. Can take a week to compress of 300GB of data.\n\nMorse code, decimal number system, natural language, rotary phones (lower numbers were quicker to dial, so New York was 212 and Chicago 312).\n\nWe use BinaryStdIn.java HexDump.java , and PictureDump.java\n\nNeed ceil(lg R) bits to specify one of R symbols. Genome.java . Uses Alphabet.java\n\nDesire unique decodable codes. One way to achieve this is to append a special stop symbol to each codeword. Better approach: prefix-free codes: no string is a prefix of another. For example, { 01, 10, 0010, 1111 } is prefix free, but { 01, 10, 0010, 1010 } is not because 10 is a prefix of 1010.\n\nSpecific way to construct optimal prefix-free codes. Invented by David Huffman while a student at MIT in 1950. Huffman.java implements Huffman algorithm.\n‚Ä¢ Limit the number of elements in the symbol table (GIF = throw away and start over, Unix compress = throw away when not effective).\n‚Ä¢ Initially dictionary has 512 elements (with 256 elements filled in for ASCII characters), so we transmit 9 bits per integer. When it fills up, we double it to 1024 and start transmitting 10 bits per integer.\n‚Ä¢ Only traverse the tree once (might break our string table abstraction).\n\nPractical issues: limit the number of elements in the symbol table.\n\nImpossible to compress all files (proof by simple counting argument). Intuitive argument: compress life work of Shakespeare, then compress result, then compress result again. If each file strictly shrinks, eventually you will be left with one bit.\n\nGuy Blelloch of CMU has an excellent chapter on data compression\n\nSuppose channel for sending information is noisy and each bit gets flipped with probability p. Send each bit 3 times; to decode take the majority of the 3 bits. Decoded bit is correct with probability 3p^2 - 2p^3. This is less than p (if p < 1/2). Can reduce probability of decoding the bit incorrectly by sending each bit k times, but this is wasteful in terms of the transmission rate.\n\nRoughly speaking, if channel capacity is C, then we can send bits at a rate slightly less than C with an encoding scheme that will reduce probability of a decoding error to any desired level. Proof is nonconstructive.\n‚Ä¢ Which of the following codes are prefix free? Uniquely decodable? For those that are uniquely decodable, give the encoding of 1000000000000.\n‚Ä¢ Given an example of a uniquely-decodable code that is not prefix free.\n‚Ä¢ Given an example of a uniquely-decodable code that is not prefix free or suffix free.\n‚Ä¢ Are { 1, 100000, 00 }, { 01, 1001, 1011, 111, 1110 }, and { 1, 011, 01110, 1110, 10011 } uniquely decodable? If not, find a string with two encodings. Solution. The first set of codewords is uniquely decodable. The second set of codewords is not uniquely decodable because 111-01-1110-01 and 1110-111-1001 are two decodings of 11101111001. The third set of codewords ins not uniquely decodable because 01110-1110-011 and 011-1-011-10011 are two decodings of 011101110011.\n‚Ä¢ Test for uniquely decodability. Implement the Sardinas-Patterson algorithm for testing whether a set of codewords is uniquely decodable: Add all of the codewords to a set. Examine all pairs of codewords to see if any one is a prefix of another; if so, extract the dangling suffix (i.e., the part of the longer string that is not a prefix of the shorter one). If the dangling suffix is a codeword, then the code is not uniquely decodable; otherwise, add the dangling suffix to the list (provided it is not already there). Repeat this process with the larger list until there are no remaining new dangling suffix. The algorithm is finite because all dangling suffixes added to the list are suffixes of a finite set of codewords, and a dangling suffix can be added at most once.\n‚Ä¢ { 0, 01, 11 }. The codeword 0 is a prefix of 01, so add the dangling suffix 1. { 0, 01, 11, 1 }. The codeword 0 is a prefix of 01, but the dangling suffix 1 is already in the list; the codeword 1 is a prefix of 11, but the dangling suffix 1 is already in the list. There are no other dangling suffixes, so conclude that the set is uniquely decodable.\n‚Ä¢ { 0, 01, 10 }. The codeword 0 is a prefix of 01, so add the dangling suffix 1 to the list. { 0, 01, 10, 1 }. The codeword 1 is a prefix of 10, but the dangling suffix 0 is a codewords. So, conclude that the code is not uniquely decodeable.\n‚Ä¢ Kraft-McMillan inequality. Conside a code C with N codewords of lengths n1, n2, ..., nN. Prove that if the code is uniquely decodable, then K(C) = sum_i = 1 to N 2^(-ni) ‚â§ 1.\n‚Ä¢ Kraft-McMillan construction. Suppose that we have a set of integers n1, n2, ..., nN that satisfy the inequality sum_i = 1 to N 2^(-ni) ‚â§ 1. Prove that it is always possible to find a prefix-free code with codewords lengths n1, n2, ..., nN. Thus, by restricting attention to prefix-free codes (instead of uniquely decodable codes), we do not lose much.\n‚Ä¢ Kraft-McMillan equality for optimal prefix-free codes. Prove that if C is an optimal prefix-free code then the Kraft-McMillan inequality is an equality: K(C) = sum_i = 1 to N 2^(-ni) = 1.\n‚Ä¢ Suppose that all of the symbol probabilities are negative powers of 2. Describe the Huffman code.\n‚Ä¢ Suppose that all of the symbol frequencies are equal. Describe the Huffman code.\n‚Ä¢ Find a Huffman code where the length of a symbol with probability pi is greater than ceil(-lg pi). Solution. .01 (000), .30 (001), .34 (01), .35 (1). The codeword 001 has length greater than ceil(-lg .30).\n‚Ä¢ True or false. Any optimal prefix-free code can be obtained via Huffman's algorithm. Solution. False. Consider the following set of symbols and frequencies (A 26, B 24, C 14, D 13, E 12, F 11). In any Huffman code, the codings for A and B must begin with different bits, but the code C3 does not have this property (yet it is an optimal prefix-free code).\n‚Ä¢ What is the LZW encoding of the following inputs?\n‚Ä¢ Characterize the tricky situation in LZW coding. Solution. Whenever it encounteres cScSc, where c is a symbol, S is a string, cS is in the dictionary but cSc is not.\n‚Ä¢ As a function of N, how many bits are needed to encode N copies of the symbol A? N copies of the sequence ABC?\n‚Ä¢ Let F(i) be the ith Fibonacci number. Consider N symbols, where the ith symbol has frequency F(i). Note that F(1) + F(2) + ... + F(N) = F(N+2) - 1. Describe the Huffman code.\n‚Ä¢ Show that there are at least 2^(N-1) different Huffman codes corresponding to a given set of N symbols. Solution. There are N-1 internal nodes and each one has an arbitrary choice to assign its left and right children.\n‚Ä¢ Give a Huffman code where the frequency of 0s in the output is much much higher than the frequency of 1s. Solution. If the character 'A' occurs one million times and the character 'B' occurs once, the code word for 'A' will be 0 and the codeword for 'B' will be 1.\n‚Ä¢ Prove the following facts about Huffman tries.\n‚Ä¢ The two longest codewords have the same length.\n‚Ä¢ If the frequency of symbol i is strictly larger than the frequency of symbol j, then the length of the codeword for symbol i is less than or equal to the length of the codeword for symbol j.\n‚Ä¢ Describe how to transmit a Huffman code (or optimal prefix-free code) on a set of symbols { 0, 1, ..., N-1 } using 2N - 1 + N ceil(lg N) bits. Hint: use 2N-1 bits to specify the structure of the corresponding trie.\n‚Ä¢ Suppose that in an extended ASCII file (8-bit characters), the maximum character frequency is at most twice the minimum character frequency. Prove that and fixed-length 8-bit extended ASCII code is optimal.\n‚Ä¢ Shannon-Fano coding. Prove that the following top-down version of Huffman's algorithm is not optimal. Split the set of codewords C into two subsets C1 and C2 with (almost) equal frequencies. Recursively build the tree for C1 and C2, starting all codewords for C1 with 0 and all codewords for C2 with 1. To implement the first step, Shannon and Fano propose sorting the codewords by frequency and breaking the set up into two subarrays as best as possible.\n‚Ä¢ LZMW coding (Miller-Wegman 1985). LZ variant: search input for longest string already in the dictionary (the current match); add concatenation of previous match to current match to the dictionary. Dictionary entries grow more rapidly. Can also delete low-frequency entries when the dictionary fills up. Hard to implement.\n‚Ä¢ LZAP coding. Similar to LZMW: instead of adding just the concatenation of the previous match with the current match, add the concatenation of the previous match with all prefixes of the current match. Easier than LZMW to implement, but even more dictionary entries.\n‚Ä¢ Identify an optimal code that is not prefix-free. Hint: only need 3 symbols with equal frequencies.\n‚Ä¢ Identify two optimal prefix-free codes for the same input that have a different distribution of codeword lengths.\n‚Ä¢ Minimum variance Huffman coding. Due to the nondeterminism associated with tiebraking, Huffman's algorithm may produce codes with different distributions of codeword lengths. When transmitting the compressed stream as it is being generated, it is desirable to transmit bits at a (near) constant rate. Find Huffman code that minimize sum_i (p_i (l_i - l_average(T)) ^2). Solution. When combining tries, break ties by picking the earliest produced trie with the smallest probability.\n‚Ä¢ Two-queue algorithm for Huffman coding. Prove that the following algorithm computes a Huffman code (and runs in linear time if the input symbols are already sorted by frequency). Maintain two FIFO queues: the first queue contains the input symbols, in ascending order of frequency, the second queue contains the internal nodes with combined weights. As long as there is more than one node in the two queues, dequeue the two nodes with the smallest weight by examining the fronts of both queues. Create a new internal node (left and right child = two nodes, weight = sum of weight of two nodes) and enqueue on the second queue. To obtain a minimum variance Huffman code, break ties by choosing nodes from the first queue. Hint: prove that the second queue is sorted in ascending order of frequency.\n‚Ä¢ Sibling property. A binary tree has the sibling property if (i) every node (except the root) has a sibling and (ii) the binary tree can be listed in non-increasing order of probability such that, in the list, all siblings are adjacent. Prove that a binary tree represents a Huffman tree if and only if it has the sibling property.\n‚Ä¢ Relative coding. Instead of compressing each pixel in an image, consider the difference between a pixel and the previous one and encode the difference. Intuition: usually the pixels don't change much. Use with LZW over color table alphabet.\n‚Ä¢ Variable-width LZW codes. Increase the width of the table from p to p+1 after 2^p th codeword is inserted into table. Used with color table alphabet.\n‚Ä¢ Adaptive Huffman coding. One-pass algorithm and don't need to send prefix-free code. Build Huffman tree based on frequency of characters read in so far. Update tree after reading in each character. Encoder and decoder need to coordinate on tie-breaking conventions.\n‚Ä¢ Shannon entropy. The entropy H of a discrete random variable X with possible values x1, ..., xN that occur with probability p1, ..., pN is defined as H(X) = -p1 lg p1 - p2 lg p2 - ... - pN lg pN, where 0 lg 0 = 0 is consistent with the limit.\n‚Ä¢ What is the entropy of a fair coin?\n‚Ä¢ What is the entropy of a coin where both sides are heads?\n‚Ä¢ What is the entropy of a six-sided die? Solution. -lg (1/6) which is about 2.584962.\n‚Ä¢ What is the entropy of the sum of two fair dice?\n‚Ä¢ Given a random variable that takes on N values. What distribution maximizes the entropy? The entropy is a fundamental concept in information theory. Shannon's source coding theorem asserts that to compress the data from a stream of independent and identically distributed random variables requires at least H(X) bits per symbol in the limit. For example, to send the results of a sequence of fair die tosses requires at least 2.584962 bits per die toss.\n‚Ä¢ Empirical entropy. The empirical entropy of a piece of text is obtained by computing the frequency of occurrence of each symbol and using these as the probabilities for a discrete random variable. Compute the empirical entropy of your favorite novel. Compare it to the data compression rate achieved by a Huffman code.\n‚Ä¢ Shannon experiment. Perform the following experiment. Give a subject a sequence of k letters from a piece of text (or Leipzig corpus) and ask them to predict the next letter. Estimate the fraction of times the subject gets the answer right for k = 1, 2, 5, 100.\n‚Ä¢ Give two different Huffman trees the string ABCCDD, with different heights.\n‚Ä¢ Prefix-free codes. Design an efficient algorithm to determine if a set of binary code words is prefix-free. Hint: use a binary trie or sort.\n‚Ä¢ Uniquely decodable code. Devise a uniquely decodable code that is not a prefix free code. Hint: suffix free codes = reverse of prefix free codes. Reverse of suffix free code is prefix free code -> can decode by reading compressed message in reverse order. Not very convenient.\n‚Ä¢ Huffman tree. Modify Huffman.java so that the encoder prints out the lookup table instead of the preorder traversal, and modify the decoder so that it constructs the tree by reading in the lookup table.\n‚Ä¢ True or false. In an optimal prefix-free ternary code, the three symbols that occur least frequently have the same length.\n‚Ä¢ Ternary Huffman codes. Generalize the Huffman algorithm to codewords over the ternary alphabet (0, 1, and 2) instead of the binary alphabet. That is, given a bytestream, find a prefix-free ternary code that uses as few trits (0s, 1s, and 2s) as possible. Prove that it yields optimal prefix-free ternary code. Solution. Combine smallest 3 probabilities at each step (instead of smallest 2). This works when there are 3 + 2k symbols for some integer k. To reduce to this case, add 1 or 2 dummy symbols of probability 0. (Alternatively, combine fewer than 3 symbols in the first step if the number of symbols is not 3 + 2k.) Ex: { 0.1, 0.2, 0.2, 0.5 }.\n‚Ä¢ Nonbinary Huffman codes. Extend the Huffman algorithm to codewords over the m-ary alphabet (0, 1, 2, ..., m-1) instead of the binary alphabet.\n‚Ä¢ Consider the following 21 character message that consists of 3 a's, 7c's, 6 t's and 5 g's. a a c c c c a c t t g g g t t t t c c g g Are the following 43 bits a possible Huffman encoding of the message above? Justify your answer as concisely and rigorously as possible. Solution. A Huffman encoding for a message produces an encoding that uses the fewest bits among any prefix free code. The 2-bit binary code a = 00, c = 01, g = 10, t = 11 is a prefix free code that uses 21 * 2 = 42 bits. Thus, a Huffman code would use fewer than 43 bits.\n‚Ä¢ A binary tree is full if every node that is not a leaf has two children. Prove that any binary tree corresponding to an optimal prefix-free code is full. Hint: if an internal node has only one child, replace that internal node with its unique child.\n‚Ä¢ Move-to-front coding (Bentley, Sleator, Tarjan, and Wei 1986). Write a program that implements move-to-front encoding and decoding. Maintain alphabet of symbols in a list, where frequently occurring symbols are towards the front. A symbol is encoded as the number of symbols that precede it in the list. After encoding a symbol, move it to the front of the list. reference\n‚Ä¢ Move-ahead-k coding. Same as move-to-front coding, but move symbol k positions toward the front.\n‚Ä¢ Wait-c-and-move. Same as move-to-front coding, but move symbol to the front only after it has been encountered c times since the last time it was moved to the front.\n‚Ä¢ Double Huffman compression. Find an input for which applying the method in Huffman.java twice leads to a strictly smaller output than applying only once.\n‚Ä¢ Merging k sorted arrays. You have k sorted lists, of lenths n1, n2, ..., nk. Supposet that the only operation you can perform to combine lists is a 2-way merge: given one sorted array of length n1 and another sorted array of length n2, replace them with a sorted array of length n = n1 + n2. Moreover, the 2-way merge operation takes exactly n units of time. What is the optimal way to merge the k sorted arrays? Solution. Sort the list lengths so that n1 < n2 < ... < nk. Repetedly take the two smallest lists and apply the 2-way merge operation. The proof of optimality is the same as the proof of optimality of Huffman codes: repeatedly applying 2-way merge operations induces a binary tree in which each leaf node corresponds to one of the original sorted lists and each internal node corresponds to a 2-way merge operation. The contribution of any original list to the overall cost is the length of the list multiplied by its tree depth (because that is the number of times its elements are involved in a 2-way merge)."
    },
    {
        "link": "https://ics.uci.edu/~dhirschb/pubs/DC-Sec1.html",
        "document": "A brief introduction to information theory is provided in this section. The definitions and assumptions necessary to a comprehensive discussion and evaluation of data compression methods are discussed. The following string of characters is used to illustrate the concepts defined: = . A code is a mapping of source messages (words from the source alphabet ) into codewords (words of the code alphabet ). The source messages are the basic units into which the string to be represented is partitioned. These basic units may be single symbols from the source alphabet, or they may be strings of symbols. For string , = { a, b, c, d, e, f, g, }. For purposes of explanation, will be taken to be { 0, 1 }. Codes can be categorized as block-block, block-variable, variable-block or variable-variable, where block-block indicates that the source messages and codewords are of fixed length and variable-variable codes map variable-length source messages into variable-length codewords. A block-block code for is shown in Figure 1.1 and a variable-variable code is given in Figure 1.2. If the string were coded using the Figure 1.1 code, the length of the coded message would be 120; using Figure 1.2 the length would be 30.\n\nWhen source messages of variable length are allowed, the question of how a message ensemble (sequence of messages) is parsed into individual messages arises. Many of the algorithms described here are defined-word schemes. That is, the set of source messages is determined prior to the invocation of the coding scheme. For example, in text file processing each character may constitute a message, or messages may be defined to consist of alphanumeric and non-alphanumeric strings. In Pascal source code, each token may represent a message. All codes involving fixed-length source messages are, by default, defined-word codes. In free-parse methods, the coding algorithm itself parses the ensemble into variable-length sequences of symbols. Most of the known data compression methods are defined-word schemes; the free-parse model differs in a fundamental way from the classical coding paradigm.\n\nA code is distinct if each codeword is distinguishable from every other (i.e., the mapping from source messages to codewords is one-to-one). A distinct code is uniquely decodable if every codeword is identifiable when immersed in a sequence of codewords. Clearly, each of these features is desirable. The codes of Figure 1.1 and Figure 1.2 are both distinct, but the code of Figure 1.2 is not uniquely decodable. For example, the coded message 11 could be decoded as either or . A uniquely decodable code is a prefix code (or prefix-free code) if it has the prefix property, which requires that no codeword is a proper prefix of any other codeword. All uniquely decodable block-block and variable-block codes are prefix codes. The code with codewords { 1, 100000, 00 } is an example of a code which is uniquely decodable but which does not have the prefix property. Prefix codes are instantaneously decodable; that is, they have the desirable property that the coded message can be parsed into codewords without the need for lookahead. In order to decode a message encoded using the codeword set { 1, 100000, 00 }, lookahead is required. For example, the first codeword of the message 1000000001 is 1, but this cannot be determined until the last (tenth) symbol of the message is read (if the string of zeros had been of odd length, then the first codeword would have been 100000).\n\nA minimal prefix code is a prefix code such that if is a proper prefix of some codeword, then is either a codeword or a proper prefix of a codeword, for each letter in . The set of codewords { } is an example of a prefix code which is not minimal. The fact that is a proper prefix of the codeword requires that be either a codeword or a proper prefix of a codeword, and it is neither. Intuitively, the minimality constraint prevents the use of codewords which are longer than necessary. In the above example the codeword could be replaced by the codeword , yielding a minimal prefix code with shorter codewords. The codes discussed in this paper are all minimal prefix codes.\n\nIn this section, a code has been defined to be a mapping from a source alphabet to a code alphabet; we now define related terms. The process of transforming a source ensemble into a coded message is coding or encoding. The encoded message may be referred to as an encoding of the source ensemble. The algorithm which constructs the mapping and uses it to transform the source ensemble is called the encoder. The decoder performs the inverse operation, restoring the coded message to its original form. In addition to the categorization of data compression schemes with respect to message and codeword lengths, these methods are classified as either static or dynamic. A static method is one in which the mapping from the set of messages to the set of codewords is fixed before transmission begins, so that a given message is represented by the same codeword every time it appears in the message ensemble. The classic static defined-word scheme is Huffman coding [Huffman 1952]. In Huffman coding, the assignment of codewords to source messages is based on the probabilities with which the source messages appear in the message ensemble. Messages which appear more frequently are represented by short codewords; messages with smaller probabilities map to longer codewords. These probabilities are determined before transmission begins. A Huffman code for the ensemble is given in Figure 1.3. If were coded using this Huffman mapping, the length of the coded message would be 117. Static Huffman coding is discussed in Section 3.2. Other static schemes are discussed in Sections 2 and 3.\n\nAll of the adaptive methods are one-pass methods; only one scan of the ensemble is required. Static Huffman coding requires two passes: one pass to compute probabilities and determine the mapping, and a second pass for transmission. Thus, as long as the encoding and decoding times of an adaptive method are not substantially greater than those of a static method, the fact that an initial scan is not needed implies a speed improvement in the adaptive case. In addition, the mapping determined in the first pass of a static coding scheme must be transmitted by the encoder to the decoder. The mapping may preface each transmission (that is, each file sent), or a single mapping may be agreed upon and used for multiple transmissions. In one-pass methods the encoder defines and redefines the mapping dynamically, during transmission. The decoder must define and redefine the mapping in sympathy, in essence \"learning\" the mapping as codewords are received. Adaptive methods are discussed in Sections 4 and 5.\n\nAn algorithm may also be a hybrid, neither completely static nor completely dynamic. In a simple hybrid scheme, sender and receiver maintain identical codebooks containing static codes. For each transmission, the sender must choose one of the previously-agreed-upon codes and inform the receiver of his choice (by transmitting first the \"name\" or number of the chosen code). Hybrid methods are discussed further in Section 2 and Section 3.2. In order to discuss the relative merits of data compression techniques, a framework for comparison must be established. There are two dimensions along which each of the schemes discussed here may be measured, algorithm complexity and amount of compression. When data compression is used in a data transmission application, the goal is speed. Speed of transmission depends upon the number of bits sent, the time required for the encoder to generate the coded message, and the time required for the decoder to recover the original ensemble. In a data storage application, although the degree of compression is the primary concern, it is nonetheless necessary that the algorithm be efficient in order for the scheme to be practical. For a static scheme, there are three algorithms to analyze: the map construction algorithm, the encoding algorithm, and the decoding algorithm. For a dynamic scheme, there are just two algorithms: the encoding algorithm, and the decoding algorithm.\n\nSeveral common measures of compression have been suggested: redundancy [Shannon and Weaver 1949], average message length [Huffman 1952], and compression ratio [Rubin 1976; Ruth and Kreutzer 1972]. These measures are defined below. Related to each of these measures are assumptions about the characteristics of the source. It is generally assumed in information theory that all statistical parameters of a message source are known with perfect accuracy [Gilbert 1971]. The most common model is that of a discrete memoryless source; a source whose output is a sequence of letters (or messages), each letter being a selection from some fixed alphabet ,... The letters are taken to be random, statistically independent selections from the alphabet, the selection being made according to some fixed probability assignment ( ),... [Gallager 1968]. Without loss of generality, the code alphabet is assumed to be {0,1} throughout this paper. The modifications necessary for larger code alphabets are straightforward.\n\nIt is assumed that any cost associated with the code letters is uniform. This is a reasonable assumption, although it omits applications like telegraphy where the code symbols are of different durations. The assumption is also important, since the problem of constructing optimal codes over unequal code letter costs is a significantly different and more difficult problem. Perl et al. and Varn have developed algorithms for minimum-redundancy prefix coding in the case of arbitrary symbol cost and equal codeword probability [Perl et al. 1975; Varn 1971]. The assumption of equal probabilities mitigates the difficulty presented by the variable symbol cost. For the more general unequal letter costs and unequal probabilities model, Karp has proposed an integer linear programming approach [Karp 1961]. There have been several approximation algorithms proposed for this more difficult problem [Krause 1962; Cot 1977; Mehlhorn 1980].\n\nWhen data is compressed, the goal is to reduce redundancy, leaving only the informational content. The measure of information of a source message (in bits) is -lg ( ) [lg denotes the base 2 logarithm]. This definition has intuitive appeal; in the case that ( =1, it is clear that is not at all informative since it had to occur. Similarly, the smaller the value of ( , the more unlikely is to appear, hence the larger its information content. The reader is referred to Abramson for a longer, more elegant discussion of the legitimacy of this technical definition of the concept of information [Abramson 1963, pp. 6-13]. The average information content over the source alphabet can be computed by weighting the information content of each source letter by its probability of occurrence, yielding the expression SUM{i=1 to n} [- ( ( )) lg ( ( ))]. This quantity is referred to as the of a source letter, or the entropy of the source, and is denoted by . Since the length of a codeword for message ( ) must be sufficient to carry the information content of ( ), entropy imposes a lower bound on the number of bits required for the coded message. The total number of bits must be at least as large as the product of and the length of the source ensemble. Since the value of is generally not an integer, variable length codewords must be used if the lower bound is to be achieved. Given that message is to be encoded one letter at a time, the entropy of its source can be calculated using the probabilities given in Figure 1.3: = 2.894, so that the minimum number of bits contained in an encoding of is 116. The Huffman code given in Section 1.2 does not quite achieve the theoretical minimum in this case.\n\nBoth of these definitions of information content are due to Shannon. A derivation of the concept of entropy as it relates to information theory is presented by Shannon [Shannon and Weaver 1949]. A simpler, more intuitive explanation of entropy is offered by Ash [Ash 1965].\n\nThe most common notion of a \"good\" code is one which is optimal in the sense of having minimum redundancy. Redundancy can be defined as: SUM ( ( )) ( ) - SUM [- ( ( )) lg ( ( ))] where ( ) is the length of the codeword representing message ( ). The expression SUM ( ( )) ( ) represents the lengths of the codewords weighted by their probabilities of occurrence, that is, the average codeword length. The expression SUM [- ( ( )) lg ( ( ))] is entropy, . Thus, redundancy is a measure of the difference between average codeword length and average information content. If a code has minimum average codeword length for a given discrete probability distribution, it is said to be a minimum redundancy code.\n\nWe define the term local redundancy to capture the notion of redundancy caused by local properties of a message ensemble, rather than its global characteristics. While the model used for analyzing general-purpose coding techniques assumes a random distribution of the source messages, this may not actually be the case. In particular applications the tendency for messages to cluster in predictable patterns may be known. The existence of predictable patterns may be exploited to minimize local redundancy. Examples of applications in which local redundancy is common, and methods for dealing with local redundancy, are discussed in Section 2 and Section 6.2.\n\nHuffman uses average message length, SUM ( ( )) ( ), as a measure of the efficiency of a code. Clearly the meaning of this term is the average length of a coded message. We will use the term average codeword length to represent this quantity. Since redundancy is defined to be average codeword length minus entropy and entropy is constant for a given probability distribution, minimizing average codeword length minimizes redundancy.\n\nA code is asymptotically optimal if it has the property that for a given probability distribution, the ratio of average codeword length to entropy approaches 1 as entropy tends to infinity. That is, asymptotic optimality guarantees that average codeword length approaches the theoretical minimum (entropy represents information content, which imposes a lower bound on codeword length).\n\nThe amount of compression yielded by a coding scheme can be measured by a compression ratio. The term compression ratio has been defined in several ways. The definition = (average message length)/(average codeword length) captures the common meaning, which is a comparison of the length of the coded message to the length of the original ensemble [Cappellini 1985]. If we think of the characters of the ensemble as 6-bit ASCII characters, then the average message length is 6 bits. The Huffman code of Section 1.2 represents in 117 bits, or 2.9 bits per character. This yields a compression ratio of 6/2.9, representing compression by a factor of more than 2. Alternatively, we may say that Huffman encoding produces a file whose size is 49% of the original ASCII file, or that 49% compression has been achieved. A somewhat different definition of compression ratio, by Rubin, = ( - - )/ , includes the representation of the code itself in the transmission cost [Rubin 1976]. In this definition represents the length of the source ensemble, the length of the output (coded message), and the size of the \"output representation\" (eg., the number of bits required for the encoder to transmit the code mapping to the decoder). The quantity constitutes a \"charge\" to an algorithm for transmission of information about the coding scheme. The intention is to measure the total size of the transmission (or file to be stored). As discussed in the Introduction, data compression has wide application in terms of information storage, including representation of the abstract data type string [Standish 1980] and file compression. Huffman coding is used for compression in several file archival systems [ARC 1986; PKARC 1987], as is Lempel-Ziv coding, one of the adaptive schemes to be discussed in Section 5. An adaptive Huffman coding technique is the basis for the compact command of the UNIX operating system, and the UNIX compress utility employs the Lempel-Ziv approach [UNIX 1984].\n\nIn the area of data transmission, Huffman coding has been passed over for years in favor of block-block codes, notably ASCII. The advantage of Huffman coding is in the average number of bits per character transmitted, which may be much smaller than the lg bits per character (where is the source alphabet size) of a block-block system. The primary difficulty associated with variable-length codewords is that the rate at which bits are presented to the transmission channel will fluctuate, depending on the relative frequencies of the source messages. This requires buffering between the source and the channel. Advances in technology have both overcome this difficulty and contributed to the appeal of variable-length codes. Current data networks allocate communication resources to sources on the basis of need and provide buffering as part of the system. These systems require significant amounts of protocol, and fixed-length codes are quite inefficient for applications such as packet headers. In addition, communication costs are beginning to dominate storage and processing costs, so that variable-length coding schemes which reduce communication costs are attractive even if they are more complex. For these reasons, one could expect to see even greater use of variable-length coding in the future.\n\nIt is interesting to note that the Huffman coding algorithm, originally developed for the efficient transmission of data, also has a wide variety of applications outside the sphere of data compression. These include construction of optimal search trees [Zimmerman 1959; Hu and Tucker 1971; Itai 1976], list merging [Brent and Kung 1978], and generating optimal evaluation trees in the compilation of expressions [Parker 1980]. Additional applications involve search for jumps in a monotone function of a single variable, sources of pollution along a river, and leaks in a pipeline [Glassey and Karp 1976]. The fact that this elegant combinatorial algorithm has influenced so many diverse areas underscores its importance."
    },
    {
        "link": "https://geeksforgeeks.org/huffman-coding-greedy-algo-3",
        "document": "Huffman coding is a lossless data compression algorithm. The idea is to assign variable-length codes to input characters, lengths of the assigned codes are based on the frequencies of corresponding characters. \n\nThe variable-length codes assigned to input characters are Prefix Codes, means the codes (bit sequences) are assigned in such a way that the code assigned to one character is not the prefix of code assigned to any other character. This is how Huffman Coding makes sure that there is no ambiguity when decoding the generated bitstream. \n\nLet us understand prefix codes with a counter example. Let there be four characters a, b, c and d, and their corresponding variable length codes be 00, 01, 0 and 1. This coding leads to ambiguity because code assigned to c is the prefix of codes assigned to a and b. If the compressed bit stream is 0001, the de-compressed output may be ‚Äúcccd‚Äù or ‚Äúccb‚Äù or ‚Äúacd‚Äù or ‚Äúab‚Äù.\n\nSee this for applications of Huffman Coding. \n\nThere are mainly two major parts in Huffman Coding\n‚Ä¢ Traverse the Huffman Tree and assign codes to characters.\n\nThe method which is used to construct optimal prefix code is called Huffman coding.\n\nThis algorithm builds a tree in bottom up manner. We can denote this tree by\n\nLet, |c| be number of leaves\n\n|c| -1 are number of operations required to merge the nodes. Q be the priority queue which can be used while constructing binary heap.\n\nSteps to build Huffman Tree\n\nInput is an array of unique characters along with their frequency of occurrences and output is Huffman Tree.\n‚Ä¢ Create a leaf node for each unique character and build a min heap of all leaf nodes (Min Heap is used as a priority queue. The value of frequency field is used to compare two nodes in min heap. Initially, the least frequent character is at root)\n‚Ä¢ Extract two nodes with the minimum frequency from the min heap.\n‚Ä¢ Create a new internal node with a frequency equal to the sum of the two nodes frequencies. Make the first extracted node as its left child and the other extracted node as its right child. Add this node to the min heap.\n‚Ä¢ Repeat steps#2 and #3 until the heap contains only one node. The remaining node is the root node and the tree is complete.\n\nLet us understand the algorithm with an example:\n\nStep 1. Build a min heap that contains 6 nodes where each node represents root of a tree with single node.\n\nStep 2 Extract two minimum frequency nodes from min heap. Add a new internal node with frequency 5 + 9 = 14. \n\n\n\nNow min heap contains 5 nodes where 4 nodes are roots of trees with single element each, and one heap node is root of tree with 3 elements\n\nStep 3: Extract two minimum frequency nodes from heap. Add a new internal node with frequency 12 + 13 = 25\n\n\n\nNow min heap contains 4 nodes where 2 nodes are roots of trees with single element each, and two heap nodes are root of tree with more than one nodes\n\nStep 4: Extract two minimum frequency nodes. Add a new internal node with frequency 14 + 16 = 30\n\n\n\nStep 5: Extract two minimum frequency nodes. Add a new internal node with frequency 25 + 30 = 55\n\n\n\nStep 6: Extract two minimum frequency nodes. Add a new internal node with frequency 45 + 55 = 100\n\n\n\nNow min heap contains only one node.\n\nSince the heap contains only one node, the algorithm stops here.\n\nSteps to print codes from Huffman Tree:\n\nTraverse the tree formed starting from the root. Maintain an auxiliary array. While moving to the left child, write 0 to the array. While moving to the right child, write 1 to the array. Print the array when a leaf node is encountered.\n\n\n\nThe codes are as follows:\n\nBelow is the implementation of above approach:\n\nTime complexity: O(nlogn) where n is the number of unique characters. If there are n nodes, extractMin() is called 2*(n ‚Äì 1) times. extractMin() takes O(logn) time as it calls minHeapify(). So, the overall complexity is O(nlogn).\n\nIf the input array is sorted, there exists a linear time algorithm. We will soon be discussing this in our next post.\n‚Ä¢ They are used for transmitting fax and text.\n‚Ä¢ They are used by conventional compression formats like PKZIP, GZIP, etc.\n‚Ä¢ Multimedia codecs like JPEG, PNG, and MP3 use Huffman encoding(to be more precise the prefix codes).\n\nIt is useful in cases where there is a series of frequently occurring characters.\n\nReference:\n\nhttp://en.wikipedia.org/wiki/Huffman_coding\n\nThis article is compiled by Aashish Barnwal and reviewed by GeeksforGeeks team."
    },
    {
        "link": "https://stringology.org/event/2020/psc20p03_presentation.pdf",
        "document": ""
    },
    {
        "link": "https://researchgate.net/publication/224621635_Non_prefix-free_codes_for_constrained_sequences",
        "document": "(slightly better) compression performance. The point is that\n\nwe are making a different use of the decoder knowledge\n\nabout possible transitions. Note that, even for the Huffman\n\ncode, we are supposing that the decoder exactly knows what\n\ntransitions are possible and what are not. The difference is\n\nthat with the non preÔ¨Åx-free code we are making the decoder\n\nmore active. This relates the presented idea to other dev eloped\n\ncoding paradigms. W e should note in fact, that in practice the\n\nproposed approach was already used in other contexts. One\n\nof the oldest examples may be that of modulo-PCM codes\n\n([9]) for numerical sequences; here only the modulo-4 value\n\nof every sample is encoded, leaving to the decoder the task\n\nof understanding the original value using its knowledge on\n\nthe memory of the source. In that case the used code is even\n\na singular code but under certain hypothesis this does not\n\naffect the decodability . Similar ideas are then used in the\n\nfor example [10] and references therein). Let us consider\n\nfor a moment the problem of noiseless separated coding of\n\ndependent sequences. The well known Slepian-W olf theorem\n\n([11]) says that two correlated memoryless sources, and\n\nY , can be separately lossless coded at rates\n\nrespectively when jointly decoded, if ,\n\nextended the result to the case of general ergodic sources in\n\n[12]. Roughly speaking, the used encoding process at every\n\nencoder consists on considering large blocks of symbols; the\n\nset of all such blocks is split into disjoint bins and only the\n\nindex of the bin that contains the extracted block is encoded.\n\nAt the decoder the original block for both sequences\n\nis recovered by extracting from the pair of speciÔ¨Åed bins the\n\nonly pair of jointly typical blocks. It is interesting to note that\n\nthis encoding technique actually uses singular codes in order\n\nto achieve compression, leaving to the decoder the task of\n\ndisambiguating, based on the joint statistic of the two sources.\n\nThe same idea is now being used, in order to shift the\n\ncomplexity from the encoder to the decoder, in what may\n\nprinciples‚Äù. Special attention in this Ô¨Åeld is being payed to\n\nthe case of video coding (see for example [13], [14]). In this\n\ncontest the memory of the video source is not exploited in\n\nthe encoding phase but in the decoding one. Again, roughly\n\nspeaking, in the encoding phase no motion compensation and\n\nprediction is applied and singular codes are used for the data\n\ncompression task. In the decoding phase, on the contrary,\n\nthe memory of the source is exploited in order to remove\n\nambiguity by using motion compensation. It is interesting\n\nto see that practical architectural speciÔ¨Åcation for this video\n\ncoding techniques have been presented only in recent years,\n\neven if the idea of such an approach to video coding was\n\nalready patented by Witsenhausen and W yner in late ‚Äô70s\n\n([15]). Moreover, it is also interesting to note that there is\n\n4 In our examples here we have considered only non singular codes with\n\nsingular extension. The non singularity of the code is required in our setting\n\nwhere we want to be able to decode a sequence composed of one single\n\nnot much difference in this approach with respect to the idea\n\nbehind the modulo-PCM coding above mentioned.\n\nAs a Ô¨Ånal comment on the general problem of Ô¨Ånding\n\nvariable length non preÔ¨Åx-free codes for a given constrained\n\nsequence, we note that there are many connections with the\n\narea of coding for constrained channels (see for example [16]).\n\nThe relation between the transition graphs of our sources\n\nand the possible codeword assignments may Ô¨Ånd interesting\n\ncounterparts (and maybe answers) in that Ô¨Åeld, where graph\n\ntheory has already been successfully applied.\n\n[3] J. Karush, ‚Äú A simple proof of an inequality of McMillan,‚Äù\n\n[5] A.A. Sardinas and G.W. Patterson, ‚ÄúA necessary and sufÔ¨Åcient condition\n\nfor the unique decomposition of coded messages,‚Äù in\n\n[12] T.M. Cover , ‚ÄúA proof of the data compression theorem of Slepian and\n\n[16] D. Lind and B. Marcus, An introduction to Symbolic Dynamics and"
    }
]