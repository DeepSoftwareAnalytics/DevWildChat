[
    {
        "link": "https://dylanmeeus.github.io/posts/audio-from-scratch-pt10",
        "document": "With everything that we have added to our library so far we are almost capable of generated small tunes. One thing that’s missing to make it sound more ‘natural’ is a way for the notes to start and stop.\n\nIn this post we will implement a type of envelope called “ADSR”, for “Attack, Decay, Sustain, Release”. Which will make the notes sound more natural as they are played in sequence.\n\nTo see why we need this, listen to this sound generated without an ADSR envelope around the generated frames.\n\nIf you want to read the (not pretty) code that generated this, check out this github gist.\n\nThe Attack, Decay, Sustain and Release envelope is a common type of envelope. Schematically this can be represented as below (from wikipedia):\n\nAs we apply this envelope to a signal, the signal will change in amplitude depending on which phase we are in of the ADSR envelope. In the image it is visible that the amplitude rises during the attack step, reaches a peak amplitude before decreasing a bit. After decreasing it reached the sustain amplitude, where it will stay until the note is released, and after release it decays until the ampltide is zero.\n\nFor our parameters, three will relate to time:\n• release (time to decay from sutan to zero)\n\nSo the Sustain parameter does not refer to time, but rather to the amplitude we will maintain.\n\nTurning this schematic into code, we get :\n\nOne parameter in this function that you don’t find in the schematic is the need for a control rate. The control rate will be used to turn a duration in seconds into an amount of frames. The control rate could just be sample rate but this does not necessarily have to be the case. One such use-case us sub-audio modulation, whereby the modulating oscillator is running below 20Hz. You can check this chapter for a bit more on that.\n\nTo apply the ADSR envelope to a signal, for example to one that was generating using the oscillator we created we have to iterate over each frame, pass the current frame to the ADSR function, and modify the amplitude of the frame with the result. For example, this is the complete example program included in GoAudio.\n\nIn this example, notice that our control rate is the same as our sample rate, and the adsrtime increases together with the frames that we have processed. (We could thus pass the iterating variable to the function, but I thought making it explit was clearer).\n\nIf you liked this and want to know when I write new posts, the best way to keep up to date is by following me on twitter."
    },
    {
        "link": "https://thewolfsound.com/envelopes",
        "document": "Everything on envelopes for coding your own synthesizer.\n\nPlease accept marketing cookies to access the video player.\n\nIf you have ever used a synthesizer, you definitely stumbled upon an envelope.\n\nSometimes it is referred to as contour or simply ADSR (the most popular type of the envelope).\n\nThese synthesizer elements help the musicians and sound designers to effortlessly make the sound more lively, more interesting. They achieve it via automatic control of:\n\nIn this article, you will learn what is an envelope, all the types of envelopes, and what to consider when implementing them.\n• What Is An Envelope?\n• What Can an Envelope Control?\n\nWhat Is An Envelope?\n\nFrom the digital signal processing (DSP) perspective, an envelope is a curve that outlines the extremes of a signal [Pluta2019].\n\nAs such, it relates to the analysis of the signal: we have a waveform, we connect its peaks and obtain an envelope.\n\nYou can see an example of a signal with its envelope in orange in Figure 1. Note that the envelope is two-sided in this case.\n\nFrom the sound synthesis perspective, an envelope is a curve that controls a certain parameter of the generated signal.\n\nAs such, it relates to the synthesis of the signal: we want to generate a certain waveform and, thus, we apply an envelope to it; an envelope is a control data source [Pluta2019].\n\nFor controlling, we use only the non-negative part of the envelope. Actually, the signal from Figure 1 was generated by applying an ADSR envelope to a sine.\n\nAccording to the Merriam-Webster dictionary, to envelop means “to enclose or enfold completely with or as if with a covering.” So an envelope is a curve that is enclosing the signal.\n\nIn this article, we consider the sound synthesis perspective of the envelope: we use it to control some parameter of the generated sound.\n\nWhat Can an Envelope Control?\n\nIn principle, an envelope can control just about anything.\n\nIn sound synthesis, it is typically used to control the amplitude, cutoff frequency of a low-pass filter, or the frequency of the generated signal.\n\nAn amplitude envelope is the most commonly found envelope application. Why?\n\nBecause they are everywhere!\n\nA fade-in and a fade-out of a song (also after pausing or playing your YouTube video) are forms of an envelope. But the origins of the amplitude envelope are far more ancient.\n\nOriginally, amplitude envelopes appeared with the invention of the first musical instruments. Every one of them has a characteristic amplitude envelope.\n\nPlucking a string has a sharp attack and an automatic release.\n\nHitting a key on the piano starts with an increasing volume, which then decays to a certain level. Then the sound is slowly fading until the key is released, which causes the sound to fade out completely.\n\nOn a more fine-grained level, each of the partials in the amplitude spectrum of an instrument sound can have its own amplitude envelope.\n\nSynthesizers tried to mimic the behavior of natural instruments and so they introduced predefined amplitude envelopes, somewhat simplified with respect to the naturally occurring ones.\n\nHere’s an example of a 220 Hz sine tone with the ADSR envelope from Figure 1 applied as an amplitude envelope.\n\nThis single change, the introduction of amplitude envelopes, sufficed to make the synthesizers sound more natural. But to make them sound even more natural, another envelope was needed…\n\nThe cutoff envelope controls the cutoff of a low-pass filter.\n\nWhen we hit a piano key, its timbre is bright at first (high energy in the high-frequency partials in the amplitude spectrum) and then softens (low energy in the high-frequency partials).\n\nSynthesizers imitate this by an envelope of the cutoff of a low-pass filter.\n\nWhen we hit a synthesizer key, the cutoff frequency rises, the sound becomes brighter and brighter. After some time (or after releasing the key), the sound becomes darker as the cutoff lowers and high-frequency components are more attenuated.\n\nHere’s an example of the ADSR envelope controlling the cutoff of a lowpass filter processing a 220 Hz sawtooth:\n\nAs you can hear, the cutoff envelope influences the amplitude envelope because by decreasing the energy of partials, it decreases the overall signal energy.\n\nI specifically mention the cutoff envelope not the cutoff frequency envelope. That is because we typically want the cutoff frequency to increase with the pitch of the key that we hit. Otherwise, high notes could be inaudible.\n\nThe cutoff envelope controls what percentage of the cutoff frequency should be set. Typically, the value of 1 (100%) means that the cutoff frequency corresponds to the value set by the user.\n\nSometimes the synthesizers allow the user to control the contour amount, i.e., the range of the cutoff change. For example, we may want to have the cutoff change only between 80% and 100% because starting the envelope always from 0% tends to sound too repetitive.\n\nIn some sound design scenarios, I can imagine envelopes controlling the frequency of an oscillator.\n\nIn these cases, the sound’s pitch would change over time according to the envelope.\n\nAs this is very specialized and does not concern traditional sound synthesizers (with a MIDI-based control), I won’t discuss it here in detail.\n\nIn analog sound synthesis, an envelope generator (EG) is a source of the control signal (the envelope).\n\nTherefore, on module connection diagrams, you can often see EG blocks connected to VCA blocks (voltage-controlled amplifiers), VCF blocks (voltage-controlled filters), or (in rare cases) to VCO blocks (voltage-controlled oscillators).\n\nThe connection between any module and an EG means that this EG is controlling a parameter of that module. For VCAs, that’s amplitude, for VCFs, it’s cutoff, and for VCOs, it’s frequency.\n\nNowadays, EG blocks are also used to depict the interconnections of digital modules but their meaning is the same: they are sources of a control signal, an envelope.\n\nAmplitude and cutoff envelopes are used for various purposes. For example, to\n• make the sound more natural by imitating real instruments’ envelopes,\n• make the sound less natural with obscure envelopes,\n• avoid clicks and other artifacts (e.g., via a fade-in and a fade-out).\n\nSome more specialized applications of envelopes in sound synthesis include\n• envelope of the amplitude and the cutoff in subtractive synthesis,\n• frequency envelope of an oscillator for sound design purposes.\n\nEnvelopes consist of segments (ramps). For example, the most popular Attack-Decay-Sustain-Release (ADSR) envelope consists of 4 segments: attack, decay, sustain, and release.\n\nThe segments are crude piece-wise approximations to the natural envelopes but they represent a good trade-off between the quality of the result and the complexity of control.\n\nThe following is a comprehensive (to my best knowledge) list of envelope segment types:\n• Delay: the amount of time between the note-on event and the start of the attack segment. Delaying the appearance of sound after a key-press is especially important in ambient music, where the musician can use this time to adjust the timbre parameters. We can control the length of this delay.\n• Attack: the initial portion of every envelope after a note-on event. In this segment the value is rising from the minimum envelope value to the maximum envelope value. When we “control the attack” we change the duration of this segment.\n• Hold: a segment where the envelope value is at its maximum; by controlling its length, we adjust how long will the controlled parameter be at its peak value.\n• Decay: the segment where the envelope falls from the peak value to the initial sustain value. We can control its length.\n• Sustain: the segment where the envelope maintains a constant level until a note-off event. We set the value of this level but its length is controlled by the performer.\n• Release: the final segment of any envelope, where the value falls from its current value to 0.\n\nA very important consideration when implementing any envelope is how its value should change.\n\nTo be exact, should the amplitude increase linearly or exponentially (linearly on the logarithmic scale)? Below is a comparison of these two approaches:\n\nFigure 3. A linear change in value (left) vs an exponential change (right).\n\nThe caveat here is that we perceive the exponential change as a linear one. To hear this, listen to these two examples.\n\nEach one plays a sine at 220 Hz.\n\nThis one has the linear attack envelope (left in Figure 3).\n\nThis one has the exponential attack envelope (right in Figure 3).\n\nWhich change sounds more “linearly” to you?\n\nFor me, the exponential envelope.\n\nIn the linear envelope case, I can hear the sound instantaneously and then it becomes kind of louder whereas in the exponential case, I can hear a steady increase in volume.\n\nThis applies to amplitude envelopes, what about cutoff or frequency envelopes?\n\nAs our perception of frequency is logarithmic as well, these envelopes should also use exponential segments to make the impression of a linear change.\n\nWarning: Although most depictions of envelopes show linear segments not exponential segments, in fact, an exponential change is meant. I follow this linear simplification in every depiction of an envelope in this article apart from Figure 1.\n\nAfter learning the building blocks of envelopes, now it is time to see what types of envelopes are out there.\n\nBelow, I listed all types of envelopes that exist based on [Pluta2019, Russ09].\n\nTheir visualizations were created by me but I was heavily inspired by those great books so they should take all the credit.\n\nSound examples were created by generating 5 seconds of a sine at 220 Hz and applying the specified envelope as the amplitude envelope.\n\nThe Attack-Decay (AD) envelope consists of just two segments. It is a kind of “one shot” envelope that you cannot sustain. When you release a key during the attack, the envelope transitions to the decay slope with the current amplitude value.\n\nThe Attack-Release (AR) envelope has 3 segments: attack, sustain and release. Sustain’s value is fixed to the maximum.\n\nThe Attack-Decay-Release (ADR) envelope has 3 segments. When the key is released, the envelope transitions to the release segment with the value it currently holds. It may happen that the envelope reaches 0 already in the decay segment. In such case, the release segment is omitted.\n\nThe Attack-Decay-Sustain (ADS) envelope has 4 segments, where that last is either a short, non-parameterized release segment (Figure 7) or a repeated decay segment (Figure 8).\n\nThe Attack-Decay-Sustain-Release (ADSR) envelope is in my experience the most popular envelope type. It is an approximation of the impression of most musical instruments. It is also easy to control. Its practical usefulness resulted in its popularity among synthesizer players.\n\nThe Attack-Hold-Decay-Sustain-Release (AHDSR) envelope in comparison to ADSR has an additional hold segment between the attack and the decay, whose duration is an adjustable parameter.\n\nThe Attack-Decay1-Break-Decay2-Release (ADBDR) envelope is my personal favorite because it approximates the amplitude envelope of the piano; while the key is being held, the sound slowly decays. This is opposite of the sustain segment in ADSR, which to my taste sounds a little bit artificial. The “break” element allows to set the value at which decay 1 transitions to decay 2.\n\nCurrent synthesizers are capable of having an arbitrary envelope: one consisting of many segments where the envelope value is rising, falling, or constant. Although these give you the complete control over the sound, they are hard to change during performance and tend to sound repetitive. Therefore, I would restrict their usage to ambient/sound design applications.\n\nAn example of a commercial synthesizer that allows an arbitrary envelope is Massive from Native Instruments (Figure 12).\n\nFigure 12. The user interface of the Massive synthesizer from Native Instruments (source).\n\nIn this article, you learned what is an envelope, what it can control, what are the possible envelope segments, and what is the difference between a linear and an exponential change in an envelope. Finally, you learned every possible type of envelope that exists. With this knowledge you are ready to use and code your own envelopes.\n\nEnvelopes is just one subject that you must learn if you want to develop audio plugins. To see which other subjects you should learn, download my free audio plugin developer checklist.\n\nThese two books are great resources on envelopes:\n\n[Pluta2019] Marek Pluta, Sound Synthesis for Music Reproduction and Performance, monograph, AGH University of Science and Technology Press 2019.\n\nLinks above may be affiliate links. That means that I may earn a commission if you decide to make a purchase. This does not incur any cost for you. Thank you."
    },
    {
        "link": "https://pytorch.org/audio/main/tutorials/oscillator_tutorial.html",
        "document": "Click here to download the full example code\n\nThis tutorial shows how to synthesize various waveforms using and ."
    },
    {
        "link": "https://adsrsounds.com/adsr_sample_manager_tutorials/using-samples-to-build-a-project-in-adsr-sample-manager?srsltid=AfmBOoojN70OeDORaFjxpMT32GNt-yAJQaUBZ0btAQfnP-lcQ8LGDlCq",
        "document": "ADSR Sample Manager makes your entire sample library searchable and audible directly in your DAW with smart and custom tags.\n\nPreview any sample in your library in the context of your track using MIDI or drag audio directly to your project.\n\nThe optimal starting point for every production session, ADSR Sample Manager makes organizing, finding and auditioning samples refreshingly simple.\n\nThe most effective way to access all your samples at once, ADSR Sample Manager keeps your project in the flow while keeping all the action right in your DAW where it should be:\n\n– All your samples searchable in an instant, directly in your DAW\n\n – Hear samples in the context of your music with MIDI input\n\n – Drag samples directly into your project\n\n – Use multiple instances directly in your project\n\n – Loops sync to your project tempo\n\n – Automatically tags all your samples in a flash\n\n – Add your own custom tags\n\n – Use with services like Splice, Noiiz and Loopcloud\n\n – VST & AU available\n\n – Save time and money on finding the perfect sound so you can spend both on making better music.\n\nEver gotten half way through a track and wished you’d tried a different sound? Do you feel like half your production time is lost to sorting through sample folders and files? Do you even really know what sounds you actually have in your current sample library?\n\nForget empty promises made at midnight that one day you’re going to clean up your hard drive. Download ADSR Sample Manager – it’s free and it works."
    },
    {
        "link": "https://forum.juce.com/t/implementation-of-adsr-in-samplervoice-leads-to-clicking-when-playing-notes-very-fast/28482",
        "document": "Hey all,\n\n I’m just starting out with JUCE so please bear with me\n\nWhat I am trying to build:\n\n I try to build a Drum-Sampler within the Juce Framework, where a Syntesiser-Object is supplied with derived classes from SynthesiserSound, where i hold the sample data plus an ADSR-envelope (I used this Implementation: ADSR by Nigel Redmon on earlevel.com) and a panning control plus a derived class from SynthesiserVoice, where all the rendering is happening.\n\n Because it is a Drum-Sampler no pitching is applied to the sounds and they only should be playing when a certain midi note gets triggerd.\n\nThe Implementation:\n\n In my class derived from SynthesiserVoice I have overidden the startNote(), stopNote(), and renderNextBlock() - functions to cope with my special case.\n\n startNote():\n\nThe Problem:\n\n My Problem now is, that when I press the same Key two times very fast I get a clicking between the two sounds when applying the envelope. I think that the problem is, that the sound playing is not finished and gets killed by the next one but without a fade applied at the end, because the release phase from the envelope hasn’t finished yet. So I think I would need to apply a small fade out to the first sound. But I really don’t now how to do this and I havn’t found any resources online which tackle this. So hopefully somebody here on the forum could help me out, I allready tried a lot but nothing worked."
    },
    {
        "link": "https://stackoverflow.com/questions/6343450/generating-sound-on-the-fly-with-javascript-html5",
        "document": "You can use the Web Audio API in most browsers now (excepting IE and Opera Mini).\n\nTry out this code:\n\nIf you want the volume lower, you can do something like this:\n\nI got most of this from experimenting in chromium while reading the Web Audio API Working Draft, which I found from @brainjam 's link.\n\nLastly, it is very helpful to inspect the various objects in the chrome inspector (ctrl-shift-i)."
    },
    {
        "link": "https://stackoverflow.com/questions/34708980/generate-sine-wave-and-play-it-in-the-browser",
        "document": "I need a sample code that could:\n• None generate sine wave (an array of samples) and then\n\nAll done in browser using some HTML5 API in JavaScript."
    },
    {
        "link": "https://teropa.info/blog/2016/08/04/sine-waves",
        "document": "First Things First: What Is A Digital Audio Signal?\n\nWhen you represent a sound wave in digital form, what you have is essentially a big array of numbers. Each number in the array represents the position of the sound wave at a particular point in time, or in other words, its amplitude at that point.\n\nWhen you visualize the numbers from one of these arrays, you get a waveform, such as the ones you see on Soundcloud. By looking at it you get an idea of the sound wave's amplitude over time.\n\nThese arrays are usually very large, since you need a fine-grained sampling of a sound wave in order to capture it accurately. A commonly used sampling frequency for digital audio is 44.1 kHz, which means that a sample is taken 44,100 times per second. A minute of sound takes up an array of 2,646,000 numbers!\n\nWhat this means is that the kinds of waveforms you see on Soundcloud are really very rough approximations of the original sound waves. The 9-minute Death Hawks song above would have about 23 million samples in it. (Or actually 46 million since it's in stereo and you need two signals: left and right.) So what's actually visualized in a waveform like that are averages of the amplitude over time.\n\nBut if you open an audio file in an audio editor like Audacity, and zoom all the way in, you can see the individual samples in there - thousands of them per each second of audio. These sample numbers make up a digital audio signal.\n\nAnd How Do I Work With Such Signals In JavaScript?\n\nThe Web Audio API, like any system that works with digital audio, basically shuffles around little audio buffers stored in numeric arrays. In many cases we don't directly need to process those arrays in JavaScript, but when we do, we use the most efficient API available for working with raw binary data in JavaScript: Typed Arrays. Let's see how that might happen.\n\nThe first thing we always do when we want to do anything audio related in the browser is establish an AudioContext. This is our gateway to all things Web Audio:\n\nSay we wanted to generate a two-second sound clip. To store that clip, the first thing we need to do is create an AudioBuffer:\n\nThe three numbers used here are:\n• - the number of audio channels we want. For mono sound, we only need 1. For stereo we'd have 2.\n• - the number of samples the buffer should hold. We want two seconds worth of audio and we have a sample frequency of 44,1 kHz. .\n• - the sample rate of the buffer - 44.1kHz\n\nWhen we have an , we can ask it to give us the raw audio data array for a given channel:\n\nThe means we're getting the first channel, which in this case is also the only channel. If we were working with stereo sound, we would have to get a separate array for the second channel.\n\nThe variable now holds a typed array, which has been created by the Web Audio API. It's a Float32Array with length , to be exact. We can put our 88,200 numbers in it, and the audio signal formed by them is what will be played when the AudioBuffer is played back. We can, for example, go and generate those samples in a loop:\n\nBut how exactly can we generate audio samples that actually produce something audible, let alone recognizable or even musical? Today we'll discuss one basic way to do that, which is by producing a sine wave.\n\nA sine wave (often also called a sinusoid) is one of the most basic building blocks of audio signals. They're easy to generate, transform, and combine, which makes them a very useful primitive for all kinds of purposes.\n\nTo make a sine wave, we first need to talk about angles and the sine function. And to approach that, we need circles. That's because the sine function has an intimate relationship with the so-called unit circle. Here's how the two are connected:\n• Draw a circle with a radius of , whose center is at the origin .\n• Draw a line from the circle's centerpoint to anywhere on the circle's circumference.\n• An angle forms between that line and the positive x-axis. Depending on where you drew the line, the angle could be anything between to degrees, or to radians. (Radians will be the unit of measure more useful to us).\n• To find the sine of that angle, find the y coordinate of the point where the line meets the circle's circumference. The sine's value could thus be anything between and .\n\nSee how the sine forms from different angles:\n\nSo, the way sine works is that as the angle changes, it oscillates between and . It's positive half of the time and negative the other half, depending on whether the endpoint of our line is currently above or below the x axis.\n\nNow, imagine a process that grows the angle constantly over time, gradually revolving it round and round the circle. If we follow that process and record all the sines that we get along the way, we get a sine wave:\n\nWhat we have here is an oscillator: A process that produces a repeating signal. In this case the signal is a sinusoid.\n\nHow Can I Make A Sine Wave Signal with Web Audio?\n\nIn order to represent and play a sine wave in Web Audio, we need to turn it into one of those numeric arrays we talked about, essentially capturing samples of the continuous wave over time.\n\nThere are basically two ways we could do this: There's a hard way, which is to calculate the values manually. And there's an easy way, which is to let the Web Audio API do it for us. The benefit of doing it the hard way is that then we learn much more about how this \"samping\" actually works.\n\nOK, so How Do I Make One the Hard Way?\n\nLet's talk about the frequency of the sine wave. What we mean by that is \"how many times per second does the circle go all the way around\"? Or alternatively, \"how many times per second does the resulting wave go all the way up, then all the way down, and back again\"?\n\nIn the animation above this happens once per second, so the frequency of that wave is . For an actual sound wave, this is way, way too slow. For audible signals we need to start talking about waves that are much faster than that: s, s, or s of Hz.\n\nA good frequency for us to use today is 440Hz. This is the so-called A4 note, commonly used as a standard note for tuning musical instruments. We can produce this note with a sine oscillator of 440Hz, meaning one that goes around full circle 440 times per second. 440Hz is its so-called real-time frequency.\n\nAnother useful way to think about how fast this thing oscillates is its angular frequency, meaning how many radians per second does it go?. Since we're doing 440 circles per second and there are radians in a circle, the radial velocity is . Here's how we can express these frequencies in code:\n\nWe want to use this oscillator to fill up the array we initialized earlier - the one for the two-second audio buffer of samples. That is, we need to write the function that gets called from this loop:\n\nThat function is going to be called 88200 times. The first thing we need to do there is convert the sample number into the sample time in seconds. This measures the time between the beginning of the buffer and this sample. In this case it will be a number between and , since our sample is 2 seconds long. We can do get the sample time by dividing the sample number by our sample frequency, 44,100:\n\nThen we need to figure out what the angle of our oscillator is at this point in time. Here we can plug in the angular frequency that we calculated earlier. The sample angle will be the angular frequency multiplied by the sample time. (Just like the distance travelled by a moving car is the car's velocity multiplied by elapsed time.)\n\nThe value of the sine oscillator is then simply the sine of this angle. We can just return it, which means it'll end up in the audio buffer:\n\nWith this we've calculated two seconds worth of samples from a sine wave that oscillates at 440Hz. And now that we've got those samples in a buffer, we can play it back. We can do so using a Web Audio AudioBufferSourceNode, which is an audio node that knows how to play back an object.\n\nHere's the complete source code that plays back two seconds worth of a sine wave.\n\nThe four steps taken there to play the sound are:\n• Ask the to create a new .\n• Assign our , which now contains the sine wave data, as the buffer the source node should play.\n• Route the audio signal from the source to the built-in of the audio context. That's going to send it to the sound card and makes it audible.\n\nThe result of this is that the 88,200 numbers in the are streamed into the AudioDestinationNode of the , 44,100 samples per second, over two seconds. This will cause the signal to be sent them over to your sound card's digital-to-analog converter, and from there as an analog audio signal to your speakers, and then as air vibrations to your eardrums. We've essentially written a script that transfers a sine wave from your web browser to your brain. Not too shabby!\n\nAnd How About The Easy Way?\n\nNow we now how we can generate a two-second clip of a sine wave using JavaScript. But that's a very cumbersome and resource-intensive way to do it, for at least a couple of reasons:\n• We're doing all that calculation in JavaScript code. No matter how fast the JS engine might be, it's easy to see why this can become a performance problem.\n• We're very constrained with relation to the kinds of things we can do. What if we need a continuous sine wave that could run for hours on end? Or what if we need hundreds sine waves of different frequencies? Our current code is not going to cut it.\n\nLuckily the Web Audio API has an abstraction that can do this for us: The OscillatorNode can emit a continuous audio signal that contains a sine wave. This is pretty much always preferable to the manual approach, for the following reasons:\n• is much, much more efficient. It does its work in native browser code (C++ or Assembly) rather than in JavaScript.\n• works in the background, in the browser's audio thread. It doesn't take up time from your JavaScript thread.\n• You can easily make a continuous wave that can run forever, instead of just filling up a buffer of a predefined duration.\n• You don't have to do the math yourself. You just give a real-time frequency in and it will oscillate on that frequency.\n\nHere's how we can use an to make the same sine wave as before:\n\nNow we don't have all that code for generating individual samples. But it's good to understand that it is still happening behind the scenes. There's some native browser code running in the audio thread, which is generating sine wave samples and putting them into tiny buffers of 128 samples each. Those buffers are then being streamed to the sound card. And we never need to see any of that in our JavaScript code.\n\nSo that's how you make a sine wave. In the next article we'll talk about dynamically changing the wave frequency and how that affects the pitch of the sound."
    },
    {
        "link": "https://teropa.info/blog/2016/08/04/sine-waves.html",
        "document": "First Things First: What Is A Digital Audio Signal?\n\nWhen you represent a sound wave in digital form, what you have is essentially a big array of numbers. Each number in the array represents the position of the sound wave at a particular point in time, or in other words, its amplitude at that point.\n\nWhen you visualize the numbers from one of these arrays, you get a waveform, such as the ones you see on Soundcloud. By looking at it you get an idea of the sound wave's amplitude over time.\n\nThese arrays are usually very large, since you need a fine-grained sampling of a sound wave in order to capture it accurately. A commonly used sampling frequency for digital audio is 44.1 kHz, which means that a sample is taken 44,100 times per second. A minute of sound takes up an array of 2,646,000 numbers!\n\nWhat this means is that the kinds of waveforms you see on Soundcloud are really very rough approximations of the original sound waves. The 9-minute Death Hawks song above would have about 23 million samples in it. (Or actually 46 million since it's in stereo and you need two signals: left and right.) So what's actually visualized in a waveform like that are averages of the amplitude over time.\n\nBut if you open an audio file in an audio editor like Audacity, and zoom all the way in, you can see the individual samples in there - thousands of them per each second of audio. These sample numbers make up a digital audio signal.\n\nAnd How Do I Work With Such Signals In JavaScript?\n\nThe Web Audio API, like any system that works with digital audio, basically shuffles around little audio buffers stored in numeric arrays. In many cases we don't directly need to process those arrays in JavaScript, but when we do, we use the most efficient API available for working with raw binary data in JavaScript: Typed Arrays. Let's see how that might happen.\n\nThe first thing we always do when we want to do anything audio related in the browser is establish an AudioContext. This is our gateway to all things Web Audio:\n\nSay we wanted to generate a two-second sound clip. To store that clip, the first thing we need to do is create an AudioBuffer:\n\nThe three numbers used here are:\n• - the number of audio channels we want. For mono sound, we only need 1. For stereo we'd have 2.\n• - the number of samples the buffer should hold. We want two seconds worth of audio and we have a sample frequency of 44,1 kHz. .\n• - the sample rate of the buffer - 44.1kHz\n\nWhen we have an , we can ask it to give us the raw audio data array for a given channel:\n\nThe means we're getting the first channel, which in this case is also the only channel. If we were working with stereo sound, we would have to get a separate array for the second channel.\n\nThe variable now holds a typed array, which has been created by the Web Audio API. It's a Float32Array with length , to be exact. We can put our 88,200 numbers in it, and the audio signal formed by them is what will be played when the AudioBuffer is played back. We can, for example, go and generate those samples in a loop:\n\nBut how exactly can we generate audio samples that actually produce something audible, let alone recognizable or even musical? Today we'll discuss one basic way to do that, which is by producing a sine wave.\n\nA sine wave (often also called a sinusoid) is one of the most basic building blocks of audio signals. They're easy to generate, transform, and combine, which makes them a very useful primitive for all kinds of purposes.\n\nTo make a sine wave, we first need to talk about angles and the sine function. And to approach that, we need circles. That's because the sine function has an intimate relationship with the so-called unit circle. Here's how the two are connected:\n• Draw a circle with a radius of , whose center is at the origin .\n• Draw a line from the circle's centerpoint to anywhere on the circle's circumference.\n• An angle forms between that line and the positive x-axis. Depending on where you drew the line, the angle could be anything between to degrees, or to radians. (Radians will be the unit of measure more useful to us).\n• To find the sine of that angle, find the y coordinate of the point where the line meets the circle's circumference. The sine's value could thus be anything between and .\n\nSee how the sine forms from different angles:\n\nSo, the way sine works is that as the angle changes, it oscillates between and . It's positive half of the time and negative the other half, depending on whether the endpoint of our line is currently above or below the x axis.\n\nNow, imagine a process that grows the angle constantly over time, gradually revolving it round and round the circle. If we follow that process and record all the sines that we get along the way, we get a sine wave:\n\nWhat we have here is an oscillator: A process that produces a repeating signal. In this case the signal is a sinusoid.\n\nHow Can I Make A Sine Wave Signal with Web Audio?\n\nIn order to represent and play a sine wave in Web Audio, we need to turn it into one of those numeric arrays we talked about, essentially capturing samples of the continuous wave over time.\n\nThere are basically two ways we could do this: There's a hard way, which is to calculate the values manually. And there's an easy way, which is to let the Web Audio API do it for us. The benefit of doing it the hard way is that then we learn much more about how this \"samping\" actually works.\n\nOK, so How Do I Make One the Hard Way?\n\nLet's talk about the frequency of the sine wave. What we mean by that is \"how many times per second does the circle go all the way around\"? Or alternatively, \"how many times per second does the resulting wave go all the way up, then all the way down, and back again\"?\n\nIn the animation above this happens once per second, so the frequency of that wave is . For an actual sound wave, this is way, way too slow. For audible signals we need to start talking about waves that are much faster than that: s, s, or s of Hz.\n\nA good frequency for us to use today is 440Hz. This is the so-called A4 note, commonly used as a standard note for tuning musical instruments. We can produce this note with a sine oscillator of 440Hz, meaning one that goes around full circle 440 times per second. 440Hz is its so-called real-time frequency.\n\nAnother useful way to think about how fast this thing oscillates is its angular frequency, meaning how many radians per second does it go?. Since we're doing 440 circles per second and there are radians in a circle, the radial velocity is . Here's how we can express these frequencies in code:\n\nWe want to use this oscillator to fill up the array we initialized earlier - the one for the two-second audio buffer of samples. That is, we need to write the function that gets called from this loop:\n\nThat function is going to be called 88200 times. The first thing we need to do there is convert the sample number into the sample time in seconds. This measures the time between the beginning of the buffer and this sample. In this case it will be a number between and , since our sample is 2 seconds long. We can do get the sample time by dividing the sample number by our sample frequency, 44,100:\n\nThen we need to figure out what the angle of our oscillator is at this point in time. Here we can plug in the angular frequency that we calculated earlier. The sample angle will be the angular frequency multiplied by the sample time. (Just like the distance travelled by a moving car is the car's velocity multiplied by elapsed time.)\n\nThe value of the sine oscillator is then simply the sine of this angle. We can just return it, which means it'll end up in the audio buffer:\n\nWith this we've calculated two seconds worth of samples from a sine wave that oscillates at 440Hz. And now that we've got those samples in a buffer, we can play it back. We can do so using a Web Audio AudioBufferSourceNode, which is an audio node that knows how to play back an object.\n\nHere's the complete source code that plays back two seconds worth of a sine wave.\n\nThe four steps taken there to play the sound are:\n• Ask the to create a new .\n• Assign our , which now contains the sine wave data, as the buffer the source node should play.\n• Route the audio signal from the source to the built-in of the audio context. That's going to send it to the sound card and makes it audible.\n\nThe result of this is that the 88,200 numbers in the are streamed into the AudioDestinationNode of the , 44,100 samples per second, over two seconds. This will cause the signal to be sent them over to your sound card's digital-to-analog converter, and from there as an analog audio signal to your speakers, and then as air vibrations to your eardrums. We've essentially written a script that transfers a sine wave from your web browser to your brain. Not too shabby!\n\nAnd How About The Easy Way?\n\nNow we now how we can generate a two-second clip of a sine wave using JavaScript. But that's a very cumbersome and resource-intensive way to do it, for at least a couple of reasons:\n• We're doing all that calculation in JavaScript code. No matter how fast the JS engine might be, it's easy to see why this can become a performance problem.\n• We're very constrained with relation to the kinds of things we can do. What if we need a continuous sine wave that could run for hours on end? Or what if we need hundreds sine waves of different frequencies? Our current code is not going to cut it.\n\nLuckily the Web Audio API has an abstraction that can do this for us: The OscillatorNode can emit a continuous audio signal that contains a sine wave. This is pretty much always preferable to the manual approach, for the following reasons:\n• is much, much more efficient. It does its work in native browser code (C++ or Assembly) rather than in JavaScript.\n• works in the background, in the browser's audio thread. It doesn't take up time from your JavaScript thread.\n• You can easily make a continuous wave that can run forever, instead of just filling up a buffer of a predefined duration.\n• You don't have to do the math yourself. You just give a real-time frequency in and it will oscillate on that frequency.\n\nHere's how we can use an to make the same sine wave as before:\n\nNow we don't have all that code for generating individual samples. But it's good to understand that it is still happening behind the scenes. There's some native browser code running in the audio thread, which is generating sine wave samples and putting them into tiny buffers of 128 samples each. Those buffers are then being streamed to the sound card. And we never need to see any of that in our JavaScript code.\n\nSo that's how you make a sine wave. In the next article we'll talk about dynamically changing the wave frequency and how that affects the pitch of the sound."
    },
    {
        "link": "https://marcgg.com/blog/2016/11/01/javascript-audio",
        "document": "During a recent hackathon, I decided to build a multiplayer 8 bits sequencer using sounds generated programatically with the Web Audio API. I didn’t want to only use the HTML 5 tag because I found it too limiting… but the first thing I discovered is that getting the right kind of sound is not straightforward at all, especially if you only have a very basic musical background like myself. So how exactly do you create a clear, ringing and nice sounding note?\n\nIn this article I’ll give some pointers with usable code. I’ve also added examples that you can actually run if your browser supports it.\n\nFirst let’s create a very basic beep using a sinusoid. We’ll initiate an audio context, which is the central object for generating sound. Then we’ll create an oscillator producing the sine wave. Finally we connect the oscillator to the context and start.\n\nYou’ll notice that the sound produced here is not great. It seems like you let a phone off the hook and when you stop it, you hear a “click” and it’s not pleasant at all. This is because the human hear reacts this way as explained in this great article. Basically when you stop the sound anywhere else than the zero crossing point, you’ll hear this clicking sound.\n\nGetting Rid Of The Clicking Sound\n\nThe best solution to get rid of this click is to ramp the sine wave down with an exponentional function, using as documented here.\n\nThis time we need to add a gain node to our oscillator. A gain node allows us to change the volume of a signal as explained in this schema from the documentation:\n\nThe code to start the sound now looks like this:\n\nIn order to stop the sound we change the gain value, effectively reducing the volume. Note that we don’t ramp down to 0 since there is a limitation in this function where the value has to be positive.\n\nAs you can hear, the clicking sound is gone! But that’s not the only interesting thing that you can do with this exponential ramp down.\n\nIn the example above, we decided to stop the sound really quickly, in seconds. But what happens when we change this value?\n\nGiving more time to the sound to fade out gives it a totally different feel. It gets more visible when we start and stop the signal right away:\n\nThe first one sounds like a ticking noise when the other sounds like an actual note played on an instrument.\n\nSo far we’ve been using a sine wave for our main signal, but we have other options:\n\nIt’s enven more interesting when we start playing around with the type of oscilators by setting .\n\nWith the previous code, it becomes fairly simple to have a nice sounding note, but what exactly were we playing? That’s when you have to take frequency into account. For instance, the one people know is that A4 is 440Hz, but there are others.\n\nWith this table, you can easily create a mapping in your code to play any given note using its . For the hackathon I used a simple hash mapping that is available in this gist.\n\nTo implement this, we just need to add a frequency to our oscilator:\n\nIf we change with the value of , we can play any note. For instance:\n\nMix this with the ramp down timings and different signals, and you start to be able to create more interesting sounds."
    }
]