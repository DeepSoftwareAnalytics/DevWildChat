[
    {
        "link": "https://blog.csdn.net/qq_39731597/article/details/125761376",
        "document": ""
    },
    {
        "link": "https://zhuanlan.zhihu.com/p/354941777",
        "document": ""
    },
    {
        "link": "https://zhuanlan.zhihu.com/p/542393490",
        "document": ""
    },
    {
        "link": "https://blog.csdn.net/modi000/article/details/127319254",
        "document": ""
    },
    {
        "link": "https://docs.pingcode.com/baike/1186140",
        "document": ""
    },
    {
        "link": "https://embeddedrelated.com/showarticle/1182.php",
        "document": "Last time, we talked about error correction and detection, covering some basics like Hamming distance, CRCs, and Hamming codes. If you are new to this topic, I would strongly suggest going back to read that article before this one.\n\nThis time we are going to cover Reed-Solomon codes. (I had meant to cover this topic in Part XV, but the article was getting to be too long, so I’ve split it roughly in half.) These are one of the workhorses of error-correction, and they are used in overwhelming abundance to correct short bursts of errors. We’ll see that encoding a message using Reed-Solomon is fairly simple, so easy that we can even do it in C on an embedded system.\n\nWe’re also going to talk about some performance metrics of error-correcting codes.\n\nReed-Solomon codes, like many other error-correcting codes, are parameterized. A Reed-Solomon \\( (n,k) \\) code, where \\( n=2^m-1 \\) and \\( k=n-2t \\), can correct up to \\( t \\) errors. They are not binary codes; each symbol must have \\( 2^m \\) possible values, and is typically an element of \\( GF(2^m) \\).\n\nThe Reed-Solomon codes with \\( n=255 \\) are very common, since then each symbol is an element of \\( GF(2^8) \\) and can be represented as a single byte. A \\( (255, 247) \\) RS code would be able to correct 4 errors, and a \\( (255,223) \\) RS code would be able to correct 16 errors, with each error being an arbitrary error in one transmitted symbol.\n\nYou may have noticed a pattern in some of the coding techniques discussed in the previous article:\n• If we compute the parity of a string of bits, and concatenate a parity bit, the resulting message has a parity of zero modulo 2\n• If we compute the checksum of a string of bytes, and concatenate a negated checksum, the resulting message has a checksum of zero modulo 256\n• If we compute the CRC of a message (with no initial/final bit flips), and concatenate the CRC, the resulting message has a remainder of zero modulo the CRC polynomial\n• If we compute parity bits of a Hamming code, and concatenate them to the data bits, the resulting message has a remainder of zero modulo the Hamming code polynomial\n\nIn all cases, we have this magic technique of taking data bits, using a division algorithm to compute the remainder, and concatenating the remainder, such that the resulting message has zero remainder. If we receive a message which does have a nonzero remainder, we can detect this, and in some cases use the nonzero remainder to determine where the errors occurred and correct them.\n\nParity and checksum are poor methods because they are unable to detect transpositions. It’s easy to cancel out a parity flip; it’s much harder to cancel out a CRC error by accidentally flipping another bit elsewhere.\n\nWhat if there was an encoding algorithm that was somewhat similar to a checksum, in that it operated on a byte-by-byte basis, but had robustness to swapped bytes? Reed-Solomon is such an algorithm. And it amounts to doing the same thing as all the other linear coding techniques we’ve talked about to encode a message:\n• create a codeword by concatenating the data bits and the remainder bits\n• a valid codeword now has a zero remainder\n\nIn this article, I am going to describe Reed-Solomon encoding, but not the typical decoding methods, which are somewhat complicated, and include fancy-sounding steps like the Berlekamp-Massey algorithm (yes, this is the same one we saw in part VI, only it’s done in \\( GF(2^m) \\) instead of the binary \\( GF(2) \\) version), the Chien search, and the Forney algorithm. If you want to learn more, I would recommend reading the BBC white paper WHP031, in the list of references, which does go into just the right level of detail, and has lots of examples.\n\nIn addition to being more complicated to understand, decoding is also more computationally intensive. Reed-Solomon encoders are straightforward to implement in hardware — which we’ll see a bit later — and are simple to implement in software in an embedded system. Decoders are trickier, and you’d probably want something more powerful than an 8-bit microcontroller to execute them, or they’ll be very slow. The first use of Reed-Solomon in space communications was on the Voyager missions, where an encoder was included without certainty that the received signals could be decoded back on earth. From a NASA Tutorial on Reed-Solomon Error Correction Coding by William A. Geisel:\n\nThe statement “accurate decoding methods were not even available” in 1977 is a bit suspicious, although this general idea of faith-that-a-decoder-could-be-constructed is in several other accounts:\n\nGeisel probably meant “efficient” rather than “accurate”; decoding algorithms were well-known before a NASA report in 1978 which states\n\nAt any rate, with Reed-Solomon, we have a polynomial, but instead of the coefficients being or like our usual polynomials in \\( GF(2)[x] \\), this time the coefficients are elements of \\( GF(2^m) \\). A byte-oriented Reed-Solomon encoding uses \\( GF(2^8) \\). The polynomial used in Reed-Solomon is usually written as \\( g(x) \\), the generator polynomial, with\n\nwhere \\( \\lambda \\) is a primitive element of \\( GF(2^m) \\) and \\( b \\) is some constant, usually 0 or 1 or \\( -t \\). This means the polynomial’s roots are successive powers of \\( \\lambda \\), and that makes the math work out just fine.\n\nAll we do is treat the message as a polynomial \\( M(x) \\), calculate a remainder \\( r(x) = x^{n-k} M(x) \\bmod g(x) \\), and then concatenate it to the message; the resulting codeword \\( C(x) = x^{n-k}M(x) + r(x) \\), is guaranteed to be a multiple of the generator polynomial \\( g(x) \\). At the receiving end, we calculate \\( C(x) \\bmod g(x) \\) and if it’s zero, everything’s great; otherwise we have to do some gory math, but this allows us to find and correct errors in up to \\( t \\) symbols.\n\nThat probably sounds rather abstract, so let’s work through an example.\n\nWe’re going to encode some messages using Reed-Solomon in two ways, first using the library, and then from scratch using . If you’re going to use a library, you’ll need to be really careful; the library should be universal, meaning you can encode or decode any particular flavor of Reed-Solomon — like the CRC, there are many possible variants, and you need to be able to specify the polynomial of the Galois field used for each symbol, along with the generator element \\( \\lambda \\) and the initial power \\( b \\). If anyone tells you they’re using the Reed-Solomon \\( (255,223) \\) code, they’re smoking something funny, because there’s more than one, and you need to know which particular variant in order to communicate properly.\n\nThere are two examples in the BBC white paper:\n• one using \\( RS(15,11) \\) for 4-bit symbols, with symbol field \\( GF(2)[y]/(y^4 + y + 1) \\); for the generator, \\( \\lambda = y = {\\tt 2} \\) and \\( b=0 \\), forming the generator polynomial\n• another using the DVB-T specification that uses \\( RS(255,239) \\) for 8-bit symbols, with symbol field \\( GF(2)[y]/(y^8 + y^4 + y^3 + y^2 + 1) \\) (hex representation ) and for the generator, \\( \\lambda = y = {\\tt 2} \\) and \\( b=0 \\), forming the generator polynomial\n\nIn , the class is used, with these parameters:\n• — this is the number of bits per symbol, 4 and 8 for our two examples\n• — this is the symbol field polynomial, 0x13 and 0x11d for our two examples\n• — this is the generator element in the symbol field, 0x2 for both examples\n• — this is the “first consecutive root” \\( b \\), with \\( b=0 \\) for both examples\n\nYou can verify the generator polynomial by encoding the message \\( M(x) = 1 \\), all zero symbols with a trailing one symbol at the end; this will create a codeword \\( C(x) = x^{n-k} + r(x) = g(x) \\).\n\nEncoding messages is just a matter of using either a string (byte array) or a list of message symbols, and we get a string back by default. (You can use but then you get a bunch of objects back rather than integer coefficients.)\n\nThe BBC white paper has a sample encoding only for the 4-bit \\( RS(15,11) \\) example, using \\( [1,2,3,4,5,6,7,8,9,10,11] \\) which produces remainder \\( [3,3,12,12] \\):\n\nThe real fun begins when we can start encoding actual messages, like Ernie, you have a banana in your ear! Since this has length 37, let’s use a shortened version of the code, \\( RS(53,37) \\).\n\nOh, we haven’t talked about shortened codes yet — this is where we just omit the initial symbols in a codeword, and both transmitter and receiver agree treat them as implied zeros. An eight-bit \\( RS(53,37) \\) transmitter encodes as codeword polynomials of degree 52, and the receiver just zero-extends them to degree 254 (with 202 implied zero bytes added at the beginning) for the purposes of decoding just like any other \\( RS(255,239) \\) code.\n\nThat binary stuff after the message is the remainder. Now we can decode all sorts of messages with errors:\n\nThis last message failed to decode because there were 9 errors and \\( RS(53,37) \\) or \\( RS(255,239) \\) can only correct 8 errors.\n\nBut if there are few enough errors, Reed-Solomon will fix them all.\n\nWhy does it work?\n\nThe reason why Reed-Solomon encoding and decoding works is a bit hard to grasp. There are quite a few good explanations of how encoding and decoding works, my favorite being the BBC whitepaper, but none of them really dwells upon why it works. We can handwave about redundancy and the mysteriousness of finite fields spreading that redundancy evenly throughout the remainder bits, or a spectral interpretation of the parity-check matrix where the roots of the generator polynomial can be considered as “frequencies”, but I don’t have a good explanation. Here’s the best I can do:\n\nLet’s say we have some number of errors where the locations are known. This is called an erasure, in contrast to a true error, where the location is not known ahead of time. Think of a bunch of symbols on a piece of paper, and some idiot erases one of them or spills vegetable soup, and you’ve lost one of the symbols, but you know where it is. The message received could be where the represents an erasure. Of course, binary computers don’t have a way to compute arithmetic on , so what we would do instead is just pick an arbitrary pattern like or to replace the erasure, and mark its position (in this case, byte 7) for later processing.\n\nAnyway, in a Reed-Solomon code, the error values are linearly independent. This is really the key here. We can have up to \\( 2t \\) erasures (as opposed to \\( t \\) true errors) and still maintain this linear independence property. Linear independence means that for a system of equations, there is only one solution. An error in bytes 3 and 28, for example, can’t be mistaken for an error in bytes 9 and 14. Once you exceed the maximum number of errors or erasures, the linear independence property falls apart and there are many possible ways to correct received errors, so we can’t be sure which is the right one.\n\nTo get a little more technical: we have a transmitted coded message \\( C_t(x) = x^{n-k}M(x) + r(x) \\) where \\( M(x) \\) is the unencoded message and \\( r(x) \\) is the remainder modulo the generator polynomial \\( g(x) \\), and a received message \\( C_r(x) = C_t(x) + e(x) \\) where the error polynomial \\( e(x) \\) represents the difference between transmitted and received messages. The receiver can compute \\( e(x) \\bmod g(x) \\) by calculating the remainder \\( C_r(x) \\bmod g(x) \\). If there are no errors, then \\( e(x) \\bmod g(x) = 0 \\). With erasures, we know the locations of the errors. Let’s say we had \\( n=255 \\), \\( 2t=16 \\), and we had three errors at known locations \\( i_1, i_2, \\) and \\( i_3, \\) in which case \\( e(x) = e_{i_1}x^{i_1} + e_{i_2}x^{i_2} + e_{i_3}x^{i_3} \\). These terms \\( e_{i_1}x^{i_1}, e_{i_2}x^{i_2}, \\) etc. are the error values that form a linear independent set. For example, if the original message had in byte 13, but we received instead, then the error coefficient in that location would be , so \\( i_1 = 13 \\) and the error value would be \\( {\\tt 1B}x^{13} \\).\n\nWhat the algebra of finite fields assures us, is that if we have a suitable choice of generator polynomial \\( g(x) \\) of degree \\( 2t \\) — and the one used for Reed-Solomon, \\( g(x) = \\prod\\limits_{i=0}^{2t-1}(x-\\lambda^{b+i}) \\), is suitable — then any selection of up to \\( 2t \\) distinct powers of \\( x \\) from \\( x^0 \\) to \\( x^{n-1} \\) are linearly independent. If \\( 2t = 16 \\), for example, then we cannot write \\( x^{15} = ax^{13} + bx^{8} + c \\) for some choices of \\( a,b,c \\) — otherwise it would mean that \\( x^{15}, x^{13}, x^8, \\) and \\( 1 \\) do not form a linearly independent set. This means that if we have up to \\( 2t \\) erasures at known locations, then we can figure out the coefficients at each location, and find the unknown values.\n\nIn our 3-erasures example above, if \\( 2t=16, i_1 = 13, i_2 = 8, i_3 = 0 \\), then we have a really easy case. All the errors are in positions below the degree of the remainder, which means all the errors are in the remainder, and the message symbols arrived intact. In this case, the error polynomial looks like \\( e(x) = e_{13}x^{13} + e_8x^8 + e_0 \\) for some error coefficients \\( e_{13}, e_8, e_0 \\) in \\( GF(256) \\). The received remainder \\( e(x) \\bmod g(x) = e(x) \\) and we can just find the error coefficients directly.\n\nOn the other hand, let’s say we knew the errors were in positions \\( i_1 = 200, i_2 = 180, i_3 = 105 \\). In this case we could calculate \\( x^{200} \\bmod g(x) \\), \\( x^{180} \\bmod g(x) \\), and \\( x^{105} \\bmod g(x) \\). These each have a bunch of coefficients from degree 0 to 15, but we know that the received remainder \\( e(x) \\bmod g(x) \\) must be a linear combination of the three polynomials, \\( e_{200}x^{200} + e_{180}x^{180} + e_{105}x^{105} \\bmod g(x) \\) and we could solve for \\( e_{200} \\), \\( e_{180} \\), and \\( e_{105} \\) using linear algebra.\n\nA similar situation applies if we have true errors where we don’t know their positions ahead of time. In this case we can correct only \\( t \\) errors, but the algebra works out so that we can write \\( 2t \\) equations with \\( 2t \\) unknowns (an error location and value for each of the \\( t \\) errors) and solve them.\n\nAll we need is for someone to prove this linear independence property. That part I won’t be able to explain intuitively. If you look at the typical decoder algorithms, they will show constructively how to find the unique solution, but not why it works. Reed-Solomon decoders make use of the so-called “key equation”; for Euclidean algorithm decoders (as opposed to Berlekamp-Massey decoders), the key equation is \\( \\Omega(x) = S(x)\\Lambda(x) \\bmod x^{2t} \\) and you’re supposed to know what that means and how to use it… sigh. A bit of blind faith sometimes helps.\n\nWhich is why I won’t have anything more to say on decoding. Read the BBC whitepaper.\n\nWe don’t have to use the library as a black box. We can do the encoding step in using . This is a helper class that the function uses. I talked a little bit about this in part XI, along with some of the details of this “two-variable” kind of mathematical thing, a polynomial ring over a finite field. Let’s forget about the abstract math for now, and just start computing. The just defines a particular arithmetic of polynomials, represented as lists of coefficients in ascending order (so \\( x^2 + 2 \\) is represented as ):\n\nNow, to encode a message, we just have to convert binary data to polynomials and then take the remainder \\( \\bmod g(x) \\):\n\nSee how easy that is?\n\nWell… you probably didn’t, because hides the implementation of the abstract algebra. We can make the encoding step even easier to understand if we take the perspective of an LFSR.\n\nWe can handle Reed-Solomon encoding in either software or hardware (and at least the remainder-calculating part of the decoder) using LFSRs. Now, we have to loosen up what we mean by an LFSR. So far, all the LFSRs have dealt directly with binary bits, and each shift cell either has a tap, or doesn’t, depending on the coefficients of the polynomial associated with the LFSR. For Reed-Solomon, the units of computation are elements of \\( GF(256) \\), represented as 8-bit bytes for a particular degree-8 primitive polynomial. So the shift cells, instead of containing bits, will contain bytes, and the taps will change from simple taps to multipliers over \\( GF(256) \\). (This gets a little hairy in hardware, but it’s not that bad.) A 10-byte LFSR over \\( GF(256) \\) implementing Reed-Solomon encoding would look something like this:\n\nWe would feed the bytes \\( b[k] \\) of the message in at the right; bytes \\( r_0 \\) through \\( r_9 \\) represent a remainder, and coefficients \\( p_0 \\) through \\( p_9 \\) represent the non-leading coefficients of the RS code’s generator polynomial. In this case, we would use a 10th-degree generator polynomial with a leading coefficient 1.\n\nFor each new byte, we do two things:\n• Shift all cells left, shifting in one byte of the message\n• Take the byte shifted out, and multiply it by the generator polynomial, then add (XOR) with the contents of the shift register.\n\nWe also have to remember to do this another \\( n-k \\) times (the length of the shift register) to cover the \\( x^{n-k} \\) factor. (Remember, we’re trying to calculate \\( r(x) = x^{n-k}M(x) \\).)\n\nThat worked, but we still have to handle multiplication in the symbol field, which is annoying and cumbersome for embedded processors. There are two table-driven methods that make this step easier, relegating the finite field stuff either to initialization steps only, or something that can be done at design time if you’re willing to generate a table in source code. (For example, using Python to generate the table coefficients in C.)\n\nAlso we have that call to , which is too high-level for a language like C, so let’s look at things from this low-level, table-driven approach.\n\nOne table method is to take the generator polynomial and multiply it by each of the 256 possible coefficients, so that we get a table of 256 × \\( n-k \\) bytes.\n\nAlternatively, if none of the generating polynomial’s coefficients are zero, we can just represent these coefficients as powers of a generating element of a field, and then store log and antilog tables of \\( x^n \\) in the symbol field. These are each a smaller table of length 256 (with one dummy entry) and they remove all dependencies on doing arithmetic in the symbol field.\n\nThere. It’s very simple; the hardest thing here is making sure you get the coefficients in the right order. You wouldn’t want to get those the wrong way around.\n\nI tested a C implementation using XC16 and the MPLAB X debugger:\n\nAgain, this is only the encoder, but remember the asymmetry here:\n\nAnd we can still do error correction!\n\nIs It Worth It?\n\nWait. We got all the way through Part XV and well into this article, you say, and he’s asking is it worth it?!\n\nYes, that’s the question. We got a benefit, namely the ability to detect and sometimes even correct errors, but it has a cost. We have to send extra information, and that uses up part of the available communications bandwidth. And we had to add some complexity. Is it a net gain?\n\nThere are at least two ways of looking at this cost-benefit tradeoff, and it depends on what the constraints are.\n\nThe first way of analyzing cost-benefits is to consider a situation where we’re given a transmitter/receiver pair, and all we have control over is the digital part of the system that handles the bits that are sent. Error correcting codes like Reed-Solomon give us the ability to lower the error rate substantially — and we’ll calculate this in just a minute — with only a slight decrease in transmission rate. A \\( (255, 239) \\) Reed-Solomon code lets us correct up to 8 errors in each block of 255 bytes, with 93.7% of the transmission capacity of the uncoded channel. The encoding complexity cost is minimal, and the decoding cost is small enough to merit use of Reed-Solomon decoders in higher-end embedded systems or in desktop computers.\n\nThe second way of analyzing cost-benefits is to consider a situation where we have control over the transmitter energy, as well as the digital encoding/decoding mechanisms. We can lower the error rate by using an error-correcting code, or we can increase the transmitter energy to improve the signal-to-noise ratio. The Shannon-Hartley theorem, which says channel capacity \\( C \\leq B \\log_2 (1+\\frac{S}{N}) \\), can help us quantitatively compare those possibilities. The standard metric is \\( E_b/N_0 \\) in decibels, so let’s try to frame things in terms of it. First we have to understand what it is.\n\nOne common assumption in communications is an additive Gaussian white noise (AWGN) channel, which has some amount of noise \\( N_0 \\) per unit bandwidth. This is why oscilloscopes let you limit the bandwidth; if you don’t need it, it reduces the noise level. In any case, if we have an integrate-and-dump receiver, where we integrate our received signal, and every \\( T \\) seconds we sample the integrator value, divide by \\( T \\) to compute an average value over the interval, and reset the integrator, then the received noise can be modeled as a Gaussian random variable with zero mean and variance of \\( \\sigma_N^2 = \\frac{N_0}{2T} \\). (This can be computed from \\( \\sigma_N^2 = \\frac{N_0}{2}\\int_{-\\infty}^{\\infty}\\left|h(t)\\right|^2\\, dt \\) with \\( h(t) \\) of the integrate-and-dump filter being a unit rectangular pulse of duration \\( T \\) and height \\( 1/T \\).)\n\nThe signal waveform has some energy \\( E_b \\) per bit. Consider a raw binary pulse signal \\( x(t) = \\pm A \\) for a duration of \\( T \\) seconds for each bit, with \\( +A \\) representing a binary and \\( -A \\) representing a binary . If we compute \\( E_b = \\int_{-\\infty}^{\\infty}\\left|x(t)\\right|^2\\, dt \\) we get \\( E_b = A^2T \\). The integrate-and-dump receiver outputs a if its output sample is positive, and if its output sample is negative. We get a bit flip if the received noise is less than \\( -A \\) for a transmitted bit, or greater than \\( A \\) for a transmitted bit. This probability is just the Q-function evaluated at \\( Q(A/\\sigma_n) = Q(\\sqrt{A^2/\\sigma_n{}^2}) = Q(\\sqrt{(E_b/T)/(N_0/2T)}) = Q(\\sqrt{2E_b/N_0}). \\)\n\nIn other words, if we know how bits are represented as signals, we can determine the probability of a bit error just by knowing what \\( E_b/N_0 \\) is.\n\nBit Error Rate Calculations Are the Bane of My Existence\n\nJust one quick aside: this section showing bit error rate graphs was a beastly thing to manage. Don’t worry about the Python code, or how the calculations work, unless you want to enter this quagmire. Otherwise, just look at the pretty pictures, and my descriptions of how to interpret them.\n\nAs I was saying, let’s graph it!\n\nIf we have a nice healthy noise margin of 12dB for \\( E_b/N_0 \\), then the bit error rate is \\( 9.0\\times 10^{-9} \\), or one error in 110 million. Again, this is just raw binary pulses using an integrate-and-dump receiver. More complex signal shaping techniques like quadrature amplitude modulation can make better use of available spectrum. But that’s an article for someone else to write.\n\nWith an \\( E_b/N_0 \\) ratio of 1 (that’s 0dB), probability of a bit error is around 0.079, or about one in 12. Lousy. You want a lower error rate? Put more energy into the signal, or make sure there’s less energy in noise.\n\nOkay, now suppose we use a Hamming (7,4) code. Let’s say, for example, that the raw bit error rate is 0.01, which occurs at about \\( E_b/N_0 \\approx 4.3 \\)dB. With the Hamming (7,4) code, we can correct one error, so there are a few cases to consider for an upper bound that is conservative and assumes some worst-case conditions:\n• There were no received bit errors. Probability of this is \\( (1-0.01)^7 \\approx 0.932065. \\) The receiver will output 4 correct output bits, and there are no errors after decoding\n• There was one received bit error, in one of the seven codeword bits. Probability of this is \\( 7\\cdot 0.01\\cdot(1-0.01)^6 \\approx 0.065904. \\) The receiver will output 4 correct output bits, and there are no errors after decoding.\n• There are two or more received bit errors. Probability of this is \\( \\approx 0.002031 \\). We don’t know how many correct output bits the receiver will produce; a true analysis is complicated. Let’s just assume, as a worst-case estimate, we’ll receive no correct output bits, so there are 4 errors after decoding.\n\nIn this case the expected number of errors (worst-case) in the output bits is 4 × 0.002031, or 0.002031 per output bit.\n\nWe went from a bit error rate of 0.01 down to about 0.002031, which is good, right? The only drawback is that to send data 4 bits we had to transmit 7 bits, so the transmitted energy per bit is a factor of 7/4 higher.\n\nWe can find the expected number of errors more precisely if we look at the received error count for each combination of the 128 possible error patterns, and compute the “weighted count” W as the sum of the received error counts, weighted by the number of patterns that cause that combination of raw and received error count:\n\nThis then lets us calculate the net error rate as \\( (W_0(1-p)^7 + W_1 p(1-p)^6 + W_2 p^2(1-p)^5 + \\ldots + W_6p^6(1-p) + W_7p^7) / 4 \\):\n\nNow let’s look at that bit error rate graph again, this time with both the curve for uncoded data, and with another curve representing the use of a Hamming (7,4) code.\n\nBoth the exact calculation and the upper bound are fairly close.\n\nWe can do something similar with Reed-Solomon error rates. For an \\( (n,k) = (255,255-2t) \\) Reed-Solomon code, with transmitted bit error probability \\( p \\), the error probability per byte is \\( q = 1 - (1-p)^8 \\), and there are a couple of situations, given the number of byte errors \\( v \\) per 255-byte block:\n• \\( v \\le t \\): Reed-Solomon will decode them all correctly, and the number of decoded byte errors is zero.\n• \\( v > t \\): More than \\( t \\) byte errors: This is the tricky part.\n• Best-case estimate: we are lucky and the decoder actually corrects \\( t \\) errors, leading to \\( v-t \\) errors. This is ludicrously optimistic, but it does represent a lower bound on the bit error rate.\n• Worst-case estimate: we are unlucky and the decoder alters \\( t \\) values which were correct, leading to \\( v+t \\) errors. This is ludicrously pessimistic, but it does represent an upper bound on the bit error rate.\n• Empirical estimate: we can actually generate some random samples covering various numbers of errors, and see what a decoder actually does, then use the results to extrapolate a data bit error rate. This will be closer to the expected results, but it can take quite a bit of computation to generate enough random samples and run the decoder algorithm.\n\nOK, now we’re going to tie all this together and show four graphs:\n• Bit error rate of the decoded data (after error correction) vs. raw bit error rate during transmission. This tells us how much the error rate decreases.\n• Another will show the same data in closer detail.\n• Bit error rate of the decoded data vs. \\( E_b/N_0 \\). This lets us compare bit error rates by how much transmit energy they require to achieve that error rate.\n• Effective gain in \\( E_b/N_0 \\), in decibels, vs. bit error rate of the decoded data.\n\nOK, so what are we looking at here?\n\nThe top two graphs show us a few things:\n• At low channel bit error rates (= “raw bit error rate”), the effective data bit error rate can be made much lower. For example, at a \\( 10^{-4} \\) channel bit error rate, a Hamming (7,4) code can bring the effective data bit error rate down to about \\( 10^{-7} \\), and an RS(255,239) code can bring the effective data bit error rate down to about \\( 10^{-14} \\). Much lower, and we don’t have to sacrifice much bandwidth to do it; the code rate of RS(255,239) is 239/255 ≈ 0.9373.\n• At high channel bit error rates, on the other hand, the effective data bit error rate may not be much better, and may even be a little higher than the channel data bit error rate. It looks like on average this could be about a factor of 2 worse, in the cases of low-robustness Reed-Solomon codes like RS(255,253) and RS(255,251).\n• The meaning of “low” and “high” channel bit error rates depends on the coding technique used. Hamming codes are short, so you still see benefits from them at channel bit error rates as high as 0.2, whereas RS(255,251) doesn’t help decrease the error rate unless you have channel bit error rates below 0.0005, and the higher-robustness Reed-Solomon codes like RS(255,223) and RS(255,191) work well up to error rates of about 0.01.\n\nThe bottom two graphs show us how this reduction in effective bit error rate trades off against the \\( E_b/N_0 \\) penalty we get for using up part of our channel capacity for the redundancy of parity bits. (Remember, longer average transmission times per bit times mean that we need a larger \\( E_b \\) per encoded data bit.)\n• Hamming codes can gain between 0.5-1.5dB of \\( E_b/N_0 \\) compared to uncoded data, for the same effective data bit error rate.\n• Reed-Solomon codes can gain between 2-10dB of \\( E_b/N_0 \\) compare to uncoded data, for the same effective data bit error rate.\n• This gain works better at low channel bit error rates; the advantages in \\( E_b/N_0 \\) compared to uncoded data show up if the data bit error rate is somewhere below the 0.0001 to 0.01 range. Reed-Solomon codes beat Hamming codes once you get below about 0.005 data bit error rate, at least for the high-robustness codes; the Reed-Solomon codes which don’t add much redundancy, like RS(255,253), don’t beat Hamming codes unless the data bit error rate is below \\( 10^{-7} \\).\n• At high effective bit error rates, the coded messages can have worse performance in terms of \\( E_b/N_0 \\) for the same effective data bit error rate.\n• We generally want low effective bit error rates anyway; something in the \\( 10^{-6} \\) to \\( 10^{-12} \\) range is probably reasonable, depending on the application and how it is affected by errors. Those applications that have other higher-level layers which can handle retries in case of detected errors, can usually get away with bit error rates of \\( 10^{-6} \\) or even higher. Storage applications, on the other hand, need to keep errors low, because there’s no possible mechanism for retransmission; the only way to deal with it is to add another layer of error correction — which is something that is used in certain applications.\n\nWhy Not Use Reed-Solomon Instead of a CRC?\n\nSo here’s a question that might come to mind. Consider the following two pairs of cases:\n• RS(255,253) vs. a 253-byte packet followed by a 16-bit CRC\n• RS(255,251) vs. a 251-byte packet followed by a 32-bit CRC\n\nWithin each pair, there is the same overhead, and about the same encoding complexity and memory requirements if a lookup-table-based encoder is used. Decoders that check for the presence of transmission errors are essentially the same complexity as an encoder; it’s only the error-correction step that takes more CPU horsepower.\n\nA CRC can be used to detect errors, but not to correct them. A Reed-Solomon code can be used to detect errors, up to 1 byte error in case of RS(255,253) and two byte errors in case of RS(255,251).\n\nSo why shouldn’t we be using Reed-Solomon instead of a CRC?\n\nWell, CRCs are meant for detecting a small number of bit errors with guaranteed certainty. Reed-Solomon codes are designed to handle byte errors, and that means bursts of a byte or two, depending on the code parameters. So the nature of the received noise may make a big difference in terms of which is better. Noise models like AWGN, where the likelihood of any individual bit error is independent of any other, favor bit-error detection techniques; noise models in which multi-bit bursts are common will favor Reed-Solomon.\n\nBut the biggest impact is that by allowing error correction, we give up some of our ability to detect errors. I mentioned this toward the end of Part XV. Let’s look at it again.\n\nLet’s take a look at our fictitious encoding scheme again. Three codewords — , , and — and three symbols per word.\n\nThis encoding scheme has a Hamming distance of 3, which means we can detect up to 2 errors or correct up to 1 error.\n\nIf we don’t do any error correction, there are only 3 codewords out of 27 possible messages, so our ability to detect invalid messages is very good. The only way we’re going to misinterpret as or is if we happen to get the right kind of error to switch from one valid codeword to another, which needs three simultaneous errors.\n\nOn the other hand, if we do decide to use error correction, then all of a sudden we have 21 acceptable messages. By “acceptable” I mean a message that is either correct or correctable. If we get a message like that was originally , sorry, we’re going to turn it into , because that’s the closest codeword. The only unacceptable messages are , , , , , and , which have a minimum distance of 2 to any codeword.\n\nThere are a few limited codes — Hamming codes are one of them — which have the property that there are no unacceptable messages. These are called perfect codes, and it means that any received message is either correct or can be corrected to the closest codeword. Here we have to assume that errors are infrequent, and if we do get more errors than we can correct, then we are just going to make an erroneous correction. Oh well.\n\nI looked into this a little bit for Reed-Solomon codes. For \\( RS(255,255-2t) \\), it looks like \\( t=1 \\) is close to perfect, and \\( t=2 \\) isn’t too far away either, but as you start adding more redundancy symbols, the number of uncorrectable messages increases much faster than the number of correctable messages, and these boundaries around each codeword — which are called Hamming spheres, even though they’re more like N-dimensional hypercubes — cover less and less of the message space. Which is good, at least for error detection, because it means we’re more likely to be able to detect errors even if there are more than \\( t \\) of them.\n\nBelow is some data from the Monte Carlo simulations I used to compute bit error rates — this time presented a little differently, showing tables of what happens when there are more than \\( t \\) errors. Each random error pattern of \\( v \\) bit errors (and note that this may produce less than \\( v \\) byte errors, if more than one error occurs in the same byte) can have three outcomes:\n• : the decoding step fails, so we don’t change the received message, and the number of received errors stays the same\n• : the error pattern decodes incorrectly, to a different codeword\n\nThe table shows the number of samples in each case, and the fraction of those samples that fall into the , , and outcomes.\n\nThe interesting trends here are\n• For RS(255,253), with more than 1 error, the outcome is usually . With \\( v=2 \\) errors we occasionally get a successful correction. Compared to a 16-bit CRC, we risk making errors worse by trying to correct them, so unless we have a system with very low probability of 2 or more errors, we are better off using a 16-bit CRC and not trying to correct errors.\n• For RS(255,251), with more than 2 errors, the outcome is about a 50-50 split between and . With \\( v=3 \\) errors we occasionally get a successful correction. Compared to a 32-bit CRC, we risk making errors worse by trying to correct them. It’s not as bad as RS(255,253), but a 50% chance of making things worse if there are three or more errors, makes it more attractive to use a 32-bit CRC.\n• For RS(255,247), with more than 4 errors, the outcome is usually with a small probability of . With \\( v=4 \\) and \\( v=5 \\) errors we occasionally get a successful correction. Compared to a 64-bit CRC, now the use of Reed-Solomon becomes much more attractive, at least for packets of length 255 or less.\n• For RS(255,239) and other decoders with higher \\( t \\) values, the output almost never is . Probability of becomes greater even though we are beyond the Hamming bound of at most \\( t \\) correctible errors. Reed-Solomon is very attractive, but the encoding cost starts to get higher.\n\nThese probabilities of a outcome are fairly close to their theoretical values, which are just the fraction of all possible messages which are within a Hamming distance of \\( t \\) of a valid codeword.\n\nFor example, if we have RS(255,253), then the last two bytes of valid codewords are completely dependent on the preceding bytes, so the fraction of valid codewords is \\( 1/256^2 = 1/65536 \\), and the fraction of all possible messages which are within a distance of 1 of those valid codewords is \\( \\frac{1 + 255 \\times 255}{256^2} \\approx 0.9922 \\). For each codeword we have one message with distance 0 (the codeword itself) and \\( 255 \\times 255 \\) messages with distance 1: each of the 255 message bytes can be corrupted with 255 other values.\n\nOr if we have RS(255,251), this fraction becomes \\( \\frac{1 + 255 \\times 255 + \\frac{255 \\times 254}{2} \\times 255^2}{256^4} \\approx 0.4903 \\).\n\nThe general formula for this fraction is\n\nwhich we can graph:\n\nThe sweet spot for low-end embedded systems is probably to use Reed-Solomon (255,247), where \\( t=4 \\). Possibly (255,239), where \\( t=8 \\). Smaller values of \\( t \\) allow a table-based implementation with a minimum amount of CPU time and memory; larger values of \\( t \\) lower the probability of erroneous correction.\n\nBut the most important thing is really to understand the behavior of errors in your system:\n• Probability of errors – run some tests and try to figure out the rate of errors without any attempt at error correction. A system with an average error rate of 1 error per \\( 10^{6} \\) bytes is a much different system to work with, than one that has an average error rate of 1 error per 1000 bytes.\n• Impact of errors – if you’re working with audio data, an error might sound like a brief “pop” in the sound, and this may not be a big deal. If you have a sensor gathering and transmitting data from a once-in-a-lifetime experiment, and losing data can make or break the results, then error correction coding is relatively cheap insurance.\n\nReed-Solomon codes are now in widespread use.\n\nWe already talked about the use of Reed-Solomon codes in the Voyager space probes from Uranus and Neptune onwards. They have been supplanted in more recent NASA missions, such as the Mars Reconaissance Encoder by turbo codes, which have higher \\( E_b/N_0 \\) gain.\n\nCompact discs and DVDs use cross-interleaved Reed-Solomon encoding to increase the robustness against errors even if they show up as scratches impacting large numbers of bits.\n\nBroadband data transmission systems like VDSL2 use Reed-Solomon codes to reduce error rates for a given transmitter energy.\n\nThe application I find most interesting is in QR codes, where Reed-Solomon codes are used along with a clever scheme to distribute the data bits spatially over a wide area. This makes them robust to severe data loss errors.\n\nNormally you see QR code images like this:\n\nBut they can still be decoded even when part of the information has been corrupted, like this:\n\nor even this:\n\nI couldn’t find a QR code application that outputs the raw binary bits before decoding, but it would be interesting to see how many errors or erasures these images have. The four levels of error correction in a QR code, L, M, Q, and H, allow up to 30% errors by requiring more bits to encode a given message. The code above is an H-level code, and I kept adding larger and larger disturbances until the decoder was unable to process an image; these are slightly below the failure point, so they are probably in the 25-30% range.\n\nOK! We’ve wandered around the topic of Reed-Solomon codes today, covering a few important points:\n• Reed-Solomon codes represent messages as polynomials with coefficients in \\( GF(2^m) \\), and are often denoted as \\( RS(n,k) \\)\n• The Reed-Solomon \\( (255,255-2t) \\) code is a common subset, representing a codeword as 255 bytes, each byte an element of \\( GF(2^8) \\). This allows for correction of up to \\( t \\) errors or \\( 2t \\) erasures.\n• To specify a particular Reed-Solomon code, in addition to knowing \\( n \\) and \\( k \\), you need the characteristic polynomial of the symbol field — for \\( GF(2^8) \\), this is a degree 8 polynomial — and the generator polynomial, which is of the form \\( g(x) = (x+\\lambda^b)(x+\\lambda^{b+1})(x+\\lambda^{b+2})\\ldots (x+\\lambda^{b+2t-1}) \\), where \\( b \\) is an integer and \\( \\lambda \\) is any generating element in \\( GF(2^8) \\) — meaning that all nonzero elements of \\( GF(2^8) \\) are expressible as \\( \\lambda^i \\) for some integer \\( i \\) between 0 and 254. The important property of \\( g(x) \\) is that its roots are \\( 2t \\) consecutive powers of \\( \\lambda \\).\n• Algebraically, encoding a message involves expressing it as a polynomial \\( M(x) \\), computing \\( r(x) = x^{2t}M(x) \\bmod g(x) \\) and constructing the codeword \\( C_t(x) = x^{2t}M(x) + r(x) \\)\n• In practical terms, we can encode a message using an LFSR where the shift cells contain elements of \\( GF(2^8) \\) and the LFSR taps are multiplier coefficients in \\( GF(2^8) \\), each tap corresponding to the corresponding coefficient in the generator polynomial.\n• This LFSR approach can be implemented using lookup tables to handle the finite field mathematics\n• Decoding takes considerably more processing power, and we didn’t cover the details in this article.\n• Shortened Reed-Solomon codes are used by taking an \\( (n,k) \\) code and using it as an \\( (n-l,k-l) \\) code, where both transmitter and receiver assume that there are \\( l \\) implicit zeros at the beginning of the message.\n• Calculating bit error rates is a pain. But if you do it right, you will end up with curves of data bit error rate vs. \\( E_b/N_0 \\), like this graph:\n• Higher values of \\( t \\) reduce the code rate somewhat, but allow for greater gains in \\( E_b/N_0 \\) for the same data bit error rate. (Or alternatively, much lower data bit error rates for the same amount of signal energy per bit per unit noise.)\n• Use of error correction reduces the robustness to error detection, especially at low values of \\( t \\), but for \\( t > 4 \\) or so, the probability of mistakenly correcting more than \\( t \\) errors to the wrong codeword is very small. The vast majority of the time if there are more than \\( t \\) errors, the decoder just fails and the number of errors remains unchanged. There are cases with more than \\( t \\) errors where the decoder would produce a correction to the wrong codeword, but it has a low probability unless the errors are introduced intentionally.\n• Reed-Solomon has lots of applications for reducing the probability of errors: space transmission, storage formats like CDs and DVDs, QR codes, and DSL.\n\nNext time we will tackle an interesting problem relating to CRCs.\n• Gregory Mitchell, ARL-TR-4901: Investigation of Hamming, Reed-Solomon, and Turbo Forward Error Correcting Codes, US Army Research Laboratory, July 2009.\n• Fabrizio Pollara, A Software Simulation Study of a (255,223) Reed-Solomon Encoder-decoder, Jet Propulsion Laboratory, JPL Publication 85-23, April 1985. (This report is interesting because it contains C code for both an encoder and decoder. I have not tried to use either, and it’s nearly unreadable code, using short cryptic identifiers everywhere.)\n• I. S. Reed and G. Solomon, Polynomial Codes over Certain Finite Fields, Journal of the Society for Industrial and Applied Mathematics, Volume 8, issue 2, pp. 300–304, June 1960.\n• Yasuo Sugiyama, Masao Kasahara, Shigeichi Hirasawa, and Toshihiko Namekawa, A Method for Solving Key Equation for Decoding Goppa Codes, Information and Control Volume 27, issue 1, January 1975, pp 87-99.\n• Dilip Sarwate, ECE 361: Lecture 3: Matched Filters – Part I, 2011. This has the equation for determining the variance of received white noise after it goes through a filter \\( h(t) \\).\n• Bill Casselman, How to Read QR Symbols Without Your Mobile Telephone, American Mathematical Society, 2013."
    },
    {
        "link": "https://en.wikiversity.org/wiki/Reed%E2%80%93Solomon_codes_for_coders",
        "document": "Error correcting codes are a signal processing technique to correct errors. They are nowadays ubiquitous, such as in communications (mobile phone, internet), data storage and archival (hard drives, optical discs CD/DVD/BluRay, archival tapes), warehouse management (barcodes) and advertisement (QR codes).\n\nReed–Solomon error correction is a specific type of error correction code. It is one of the oldest but it is still widely used, as it is very well defined and several efficient algorithms are now available under the public domain.\n\nUsually, error correction codes are hidden and most users do not even know about them, nor when they are used. Yet, they are a critical component for some applications to be viable, such as communication or data storage. Indeed, a hard drive that would randomly lose data every few days would be useless, and a phone being able to call only on days with a cloud-less weather would be seldom used. Using error correction codes allows to recover a corrupted message into the full original message.\n\nBarcodes and QR codes are interesting applications to study, as they have the specificity of displaying visually the error correction code, rendering these codes readily accessible to the curious user.\n\nIn this essay, we will attempt to introduce the principles of Reed–Solomon codes from the point of view of a programmer rather than a mathematician, which means that we will focus more on the practice than the theory, although we will also explain the theory, but only the necessary knowledge for intuition and implementation. Notable references in the domain will be provided, so that the interested reader can dig deeper into the mathematical theory at will. We will provide real-world examples taken from the popular QR code barcode system as well as working code samples. We chose to use Python for the samples (mainly because it looks pretty and similar to pseudocode), but we will try to explain any non-obvious features for those who are not familiar with it. The mathematics involved is advanced in the sense that it is not usually taught below the university level, but it should be understandable to someone with a good grasp of high-school algebra.\n\nWe will first gently introduce the intuitions behind error correction codes principles, then in a second section we will introduce the structural design of QR codes, in other words how information is stored in a QR code and how to read and produce it, and in a third section we will study error correction codes via the implementation of a Reed–Solomon decoder, with a quick introduction of the bigger BCH codes family, in order to reliably read damaged QR codes.\n\nNote for the curious readers that extended information can be found in the appendix and on the discussion page.\n\nBefore detailing the code, it might be useful to understand the intuition behind error correction. Indeed, although error correcting codes may seem daunting mathematically-wise, most of the mathematical operations are high school grade (with the exception of Galois Fields, but which are in fact easy and common for any programmer: it's simply doing operations on integers modulo a number). However, the complexity of the mathematical ingenuity behind error correction codes hide the quite intuitive goal and mechanisms at play.\n\nError correcting codes might seem like a difficult mathematical concept, but they are in fact based on an intuitive idea with an ingenious mathematical implementation: let's make the data structured, in a way that we can \"guess\" what the data was if it gets corrupted, just by \"fixing\" the structure. Mathematically-wise, we use polynomials from the Galois Field to implement this structure.\n\nLet's take a more practical analogy: let's say you want to communicate messages to someone else, but these messages can get corrupted along the way. The main insight of error correcting codes is that, instead of using a whole dictionary of words, we can use a smaller set of carefully selected words, a \"reduced dictionary\", so that each word is as different as any other. This way, when we get a message, we just have to lookup inside our reduced dictionary to 1) detect which words are corrupted (as they are not in our reduced dictionary); 2) correct corrupted words by finding the most similar word in our dictionary.\n\nLet's take a simple example: we have a reduced dictionary with only three words of 4 letters: , and . Let's say we receive a corrupted word: , where is an erasure. Since we have only 3 words in our dictionary, we can easily compare our received word with our dictionary to find the word that is the closest. In this case, it's . Thus the missing letters are .\n\nNow let's say we receive the word . Here the problem is that we have two words in our dictionary that match the received word: and . In this case, we cannot be sure which one it is, and thus we cannot decode. This means that our dictionary is not very good, and we should replace with another more different word, such as to maximize the difference between each word. This difference, or more precisely the minimum number of different letters between any 2 words of our dictionary, is called the maximum Hamming distance of our dictionary. Making sure that any 2 words of the dictionary share a minimum number of letters at the same position is called maximum separability.\n\nThe same principle is used for most error correcting codes: we generate a reduced dictionary containing only words with maximum separability (we will detail more how to do that in the third section), and then we communicate only with the words of this reduced dictionary. What Galois Fields provide is the structure (ie, reduced dictionary basis), and Reed–Solomon is a way to automatically create a suitable structure (make a reduced dictionary with maximum separability tailored for a dataset), as well as provide the automated methods to detect and correct errors (ie, lookups in the reduced dictionary). To be more precise, Galois Fields are the structure (thanks to their cyclic nature, the modulo an integer) and Reed–Solomon is the codec (encoder/decoder) based on Galois Fields.\n\nIf a word gets corrupted in the communication, that's no big deal since we can easily fix it by looking inside our dictionary and find the closest word, which is probably the correct one (there is however a chance of choosing a wrong one if the input message is too heavily corrupted, but the probability is very small). Also, the longer our words are, the more separable they are, since more characters can be corrupted without any impact.\n\nThe simplest way to generate a dictionary of maximally separable words is to make words longer than they really are.\n\nLet's take again our example:\n\nAppend a unique set of characters so that there are no duplicated characters at any of the appended positions, and add one more word to help with the explanation:\n\nNote that each word in this dictionary differs from every other word by at least 6 characters, so the distance is 6. This allows up to 5 errors in known positions (which are called erasures), or 3 errors in unknown positions, to be corrected.\n\nThen a search of the dictionary for the 4 non-erased characters can be done to find the only entry that matches those 4 characters, since the distance is 5. Here it gives:\n\nAssume that 2 errors occur as in one of these patterns:\n\nThe issue here is the location of the errors is unknown. The erasures might have happened in any 2 positions meaning that there are or 28 possible sub-sets of 6 characters:\n\nIf we do a dictionary search on each of these sub-sequences, we find that there is only one sub-set that matches 6 characters. matches .\n\nWith these examples, one can see the advantage of redundancy in recovering lost information: redundant characters help you recover your original data. The previous examples show how a crude error correcting scheme could work. Reed–Solomon's core idea is similar, append redundant data to a message based on Galois Field mathematics. The original error correcting decoder was similar to the error example above, search sub-sets of a received message that correspond to a valid message, and choose the one with the most matches as the corrected message. This isn't practical for larger messages, so mathematical algorithms were developed to perform error correction in a reasonable time.\n\nThis section introduces the structure of QR codes, which is how data is stored in a QR code. The information in this section is deliberately incomplete. Only the most common features of the small 21×21 size symbols (also known as version 1) are presented here, but see the appendix for additional information.\n\nHere is a QR symbol that will be used as an example. It consists of dark and light squares, known as modules in the barcoding world. The three square locator patterns in the corners are a visually distinctive feature of QR symbols.\n\nA masking process is used to avoid features in the symbol that might confuse a scanner, such as misleading shapes that look like the locator patterns and large blank areas. Masking inverts certain modules (white becomes black and black becomes white) while leaving others alone.\n\nIn the diagram below, the red areas encode format information and use a fixed masking pattern. The data area (in black and white) is masked with a variable pattern. When the code is created, the encoder tries a number of different masks and chooses the one that minimizes undesirable features in the result. The chosen mask pattern is then indicated in the format information so that the decoder knows which one to use. The light gray areas are fixed patterns which do not encode any information. In addition to the obvious locator patterns, there are also timing patterns which contain alternating light and dark modules.\n\nThe masking transformation is easily applied (or removed) using the exclusive-or operation (denoted by a caret ^ in many programming languages). The unmasking of the format information is shown below. Reading counter-clockwise around the upper-left locator pattern, we have the following sequence of bits. White modules represent 0 and black modules represent 1.\n\nThere are two identical copies of the formatting information, so that the symbol can still be decoded even if it is damaged. The second copy is broken in two pieces and placed around the other two locators, and is read in a clockwise direction (upwards in the lower-left corner, then left-to-right in the upper-right corner).\n\nThe first two bits of formatting information give the error correction level used for the message data. A QR symbol this size contains 26 bytes of information. Some of these are used to store the message and some are used for error correction, as shown in the table below. The left-hand column is simply a name given to that level.\n\nThe next three bits of format information select the masking pattern to be used in the data area. The patterns are illustrated below, including the mathematical formula that tells whether a module is black (i and j are the row and column numbers, respectively, and start with 0 in the upper-left hand corner).\n\nThe remaining ten bits of formatting information are for correcting errors in the format itself. This will be explained in a later section.\n\nHere is a larger diagram showing the \"unmasked\" QR code. Different regions of the symbol are indicated, including the boundaries of the message data bytes.\n\nData bits are read starting from the lower-right corner and moving up the two right-hand columns in a zig-zag pattern. The first three bytes are 01000000 11010010 01110101. The next two columns are read in a downward direction, so the next byte is 01000111. Upon reaching the bottom, the two columns after that are read upward. Proceed in this up-and-down fashion all the way to the left side of the symbol (skipping over the timing pattern where necessary). Here is the complete message in hexadecimal notation.\n\nThe final step is to decode the message bytes into something readable. The first four bits indicate how the message is encoded. QR codes use several different encoding schemes, so that different kinds of messages can be stored efficiently. These are summarized in the table below. After the mode indicator is a length field, which tells how many characters are stored. The size of the length field depends on the specific encoding.\n\nOur sample message starts with 0100 (hex 4), indicating that there are 8 bits per character. The next 8 bits (hex 0d) are the length field, 13 in decimal notation. The bits after that can be rearranged in bytes representing the actual characters of the messageː 27 54 77 61 73 20 62 72 69 6c 6c 69 67, and additionally 0e c. The first two, hex 27 and 54 are the ASCII codes for apostrophe and T. The whole message is \"'Twas brillig\" (from w:Jabberwocky#Lexicon).\n\nAfter the last of the data bits is another 4-bit mode indicator. It can be different from the first one, allowing different encodings to be mixed within the same QR symbol. When there is no more data to store, the special end-of-message code 0000 is given. (Note that the standard allows the end-of-message code to be omitted if it wouldn't fit in the available number of data bytes.)\n\nAt this point, we know how to decode, or read, a whole QR code. However, in real life conditions, a QR code is rarely whole: usually, it is scanned via a phone's camera, under potentially poor lighting conditions, or on a scratched surface where part of the QR code was ripped, or on a stained surface, etc.\n\nTo make our QR code decoder **reliable**, we need to be able to **correct** errors. The next part of this article will describe how to correct errors, by constructing a BCH decoder, and more specifically a Reed–Solomon decoder.\n\nIn this section, we introduce a general class of error correction codes: the BCH codes, the parent family of modern Reed–Solomon codes, and the basic detection and correction mechanisms.\n\nThe formatting information is encoded with a BCH code which allows a certain number of bit-errors to be detected and corrected. BCH codes are a generalization of Reed–Solomon codes (modern Reed–Solomon codes are BCH codes). In the case of QR codes, the BCH code used for the format information is much simpler than the Reed–Solomon code used for the message data, so it makes sense to start with the BCH code for format information.\n\nThe process for checking the encoded information is similar to long division, but uses exclusive-or instead of subtraction. The format code should produce a remainder of zero when it is \"divided\" by the so-called generator of the code. QR format codes use the generator 10100110111. This process is demonstrated for the format information in the example code (000111101011001) below.\n\nHere is a Python function which implements this calculation.\n\nPython note: The function may not be clear to non-Python programmers. It produces a list of numbers counting down from 4 to 0 (the code has \"-1\" because the interval returned by \"range\" includes the start but not the end value). In C-derived languages, the for loop might be written as ; in Pascal-derived languages, .\n\nPython note 2: The operator performs bitwise and, while is a left bit-shift. This is consistent with C-like languages.\n\nThis function can also be used to encode the 5-bit format information.\n\nReaders may find it an interesting exercise to generalize this function to divide by different numbers. For example, larger QR codes contain six bits of version information with 12 error correction bits using the generator 1111100100101.\n\nIn mathematical formalism, these binary numbers are described as polynomials whose coefficients are integers mod 2. Each bit of the number is a coefficient of one term. For example:\n\nIf the remainder produced by is not zero, then the code has been damaged or misread. The next step is to determine which format code is most likely the one that was intended (ie, lookup in our reduced dictionary).\n\nAlthough sophisticated algorithms for decoding BCH codes exist, they are probably overkill in this case. Since there are only 32 possible format codes, it's much easier to simply try each one and pick the one that has the smallest number of bits different from the code in question (the number of different bits is known as the Hamming distance). This method of finding the closest code is known as exhaustive search, and is possible only because we have very few codes (a code is a valid message, and here there are only 32, all other binary numbers aren't correct).\n\nThe function returns -1 if the format code could not be unambiguously decoded. This happens when two or more format codes have the same distance from the input.\n\nTo run this code in Python, first start IDLE, Python's integrated development environment. You should see a version message and the interactive input prompt . Open a new window, copy the functions , , and into it, and save as . Return to the prompt and type the lines following below.\n\nYou can also start Python by typing at a command prompt.\n\nIn the next sections, we will study Finite Field Arithmetics and Reed–Solomon code, which is a subtype of BCH codes. The basic idea (ie, using a limited words dictionary with maximum separability) is the same, but since we will encode longer words (256 bytes instead of 2 bytes), with more symbols available (encoded on all 8bits, thus 256 different possible values), we cannot use this naive, exhaustive approach, because it would take way too much time: we need to use cleverer algorithms, and Finite Field mathematics will help us do just that, by giving us a structure.\n\nBefore discussing the Reed–Solomon codes used for the message, it will be useful to introduce a bit more mathematics.\n\nWe'd like to define addition, subtraction, multiplication, and division for 8-bit bytes and always produce 8-bit bytes as a result, so as to avoid any overflow. Naively, we might attempt to use the normal definitions for these operations, and then mod by 256 to keep results from overflowing. And this is exactly what we will be doing, and is what is called a Galois Field 2^8. You can easily imagine why it works for everything, except for division: what is 5/4?\n\nHere's a brief introduction to Galois Fields: a finite field is a set of numbers, and a field needs to have six properties governing addition, subtraction, multiplication and division: Closure, Associative, Commutative, Distributive, Identity and Inverse. More simply put, using a field allows us to study the relationship between numbers of this field, and apply the result to any other field that follows the same properties. For example, the set of reals ℝ is a field. In other words, mathematical fields studies the structure of a set of numbers.\n\nHowever, integers ℤ aren't a field, because as we said above, not all divisions are defined (such as 5/4), which violates multiplicative inverse property (x such that x*4=5 does not exist). One simple way to fix that is to do modulo using a prime number, such as 257, or any positive integer power of a prime number: in this way, we are guaranteed that x*4=5 exists since we will just wrap around. ℤ modulo any prime number is called a Galois Field, and modulo 2 is an extra interesting Galois Field: since an 8-bit string can express a total of 256 = 2^8 values, we say that we use a Galois Field of 2^8, or GF(2^8). In spoken language, 2 is the characteristic of the field, 8 is the exponent, and 256 is the field's cardinality. More information on finite fields can be found here.\n\nHere we will define the usual mathematical operations that you are used to doing with integers, but adapted to GF(2^8), which is basically doing usual operations but modulo 2^8.\n\nAnother way to consider the link between GF(2) and GF(2^8) is to think that GF(2^8) represents a polynomial of 8 binary coefficients. For example, in GF(2^8), 170 is equivalent to . Both representations are equivalent, it's just that in the first case, 170, the representation is decimal, and in the other case it's binary, which can be thought as representing a polynomial by convention (only used in GF(2^p) as explained here). The latter is often the representation used in academic books and in hardware implementations (because of logical gates and registers, which work at the binary level). For a software implementation, the decimal representation can be preferred for clearer and more close-to-the-mathematics code (this is what we will use for the code in this tutorial, except for some examples that will use the binary representation).\n\nIn any case, try to not confuse the polynomial representing a single GF(2^p) symbol (each coefficient is a bit/boolean: either 0 or 1), and the polynomial representing a list of GF(2^p) symbols (in this case the polynomial is equivalent to the message+RScode, each coefficient is a value between 0 and 2^p and represent one character of the message+RScode). We will first describe operations on single symbol, then polynomial operations on a list of symbols.\n\nBoth addition and subtraction are replaced with exclusive-or in Galois Field base 2. This is logical: addition modulo 2 is exactly like an XOR, and subtraction modulo 2 is exactly the same as addition modulo 2. This is possible because additions and subtractions in this Galois Field are carry-less.\n\nThinking of our 8-bit values as polynomials with coefficients mod 2:\n\nThe same way (in binary representation of two single GF(2^8) integers):\n\nSince , every number is its own opposite, so (x - y) is the same as (x + y).\n\nNote that in books, you will find additions and subtractions to define some mathematical operations on GF integers, but in practice, you can just XOR (as long as you are in a Galois Field base 2; this is not true in other fields).\n\nHere is the equivalent Python code:\n\nMultiplication is likewise based on polynomial multiplication. Simply write the inputs as polynomials and multiply them out using the distributive law as normal. As an example, 10001001 times 00101010 is calculated as follows.\n\nThe same result can be obtained by a modified version of the standard grade-school multiplication procedure, in which we replace addition with exclusive-or.\n\nNote: the XOR multiplication here is carry-less! If you do it with-carry, you will get the wrong result 1011001111010 with the extra term x9 instead of the correct result 1010001111010.\n\nHere is a Python function which implements this polynomial multiplication on single GF(2^8) integers.\n\nNote: this function (and some other functions below) use a lot of bitwise operators such as >> and <<, because they are both faster and more concise to do what we want to do. These operators are available in most languages, they are not specific to Python, and you can get more information about them here.\n\nOf course, the result no longer fits in an 8-bit byte (in this example, it is 13 bits long), so we need to perform one more step before we are finished. The result is reduced modulo 100011101 (the choice of this number is explained below the code), using the long division process described previously. In this instance, this is called \"modular reduction\", because basically what we do is that we divide and keep only the remainder, using a modulo. This produces the final answer 11000011 in our example.\n\nHere is the Python code to do the whole Galois Field multiplication with modular reduction:\n\nWhy mod 100011101 (in hexadecimal: 0x11d)? The mathematics is a little complicated here, but in short, 100011101 represents an 8th degree polynomial which is \"irreducible\" (meaning it can't be represented as the product of two smaller polynomials). This number is called a primitive polynomial or irreducible polynomial, or prime polynomial (we will mainly use this latter name for the rest of this tutorial). This is necessary for division to be well-behaved, which is to stay in the limits of the Galois Field, but without duplicating values. There are other numbers we could have chosen, but they're all essentially the same, and 100011101 (0x11d) is a common primitive polynomial for Reed–Solomon codes. If you are curious to know how to generate those prime polynomials, please see the appendix.\n\nAdditional infos on the prime polynomial (you can skip): What is a prime polynomial? It is the equivalent of a prime number, but in the Galois Field. Remember that a Galois Field uses values that are multiples of 2 as the generator. Of course, a prime number cannot be a multiple of two in standard arithmetics, but in a Galois Field it is possible. Why do we need a prime polynomial? Because to stay in the bound of the field, we need to compute the modulo of any value above the Galois Field. Why don't we just modulo with the Galois Field size? Because we will end up with lots of duplicate values, and we want to have as many unique values as possible in the field, so that a number always has one and only projection when doing a modulo or a XOR with the prime polynomial.\n\nNote for the interested reader: as an example of what you can achieve with clever algorithms, here is another way to achieve multiplication of GF numbers in a more concise and faster way, using the Russian Peasant Multiplication algorithm:\n\nNote that using this last function with parameters prim=0 and carryless=False will return the result for a standard integers multiplication (and thus you can see the difference between carryless and with-carry addition and its impact on multiplication).\n\nThe procedure described above is not the most convenient way to implement Galois field multiplication. Multiplying two numbers takes up to eight iterations of the multiplication loop, followed by up to eight iterations of the division loop. However, we can multiply with no looping by using lookup tables. One solution would be to construct the entire multiplication table in memory, but that would require a bulky 64k table. The solution described below is much more compact.\n\nFirst, notice that it is particularly easy to multiply by 2=00000010 (by convention, this number is referred to as α or the generator number): simply left-shift by one place, then exclusive-or with the modulus 100011101 if necessary (why xor is sufficient for taking the mod in this case is an exercise left to the reader). Here are the first few powers of α.\n\nIf this table is continued in the same fashion, the powers of α do not repeat themselves until α255 = 00000001. Thus, every element of the field except zero is equal to some power of α. The element α, that we define, is known as a primitive element or generator of the Galois field.\n\nThis observation suggests another way to implement multiplication: by adding the exponents of α.\n\nThe problem is, how do we find the power of α that corresponds to 10001001? This is known as the discrete logarithm problem, and no efficient general solution is known. However, since there are only 256 elements in this field, we can easily construct a table of logarithms. While we're at it, a corresponding table of antilogs (exponentials) will also be useful. More mathematical information about this trick can be found here.\n\nPython note: The operator's upper bound is exclusive, so is not set twice by the above.\n\nThe table is oversized in order to simplify the multiplication function. This way, we don't have to check to make sure that is within the table size.\n\nAnother advantage of the logarithm table approach is that it allows us to define division using the difference of logarithms. In the code below, 255 is added to make sure the difference isn't negative.\n\nPython note: The statement throws an exception and aborts execution of the function.\n\nWith this definition of division, for any and any nonzero .\n\nReaders who are more advanced programmers may find it interesting to write a class encapsulating Galois field arithmetic. Operator overloading can be used to replace calls to and with the familiar operators and , but this can lead to confusion as to exactly what type of operation is being performed. Certain details can be generalized in ways that would make the class more widely useful. For example, Aztec codes use five different Galois fields with element sizes ranging from 4 to 12 bits.\n\nThe logarithm table approach will once again simplify and speed up our calculations when computing the power and the inverse:\n\nBefore moving on to Reed–Solomon codes, we need to define several operations on polynomials whose coefficients are Galois field elements. This is a potential source of confusion, since the elements themselves are described as polynomials; my advice is not to think about it too much. Adding to the confusion is the fact that x is still used as the placeholder. This x has nothing to do with the x mentioned previously, so don't mix them up.\n\nThe binary notation used previously for Galois field elements starts to become inconveniently bulky at this point, so I will switch to hexadecimal instead.\n\nIn Python, polynomials will be represented by a list of numbers in descending order of powers of x, so the polynomial above becomes . (The reverse order could have been used instead; both choices have their advantages and disadvantages.)\n\nThe first function multiplies a polynomial by a scalar.\n\nNote to Python programmers: This function is not written in a \"pythonic\" style. It could be expressed quite elegantly as a list comprehension, but I have limited myself to language features that are easier to translate to other programming languages.\n\nThis function \"adds\" two polynomials (using exclusive-or, as usual).\n\nThe next function multiplies two polynomials.\n\nFinally, we need a function to evaluate a polynomial at a particular value of x, producing a scalar result. Horner's method is used to avoid explicitly calculating powers of x. Horner's method works by factorizing consecutively the terms, so that we always deal with x^1, iteratively, avoiding the computation of higher degree terms:\n\nThere's still one missing polynomial operation that we will need: polynomial division. This is more complicated than the other operations on polynomial, so we will study it in the next chapter, along with Reed–Solomon encoding.\n\nNow that the preliminaries are out of the way, we are ready to begin looking at Reed–Solomon codes.\n\nBut first, why did we have to learn about finite fields and polynomials? Because this is the main insight of error-correcting codes like Reed–Solomon: instead of just seeing a message as a series of (ASCII) numbers, we see it as a polynomial following the very well-defined rules of finite field arithmetic. In other words, by representing the data using polynomials and finite fields arithmetic, we added a structure to the data. The values of the message are still the same, but this conceptual structure now allows us to operate on the message, on its characters values, using well defined mathematical rules. This structure, that we always know because it's outside and independent of the data, is what allows us to repair a corrupted message.\n\nThus, even if in your code implementation you may choose to not explicitly represent the polynomials and the finite field arithmetic, these notions are essential for the error-correcting codes to work, and you will find these notions to underlie (even if implicitly) any implementation.\n\nAnd now we will put these notions into practice!\n\nLike BCH codes, Reed–Solomon codes are encoded by dividing the polynomial representing the message by an irreducible generator polynomial, and then the remainder is the RS code, which we will just append to the original message.\n\nWhy? We previously said that the principle behind BCH codes, and most other error correcting codes, is to use a reduced dictionary with very different words as to maximize the distance between words, and that longer words have greater distance: here it's the same principle, first because we lengthen the original message with additional symbols (the remainder) which raises the distance, and secondly because the remainder is almost unique (thanks to the carefully designed irreducible generator polynomial), so that it can be exploited by clever algorithms to deduce parts of the original message.\n\nTo summarize, with an approximated analogy to encryption: our generator polynomial is our encoding dictionary, and polynomial division is the operator to convert our message using the dictionary (the generator polynomial) into a RS code.\n\nTo manage errors and cases where we can't correct a message, we will display a meaningful error message, by raising an exception. We will make our own custom exception so that users can easily catch and manage them:\n\nTo display an error by raising our custom exception, we can then simply do the following:\n\nAnd you can easily catch this exception to manage it by using a try/except block:\n\nReed–Solomon codes use a generator polynomial similar to BCH codes (not to be confused with the generator number alpha). The generator is the product of factors (x - αn), starting with n=0 for QR codes.\n\nThe same as (x + ai) because of GF(2^8).\n\nHere is a function that computes the generator polynomial for a given number of error correction symbols.\n\nThis function is somewhat inefficient in that it allocates successively larger arrays for . While this is unlikely to be a performance problem in practice, readers who are inveterate optimizers may find it interesting to rewrite it so that is only allocated once, or you can compute once and memorize g since it is fixed for a given nsym, so you can reuse g.\n\nSeveral algorithms for polynomial division exist, the simplest one that is often taught in elementary school is long division. This example shows the calculation for the message .\n\nNote: The concepts of polynomial long division apply, but there are a few important differences: When computing the resulting terms/coefficients that will be Galois Field subtracted from the divisor, bitwise carryless multiplication is performed and the result \"bitstream\" is XORed from the first encountered MSB with the chosen primitive polynomial until the answer is less than the Galois Field value, in this case, 256. The XOR \"subtractions\" are then performed as usual.\n\nTo illustrate the method for one operation (0x12 * 0x36):\n\nThe remainder is concatenated with the message, so the encoded message is .\n\nHowever, long division is quite slow as it requires a lot of recursive iterations to terminate. More efficient strategies can be devised, such as using synthetic division (also called Horner's method, a good tutorial video can be found on Khan Academy). Here is a function that implements extended synthetic division of GF(2^p) polynomials (extended because the divisor is a polynomial instead of a monomial):\n\nAnd now, here's how to encode a message to get its RS code:\n\nSimple, isn't it? Encoding is in fact the easiest part in Reed–Solomon, and it's always the same approach (polynomial division). Decoding is the tough part of Reed–Solomon, and you will find a lot of different algorithms depending on your needs, but we will touch on that later on.\n\nThis function is quite fast, but since encoding is quite critical, here is an enhanced encoding function that inlines the polynomial synthetic division, which is the form that you will most often find in Reed–Solomon software libraries:\n\nThis algorithm is faster, but it's still quite slow for practical use, particularly in Python. There are some ways to optimize the speed by using various tricks, such as inlining (instead of gf_mul, replace by the operation to avoid a call), by precomputing (the logarithm of gen and of coef, or even by generating a multiplication table – but it seems the latter does not work well in Python), by using statically typed constructs (assign gf_log and gf_exp to ), by using memoryviews (like by changing all your lists to bytearrays), by running it with PyPy, or by converting the algorithm into a Cython or a C extension[1].\n\nThis example shows the encode function applied to the message in the sample QR code introduced earlier. The calculated error correction symbols (on the second line) match the values decoded from the QR code.\n\nPython version note: The syntax for the function has changed, and this example uses the Python 3.0+ version. In previous versions of Python (particularly Python 2.x), replace the line with (including the final comma) and by .\n\nReed–Solomon decoding is the process that, from a potentially corrupted message and a RS code, returns a corrected message. In other words, decoding is the process of repairing your message using the previously computed RS code.\n\nAlthough there is only one way to encode a message with Reed–Solomon, there are lots of different ways to decode them, and thus there are a lot of different decoding algorithms.\n\nHowever, we can generally outline the decoding process in 5 steps[2][3]:\n• Compute the syndromes polynomial. This allows us to analyze what characters are in error using Berlekamp-Massey (or another algorithm), and also to quickly check if the input message is corrupted at all.\n• Compute the erasure/error locator polynomial (from the syndromes). This is computed by Berlekamp-Massey, and is a detector that will tell us exactly what characters are corrupted.\n• Compute the erasure/error evaluator polynomial (from the syndromes and erasure/error locator polynomial). Necessary to evaluate how much the characters were tampered (ie, helps to compute the magnitude).\n• Compute the erasure/error magnitude polynomial (from all 3 polynomials above): this polynomial can also be called the corruption polynomial, since in fact it exactly stores the values that need to be subtracted from the received message to get the original, correct message (i.e., with correct values for erased characters). In other words, at this point, we extracted the noise and stored it in this polynomial, and we just have to remove this noise from the input message to repair it.\n• Repair the input message simply by subtracting the magnitude polynomial from the input message.\n\nWe will describe each of those five steps below.\n\nIn addition, decoders can also be classified by the type of error they can repair: erasures (we know the location of the corrupted characters but not the magnitude), errors (we ignore both the location and magnitude), or a mix of errors-and-erasures. We will describe how to support all of these.\n\nDecoding a Reed–Solomon message involves several steps. The first step is to calculate the \"syndrome\" of the message. Treat the message as a polynomial and evaluate it at α0, α1, α2, ..., αn. Since these are the zeros of the generator polynomial, the result should be zero if the scanned message is undamaged (this can be used to check if the message is corrupted, and after correction of a corrupted message if the message was completely repaired). If not, the syndromes contain all the information necessary to determine the correction that should be made. It is simple to write a function to calculate the syndromes.\n\nContinuing the example, we see that the syndromes of the original codeword without any corruption are indeed zero. Introducing a corruption of at least one character into the message or its RS code gives nonzero syndromes.\n\nHere is the code to automate this checking:\n\nIt is simplest to correct mistakes in the code if the locations of the mistakes are already known. This is known as erasure correction. It is possible to correct one erased symbol (ie, character) for each error-correction symbol added to the code. If the error locations are not known, two EC symbols are needed for each symbol error (so you can correct twice less errors than erasures). This makes erasure correction useful in practice if part of the QR code being scanned is covered or physically torn away. It may be difficult for a scanner to determine that this has happened, though, so not all QR code scanners can perform erasure correction.\n\nNow that we already have the syndromes, we need to compute the locator polynomial. This is easy:\n\nNext, computing the erasure/error evaluator polynomial from the locator polynomial is easy, it's simply a polynomial multiplication followed by a polynomial division (that you can replace by a list slicing because that's the effect we want in the end):\n\nFinally, the Forney algorithm is used to calculate the correction values (also called the error magnitude polynomial). It is implemented in the function below.\n\nMathematics note: The denominator of the expression for the error value is the formal derivative of the error locator polynomial . This is calculated by the usual procedure of replacing each term c xn with n c xn-1. Since we're working in a field of characteristic two, n c is equal to c when n is odd, and 0 when n is even. Thus, we can simply remove the even coefficients (resulting in the polynomial ) and evaluate .\n\nPython note: This function uses [::-1] to inverse the order of the elements in a list. This is necessary because the functions do not all use the same ordering convention (ie, some use the least item first, others use the biggest item first). It also use a list comprehension, which is simply a concise way to write a for loop where items are appended in a list, but the Python interpreter can optimize this a bit more than a loop.\n\nContinuing the example, here we use to restore the first byte of the message.\n\nIn the more likely situation where the error locations are unknown (what we usually call errors, in opposition to erasures where the locations are known), we will use the same steps as for erasures, but we now need additional steps to find the location. The Berlekamp–Massey algorithm is used to calculate the error locator polynomial, which we can use later on to determine the errors locations:\n\n# The idea is that BM will iteratively estimate the error locator polynomial. # To do this, it will compute a Discrepancy term called Delta, which will tell us if the error locator polynomial needs an update or not # (hence why it's called discrepancy: it tells us when we are getting off board from the correct value). # if the erasure locator polynomial is supplied, we init with its value, so that we include erasures in the final locator polynomial # This is the main variable we want to fill, also called Sigma in other notations or more formally the errors/errata locator polynomial. # BM is an iterative algorithm, and we need the errata locator polynomial of the previous iteration in order to update other necessary variables. #L = 0 # update flag variable, not needed here because we use an alternative equivalent way of checking if update is needed (but using the flag could potentially be faster depending on if using length(list) is taking linear time in your language, here in Python it's constant so it's as fast. # Fix the syndrome shifting: when computing the syndrome, some implementations may prepend a 0 coefficient for the lowest degree term (the constant). This is a case of syndrome shifting, thus the syndrome will be bigger than the number of ecc symbols (I don't know what purpose serves this shifting). If that's the case, then we need to account for the syndrome shifting when we use the syndrome such as inside BM, by skipping those prepended coefficients. # Another way to detect the shifting is to detect the 0 coefficients: by definition, a syndrome does not contain any 0 coefficient (except if there are no errors/erasures, in this case they are all 0). This however doesn't work with the modified Forney syndrome, which set to 0 the coefficients corresponding to erasures, leaving only the coefficients corresponding to errors. # generally: nsym-erase_count == len(synd), except when you input a partial erase_loc and using the full syndrome instead of the Forney syndrome, in which case nsym-erase_count is more correct (len(synd) will fail badly with IndexError). # if an erasures locator polynomial was provided to init the errors locator polynomial, then we must skip the FIRST erase_count iterations (not the last iterations, this is very important!) # if erasures locator is not provided, then either there's no erasures to account or we use the Forney syndromes, so we don't need to use erase_count nor erase_loc (the erasures have been trimmed out of the Forney syndromes). # Here is the close-to-the-books operation to compute the discrepancy Delta: it's a simple polynomial multiplication of error locator with the syndromes, and then we get the Kth element. #delta = gf_poly_mul(err_loc[::-1], synd)[K] # theoretically it should be gf_poly_add(synd[::-1], [1])[::-1] instead of just synd, but it seems it's not absolutely necessary to correctly decode. # But this can be optimized: since we only need the Kth element, we don't need to compute the polynomial multiplication for any other element but the Kth. Thus to optimize, we compute the polymul only at the item we need, skipping the rest (avoiding a nested loop, thus we are linear time instead of quadratic). # This optimization is actually described in several figures of the book \"Algebraic codes for data transmission\", Blahut, Richard E., 2003, Cambridge university press. # delta is also called discrepancy. Here we do a partial polynomial multiplication (ie, we compute the polynomial multiplication only for the term of degree K). Should be equivalent to brownanrs.polynomial.mul_at(). # Shift polynomials to compute the next degree # Update only if there's a discrepancy # Rule B (rule A is implicitly defined because rule A just says that we skip any modification for this iteration) #if 2*L <= K+erase_count: # equivalent to len(old_loc) > len(err_loc), as long as L is correctly computed # effectively we are doing err_loc * 1/delta = err_loc // delta #L = K - L # the update flag L is tricky: in Blahut's schema, it's mandatory to use `L = K - L - erase_count` (and indeed in a previous draft of this function, if you forgot to do `- erase_count` it would lead to correcting only 2*(errors+erasures) <= (n-k) instead of 2*errors+erasures <= (n-k)), but in this latest draft, this will lead to a wrong decoding in some cases where it should correctly decode! Thus you should try with and without `- erase_count` to update L on your own implementation and see which one works OK without producing wrong decoding failures. # Check if the result is correct, that there's not too many errors to correct # drop leading 0s, else errs will not be of the correct size \"Too many errors to correct\" # too many errors to correct\n\nThen, using the error locator polynomial, we simply use a brute-force approach called trial substitution to find the zeros of this polynomial, which identifies the error locations (ie, the index of the characters that need to be corrected). A more efficient algorithm called Chien search exists, which avoids recomputing the whole evaluation at each iteration step, but this algorithm is left as an exercise to the reader.\n\nMathematics note: When the error locator polynomial is linear ( has length 2), it can be solved easily without resorting to a brute-force approach. The function presented above does not take advantage of this fact, but the interested reader may wish to implement the more efficient solution. Similarly, when the error locator is quadratic, it can be solved by using a generalization of the quadratic formula. A more ambitious reader may wish to implement this procedure as well.\n\nHere is an example where three errors in the message are corrected:\n\nIt is possible for a Reed–Solomon decoder to decode both erasures and errors at the same time, up to a limit (called the Singleton Bound) of , where is the number of errors, the number of erasures and the number of RS code characters (called in the code). Basically, it means that for every erasures, you just need one RS code character to repair it, while for every errors you need two RS code characters (because you need to find the position in addition of the value/magnitude to correct). Such a decoder is called an errors-and-erasures decoder, or an errata decoder.\n\nIn order to correct both errors and erasures, we must prevent the erasures from interfering with the error location process. This can be done by calculating the Forney syndromes, as follows.\n\nThe Forney syndromes can then be used in place of the regular syndromes in the error location process.\n\nThe function below brings the complete procedure together. Erasures are indicated by providing , a list of erasures index positions in the message (the full received message: original message + ecc).\n\nPython note: The lists and are concatenated with the operator.\n\nThis is the last piece needed for a fully operational error-and-erasure correcting Reed–Solomon decoder. If you want to delve more into the inner workings of errata (errors-and-erasures) decoders, you can read the excellent book \"Algebraic Codes for Data Transmission\" (2003) by Richard E. Blahut.\n\nMathematics note: in some software implementations, particularly the ones using a language optimized for linear algebra and matrix operations, you will find that the algorithms (encoding, Berlekamp-Massey, etc.) will seem totally different and use the Fourier Transform. This is because this is totally equivalent: when stated in the jargon of spectral estimation, decoding Reed–Solomon consists of a Fourier transform (syndrome computer), followed by a spectral analysis (Berlekamp-Massey or Euclidian algorithm), followed by an inverse Fourier transform (Chien search). See the Blahut book for more info[4]. Indeed, if you are using a programming language optimized for linear algebra, or if you want to use fast linear algebra libraries, it can be a very good idea to use Fourier Transform since it's very fast nowadays (particularly the Fast Fourier Transform or Number Theoretic Transform[5]).\n\nHere's an example of how to use the functions you have just made, and how to decode both errors-and-erasures:\n\nThis should output the following:\n\nThe basic principles of Reed–Solomon codes have been presented in this essay. Working Python code for a particular implementation (QR codes using a generic Reed–Solomon codec to correct misreadings) has been included. The code presented here is quite generic and can be used for any purpose beyond QR codes where you need to correct errors/erasures, such as file protection, networking, etc. Many variations and refinements of these ideas are possible, since coding theory is a very rich field of study.\n\nIf your code is just intended for your own data (eg, you want to be able to generate and read your own QR codes), then you're fine, but if you intend to work with data provided by others (eg, you want to read and decode QR codes of other apps), then this decoder probably won't be enough, because there are some hidden parameters that were here fixed for simplicity (namely: the generator/alpha number and the first consecutive root). If you want to decode Reed–Solomon codes generated by other libraries, you will need to use a universal Reed–Solomon codec, which will allow you to specify your own parameters, and even go beyond the field 2^8.\n\nOn the complementary resource page, you will find an extended, universal version of the code presented here that you can use to decode almost any Reed–Solomon code, with also a function to generate the list of prime polynomials, and an algorithm to detect the parameters of an unknown RS code. Note that whatever the parameters you use, the repairing capabilities will always be the same: the generated values for the log/antilog tables and for the generator polynomial do not change the structure of Reed–Solomon code, so that you always get the same functionality whatever the parameters. Indeed, modifying any of the available parameter will not change the theoretical Singleton bound which defines the maximal repairing capacity of Reed-Solomon (and in theory of any error correction code).\n\nOne immediate issue that you may have noticed is that we can only encode messages of up to 256 characters. This limit can be circumvented by several ways, the three most common being:\n• using a higher Galois Field, for example 216 which would allow for 65536 characters, or 232, 264, 2128, etc. The issue here is that polynomial computations required to encode and decode Reed–Solomon become very costly with big polynomials (most algorithms being in quadratic time, the most efficient being in n log n such as with number theoretic transform 5 ).\n• by \"chunking\", which means that you simply encode your big data stream by chunks of 256 characters.\n• using a variant algorithm that includes a packet size such as Cauchy Reed–Solomon (see below).\n\nIf you want to go further, there are a lot of books and scientific articles on Reed–Solomon codes, a good starting point is the author Richard Blahut who is notable in the domain. Also, there are a lot of different ways that Reed–Solomon codes can be encoded and decoded, and thus you will find many different algorithms, in particular for decoding (Berlekamp-Massey, Berlekamp-Welch, Euclidian algorithm, etc.).\n\nIf you are looking for more performance, you will find in the literature several variants of the algorithms presented here, such as Cauchy–Reed–Solomon. The programming implementation also plays a big role in the performance of your Reed–Solomon codec, which can lead into a 1000x speed difference. For more information, please read the \"Optimizing performances\" section of the additional resources.\n\nEven if near-optimal forward error correction algorithms are all the rage nowadays (such as LDPC codes, Turbo codes, etc.) because of their great speed, Reed–Solomon is an optimal FEC, which means that it can attain the theoretical limit known as the Singleton bound. In practice, this means that RS can correct up to errors and erasures at the same time, where e is the number of errors, v the number of erasures, k the message size, n the message+code size and the minimum distance. This is not to say that near-optimal FEC are useless: they are unimaginably faster than Reed–Solomon could ever be, and they may suffer less from the cliff effect (which means they may still partially decode parts of the message even if there are too many errors to correct all errors, contrary to RS which will surely fail and even silently by decoding wrong messages without any detection[6]), but they surely can't correct as many errors as Reed–Solomon. Choosing between a near-optimal and an optimal FEC is mainly a concern of speed.\n\nLately, the research field on Reed–Solomon has regained some vitality since the discovery of w:List_decoding (not to confuse with soft decoding), which allows to decode/repair more symbols than the theoretical optimal limit. The core idea is that, instead of standard Reed–Solomon which only do a unique decoding (meaning that it always results in a single solution, if it cannot because it's above the theoretical limit the decoder will return an error or a wrong result), Reed–Solomon with list decoding will still try to decode beyond the limit and get several possible results, but by a clever examination of the different results, it's often possible to discriminate only one polynomial that is probably the correct one.\n\nA few list decoding algorithms are already available that allows to repair up to [7] instead of , and other list decoding algorithms (more efficient or decoding more symbols) are currently being investigated.\n\nHere are a few implementations of Reed–Solomon if you want to see practical examples:\n• Purely functional pure-Python Reedsolomon library by Tomer Filiba and LRQ3000, inspired and expanding on this tutorial by supporting more features.\n• Object-oriented Reed Solomon library in pure-Python by Andrew Brown and LRQ3000 (same features as Tomer Filiba's lib, but object-oriented so closer to mathematical nomenclatura).\n• Reed-Solomon in the Linux Kernel (with a userspace port here, initially ported from Phil Karn's library libfec and libfec clone).\n• Speed-optimized Reed-Solomon and Cauchy-Reed-Solomon with lots of comments and an associated blog for more details.\n• Another high speed-optimized Reed-Solomon in Go language.\n• Port of code in the article in Rust language.\n• C++ Reed Solomon implementation with on-stack memory allocation and compile-time changable msg\\ecc sizes for embedded, inspired by this tutorial.\n• Interleaved Reed Solomon implementation in C++ by NinjaDevelper.\n• FastECC, C++ Reed Solomon implementation in O(n log n) using Number Theoretic Transforms (NTT) (open source, Apache License 2.0). Claims to have fast encoding rates even for large data.\n• Leopard-RS, another library in C++ for fast large data encoding, with a similar (but a bit different) algorithm as FastECC.\n• Shorthair, an implementation of error correction code combined with UDP for fast reliable networking to replace the TCP stack or UDP duplication technique (which can be seen as a low efficiency redundancy scheme). Slides are provided, describing this approach for realtime game networking.\n• Pure C Implementation optimised using uint8_t and very efficient.\n• Short tutorial on Reed-Solomon encoding with an introduction to finite fields\n• Tilavat, V., & Shukla, Y. (2014). Simplification of procedure for decoding Reed–Solomon codes using various algorithms: an introductory survey. International Journal of Engineering Development and Research, 2(1), 279-283.\n• Lin, S. J., Chung, W. H., & Han, Y. S. (2014, October). Novel polynomial basis and its application to reed-solomon erasure codes. In Foundations of Computer Science (FOCS), 2014 IEEE 55th Annual Symposium on (pp. 316-325). IEEE.\n• Sofair, Isaac. \"Probability of miscorrection for Reed-Solomon codes.\" Information Technology: Coding and Computing, 2000. Proceedings. International Conference on. IEEE, 2000."
    },
    {
        "link": "https://nayuki.io/res/reed-solomon-error-correcting-code-decoder/reedsolomon.py",
        "document": ""
    },
    {
        "link": "https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction",
        "document": "In information theory and coding theory, Reed–Solomon codes are a group of error-correcting codes that were introduced by Irving S. Reed and Gustave Solomon in 1960.[1] They have many applications, including consumer technologies such as MiniDiscs, CDs, DVDs, Blu-ray discs, QR codes, Data Matrix, data transmission technologies such as DSL and WiMAX, broadcast systems such as satellite communications, DVB and ATSC, and storage systems such as RAID 6.\n\nReed–Solomon codes operate on a block of data treated as a set of finite-field elements called symbols. Reed–Solomon codes are able to detect and correct multiple symbol errors. By adding t = n − k check symbols to the data, a Reed–Solomon code can detect (but not correct) any combination of up to t erroneous symbols, or locate and correct up to ⌊t/2⌋ erroneous symbols at unknown locations. As an erasure code, it can correct up to t erasures at locations that are known and provided to the algorithm, or it can detect and correct combinations of errors and erasures. Reed–Solomon codes are also suitable as multiple-burst bit-error correcting codes, since a sequence of b + 1 consecutive bit errors can affect at most two symbols of size b. The choice of t is up to the designer of the code and may be selected within wide limits.\n\nThere are two basic types of Reed–Solomon codes – original view and BCH view – with BCH view being the most common, as BCH view decoders are faster and require less working storage than original view decoders.\n\nReed–Solomon codes were developed in 1960 by Irving S. Reed and Gustave Solomon, who were then staff members of MIT Lincoln Laboratory. Their seminal article was titled \"Polynomial Codes over Certain Finite Fields\" (Reed & Solomon 1960). The original encoding scheme described in the Reed and Solomon article used a variable polynomial based on the message to be encoded where only a fixed set of values (evaluation points) to be encoded are known to encoder and decoder. The original theoretical decoder generated potential polynomials based on subsets of k (unencoded message length) out of n (encoded message length) values of a received message, choosing the most popular polynomial as the correct one, which was impractical for all but the simplest of cases. This was initially resolved by changing the original scheme to a BCH-code-like scheme based on a fixed polynomial known to both encoder and decoder, but later, practical decoders based on the original scheme were developed, although slower than the BCH schemes. The result of this is that there are two main types of Reed–Solomon codes: ones that use the original encoding scheme and ones that use the BCH encoding scheme.\n\nAlso in 1960, a practical fixed polynomial decoder for BCH codes developed by Daniel Gorenstein and Neal Zierler was described in an MIT Lincoln Laboratory report by Zierler in January 1960 and later in an article in June 1961.[2] The Gorenstein–Zierler decoder and the related work on BCH codes are described in a book \"Error-Correcting Codes\" by W. Wesley Peterson (1961).[3] By 1963 (or possibly earlier), J. J. Stone (and others) recognized that Reed–Solomon codes could use the BCH scheme of using a fixed generator polynomial, making such codes a special class of BCH codes,[4] but Reed–Solomon codes based on the original encoding scheme are not a class of BCH codes, and depending on the set of evaluation points, they are not even cyclic codes.\n\nIn 1969, an improved BCH scheme decoder was developed by Elwyn Berlekamp and James Massey and has since been known as the Berlekamp–Massey decoding algorithm.\n\nIn 1975, another improved BCH scheme decoder was developed by Yasuo Sugiyama, based on the extended Euclidean algorithm.[5]\n\nIn 1977, Reed–Solomon codes were implemented in the Voyager program in the form of concatenated error correction codes. The first commercial application in mass-produced consumer products appeared in 1982 with the compact disc, where two interleaved Reed–Solomon codes are used. Today, Reed–Solomon codes are widely implemented in digital storage devices and digital communication standards, though they are being slowly replaced by Bose–Chaudhuri–Hocquenghem (BCH) codes. For example, Reed–Solomon codes are used in the Digital Video Broadcasting (DVB) standard DVB-S, in conjunction with a convolutional inner code, but BCH codes are used with LDPC in its successor, DVB-S2.\n\nIn 1986, an original scheme decoder known as the Berlekamp–Welch algorithm was developed.\n\nIn 1996, variations of original scheme decoders called list decoders or soft decoders were developed by Madhu Sudan and others, and work continues on these types of decoders (see Guruswami–Sudan list decoding algorithm).\n\nIn 2002, another original scheme decoder was developed by Shuhong Gao, based on the extended Euclidean algorithm.[6]\n\nReed–Solomon coding is very widely used in mass storage systems to correct the burst errors associated with media defects.\n\nReed–Solomon coding is a key component of the compact disc. It was the first use of strong error correction coding in a mass-produced consumer product, and DAT and DVD use similar schemes. In the CD, two layers of Reed–Solomon coding separated by a 28-way convolutional interleaver yields a scheme called Cross-Interleaved Reed–Solomon Coding (CIRC). The first element of a CIRC decoder is a relatively weak inner (32,28) Reed–Solomon code, shortened from a (255,251) code with 8-bit symbols. This code can correct up to 2 byte errors per 32-byte block. More importantly, it flags as erasures any uncorrectable blocks, i.e., blocks with more than 2 byte errors. The decoded 28-byte blocks, with erasure indications, are then spread by the deinterleaver to different blocks of the (28,24) outer code. Thanks to the deinterleaving, an erased 28-byte block from the inner code becomes a single erased byte in each of 28 outer code blocks. The outer code easily corrects this, since it can handle up to 4 such erasures per block.\n\nThe result is a CIRC that can completely correct error bursts up to 4000 bits, or about 2.5 mm on the disc surface. This code is so strong that most CD playback errors are almost certainly caused by tracking errors that cause the laser to jump track, not by uncorrectable error bursts.[7]\n\nDVDs use a similar scheme, but with much larger blocks, a (208,192) inner code, and a (182,172) outer code.\n\nReed–Solomon error correction is also used in parchive files which are commonly posted accompanying multimedia files on USENET. The distributed online storage service Wuala (discontinued in 2015) also used Reed–Solomon when breaking up files.\n\nAlmost all two-dimensional bar codes such as PDF-417, MaxiCode, Datamatrix, QR Code, Aztec Code and Han Xin code use Reed–Solomon error correction to allow correct reading even if a portion of the bar code is damaged. When the bar code scanner cannot recognize a bar code symbol, it will treat it as an erasure.\n\nReed–Solomon coding is less common in one-dimensional bar codes, but is used by the PostBar symbology.\n\nSpecialized forms of Reed–Solomon codes, specifically Cauchy-RS and Vandermonde-RS, can be used to overcome the unreliable nature of data transmission over erasure channels. The encoding process assumes a code of RS(N, K) which results in N codewords of length N symbols each storing K symbols of data, being generated, that are then sent over an erasure channel.\n\nAny combination of K codewords received at the other end is enough to reconstruct all of the N codewords. The code rate is generally set to 1/2 unless the channel's erasure likelihood can be adequately modelled and is seen to be less. In conclusion, N is usually 2K, meaning that at least half of all the codewords sent must be received in order to reconstruct all of the codewords sent.\n\nReed–Solomon codes are also used in xDSL systems and CCSDS's Space Communications Protocol Specifications as a form of forward error correction.\n\nOne significant application of Reed–Solomon coding was to encode the digital pictures sent back by the Voyager program.\n\nVoyager introduced Reed–Solomon coding concatenated with convolutional codes, a practice that has since become very widespread in deep space and satellite (e.g., direct digital broadcasting) communications.\n\nViterbi decoders tend to produce errors in short bursts. Correcting these burst errors is a job best done by short or simplified Reed–Solomon codes.\n\nModern versions of concatenated Reed–Solomon/Viterbi-decoded convolutional coding were and are used on the Mars Pathfinder, Galileo, Mars Exploration Rover and Cassini missions, where they perform within about 1–1.5 dB of the ultimate limit, the Shannon capacity.\n\nThese concatenated codes are now being replaced by more powerful turbo codes:\n\nThe Reed–Solomon code is actually a family of codes, where every code is characterised by three parameters: an alphabet size , a block length , and a message length , with . The set of alphabet symbols is interpreted as the finite field of order , and thus, must be a prime power. In the most useful parameterizations of the Reed–Solomon code, the block length is usually some constant multiple of the message length, that is, the rate is some constant, and furthermore, the block length is either equal to the alphabet size or one less than it, i.e., or .[citation needed]\n\nThere are different encoding procedures for the Reed–Solomon code, and thus, there are different ways to describe the set of all codewords. In the original view of Reed & Solomon (1960), every codeword of the Reed–Solomon code is a sequence of function values of a polynomial of degree less than . In order to obtain a codeword of the Reed–Solomon code, the message symbols (each within the q-sized alphabet) are treated as the coefficients of a polynomial of degree less than , over the finite field with elements. In turn, the polynomial is evaluated at distinct points of the field , and the sequence of values is the corresponding codeword. Common choices for a set of evaluation points include , , or for , , ... , where is a primitive element of .\n\nFormally, the set of codewords of the Reed–Solomon code is defined as follows: {\\displaystyle \\mathbf {C} ={\\Bigl \\{}\\;{\\bigl (}p(a_{1}),p(a_{2}),\\dots ,p(a_{n}){\\bigr )}\\;{\\Big |}\\;p{\\text{ is a polynomial over }}F{\\text{ of degree }}<k\\;{\\Bigr \\}}\\,.} Since any two distinct polynomials of degree less than agree in at most points, this means that any two codewords of the Reed–Solomon code disagree in at least positions. Furthermore, there are two polynomials that do agree in points but are not equal, and thus, the distance of the Reed–Solomon code is exactly . Then the relative distance is , where is the rate. This trade-off between the relative distance and the rate is asymptotically optimal since, by the Singleton bound, every code satisfies . Being a code that achieves this optimal trade-off, the Reed–Solomon code belongs to the class of maximum distance separable codes.\n\nWhile the number of different polynomials of degree less than k and the number of different messages are both equal to , and thus every message can be uniquely mapped to such a polynomial, there are different ways of doing this encoding. The original construction of Reed & Solomon (1960) interprets the message x as the coefficients of the polynomial p, whereas subsequent constructions interpret the message as the values of the polynomial at the first k points and obtain the polynomial p by interpolating these values with a polynomial of degree less than k. The latter encoding procedure, while being slightly less efficient, has the advantage that it gives rise to a systematic code, that is, the original message is always contained as a subsequence of the codeword.\n\nIn the original construction of Reed & Solomon (1960), the message is mapped to the polynomial with The codeword of is obtained by evaluating at different points of the field . Thus the classical encoding function for the Reed–Solomon code is defined as follows: This function is a linear mapping, that is, it satisfies for the following -matrix with elements from :\n\nThis matrix is a Vandermonde matrix over . In other words, the Reed–Solomon code is a linear code, and in the classical encoding procedure, its generator matrix is .\n\nThere are alternative encoding procedures that produce a systematic Reed–Solomon code. One method uses Lagrange interpolation to compute polynomial such that Then is evaluated at the other points .\n\nThis function is a linear mapping. To generate the corresponding systematic encoding matrix G, multiply the Vandermonde matrix A by the inverse of A's left square submatrix.\n\nfor the following -matrix with elements from :\n\nA discrete Fourier transform is essentially the same as the encoding procedure; it uses the generator polynomial to map a set of evaluation points into the message values as shown above:\n\nThe inverse Fourier transform could be used to convert an error free set of n < q message values back into the encoding polynomial of k coefficients, with the constraint that in order for this to work, the set of evaluation points used to encode the message must be a set of increasing powers of α:\n\nHowever, Lagrange interpolation performs the same conversion without the constraint on the set of evaluation points or the requirement of an error free set of message values and is used for systematic encoding, and in one of the steps of the Gao decoder.\n\nIn this view, the message is interpreted as the coefficients of a polynomial . The sender computes a related polynomial of degree where and sends the polynomial . The polynomial is constructed by multiplying the message polynomial , which has degree , with a generator polynomial of degree that is known to both the sender and the receiver. The generator polynomial is defined as the polynomial whose roots are sequential powers of the Galois field primitive\n\nis a polynomial that has at least the roots {\\displaystyle \\mathbf {C} =\\left\\{\\left(s_{1},s_{2},\\dots ,s_{n}\\right)\\;{\\Big |}\\;s(a)=\\sum _{i=1}^{n}s_{i}a^{i}{\\text{ is a polynomial that has at least the roots }}\\alpha ^{1},\\alpha ^{2},\\dots ,\\alpha ^{n-k}\\right\\}.}\n\nThe encoding procedure for the BCH view of Reed–Solomon codes can be modified to yield a systematic encoding procedure, in which each codeword contains the message as a prefix, and simply appends error correcting symbols as a suffix. Here, instead of sending , the encoder constructs the transmitted polynomial such that the coefficients of the largest monomials are equal to the corresponding coefficients of , and the lower-order coefficients of are chosen exactly in such a way that becomes divisible by . Then the coefficients of are a subsequence of the coefficients of . To get a code that is overall systematic, we construct the message polynomial by interpreting the message as the sequence of its coefficients.\n\nFormally, the construction is done by multiplying by to make room for the check symbols, dividing that product by to find the remainder, and then compensating for that remainder by subtracting it. The check symbols are created by computing the remainder :\n\nThe remainder has degree at most , whereas the coefficients of in the polynomial are zero. Therefore, the following definition of the codeword has the property that the first coefficients are identical to the coefficients of :\n\nAs a result, the codewords are indeed elements of , that is, they are divisible by the generator polynomial :[10]\n\nThis function is a linear mapping. To generate the corresponding systematic encoding matrix G, set G's left square submatrix to the identity matrix and then encode each row:\n\nfor the following -matrix with elements from :\n\nThe Reed–Solomon code is a [n, k, n − k + 1] code; in other words, it is a linear block code of length n (over F) with dimension k and minimum Hamming distance The Reed–Solomon code is optimal in the sense that the minimum distance has the maximum value possible for a linear code of size (n, k); this is known as the Singleton bound. Such a code is also called a maximum distance separable (MDS) code.\n\nThe error-correcting ability of a Reed–Solomon code is determined by its minimum distance, or equivalently, by , the measure of redundancy in the block. If the locations of the error symbols are not known in advance, then a Reed–Solomon code can correct up to erroneous symbols, i.e., it can correct half as many errors as there are redundant symbols added to the block. Sometimes error locations are known in advance (e.g., \"side information\" in demodulator signal-to-noise ratios)—these are called erasures. A Reed–Solomon code (like any MDS code) is able to correct twice as many erasures as errors, and any combination of errors and erasures can be corrected as long as the relation 2E + S ≤ n − k is satisfied, where is the number of errors and is the number of erasures in the block.\n\nThe theoretical error bound can be described via the following formula for the AWGN channel for FSK:[11] and for other modulation schemes: where , , , is the symbol error rate in uncoded AWGN case and is the modulation order.\n\nFor practical uses of Reed–Solomon codes, it is common to use a finite field with elements. In this case, each symbol can be represented as an -bit value. The sender sends the data points as encoded blocks, and the number of symbols in the encoded block is . Thus a Reed–Solomon code operating on 8-bit symbols has symbols per block. (This is a very popular value because of the prevalence of byte-oriented computer systems.) The number , with , of data symbols in the block is a design parameter. A commonly used code encodes eight-bit data symbols plus 32 eight-bit parity symbols in an -symbol block; this is denoted as a code, and is capable of correcting up to 16 symbol errors per block.\n\nThe Reed–Solomon code properties discussed above make them especially well-suited to applications where errors occur in bursts. This is because it does not matter to the code how many bits in a symbol are in error — if multiple bits in a symbol are corrupted it only counts as a single error. Conversely, if a data stream is not characterized by error bursts or drop-outs but by random single bit errors, a Reed–Solomon code is usually a poor choice compared to a binary code.\n\nThe Reed–Solomon code, like the convolutional code, is a transparent code. This means that if the channel symbols have been inverted somewhere along the line, the decoders will still operate. The result will be the inversion of the original data. However, the Reed–Solomon code loses its transparency when the code is shortened (see 'Remarks' at the end of this section). The \"missing\" bits in a shortened code need to be filled by either zeros or ones, depending on whether the data is complemented or not. (To put it another way, if the symbols are inverted, then the zero-fill needs to be inverted to a one-fill.) For this reason it is mandatory that the sense of the data (i.e., true or complemented) be resolved before Reed–Solomon decoding.\n\nWhether the Reed–Solomon code is cyclic or not depends on subtle details of the construction. In the original view of Reed and Solomon, where the codewords are the values of a polynomial, one can choose the sequence of evaluation points in such a way as to make the code cyclic. In particular, if is a primitive root of the field , then by definition all non-zero elements of take the form for , where . Each polynomial over gives rise to a codeword . Since the function is also a polynomial of the same degree, this function gives rise to a codeword ; since holds, this codeword is the cyclic left-shift of the original codeword derived from . So choosing a sequence of primitive root powers as the evaluation points makes the original view Reed–Solomon code cyclic. Reed–Solomon codes in the BCH view are always cyclic because BCH codes are cyclic.\n\nDesigners are not required to use the \"natural\" sizes of Reed–Solomon code blocks. A technique known as \"shortening\" can produce a smaller code of any desired size from a larger code. For example, the widely used (255,223) code can be converted to a (160,128) code by padding the unused portion of the source block with 95 binary zeroes and not transmitting them. At the decoder, the same portion of the block is loaded locally with binary zeroes.\n\nThe QR code, Ver 3 (29×29) uses interleaved blocks. The message has 26 data bytes and is encoded using two Reed-Solomon code blocks. Each block is a (255,233) Reed Solomon code shortened to a (35,13) code.\n\nThe Delsarte–Goethals–Seidel[12] theorem illustrates an example of an application of shortened Reed–Solomon codes. In parallel to shortening, a technique known as puncturing allows omitting some of the encoded parity symbols.\n\nThe decoders described in this section use the BCH view of a codeword as a sequence of coefficients. They use a fixed generator polynomial known to both encoder and decoder.\n\nDaniel Gorenstein and Neal Zierler developed a decoder that was described in a MIT Lincoln Laboratory report by Zierler in January 1960 and later in a paper in June 1961.[13] The Gorenstein–Zierler decoder and the related work on BCH codes are described in a book Error Correcting Codes by W. Wesley Peterson (1961).[14]\n\nThe transmitted message, , is viewed as the coefficients of a polynomial\n\nAs a result of the Reed–Solomon encoding procedure, s(x) is divisible by the generator polynomial where α is a primitive element.\n\nSince s(x) is a multiple of the generator g(x), it follows that it \"inherits\" all its roots: Therefore,\n\nThe transmitted polynomial is corrupted in transit by an error polynomial to produce the received polynomial\n\nCoefficient e will be zero if there is no error at that power of x, and nonzero if there is an error. If there are ν errors at distinct powers i of x, then\n\nThe goal of the decoder is to find the number of errors (ν), the positions of the errors (i ), and the error values at those positions (e ). From those, e(x) can be calculated and subtracted from r(x) to get the originally sent message s(x).\n\nThe decoder starts by evaluating the polynomial as received at points . We call the results of that evaluation the \"syndromes\" S . They are defined as Note that because has roots at , as shown in the previous section.\n\nThe advantage of looking at the syndromes is that the message polynomial drops out. In other words, the syndromes only relate to the error and are unaffected by the actual contents of the message being transmitted. If the syndromes are all zero, the algorithm stops here and reports that the message was not corrupted in transit.\n\nFor convenience, define the error locators X and error values Y as\n\nThen the syndromes can be written in terms of these error locators and error values as\n\nThis definition of the syndrome values is equivalent to the previous since .\n\nThe syndromes give a system of n − k ≥ 2ν equations in 2ν unknowns, but that system of equations is nonlinear in the X and does not have an obvious solution. However, if the X were known (see below), then the syndrome equations provide a linear system of equations which can easily be solved for the Y error values.\n\nConsequently, the problem is finding the X , because then the leftmost matrix would be known, and both sides of the equation could be multiplied by its inverse, yielding Y\n\nIn the variant of this algorithm where the locations of the errors are already known (when it is being used as an erasure code), this is the end. The error locations (X ) are already known by some other method (for example, in an FM transmission, the sections where the bitstream was unclear or overcome with interference are probabilistically determinable from frequency analysis). In this scenario, up to errors can be corrected.\n\nThe rest of the algorithm serves to locate the errors and will require syndrome values up to , instead of just the used thus far. This is why twice as many error-correcting symbols need to be added as can be corrected without knowing their locations.\n\nThere is a linear recurrence relation that gives rise to a system of linear equations. Solving those equations identifies those error locations X .\n\nThe zeros of Λ(x) are the reciprocals . This follows from the above product notation construction, since if , then one of the multiplied terms will be zero, , making the whole polynomial evaluate to zero:\n\nLet be any integer such that . Multiply both sides by , and it will still be zero:\n\nSum for k = 1 to ν, and it will still be zero:\n\nCollect each term into its own sum:\n\nExtract the constant values of that are unaffected by the summation:\n\nThese summations are now equivalent to the syndrome values, which we know and can substitute in. This therefore reduces to\n\nRecall that j was chosen to be any integer between 1 and v inclusive, and this equivalence is true for all such values. Therefore, we have v linear equations, not just one. This system of linear equations can therefore be solved for the coefficients Λ of the error-location polynomial: The above assumes that the decoder knows the number of errors ν, but that number has not been determined yet. The PGZ decoder does not determine ν directly but rather searches for it by trying successive values. The decoder first assumes the largest value for a trial ν and sets up the linear system for that value. If the equations can be solved (i.e., the matrix determinant is nonzero), then that trial value is the number of errors. If the linear system cannot be solved, then the trial ν is reduced by one and the next smaller system is examined (Gill n.d., p. 35).\n\nUse the coefficients Λ found in the last step to build the error location polynomial. The roots of the error location polynomial can be found by exhaustive search. The error locators X are the reciprocals of those roots. The order of coefficients of the error location polynomial can be reversed, in which case the roots of that reversed polynomial are the error locators (not their reciprocals ). Chien search is an efficient implementation of this step.\n\nOnce the error locators X are known, the error values can be determined. This can be done by direct solution for Y in the error equations matrix given above, or using the Forney algorithm.\n\nCalculate i by taking the log base of X . This is generally done using a precomputed lookup table.\n\nFinally, e(x) is generated from i and e and then is subtracted from r(x) to get the originally sent message s(x), with errors corrected.\n\nConsider the Reed–Solomon code defined in GF(929) with α = 3 and t = 4 (this is used in PDF417 barcodes) for a RS(7,3) code. The generator polynomial is If the message polynomial is p(x) = 3 x2 + 2 x + 1, then a systematic codeword is encoded as follows: Errors in transmission might cause this to be received instead: The syndromes are calculated by evaluating r at powers of α: yielding the system\n\nUsing Gaussian elimination, so with roots x = 757 = 3−3 and x = 562 = 3−4. The coefficients can be reversed: to produce roots 27 = 33 and 81 = 34 with positive exponents, but typically this isn't used. The logarithm of the inverted roots corresponds to the error locations (right to left, location 0 is the last term in the codeword).\n\nTo calculate the error values, apply the Forney algorithm:\n\nSubtracting from the received polynomial r(x) reproduces the original codeword s.\n\nThe Berlekamp–Massey algorithm is an alternate iterative procedure for finding the error locator polynomial. During each iteration, it calculates a discrepancy based on a current instance of Λ(x) with an assumed number of errors e: and then adjusts Λ(x) and e so that a recalculated Δ would be zero. The article Berlekamp–Massey algorithm has a detailed description of the procedure. In the following example, C(x) is used to represent Λ(x).\n\nUsing the same data as the Peterson Gorenstein Zierler example above:\n\nThe final value of C is the error locator polynomial, Λ(x).\n\nAnother iterative method for calculating both the error locator polynomial and the error value polynomial is based on Sugiyama's adaptation of the extended Euclidean algorithm .\n\nDefine S(x), Λ(x), and Ω(x) for t syndromes and e errors:\n\nThe middle terms are zero due to the relationship between Λ and syndromes.\n\nThe extended Euclidean algorithm can find a series of polynomials of the form\n\nwhere the degree of R decreases as i increases. Once the degree of R (x) < t/2, then\n\nB(x) and Q(x) don't need to be saved, so the algorithm becomes:\n\nto set low order term of Λ(x) to 1, divide Λ(x) and Ω(x) by A (0):\n\nA (0) is the constant (low order) term of A .\n\nUsing the same data as the Peterson–Gorenstein–Zierler example above:\n\nA discrete Fourier transform can be used for decoding.[15] To avoid conflict with syndrome names, let c(x) = s(x) the encoded codeword. r(x) and e(x) are the same as above. Define C(x), E(x), and R(x) as the discrete Fourier transforms of c(x), e(x), and r(x). Since r(x) = c(x) + e(x), and since a discrete Fourier transform is a linear operator, R(x) = C(x) + E(x).\n\nTransform r(x) to R(x) using discrete Fourier transform. Since the calculation for a discrete Fourier transform is the same as the calculation for syndromes, t coefficients of R(x) and E(x) are the same as the syndromes:\n\nUse through as syndromes (they're the same) and generate the error locator polynomial using the methods from any of the above decoders.\n\nLet v = number of errors. Generate E(x) using the known coefficients to , the error locator polynomial, and these formulas\n\nThen calculate C(x) = R(x) − E(x) and take the inverse transform (polynomial interpolation) of C(x) to produce c(x).\n\nThe Singleton bound states that the minimum distance d of a linear block code of size (n,k) is upper-bounded by n − k + 1. The distance d was usually understood to limit the error-correction capability to ⌊(d−1) / 2⌋. The Reed–Solomon code achieves this bound with equality, and can thus correct up to ⌊(n−k) / 2⌋ errors. However, this error-correction bound is not exact.\n\nIn 1999, Madhu Sudan and Venkatesan Guruswami at MIT published \"Improved Decoding of Reed–Solomon and Algebraic-Geometry Codes\" introducing an algorithm that allowed for the correction of errors beyond half the minimum distance of the code.[16] It applies to Reed–Solomon codes and more generally to algebraic geometric codes. This algorithm produces a list of codewords (it is a list-decoding algorithm) and is based on interpolation and factorization of polynomials over and its extensions.\n\nIn 2023, building on three exciting works,[17][18][19] coding theorists showed that Reed-Solomon codes defined over random evaluation points can actually achieve list decoding capacity (up to n−k errors) over linear size alphabets with high probability. However, this result is combinatorial rather than algorithmic.\n\nThe algebraic decoding methods described above are hard-decision methods, which means that for every symbol a hard decision is made about its value. For example, a decoder could associate with each symbol an additional value corresponding to the channel demodulator's confidence in the correctness of the symbol. The advent of LDPC and turbo codes, which employ iterated soft-decision belief propagation decoding methods to achieve error-correction performance close to the theoretical limit, has spurred interest in applying soft-decision decoding to conventional algebraic codes. In 2003, Ralf Koetter and Alexander Vardy presented a polynomial-time soft-decision algebraic list-decoding algorithm for Reed–Solomon codes, which was based upon the work by Sudan and Guruswami.[20] In 2016, Steven J. Franke and Joseph H. Taylor published a novel soft-decision decoder.[21]\n\nHere we present a simple MATLAB implementation for an encoder.\n\nThe decoders described in this section use the Reed Solomon original view of a codeword as a sequence of polynomial values where the polynomial is based on the message to be encoded. The same set of fixed values are used by the encoder and decoder, and the decoder recovers the encoding polynomial (and optionally an error locating polynomial) from the received message.\n\nReed & Solomon (1960) described a theoretical decoder that corrected errors by finding the most popular message polynomial. The decoder only knows the set of values to and which encoding method was used to generate the codeword's sequence of values. The original message, the polynomial, and any errors are unknown. A decoding procedure could use a method like Lagrange interpolation on various subsets of n codeword values taken k at a time to repeatedly produce potential polynomials, until a sufficient number of matching polynomials are produced to reasonably eliminate any errors in the received codeword. Once a polynomial is determined, then any errors in the codeword can be corrected, by recalculating the corresponding codeword values. Unfortunately, in all but the simplest of cases, there are too many subsets, so the algorithm is impractical. The number of subsets is the binomial coefficient, , and the number of subsets is infeasible for even modest codes. For a code that can correct 3 errors, the naïve theoretical decoder would examine 359 billion subsets.\n\nIn 1986, a decoder known as the Berlekamp–Welch algorithm was developed as a decoder that is able to recover the original message polynomial as well as an error \"locator\" polynomial that produces zeroes for the input values that correspond to errors, with time complexity , where is the number of values in a message. The recovered polynomial is then used to recover (recalculate as needed) the original message.\n\nUsing RS(7,3), GF(929), and the set of evaluation points a = i − 1\n\nIf the message polynomial is\n\nErrors in transmission might cause this to be received instead.\n\nAssume maximum number of errors: e = 2. The key equations become:\n\nRecalculate P(x) where E(x) = 0 : {2, 3} to correct b resulting in the corrected codeword:\n\nIn 2002, an improved decoder was developed by Shuhong Gao, based on the extended Euclid algorithm.[6]\n\nUsing the same data as the Berlekamp Welch example above:\n• Lagrange interpolation of for i = 1 to n\n\ndivide Q(x) and E(x) by most significant coefficient of E(x) = 708. (Optional)\n\nRecalculate P(x) where E(x) = 0 : {2, 3} to correct b resulting in the corrected codeword:\n• None Gill, John (n.d.), EE387 Notes #7, Handout #28 , Stanford University, archived from the original on June 30, 2014\n• None Peterson, Wesley W. (1960), \"Encoding and Error Correction Procedures for the Bose-Chaudhuri Codes\", IRE Transactions on Information Theory, IT-6 (4): 470, doi:10.1109/TIT.1960.1057586\n• None Reed, Irving S.; Solomon, Gustave (1960), \"Polynomial Codes over Certain Finite Fields\" , Journal of the Society for Industrial and Applied Mathematics, 8 (2): 304, doi:10.1137/0108018\n• None Koetter, Ralf (2005), Reed–Solomon Codes, MIT Lecture Notes 6.451 (Video), archived from the original on 2013-03-13\n• A Tutorial on Reed–Solomon Coding for Fault-Tolerance in RAID-like Systems\n• FEC library in C by Phil Karn (aka KA9Q) includes Reed–Solomon codec, both arbitrary and optimized (223,255) version"
    },
    {
        "link": "https://tomverbeure.github.io/2022/08/07/Reed-Solomon.html",
        "document": "\n• Some Often Used Terminology\n\nI’ve always been intimidated by coding techniques: encryption and decryption, hashing operations, error correction codes, and even compression and decompression techniques. It’s not that I didn’t know what they do, but I often felt that I never quite understood the basics, let alone have an intuitive understanding of how they worked.\n\nReed-Solomon forward error correction (FEC) is one such coding method. Until the discovery of better coding techniques such as turbo and low-density parity check codes (LDPC), it was one of the most powerful ways to make data storage or data transmission resilient against corruption: the Voyager spacecrafts used Reed-Solomon coding to transmit images when it was between Saturn and Uranus, and CDs can recover from scratches that corrupt up to 4000 bits thanks to the clever use of not one but two Reed-Solomon codes.\n\nThe subject is covered in many college-level courses on coding and signal processing techniques, but a lot of the material online is theoretical, math heavy, and not intuitive. At least not for me…\n\nThat changed when I found this Introduction to Reed-Solomon article. It explains how polynomials and polynomial evaluation at certain points are a way to create a code with redundancy, and how to recover the original message back from it. The article is excellent, and it makes some of what I’m covering below unnecessary or redundant (ha!), because parts of what follows will be a recreation of that material. However my take on it has dumbed things down even more yet also covers a larger variety of Reed-Solomon codes.\n\nOne of the best aspects of that article is the focus on integer math. Academic literature about coding theory almost always starts with the theory of finite fields, also known as Galois fields, and then build on that when explaining coding algorithms. I found this one of the bigger obstacles in understanding the fundamentals of Reed-Solomon codes (and BCH codes, a close relative) because instead of getting to know one new topic, you now have to tackle two at the same time.\n\nFor this reason, everything in this blog post will use integer math as well. Integer math makes Reed-Solomon codes impractical for real world use, but it results in a better understanding about the fundamentals, before stepping up to the next level with the introduction of finite field math.\n\nMany books are written about coding theory and applications, and a single blog post won’t even scratch the surface of the topic. I’m also still only beginning to understand some of the finer points. So my usual disclaimer applies:\n\nI’m writing these blog posts primarily for myself, as a way to solidify what I’ve learned after reading stuff on the web. Major errors are possible, inaccuracies should be expected.\n\nYou’ll find some mathematical formulas below, but I always try to switch to real examples as soon as possible.\n\nIt’s impossible to discuss anything that’s related to coding without touching the subject of polynomials. You’ve probably learned about integer based polynomials during algebra classes in high school or in college but here’s a quick recap.\n\nA polynomial \\(f(x)\\) of degree \\(n\\) is a function that looks like this:\n\n\\(n+1\\) fixed coefficients \\(c_i\\) are multiplied by function variable \\(x\\) to the power of \\(i\\) and added together. The polynomial can be evaluated by replacing variable \\(x\\) by some number for which you want to know the value of the function.\n\nYou can add, subtract, multiply or divide polynomials with each other.\n\nLet’s illustrate this with some examples, and define \\(f(x)\\) and \\(g(x)\\) as follows:\n\nAddition and subtraction work by adding or subtracting together the coefficients that belong to the same \\(x^i\\):\n\nAnd you can divide them, using long division\n\nIn the examples above, the coefficients \\(c_i\\), and function variable \\(x\\) are regular integers, but polynomials can be used for any mathematical system that has the concept of addition and multiplication.\n\nHere’s one the most important characteristics of polynomials:\n\nAny polynomial function of degree \\(n-1\\) is uniquely defined by any \\(n\\) points that lay on this function.\n\nIn other words, when I give you the value \\(f(x) = c_0 + c_1 x + c_2 x^2 + c_3 x^3\\) for any \\(n\\) distinct values of \\(x\\), you can derive the \\(n\\) coefficients \\(c_0\\) to \\(c_{n-1}\\), and these values \\(c_i\\) will be the same no matter which \\(n\\) values of \\(x\\) I used.\n\nHere’s an example of a polynomial of degree 3, \\(f(x)=2 + 3x -5x^2 + x^3\\), evaluated for \\(x\\) values of \\(-1,0,1,2\\):\n\nI used the excellent desmos graphing calculator to create this function plot.\n\nWhen given the points \\((-1, -7), (0,2), (1, 1), (2, -4)\\), we can fill in the values into \\(f(x) = c_0 + c_1 x + c_2 x^2 + c_3 x^3\\) for each of point, which results in the following set of 4 linear equations with 4 unknowns:\n\nThis can be solved with the well-known Gaussian elimination algorithm. In our case, the values are such that we can solve it using a less systematic method:\n\nThe second row immediately reduces to \\(\\bf{c_0 = 2}\\). When we fill that back in, the remaining rows become:\n\nIf we add the first and second row, we get: \\(2c_2 = -10\\), or \\(\\bf{c_2=-5}\\), which simplifies the 3 rows to:\n\nAfter substituting the second equation in the third one, we get:\n\nwhich reduces to \\(-6 c_1 = -18\\) or \\(\\bf{c_1 = 3}\\).\n\nAnd if we fill that into \\(c_1 + c_3 = 4\\), we get \\(\\bf{c_3 = -1}\\).\n\nConclusion: \\((c_0, c_1, c_2, c_3) = (2, 3, -5, 1)\\). These coefficients match those of function \\(f(x)\\) that we started with!\n\nWe can do the same exercise with points \\((1, 1), (2, -4), (3, -7), (4, -2)\\), and we’ll end up with the same result: \\((c_0, c_1, c_2, c_3) = (2, 3, -5, 1)\\). This is left as an exercise for the reader.\n\nNote that Gaussian elimination isn’t the most efficient way to come up with \\(f(x)\\), since it requires a number of operations that is \\(O(n^3)\\). Construction of a Lagrange polynomial works just as well and that’s only a \\(O(n^2)\\) operations.\n\nSome Often Used Terminology\n\nThere is no rigid standard about which kind of letters or symbols to use when discussing coding theory, but there’s a least some commonality between different texts.\n• A symbol is the smallest piece of information. In many coding methods, a symbol is a single bit, with only two possible values. Reed-Solomon coding, however, uses symbols that have more than 2 values.\n• A message is the original piece of information that need to be encoded. A message is a sequence of symbols. For example, a message could be “Hello world “, where each character is a symbol.\n• A message word is a fixed length section of the overall message. Reed-Solomon coding is a block coding algorithm where a message is split up into multiple message words, and the encoding algorithm is performed on each message word without any dependency on previously received message words. Message words are usually indicated with the letter \\(m\\), and \\(k\\) is often used to indicate the number of symbols in the message word. In other words, \\(k\\) is the size or the length of the message word. For example, if a message word is defined as having a size 4, our previous message would be split into the following message words: (‘H’, ‘e’, ‘l’, ‘l’), (‘o’, ‘ ‘, ‘w’, ‘o’), (‘r’, ‘l’, ‘d’, ‘ ‘).\n• An alphabet is the full set of values that can be assigned to a symbol. \\(q\\) is often used as the number of values in an alphabet. When a symbol is a single bit, the alphabet consists of 2 values, 0 and 1. If we had a system where messages only consist of upper case and lower case letters and a space, then an alphabet could be (‘A’, …, ‘Z’, ‘a’, ‘b’, ‘c’, … , ‘z’, ‘ ‘), and the size of the alphabet would be 53. Almost all coding algorithms require the ability to perform mathematical operations such as addition, subtraction, multiplication and division on the symbols of a word, so don’t expect to see this kind of alphabet in the real world! In practice, the alphabet of most coding algorithms is a sequence of binary digits. 8 bits is common, but it doesn’t have to be. I hesitate to call it an 8-bit number, because that would suggest that regular integer math can be used on it, and that’s almost never the case. Instead, the operations of such an alphabet use the rules of a Galois field.\n• A code word is what you get after you run a message word through an encoder. In the case of a Reed-Solomon encoder, a code word has a length \\(n\\) symbols, where \\(n>k\\) and the symbols are using the same alphabet as the message word. Code words are often indicated with a vector \\(s = (s_0, s_1, ... , s_{n-1})\\). When our message of three 4-symbol message words is converted into three 6-symbol code words, it could have the following code words: Notice how the first 4 symbols of each code word are the same as their corresponding message word, but that 2 symbols are added for error detection and correction. This makes it a systematic code. As we’ll soon see, not all coding schemes are systematic in nature, but it’s a nice property to have. For Reed-Solomon codes, the length \\(n\\) of a code word must be smaller or equal than the number of values in the alphabet that it uses.\n\nIn all the examples below, the message words will have a length of 4, code words will have a length 6, and the symbols are using an alphabet of regular, unlimited range integers.\n\nIn other words: \\(k=4\\), \\(n=6\\), and the size \\(q\\) of the alphabet is infinite.\n\nUsing integers as symbols is a terrible choice: there are no real-world, practical Reed-Solomon implementations that use them. But as stated in the introduction, integer symbols allow us to focus on just the fundamentals of Reed-Solomon coding without getting distracted by the specifics of Galois field math.\n\nIf we’d want to send the “Hello world” message using an alphabet of integers, one possibility is to just convert each letter to their corresponding integer. For example, “Hello world “ would be converted to \\((7, 30, 37, 37, 40, ...)\\), because ‘A’ is assigned a value of 0, ‘B’ a value of 1, ‘a’ a value of 26, and so forth.\n\nReed-Solomon codes were introduced to the world by Irving S. Reed and Gustave Solomon with a paper with the unassuming title “Polynomial Codes over Certain Finite Fields.” The 4-page paper can be purchased for the low price of $36.75. You should definitely not use Google to find one of the many copies for free.\n\nOnce you understand how Reed-Solomon coding and Galois fields work, the paper is quite readable, by today’s standards at least, and light on math too. Let’s get to business and explain how things work so that you can read the paper as well.\n\nIn an earlier section, we saw that we can set up a third degree polynomial with 4 numbers: either the 4 coefficients \\((c_0, c_1, c_2, c_3)\\), or 4 points \\(f(x)\\) on the polynomial for some given values of \\(x\\).\n\nOnce we know the coefficients, we can evaluate the polynomial at more than 4 values of \\(x\\). Those additional points are not required to specify the polynomial, they are redundant, which is exactly what we’re looking for: redundant information that allows us find the original values in case a code word gets corrupted during transmission!\n\nAnd that’s what the original way of Reed-Solomon coding is all about:\n\nCreating redundant information by evaluating a polynomial at more values of \\(x\\) than is strictly necessary.\n\nLet’s go through a concrete example: I want to setup a communication protocol to transmit information that consists of a sequence of numbers, but I also want to add redundancy so that the message still can be recovered after a corruption.\n\nHere’s one way to go about it:\n\nThe encoding and decoding parties must come up with some fixed parameters of the coding protocol that are not message dependent:\n• Agree on the length \\(k\\) of the message word.\n• Agree on the length \\(n\\) of the code word. The longer the difference in length between code word and message word, the more redundancy, and the more corrupted symbols can be error corrected.\n• Agree on how some polynomial \\(p(x)\\) should be constructed out of the symbols of the message word. In the original paper the message word symbols are used as coefficients of this polynomial, but that’s not always the best choice. See later sections…\n• Agree on values of \\(x\\) for which to evaluate the polynomial \\(p(x)\\).\n• Agree that the code word is formed by evaluating \\(p(x)\\) at the numbers of \\(x\\) that were agreed on in the previous step.\n\nLet’s apply this to an example with the following protocol settings:\n• The size \\(k\\) of the message word \\(m\\) is 4.\n• We want to 2 redundant symbols, so the code word has length \\(n\\) of 6.\n• The polynomial \\(p(x)\\) is always evaluated for the following values of \\(x\\): \\((-1, 0, 1, 2, 3, 4)\\). Note that there are as many values \\(x_i\\) as there are symbols in a code word.\n\nHere’s what happens to a message word with a value of \\((2, 3, -5, 1)\\):\n• Create polynomial \\(p(x)=2 + 3x -5x^2 + x^3\\). It uses the message word symbols as coefficients. I’m reusing here the polynomial that I used for the example in a previous section.\n• Evaluate the polynomial \\(p(x)\\) at the 6 locations of \\(x\\): \\((-1, 7), (0,2), (1, 1), (2, -4), (3, -7), (4, -2)\\).\n\nMeanwhile, the decoder does the following:\n• Get the 6 symbols of the code word. If there was no corruption, that’s still \\((7,2,1,-4,-7,-2)\\).\n• Take any 4 of the 6 received symbols, and link them to their corresponding \\(x\\) value. If we take the first 4 code word symbols, we get \\((-1, 7), (0,2), (1, 1), (2, -4)\\).\n• Use these 4 points to derive the coefficients of the polynomial \\(p(x)\\) that was used by the transmitter. We already saw how Gaussian elimination with the points \\((-1, 7), (0,2), (1, 1), (2, -4)\\) results in coefficients \\((2, 3, -5, 1)\\). These coefficients are the symbols of the original message word!\n\nThe decoder picked 4 of the 6 code word symbols to recover the coefficients, and ignored the 2 others, so what was the point of sending those 2 extra symbols? A real decoder will be smarter and use those 2 additional symbols to either check the integrity of the received message, or to correct corrupted values.\n\nBefore decoding, let’s first talk about how many redundant symbols are needed to correct to correct errors:\n\nTo correct up to \\(s\\) symbol errors, you need at least \\(2s\\) redundant symbols.\n\nIn the example above, 2 additional symbols were added, which allows us to correct 1 corrupt symbol.\n\nThe original Reed-Solomon paper proposes the following algorithm:\n• From \\(n\\) the received symbols of the code word, go through all combinations of \\(k\\) out of \\(n\\) symbols.\n• For each such combination, calculate the coefficients of the polynomial.\n• Count how many times each unique set of coefficients occurs.\n• If there were \\(s\\) corruptions or less, the set of coefficients with the highest count will be coefficients of the polynomial that was used by the encoder.\n\nHere’s how that works out for our example:\n• The decoder received the following code word: \\((7, 2, 6, -4, -7, -2)\\). Notice how the third symbol isn’t 1 but 6. There was a corruption!\n• Associate each symbol with its corresponding \\(x\\) value: \\((-1, 7), (0,2), (1, 6), (2, -4), (3, -7), (4, -2)\\).\n• From these 6 coordinates, draw all combinations of 4 elements: \\[(-1, 7),(0, 2),(1, 6),(2, -4)\\] \\[(-1, 7),(0, 2),(1, 6),(3, -7)\\] \\[(-1, 7),(0, 2),(1, 6),(4, -2)\\] \\[(-1, 7),(0, 2),(2, -4),(3, -7)\\] \\[(-1, 7),(0, 2),(2, -4),(4, -2)\\] \\[(-1, 7),(0, 2),(3, -7),(4, -2)\\] \\[(-1, 7),(1, 6),(2, -4),(3, -7)\\] \\[(-1, 7),(1, 6),(2, -4),(4, -2)\\] \\[(-1, 7),(1, 6),(3, -7),(4, -2)\\] \\[(-1, 7),(2, -4),(3, -7),(4, -2)\\] \\[(0, 2),(1, 6),(2, -4),(3, -7)\\] \\[(0, 2),(1, 6),(2, -4),(4, -2)\\] \\[(0, 2),(1, 6),(3, -7),(4, -2)\\] \\[(0, 2),(2, -4),(3, -7),(4, -2)\\] \\[(1, 6),(2, -4),(3, -7),(4, -2)\\]\n• For each of these 15 combinations, determine the 4 polynomial coefficients: \\[(-1, 7),(0, 2),(1, 6),(2, -4) \\rightarrow (2, 8, -5/2, -3/2)\\] \\[(-1, 7),(0, 2),(1, 6),(3, -7) \\rightarrow (2, 27/4, -5/2, -1/4)\\] \\[(-1, 7),(0, 2),(1, 6),(4, -2) \\rightarrow (2, 19/3, -5/2, 1/6)\\] \\[(-1, 7),(0, 2),(2, -4),(3, -7) \\rightarrow (2, 3, -5, 1)\\] \\[(-1, 7),(0, 2),(2, -4),(4, -2) \\rightarrow (2, 3, -5, 1)\\] \\[(-1, 7),(0, 2),(3, -7),(4, -2) \\rightarrow (2, 3, -5, 1)\\] \\[(-1, 7),(1, 6),(2, -4),(3, -7) \\rightarrow (19/2, 17/4, -10, 9/4)\\] \\[(-1, 7),(1, 6),(2, -4),(4, -2) \\rightarrow (26/3, 14/3, -55/6, 11/6)\\] \\[(-1, 7),(1, 6),(3, -7),(4, -2) \\rightarrow (7, 61/12, -15/2, 17/12)\\] \\[(-1, 7),(2, -4),(3, -7),(4, -2) \\rightarrow (2, 3, -5, 1)\\] \\[(0, 2),(1, 6),(2, -4),(3, -7) \\rightarrow (2, 18, -35/2, 7/2)\\] \\[(0, 2),(1, 6),(2, -4),(4, -2) \\rightarrow (2, 49/3, -15, 8/3)\\] \\[(0, 2),(1, 6),(3, -7),(4, -2) \\rightarrow (2, 13, -65/6, 11/6)\\] \\[(0, 2),(2, -4),(3, -7),(4, -2) \\rightarrow (2, 3, -5, 1)\\] \\[(1, 6),(2, -4),(3, -7),(4, -2) \\rightarrow (2, 3, -5, 1)\\]\n• The coefficients \\((2,3,-5,1)\\) come up 6 times. All other solutions are different from each other, so \\((2,3,-5,1)\\) is the winner. This is also the correct solution.\n\nThis may be a straightforward error correcting algorithm, but it’s not a usable one: even for this toy example with a message word of size 4 and only 2 additional redudant symbols, we need to determine the polynomial coefficients 15 times.\n\nIn the real world, a popular configuration is to have a message word with 223, and a code word with 255 symbols.\n\nThe formula to calculate the number of combinations is:\n\nWith \\(r=223\\) and \\(n=255\\), this gives a total of 50,964,019,775,576,912,153,703,782,274,307,996,667,625 combinations!\n\nThat’s just not very practical…\n\nIt’s clear that a much better algorithm is required to make Reed-Solomon decoding useful. Luckily, these algorithms exist, and some of them aren’t even that complicated, but that’s worthy of a separate blog post.\n• The symbols of the message word are used as coefficients of a polynomial \\(p(x)\\).\n• The polynomial \\(p(x)\\) is evaluated for \\(n\\) fixed values of \\(x\\).\n• The code word is the \\(n\\) values of \\(p(x)\\).\n\nThis encoding system works fine, but note how all symbols of the code word are different from the symbols of the message word. In our example, message word \\((2,3,-5,1)\\) was encoded as \\((7, 2, 6, -4, -7, -2)\\).\n\nWouldn’t it be nice if we could encode our message word so that the original symbols are part of the code word, with some additional symbols tacked on for redunancy? In other words, encode \\((2,3,-5,1)\\) into something like \\((2,3,-5,1,r_4, r_5)\\). I already mentioned earlier that such a code is called a systematic code.\n\nOne of the benefits of a systematic code is that you don’t need to run a decoder at all if there is some way to show early on that the code word was received without any error.\n\nIt’s easy to modify the original encoding scheme into one that is systematic by treating the message word symbols as the result of evaluating a \\(p(x)\\) polynomial.\n• The message word symbols \\((m_0, m_1, ..., m_{k-1})\\) are the values of the polynomial \\(p(x)\\) at certain agreed upon values of \\(x\\).\n• Determine the coefficients of this polynomial \\(p(x)\\) from the \\((x_i, m_i)\\) pairs.\n• Evaluate polynomial \\(p(x)\\) for an additional \\((n-k)\\) values of \\(x\\) to create redundant symbols.\n• The code word consists of the message word followed by the redundant symbols.\n\nLet’s try this out with the same number sequence as before:\n• The message word is still \\((2,3,-5,1)\\). These are now considered the result of evaluating polynomial \\(p(x)\\) for the corresponding values \\((-1, 0, 1, 2)\\) of \\(x\\).\n• Construct the polynomial \\(p(x)\\) out of these coordinate pairs: \\(((-1, 2), (0, 3), (1, -5), (2, 1))\\). I found this website to do that for me. It uses the Lagrange interpolation method, but Gaussian elimination would have given the same polynomial. Note how some of the coefficients of \\(p(x)\\) are rational numbers instead of integers. This is one of the reasons why integers shouldn’t be used for Reed-Solomon coding!\n• Evaluating \\(p(x)\\) for \\(x\\) values of 3 and 4 gives 44 and 147.\n\nThe decoder is exactly the same as before!\n\nIn the two encoding methods above, the code word consists of evaluated values of some polynomial \\(p(x)\\). There is yet another Reed-Solomon coding variant where the code word consists of the coefficients of a polynomial. And since this variant is the most popular, we have to cover it too. To understand how it works, there’s a bit more math involved, but an example later should make it all clear.\n\nA message word has \\(k\\) symbols, and we need \\(n\\) symbols to form a code word. If a code word consists of coefficients of a polynomial, we need to create a polynomial \\(s(x)\\) of degree \\((n-1)\\).\n\nHere are some desirable properties for \\(s(x)\\):\n• \\(k\\) of its coefficients should be the same as the symbols of the message word to create a systematic code.\n• When \\(s(x)\\) is evaluated at \\((n-k)\\) values of \\(x\\), the result should be 0. We’ll soon see why that’s useful.\n\nHere’s a way to create a polynomial with these properties:\n• Create a polynomial \\(p(x)\\) with symbols \\((m_0, m_1, ..., m_{k-1})\\) as coefficients, just like before.\n• As before, the values \\((x_0, x_1, ..., x_{n-k-1})\\) are fixed parameters of the protocol and agreed upon between encoder and decoder up front. However, this time there are only are \\((n-k)\\) values of \\(x_i\\), as many as there are redundant symbols. \\(g(x)\\) expands to \\(g_0 + g_1 x + ... + g_{n-k}x^{n-k}\\) and has a degree of \\((n-k)\\). Coefficient \\(g_{n-k}\\) will always be 1 due to the way \\(g(x)\\) is constructed.\n• Perform a polynomial division of \\(\\frac{p(x)x^{n-k}}{g(x)}\\), such that \\(p(x)x^{n-k} = q(x)g(x) + r(x)\\). In other words, \\(q(x)\\) is the quotient of the divison, and \\(r(x)\\) is the remainder.\n\nLet’s see what this gets us:\n• \\(p(x)\\) has degree \\((k-1)\\). When multiplied by \\(x^{n-k}\\), that results in a polynomial \\(m_{0}x^{n-k} + m_{1}x^{n-k+1} + ... + m_{k-1}x^{n-1}\\). The degree of \\(p(x)x^{n-k}\\) is \\((n-1)\\), but its first non-zero coefficient is the one for \\(x^{n-k}\\).\n• \\(r(x)\\) is the remainder of the division of polynomial \\(p(x)x^{n-k}\\), degree \\((n-1)\\), and \\(g(x)\\), degree \\((n-k)\\). The remainder of a polynomial division has a degree that is at most the degree of the divisor, \\(g(x)\\), minus 1: \\((n-k-1)\\). And thus, \\(r(x) = r_0 + r_1 + ... + r_{n-k-1}\\).\n• There is no overlap in coefficients for \\(x^i\\) between \\(p(x)x^{n-k}\\) and \\(r(x)\\): \\(r(x)\\) goes from 0 to \\((n-k-1)\\) and \\(p(x)x^{n-k}\\) goes from \\((n-k)\\) to \\((n-1)\\). So if we subtract \\(r(x)\\) from \\(p(x)x^{n-k}\\), the coefficients of the 2 sides don’t interact. As a result, the coefficients of \\(s(x)\\) for terms \\(x^{n-k}\\) to \\(x^{n-1}\\) are the same as coefficients \\(m_0..m_{k-1}\\) of the message word. This satisfies the first desirable property.\n• By definition, \\(p(x)x^{n-k} = q(x)g(x) + r(x)\\), so after substitution: \\(s(x) = q(x)g(x) + r(x) - r(x)\\) and \\(s(x) = q(x)g(x)\\).\n• By definition, \\(g(x)= (x-x_0)(x-x_1)...(x-x_{n-k-1})\\). Another substitution gives: \\(s(x) = q(x)(x-x_0)(x-x_1)...(x-x_{n-k-1})\\).\n• \\(x_i\\) are roots of \\(g(x)\\), so fill in any value \\(x_i\\) and \\(s(x_i)\\) evaluates to 0!\n\nWhat can we do with these properties? For that, we need to look at the decoder.\n\nThe decoder will use the code word symbols as coefficients of a polynomial \\(s'(x)\\). Without corruption, \\(s'(x)\\) is equal to \\(s(x)\\). Due to property 2, the decoder can verify this by filling in \\(x_i\\) into \\(s'(x)\\) and checking that the result is 0.\n\nEach value \\(s'(x_i)\\) is called a syndrome of the received code word, so vector \\((s'(x_0), s'(x_1), ..., s'(x_{n-k-1}))\\) contains all the syndromes.\n\nIf a syndrome is not equal to 0, we know that the received polynomial \\(s'(x)\\) is not the same as the transmitted polynomial, and we can define an error polynomial \\(e(x)\\) so that \\(s'(x) = s(x) + e(x)\\). Polynomial \\(e(x)\\) has the same degree \\((n-1)\\) as \\(s(x)\\).\n\nSince \\(s(x_i) = 0\\), it follows that \\(e(x_i) = s'(x_i)\\). There are \\((n-k)\\) values \\(x_i\\) and thus there are \\((n-k)\\) such equations. Expanded, the set of equations looks like this:\n\nIf we can find a way to derive the \\(n\\) coefficients of \\(e(x)\\) out of the \\((n-k)\\) equations, we can derive \\(s(x)\\) as \\(s'(x)-e(x)\\). This is not generally possible: there are more unknowns than there are equations. But it is possible when the number of corrupted coefficients is half or less than \\(n-k\\), just like it was for the other coding variant.\n\nA simple way to figure this out is again by going through all combinations and solving a set of equations.\n\nDeriving an efficient general error correction procedure is complicated and outside of the scope of this blog post, but I’ll show a practical example where there’s only 1 corrupted but unknown symbol, just like the examples above.\n\nLet’s once again start with message word \\((2,3,-5,1)\\) and go through the encoding and decoding steps.\n• Let’s use \\(x_0 = 1\\) and \\(x_1 = 2\\) as roots of the generator polynomial: \\(g(x) = (x-1)(x-2) = 2 -3x + x^2\\).\n• Divide \\(p(x)x^2\\) by \\(g(x)\\). WolframAlpha is perfect for this! It returns: \\(x^5 - 5 x^4 + 3 x^3 + 2 x^2 = (x^3 - 2 x^2 - 5 x - 9)_{q(x)}(x^2 - 3 x + 2)_{g(x)} + (18 - 17 x)_{r(x)}\\). We’re only interested in the last part, the remainder \\(r(x) = (18 - 17 x)\\).\n• As a verification step, you can fill in the values of 1 and 2 in \\(s(x)\\). They will evaluate to 0, as they should. Here is a way to do that with WolframAlpha…\n\nLet’s now do a decoding step when there’s was a corruption.\n• The received code word is \\((2, 3, -4, 1, -18, 17)\\). The third symbol has been changed from -5 to -4.\n• The received polynomial \\(s'(x)\\) is thus \\(s'(x) = (2 x^2 + 3 x^3 -4 x^4 + x^5 ) - (18 - 17 x)\\).\n• Fill in the roots of \\(g(x)\\), 1 and 2, into \\(s'(x)\\) to find the 2 syndromes: \\(s'(1)=1\\) and \\(s'(2)=16\\).\n• The syndromes are not 0, there was a corruption!\n• There are 2 redundant symbols, so the maximum number of corrupted symbols we can recover is 1. The most straightforward way to figure out which symbol was corrupted is to go through all 6 possibilities and see if we get a consistent equation. Like this:\n• Let’s assume the first coefficient \\(s_0\\) is wrong, and all the others are right. In that case, \\(s_0\\) is an unknown, and all other coefficients are known: \\(s(x) = s_0 x^2 + 3 x^3 -4 x^4 + x^5 - 18 + 17x\\). If we fill in \\(x=1\\), we get: \\(s_0 +3 -4 +1 -18 +17 = 0 \\rightarrow s_0 = -1\\). We have contradicting values for \\(s_0\\), so we can conclude that \\(s_0\\) was not a corrupted coefficient.\n• We can do the same for all other coefficients. For all of them, you’ll get conflicting values, except for \\(s_2\\):\n• The received code word has been corrected to \\((2, 3, -5, 1, -18, 17)\\). The message word is the first 4 symbols of the code word.\n\nReed-Solomon encoding with coefficients as code word is the method that you’ll most often find in the wild, and we just saw that it uses polynomial division.\n\nHow can we implement that in hardware?\n• Start with the dividend as initial remainder.\n• Subtract a multiple of the divisor from the remainder so that its highest power of \\(x\\) becomes zero. The multiple becomes a part of the quotient. What remains after the subtraction becomes the new remainder.\n• Repeat the previous step until the highest non-zero coefficient of the remainder is for a power of \\(x\\) that is lower than the divisor.\n\nThis is exaclty what you do when performing a long division…\n\nHere’s an example of dividing \\((-2 x^7 + 3 x^5 - x^4 + 10 x^3 -4 x^2)/(x^3-3x^2+2x-1)\\), performed in a spreadsheet:\n\nThe quotient \\(q(x)\\) is \\(-2 x^4 -3 x^3 -6 x^2 -4 x -7\\) and the remainder \\(r(x)\\) is \\(-19 x^2 + 10 x -7\\).\n\nMarked in blue is the initial remainder, which is identical to the dividend. Since the highest coefficient of the divisor is \\(1\\), the multiple by which the divisor gets adjusted is equal to the highest order coefficient of the remainder every step of the way.\n\nHowever, the steps above require that the dividend is fully known at the start of the whole operation. In a real Reed-Solomon encoder, the dividend, \\(p(x)x^{n-k}\\) can have a lot of coefficients. For example, in the Voyager program, \\(p(x)\\) has 223 coefficients. It’d be great if we could rework the division so that we can perform it sequentially without the need to know the full dividend at the start.\n\nWe can easily do that:\n\nIn the modified version above, instead of starting with a remainder that has all the terms of the dividend added to it, the coefficients of the dividend are added step by step, right when they’re needed to determine the next divisor multiplier. The result of the division is obviously still the same.\n\nAlso notice that the remainder, marked in green, has never more than 3 non-zero coefficients.\n\nWhen performed sequentially, the divider above can be implemented with the following logic:\n• At the bottom left, we have an input with the dividend \\(p(x)\\).\n• On the right, there’s the output, which can either be the quotient \\(q(x)\\) or the remainder \\(r(x)\\), dependent on whether is deasserted or not.\n• The circles marked \\(g_0, g_1, g_2\\) multiply the divisor by the adjustment factor.\n\nWhen we apply the dividend to the \\(p(x)\\) input in order of descending powers of \\(x^i\\), we get the following animation:\n• during the first 5 steps, the output contains the same quotient values as the one calculated by the long division.\n• during the last 3 steps, the remainder rolls out.\n• while shifting out the remainder values, the remainder registers are gradually re-initialized with a value of 0, so that a division operation can immediately restart again at step 1 for the next dividend.\n\nIn the previous section, we developed a sequential polynomial divider in hardware that outputs both the quotient and the remainder.\n\nFor a systematic Reed-Solomon encoder that outputs polynomial coefficients, the output should be the input, followed by the remainder. So a small modification is required:\n\nThe only difference compared to the hardware of a regular polynomial divider is the addition of an output multiplexer that allows us to route the input directly to the output. and can be wired together.\n\nIf you do a Google image search for “reed solomon encoder”, you’ll get a million variations of this diagram, with one exception: you won’t find any \\(-1\\) multiplier. This is because my Reed-Solomon encoder uses integer operations. In Galois math, subtraction and addition are the same operation.\n\nThe bus size of the hardware presented here is not defined, but for integer operations, the number of bits needed would be be much larger than the number of bits of the incoming symbols. That’s another big negative of using integers for these kind of coders. With Galois math, the result of any operation stays within the same range as the operands: an operation between 2 symbols that can be represented with 8 bits, will still be 8 bits, even if it’s a multiplication.\n\nThis concludes a first look at Reed-Solomon codes. There is a lot that was not discussed: Galois field mathematics, Galois field hardware logic, decoding algorithms, the link between Reed-Solomon codes and BCH codes, error corrections with erasures, and much more.\n\nIn the future, I’d love to try out Reed-Solomon codes on some practical examples, but who knows if I’ll ever get to that.\n\nIf you’ve made it this far, thanks for reading! I hope that this writedown was as useful for you as it was for myself.\n• Very good explanation on polynomial based interpolation and error correction.\n• Code that goes with the RS article.\n• A very comprehensive but long tutorial that covers Galois fields, encoding, and decoding, with examples.\n• Relatively heavy on math, but one of the few articles that covers the different types of Reed-Solomon coding: the original method of encoding through polynomial values as well as the method of encoding polynomial coefficients.\n• Not very relevant, but an interesting overview of the telecommunications systems and the design considerations of the Voyager program."
    }
]