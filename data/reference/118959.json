[
    {
        "link": "https://postgresql.org/docs/current/sql-createtable.html",
        "document": "The optional clause specifies a list of tables from which the new table automatically inherits all columns. Parent tables can be plain tables or foreign tables. Use of creates a persistent relationship between the new child table and its parent table(s). Schema modifications to the parent(s) normally propagate to children as well, and by default the data of the child table is included in scans of the parent(s). If the same column name exists in more than one parent table, an error is reported unless the data types of the columns match in each of the parent tables. If there is no conflict, then the duplicate columns are merged to form a single column in the new table. If the column name list of the new table contains a column name that is also inherited, the data type must likewise match the inherited column(s), and the column definitions are merged into one. If the new table explicitly specifies a default value for the column, this default overrides any defaults from inherited declarations of the column. Otherwise, any parents that specify default values for the column must all specify the same default, or an error will be reported. constraints are merged in essentially the same way as columns: if multiple parent tables and/or the new table definition contain identically-named constraints, these constraints must all have the same check expression, or an error will be reported. Constraints having the same name and expression will be merged into one copy. A constraint marked in a parent will not be considered. Notice that an unnamed constraint in the new table will never be merged, since a unique name will always be chosen for it. Column settings are also copied from parent tables. If a column in the parent table is an identity column, that property is not inherited. A column in the child table can be declared identity column if desired.\n\nCreates the table as a partition of the specified parent table. The table can be created either as a partition for specific values using or as a default partition using . Any indexes, constraints and user-defined row-level triggers that exist in the parent table are cloned on the new partition. The must correspond to the partitioning method and partition key of the parent table, and must not overlap with any existing partition of that parent. The form with is used for list partitioning, the form with and is used for range partitioning, and the form with is used for hash partitioning. is any variable-free expression (subqueries, window functions, aggregate functions, and set-returning functions are not allowed). Its data type must match the data type of the corresponding partition key column. The expression is evaluated once at table creation time, so it can even contain volatile expressions such as . When creating a list partition, can be specified to signify that the partition allows the partition key column to be null. However, there cannot be more than one such list partition for a given parent table. cannot be specified for range partitions. When creating a range partition, the lower bound specified with is an inclusive bound, whereas the upper bound specified with is an exclusive bound. That is, the values specified in the list are valid values of the corresponding partition key columns for this partition, whereas those in the list are not. Note that this statement must be understood according to the rules of row-wise comparison (Section 9.25.5). For example, given , a partition bound allows with any , with any non-null , and with any . The special values and may be used when creating a range partition to indicate that there is no lower or upper bound on the column's value. For example, a partition defined using allows any values less than 10, and a partition defined using allows any values greater than or equal to 10. When creating a range partition involving more than one column, it can also make sense to use as part of the lower bound, and as part of the upper bound. For example, a partition defined using allows any rows where the first partition key column is greater than 0 and less than or equal to 10. Similarly, a partition defined using allows any rows where the first partition key column starts with \"a\". Note that if or is used for one column of a partitioning bound, the same value must be used for all subsequent columns. For example, is not a valid bound; you should write . Also note that some element types, such as , have a notion of \"infinity\", which is just another value that can be stored. This is different from and , which are not real values that can be stored, but rather they are ways of saying that the value is unbounded. can be thought of as being greater than any other value, including \"infinity\" and as being less than any other value, including \"minus infinity\". Thus the range is not an empty range; it allows precisely one value to be stored — \"infinity\". If is specified, the table will be created as the default partition of the parent table. This option is not available for hash-partitioned tables. A partition key value not fitting into any other partition of the given parent will be routed to the default partition. When a table has an existing partition and a new partition is added to it, the default partition must be scanned to verify that it does not contain any rows which properly belong in the new partition. If the default partition contains a large number of rows, this may be slow. The scan will be skipped if the default partition is a foreign table or if it has a constraint which proves that it cannot contain rows which should be placed in the new partition. When creating a hash partition, a modulus and remainder must be specified. The modulus must be a positive integer, and the remainder must be a non-negative integer less than the modulus. Typically, when initially setting up a hash-partitioned table, you should choose a modulus equal to the number of partitions and assign every table the same modulus and a different remainder (see examples, below). However, it is not required that every partition have the same modulus, only that every modulus which occurs among the partitions of a hash-partitioned table is a factor of the next larger modulus. This allows the number of partitions to be increased incrementally without needing to move all the data at once. For example, suppose you have a hash-partitioned table with 8 partitions, each of which has modulus 8, but find it necessary to increase the number of partitions to 16. You can detach one of the modulus-8 partitions, create two new modulus-16 partitions covering the same portion of the key space (one with a remainder equal to the remainder of the detached partition, and the other with a remainder equal to that value plus 8), and repopulate them with data. You can then repeat this -- perhaps at a later time -- for each modulus-8 partition until none remain. While this may still involve a large amount of data movement at each step, it is still better than having to create a whole new table and move all the data at once. A partition must have the same column names and types as the partitioned table to which it belongs. Modifications to the column names or types of a partitioned table will automatically propagate to all partitions. constraints will be inherited automatically by every partition, but an individual partition may specify additional constraints; additional constraints with the same name and condition as in the parent will be merged with the parent constraint. Defaults may be specified separately for each partition. But note that a partition's default value is not applied when inserting a tuple through a partitioned table. Rows inserted into a partitioned table will be automatically routed to the correct partition. If no suitable partition exists, an error will occur. Operations such as which normally affect a table and all of its inheritance children will cascade to all partitions, but may also be performed on an individual partition. Note that creating a partition using requires taking an lock on the parent partitioned table. Likewise, dropping a partition with requires taking an lock on the parent table. It is possible to use to perform these operations with a weaker lock, thus reducing interference with concurrent operations on the partitioned table.\n\nThe clause specifies a table from which the new table automatically copies all column names, their data types, and their not-null constraints. Unlike , the new table and original table are completely decoupled after creation is complete. Changes to the original table will not be applied to the new table, and it is not possible to include data of the new table in scans of the original table. Also unlike , columns and constraints copied by are not merged with similarly named columns and constraints. If the same name is specified explicitly or in another clause, an error is signaled. The optional clauses specify which additional properties of the original table to copy. Specifying copies the property, specifying omits the property. is the default. If multiple specifications are made for the same kind of object, the last one is used. The available options are: Comments for the copied columns, constraints, and indexes will be copied. The default behavior is to exclude comments, resulting in the copied columns and constraints in the new table having no comments. Compression method of the columns will be copied. The default behavior is to exclude compression methods, resulting in columns having the default compression method. constraints will be copied. No distinction is made between column constraints and table constraints. Not-null constraints are always copied to the new table. Default expressions for the copied column definitions will be copied. Otherwise, default expressions are not copied, resulting in the copied columns in the new table having null defaults. Note that copying defaults that call database-modification functions, such as , may create a functional linkage between the original and new tables. Any generation expressions of copied column definitions will be copied. By default, new columns will be regular base columns. Any identity specifications of copied column definitions will be copied. A new sequence is created for each identity column of the new table, separate from the sequences associated with the old table. Indexes, , , and constraints on the original table will be created on the new table. Names for the new indexes and constraints are chosen according to the default rules, regardless of how the originals were named. (This behavior avoids possible duplicate-name failures for the new indexes.) Extended statistics are copied to the new table. settings for the copied column definitions will be copied. The default behavior is to exclude settings, resulting in the copied columns in the new table having type-specific default settings. For more on settings, see Section 65.2. is an abbreviated form selecting all the available individual options. (It could be useful to write individual clauses after to select all but some specific options.) The clause can also be used to copy column definitions from views, foreign tables, or composite types. Inapplicable options (e.g., from a view) are ignored.\n\nThe constraint specifies that a group of one or more columns of a table can contain only unique values. The behavior of a unique table constraint is the same as that of a unique column constraint, with the additional capability to span multiple columns. The constraint therefore enforces that any two rows must differ in at least one of these columns. For the purpose of a unique constraint, null values are not considered equal, unless is specified. Each unique constraint should name a set of columns that is different from the set of columns named by any other unique or primary key constraint defined for the table. (Otherwise, redundant unique constraints will be discarded.) When establishing a unique constraint for a multi-level partition hierarchy, all the columns in the partition key of the target partitioned table, as well as those of all its descendant partitioned tables, must be included in the constraint definition. Adding a unique constraint will automatically create a unique btree index on the column or group of columns used in the constraint. The created index has the same name as the unique constraint. The optional clause adds to that index one or more columns that are simply “payload”: uniqueness is not enforced on them, and the index cannot be searched on the basis of those columns. However they can be retrieved by an index-only scan. Note that although the constraint is not enforced on included columns, it still depends on them. Consequently, some operations on such columns (e.g., ) can cause cascaded constraint and index deletion.\n\nThe clause defines an exclusion constraint, which guarantees that if any two rows are compared on the specified column(s) or expression(s) using the specified operator(s), not all of these comparisons will return . If all of the specified operators test for equality, this is equivalent to a constraint, although an ordinary unique constraint will be faster. However, exclusion constraints can specify constraints that are more general than simple equality. For example, you can specify a constraint that no two rows in the table contain overlapping circles (see Section 8.8) by using the operator. The operator(s) are required to be commutative. Exclusion constraints are implemented using an index that has the same name as the constraint, so each specified operator must be associated with an appropriate operator class (see Section 11.10) for the index access method . Each defines a column of the index, so it can optionally specify a collation, an operator class, operator class parameters, and/or ordering options; these are described fully under CREATE INDEX. The access method must support (see Chapter 62); at present this means cannot be used. Although it's allowed, there is little point in using B-tree or hash indexes with an exclusion constraint, because this does nothing that an ordinary unique constraint doesn't do better. So in practice the access method will always be or . The allows you to specify an exclusion constraint on a subset of the table; internally this creates a partial index. Note that parentheses are required around the predicate.\n\nThese clauses specify a foreign key constraint, which requires that a group of one or more columns of the new table must only contain values that match values in the referenced column(s) of some row of the referenced table. If the list is omitted, the primary key of the is used. Otherwise, the list must refer to the columns of a non-deferrable unique or primary key constraint or be the columns of a non-partial unique index. The user must have permission on the referenced table (either the whole table, or the specific referenced columns). The addition of a foreign key constraint requires a lock on the referenced table. Note that foreign key constraints cannot be defined between temporary tables and permanent tables. A value inserted into the referencing column(s) is matched against the values of the referenced table and referenced columns using the given match type. There are three match types: , , and (which is the default). will not allow one column of a multicolumn foreign key to be null unless all foreign key columns are null; if they are all null, the row is not required to have a match in the referenced table. allows any of the foreign key columns to be null; if any of them are null, the row is not required to have a match in the referenced table. is not yet implemented. (Of course, constraints can be applied to the referencing column(s) to prevent these cases from arising.) In addition, when the data in the referenced columns is changed, certain actions are performed on the data in this table's columns. The clause specifies the action to perform when a referenced row in the referenced table is being deleted. Likewise, the clause specifies the action to perform when a referenced column in the referenced table is being updated to a new value. If the row is updated, but the referenced column is not actually changed, no action is done. Referential actions other than the check cannot be deferred, even if the constraint is declared deferrable. There are the following possible actions for each clause: Produce an error indicating that the deletion or update would create a foreign key constraint violation. If the constraint is deferred, this error will be produced at constraint check time if there still exist any referencing rows. This is the default action. Produce an error indicating that the deletion or update would create a foreign key constraint violation. This is the same as except that the check is not deferrable. Delete any rows referencing the deleted row, or update the values of the referencing column(s) to the new values of the referenced columns, respectively. Set all of the referencing columns, or a specified subset of the referencing columns, to null. A subset of columns can only be specified for actions. Set all of the referencing columns, or a specified subset of the referencing columns, to their default values. A subset of columns can only be specified for actions. (There must be a row in the referenced table matching the default values, if they are not null, or the operation will fail.) If the referenced column(s) are changed frequently, it might be wise to add an index to the referencing column(s) so that referential actions associated with the foreign key constraint can be performed more efficiently."
    },
    {
        "link": "https://postgresql.org/docs/current/sql-altertable.html",
        "document": "This form changes the type of a column of a table. Indexes and simple table constraints involving the column will be automatically converted to use the new column type by reparsing the originally supplied expression. The optional clause specifies a collation for the new column; if omitted, the collation is the default for the new column type. The optional clause specifies how to compute the new column value from the old; if omitted, the default conversion is the same as an assignment cast from old data type to new. A clause must be provided if there is no implicit or assignment cast from old to new type. When this form is used, the column's statistics are removed, so running on the table afterwards is recommended.\n\nThese forms change whether a column is marked to allow null values or to reject null values. may only be applied to a column provided none of the records in the table contain a value for the column. Ordinarily this is checked during the by scanning the entire table; however, if a valid constraint is found which proves no can exist, then the table scan is skipped. If this table is a partition, one cannot perform on a column if it is marked in the parent table. To drop the constraint from all the partitions, perform on the parent table. Even if there is no constraint on the parent, such a constraint can still be added to individual partitions, if desired; that is, the children can disallow nulls even if the parent allows them, but not the other way around.\n\nThis form sets or resets per-attribute options. Currently, the only defined per-attribute options are and , which override the number-of-distinct-values estimates made by subsequent operations. affects the statistics for the table itself, while affects the statistics gathered for the table plus its inheritance children. When set to a positive value, will assume that the column contains exactly the specified number of distinct nonnull values. When set to a negative value, which must be greater than or equal to -1, will assume that the number of distinct nonnull values in the column is linear in the size of the table; the exact count is to be computed by multiplying the estimated table size by the absolute value of the given number. For example, a value of -1 implies that all values in the column are distinct, while a value of -0.5 implies that each value appears twice on the average. This can be useful when the size of the table changes over time, since the multiplication by the number of rows in the table is not performed until query planning time. Specify a value of 0 to revert to estimating the number of distinct values normally. For more information on the use of statistics by the PostgreSQL query planner, refer to Section 14.2.\n\nThis form adds a new constraint to a table using the same constraint syntax as , plus the option , which is currently only allowed for foreign key and CHECK constraints. Normally, this form will cause a scan of the table to verify that all existing rows in the table satisfy the new constraint. But if the option is used, this potentially-lengthy scan is skipped. The constraint will still be enforced against subsequent inserts or updates (that is, they'll fail unless there is a matching row in the referenced table, in the case of foreign keys, or they'll fail unless the new row matches the specified check condition). But the database will not assume that the constraint holds for all rows in the table, until it is validated by using the option. See Notes below for more information about using the option. Although most forms of require an lock, requires only a lock. Note that also acquires a lock on the referenced table, in addition to the lock on the table on which the constraint is declared. Additional restrictions apply when unique or primary key constraints are added to partitioned tables; see . Also, foreign key constraints on partitioned tables may not be declared at present.\n\nThis form adds a new or constraint to a table based on an existing unique index. All the columns of the index will be included in the constraint. The index cannot have expression columns nor be a partial index. Also, it must be a b-tree index with default sort ordering. These restrictions ensure that the index is equivalent to one that would be built by a regular or command. If is specified, and the index's columns are not already marked , then this command will attempt to do against each such column. That requires a full table scan to verify the column(s) contain no nulls. In all other cases, this is a fast operation. If a constraint name is provided then the index will be renamed to match the constraint name. Otherwise the constraint will be named the same as the index. After this command is executed, the index is “owned” by the constraint, in the same way as if the index had been built by a regular or command. In particular, dropping the constraint will make the index disappear too. This form is not currently supported on partitioned tables. Adding a constraint using an existing index can be helpful in situations where a new constraint needs to be added without blocking table updates for a long time. To do that, create the index using , and then convert it to a constraint using this syntax. See the example below.\n\nThese forms configure the firing of trigger(s) belonging to the table. A disabled trigger is still known to the system, but is not executed when its triggering event occurs. (For a deferred trigger, the enable status is checked when the event occurs, not when the trigger function is actually executed.) One can disable or enable a single trigger specified by name, or all triggers on the table, or only user triggers (this option excludes internally generated constraint triggers, such as those that are used to implement foreign key constraints or deferrable uniqueness and exclusion constraints). Disabling or enabling internally generated constraint triggers requires superuser privileges; it should be done with caution since of course the integrity of the constraint cannot be guaranteed if the triggers are not executed. The trigger firing mechanism is also affected by the configuration variable session_replication_role. Simply enabled triggers (the default) will fire when the replication role is “origin” (the default) or “local”. Triggers configured as will only fire if the session is in “replica” mode, and triggers configured as will fire regardless of the current replication role. The effect of this mechanism is that in the default configuration, triggers do not fire on replicas. This is useful because if a trigger is used on the origin to propagate data between tables, then the replication system will also replicate the propagated data; so the trigger should not fire a second time on the replica, because that would lead to duplication. However, if a trigger is used for another purpose such as creating external alerts, then it might be appropriate to set it to so that it is also fired on replicas. When this command is applied to a partitioned table, the states of corresponding clone triggers in the partitions are updated too, unless is specified.\n\nThis form attaches an existing table (which might itself be partitioned) as a partition of the target table. The table can be attached as a partition for specific values using or as a default partition by using . For each index in the target table, a corresponding one will be created in the attached table; or, if an equivalent index already exists, it will be attached to the target table's index, as if had been executed. Note that if the existing table is a foreign table, it is currently not allowed to attach the table as a partition of the target table if there are indexes on the target table. (See also CREATE FOREIGN TABLE.) For each user-defined row-level trigger that exists in the target table, a corresponding one is created in the attached table. A partition using uses same syntax for as . The partition bound specification must correspond to the partitioning strategy and partition key of the target table. The table to be attached must have all the same columns as the target table and no more; moreover, the column types must also match. Also, it must have all the and constraints of the target table, not marked . Currently constraints are not considered. and constraints from the parent table will be created in the partition, if they don't already exist. If the new partition is a regular table, a full table scan is performed to check that existing rows in the table do not violate the partition constraint. It is possible to avoid this scan by adding a valid constraint to the table that allows only rows satisfying the desired partition constraint before running this command. The constraint will be used to determine that the table need not be scanned to validate the partition constraint. This does not work, however, if any of the partition keys is an expression and the partition does not accept values. If attaching a list partition that will not accept values, also add a constraint to the partition key column, unless it's an expression. If the new partition is a foreign table, nothing is done to verify that all the rows in the foreign table obey the partition constraint. (See the discussion in CREATE FOREIGN TABLE about constraints on the foreign table.) When a table has a default partition, defining a new partition changes the partition constraint for the default partition. The default partition can't contain any rows that would need to be moved to the new partition, and will be scanned to verify that none are present. This scan, like the scan of the new partition, can be avoided if an appropriate constraint is present. Also like the scan of the new partition, it is always skipped when the default partition is a foreign table. Attaching a partition acquires a lock on the parent table, in addition to the locks on the table being attached and on the default partition (if any). Further locks must also be held on all sub-partitions if the table being attached is itself a partitioned table. Likewise if the default partition is itself a partitioned table. The locking of the sub-partitions can be avoided by adding a constraint as described in Section 5.12.2.2.\n\nThis form detaches the specified partition of the target table. The detached partition continues to exist as a standalone table, but no longer has any ties to the table from which it was detached. Any indexes that were attached to the target table's indexes are detached. Any triggers that were created as clones of those in the target table are removed. lock is obtained on any tables that reference this partitioned table in foreign key constraints. If is specified, it runs using a reduced lock level to avoid blocking other sessions that might be accessing the partitioned table. In this mode, two transactions are used internally. During the first transaction, a lock is taken on both parent table and partition, and the partition is marked as undergoing detach; at that point, the transaction is committed and all other transactions using the partitioned table are waited for. Once all those transactions have completed, the second transaction acquires on the partitioned table and on the partition, and the detach process completes. A constraint that duplicates the partition constraint is added to the partition. cannot be run in a transaction block and is not allowed if the partitioned table contains a default partition. If is specified, a previous invocation that was canceled or interrupted is completed. At most one partition in a partitioned table can be pending detach at a time."
    },
    {
        "link": "https://w3schools.com/postgresql/postgresql_add_column.php",
        "document": "To add a column to an existing table, we have to use the statement.\n\nThe statement is used to add, delete, or modify columns in an existing table.\n\nThe statement is also used to add and drop various constraints on an existing table.\n\nWe want to add a column named to our table.\n\nWhen adding columns we must also specify the data type of the column. Our column will be a string, and we specify string types with the keyword. we also want to restrict the number of characters to 255:\n\nTo check the result we can display the table with this SQL statement:\n\nAs you can see, the table now has a column.\n\nThe new column is empty, you will learn how to fill it with values in the next chapter."
    },
    {
        "link": "https://stackoverflow.com/questions/1243547/how-to-add-a-new-column-in-a-table-after-the-2nd-or-3rd-column-in-the-table-usin",
        "document": "My code looks as follows\n\nHow to add a new column in a table after the 2nd or 3rd column in the table using postgres?\n\nNo, there's no direct way to do that. And there's a reason for it - every query should list all the fields it needs in whatever order (and format etc) it needs them, thus making the order of the columns in one table insignificant. If you really need to do that I can think of one workaround:\n• dump and save the description of the table in question (using )\n• add the column you want where you want it in the saved definition\n• rename the table in the saved definition so not to clash with the name of the old table when you attempt to create it\n• create the new table using this definition\n• populate the new table with the data from the old table using 'INSERT INTO SELECT field1, field2, , field3,... FROM ';\n• rename the new table to the original name\n• eventually drop the old, renamed table after you make sure everything's alright\n\nThe order of columns is not irrelevant, putting fixed width columns at the front of the table can optimize the storage layout of your data, it can also make working with your data easier outside of your application code. PostgreSQL does not support altering the column ordering (see Alter column position on the PostgreSQL wiki); if the table is relatively isolated, your best bet is to recreate the table: CREATE TABLE foobar_new ( ... ); INSERT INTO foobar_new SELECT ... FROM foobar; DROP TABLE foobar CASCADE; ALTER TABLE foobar_new RENAME TO foobar; If you have a lot of views or constraints defined against the table, you can re-add all the columns after the new column and drop the original columns (see the PostgreSQL wiki for an example).\n\nThe real problem here is that it's not done yet. Currently PostgreSQL's logical ordering is the same as the physical ordering. That's problematic because you can't get a different logical ordering, but it's even worse because the table isn't physically packed automatically, so by moving columns you can get different performance characteristics. Arguing that it's that way by intent in design is pointless. It's somewhat likely to change at some point when an acceptable patch is submitted. All of that said, is it a good idea to rely on the ordinal positioning of columns, logical or physical? Hell no. In production code you should never be using an implicit ordering or . Why make the code more brittle than it needs to be? Correctness should always be a higher priority than saving a few keystrokes. As a work around, you can in fact modify the column ordering by recreating the table, or through the \"add and reorder\" game\n• Column tetris reordering in order to make things more space-efficient\n\nThe column order is relevant to me, so I created this function. See if it helps. It works with indexes, primary key, and triggers. Missing Views and Foreign Key and other features are missing. CREATE OR REPLACE FUNCTION xaddcolumn(ptable text, pcol text, pafter text) RETURNS void AS $BODY$ DECLARE rcol RECORD; rkey RECORD; ridx RECORD; rtgr RECORD; vsql text; vkey text; vidx text; cidx text; vtgr text; ctgr text; etgr text; vseq text; vtype text; vcols text; BEGIN EXECUTE 'CREATE TABLE zzz_' || ptable || ' AS SELECT * FROM ' || ptable; --colunas vseq = ''; vcols = ''; vsql = 'CREATE TABLE ' || ptable || '('; FOR rcol IN SELECT column_name as col, udt_name as coltype, column_default as coldef, is_nullable as is_null, character_maximum_length as len, numeric_precision as num_prec, numeric_scale as num_scale FROM information_schema.columns WHERE table_name = ptable ORDER BY ordinal_position LOOP vtype = rcol.coltype; IF (substr(rcol.coldef,1,7) = 'nextval') THEN vtype = 'serial'; vseq = vseq || 'SELECT setval(''' || ptable || '_' || rcol.col || '_seq''' || ', max(' || rcol.col || ')) FROM ' || ptable || ';'; ELSIF (vtype = 'bpchar') THEN vtype = 'char'; END IF; vsql = vsql || E'\n\n' || rcol.col || ' ' || vtype; IF (vtype in ('varchar', 'char')) THEN vsql = vsql || '(' || rcol.len || ')'; ELSIF (vtype = 'numeric') THEN vsql = vsql || '(' || rcol.num_prec || ',' || rcol.num_scale || ')'; END IF; IF (rcol.is_null = 'NO') THEN vsql = vsql || ' NOT NULL'; END IF; IF (rcol.coldef <> '' AND vtype <> 'serial') THEN vsql = vsql || ' DEFAULT ' || rcol.coldef; END IF; vsql = vsql || E','; vcols = vcols || rcol.col || ','; -- IF (rcol.col = pafter) THEN vsql = vsql || E'\n\n' || pcol || ','; END IF; END LOOP; vcols = substr(vcols,1,length(vcols)-1); --keys vkey = ''; FOR rkey IN SELECT constraint_name as name, column_name as col FROM information_schema.key_column_usage WHERE table_name = ptable LOOP IF (vkey = '') THEN vkey = E'\n\nCONSTRAINT ' || rkey.name || ' PRIMARY KEY ('; END IF; vkey = vkey || rkey.col || ','; END LOOP; IF (vkey <> '') THEN vsql = vsql || substr(vkey,1,length(vkey)-1) || ') '; END IF; vsql = substr(vsql,1,length(vsql)-1) || ') WITHOUT OIDS'; --index vidx = ''; cidx = ''; FOR ridx IN SELECT s.indexrelname as nome, a.attname as col FROM pg_index i LEFT JOIN pg_class c ON c.oid = i.indrelid LEFT JOIN pg_attribute a ON a.attrelid = c.oid AND a.attnum = ANY(i.indkey) LEFT JOIN pg_stat_user_indexes s USING (indexrelid) WHERE c.relname = ptable AND i.indisunique != 't' AND i.indisprimary != 't' ORDER BY s.indexrelname LOOP IF (ridx.nome <> cidx) THEN IF (vidx <> '') THEN vidx = substr(vidx,1,length(vidx)-1) || ');'; END IF; cidx = ridx.nome; vidx = vidx || E'\n\nCREATE INDEX ' || cidx || ' ON ' || ptable || ' ('; END IF; vidx = vidx || ridx.col || ','; END LOOP; IF (vidx <> '') THEN vidx = substr(vidx,1,length(vidx)-1) || ')'; END IF; --trigger vtgr = ''; ctgr = ''; etgr = ''; FOR rtgr IN SELECT trigger_name as nome, event_manipulation as eve, action_statement as act, condition_timing as cond FROM information_schema.triggers WHERE event_object_table = ptable LOOP IF (rtgr.nome <> ctgr) THEN IF (vtgr <> '') THEN vtgr = replace(vtgr, '_@eve_', substr(etgr,1,length(etgr)-3)); END IF; etgr = ''; ctgr = rtgr.nome; vtgr = vtgr || 'CREATE TRIGGER ' || ctgr || ' ' || rtgr.cond || ' _@eve_ ' || 'ON ' || ptable || ' FOR EACH ROW ' || rtgr.act || ';'; END IF; etgr = etgr || rtgr.eve || ' OR '; END LOOP; IF (vtgr <> '') THEN vtgr = replace(vtgr, '_@eve_', substr(etgr,1,length(etgr)-3)); END IF; --exclui velha e cria nova EXECUTE 'DROP TABLE ' || ptable; IF (EXISTS (SELECT sequence_name FROM information_schema.sequences WHERE sequence_name = ptable||'_id_seq')) THEN EXECUTE 'DROP SEQUENCE '||ptable||'_id_seq'; END IF; EXECUTE vsql; --dados na nova EXECUTE 'INSERT INTO ' || ptable || '(' || vcols || ')' || E'\n\nSELECT ' || vcols || ' FROM zzz_' || ptable; EXECUTE vseq; EXECUTE vidx; EXECUTE vtgr; EXECUTE 'DROP TABLE zzz_' || ptable; END; $BODY$ LANGUAGE plpgsql VOLATILE COST 100;\n\n@Jeremy Gustie's solution above almost works, but will do the wrong thing if the ordinals are off (or fail altogether if the re-ordered ordinals make incompatible types match). Give it a try: CREATE TABLE test1 (one varchar, two varchar, three varchar); CREATE TABLE test2 (three varchar, two varchar, one varchar); INSERT INTO test1 (one, two, three) VALUES ('one', 'two', 'three'); INSERT INTO test2 SELECT * FROM test1; SELECT * FROM test2; testdb=> select * from test2; three | two | one -------+-----+------- one | two | three (1 row) You can remedy this by specifying the column names in the insert: INSERT INTO test2 (one, two, three) SELECT * FROM test1; That gives you what you really want: testdb=> select * from test2; three | two | one -------+-----+----- three | two | one (1 row) The problem comes when you have legacy that doesn't do this, as I indicated above in my comment on peufeu's reply. Update: It occurred to me that you can do the same thing with the column names in the INSERT clause by specifying the column names in the SELECT clause. You just have to reorder them to match the ordinals in the target table: INSERT INTO test2 SELECT three, two, one FROM test1; And you can of course do both to be very explicit: INSERT INTO test2 (one, two, three) SELECT one, two, three FROM test1; That gives you the same results as above, with the column values properly matched.\n\nThe order of the columns is totally irrelevant in relational databases For instance if you use Python, you would do : cursor.execute( \"SELECT id, name FROM users\" ) for id, name in cursor: print id, name cursor.execute( \"SELECT * FROM users\" ) for row in cursor: print row['id'], row['name'] But no sane person would ever use positional results like this : cursor.execute( \"SELECT * FROM users\" ) for id, name in cursor: print id, name\n\nThe irrelevant need from having a set order of columns is not always defined by the query that pulls them. In the values from does not include the associated column name and therefore would require the columns to be defined by the SQL statement. A simple would require innate knowledge of the table structure, and would sometimes cause issues if the order of the columns were to change. Using is a more reliable method as you can reference the column names, and therefore use a simple ."
    },
    {
        "link": "https://beekeeperstudio.io/blog/postgres-create-tables",
        "document": "In order to organize data in Postgres, you must first know how to create tables to store your data. This article covers ten different scenarios of creating tables. This includes but certainly is not limited to creating tables with primary keys, constraints, and indexes.\n\nLet’s create a simple table to store data about students in a university. You will create a table with the query followed by the table name and the list of columns.\n\nThe statement lets you create a table in the database. Our table name is students. The students table has five columns to store the students’ ID, name, GPA, enrollment date, and status of completion of the degree. A column must be declared with its data type, and each columns should be separated by a comma.\n\nA data type tells the database what sort of data you are allowed to enter for a particular column. For example, allows only whole number values. allows adding string values with varying character lengths. As you can see, we have given a maximum length of 100 characters to student_name. Furthermore, allows you to add decimal values. The total number of digits and digits after the decimal point is given in brackets. allows adding date values without the time. allows adding true (yes) or false (no) values. Although there are many other data types in Postgres, these are some of the most commonly used types.\n\nIf you use Beekeeper Studio, you can easily create a table in two clicks.\n\nA primary key is a column that uniquely identifies each row using a unique value. In our example, each student has a distinctive student ID value that cannot be repeated. The other columns we do not restrict, and may have repeated values.\n\nTo declare the student_id column as the primary key, simply use the keywords PRIMARY KEY after its data type.\n\nIf you want the primary key column values to increment automatically, the easiest method is to use the datatype. This creates a series of integer values automatically for the student id column when you add new records.\n\nLet’s put everything together. This is how the code looks now.\n\nLet’s create another table to store data about student user accounts. Suppose that, when a student account is created we need to store the date and exact time of creation. Also, whenever a student logs in the date and time have to be updated. To store these types of values we use the data type. See the example below to understand it better.\n\nNow let’s create a sub-table from our students table to show the students who have a GPA higher than 3.7. This can be done by using the CREATE TABLE AS statement and fetching data using a SELECT query. You may select the columns you need from the students table by checking if the GPA is greater than or equal to 3.7.\n\nHere is what the output table looks like. This is a new permanent table named first_class_students and you can insert or modify data later too. Note that will not update when the original students table updates, it is a snapshot in time from when we ran this query.\n\nThe statement doesn’t work for tables in PostgreSQL, unlike other database systems. can be used with a , but otherwise we are forced to use the statement to modify tables.\n\nViews are virtual tables that do not contain any data, but are simply a ‘view’ to other tables in your database. Let’s create a view to find the students who have enrolled after the deadline. This can be done by checking whether they enrolled on or after 2022-03-01.\n\nUsing the ALTER TABLE statement you may add or modify column names, data types, or constraints in an existing table.\n\nYou may even rename your table.\n\nAfter creating a table you can import data from a CSV (Comma Separated Values) file using its absolute path.\n\nThe table should be created prior to importing data, otherwise, it causes an error. I have used the new table name in this example since we altered it in the previous example. Delimiter specifies the character used to separate columns in the CSV file. CSV HEADER statement lets the database know that the file contains header values.\n\nIndexes allow queries against specific columns execute faster. Think of indexes like the index of a book – they allow you to find the page you need quickly. This is similar for databases, they let you find the record you need quickly.\n\nAn index created on the primary key column will be automatically created when we define the primary key. That means in our students table an index is created on the student_id column.\n\nYou can of course create indexes on other columns. Use the statement with the table name and the relevant column name to do this. I have used student_name_index as the index name here.\n\nThis index allows us to quickly find a student by name.\n\nConstraints are specific limitations. You already are familiar with the statement. It is a type of constraint. , , and are some other examples of constraints. A constraint can be either declared as a column constraint or a table constraint.\n\nColumn constraints are written after the data type. You may even have multiple constraints. There is no specific order in writing them. Let’s rewrite a few column declarations from the students table using column constraints.\n\nTable constraints must be written after the column names list. The NOT NULL constraint can not be directly used as a table constraint, unlike the other constraint types. So, you may use IS NOT NULL with each column inside a CHECK constraint. Let’s rewrite a few column declarations from the student_accounts table using table constraints.\n\nIf you want a table for temporary use, you can create a temporary table using the (or ) keyword. This table will be automatically dropped when your session ends.\n\nLet’s create a temp table to fetch students who have graduated.\n\nSo far we created tables directly on the database. However, there is another layer between databases and tables. These are called schemas. All the tables, views, functions, sequences we create in PostgreSQL should belongs to a schema.\n\nPostgreSQL comes with a default schema called public. So all the tables we created so far belong to the public schema. So let’s take our first example which was creating a simple student table.\n\nSo you can access the above table with the following query.\n\nHowever, it’s not necessary to mention the schema as it’s the default schema.\n\nIt’s a good practice to break your database into several schemas without putting all the tables, views, and functions into one database. There are four main benefits to schemas.\n• You can apply bulk actions easily - For example, it’s easier to backup and restore data when they are in separate schemas.\n• Users can be restricted to work in certain schemas. - Imagine a school database containing various tables such as teachers, students, non-academic staff, finance, etc. So you can put tables related to student details into one schema and tables related to finance into another schema. Then you restrict access of users based on the schema. This is easier than giving access table by table.\n• Schema works similarly to namespaces - You can have tables with the same name in different schemas. For example, when you create a database for a university, you can have separate schemas for the undergraduate section and postgraduate section. Both schemas can have separate student tables.\n• Easier to update to new versions. - Suppose you are releasing a new version of your application and you have to change the structure of the database. It’s overall easier to cut some schema and add new schemas than dealing with the full database.\n\nNow let’s see how to create a new schema.\n\nHowever, if there is already a table with that schema name, the database will send an error. So it’s better to write the query like this\n\nNow let’s create a table in that schema. We will create a student table again in the school schema.\n\nSo as you can see, all you had to do was to include the schema name before the table name.\n\nSuppose that you want to shift a table you created in a public schema to a newly created schema. So how do you do it? The following code will help\n\nYou have to use the following query to access the data in the students table in the school schema.\n\nOne of the main reasons behind creating schemas is to restrict the access of users. So let’s see how to do that. Suppose there is a user called finance_manager. Let’s give him access to the finance schema.\n\nWhen we create a new database in PostgreSQL, any role is allowed to create tables in the public schema. To prevent that from happening, you have to first remove that privilege from all users except the superuser. You can do it with the following code\n\nThen you can give privileges to create a table to a specific user. (let’s call him designer).\n\nHere is a command in PostgreSQL to provide privileges to users. is a type of which includes the permission. A person who has permission can create tables, views, functions, and any type of object in that schema.\n\nHowever, when it comes to temporary tables, every user can create them.\n\nTo summarize, data types and constraints are used to specify rules to table columns. You can create sub-tables, views, or temporary tables from existing tables. Tables can be modified by using the ALTER statement. Indexes are used for efficient data retrieval."
    },
    {
        "link": "https://dba.stackexchange.com/questions/320168/what-is-a-best-way-to-add-new-column-to-existing-table-with-milions-of-rows-in-s",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://stackoverflow.com/questions/45720394/how-to-write-update-to-add-column-with-value-for-existing",
        "document": "I have a table Members with existing data, I want to add a non-nullable bit column called 'IsOnlineUser', I want all my existing rows to be set to false. I have a set of scripts that run each time I deploy so I need a check to see if the table\n\nThe first SQL I tried was\n\nBut that gives me\n\n. Assumedly this is because the UPDATE fails to find the created column so I thought if I put a 'GO' between the two statements it would help so I did the following:\n\nHowever this says\n\nAssumedly this is because of the GO stopping me access the scalar variable between the two.\n\nIt seems like a fairly common use case, so I assume I am just missing something, any help would be much appreciated"
    },
    {
        "link": "https://devart.com/dbforge/sql/studio/add-column-to-table-sql-server.html",
        "document": "ALTER TABLE ADD: How to add a column to a table in SQL Server SQL Server databases are widely used in various industries to store, manage and manipulate large amounts of data. As the data grows, it becomes necessary to continuously maintain the database and optimize its performance. Adding new columns to the existing tables is a common task in this regard. Whether it's to store additional information or to improve data indexing and query processes, the ALTER TABLE command in SQL Server provides a straightforward and efficient way to accomplish this task. In this article, we'll explore the syntax and usage of the ALTER TABLE command in SQL Server, along with some best practices and considerations to keep in mind when adding columns to your database tables.\n\nThe basic syntax of the ALTER TABLE command used to add a new column to the existing database table is below: table_name is the name of the table where you want to add a new column. column_name is the name of the new column you are adding. data_type is the data type of the new column you are adding (VARCHAR, DATE, INT, etc.). Specify the size of the table where applicable, e.g., for the column of the VARCHAR data type. Specify the size of the table where applicable, e.g., for the column of the VARCHAR data type. To add several columns to a table in SQL Server, you can modify the standard command syntax in the following way: You can add as many columns to a table as needed, and specify different data types for them all.\n\nAdd new columns to a SQL Server table using queries Here, let us demonstrate how to use the ALTER TABLE command to add new columns to an existing database table. Several practical examples will be provided to illustrate the process - we'll use the sakila sample database. In this example, we'll demonstrate how to use the ALTER TABLE command in SQL Server to add a new column to the film table in the sakila test database. The new column will store information about the genres each movie belongs to. As you can see, the database management system automatically adds the new genre column to the end of the table. The following example demonstrates how to add multiple columns to an existing table in a SQL Server database. Let's imagine that we need to add additional information about the movies in our DVD store. Let us run the query. As you can see, SQL Server puts the new columns after the last table column in the same order as specified in the query. You might need to add a column to the existing table for a number of reasons from maintaining backward compatibility to accommodating UI or presentation layer considerations. Let us look at how you can do this using SQL statements. Step 1: Create a new table with the desired column order, including the column you want to add. Step 2: Copy the data from the existing table to the new table. Step 4: Rename the new table to the original table name. Add a column to one table based on information from another table To add a column to one table based on information from another table in SQL Server, you can use the ALTER TABLE statement with the ADD COLUMN clause and a SELECT statement. The ALTER TABLE statement adds the new column to table1, and the UPDATE statement populates the new column by retrieving the corresponding information from table2 based on the specified join condition.\n\nAdd a column to a table using a visual table editor in SSMS To add a new column to an existing SQL Server table using SQL Server Management Studio (SSMS), first, open SSMS and connect to the required database server. In Object Explorer, navigate to the database that contains the table where you want to add a new column, expand the database, and right-click the table you want to add a new column to. In the context menu that appears, select Design. In the Table Designer window, that appears, put the cursor on an empty row at the end of the list of columns. In the new column's row, enter the desired name for the new column in the Column Name field. Next, use the Data Type drop-down menu to select the appropriate data type for the new column (e.g. VARCHAR, INT, DATE, etc.). Then, specify any additional properties for the new column, such as its length, precision, or scale, using the corresponding fields in the Column Properties pane below. Save the new column by clicking the Save icon or selecting Save from the File menu. Finally, close the Table Designer window.\n\ndbForge Studio for SQL Server is a powerful IDE that provides a range of features to enhance SQL Server database development and management. Some of its capabilities include code completion, schema comparison and synchronization, SQL debugging, and data generation. It also provides a graphical interface that allows modifying the table (in particular -adding a new column or columns) without the need for manual coding. Add a new column using the Studio Let's take a look at how to visually add a new column to a SQL Server table using dbForge Studio. This approach can be more user-friendly and intuitive than using SQL queries directly, especially for those who may not be as comfortable with writing SQL code. In Database Explorer, right-click the table you want to change and select Edit Table: The Table Editor window will appear. Right-click the column area and select New Column (or just scroll the list of columns down to get to the empty lines): Enter the new column name and customize its properties such as data type, NOT NULL, identity, etc. Once done, click Apply Changes to save the modifications. Add a column in the middle of a SQL Server table Adding a new column to an SQL Server table using the ALTER TABLE command or dbForge Studio typically results in the column being appended to the end of the table. However, there may be situations where you need to insert a new column in the middle of existing columns. Let us look at how it can be achieved in dbForge Studio for SQL Server. Microsoft generally does not recommend changing the order of columns in a table, as it can have unintended consequences for code and applications that rely on the specific order of columns. Changing the order of columns can also cause problems with referential integrity constraints and other database objects that depend on the structure of the table. Microsoft generally does not recommend changing the order of columns in a table, as it can have unintended consequences for code and applications that rely on the specific order of columns. Changing the order of columns can also cause problems with referential integrity constraints and other database objects that depend on the structure of the table. To insert a new column in the middle of an existing SQL Server table using dbForge Studio for SQL Server, open the Table Editor the same way we accessed it earlier. Then right-click the column name after which you want to insert your new column and select Insert Column. Once done, click Apply Changes to save the modifications. In dbForge Studio for SQL Server, you can choose between two methods to add a nullable column to an existing table. The first method involves utilizing the built-in Table Editor, which provides a visual interface for performing the operation without the need for coding. The second method involves using the SQL Editor, where you can manually enter the relevant SQL statement to add the nullable column. Add a nullable column to a table using the Studio's Table Editor To access the Table Editor, right-click the table you want to add a nullable column to in Database Explorer and then select Edit Table from the context menu. In the Table Editor, enter the name of the new column and specify its datatype. To allow the new column to accept NULL values, ensure that the Not Null checkbox remains unselected. In SQL Server, you can use the following code to add a nullable column to an existing table. This statement adds a new column named with the specified data type of int. By including the NULL keyword, it allows the column to accept null values during data insertion.\n\nAdding a new column to an SQL Server table is a relatively straightforward task that can be accomplished using either SQL queries or visual interfaces such as SQL Server Management Studio and dbForge Studio for SQL Server. Although both tools provide a visual interface for adding columns to a table, dbForge Studio offers more flexibility and customizations for changing the scope of columns. Ultimately, the method you choose to add a column to an SQL Server table will depend on your specific needs and preferences."
    },
    {
        "link": "https://stackoverflow.com/questions/4443262/add-column-to-table-and-then-update-it-inside-transaction",
        "document": "I am creating a script that will be run in a MS SQL server. This script will run multiple statements and needs to be transactional, if one of the statement fails the overall execution is stopped and any changes are rolled back.\n\nI am having trouble creating this transactional model when issuing ALTER TABLE statements to add columns to a table and then updating the newly added column. In order to access the newly added column right away, I use a GO command to execute the ALTER TABLE statement, and then call my UPDATE statement. The problem I am facing is that I cannot issue a GO command inside an IF statement. The IF statement is important within my transactional model. This is a sample code of the script I am trying to run. Also notice that issuing a GO command, will discard the @errorCode variable, and will need to be declared down in the code before being used (This is not in the code below).\n\nSo what I would like to know is how to go around this problem, issuing ALTER TABLE statements to add a column and then updating that column, all within a script executing as a transactional unit."
    },
    {
        "link": "https://sqlshack.com/sql-server-alter-table-add-column-overview",
        "document": "In this article, we will explore SQL Server ALTER TABLE ADD Column statements to add column(s) to an existing table. We will also understand the impact of adding a column with a default value and adding and updating the column with a value later on larger tables.\n\nAs a database developer, you need to add columns to the existing tables too offen. You would think that adding a column to the SQL Server database table would not be a major deal. Sometimes you might be adding the column from the SQL Server Management Studio itself. Well, that would be fine for a small table or a table which does not have a large number of transactions. Let’s go ahead and see how we can add columns to an existing table and also understand the reasons and the best practices to add a column to a table that is large in size.\n\nLet us create a sample table with sufficient data set from the following script.\n\nThe above script will create a sample table called SampleTable. Data fields are added so that the large size table will be created. Next, a large number of records were added by executing the following query multiple times.\n\nAfter the above query is executed, 500,000 records are updated to the SampleTable. After executing the above query following is the table size and other parameters for the table.\n\nThis can be retrieved by sp_spaceused ‘SampleTable’\n\nThe following is the database size for data and the log file.\n\nLet’s quickly go over the syntax of adding one column to an existing table by using ALTER TABLE ADD statement as shown below.\n\nYou can use the below statement to add column NewColumn1 to our table SampleTable.\n\nAlso, you can add multiple columns to a table using the single SQL Server ALTER TABLE statement as below.\n\nWhen adding a column to a large table, typically you want to add the column and fill the added column with a default value. For example, if you want to add a status column, next is to fill the column with a default value.\n\nTo achieve the above objective, the easiest way is to add a column with a default constraint. When the default column is added, the default value will be added to the table. The following script makes use of SQL Server ALTER TABLE ADD Column (Status in our case) statement to add a column named Status with default constraint.\n\nWhen the column is added as above, the Status column will be added with the Value INC for all records.\n\nFrom the profiler following statics are captured via SQL Profiler during the column addition with default values.\n\nYou can see that the column is added to the table even less than one second and operation is very minimal cost.\n\nThe following are the locking stats during the column that are added with a constraint.\n\nPlease note that the following query should be executed in an open transaction in order to capture the above locking statistics.\n\nThis shows that Table (Object) has intended Exclusive lock which means that the table is not exclusively locked during the addition of the column. Also, adding a column with default value has not taken even a one minute though it has 500,000 records.\n\nLet us see the table size.\n\nYou will see that nothing has changed.\n\nLet us see the file sizes of the database.\n\nNothing much has changed to the data file as well as for the log file. All of these results indicate that adding a column with a default constraint will result in only a metadata change.\n\nLet us update the same column with a different value and let us get the same stats.\n\nThe above table shows that the table is exclusively locked which means that the table is not accessible during the update.\n\nLet us look at the table size.\n\nThe table has grown by some value as shown in the above figure.\n\nThe following are the details for the query expenses captured from the SQL Profiler.\n\nEvidently, when updating a column for a large table, resource consumption is high.\n\nThe following is the database file sizes when the column is added and updated the values.\n\nThese stats show that there is a remarkable difference between adding a column with a default value and adding a column and updating the column with a value later. During the column update, the transaction log will grow and exclusive locking will be placed on the table prohibiting any reads or writes to the table.\n\nWhat is the difference in these scenarios? Prior to the SQL Server 2012, when adding a column with default value will cause the same behavior. In SQL Server 2012 and onwards, this is no longer the situation, the column is added online to the table and no update occurs and it is only a metadata change.\n\nHow This is Achieved\n\nThis is achieved by a somewhat very simple but novel approach. sys.system_internals_partition_columns DMV has two additional columns named has_default and default_value as shown below.\n\nSo when the column added with a default value, it will not update the data page instead it will update this system table. When a row is updated, then the default value will be pushed to the table even if the default value column is not updated.\n\nNow, the next question is what if the Default constraint is dropped just after it is created. In that scenario, still, the above setting will prevail hence the previously set default value will be kept.\n\nWe covered the basic syntax of SQL Server ALTER TABLE in this article and implemented it to add columns to an existing table.\n\nBefore SQL Server 2012, there was no difference between adding a column with the default and adding a column and updating it. Therefore, in the prior SQL Server 2012 era, rather than adding a column with a default constraint, it is better to add a column and updating with batches so that the table is not exclusively locked.\n\nHowever, from SQL Server 2012 onwards, the approach has changed so that adding a column with default constraints is much better."
    }
]