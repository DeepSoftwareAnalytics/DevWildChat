[
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html",
        "document": ""
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/10min.html",
        "document": "This is a short introduction to pandas, geared mainly for new users. You can see more complex recipes in the Cookbook.\n\nCustomarily, we import as follows:\n\nSee the Intro to data structures section. Creating a by passing a NumPy array with a datetime index using and labeled columns: Creating a by passing a dictionary of objects where the keys are the column labels and the values are the column values. The columns of the resulting have different dtypes: If you’re using IPython, tab completion for column names (as well as public attributes) is automatically enabled. Here’s a subset of the attributes that will be completed: As you can see, the columns , , , and are automatically tab completed. and are there as well; the rest of the attributes have been truncated for brevity.\n\nUse and to view the top and bottom rows of the frame respectively: Return a NumPy representation of the underlying data with without the index or column labels: NumPy arrays have one dtype for the entire array while pandas DataFrames have one dtype per column. When you call , pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. If the common data type is , will require copying data.\n\nWhile standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, , , and . See the indexing documentation Indexing and Selecting Data and MultiIndex / Advanced Indexing. For a , passing a single label selects a columns and yields a equivalent to : See more in Selection by Label using or . For label slicing, both endpoints are included: For getting fast access to a scalar (equivalent to the prior method): See more in Selection by Position using or . Select via the position of the passed integers: For getting a value explicitly: For getting fast access to a scalar (equivalent to the prior method): Select rows where is greater than . Selecting values from a where a boolean condition is met: Setting a new column automatically aligns the data by the indexes: The result of the prior setting operations:\n\nFor NumPy data types, represents missing data. It is by default not included in computations. See the Missing Data section. Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data: drops any rows that have missing data: gets the boolean mask where values are :\n\nSee the Basic section on Binary Ops. Calculate the mean value for each column: Calculate the mean value for each row: Operating with another or with a different index or column will align the result with the union of the index or column labels. In addition, pandas automatically broadcasts along the specified dimension and will fill unaligned labels with . and applies a user defined function that reduces or broadcasts its result respectively. See more at Histogramming and Discretization. is equipped with a set of string processing methods in the attribute that make it easy to operate on each element of the array, as in the code snippet below. See more at Vectorized String Methods.\n\nBy “group by” we are referring to a process involving one or more of the following steps:\n• None Splitting the data into groups based on some criteria Grouping by a column label, selecting column labels, and then applying the function to the resulting groups:\n\nSee the sections on Hierarchical Indexing and Reshaping. The method “compresses” a level in the DataFrame’s columns: With a “stacked” DataFrame or Series (having a as the ), the inverse operation of is , which by default unstacks the last level: See the section on Pivot Tables. pivots a specifying the , and\n\npandas can include categorical data in a . For full docs, see the categorical introduction and the API documentation. Rename the categories to more meaningful names: Reorder the categories and simultaneously add the missing categories (methods under return a new by default): Sorting is per order in the categories, not lexical order: Grouping by a categorical column with also shows empty categories:\n\nIf you are attempting to perform a boolean operation on a or you might see an exception like: Traceback (most recent call last) in in : The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). See Comparisons and Gotchas for an explanation and what to do."
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/frame.html",
        "document": "Access a single value for a row/column pair by integer position. Access a group of rows and columns by label(s) or a boolean array. Insert column into DataFrame at specified location. Get the 'info axis' (see Indexing for more). Get item from object for given key (ex: DataFrame column). Whether each element in the DataFrame is contained in values. Replace values where the condition is False. Replace values where the condition is True. Query the columns of a DataFrame with a boolean expression. For more information on , , , and , see the indexing documentation.\n\nGet Addition of DataFrame and other, column-wise. Get Addition of dataframe and other, element-wise (binary operator ). Get Subtraction of dataframe and other, element-wise (binary operator ). Get Multiplication of dataframe and other, element-wise (binary operator ). Get Floating division of dataframe and other, element-wise (binary operator ). Get Floating division of dataframe and other, element-wise (binary operator ). Get Integer division of dataframe and other, element-wise (binary operator ). Get Modulo of dataframe and other, element-wise (binary operator ). Get Exponential power of dataframe and other, element-wise (binary operator ). Compute the matrix multiplication between the DataFrame and other. Get Addition of dataframe and other, element-wise (binary operator ). Get Subtraction of dataframe and other, element-wise (binary operator ). Get Multiplication of dataframe and other, element-wise (binary operator ). Get Floating division of dataframe and other, element-wise (binary operator ). Get Floating division of dataframe and other, element-wise (binary operator ). Get Integer division of dataframe and other, element-wise (binary operator ). Get Modulo of dataframe and other, element-wise (binary operator ). Get Exponential power of dataframe and other, element-wise (binary operator ). Get Less than of dataframe and other, element-wise (binary operator ). Get Greater than of dataframe and other, element-wise (binary operator ). Get Less than or equal to of dataframe and other, element-wise (binary operator ). Get Greater than or equal to of dataframe and other, element-wise (binary operator ). Get Not equal to of dataframe and other, element-wise (binary operator ). Get Equal to of dataframe and other, element-wise (binary operator ). Update null elements with value in the same location in .\n\nAlign two objects on their axes with the specified join method. Select values at particular time of day (e.g., 9:30AM). Select values between particular times of the day (e.g., 9:00-9:30 AM). Drop specified labels from rows or columns. Test whether two objects contain the same elements. Subset the dataframe rows or columns according to the specified index labels. Return index of first occurrence of maximum over requested axis. Return index of first occurrence of minimum over requested axis. Conform DataFrame to new index with optional filling logic. Return an object with matching indices as other object. Set the name of the axis for the index or columns. Reset the index, or a level of it. Return a random sample of items from an axis of object. Return the elements in the given positional indices along an axis. Truncate a Series or DataFrame before and after some index value.\n\n(DEPRECATED) Fill NA/NaN values by using the next valid observation to fill the gap. Fill NA/NaN values by using the next valid observation to fill the gap. Fill NA/NaN values by propagating the last valid observation to next valid. Fill NA/NaN values using the specified method. DataFrame.isnull is an alias for DataFrame.isna. DataFrame.notnull is an alias for DataFrame.notna. (DEPRECATED) Fill NA/NaN values by propagating the last valid observation to next valid. Replace values given in with .\n\nSort by the values along either axis. Sort object by labels (along an axis). Return the first rows ordered by in descending order. Return the first rows ordered by in ascending order. Stack the prescribed level(s) from columns to index. Transform each element of a list-like to a row, replicating index values. Return an xarray object from the pandas object. The transpose of the DataFrame.\n\nReturn the last row(s) without any NaNs before . Shift index by desired number of periods with an optional time . Return index for first non-NA value or None, if no non-NA value is found. Return index for last non-NA value or None, if no non-NA value is found. Cast to DatetimeIndex of timestamps, at beginning of period. Localize tz-naive index of a Series or DataFrame to target time zone.\n\nFlags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in ."
    },
    {
        "link": "https://developers.lseg.com/en/article-catalog/article/dataframe-manipulation-with-pandas-a-beginners-guide",
        "document": "Pandas, a very popular library in Python, helps us to work with data easily. You can think of it as a tool that allows us to play with data, like moving columns and rows in an Excel sheet. Pandas makes it easy to clean, modify, and analyze data, making it very useful in the data-related projects such as data science projects. DataFrame is like a table with rows and columns. It's similar to a table in SQL or an Excel spreadsheet, making it easy to work with structured data and allow you to organize data in a way that is easy to read and work with.\n\nIn this article, we're retrieving the data from LSEG Data Library for Python, which provides a set of ease-of-use interfaces offering coders uniform access to the breadth and depth of financial data and services available on the LSEG Data Platform. The API is designed to provide consistent access through multiple access channels and target both Professional Developers and Financial Coders. Developers can choose to access content from the desktop, through their deployed streaming services, or directly to the cloud. With the LSEG Data Library, the same Python code can be used to retrieve data regardless of which access point you choose to connect to the platform.\n\n The example code can be found in GitHub Example - Data Library Python, such as EX-1.01.01-GetData.ipynb. For example, let's retrieve data of MAMAA stocks (Meta, Amazon, Microsoft, Apple and Alphabet). To find the instruments and fields you're interested in, Data Item Browser can be used.\n\nBasic operations that are commonly used to explore and understand data by viewing, selecting, and filtering data. 1.1.1 ) See the first few rows of a dataframe using 'head()' - default is 5 rows and you can put the number of rows of data you want to get as DataFrame.head(n)\n\n - If n is negative value, the function returns all rows except the last |n| rows\n\n - If n is larger than the number of rows, this function returns all rows\n\n df.head() 1.1.2) Similar to head, to retrieve the last few rows of data, 'tail()' can be used\n\n df.tail()\n\nIn this section, let's explore more complex dataframe manipulations that can help us reshape and aggregate data in different ways. These techniques are useful when we want to pivot data or change the layout of our dataframe for specific analyzes. 6.1 ) Creating pivot tables with 'pivot_table()'. This allows us to summarize data and group it in various ways with ease.\n\n pivot = df.pivot_table(index='column1', columns='column2', values='column3', aggfunc='sum') Imagine we have the dataframe that contains last 3 years of revenue data of these 3 companies and we want to find out the total revenue of each company. The pivot table can be created like this.\n\n - Index='Instrument' specifies that the rows should be grouped by the 'Instrument' column\n\n - columns='Date' creates separate columns for each 'Date'\n\n - values='Revenue' indicates that the values in the table should be from 'Revenue' column\n\n - aggfunc='sum' sums up the revenue for each combination of instrument and date. This can also be changed to other functions like 'mean', 'min', or 'max' based on what is needed.\n\n6.2) Reshaping data with 'melt()' to change the format. This is useful to tranform the data from wide format (many columns) to a long format (fewer columns, more rows). This is often required when we need to prepare the data for plotting or more advanced analysis.\n\n Let's say we have the dataframe which each column represents a total return cross asset on 1, 3, and 5 years period then we we're transforming it into a long format where each row represents total return cross asset for a specific instrument in a specific number of years period, as below.\n\n - id_vars=['Instrument'], this column will remain unchanged, meaning the instrument RIC will stay the same for each row.\n\n - value_vars=[''1 Year Total Return Cross Asset, ... , '5 Year Total Return Cross Asset'], are the columns that will be transformed into rows.\n\n - var_name='Year' is the column name for the melted variable (inthis case, the year).\n\n - value_name='Total Return Cross Asset', is the name of the new column where the values will be sorted.\n\nIn this beginners' guide to dataframe manipulation with Pandas, we've covered the essential functions that are the backbone of data analysis in Python from loading data and inspecting it, to filtering, grouping, and transforming. Pandas provides a powerful toolkit that simplifies working with complex datasets. By using practical examples and applying these techniques to datasets, you should now have a solid foundation to manipulate dataframes. Whether you're cleaning data, performing exploratory analysis, or preparing data for machine learning models, mastering these Pandas basics will signigicantly enhance your data science capabilities.\n\n For further reading and more advanced tutorials, consider exploring the official Pandas documentation and using it with the datasets provided by LSEG via LSEG Data Library for Python."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/index.html",
        "document": "The User Guide covers all of pandas by topic area. Each of the subsections introduces a topic (such as “working with missing data”), and discusses how pandas approaches the problem, with many examples throughout.\n\nUsers brand-new to pandas should start with 10 minutes to pandas.\n\nFor a high level summary of the pandas fundamentals, see Intro to data structures and Essential basic functionality.\n\nFurther information on any specific method can be obtained in the API reference.\n\nHow to read these guides# In these guides you will see input code inside code blocks such as: The first block is a standard python input, while in the second the indicates the input is inside a notebook. In Jupyter Notebooks the last line is printed and plots are shown inline."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/missing_data.html",
        "document": "pandas uses different sentinel values to represent a missing (also referred to as NA) depending on the data type. for NumPy data types. The disadvantage of using NumPy data types is that the original data type will be coerced to or . for NumPy , , and . For typing applications, use . for , (and other bit widths), and . These types will maintain the original data type of the data. For typing applications, use . To detect these missing value, use the or methods. or will also consider a missing value. Equality compaisons between , , and do not act like Therefore, an equality comparison between a or with one of these missing values does not provide the same information as or .\n\nExperimental: the behaviour of can still change without warning. Starting from pandas 1.0, an experimental value (singleton) is available to represent scalar missing values. The goal of is provide a “missing” indicator that can be used consistently across data types (instead of , or depending on the data type). For example, when having missing values in a with the nullable integer dtype, it will use : Currently, pandas does not yet use those data types using by default a or , so you need to specify the dtype explicitly. An easy way to convert to those dtypes is explained in the conversion section. In general, missing values propagate in operations involving . When one of the operands is unknown, the outcome of the operation is also unknown. For example, propagates in arithmetic operations, similarly to : There are a few special cases when the result is known, even when one of the operands is . In equality and comparison operations, also propagates. This deviates from the behaviour of , where comparisons with always return . To check if a value is equal to , use An exception on this basic propagation rule are reductions (such as the mean or the minimum), where pandas defaults to skipping missing values. See the calculation section for more. For logical operations, follows the rules of the three-valued logic (or Kleene logic, similarly to R, SQL and Julia). This logic means to only propagate missing values when it is logically required. For example, for the logical “or” operation ( ), if one of the operands is , we already know the result will be , regardless of the other value (so regardless the missing value would be or ). In this case, does not propagate: On the other hand, if one of the operands is , the result depends on the value of the other operand. Therefore, in this case propagates: The behaviour of the logical “and” operation ( ) can be derived using similar logic (where now will not propagate if one of the operands is already ): Since the actual value of an NA is unknown, it is ambiguous to convert NA to a boolean value. Traceback (most recent call last) in : boolean value of NA is ambiguous This also means that cannot be used in a context where it is evaluated to a boolean, such as where can potentially be . In such cases, can be used to check for or being can be avoided, for example by filling missing values beforehand. A similar situation occurs when using or objects in statements, see Using if/truth statements with pandas. implements NumPy’s protocol. Most ufuncs work with , and generally return : Currently, ufuncs involving an ndarray and will return an object-dtype filled with NA values. The return type here may change to return a different array type in the future. See DataFrame interoperability with NumPy functions for more on ufuncs. If you have a or using , and in that can convert data to use the data types that use such as or . This is especially helpful after reading in data sets from IO methods where data types were inferred. In this example, while the dtypes of all columns are changed, we show the results for the first 10 columns.\n\nNA values can be replaced with corresponding value from a or where the index and column aligns between the original object and the filled object. can also be used to fill NA values.Same result as above. and fills NA values using various interpolation methods. Interpolation relative to a in the is available by setting If you have scipy installed, you can pass the name of a 1-d interpolation routine to . as specified in the scipy interpolation documentation and reference guide. The appropriate interpolation method will depend on the data type. If you are dealing with a time series that is growing at an increasing rate, use . If you have values approximating a cumulative distribution function, use . To fill missing values with goal of smooth plotting use . When interpolating via a polynomial or spline approximation, you must also specify the degree or order of the approximation: Interpolating new observations from expanding data with . accepts a keyword argument to limit the number of consecutive values filled since the last valid observation By default, values are filled in a direction. Use parameter to fill or from directions. By default, values are filled whether they are surrounded by existing valid values or outside existing valid values. The parameter restricts filling to either inside or outside values. # fill one consecutive inside value in both directions # fill all consecutive outside values in both directions and can be used similar to and to replace or insert missing values. Replacing more than one value is possible by passing a list. Python strings prefixed with the character such as are “raw” strings. They have different semantics regarding backslashes than strings without this prefix. Backslashes in raw strings will be interpreted as an escaped backslash, e.g., . Replace the ‘.’ with with regular expression that removes surrounding whitespace Pass nested dictionaries of regular expressions that use the keyword. Pass a list of regular expressions that will replace matches with a scalar. All of the regular expression examples can also be passed with the argument as the argument. In this case the argument must be passed explicitly by name or must be a nested dictionary. A regular expression object from is a valid input as well."
    },
    {
        "link": "https://geeksforgeeks.org/working-with-missing-data-in-pandas",
        "document": "In Pandas, missing values are represented by None or NaN, which can occur due to uncollected data or incomplete entries. Let’s explore how to detect, handle, and fill in missing values in a DataFrame to ensure accurate analysis.\n\n\n\nTo identify and handle the missing values, Pandas provides two useful functions: isnull() and notnull(). These functions help detect whether a value is NaN or not, making it easier to clean and preprocess data in a DataFrame or Series.\n\nisnull() returns a DataFrame of Boolean values, where True represents missing data (NaN). This is useful when you want to locate and address missing data within a dataset.\n\nIn this case, the isnull() function is applied to the “Gender” column to filter and display rows with missing gender information.\n\nnotnull() returns a DataFrame of Boolean values, where True indicates non-missing data. This function can be useful when you want to focus on the rows that contain valid, non-missing data.\n\nThis code snippet uses the notnull() function to filter out rows where the “Gender” column does not have missing values.\n\nFilling Missing Values in Pandas Using fillna(), replace(), and interpolate()\n\nWhen working with missing data in Pandas, the fillna(), replace(), and interpolate() functions are commonly used to fill NaN values. These functions allow you to replace missing values with a specific value or use interpolation techniques.\n\n1. Filling Missing Values with a Specific Value Using fillna()\n\nThe fillna() function is used to replace missing values (NaN) with a specified value. For example, you can fill missing values with 0.\n\nExample: Fill Missing Values with Zero\n\n2. Filling Missing Values with the Prev/Next Value Using fillna\n\nYou can use the pad method to fill missing values with the previous value, or bfill to fill with the next value. We will be using the above dataset for the demonstration.\n\nExample: Fill with Previous Value (Forward Fill)\n\nExample: Fill with Next Value (Backward Fill)\n\nExample: Fill NaN Values with ‘No Gender’ using fillna()\n\nDownload the csv file from here.\n\nNow we are going to fill all the null values in Gender column with “No Gender”\n\nUse replace() to replace NaN values with a specific value like -99.\n\nNow, we are going to replace the all Nan value in the data frame with -99 value.\n\nThe interpolate() function fills missing values using interpolation techniques, such as the linear method.\n\nLet’s interpolate the missing values using Linear method. Note that Linear method ignore the index and treat the values as equally spaced.\n\nThis method fills missing values by treating the data as equally spaced.\n\nThe dropna()function in Pandas removes rows or columns with NaN values. It can be used to drop data based on different conditions.\n\n1. Dropping Rows with At Least One Null Value\n\nUse dropna() to remove rows that contain at least one missing value.\n\nExample: Drop Rows with At Least One NaN\n\nYou can drop rows where all values are missing using dropna(how=’all’).\n\nExample: Drop Rows with All NaN Values\n\n3. Dropping Columns with At Least One Null Value\n\nTo remove columns that contain at least one missing value, use dropna(axis=1).\n\nExample: Drop Columns with At Least One NaN\n\nWhen working with data from CSV files, you can drop rows with missing values using dropna().\n\nExample: Drop Rows with NaN in a CSV File\n\nSince the difference is 236, there were 236 rows which had at least 1 Null value in any column.\n\nHow to get rows with missing data in pandas?\n\nHow to handle missing data in a dataset?\n\nHow to fill missing values in pandas using mean?\n\nWhat are some methods to handle missing or corrupted data?\n\nHow to count missing values in pandas?"
    },
    {
        "link": "https://stackoverflow.com/questions/54618253/how-to-deal-with-missing-values-in-pandas-dataframe",
        "document": "I have a Pandas Dataframe that has some missing values. I would like to fill the missing values with something that doesn't influence the statistics that I will do on the data.\n\nAs an example, if in Excel you try to average a cell that contains 5 and an empty cell, the average will be 5. I'd like to have the same in Python.\n\nI tried to fill with but if I sum a certain column, for example, the result is . I also tried to fill with None but I get an error because I'm summing different datatypes.\n\nCan somebody help? Thank you in advance."
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html",
        "document": "Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list.\n\nMethod to use for filling holes in reindexed Series:\n• None ffill: propagate last valid observation forward to next valid.\n• None backfill / bfill: use next valid observation to fill gap. Deprecated since version 2.1.0: Use ffill or bfill instead.\n\nAxis along which to fill missing values. For this parameter is unused and defaults to 0.\n\nIf True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame).\n\nIf method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None.\n\nA dict of item->dtype of what to downcast if possible, or the string ‘infer’ which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible)."
    },
    {
        "link": "https://medium.com/@abhishekjainindore24/handling-missing-data-in-pandas-part-1-71547b1a55ad",
        "document": "In this blog, we are gonna look at some of the methods by which we can handle missing data. Let’s look at some of these:\n\nUse to remove rows or columns with missing values\n\nUse to replace missing values with a specified value.\n\nWe can fill missing values by\n• Mean of the column\n\nForward-fill copies the previous non-null value to fill missing values, while backward-fill copies the next non-null value.\n\n# Fill missing values with 0 (Constant value)\n\ndf_filled = df.fillna(0)\n\n\n\n# Fill missing values with the mean of the column\n\ndf_filled_mean = df.fillna(df.mean())\n\n\n\n# Filling missing values with the median of the column\n\ndf_filled_median = df.fillna(df.median())\n\n\n\n# Filling missing values with the most frequent value in the column (mode)\n\ndf_filled_mode = df.fillna(df.mode())\n\n\n\n# Forward-fill missing values\n\ndf_filled_forward = df.fillna(method='ffill')\n\n\n\n# Backward-fill missing values\n\ndf_filled_backward = df.fillna(method='bfill')\n\n\n\nprint(\"DataFrame after filling with 0:\")\n\nprint(df_filled)\n\n'''\n\nOutput:\n\nDataFrame after filling with 0:\n\n A B\n\n0 1.0 5.0\n\n1 2.0 0.0\n\n2 0.0 7.0\n\n3 4.0 8.0\n\n'''\n\n\n\nprint(\"\n\nDataFrame after filling with mean values:\")\n\nprint(df_filled_mean)\n\n'''\n\nOutput:\n\nDataFrame after filling with mean values:\n\n A B\n\n0 1.000000 5.000000\n\n1 2.000000 6.666667\n\n2 2.333333 7.000000\n\n3 4.000000 8.000000\n\n'''\n\n\n\nprint(\"\n\nDataFrame after filling with median values:\")\n\nprint(df_filled_median)\n\n'''\n\nOutput:\n\nDataFrame after filling with median values:\n\n A B\n\n0 1.0 5.0\n\n1 2.0 7.0\n\n2 2.0 7.0\n\n3 4.0 8.0\n\n'''\n\n\n\nprint(\"\n\nDataFrame after filling with mode values:\")\n\nprint(df_filled_mode)\n\n'''\n\nOutput:\n\nDataFrame after filling with mode values:\n\n A B\n\n0 1.0 5.0\n\n1 2.0 7.0\n\n2 4.0 7.0\n\n3 4.0 8.0\n\n'''\n\n\n\nprint(\"\n\nDataFrame after filling with forward-fill values:\")\n\nprint(df_filled_forward)\n\n'''\n\nOutput\n\nDataFrame after filling with forward-fill values:\n\n A B\n\n0 1.0 5.0\n\n1 2.0 5.0\n\n2 2.0 7.0\n\n3 4.0 8.0\n\n'''\n\n\n\nprint(\"\n\nDataFrame after filling with backward-fill values:\")\n\nprint(df_filled_backward)\n\n'''\n\nOutput\n\nDataFrame after filling with backward-fill values:\n\n A B\n\n0 1.0 5.0\n\n1 2.0 7.0\n\n2 4.0 7.0\n\n3 4.0 8.0\n\n'''\n\nReplace specific values indicating missing data with NaN or None.\n\nApply custom functions based on your domain knowledge to fill missing values."
    }
]