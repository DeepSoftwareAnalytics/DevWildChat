[
    {
        "link": "https://github.com/potree/potree",
        "document": "\n• Potree is a free open-source WebGL based point cloud renderer for large point clouds. It is based on the TU Wien Scanopy project and research projects Harvest4D, GCD Doctoral College and Superhumans.\n• Newest information and work in progress is usually available on twitter\n\nInstall dependencies, as specified in package.json, and create a build in ./build/potree.\n\nUse the command to\n• watch for changes to the source code and automatically create a new build on change\n\nGo to http://localhost:1234/examples/ to test the examples.\n• Simply upload the Potree folderm with all your point clouds, the build directory, and your html files to a web server.\n• It is not required to install node.js on your webserver. All you need is to host your files online.\n\nDownload PotreeConverter and run it like this:\n\nCopy the converted directory into <potreeDirectory>/pointclouds/data_converted. Then, duplicate and rename one of the examples and modify the path in the html file to your own point cloud.\n• PotreeConverter - Convert your point cloud to the Potree format.\n• PotreeDesktop - Desktop version of Potree. Allows drag&drop of point clouds into the viewer.\n\nPotree is funded by a combination of research projects, companies and institutions.\n\nWe would like to thank our sponsors for their financial contributions that keep this project up and running!\n• The multi-res-octree algorithms used by this viewer were developed at the Vienna University of Technology by Michael Wimmer and Claus Scheiblauer as part of the Scanopy Project.\n• Three.js, the WebGL 3D rendering library on which potree is built.\n• plas.io point cloud viewer. LAS and LAZ support have been taken from the laslaz.js implementation of plas.io. Thanks to Uday Verma and Howard Butler for this!\n• Harvest4D Potree currently runs as Master Thesis under the Harvest4D Project\n• Christian Boucheny (EDL developer) and Daniel Girardeau-Montaut (CloudCompare). The EDL shader was adapted from the CloudCompare source code!\n• Martin Isenburg, Georepublic, Veesus, Sigeom Sa, SITN, LBI ArchPro, Pix4D as well as all the contributers to potree and PotreeConverter and many more for their support."
    },
    {
        "link": "https://linkedin.com/pulse/complete-guide-potrees-interface-functionalities-abderrazzaq",
        "document": "If you don't already have a point cloud and wish to obtain one, I recommend utilizing Geotiles for data from the Netherlands, LiDAR HD for data from France or the 3DE program for data from the USA. These sources offer valuable point cloud data that you can use for your projects. However, if you already have your own data, that's even better as it allows you to work with familiar datasets tailored to your needs. You can also manipulate some Potree examples just in your browser (Entwine & Potree).\n• Point Budget: This parameter allows users to set the number of points considered for display, influencing the point density on the screen. While it's not the sole factor affecting point density, adjusting the point budget helps optimize the level of detail for efficient visualization. The maximum is set to 10 Million, but it can be adjusted in the code.\n• Field of view: It determines the extent of the scene visible from the viewer's perspective. The optimal FoV is set to 60.\n• Eye Dome Lighting: By enabling Eye Dome Lighting, you benefit from an enhanced shading calculation method that sharpens edges, improving the perception of depth. You can manage the darkening effect caused by eye dome lighting by adjusting the trade between the Radius and Strength. This tool is very useful when dealing with uncolored point clouds, as shown in the next example.\n\nMinimum node size: This setting defines the size of the Octree nodes. Potree utilizes an innovative point-storing technique based on an octree data structure. This structure subdivides the space into cubes of varying sizes, each with a specific level of detail. As you navigate through the point cloud, Potree dynamically loads and displays cubes based on the camera and target positions, optimizing performance even for vast point clouds (you've probably already noticed). By adjusting this parameter you can control the size of these cubes and the level of detail displayed. Smaller node sizes offer higher detail, providing a more intricate view of your data. On the other hand, if you encounter performance issues, increasing the \"Min Node Size\" value can enhance rendering speed by displaying fewer cubes, especially for dense point clouds. But, be careful to not be confused it with the \"Point Budget\" even if it influences the number of points also.\n• Earth control: Hover over the point clouds and left-click to move, or right-click to rotate the model. A colored circle appears at the initial click point, representing the center of rotation. Zooming is achieved using the mouse wheel.\n• Fly control: Left-click to steer the flight direction and use keyboard arrow keys to move forward or backward. Right-click combined with movement enables the panning of the view. The mouse wheel, in this mode, is utilized to accelerate or decelerate the movement.\n• Orbit: Left click, 3D rotation around the current center (possibility to adjust the center with a double-click on one of the points of the point cloud, click right = Pan\n• Full extents: Zoom extends to the point cloud limits; 6. Cube: display the view cube; 7. Compass: for orientation; 8. Create a camera animation.\n\nThis latest version of Potree supports all the attributes that are stored in the LAS file according to ASPRS specifications. It even allows you to add additional attributes if your point cloud contains them. We give an example if your point cloud contains a tree identifier, then you'll have the trees displayed with different colors. Other attributes, such as level of detail, don't come at the time of conversion but are calculated on the fly according to the camera's field of view.\n\nAs you may have noticed, we only manipulate point clouds. However, other types of data such as oriented images, vector data, and meshes can be added to your Potree viewer. As this requires customization of the viewer, these data must be added to the HTML page. This is what we'll be looking at in the next few articles."
    },
    {
        "link": "https://giro3d.org/next/examples/potree_pointcloud.html",
        "document": "// Let's change the classification color with the color picker value // This path is specific to your project, and must be set accordingly. // We use this CRS when the point cloud does not have a CRS defined. // It is technically the WebMercator CRS, but we label it 'unknown' to make // it very explicit that it is not correct. // Let's enable Eye Dome Lighting to make the point cloud more readable. // Create the color map. The color ramp and bounds will be set later. // Let's populate the classification list with default values from the ASPRS classifications. // Let's change the classification color with the color picker value // By toggling the .visible property of a classification, // all points that have this classification are hidden/shown."
    },
    {
        "link": "https://community.opendronemap.org/t/aligning-360-images-with-point-clouds-in-potree/22155",
        "document": "This is a cumulative update that brings 3D point cloud and textured model displa…y, as well as miscellaneous fixes and additions to the UI to allow users to navigate to the new areas of the UI. @mojodna would you have some time to look over these changes? I plan to merge these by Tuesday."
    },
    {
        "link": "https://laserscanningforum.com/forum/viewtopic.php?t=19153",
        "document": "To chat about anything else.\n\n\n\nI'm looking for a bit of help regarding the hosting of clouds that I can easily share with clients through a browser link. \n\n \n\n I'm looking to avoid any third party cloud hosting costs, as for now at least this is not a common thing for me. So I've got Potree setup and running, got the clouds converted and viewable locally through XAMPP (took me a while as I'm new to this with zero coding experience but muddled through) I'm now at the 'just stick it all on a webserver and you're done' stage, which is all the instruction I've found so far, but it has me stumped. \n\n \n\n I have got VPS, just a cheap one to trial on but it has enough storage for a few tests, used cPanel to upload all the necessary files to it, aaand....? Whats next? \n\n \n\n Again, zero experience with coding or servers so forgive me if this next step is staring me in the face. I feel like I just need to alter a pathway in the .html file, but to what? And do I need to set the server up in some way to allow access to it via a .html link?\n\n \n\n Any advice will be much appreciated\n\n \n\n Thanks\n\nI have got VPS, just a cheap one to trial on but it has enough storage for a few tests, used cPanel to upload all the necessary files to it, aaand....? Whats next? \n\n \n\n Again, zero experience with coding or servers so forgive me if this next step is staring me in the face. I feel like I just need to alter a pathway in the .html file, but to what? And do I need to set the server up in some way to allow access to it via a .html link? I have got VPS, just a cheap one to trial on but it has enough storage for a few tests, used cPanel to upload all the necessary files to it, aaand....? Whats next?Again, zero experience with coding or servers so forgive me if this next step is staring me in the face. I feel like I just need to alter a pathway in the .html file, but to what? And do I need to set the server up in some way to allow access to it via a .html link? If you're using a VPS you need to make sure your potree structure and point cloud are in a folder that is linked to your main HTML page. This varies, but on the one I use, there is a folder called public_html off the root which is normally where you would put web pages. You will need to know the domain name for your VPS which will typically point to that folder. So for example I have potree sample at If you're using a VPS you need to make sure your potree structure and point cloud are in a folder that is linked to your main HTML page. This varies, but on the one I use, there is a folder called public_html off the root which is normally where you would put web pages. You will need to know the domain name for your VPS which will typically point to that folder. So for example I have potree sample at https://www.atlas-files.com/potree/potest.html which in my FTP systems appears in the folder /public_html/potree off the root on my www.atlas-files.com domain. Hope this makes sense.\n\nThat does make sense, thanks for the response. \n\n \n\n My issue now then is I can't currently access the server through a browser, to view the files as an index like your example. I think I've got some domain issues to figure out. I'd like to link this server 'database' to my existing site as a subdomain, which may be getting a little off topic...\n\n \n\n Assuming I can figure that out, I just need the project folder (containing the hierarchy, metadata and octree files) and the project.html to both be in the public_root folder (or a subfolder within I assume?) and we're good to go?\n\n \n\n Thanks again\n\nThat does make sense, thanks for the response. \n\n \n\n My issue now then is I can't currently access the server through a browser, to view the files as an index like your example. I think I've got some domain issues to figure out. I'd like to link this server 'database' to my existing site as a subdomain, which may be getting a little off topic...\n\n \n\n Assuming I can figure that out, I just need the project folder (containing the hierarchy, metadata and octree files) and the project.html to both be in the public_root folder (or a subfolder within I assume?) and we're good to go?\n\n \n\n Thanks again That does make sense, thanks for the response.My issue now then is I can't currently access the server through a browser, to view the files as an index like your example. I think I've got some domain issues to figure out. I'd like to link this server 'database' to my existing site as a subdomain, which may be getting a little off topic...Assuming I can figure that out, I just need the project folder (containing the hierarchy, metadata and octree files) and the project.html to both be in the public_root folder (or a subfolder within I assume?) and we're good to go?Thanks again Yep, when you buy a shared server it's publicly visible address is usually quite long and based around the hosting company domain name, server address and root folder. You then buy a domain name and point it to that address as an alias and wait for the DNS sites to catch up. Many hosting companies offer this as part of the overall package. Yep, when you buy a shared server it's publicly visible address is usually quite long and based around the hosting company domain name, server address and root folder. You then buy a domain name and point it to that address as an alias and wait for the DNS sites to catch up. Many hosting companies offer this as part of the overall package.\n\n\n\n \n\n I think... don't ask me to do it again..\n\n \n\n Anyway I can now see all the files on my server \n\n \n\n I'm converting with potreeconverter_2.1 into \"\\xampp\\htdocs\\potree\" to run on the localhost, and just copying the html and the three pointcloud files hierarchy, metadata and octree onto the server. I notice the files in your \"/pointclouds/potest\" folder are different to those three? Thanks again for the help Shane, I've got it. I created a subdomain on my little server and connected it back to my main domain by adding the A record to my main domains DNS providerI think... don't ask me to do it again..Anyway I can now see all the files on my server http://clouds.c-se.co.uk/ but, I'm having some trouble with the structure to get Potree to work. I have tried copying the pathway for my test project as it is on my localhost (Xampp which works), plus I've tried some other configurations but all i get from the html is a blank screen, not even Potree running without a cloud?I'm converting with potreeconverter_2.1 into \"\\xampp\\htdocs\\potree\" to run on the localhost, and just copying the html and the three pointcloud files hierarchy, metadata and octree onto the server. I notice the files in your \"/pointclouds/potest\" folder are different to those three?\n\nThanks again for the help Shane, I've got it. I created a subdomain on my little server and connected it back to my main domain by adding the A record to my main domains DNS provider\n\n \n\n I think... don't ask me to do it again..\n\n \n\n Anyway I can now see all the files on my server \n\n \n\n I'm converting with potreeconverter_2.1 into \"\\xampp\\htdocs\\potree\" to run on the localhost, and just copying the html and the three pointcloud files hierarchy, metadata and octree onto the server. I notice the files in your \"/pointclouds/potest\" folder are different to those three? Thanks again for the help Shane, I've got it. I created a subdomain on my little server and connected it back to my main domain by adding the A record to my main domains DNS providerI think... don't ask me to do it again..Anyway I can now see all the files on my server http://clouds.c-se.co.uk/ but, I'm having some trouble with the structure to get Potree to work. I have tried copying the pathway for my test project as it is on my localhost (Xampp which works), plus I've tried some other configurations but all i get from the html is a blank screen, not even Potree running without a cloud?I'm converting with potreeconverter_2.1 into \"\\xampp\\htdocs\\potree\" to run on the localhost, and just copying the html and the three pointcloud files hierarchy, metadata and octree onto the server. I notice the files in your \"/pointclouds/potest\" folder are different to those three? Your structure online is not right.\n\n If you are on the site:\n\n http://clouds.c-se.co.uk/potree/Tewin%20Hill2/\n\n You can see the TewinHill2.html file and the two subfolders, but the libs folder is empty.\n\n Once this is filled with all the stuff which goes in there it should work \n\n \n\n Edit:\n\n The structure must look like this\n\n - libs\n\n ---- brotli\n\n ---- ...\n\n ---- potree\n\n ---- ...\n\n - pointclouds\n\n ---- projectName\n\n ---- ---- hierarchy.bin\n\n ---- ---- metadata.json\n\n ---- ---- octree.bin\n\n - project.html Your structure online is not right.If you are on the site:You can see the TewinHill2.html file and the two subfolders, but the libs folder is empty.Once this is filled with all the stuff which goes in there it should workEdit:The structure must look like this- libs---- brotli---- ...---- potree---- ...- pointclouds---- projectName---- ---- hierarchy.bin---- ---- metadata.json---- ---- octree.bin- project.html\n\nPerfect! I felt it would probably be something simple I was overlooking, thanks for the assistance!\n\n \n\n One last thing while I'm here, and apologies if this is an old topic, I'm experimenting now with tweaking the display settings to improve the look of the clouds, does anyone have any tips on their preferred settings? Does anyone use 'standard profiles' for the decimation of the cloud vs the point size for example? And how does it vary on say an aerial earthworks project vs a detailed building facade? I'll figure out my preferences in time but any tips would be welcome\n\n \n\n Thanks again\n\nPerfect! I felt it would probably be something simple I was overlooking, thanks for the assistance!\n\n \n\n One last thing while I'm here, and apologies if this is an old topic, I'm experimenting now with tweaking the display settings to improve the look of the clouds, does anyone have any tips on their preferred settings? Does anyone use 'standard profiles' for the decimation of the cloud vs the point size for example? And how does it vary on say an aerial earthworks project vs a detailed building facade? I'll figure out my preferences in time but any tips would be welcome\n\n \n\n Thanks again Perfect! I felt it would probably be something simple I was overlooking, thanks for the assistance!One last thing while I'm here, and apologies if this is an old topic, I'm experimenting now with tweaking the display settings to improve the look of the clouds, does anyone have any tips on their preferred settings? Does anyone use 'standard profiles' for the decimation of the cloud vs the point size for example? And how does it vary on say an aerial earthworks project vs a detailed building facade? I'll figure out my preferences in time but any tips would be welcomeThanks again So the decimation when generating the potree files is one thing. It is basically data size + loading speed vs looks.\n\n If you decimate more points, the cloud is more sparse and might look worse.\n\n \n\n The settings for the viewer itself are total preferences.\n\n I think I normally use point adapting, but I know people who prefer fixed, but set the size of the points to 3.\n\n Generally I prefer circles over squares. So the decimation when generating the potree files is one thing. It is basically data size + loading speed vs looks.If you decimate more points, the cloud is more sparse and might look worse.The settings for the viewer itself are total preferences.I think I normally use point adapting, but I know people who prefer fixed, but set the size of the points to 3.Generally I prefer circles over squares.\n\nI have got VPS, just a cheap one to trial on but it has enough storage for a few tests, used cPanel to upload all the necessary files to it, aaand....? Whats next? \n\n \n\n Again, zero experience with coding or servers so forgive me if this next step is staring me in the face. I feel like I just need to alter a pathway in the .html file, but to what? And do I need to set the server up in some way to allow access to it via a .html link? I have got VPS, just a cheap one to trial on but it has enough storage for a few tests, used cPanel to upload all the necessary files to it, aaand....? Whats next?Again, zero experience with coding or servers so forgive me if this next step is staring me in the face. I feel like I just need to alter a pathway in the .html file, but to what? And do I need to set the server up in some way to allow access to it via a .html link? If you're using a VPS you need to make sure your potree structure and point cloud are in a folder that is linked to your main HTML page. This varies, but on the one I use, there is a folder called public_html off the root which is normally where you would put web pages. You will need to know the domain name for your VPS which will typically point to that folder. So for example I have potree sample at If you're using a VPS you need to make sure your potree structure and point cloud are in a folder that is linked to your main HTML page. This varies, but on the one I use, there is a folder called public_html off the root which is normally where you would put web pages. You will need to know the domain name for your VPS which will typically point to that folder. So for example I have potree sample at https://www.atlas-files.com/potree/potest.html which in my FTP systems appears in the folder /public_html/potree off the root on my www.atlas-files.com domain. Hope this makes sense. \n\n Thanks again, it's all clicked now and working well, I think some of my clients are frankly scared at how furturistic it all is suddenly!\n\n \n\n The next thing that will really kick it on is the addition of linework into the model. smacl, your example from above \n\n \n\n Creating the design linework is no problem (dwg, dxf, xml, I'm sure I can convert it into anything else that may be required) can anyone provide me with some direction on the process of getting it added to the potree model? What format does it need to be in? I can see linework and text in the eg above, is the whitelining a different object? Can 3d triangles / tin surfaces also be displayed?\n\n \n\n Thanks Thanks again, it's all clicked now and working well, I think some of my clients are frankly scared at how furturistic it all is suddenly!The next thing that will really kick it on is the addition of linework into the model. smacl, your example from above https://www.atlas-files.com/potree/potest.html shows exactly what I'm after.Creating the design linework is no problem (dwg, dxf, xml, I'm sure I can convert it into anything else that may be required) can anyone provide me with some direction on the process of getting it added to the potree model? What format does it need to be in? I can see linework and text in the eg above, is the whitelining a different object? Can 3d triangles / tin surfaces also be displayed?Thanks"
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/API/WebXR_Device_API/Fundamentals",
        "document": "WebXR, with the WebXR Device API at its core, provides the functionality needed to bring both augmented and virtual reality (AR and VR) to the web. Together, these technologies are referred to as mixed reality (MR) or cross reality (XR). Mixed reality is a large and complex subject, with much to learn and many other APIs to bring together to create an engaging experience for users. This guide provides an overview of what WebXR is and how it works, as well as the preliminary foundation needed to start developing augmented and virtual reality experiences for the web.\n\nWhat WebXR is and isn't WebXR is an API for web content and apps to use to interface with mixed reality hardware such as VR headsets and glasses with integrated augmented reality features. This includes both managing the process of rendering the views needed to simulate the 3D experience and the ability to sense the movement of the headset (or other motion-sensing gear) and provide the needed data to update the imagery shown to the user. WebXR additionally provides support for accepting inputs from control devices such as handheld VR controllers or specialized mixed reality gamepads. WebXR is not a rendering technology and does not provide features for managing 3D data or rendering it to the display. This is an important fact to keep in mind. While WebXR manages the timing, scheduling, and the various points of view relevant when drawing the scene, it does not know how to load and manage models, nor how to render and texture them, and so forth. That part is entirely up to you. Fortunately, WebGL and the various WebGL-based frameworks and libraries are available to make it much easier to deal with all of that.\n\nHow is WebXR different from WebVR? WebVR was considered an experimental API designed to help specification writers determine the best approaches for creating a virtual reality API on the Web. Browser implementors added WebVR support to browsers, allowing web developers to experiment. But soon it became clear that to finish an API for virtual reality on the web, it would make more sense to start a new specification than to try to \"fix\" WebVR. That led to the birth of WebXR. The fundamental difference is that WebXR supports not only virtual reality, but also augmented reality, which blends virtual objects with the user's ambient environment. Another key difference is that WebXR has integrated support for the advanced input controllers that are used with most mixed reality headsets, while WebVR relied on the Gamepad API to support the controllers. In WebXR, the primary select and squeeze actions are directly supported using events, while other controls are available through a special WebXR-specific implementation of the object.\n\nThe term field of view (FOV) is one which applies to any visual technology, from old film cameras to modern digital video cameras, including the cameras in computers and mobile devices. What is field of view? The field of view is the extent to which you are able to see the environment. The width of the field of view, specified in either degrees or radians, is measured as the angle defining the arc from the far left edge of your field of view to the far right edge. A human eye is able to take in a FOV of around 135°. Assuming a person has two healthy eyes, the total field of view ends up being about 200° to 220° wide. Why is the FOV wider with two eyes, but not double the single-eye FOV? It's because the two eyes' FOVs overlap a lot. That overlap gives us depth perception, which is around 115° wide. Outside the overlap area, our vision falls back to monocular. The drawing shown here demonstrates the concept of FOV: blue wedge for the left eye, red wedge for the right eye. The light brown overlapping area is where the viewer has binocular vision and can perceive depth. If you look carefully, you'll see that each eye sees the die slightly differently, and the combined view blends the two into a 3D shape. Generally, applications only define and manage the horizontal FOV. For more details, see The optics of 3D. To achieve a wide enough field of view that the user's eyes are tricked into believing that the virtual world completely surrounds them, the FOV needs to at least approach the width of the binocular vision area. Basic headsets typically start around 90° or so, while the best headsets generally have a field of view of around 150°. Because the FOV is a matter of the size of the lenses and how close they are to the user's eyes, there are limitations on how wide the FOV can get without installing lenses into the user's eyeballs. A wide FOV can substantially improve the user's sense of immersion. However, increasing the FOV can also increase the weight and cost of the headset.\n\nThe term degrees of freedom is an indication of how much freedom of movement the user has within the virtual world. This is directly related to how many types of movement the WebXR hardware configuration is capable of recognizing and reproducing into the virtual scene. Figure: Diagram showing the movements possible with 3 degree of freedom hardware: yaw, roll, and pitch. The first three degrees of freedom are rotational. The rotational degrees of freedom are:\n• Pitch: looking up and down\n• Yaw: looking left and right In all of these cases, the viewer remains in the same location in space while pivoting on one or more of the three axes to alter the direction in which they're looking. A system with two degrees of freedom can sense when the user looks left and right or up and down, but can't report any other kind of movement. A typical baseline headset offers three degrees of freedom, recognizing rotation around all three axes. This is often referred to by the shorthand 3DoF. The other three degrees of freedom are translational, providing the ability to sense movement through space: forward and backward, left and right, up and down. Support for all six degrees of freedom is referred to as 6DoF. Some more advanced headsets provide at least minimal support for translational movement detection, but to capture more substantial movement through the space, external sensors are usually required, such as cameras (either using visible light or infrared).\n\nWebXR offers support for both augmented reality (AR) and virtual reality (VR) sessions, using the same API. Which type of session you want to create is specified when creating the session. This is done by specifying the appropriate session mode string for the kind of session you want to create. In a VR environment, the entire image is digitally created by your app or site, from foreground objects all the way to the background or skybox. Your frame drawing code will have to redraw every pixel of each view during each frame in order to avoid the potential of artifacts being left behind. Some platforms may provide previously-cleared frames to you, while others may optimize performance by not erasing the framebuffers in order to avoid having to touch each pixel twice per frame. There are two VR session modes available in WebXR: inline and immersive. The former, specified by the session mode string , presents the rendered scene within the context of a document in a web browser, and doesn't require special XR hardware to view. The immersive session mode is indicated using the session mode . This session mode requires an XR device such as a headset, and replaces the entire world with the rendered scene using the displays shown to each of the user's eyes. In augmented reality (AR), the user sees the imagery you render presented on top of the physical, real-world environment around them. Because AR is always an immersive experience, in which the scene is the entire world around the user (rather than being enclosed in a box on a screen), the only AR session mode is . There are two basic types of AR device:\n• Devices which use cameras to capture the world in front of the user, render the WebXR content atop that image, then display the image on a screen. These devices include phones, which show the resulting scene on the device's screen in a 2D presentation, as well as goggles that use a pair of cameras, one for each eye, to capture the scene in stereo in order to retain the world's depth, with the WebXR scene then rendered for each eye with that eye's captured background in place.\n• Devices which use transparent glasses to allow the user to see the world, while overlaying the rendered image atop the scene. The user is, thus, directly viewing the real world instead of a series of digital photos of it. Both types of device should be capable of also presenting VR sessions. WebXR doesn't generally care which type of device you're using, and the rendering process is almost exactly the same as for VR, except you don't erase the background or skybox before rendering each frame.\n\nMost immersive VR experiences take place using goggles or a headset of some kind. A VR headset is worn on the head, with a strap that goes behind the head to fasten it in place, and one or two displays whose screens are focused into the eyes using lenses. By presenting a slightly different image to each eye, the illusion of depth is created, giving the user a simulated 3D experience. The vast majority of headsets use a single display whose frame is divided in half, with one half focused onto each of the user's eyes. For example, if a headset uses a 2560x1440 screen, with the left half being used for the left eye's view and the right half for the right eye's view, the framebuffer is used like this: The simplest headsets have no integrated sensors, and focus each half of the screen into the corresponding eye. A common example of this is Google Cardboard, a type of headset first created by Google which can be cheaply created using cardboard or other inexpensive materials. These devices often work by snapping your phone into the headset so that its screen and onboard graphics processor can be used to render and display the XR scene. More advanced headsets have integrated displays and are strapped to the head using an elastic or strap or a strap with Velcro closure. These headsets may include integrated speakers and microphone, and/or connectors to attach external ones. Additionally, these headsets may have various sensors for detecting when the headset moves through space. The types and number of sensors included will determine how many degrees of freedom the user has.\n\nBecause 3D graphics—and mixed reality in particular—involve a lot of often intricate math, data management, and other complex tasks, it's unlikely that you'll directly use WebGL to render your scene in most cases. Instead, you'll probably do most of your work making use of one of the frameworks or libraries that are built atop WebGL to make it more convenient to use. A particular benefit to using a framework rather than directly using the WebGL API is that libraries tend to implement virtual camera functionality. OpenGL (and thus WebGL by extension) does not directly offer a camera view, using a library that simulates one on your behalf can make your job much, much easier, especially when building code that allows free movement through your virtual world. Since WebGL is used for rendering the 3D world into the WebXR session, you should first be familiar with WebGL's general usage, and with the basics of 3D graphics in general."
    },
    {
        "link": "https://w3.org/TR/webxr",
        "document": "This specification describes support for accessing virtual reality (VR) and augmented reality (AR) devices, including sensors and head-mounted displays, on the Web.\n\nFor changes since the last draft, see the Changes section.\n\nThis document is governed by the 03 November 2023 W3C Process Document .\n\nThis document was produced by a group operating under the W3C Patent Policy . W3C maintains a public list of any patent disclosures made in connection with the deliverables of the group; that page also includes instructions for disclosing a patent. An individual who has actual knowledge of a patent which the individual believes contains Essential Claim(s) must disclose the information in accordance with section 6 of the W3C Patent Policy .\n\nThe entrance criteria for this document to enter the Proposed Recommendation stage is to have a minimum of two independent and interoperable user agents that implementation all the features of this specification, which will be determined by passing the user agent tests defined in the test suite developed by the Working Group. The Working Group will prepare an implementation report to track progress.\n\nPublication as a Candidate Recommendation does not imply endorsement by W3C and its Members. A Candidate Recommendation Draft integrates changes from the previous Candidate Recommendation that the Working Group intends to include in a subsequent Candidate Recommendation Snapshot. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite this document as other than work in progress.\n\nThis document was published by the Immersive Web Working Group as a Candidate Recommendation Draft using the Recommendation track . This document is intended to become a W3C Recommendation.\n\nThe Immersive Web Working Group maintains a list of all bug reports that the group has not yet addressed . This draft highlights some of the pending issues that are still to be discussed in the working group. No decision has been taken on the outcome of these issues including whether they are valid. Pull requests with proposed specification text for outstanding issues are strongly encouraged.\n\nThis section describes the status of this document at the time of its publication. A list of current W3C publications and the latest revision of this technical report can be found in the W3C technical reports index at https://www.w3.org/TR/.\n\nChanges from the First Public Working Draft 5 Feburary 2019\n\nThis document uses the acronym XR throughout to refer to the spectrum of hardware, applications, and techniques used for Virtual Reality, Augmented Reality, and other related technologies. Examples include, but are not limited to:\n• Head-mounted displays, whether they are opaque, transparent, or utilize video passthrough\n\nThe important commonality between them being that they offer some degree of spatial tracking with which to simulate a view of virtual content.\n\nTerms like \"XR device\", \"XR application\", etc. are generally understood to apply to any of the above. Portions of this document that only apply to a subset of these devices will indicate so as appropriate.\n\nThe terms 3DoF and 6DoF are used throughout this document to describe the tracking capabilities of XR devices.\n• A device, short for \"Three Degrees of Freedom\", is one that can only track rotational movement. This is common in devices which rely exclusively on accelerometer and gyroscope readings to provide tracking. 3DoF devices do not respond to translational movements from the user, though they may employ algorithms to estimate translational changes based on modeling of the neck or arms.\n• A device, short for \"Six Degrees of Freedom\", is one that can track both rotation and translation, enabling precise 1:1 tracking in space. This typically requires some level of understanding of the user’s environment. That environmental understanding may be achieved via inside-out tracking, where sensors on the tracked device itself (such as cameras or depth sensors) are used to determine the device’s position, or outside-in tracking, where external devices placed in the user’s environment (like a camera or light emitting device) provides a stable point of reference against which the XR device can determine its position.\n\nAn is a physical unit of hardware that can present immersive content to the user. Content is considered to be \"immersive\" if it produces visual, audio, haptic, or other sensory output that simulates or augments various aspects of the user’s environment. Most frequently this involves tracking the user’s motion in space and producing outputs that are synchronized to the user’s movement. On desktop clients, this is usually a headset peripheral. On mobile clients, it may represent the mobile device itself in conjunction with a viewer harness. It may also represent devices without stereo-presentation capabilities but with more advanced tracking.\n\nAn XR device has a (a list of strings) that contains the enumeration values of that the XR device supports.\n\nEach XR device has a for each in its list of supported modes, which is a set of feature descriptors which MUST be initially an empty set.\n\nThe user agent has a (a list of XR device), which MUST be initially an empty list.\n\nThe user agent has an ( or XR device) which is initially and represents the active XR device from the list of immersive XR devices. This object MAY live on a separate thread and be updated asynchronously.\n\nThe user agent MUST have a , which is an XR device that MUST contain in its list of supported modes. The default inline XR device MUST NOT report any pose information, and MUST NOT report XR input sources or events other than those created by pointer events.\n\nNote: The default inline XR device exists purely as a convenience for developers, allowing them to use the same rendering and input logic for both inline and immersive content. The default inline XR device does not expose any information not already available to the developer through other mechanisms on the page (such as pointer events for input), it only surfaces those values in an XR-centric format.\n\nThe user agent MUST have a , which is an XR device that MUST contain in its list of supported modes. The inline XR device MAY be the immersive XR device if the tracking it provides makes sense to expose to inline content or the default inline XR device otherwise.\n\nNote: On phones, the inline XR device may report pose information derived from the phone’s internal sensors, such as the gyroscope and accelerometer. On desktops and laptops without similar sensors, the inline XR device will not be able to report a pose, and as such should fall back to the default inline XR device. In case the user agent is already running on an XR device, the inline XR device will be the same device, and may support multiple views. User consent must be given before any tracking or input features beyond what the default inline XR device exposes are provided.\n\nThe current values of list of immersive XR devices, inline XR device, and immersive XR device MAY live on a separate thread and be updated asynchronously. These objects SHOULD NOT be directly accessed in steps that are not running in parallel.\n\nThe attribute’s getter MUST return the object that is associated with it.\n\nThe user agent MUST create an object when a object is created and associate it with that object.\n\nAn object is the entry point to the API, used to query for XR features available to the user agent and initiate communication with XR hardware via the creation of s.\n\nThe user agent MUST be able to attached to the system, at which time each available device is placed in the list of immersive XR devices. Subsequent algorithms requesting enumeration MUST reuse the cached list of immersive XR devices. Enumerating the devices should not initialize device tracking. After the first enumeration the user agent MUST begin monitoring device connection and disconnection, adding connected devices to the list of immersive XR devices and removing disconnected devices.\n\nNote: The user agent is allowed to use any criteria it wishes to select an immersive XR device when the list of immersive XR devices contains multiple devices. For example, the user agent may always select the first item in the list, or provide settings UI that allows users to manage device priority. Ideally the algorithm used to select the default device is stable and will result in the same device being selected across multiple browsing sessions.\n\nThe attribute is an Event handler IDL attribute for the devicechange event type.\n\nNote: The purpose of is not to report with perfect accuracy the user agent’s ability to create an , but to inform the page whether or not advertising the ability to create sessions of the given mode is advised. A certain level of false-positives are expected, even when user agent checks for the presence of the necessary hardware/software prior to resolving the method. (For example, even if the appropriate hardware is present it may have given exclusive access to another application at the time a session is requested.)\n\nIt is expected that most pages with XR content will call early in the document lifecycle. As such, calling SHOULD avoid displaying any modal or otherwise intrusive UI. Calling MUST NOT trigger device-selection UI, MUST NOT interfere with any running XR applications on the system, and MUST NOT cause XR-related applications to launch such as system trays or storefronts.\n\nThe object has a boolean, which MUST be initially , an , which MUST be initially , and a , which MUST be initially empty.\n\nThe enum defines the modes that an can operate in.\n• A session mode of indicates that the session’s output will be shown as an element in the HTML document. session content MUST be displayed in mono (i.e., with a single view). It MAY allow for viewer tracking. User agents MUST allow sessions to be created.\n• A session mode of indicates that the session’s output will be given exclusive access to the immersive XR device display and that content is not intended to be integrated with the user’s environment.\n• The behavior of the session mode is defined in the WebXR AR Module and MUST NOT be added to the immersive XR device's list of supported modes unless the UA implements that module.\n\nIn this document, the term is synonymous with an session and the term refers to either an or session.\n\nImmersive sessions MUST provide some level of viewer tracking, and content MUST be shown at the proper scale relative to the user and/or the surrounding environment. Additionally, Immersive sessions MUST be given to the immersive XR device, meaning that while the immersive session is the HTML document is not shown on the immersive XR device's display, nor does content from any other source have exclusive access. Exclusive access does not prevent the user agent from overlaying its own UI, however this UI SHOULD be minimal.\n\nNote: UA may choose to overlay content for accessibility or safety such as guardian boundaries, obstructions or the user’s hands when there are no alternative input sources.\n\nNote: Future specifications or modules may expand the definition of immersive session to include additional session modes.\n\nNote: Examples of ways exclusive access may be presented include stereo content displayed on a virtual reality headset.\n\nNote: As an example of overlaid UI, the user agent or operating system in an immersive session may show notifications over the rendered content.\n\nNote: While the HTML document is not shown on the immersive XR device's display during an immersive session, it may still be shown on a separate display, e.g. when the user is entering the immersive session from a 2d browser on their computer tethered to their immersive XR device.\n\nSome features of an may not be universally available for a number of reasons, among which is the fact not all XR devices can support the full set of features. Another consideration is that some features expose sensitive information which may require a clear signal of user intent before functioning.\n\nSince it is a poor user experience to initialize the underlying XR platform and create an only to immediately notify the user that the applications cannot function correctly, developers can indicate by passing an dictionary to . This will block the creation of the if any of the required features are unavailable due to device limitations or in the absence of a clear signal of user intent to expose sensitive information related to the feature.\n\nAdditionally, developers are encouraged to design experiences which progressively enhance their functionality when run on more capable devices. which the experience does not require but will take advantage of when available must also be indicated in an dictionary to ensure that user intent can be determined before enabling the feature if necessary.\n\nThe array contains any Required features for the experience. If any value in the list is not a recognized feature descriptor the will not be created. If any feature listed in the array is not supported by the XR device or, if necessary, has not received a clear signal of user intent the will not be created.\n\nThe array contains any Optional features for the experience. If any value in the list is not a recognized feature descriptor it will be ignored. Features listed in the array will be enabled if supported by the XR device and, if necessary, given a clear signal of user intent, but will not block creation of the if absent.\n\nValues given in the feature lists are considered a valid if the value is one of the following:\n• The string representation of any enum value\n\nFuture iterations of this specification and additional modules may expand the list of accepted feature descriptors.\n\nNote: If a feature needs additional initialization, should be extended with a new field for that feature.\n\nDepending on the requested, certain feature descriptors are added to the or lists by default. The following table describes the associated with each session type and feature list:\n\nThe combined list of feature descriptors given by the and are collectively considered the for an .\n\nSome feature descriptors, when present in the requested features list, are subject to permissions policy and/or requirements that user intent to use the feature is well understood, via either explicit consent or implicit consent. The following table describes the that must be satisfied prior to being enabled:\n\nNote: is always included in the requested features of immersive sessions as a default feature, and as such immersive sessions always need to obtain explicit consent or implicit consent.\n\nRequested features can only be enabled for a session if the XR device is the feature, which means that the feature is known to be supported by the XR device in some configurations, even if the current configuration has not yet been verified as supporting the feature. The user agent MAY apply more rigorous constraints if desired in order to yield a more consistent user experience.\n\nNote: For example, several VR devices support either configuring a safe boundary for the user to move around within or skipping boundary configuration and operating in a mode where the user is expected to stand in place. Such a device can be considered to be capable of supporting s even if they are currently not configured with safety boundaries, because it’s expected that the user could configure the device appropriately if the experience required it. This is to allow user agents to avoid fully initializing the XR device or waiting for the user’s environment to be recognized prior to resolving the requested features if desired. If, however, the user agent knows the boundary state at the time the session is requested without additional initialization it may choose to reject the feature if the safety boundary is not already configured.\n\nAny interaction with XR hardware is done via an object, which can only be retrieved by calling on the object. Once a session has been successfully acquired, it can be used to , query information about the user’s environment, and present imagery to the user.\n\nThe user agent, when possible, or rendering capabilities until an has been acquired. This is to prevent unwanted side effects of engaging the XR systems when they’re not actively being used, such as increased battery usage or related utility applications from appearing when first navigating to a page that only wants to test for the presence of XR hardware in order to advertise XR features. Not all XR platforms offer ways to detect the hardware’s presence without initializing tracking, however, so this is only a strong recommendation.\n\nEach has a , which is one of the values of .\n\nEach has an , which is an initialized with active set to , animationFrame set to , and set to the .\n\nEach has a , which is a set of s corresponding to the feature descriptors that have been granted to the .\n\nThe attribute returns the features in the set of granted features as a new array of s.\n\nThe attribute indicates that the has the ability to display the system keyboard while the is active. If is , Web APIs that would trigger the overlay keyboard (such as focus) will show the system keyboard. The MUST set the visibility state of the to while the keyboard is shown.\n\nA number of different circumstances may , which is permanent and irreversible. Once a session has been shut down the only way to access the XR device's tracking or rendering capabilities again is to request a new session. Each has an boolean, initially set to , that indicates if it has been shut down.\n\nEach has an which is a new , and a , which is an which is initially .\n\nEach has a and a , defined in radians. The values MUST be determined by the user agent and MUST fall in the range of to .\n\nEach has a and a , defined in meters. The values MUST be determined by the user agent and MUST be non-negative. The minimum near clip plane SHOULD be less than . The maximum far clip plane SHOULD be greater than (and MAY be infinite).\n\nIf the XR Compositor changes the nominal frame rate for any reason (for example during a event), it SHOULD use the internal target framerate once the event that caused the frame rate change has ended.\n\nEach has a (a list of ) and a (a list of ) which MUST both be initially an empty list.\n\nEach has an , which is an XR device set at initialization.\n\nThe attribute returns the 's list of active XR input sources.\n\nThe attribute returns the 's list of active XR tracked sources.\n\nThe user agent MUST monitor any XR input sources associated with the XR device, including detecting when XR input sources are added, removed, or changed.\n\nNOTE: The purpose of this flag is to ensure that the add input source, remove input source, and change input source algorithms do not run until the user code actually has had a chance to attach event listeners. Implementations may not need this flag if they simply choose to start listening for input source changes after the session resolves.\n\nEach has a value, which is an enum. For inline sessions the visibility state MUST mirror the 's visibilityState. For immersive sessions the visibility state MUST be set to whichever of the following values best matches the state of session.\n• A state of indicates that imagery rendered by the can be seen by the user and callbacks are processed at the XR device's native refresh rate. Input is processed by the normally.\n• A state of indicates that imagery rendered by the may be seen by the user, but is not the primary focus. callbacks MAY be throttled. Input is not processed by the .\n• A state of indicates that imagery rendered by the cannot be seen by the user. callbacks will not be processed until the visibility state changes. Input is not processed by the .\n\nThe attribute returns the 's visibility state. The attribute is an Event handler IDL attribute for the event type.\n\nThe visibility state MAY be changed by the user agent at any time other than during the processing of an XR animation frame, and the user agent SHOULD monitor the XR platform when possible to observe when session visibility has been affected external to the user agent and update the visibility state accordingly.\n\nNote: The 's visibility state does not necessarily imply the visibility of the HTML document. Depending on the system configuration the page may continue to be visible while an immersive session is active. (For example, a headset connected to a PC may continue to display the page on the monitor while the headset is viewing content from an immersive session.) Developers should continue to rely on the Page Visibility to determine page visibility.\n\nNote: The 's visibility state does not affect or restrict mouse behavior on tethered sessions where 2D content is still visible while an immersive session is active. Content should consider using the [pointerlock] API if it wishes to have stronger control over mouse behavior.\n\nIn an , there are several definitions which can describe a frame rate:\n• The : the rate at which the is asking the experience to render frames to maintain nominal performance. Experiences that miss frames may not end up actually getting calls to this many times per second, but that is what the is aiming to achieve.\n• The : a performance measurement of how many calls to the experience is actually managing to process each second. This will fluctuate based on the experience hitting or missing the 's frame timing.\n• The : the experience’s hint to the on what nominal frame rate it prefers to target.\n• The : the actual rate at which frames are drawn to the physical display, which MAY be derived from the experience’s nominal frame rate. This is a hardware implementation detail that is not exposed to the experience.\n\nEach MAY have an which is the target frame rate.\n\nEach MAY have an which is the nominal frame rate. If the effective frame rate is lower than the nominal frame rate, the XR Compositor MAY use reprojection or other techniques to improve the experience. It is optional and MUST NOT be present for sessions.\n\nThe attribute reflects the internal nominal framerate. If the has no internal nominal framerate, return .\n\nThe attribute is an Event handler IDL attribute for the event type. If 's nominal frame rate is changed for any reason, it MUST apply the nominal frame rate with the new nominal frame rate and the .\n\nThe attribute returns a list of supported target frame rate values. This attribute is optional and MUST NOT be present for sessions or for an that doesn’t let the author control the frame rate. If the supports the attribute, it also MUST support .\n\nEach has a , which is an of type with an identity transform origin offset.\n\nEach has a , which is a list of views corresponding to the views provided by the XR device. If the 's 's composition enabled boolean is set to the list of views MUST contain a single view. The list of views is immutable during the and MUST contain any views that may be surfaced during the session, including secondary views that may not initially be active.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nAn represents a set of configurable values which affect how an 's output is composited. The active render state for a given can only change between frame boundaries, and updates can be queued up via .\n\nEach has a , which is an initially set to . The output canvas is the DOM element that will display any content rendered for an .\n\nEach also has a boolean, which is initially . The is considered to have composition enabled if rendering commands are performed against a surface provided by the API and displayed by the XR Compositor. If rendering is performed for an in such a way that it is directly displayed into an output canvas, the 's composition enabled flag MUST be .\n\nNote: At this point the will only have an output canvas if it has composition enabled set to , but future versions of the specification are likely to introduce methods for setting output canvases that support more advanced uses like mirroring and layer compositing that will require composition.\n\nThe attribute defines the distance, in meters, of the near clip plane from the viewer. The attribute defines the distance, in meters, of the far clip plane from the viewer.\n\nand are used in the computation of the of s. When the is used during rendering, only geometry with a distance to the viewer that falls between and will be drawn. They also determine how the values of an depth buffer are interpreted. MAY be greater than .\n\nNote: Typically when constructing a perspective projection matrix for rendering the developer specifies the viewing frustum and the near and far clip planes. When displaying to an immersive XR device the correct viewing frustum is determined by some combination of the optics, displays, and cameras being used. The near and far clip planes, however, may be modified by the application since the appropriate values depend on the type of content being rendered.\n\nThe attribute defines the default vertical field of view in radians used when computing projection matrices for s. The projection matrix calculation also takes into account the aspect ratio of the output canvas. This value MUST be for immersive sessions.\n\nThe attribute defines an which the XR compositor will obtain images from.\n\nThe primary way an provides information about the tracking state of the XR device is via callbacks scheduled by calling on the instance.\n\nEach object has a boolean initially set to .\n\nEach has a , which is initially empty, a , which is also initially empty, and an , which is a number which is initially zero.\n\nThe behavior of the interface’s method is not changed by the presence of any active , nor does calling on any interact with 's in any way. An active immersive session MAY affect the rendering opportunity of a browsing context if it causes the page to be obscured. If the 2D browser view is visible during an active immersive session (i.e., when the sesson is running on a tethered headset), the timing of callbacks run with 's and MAY NOT coincide with that of the session’s and should not be relied upon by the user for rendering XR content.\n\nNote: User agents may wish to display a warning to the developer console if 's is called during callbacks scheduled via 's , as these callbacks are not guaranteed to occur if the active immersive session affects the rendering opportunity of the browsing context, and may not have the correct timing even if they run.\n\nThe user agent MUST maintain an which handles presentation to the XR device and frame timing. The compositor MUST use an independent rendering context whose state is isolated from that of any graphics contexts created by the document. The compositor MUST prevent the page from corrupting the compositor state or reading back content from other pages or applications. The compositor MUST also run in separate thread or processes to decouple performance of the page from the ability to present new imagery to the user at the appropriate framerate. The compositor MAY composite additional device or user agent UI over rendered content, like device menus.\n\nNote: Future extensions to this spec may utilize the compositor to composite multiple layers coming from the same page as well.\n\nAn represents a snapshot of the state of all of the tracked objects for an . Applications can acquire an by calling on an with an . When the callback is called it will be passed an . Events which need to communicate tracking state, such as the event, will also provide an .\n\nEach has an boolean which is initially set to , and an boolean which is initially set to .\n\nThe attribute returns the that produced the .\n\nFor an immersive session the attribute MUST return the corresponding to the average point in time this is expected to be displayed on the devices' display. For an , MUST return the same value as the timestamp passed to the .\n\nEach represents the state of all tracked objects for a given , and either stores or is able to query concrete information about this state at the time.\n\nA is an algorithm that can be run given an , which is intended to be run each .\n\nEvery has a , which is a list of , initially the empty list.\n\nNOTE: This spec does not define any , but other specifications may add some.\n\nA core feature of the WebXR Device API is the ability to provide spatial tracking. Spaces are the interface that enable applications to reason about how tracked entities are spatially related to the user’s physical environment and each other.\n\nAn represents a virtual coordinate system with an origin that corresponds to a physical location. Spatial data that is requested from the API or given to the API is always expressed in relation to a specific at the time of a specific . Numeric values such as pose positions are coordinates in that space relative to its origin. The interface is intentionally opaque.\n\nEach has a which is set to the that created the .\n\nEach has a which is a position and orientation in space. The 's native origin may be updated by the XR device's underlying tracking system, and different s may define different semantics as to how their native origins are tracked and updated.\n\nEach has an , which is the basis of the 's .\n\nThe transform from the effective space to the native origin's space is defined by an , which is an initially set to an identity transform. In other words, the effective origin can be obtained by multiplying origin offset and the native origin.\n\nThe effective origin of an can only be observed in the coordinate system of another as an , returned by an 's method. The spatial relationship between s MAY change between s.\n\nAn is one of several common s that applications can use to establish a spatial relationship with the user’s physical environment.\n\ns are generally expected to remain static for the duration of the , with the most common exception being mid-session reconfiguration by the user. The native origin for every describes a coordinate system where is considered \"Right\", is considered \"Up\", and is considered \"Forward\".\n\nEach has a , which is an .\n\nAn is most frequently obtained by calling , which creates an instance of an (or an interface extending it) if the enum value passed into the call is supported. The type indicates the tracking behavior that the reference space will exhibit:\n• Passing a type of creates an instance. It represents a tracking space with a native origin which tracks the position and orientation of the viewer. Every MUST support s.\n• Passing a type of creates an instance. It represents a tracking space with a native origin near the viewer at the time of creation. The exact position and orientation will be initialized based on the conventions of the underlying platform. When using this reference space the user is not expected to move beyond their initial position much, if at all, and tracking is optimized for that purpose. For devices with 6DoF tracking, reference spaces should emphasize keeping the origin stable relative to the user’s environment.\n• Passing a type of creates an instance. It represents a tracking space with a native origin at the floor in a safe position for the user to stand. The axis equals at floor level, with the and position and orientation initialized based on the conventions of the underlying platform. If the floor level isn’t known it MUST be estimated, with some . If the estimated floor level is determined with a non-default value, it MUST be rounded sufficiently to prevent fingerprinting. When using this reference space the user is not expected to move beyond their initial position much, if at all, and tracking is optimized for that purpose. For devices with 6DoF tracking, reference spaces should emphasize keeping the origin stable relative to the user’s environment. Note: If the floor level of a reference space is adjusted to prevent fingerprinting, rounded to the nearest 1cm is suggested.\n• Passing a type of creates an instance. It represents a tracking space with its native origin at the floor, where the user is expected to move within a pre-established boundary, given as the . Tracking in a reference space is optimized for keeping the native origin and stable relative to the user’s environment.\n• Passing a type of creates an instance. It represents a tracking space where the user is expected to move freely around their environment, potentially even long distances from their starting point. Tracking in an reference space is optimized for stability around the user’s current position, and as such the native origin may drift over time.\n\nNote: It is assumed that the conventions of the underlying platform regarding Y axes of the reference spaces stay consistent across different types of s. In other words, if an XR system supports multiple reference spaces, their Y axes will be parallel to each other and point in the same direction for the duration of the in which they were created. This does not apply to , which does not rely on the conventions of the underlying platform for its orientation. reference spaces should align their Y axes with other reference spaces when their origins are nearby, but may deviate if the user moves over large distances.\n\nDevices that support reference spaces MUST support reference spaces, through emulation if necessary, and vice versa.\n\nThe attribute is an Event handler IDL attribute for the event type.\n\nNote: It’s expected that some applications will use to implement scene navigation controls based on mouse, keyboard, touch, or gamepad input. This will result in being called frequently, at least once per-frame during periods of active input. As a result UAs are strongly encouraged to make the creation of new s with a lightweight operation.\n\nextends to include , indicating the pre-configured boundaries of the user’s space.\n\nThe origin of an MUST be positioned at the floor, such that the axis equals at floor level. The and position and orientation are initialized based on the conventions of the underlying platform, typically expected to be near the center of the room facing in a logical forward direction.\n\nNote: Other XR platforms sometimes refer to the type of tracking offered by a reference space as \"room scale\" tracking. An is not intended to describe multi-room spaces, areas with uneven floor levels, or very large open areas. Content that needs to handle those scenarios should use an reference space.\n\nEach has a describing the border around the , which the user can expect to safely move within. The polygonal boundary is given as an array of s, which represents a loop of points at the edges of the safe space. The points describe offsets from the native origin in meters. Points MUST be given in a clockwise order as viewed from above, looking towards the negative end of the Y axis. The value of each point MUST be and the value of each point MUST be . The bounds can be considered to originate at the floor and extend infinitely high. The shape it describes MAY be convex or concave.\n\nEach point in the native bounds geometry MUST be limited to a reasonable distance from the reference space’s native origin.\n\nNote: It is suggested that points of the native bounds geometry be limited to 15 meters from the native origin in all directions.\n\nEach point in the native bounds geometry MUST also be quantized sufficiently to prevent fingerprinting. For user’s safety, quantized points values MUST NOT fall outside the bounds reported by the platform.\n\nNote: It is suggested that points of the native bounds geometry be quantized to the nearest 5cm.\n\nThe attribute is an array of s such that each entry is equal to the entry in the 's native bounds geometry premultiplied by the of the origin offset. In other words, it provides the same border in coordinates relative to the effective origin.\n\nIf the native bounds geometry is temporarily unavailable, which may occur for several reasons such as during XR device initialization, extended periods of tracking loss, or movement between pre-configured spaces, the MUST report an empty array.\n\nNote: Bounded reference spaces may be returned if the boundaries or floor height have not been resolved at the time of the reference space request, but the XR device is known to support them.\n\nNote: Content should not require the user to move beyond the . It is possible for the user to move beyond the bounds if their physical surroundings allow for it, resulting in position values outside of the polygon they describe. This is not an error condition and should be handled gracefully by page content.\n\nNote: Content generally should not provide a visualization of the , as it’s the user agent’s responsibility to ensure that safety critical information is provided to the user.\n\nAn describes a single view into an XR scene for a given frame.\n\nA corresponds to a display or portion of a display used by an XR device to present imagery to the user. They are used to retrieve all the information necessary to render content that is well aligned to the view's physical output properties, including the field of view, eye offset, and other optical properties. Views may cover overlapping regions of the user’s vision. No guarantee is made about the number of views any XR device uses or their order, nor is the number of views required to be constant for the duration of an .\n\nA view has an associated internal , which is an describing the position and orientation of the view in the viewer reference space's coordinate system.\n\nNOTE: There are no constraints on what the view offset might be, and views are allowed to have differing orientations. This can crop up in head-mounted devices with eye displays centered at an angle, and it can also surface itself in more extreme cases like CAVE rendering. Techniques like z-sorting and culling may need to be done per-eye because of this.\n\nA view has an associated which is a matrix describing the projection to be used when rendering the view, provided by the underlying XR device. The projection matrix MAY include transformations such as shearing that prevent the projection from being accurately described by a simple frustum.\n\nA view has an associated which is an describing which eye this view is expected to be shown to. If the view does not have an intrinsically associated eye (the display is monoscopic, for example) this value MUST be set to .\n\nA view has an flag that may change through the lifecycle of an . Primary views MUST always have the active flag set to .\n\nNote: Many HMDs will request that content render two views, one for the left eye and one for the right, while most magic window devices will only request one view, but applications should never assume a specific view configuration. For example: A magic window device may request two views if it is capable of stereo output, but may revert to requesting a single view for performance reasons if the stereo output mode is turned off. Similarly, HMDs may request more than two views to facilitate a wide field of view or displays of different pixel density.\n\nA view has an internal flag that indicates if the viewport scale can be changed by a call at this point in the session. It is set to at the start of an animation frame, and set to when is called.\n\nA view has an internal value that represents the requested viewport scale for this view. It is initially set to 1.0, and can be modified by the method if the system supports dynamic viewport scaling.\n\nA view has an internal value that represents the current viewport scale for this view as used internally by the system. It is initially set to 1.0. It is updated to match the requested viewport scale when the viewport change is successfully applied by a call.\n\nNote: Dynamic viewport scaling allows applications to render to a subset of the full-sized viewport using a scale factor that can be changed every animation frame. This is intended to be efficiently modifiable on a per-frame basis without reallocation. For correct rendering, it’s essential that the XR system and application agree on the active viewport. An application can call for an multiple times within a single animation frame, but the requested scale does not take effect until the application calls for that view. The first call in an animation frame applies the change (taking effect immediately for the current animation frame), locks in the view’s current scaled viewport for the remainder of this animation frame, and sets the scale as the new default for future animation frames. Optionally, the system can provide a suggested value through the attribute based on internal performance heuristics and target framerates.\n\nThe attribute describes the eye of the underlying view. This attribute’s primary purpose is to ensure that pre-rendered stereo content can present the correct portion of the content to the correct eye.\n\nThe attribute is the projection matrix of the underlying view. It is strongly recommended that applications use this matrix without modification or decomposition. Failure to use the provided projection matrices when rendering may cause the presented frame to be distorted or badly aligned, resulting in varying degrees of user discomfort. This attribute MUST be computed by obtaining the projection matrix for the .\n\nThe attribute is the of the viewpoint. It represents the position and orientation of the viewpoint in the provided in .\n\nThe optional attribute contains a UA-recommended viewport scale value that the application can use for a call to configure dynamic viewport scaling. It is if the system does not implement a heuristic or method for determining a recommended scale. If not null, the value MUST be a numeric value greater than 0.0 and less than or equal to 1.0, and MUST be quantized to avoid providing detailed performance or GPU utilization data.\n\nNote: It is suggested to quantize the recommended viewport scale by rounding it to the nearest value from a short list of possible scale values, and using hysteresis to avoid instant changes when close to a boundary value. (This also helps avoid rapidly oscillating scale values which can be visually distracting or uncomfortable.)\n\nEach has an associated which is the that produced it.\n\nEach has an associated which is the that produced it.\n\nEach has an associated which is the underlying view that it represents.\n\nEach has an associated which stores the projection matrix of its underlying view. It is initially .\n\nNote: The can be used to position camera objects in many rendering libraries. If a more traditional view matrix is needed by the application one can be retrieved by calling .\n\nA view is a when rendering to it is necessary for an immersive experience. Primary views MUST be active for the entire duration of the .\n\nA view is a when it is possible for content to choose to not render to it and still produce a working immersive experience. When content chooses to not render to these views, the user agent MAY be able to reconstruct them via reprojection. Secondary views MUST NOT be active unless the \"secondary-views\" feature is enabled.\n\nTo provide for this, user agents that expose secondary views MUST support an \" \" feature descriptor as a hint. Content enabling this feature is expected to:\n• Handle any nonzero number of views in the array.\n• Handle the existence of multiple views that have the same eye.\n• Handle the size of the array changing from frame to frame. This can happen when video capture is enabled, for example\n\nWhen \"secondary-views\" is enabled, the user agent MAY surface any secondary views the device supports to the , when necessary. The user agent MUST NOT use reprojection to reconstruct secondary views in such a case, and instead rely on whatever the content decides to render.\n\nNote: We recommend content use to enable \"secondary-views\" to ensure maximum compatibility.\n\nIf secondary views have lower underlying frame rates, the MAY choose to do one or more of the following:\n• Lower the overall frame rate of the application while the secondary views are active.\n• Surface secondary views in the array only for some of the frames. Implementations doing this SHOULD NOT have frames where the primary views are not present.\n• Silently discard rendered content for secondary views during some of the frames.\n\nAn object describes a viewport, or rectangular region, of a graphics surface.\n\nThe and attributes define an offset from the surface origin and the and attributes define the rectangular dimensions of the viewport.\n\nThe exact interpretation of the viewport values depends on the conventions of the graphics API the viewport is associated with:\n• When used with an the and attributes specify the lower left corner of the viewport rectangle, in pixels, with the viewport rectangle extending pixels to the right of and pixels above . The values can be passed to the WebGL viewport function directly.\n\nWebXR provides various transforms in the form of . WebXR uses the WebGL conventions when communicating matrices, in which 4x4 matrices are given as 16 element s with column major storage, and are applied to column vectors by premultiplying the matrix from the left. They may be passed directly to WebGL’s function, used to create an equivalent , or used with a variety of third party math libraries.\n\nThere are several algorithms which call for a vector or quaternion to be normalized, which means to scale the components to have a collective magnitude of .\n\nAn is a transform described by a and . When interpreting an the is always applied prior to the .\n\nAn contains an which is a matrix.\n\nThe attribute is a 3-dimensional point, given in meters, describing the translation component of the transform. The 's attribute MUST be .\n\nThe attribute is a quaternion describing the rotational component of the transform. The MUST be normalized to have a length of .\n\nThe attribute returns the transform described by the and attributes as a matrix. This attribute MUST be computed by obtaining the matrix for the .\n\nNote: This matrix when premultiplied onto a column vector will rotate the vector by the 3D rotation described by , and then translate it by . Mathematically in column-vector notation, this is , where is a translation matrix corresponding to and is a rotation matrix corresponding to .\n\nThe attribute of a returns an in the relevant realm of which, if applied to an object that had previously been transformed by , would undo the transform and return the object to its initial pose. This attribute SHOULD be lazily evaluated. The returned by MUST return as its .\n\nAn with a of and an of is known as an .\n\nAn describes a position and orientation in space relative to an .\n\nThe attribute describes the position and orientation relative to the base .\n\nThe attribute describes the linear velocity in meters per second relative to the base . If the user agent can’t populate this, it’s allowed to return .\n\nThe attribute describes the angular velocity in radians per second relative to the base . If the user agent can’t populate this, it’s allowed to return .\n\nThe attribute is when the represents an actively tracked 6DoF pose based on sensor readings, or if its value includes a , such as that provided by a neck or arm model. Estimated floor levels MUST NOT be considered when determining if an includes a computed offset.\n\nAn is an describing the state of a of the XR scene as tracked by the XR device. A viewer may represent a tracked piece of hardware, the observed position of a user’s head relative to the hardware, or some other means of computing a series of viewpoints into the XR scene. s can only be queried relative to an . It provides, in addition to the values, an array of views which include rigid transforms to indicate the viewpoint and projection matrices. These values should be used by the application when rendering a frame of an XR scene.\n\nThe array is a sequence of s describing the viewpoints of the XR scene, relative to the the was queried with. Every view of the XR scene in the array must be rendered in order to display correctly on the XR device. Each includes rigid transforms to indicate the viewpoint and projection matrices, and can be used to query s from layers when needed.\n\nNote: The 's can be used to position graphical representations of the viewer for spectator views of the scene or multi-user interaction.\n\nAn represents an , which is any input mechanism which allows the user to perform targeted actions in the same virtual space as the viewer. Example XR input sources include, but are not limited to, handheld controllers, optically tracked hands, and gaze-based input methods that operate on the viewer's pose. Input mechanisms which are not explicitly associated with the XR device, such as traditional gamepads, mice, or keyboards SHOULD NOT be considered XR input sources.\n\nNote: The interface is also extended by the WebXR Gamepads Module\n\nThe attribute describes which hand the XR input source is associated with, if any. Input sources with no natural handedness (such as headset-mounted controls) or for which the handedness is not currently known MUST set this attribute .\n\nThe attribute describes the method used to produce the target ray, and indicates how the application should present the target ray to the user if desired.\n• indicates the target ray will originate at the viewer and follow the direction it is facing. (This is commonly referred to as a \"gaze input\" device in the context of head-mounted displays.)\n• indicates that the target ray originates from either a handheld device or other hand-tracking mechanism and represents that the user is using their hands or the held device for pointing. The orientation of the target ray relative to the tracked object MUST follow platform-specific ergonomics guidelines when available. In the absence of platform-specific guidance, the target ray SHOULD point in the same direction as the user’s index finger if it was outstretched. If the determines that part of the handheld device is or becomes intended to contact real-world surfaces (such as a pen tip), the target ray MUST originate at that point.\n• indicates that the input source was an interaction with the canvas element associated with an inline session’s output context, such as a mouse click or touch event.\n• indicates that the input source was generated as part of an operating system interaction intent rather than a specific piece of hardware. Some examples are user intents based on information too sensitive to expose directly such as gaze, synthesised inputs from web driver or inputs generated by assistive technologies. This should only be used for assistive technologies if it is also used as a primary input so as to not inadvertently indicate that assistive technology is being used as per the W3C design principals.\n\nThe attribute is an that has a native origin tracking the position and orientation of the preferred pointing ray of the (along its axis), as defined by the .\n\nFor input sources with a of the represents the ray to the interaction target at the start of the interaction. The pose for this should be static within the gripSpace for this XRInput.\n\nThe attribute is an that has a native origin tracking to the pose that should be used to render virtual objects such that they appear to be held in the user’s hand. If the user were to hold a straight rod, this places the native origin at the centroid of their curled fingers and where the axis points along the length of the rod towards their thumb. The axis is perpendicular to the back of the hand being described, with the back of the user’s right hand pointing towards and the back of the user’s left hand pointing towards . The axis is implied by the relationship between the and axis, with roughly pointing in the direction of the user’s arm.\n\nThe attribute indicates that this input is visible and MAY NOT need to be rendered by the current session. If is true and the targetRayMode is \"tracked-pointer\", the user agent MUST ensure that a representation of the XR input source is always shown to the user.\n\nExamples of the controller being shown to the user include the controller is in between the display and the user, the display is transparent or the controller is rendered by the operating system.\n\nis a hint to developers about not rendering input sources such as controllers. Pick rays and cursor should still be rendered.\n\nFor input sources with a of the should be the associated user gesture if there is one, otherwise it should be another space the user controls such as the ViewerSpace or the gripSpace or the targetRaySpace of another XRInput. This is to allow user the user to still manipulate the targetRaySpace.\n\nThe MUST be if the input source isn’t inherently trackable such as for input sources with a of or .\n\nThe attribute is a list of input profile names indicating both the prefered visual representation and behavior of the input source.\n\nAn is an ASCII lowercase containing no spaces, with separate words concatenated with a hyphen ( ) character. A descriptive name should be chosen, using the prefered verbiage of the device vendor when possible. If the platform provides an appropriate identifier, such as a USB vendor and product ID, it MAY be used. Values that uniquely identify a single device, such as serial numbers, MUST NOT be used. The input profile name MUST NOT contain an indication of device handedness. If multiple user agents expose the same device, they SHOULD make an effort to report the same input profile name. The WebXR Input Profiles Registry is the recommended location for managing input profile names.\n\nProfiles are given in descending order of specificity. Any input profile names given after the first entry in the list should provide fallback values that represent alternative representations of the device. This may include a more generic or prior version of the device, a more widely recognized device that is sufficiently similar, or a broad description of the device type (such as \"generic-trigger-touchpad\"). If multiple profiles are given, the layouts they describe must all represent a superset or subset of every other profile in the list.\n\nIf the 's mode is , MUST be an empty list.\n\nThe user agent MAY choose to only report an appropriate generic input profile name or an empty list at its discretion. Some scenarios where this would be appropriate are if the input device cannot be reliably identified, no known input profiles match the input device, or the user agent wishes to mask the input device being used.\n\nFor example, the Samsung HMD Odyssey’s controller is a design variant of the standard Windows Mixed Reality controller. Both controllers share the same input layout. As a result, the for a Samsung HMD Odyssey controller could be: . The appearance of the controller is most precisely communicated by the first profile in the list, with the second profile describing an acceptable substitute, and the last profile a generic fallback that describes the device in the roughest sense. (It’s a controller with a trigger, squeeze button, touchpad and thumbstick.) \n\n\n\n Similarly, the Valve Index controller is backwards compatible with the HTC Vive controller, but the Index controller has additional buttons and axes. As a result, the for the Valve Index controller could be: . In this case the input layout described by the profile is a superset of the layout described by the profile. Also, the profile indicates the precise appearance of the controller, while the controller has a significantly different appearance. In this case the UA would have deemed that difference acceptable. And as in the first example, the last profile is a generic fallback. \n\n\n\n (Exact strings are examples only. Actual profile names are managed in the WebXR Input Profiles Registry.)\n\nNote: s in an 's array are \"live\". As such, values within them are updated in-place. This means that it doesn’t work to save a reference to an 's attribute on one frame and compare it to the same attribute in a subsequent frame to test for state changes, because they will be the same object. Therefore developers that wish to compare input state from frame to frame should copy the content of the state in question.\n\nAn XR input source is a if it supports a . The primary action is a platform-specific action that, when engaged, produces , , and events. Examples of possible primary actions are pressing a trigger, touchpad, or button, speaking a command, or making a hand gesture. If the platform guidelines define a recommended primary input then it should be used as the primary action, otherwise the user agent is free to select one. The device MUST support at least one primary input source.\n\nAn XR input source is an if it does not support a primary action. These inputs are primarily intended to provide pose data. Note: An example of a tracked input source would be tracking attachments for a users legs or a prop. Tracked hands may also be considered a tracked input source if there is no gesture recognition being performed to detect primary actions.\n\nEach XR input source MAY define a . The primary squeeze action is a platform-specific action that, when engaged, produces , , and events. The primary squeeze action should be used for actions roughly mapping to squeezing or grabbing. Examples of possible primary squeeze actions are pressing a grip trigger or making a grabbing hand gesture. If the platform guidelines define a recommended primary squeeze action then it should be used as the primary squeeze action, otherwise the user agent MAY select one.\n\nSometimes platform-specific behavior can result in a primary action or primary squeeze action being interrupted or cancelled. For example, an XR input source may be removed from the XR device after the primary action or primary squeeze action is started but before it ends.\n\nSome XR devices may support , where the XR input source is only meaningful while performing a , either the primary action for a primary input source, or a device-specific for an tracked input source.\n\nOne example would be mouse, touch, or stylus input against an , which MUST produce a transient with a set to , treated as a primary action for the primary pointer, and as a non-primary auxiliary action for a non-primary pointer.\n\nAnother example would be intents from the operating system with input derived from sensitive information that cannot be exposed directly, such as interactions based on gaze. These produce a transient with a set to , treated as a primary action.\n\nTransient input sources are only present in the session’s list of active XR input sources for the duration of the transient action.\n\nTransient input sources follow the following sequence when handling transient actions instead of the algorithms for non-transient primary actions:\n\nAn represents a list of s. It is used in favor of a frozen array type when the contents of the list are expected to change over time, such as with the attribute.\n\nThe attribute of indicates how many s are contained within the .\n\nThe of retrieves the at the provided index.\n\nNote: While this specification only defines the layer, future extensions to the spec are expected to add additional layer types and the image sources that they draw from.\n\nis the base class for and other layer types introduced by future extensions.\n\nAn is a layer which provides a WebGL framebuffer to render into, enabling hardware accelerated rendering of 3D graphics to be presented on the XR device.\n\nEach has a object, initially , which is an instance of either a or a .\n\nEach has an associated , which is the it was created with.\n\nNote: If an 's composition enabled boolean is set to all values on the object are ignored, since the 's default framebuffer was already allocated using the context’s actual context parameters and cannot be overridden.\n\nThe attribute is the the was created with.\n\nEach has a boolean which is initially set to . If set to it indicates that the MUST NOT allocate its own , and all properties of the that reflect properties MUST instead reflect the properties of the context's default framebuffer.\n\nThe attribute of an is an instance of a which has been marked as opaque if composition enabled is , and otherwise. The size cannot be adjusted by the developer after the has been created.\n\nAn functions identically to a standard with the following changes that make it behave more like the default framebuffer:\n• An opaque framebuffer MAY support antialiasing, even in WebGL 1.0.\n• An opaque framebuffer's attachments cannot be inspected or changed. Calling , , , or with an opaque framebuffer MUST generate an error.\n• An opaque framebuffer has a related , which is the it was created for.\n• An opaque framebuffer is considered incomplete outside of a callback. When not in the callback of its session, calls to MUST generate a error and attempts to clear, draw to, or read from the opaque framebuffer MUST generate an error.\n• An opaque framebuffer initialized with will have an attached depth buffer.\n• An opaque framebuffer initialized with will have an attached stencil buffer.\n• An opaque framebuffer's color buffer will have an alpha channel if and only if is .\n• The XR Compositor will assume the opaque framebuffer contains colors with premultiplied alpha. This is true regardless of the value set in the 's actual context parameters.\n\nNote: User agents are required to respect values of and , which is similar to WebGL’s behavior when creating a drawing buffer.\n\nThe buffers attached to an opaque framebuffer MUST be cleared to the values in the table below when first created, or prior to the processing of each XR animation frame. This is identical to the behavior of the WebGL context’s default framebuffer. Opaque framebuffers will always be cleared regardless of the associated WebGL context’s value.\n\nNote: Implementations may optimize away the required implicit clear operation of the opaque framebuffer as long as a guarantee can be made that the developer cannot gain access to buffer contents from another process. For instance, if the developer performs an explicit clear then the implicit clear is not needed.\n\nIf an is created with an set to , the must be backed by a texture with the color format. If an is created with an set to , the must be backed by a texture with the color format.\n\nHowever, the XR Compositor MUST treat the 's backing’s pixels as if they were in the or .\n\nNOTE: this means that the XR Compositor MUST not do any gamma conversion from linear or when it processes the texture backing the . Otherwise, the pixels in the final rendering will appear too bright which will not match the rendering on a regular 2D context.\n\nWhen an is set as an immersive session's the content of the opaque framebuffer is presented to the immersive XR device immediately after an XR animation frame completes, but only if at least one of the following has occurred since the previous XR animation frame:\n• , , , or any other rendering operation which similarly affects the framebuffer’s color values has been called while the opaque framebuffer is the currently bound framebuffer of the associated with the .\n\nBefore the opaque framebuffer is presented to the immersive XR device the user agent shall ensure that all rendering operations have been flushed to the opaque framebuffer.\n\nEach has a , which is the if composition enabled is , and the context's default framebuffer otherwise.\n\nThe and attributes return the width and height of the target framebuffer's attachments, respectively.\n\nThe attribute is if the target framebuffer supports antialiasing using a technique of the UA’s choosing, and if no antialiasing will be performed.\n\nThe attribute, if , indicates the XR Compositor MUST NOT make use of values in the depth buffer attachment when rendering. When the attribute is it indicates that the content of the depth buffer attachment will be used by the XR Compositor and is expected to be representative of the scene rendered into the layer.\n\nDepth values stored in the buffer are expected to be between and , with representing the distance of and representing the distance of , with intermediate values interpolated linearly. This is the default behavior of WebGL. (See documentation for the depthRange function for additional details.)\n\nNote: Making the scene’s depth buffer available to the compositor allows some platforms to provide quality and comfort improvements such as improved reprojection.\n\nThe attribute controls the amount of foveation used by the XR Compositor. If the user agent or device does not support this attribute, they should return on getting, and setting should be a . Setting to a value less than will set it to and setting it to a value higher than will set it to . sets the minimum amount of foveation while sets the maximum. It is up to the user agent how the XR Compositor interprets these values. If the level was changed, it will take effect at the next .\n\nNOTE: Fixed foveation is a technique that reduces the resolution content renders at near the edges of the user’s field of view. It can significantly improve experiences that are limited by GPU fill performance. It reduces power consumption and enables applications to increase the resolution of eye textures. It is most useful for low contrast textures, such as background images but less for high contrast ones such as text or detailed images. Authors can adjust the level on a per frame basis to achieve the best tradeoff between performance and visual quality.\n\nEach MUST have a which is a list containing one WebGL viewport for each view the may expose, including secondary views that are not currently active but may become active for the current session. The viewports MUST have a and greater than and MUST describe a rectangle that does not exceed the bounds of the target framebuffer. The viewports MUST NOT be overlapping. If composition enabled is , the list of full-sized viewports MUST contain a single WebGL viewport that covers the context's entire default framebuffer.\n\nEach MUST have a which is a list containing one for each active view the currently exposes.\n\nqueries the the given should use when rendering to the layer.\n\nEach MUST identify a , which is the pixel resolution of a WebGL framebuffer required to match the physical pixel resolution of the XR device.\n\nAdditionally, the MUST identify a , which represents a best estimate of the WebGL framebuffer resolution large enough to contain all of the session’s s that provides an average application a good balance between performance and quality. It MAY be smaller than, larger than, or equal to the native WebGL framebuffer resolution. New opaque framebuffer will be created with this resolution, with width and height each scaled by any 's provided.\n\nNote: The user agent is free to use any method of its choosing to estimate the recommended WebGL framebuffer resolution. If there are platform-specific methods for querying a recommended size it is recommended that they be used, but not required. The scale factors used by and apply to width and height separately, so a scale factor of two results in four times the overall pixel count. If the platform exposes an area-based render scale that’s based on pixel count, the user agent needs to take the square root of that to convert it to a WebXR scale factor.\n\nIn order for a WebGL context to be used as a source for immersive XR imagery it must be created on a for the immersive XR device. What is considered a compatible graphics adapter is platform dependent, but is understood to mean that the graphics adapter can supply imagery to the immersive XR device without undue latency. If a WebGL context was not already created on the compatible graphics adapter, it typically must be re-created on the adapter in question before it can be used with an .\n\nNote: On an XR platform with a single GPU, it can safely be assumed that the GPU is compatible with the immersive XR devices advertised by the platform, and thus any hardware accelerated WebGL contexts are compatible as well. On PCs with both an integrated and discrete GPU the discrete GPU is often considered the compatible graphics adapter since it generally is a higher performance chip. On desktop PCs with multiple graphics adapters installed, the one with the immersive XR device physically connected to it is likely to be considered the compatible graphics adapter.\n\nNote: sessions render using the same graphics adapter as canvases, and thus do not need contexts.\n\nWhen a user agent implements this specification it MUST set an boolean, initially set to , on every . Once the XR compatible boolean is set to , the context can be used with layers for any requested from the current immersive XR device.\n\nNote: This flag introduces slow synchronous behavior and is discouraged. Consider calling instead for an asynchronous solution.\n\nThe XR compatible boolean can be set either at context creation time or after context creation, potentially incurring a context loss. To set the XR compatible boolean at context creation time, the context creation attribute must be set to when requesting a WebGL context. If the requesting document’s origin is not allowed to use the \"xr-spatial-tracking\" permissions policy, has no effect.\n\nThe flag on , if , affects context creation by requesting the user agent create the WebGL context using a compatible graphics adapter for the immersive XR device. If the user agent succeeds in this, the created context’s XR compatible boolean will be set to true. To obtain the immersive XR device, ensure an immersive XR device is selected SHOULD be called.\n\nNote: Ensure an immersive XR device is selected needs to be run in parallel, which introduces slow synchronous behavior on the main thread. User agents SHOULD print a warning to the console requesting that be used instead.\n\nTo set the XR compatible boolean after the context has been created, the method is used.\n\nNote: On some systems this flag may turn on a high powered discrete GPU, for example, or proxy all commands to an on-device GPU. If you are in a situation where you may or may not be using XR, it is suggested that you only call when you intend to start an immersive session.\n\nThe task source for all tasks queued in this specification is the , unless otherwise specified.\n\ns are fired to indicate changes to the state of an .\n\nThe attribute indicates the that generated the event.\n\ns are fired to indicate changes to the state of an .\n\nThe attribute indicates the that generated this event.\n\nThe attribute is an that corresponds with the time that the event took place. It may represent historical data. MUST throw an exception when called on .\n\ns are fired to indicate changes to the list of active XR input sources that are available to an .\n\nThe attribute indicates the that generated the event.\n\nThe attribute is a list of s that were added to the at the time of the event.\n\nThe attribute is a list of s that were removed from the at the time of the event.\n\ns are fired to indicate changes to the state of an .\n\nThe attribute indicates the that generated this event.\n\nThe optional attribute describes the post-event position and orientation of the 's native origin in the pre-event coordinate system. This attribute MAY be if the can’t determine the delta between the old and the new coordinate system.\n\nNOTE: situations where or can be when the headset was doffed and donned between 2 seperate locations. In such cases, if the experience relies on world-locked content, it should warn the user and reset the scene.\n\nThe user agent MUST provide the following new events. Registration for and firing of the events must follow the usual behavior of DOM Events.\n\nThe user agent MUST fire an event named event on the object to indicate that the availability of immersive XR devices has been changed unless the document’s origin is not allowed to use the \"xr-spatial-tracking\" permissions policy.\n\nA user agent MUST fire an event named using on an each time the visibility state of the has changed.\n\nA user agent MUST fire an event named using on an when the session ends, either by the application or the user agent.\n\nA user agent MUST fire an event named using on an when the session’s list of active XR input sources has changed.\n\nA user agent MUST fire an event named using on an when the session’s list of active XR tracked sources has changed.\n\nA user agent MUST fire an event named using on an when one of its s begins its primary action. The event MUST be of type .\n\nA user agent MUST fire an event named using on an when one of its s ends its primary action or when an that has begun a primary action is disconnected.\n\nA user agent MUST fire an event named using on an when one of its s has fully completed a primary action.\n\nA user agent MUST fire an event named using on an when one of its s begins its primary squeeze action.\n\nA user agent MUST fire an event named using on an when one of its s ends its primary squeeze action or when an that has begun a primary squeeze action is disconnected.\n\nA user agent MUST fire an event named using on an when one of its s has fully completed a primary squeeze action.\n\nA user agent MUST fire an event named using on an when the XR Compositor changes the 's internal nominal framerate.\n\nA user agent MUST fire an event named using on an when discontinuities of the native origin or effective origin occur, i.e. there are significant changes in the origin’s position or orientation relative to the user’s environment. (For example: After user recalibration of their XR device or if the XR device automatically shifts its origin after losing and regaining tracking.) A event MUST also be dispatched when the changes for an . A event MUST NOT be dispatched if the viewer's pose experiences discontinuities but the 's origin physical mapping remains stable, such as when the viewer momentarily loses and regains tracking within the same tracking area. A event also MUST NOT be dispatched as an reference space makes small adjustments to its native origin over time to maintain space stability near the user, if a significant discontinuity has not occurred. The event MUST be dispatched prior to the execution of any XR animation frames that make use of the new origin. A event MUST be dispatched on all offset reference spaces of a reference space that fires a event, and the of offset s should also be recomputed.\n\nNote: This does mean that the session needs to hold on to strong references to any s that have listeners.\n\nNote: Jumps in viewer position can be handled by the application by observing the boolean. If a jump in viewer position coincides with switching from to , it indicates that the viewer has regained tracking and their new position represents a correction from the previously emulated values. For experiences without a \"teleportation\" mechanic, where the viewer can move through the virtual world without moving physically, this is generally the application’s desired behavior. However, if an experience does provide a \"teleportation\" mechanic, it may be needlessly jarring to jump the viewer's position back after tracking recovery. Instead, when such an application recovers tracking, it can simply resume the experience from the viewer's current position in the virtual world by absorbing that sudden jump in position into its teleportation offset. To do so, the developer calls to create a replacement reference space with its effective origin adjusted by the amount that the viewer's position jumped since the previous frame.\n\nThe WebXR Device API provides powerful new features which bring with them several unique privacy, security, and comfort risks that user agents must take steps to mitigate.\n\nIn the context of XR, includes, but is not limited to, user-configurable data such as interpupillary distance (IPD) and sensor-based data such as s. All immersive sessions will expose some amount of sensitive data, due to the user’s pose being necessary to render anything. However, in some cases, the same sensitive information will also be exposed via sessions.\n\nfor a given action is a signal from the user that such an action was intentional and has their consent.\n\nIt is often necessary to be sure of user intent before exposing sensitive information or allowing actions with a significant effect on the user’s experience. This intent may be communicated or observed in a number of ways.\n\nNote: A common way of determining user intent is by transient activation of a UI control, typically an \"enter VR\" button. Since activation is transient, the browsing context requesting an XR session must be an ancestor or a same origin-domain descendant of the context containing the UI control, and must recently have been the active document of the browsing context.\n\nis when the user agent makes a judgement on the consent of a user without explicitly asking for it, for example, based on the install status of a web application, frequency and recency of visits or a user agent defined action where the user clearly signals intent that they want to enter an immersive experience. Given the sensitivity of XR data, caution is strongly advised when relying on implicit signals.\n\nis when the user agent makes a judgement on the consent of a user based on having explicitly asked for it. When gathering explicit consent, user agents present an explanation of what is being requested and provide users the option to decline. Requests for user consent can be presented in many visual forms based on the features being protected and user agent choice. Install status of a web application MAY count as a signal of explicit consent provided some form of explicit consent is requested at install time.\n\nRegardless of how long the user agent chooses to persist the user’s consent, sensitive information MUST only be exposed by an which has not ended.\n\nThere are multiple non-XR APIs which cause user agents to request explicit consent to use a feature. If the user agent will request the user’s consent while there is an active immersive session, the user agent MUST shut down the session prior to displaying the consent request to the user. If the user’s consent for the feature had been granted prior to the active immersive session being created the session does not need to be terminated.\n\nNote: This limitation is to ensure that there is behavioral parity between all user agents until consensus is reached about how user agents should manage mid-session explicit consent. It is not expected to be a long term requirement.\n\nIn some cases, security and privacy threats can be mitigated through s such as throttling, quantizing, rounding, limiting, or otherwise manipulating the data reported from the XR device. This may sometimes be necessary to avoid fingerprinting, even in situations when user intent has been established. However, data adjustment mitigations MUST only be used in situations which would not result in user discomfort.\n\nThe sensitive information exposed by the API can be divided into categories that share threat profiles and necessary protections against those threats.\n\nStarting an session does not implicitly carry the same requirements, though additional requirements may be imposed depending on the session’s requested features.\n\nThe primary difference between and is the inclusion of information. When more than one view is present and the physical relationship between these views is configurable by the user, the relationship between these views is considered sensitive information as it can be used to fingerprint or profile the user.\n\nIf the relationship between s could uniquely identify the XR device, then the user agent MUST anonymize the data to prevent fingerprinting. The method of anonymization is at the discretion of the user agent.\n\nNote: Furthermore, if the relationship between s is affected by a user-configured interpupillary distance (IPD), then it is strongly recommended that the user agent require explicit consent during session creation, prior to reporting any data.\n• On devices which support 6DoF tracking, reference spaces may be used to perform gait analysis, allowing user profiling and fingerprinting.\n• On devices which support 6DoF tracking, reference spaces may be used to perform gait analysis, allowing user profiling and fingerprinting. In addition, because the reference spaces provide an established floor level, it may be possible for a site to infer the user’s height, allowing user profiling and fingerprinting.\n• reference spaces, when sufficiently constrained in size, do not enable developers to determine geographic location. However, because the floor level is established and users are able to walk around, it may be possible for a site to infer the user’s height or perform gait analysis, allowing user profiling and fingerprinting. In addition, it may be possible to perform fingerprinting using the bounds reported by a bounded reference space.\n• reference spaces reveal the largest amount of spatial data and may result in user profiling and fingerprinting. For example, this data may enable determining a user’s specific geographic location or to perform gait analysis.\n\nAs a result the various reference space types have restrictions placed on their creation to ensure the sensitive information exposed is handled safely:\n\nMost reference spaces require that user intent to use the reference space is well understood, either via explicit consent or implicit consent. See the feature requirements table for details.\n\nAny group of , , and reference spaces that are capable of being related to one another MUST share a common native origin; This restriction only applies when the creation of reference spaces has been restricted.\n\nNote: The requirement for document visibility is based on [DEVICE-ORIENTATION].\n\nNote: It is suggested that poses reported relative to a or reference space be limited to a distance of 15 meters from the 's native origin.\n\nNote: It is suggested that poses reported relative to an be limited to a distance of 1 meter outside the 's native bounds geometry.\n\nA is an interface presented by the User Agent that the user is able to interact with but the page cannot. The user agent MUST support showing trusted UI.\n\nA trusted UI MUST have the following properties:\n• It must not be spoofable\n• It indicates where the request/content displayed originates from\n• If it relies on a shared secret with the user, this shared secret cannot be observed by a mixed reality capture (e.g. it may not be a gesture that can be seen by the camera)\n• It is consistent between immersive experiences in the same UA\n\nBroadly speaking, there are two options for user agents who wish to support trusted UI. One option is , which is a trusted UI which does not exit immersive mode. Implementing trusted immersive UI can be challenging because buffers fill the XR Device display and the User Agent does not typically \"reserve\" pixels for its own use. User agents are not required to support trusted immersive UI, they may instead temporarily pause/exit immersive mode and show non-immersive trusted UI to the user.\n\nThe ability to read input information (head pose, input pose, etc) poses a risk to the integrity of trusted UI as the page may use this information to snoop on the choices made by the user while interacting with the trusted UI, including guessing keyboard input. To prevent this risk the user agent MUST set the visibility state of all s to or when the user is interacting with trusted UI (immersive or non-immersive) such as URL bars or system dialogs. Additionally, to prevent a malicious page from being able to monitor input on other pages the user agent MUST set the 's visibility state to if the currently focused area does not belong to the document which created the .\n\nWhen choosing between using or for a particular instance of trusted UI, the user agent MUST consider whether head pose information is a security risk. For example, trusted UI involving text input, especially password inputs, can potentially leak the typed text through the user’s head pose as they type. The user agent SHOULD also stop exposing any eye tracking-related information in such cases.\n\nThe user agent MUST use trusted UI to show permissions prompts.\n\nIf the virtual environment does not consistently track the user’s head motion with low latency and at a high frame rate the user may become disoriented or physically ill. Since it is impossible to force pages to produce consistently performant and correct content the user agent MUST provide a tracked, trusted environment and an XR Compositor which runs asynchronously from page content. The compositor is responsible for compositing the trusted and untrusted content. If content is not performant, does not submit frames, or terminates unexpectedly the user agent should be able to continue presenting a responsive, trusted UI.\n\nAdditionally, page content has the ability to make users uncomfortable in ways not related to performance. Badly applied tracking, strobing colors, and content intended to offend, frighten, or intimidate are examples of content which may cause the user to want to quickly exit the XR experience. Removing the XR device in these cases may not always be a fast or practical option. To accommodate this the user agent MUST provide users with an action, such as pressing a reserved hardware button or performing a gesture, that escapes out of WebXR content and displays the user agent’s trusted UI.\n\nThe trusted UI must be drawn by an independent rendering context whose state is isolated from any rendering contexts used by the page. (For example, any WebGL rendering contexts.) This is to prevent the page from corrupting the state of the trusted UI’s context, which may prevent it from properly rendering a tracked environment. It also prevents the possibility of the page being able to capture imagery from the trusted UI, which could lead to private information being leaked.\n\nAlso, to prevent CORS-related vulnerabilities each browsing context will see a new instance of objects returned by the API, such as . Attributes such as the context set on an with one relevant realm should not be able to be read through an with a relevant realm that does not have the same origin. Similarly, methods invoked on the API MUST NOT cause an observable state change on other browsing contexts. For example: No method will be exposed that enables a system-level orientation reset, as this could be called repeatedly by a malicious page to prevent other pages from tracking properly. The user agent MUST, however, respect system-level orientation resets triggered by a user gesture or system menu.\n\nNote: This doesn’t apply to state changes that are caused by one browsing context entering immersive mode, acquiring a lock on the device, and potentially firing devicechange events on other browsing contexts.\n\nGiven that the API describes hardware available to the user and its capabilities it will inevitably provide additional surface area for fingerprinting. While it’s impossible to completely avoid this, user agents should take steps to mitigate the issue. This spec limits reporting of available hardware to only a single device at a time, which prevents using the rare cases of multiple headsets being connected as a fingerprinting signal. Also, the devices that are reported have no string identifiers and expose very little information about the devices capabilities until an XRSession is created, which requires additional protections when sensitive information will be exposed.\n\nBecause can be called without user activation it may be used as a fingerprinting vector.\n\nThe \"xr-session-supported\"’s permission-related algorithms and types are defined as follows:\n\nConsiderations for when to automatically grant \"xr-session-supported\"\n\nThis specification defines a policy-controlled feature that controls whether any that requires the use of spatial tracking may be returned by , and whether support for session modes that require spatial tracking may be indicated by either or devicechange events on the object.\n\nThe feature identifier for this feature is .\n\nThe default allowlist for this feature is .\n\nNote: If the document’s origin is not allowed to use the permissions policy any immersive sessions will be blocked, because all immersive sessions require some use of spatial tracking. Inline sessions will still be allowed, but restricted to only using the .\n\nThe [permissions] API provides a uniform way for websites to request permissions from users and query which permissions they have been granted.\n\nThe powerful feature’s permission-related algorithms and types are defined as follows:\n\nChanges from the Candidate Recommendation Snapshot, 31 March 2022\n• First draft for adding a property to XRInputSource to say it’s visible elsewhere (GitHub #1353)\n\nChanges from the Working Draft 24 July 2020\n• Only allow sessions to use features they explicity request or are implicitly granted based on mode (GitHub #1189)\n• Reject promise returned from end() if session is already ended (GitHub #1170)\n• During requestAnimationFrame detect if session has ended (GitHub #1169)\n• Switch isSessionSupported from using user intent to using permissions (GitHub #1136)\n• Ensure that pending render state is always applied (GitHub #1128)\n• Change timing of when updateRenderState changes apply (GitHub #1111)\n\nChanges from the Working Draft 10 October 2019\n• Primary views MUST always be active (GitHub #1105)\n• Fixup frame and viewport caching to be explicit (GitHub #1093)\n• Clarify threading nature of \"ensure an immersive device is selected\", deprecate xrCompatible (GitHub #1081)\n• Various changes around null and emulated poses (GitHub #1058)\n• Minor change to when empty input profile arrays are appropriate. (GitHub #1037)\n• Allow trusted ui to use visible-blurred, cautioning against text input leakage (GitHub #1034)\n• Cleanups on how we do tasks and promises (GitHub #1032)\n• Short circuit updateRenderState() if no render state is passed (GitHub #1031)\n• Removed use of responsible and active and focused documents (GitHub #1030)\n• Clarify situation around browsing contexts and realms in context isolation (GitHub #1029)\n• Explicitly specify that reset events work on offset spaces (GitHub #1024)\n• Made it explicit which realm each object gets created in (GitHub #1023)\n• Session feature requests no longer need the session parameter (GitHub #1012)\n• Allow cancelling rAF callbacks from within rAF (GitHub #1005)\n• Mention that the opaque framebuffer holds a reference to a particular session (GitHub #1004)\n• Documented the effects of the framebufferScaleFactor (GitHub #993)\n• Allow more flexibility in what isSessionSupported returns (GitHub #986)\n• Clarify when tracking/input data is exposed via inline devices (GitHub #985)\n• Specify that preserveDrawingBuffer has no power here (GitHub #975)\n• Clarified the behavior of visiblityState for inline sessions (GitHub #974)\n• Defining when an opaque framebuffer is considered dirty (GitHub #970)\n• Potentially update the inline device when the device changes (GitHub #947)\n• Better define how depthNear and depthFar are used (GitHub #888)\n• Clarify that emulatedPosition is not true when the local-floor space is using an estimated height (GitHub #871)\n\nChanges from the First Public Working Draft 5 Feburary 2019\n• Added XRInputSource->profiles for list of input profile name (GitHub #695)\n• Describe how the input source list is maintained (GitHub #628)\n• Added XRPose and related refactors to the spec (GitHub #496)\n• Decribe the required clear behavior of an XRWebGLLayer (GitHub #866)\n• Specified that XRWebGLLayer framebuffers always use premultiplied alpha (GitHub #840)\n• Clarify the transform direction for the reset event (GitHub #843)\n• Check whether a session is inline rather than immersive when appropriate (GitHub #834)\n• Ensure an immersive device is selected in makeXRCompatible() (GitHub #809)\n• Change features to a sequence of \\'any\\' (GitHub #807)\n• Explicitly mention how depth/alpha/stencil values get used (GitHub #800)\n• Clarify when the reset event gets fired (GitHub #637)\n• Explicitly spec out when requestReferenceSpace() can reject queries (GitHub #651)\n• Move racy parts of requestSession() to the main thread (GitHub #706)\n• Clarify that small overlay UIs are allowed in exclusive access (GitHub #709)\n• Merge \\'end the session\\' with \\'shut down the session\\', clarify, add onend event (GitHub #710)\n• Removes references to XRInputSource.gamepad from explainer and index.bs (GitHub #782)\n• getViewport with an invalid view throws an error (GitHub #771)\n• Clarify what value a touchpad should report when not being touched (GitHub #660)\n• Explicitly specify how the views array is populated (GitHub #614)\n• Identify cases where the Gamepad id should be unknown (GitHub #615)\n• inverse attribute always returns the same object (GitHub #586)\n• Specify that projection matrices may include shear (GitHub #575)\n• Change XRRigidTransform inverse from a method to an attribute (GitHub #560)\n• Indicate when compositing is using depth values (GitHub #563)\n• Specify that getViewerPose throws an error for non-rAF XRFrames (GitHub #535)\n• Changed XRHandedness enum to use \\'none\\' instead of \\'\\' (GitHub #526)\n• Indicate the preferred ergonomics of a tracked-pointer ray (GitHub #524)\n• Specify that frame callbacks are not called without a base layer (GitHub #512)\n\nThank you to the following individuals for their contributions the WebXR Device API specification:\n\nAnd a special thanks to Vladimir Vukicevic (Unity) for kick-starting this whole adventure!"
    },
    {
        "link": "https://immersive-web.github.io/webxr/explainer.html",
        "document": "The WebXR Device API provides access to input and output capabilities commonly associated with Virtual Reality (VR) and Augmented Reality (AR) devices. It allows you develop and host VR and AR experiences on the web.\n• What is WebXR?\n• What’s the X in XR mean?\n• Is this API affiliated with OpenXR?\n• Advanced functionality\n• Preventing the compositor from using the depth buffer\n• Changing the Field of View for inline sessions\n• Appendix A: I don’t understand why this is a new API. Why can’t we use…\n\nEnable XR applications on the web by allowing pages to do the following:\n• Detect if XR capabilities are available.\n• Poll the XR device and associated input device state.\n• Display imagery on the XR device at the appropriate frame rate.\n• Define how a Virtual Reality or Augmented Reality browser would work.\n• Expose every feature of every piece of VR/AR hardware.\n\nExamples of supported devices include (but are not limited to):\n\nWhat’s the X in XR mean?\n\nThere’s a lot of “_____ Reality” buzzwords flying around today. Virtual Reality, Augmented Reality, Mixed Reality… it can be hard to keep track, even though there’s a lot of similarities between them. This API aims to provide foundational elements to do all of the above. And since we don’t want to be limited to just one facet of VR or AR (or anything in between) we use “X”, not as part of an acronym but as an algebraic variable of sorts to indicate “Your Reality Here”. We’ve also heard it called “Extended Reality” and “Cross Reality”, which seem fine too, but really the X is whatever you want it to be!\n\nIs this API affiliated with OpenXR?\n\nKhronos’ upcoming OpenXR API does cover the same basic capabilities as the WebXR Device API for native applications. As such it may seem like WebXR and OpenXR have a relationship like WebGL and OpenGL, where the web API is a near 1:1 mapping of the native API. This is not the case with WebXR and OpenXR, as they are distinct APIs being developed by different standards bodies.\n\nThat said, given the shared subject matter many of the same concepts are represented by both APIs in different ways and we do expect that once OpenXR becomes publically available it will be reasonable to implement WebXR’s feature set using OpenXR as one of multiple possible native backends.\n\nGiven the marketing of early XR hardware to gamers, one may naturally assume that this API will primarily be used for development of games. While that’s certainly something we expect to see given the history of the WebGL API, which is tightly related, we’ll probably see far more “long tail”-style content than large-scale games. Broadly, XR content on the web will likely cover areas that do not cleanly fit into the app-store models being used as the primary distribution methods by all the major VR/AR hardware providers, or where the content itself is not permitted by the store guidelines. Some high level examples are:\n\n360° and 3D video are areas of immense interest (for example, see ABC’s 360° video coverage), and the web has proven massively effective at distributing video in the past. An XR-enabled video player would, upon detecting the presence of XR hardware, show a “View in VR” button, similar to the “Fullscreen” buttons present in today’s video players. When the user clicks that button, a video would render in the headset and respond to natural head movement. Traditional 2D video could also be presented in the headset as though the user is sitting in front of a theater-sized screen, providing a more immersive experience.\n\nSites can provide easy 3D visualizations through WebXR, often as a progressive improvement to their more traditional renderings. Viewing 3D models (e.g., SketchFab), architectural previsualizations, medical imaging, mapping, and basic data visualization can all be more impactful, easier to understand, and convey an accurate sense of scale in VR and AR. For those use cases, few users would justify installing a native app, especially when web content is simply a link or click away.\n\nHome shopping applications (e.g., Matterport) serve as particularly effective demonstrations. Depending on device capabilities, sites can scale all the way from a simple photo carousel to an interactive 3D model on screen to viewing the walkthrough in VR, giving users the impression of actually being present in the house. The ability for this to be a low-friction experience for users is a huge asset for both users and developers, since they don’t need to convince users to install a heavy (and possibly malicious) executable before hand.\n\nVR provides an interesting canvas for artists looking to explore the possibilities of a new medium. Shorter, abstract, and highly experimental experiences are often poor fits for an app-store model, where the perceived overhead of downloading and installing a native executable may be disproportionate to the content delivered. The web’s transient nature makes these types of applications more appealing, since they provide a frictionless way of viewing the experience. Artists can also more easily attract people to the content and target the widest range of devices and platforms with a single code base.\n\nThe basic steps most WebXR applications will go through are:\n• Query to see if the desired XR mode is supported.\n• If support is available, advertise XR functionality to the user.\n• A user-activation event indicates that the user wishes to use XR.\n• Request an immersive session from the device\n• Use the session to run a render loop that updates sensor data, and produces graphical frames to be displayed on the XR device.\n• Continue producing frames until the user indicates that they wish to exit XR mode.\n\nIn the following sections, the code examples will demonstrate the core API concepts through this lifecycle sequence using immersive VR sessions first, and then cover the differences in introduced by inline sessions afterwards. The code examples should be read as all belonging to the same application.\n\nThe UA will identify an available physical unit of XR hardware that can present immersive content to the user. Content is considered to be “immersive” if it produces visual, audio, haptic, or other sensory output that simulates or augments various aspects of the users environment. Most frequently this involves tracking the user’s motion in space and producing outputs that are synchronized to the user’s movement. On desktop clients this will usually be a headset peripheral; on mobile clients it may represent the mobile device itself in conjunction with a viewer harness (e.g., Google Cardboard/Daydream or Samsung Gear VR). It may also represent devices without stereo-presentation capabilities but with more advanced tracking, such as ARCore/ARKit-compatible devices. Any queries for XR capabilities or functionality are implicitly made against this device.\n\nIt’s possible that even if no XR device is available initially, one may become available while the application is running, or that a previously available device becomes unavailable. This will be most common with PC peripherals that can be connected or disconnected at any time. Pages can listen to the event emitted on to respond to changes in device availability after the page loads. (XR devices already available when the page loads will not cause a event to be fired.) fires an event of type .\n\nInteracting with an XR device is done through the interface, but before any XR-enabled page requests a session it should first query to determine if the type of XR content desired is supported by the current hardware and UA. If it is, the page can then advertise XR functionality to the user. (For example, by adding a button to the page that the user can click to start XR content.)\n\nThe function is used to check if the device supports the XR capabilities the application needs. It takes an “XR mode” describing the desired functionality and returns a promise which resolves with true if the device can successfully create an using that mode. The call resolves with false otherwise.\n\nQuerying for support this way is necessary because it allows the application to detect what XR modes are available prior to requesting an , which may engage the XR device sensors and begin presentation. This can incur significant power or performance overhead on some systems and may have side effects such as taking over the user’s screen, launching a status tray or storefront, or terminating another application’s access to XR hardware. Calling must not interfere with any running XR applications on the system or have any user-visible side effects.\n\nThere are two XR modes that can be requested:\n\nInline: Requested with the mode enum . Inline sessions do not have the ability to display content on the XR device, but may be allowed to access device tracking information and use it to render content on the page. (This technique, where a scene rendered to the page is responsive to device movement, is sometimes referred to as “Magic Window” mode.) UAs implementing the WebXR Device API must guarantee that inline sessions can be created, regardless of XR device presence, unless blocked by page feature policy.\n\nImmersive VR: Requested with the mode enum . Immersive VR content is presented directly to the XR device (for example: displayed on a VR headset). Immersive VR sessions must be requested within a user activation event or within another callback that has been explicitly indicated to allow immersive session requests.\n\nIt should be noted that an immersive VR session may still display the users environment on see-through displays such as a HoloLens. See Handling non-opaque displays for more details.\n\nThis document will use the term “immersive session” to refer to immersive VR sessions throughout.\n\nIn the following examples we will explain the core API concepts using immersive VR sessions first, and cover the differences introduced by inline sessions afterwards. With that in mind, this code checks for support of immersive VR sessions, since we want the ability to display content on a device like a headset.\n\nAfter confirming that the desired mode is available with , the application will need to request an instance with the method in order to interact with XR device’s presentation or tracking capabilities.\n\nIn this sample, the function, which is assumed to be run by clicking the “Enter VR” button in the previous sample, requests an that operates in mode. The method returns a promise that resolves to an upon success. In addition to the , developers may supply an dictionary containing the capabilities that the returned session must have. For more information, see Feature dependencies.\n\nIf resolved to true for a given mode, then requesting a session with the same mode should be reasonably expected to succeed, barring external factors (such as not being called in a user activation event for an immersive session.) The UA is ultimately responsible for determining if it can honor the request.\n\nOnly one immersive session per XR hardware device is allowed at a time across the entire UA. If an immersive session is requested and the UA already has an active immersive session or a pending request for an immersive session, then the new request must be rejected. All inline sessions are suspended when an immersive session is active. Inline sessions are not required to be created within a user activation event unless paired with another option that explicitly does require it.\n\nOnce the session has started, some setup must be done to prepare for rendering.\n• An should be created to establish a space in which data will be defined. See the Spatial Tracking Explainer for more information.\n• An must be created and set as the ’s . ( because future versions of the spec will likely enable multiple layers.)\n• Then must be called to start the render loop pumping.\n\nThe content to present to the device is defined by an . This is set via the ’s function. takes a dictionary containing new values for a variety of options affecting the session’s rendering, including . Only the options specified in the dictionary are updated.\n\nFuture extensions to the spec will define new layer types. For example: a new layer type would be added to enable use with any new graphics APIs that get added to the browser. The ability to use multiple layers at once and have them composited by the UA will likely also be added in a future API revision.\n\nIn order for a WebGL canvas to be used with an , its context must be compatible with the XR device. This can mean different things for different environments. For example, on a desktop computer this may mean the context must be created against the graphics adapter that the XR device is physically plugged into. On most mobile devices though, that’s not a concern so the context will always be compatible. In either case, the WebXR application must take steps to ensure WebGL context compatibility before using it with an .\n\nWhen it comes to ensuring canvas compatibility there’s two broad categories that apps will fall under.\n\nXR Enhanced: The app can take advantage of XR hardware, but it’s used as a progressive enhancement rather than a core part of the experience. Most users will probably not interact with the app’s XR features, and as such asking them to make XR-centric decisions early in the app lifetime would be confusing and inappropriate. An example would be a news site with an embedded 360 photo gallery or video. (We expect the large majority of early WebXR content to fall into this category.)\n\nThis style of application should call ’s method. This will set a compatibility bit on the context that allows it to be used. Contexts without the compatibility bit will fail when attempting to create an with them.\n\nIn the event that a context is not already compatible with the XR device the context will be lost and attempt to recreate itself using the compatible graphics adapter. It is the page’s responsibility to handle WebGL context loss properly, recreating any necessary WebGL resources in response. If the context loss is not handled by the page, the promise returned by will fail. The promise may also fail for a variety of other reasons, such as the context being actively used by a different, incompatible XR device.\n\nXR Centric: The app’s primary use case is displaying XR content, and as such it doesn’t mind initializing resources in an XR-centric fashion, which may include asking users to select a headset as soon as the app starts. An example would be a game which is dependent on XR presentation and input. These types of applications can avoid the need to call and the possible context loss that it may trigger by setting the flag in the WebGL context creation arguments.\n\nEnsuring context compatibility with an XR device through either method may have side effects on other graphics resources in the page, such as causing the entire user agent to switch from rendering using an integrated GPU to a discrete GPU.\n\nIf the system’s underlying XR device changes (signaled by the event on the object) any previously set context compatibility bits will be cleared, and will need to be called again prior to using the context with a . Any active sessions will also be ended, and as a result new s with corresponding new s will need to be created.\n\nThe WebXR Device API provides information about the current frame to be rendered via the object which developers must examine in each iteration of the render loop. From this object the frame’s can be queried, which contains the information about all the views which must be rendered in order for the scene to display correctly on the XR device.\n\nobjects are not updated automatically. To present new frames, developers must use ’s method. When the callback functions are run, they are passed both a timestamp and an . They will contain fresh rendering data that must be used to draw into the s during the callback.\n\nA new is created for each batch of callbacks or for certain events that are associated with tracking data. objects act as snapshots of the state of the XR device and all associated inputs. The state may represent historical data, current sensor readings, or a future projection. Due to it’s time-sensitive nature, an is only valid during the execution of the callback that it is passed into. Once control is returned to the browser any active objects are marked as inactive. Calling any method of an inactive will throw an .\n\nThe also makes a copy of the ’s , such as values and the , just prior to the callbacks in the current batch being called. This captured is what will be used when computing view information like projection matrices and when the frame is being composited by the XR hardware. Any subsequent calls the developer makes to will not be applied until the next ’s callbacks are processed.\n\nThe timestamp provided is acquired using identical logic to the processing of callbacks. This means that the timestamp is a set to the current time when the frame’s callbacks begin processing. Multiple callbacks in a single frame will receive the same timestamp, even though time has elapsed during the processing of previous callbacks. In the future if additional, XR-specific timing information is identified that the API should provide, it is recommended that it be via the object.\n\nThe s framebuffer is created by the UA and behaves similarly to a canvas’s default framebuffer. Using , , , and will all generate an INVALID_OPERATION error. Additionally, outside of an ’s callback the framebuffer will be considered incomplete, reporting FRAMEBUFFER_UNSUPPORTED when calling . Attempts to draw to it, clear it, or read from it generate an INVALID_FRAMEBUFFER_OPERATION error as indicated by the WebGL specification.\n\nOnce drawn to, the XR device will continue displaying the contents of the framebuffer, potentially reprojected to match head motion, regardless of whether or not the page continues processing new frames. Potentially future spec iterations could enable additional types of layers, such as video layers, that could automatically be synchronized to the device’s refresh rate.\n\nEach the scene will be drawn from the perspective of a “viewer”, which is the user or device viewing the scene, described by an . Developers retrieve the current by calling on the and providing an for the pose to be returned in. Due to the nature of XR tracking systems, this function is not guaranteed to return a value and developers will need to respond appropriately. For more information about what situations will cause to fail and recommended practices for handling the situation, refer to the Spatial Tracking Explainer.\n\nThe contains a attribute, which is an array of s. Each has a and that should be used when rendering with WebGL. The is also passed to an ’s method to determine what the WebGL viewport should be set to when rendering. This ensures that the appropriate perspectives of scene are rendered to the correct portion on the ’s in order to display correctly on the XR hardware.\n\nEach attribute of each is an consisting of a and . (See the definition of an in the spatial tracking explainer for more details.) These should be treated as the locations of virtuals “cameras” within the scene. If the application is using a library to assist with rendering, it may be most natural to apply these values to a camera object directly, like so:\n\nOr it may be easier to pass the transform in as a view matrix, especially if the application makes WebGL calls directly. In that case the matrix needed will typically be the inverse of the view transform, which can easily be acquired from the attribute of the .\n\nIn both cases the ’s should be used as-is. Altering it may cause incorrect output to the XR device and significant user discomfort.\n\nBecause the inherits from it also contains a describing the position and orientation of the viewer as a whole relative to the origin. This is primarily useful for rendering a visual representation of the viewer for spectator views or multi-user environments.\n\nEach the needs to be modified to fit with the orientation values for the AudioContext.listener front and up values. Note that in the , the position and orientation move along with the headset (and presumably the user’s head). This means it has a always at the , so only the orientation of the audio listener will change in this ` xrReferenceSpace`. It’s also important to clarify that there’s no such thing as the listener position. The scene can have multiple coexisting coordinate systems. In this example, you’re getting the viewer pose in a specific xrReferenceSpace, and using the pose transform matrix to update the AudioListener with position and orientation in that reference space’s coordinate system. The unstated assumption is that the audio sources will also use coordinates in that same reference space’s coordinate system, and if that’s the case you’ll get a consistent experience. It would be perfectly valid (if a bit odd) to do everything in viewer space, keeping the AudioListener at the viewer space origin with fixed forward along -z and up along +y in that space, and then ensure that the coordinates for audio sources are in this same viewer space, relative to the current head position and orientation.\n\nHere is an example of how to connect the to the :\n\nThe UA may temporarily hide a session at any time. While hidden a session has restricted access to the XR device state and frames will not be processed. Hidden sessions can be reasonably be expected to be made visible again at some point, usually when the user has finished performing whatever action triggered the session to hide in the first place. This is not guaranteed, however, so applications should not rely on it.\n\nThe UA may hide a session if allowing the page to continue reading the headset position represents a security or privacy risk (like when the user is entering a password or URL with a virtual keyboard, in which case the head motion may infer the user’s input), or if content external to the UA is obscuring the page’s output.\n\nIn other situations the UA may also choose to keep the session content visible but “blurred”, indicating that the session content is still visible but no longer in the foreground. While blurred the page may either refresh the XR device at a slower rate or not at all, poses queried from the device may be less accurate, and all input tracking will be unavailable. If the user is wearing a headset the UA is expected to present a tracked environment (a scene which remains responsive to user’s head motion) or reproject the throttled content when the page is being throttled to prevent user discomfort.\n\nThe session should continue requesting and drawing frames while blurred, but should not depend on them being processed at the normal XR hardware device framerate. The UA may use these frames as part of it’s tracked environment or page composition, though the exact presentation of frames produced by a blurred session will differ between platforms. They may be partially occluded, literally blurred, greyed out, or otherwise de-emphasized.\n\nSome applications may wish to respond to the session being hidden or blurred by halting game logic, purposefully obscuring content, or pausing media. To do so, the application should listen for the events from the . For example, a 360 media player would do this to pause the video/audio whenever the UA has obscured it.\n\nA is “ended” when it is no longer expected to be used. An ended session object becomes detached and all operations on the object will fail. Ended sessions cannot be restored, and if a new active session is needed it must be requested from .\n\nTo manually end a session the application calls ’s method. This returns a promise that, when resolved, indicates that presentation to the XR hardware device by that session has stopped. Once the session has ended any continued animation the application requires should be done using .\n\nThe UA may end a session at any time for a variety of reasons. For example: The user may forcibly end presentation via a gesture to the UA, other native applications may take exclusive access of the XR hardware device, or the XR hardware device may become disconnected from the system. Additionally, if the system’s underlying XR device changes (signaled by the event on the object) any active s will be ended. This applies to both immersive and inline sessions. Well behaved applications should monitor the event on the to detect when the UA forces the session to end.\n\nIf the UA needs to halt use of a session temporarily, the session should be suspended instead of ended. (See previous section.)\n\nWhen authoring content to be viewed immersively, it may be beneficial to use an session to view the same content in a 2D browser window. Using an inline session enables content to use a single rendering path for both inline and immersive presentation modes. It also makes switching between inline content and immersive presentation of that content easier.\n\nA created with an session will not allocate a new WebGL framebuffer but instead set the attribute to . That way when is bound all WebGL commands will naturally execute against the WebGL context’s default framebuffer and display on the page like any other WebGL content. When that layer is set as the ’s the inline session is able to render it’s output to the page.\n\nImmersive and inline sessions may run their render loops at at different rates. During immersive sessions the UA runs the rendering loop at the XR device’s native refresh rate. During inline sessions the UA runs the rendering loop at the refresh rate of page (aligned with .) The method of computation of projection and view matrices also differs between immersive and inline sessions, with inline sessions taking into account the output canvas dimensions and possibly the position of the users head in relation to the canvas if that can be determined.\n\nwill always resolve to when checking the support of sessions. The UA should not reject requests for an inline session unless the page’s feature policy prevents it or unless a required feature is unavailable as described in Feature dependencies). For example, the following use cases all depend on additional reference space types which would need to be enabled via the :\n• Taking advantage of 6DoF tracking on devices with no associated headset, like ARCore or ARKit enabled phones. (Note that this does not provide automatic camera access or composition)\n• Making use of head-tracking features for devices like zSpace systems.\n\nBeyond the core APIs described above, the WebXR Device API also exposes several options for taking greater advantage of the XR hardware’s capabilities.\n\nOnce developers have mastered session creation and rendering, they will often be interested in using additional WebXR features that may not be universally available. While developers are generally encouraged to design for progressive enhancement, some experiences may have requirements on features that are not guaranteed to be universally available. For example, an experience which requires users to move around a large physical space, such as a guided tour, would not function on an Oculus Go because it is unable to provide an reference space. If an experience is completely unusable without a specific feature, it would be a poor user experience to initialize the underlying XR platform and create a session only to immediately notify the user it won’t work.\n\nFeatures may be unavailable for a number of reasons, among which is the fact not all devices which support WebXR can support the full set of features. Another consideration is that some features expose sensitive information which may require a clear signal of user intent before functioning. Any feature which requires this signal to be provided via explicit consent must request this consent prior to the session being created. This ensures a consistent experience across all hardware form-factors, regardless of whether the UA has a trusted immersive UI available.\n\nWebXR allows the following features to be requested:\n\nThis list is currently limited to a subset of reference space types, but in the future will expand to include additional features. Some potential future features under discussion that would be candidates for this list are: eye tracking, plane detection, geo alignment, etc.\n\nDevelopers communicate their feature requirements by categorizing them into one of the following sequences in the that can be passed into :\n• This feature must be available in order for the experience to function at all. If explicit consent is necessary, users will be prompted in response to . Session creation will be rejected if the feature is unavailable for the XR device, if the UA determines the user does not wish the feature enabled, or if the UA does not recognize the feature being requested.\n• The experience would like to use this feature for the entire session, but can function without it. Again, if explicit consent is necessary, users will be prompted in response to . However, session creation will succeed regardless of the feature’s hardware support or user intent. Developers must not assume optional features are available in the session and check the result from attempting to use them.\n\nThe following sample code represents the likely behavior of a warehouse-size experience. It depends on having an reference space and will reject creating the session if not available.\n\nThe following sample code shows an inline experience that would prefer to use motion tracking if available, but will fall back to using touch/mouse input if not.\n\nSome features recognized by the UA but not explicitly listed in these arrays will be enabled by default for a session. This is only done if the feature does not require a signal of user intent nor impact performance or the behavior of other features when enabled. At this time, only the following features will be enabled by default:\n\nWhile in immersive sessions, the UA is responsible for providing a framebuffer that is correctly optimized for presentation to the in each . Developers can optionally request the framebuffer size be scaled, though the UA may not respect the request. Even when the UA honors the scaling requests, the result is not guaranteed to be the exact percentage requested.\n\nFramebuffer scaling is done by specifying a at creation time. Each XR device has a default framebuffer size, which corresponds to a of . This default size is determined by the UA and should represent a reasonable balance between rendering quality and performance. It may not be the ‘native’ size for the device (that is, a buffer which would match the native screen resolution 1:1 at point of highest magnification). For example, mobile platforms such as GearVR or Daydream frequently suggest using lower resolutions than their screens are capable of to ensure consistent performance.\n\nIf the is set to a number higher or lower than the UA should create a framebuffer that is the default resolution multiplied by the given scale factor. So a of would specify a framebuffer with 50% the default height and width, and so on. The UA may clamp the scale factor however it sees fit, or may round it to a desired increment if needed (for example, fitting the buffer dimensions to powers of two if that is known to increase performance.)\n\nIn some cases the developer may want to ensure that their application is rendering at the ‘native’ size for the device. To do this the developer can query the scale factor that should be passed during layer creation with the function. (Note that in some cases the native scale may actually be less than the recommended scale of if the system is configured to render “superscaled” by default.)\n\nThis technique should be used carefully, since the native resolution on some headsets may be higher than the system is capable of rendering at a stable framerate without use of additional techniques such as foveated rendering. Also note that the UA’s scale clamping is allowed to prevent the allocation of native resolution framebuffers if it deems it necessary to maintain acceptable performance.\n\nFramebuffer scaling is typically configured once per session, but can be changed during a session by creating a new and updating the render state to apply that on the next frame:\n\nRescaling the framebuffer may involve reallocating render buffers and should only be done rarely, for example when transitioning from a game mode to a text-heavy menu mode or similar. See Dynamic viewport scaling for an alternative if your application needs more frequent adjustments.\n\nDynamic viewport scaling allows applications to only use a subset of the available framebuffer. This is intended for fine-grained performance tuning where the desired render resolution changes frequently, and can be adjusted on a frame-by-frame basis. A typical use case would be rendering scenes with highly variable complexity, for example where the user may move their viewpoint to closely examine a model with a complex shader. (If an application wanted to keep this constant for a session, it should use instead, see Controlling rendering quality.)\n\nThis is an opt-in feature for applications, it is activated by calling on an , followed by a call to which applies the change and returns the updated viewport:\n\nNOTE: Dynamic viewport scaling is a recent addition to WebXR, and implementations may not provide the API yet. For compatibility, consider adding a check to ensure that the API exists.\n\nThe feature may not be available on all systems since it depends on driver support. If it is unsupported, the system will ignore the requested scale and continue using the full-sized viewport. If necessary, the application can compare the sizes returned by across animation frames to confirm if the feature is active, for example if it would want to use an alternate performance tuning method such as reducing scene complexity as a fallback.\n\nFor consistency, the result for any given view is always fixed for the duration of an animation frame. If is used before the first call, the change applies immediately for the current animation frame. Otherwise, the change is deferred until is called again in a future animation frame.\n\nUser agents can optionally provide a attribute on an with a suggested value based on internal performance heuristics. This attribute is null if the user agent doesn’t provide a recommendation. A call has no effect, so applications could use the following code to apply the heuristic only if it exists:\n\nAlternatively, applications could modify the recommended scale, i.e. clamping it to a minimum scale to avoid text becoming unreadable, or use their own heuristic based on data such as current visible scene complexity and recent framerate average.\n\nThe projection matrices given by the s take into account not only the field of view of presentation medium but also the depth range for the scene, defined as a near and far plane. WebGL fragments rendered closer than the near plane or further than the far plane are discarded. By default the near plane is 0.1 meters away from the user’s viewpoint and the far plane is 1000 meters away.\n\nSome scenes may benefit from changing that range to better fit the scene’s content. For example, if all of the visible content in a scene is expected to remain within 100 meters of the user’s viewpoint, and all content is expected to appear at least 1 meter away, reducing the range of the near and far plane to will lead to more accurate depth precision. This reduces the occurrence of z fighting (or aliasing), an artifact which manifests as a flickery, shifting pattern when closely overlapping surfaces are rendered. Conversely, if the visible scene extends for long distances you’d want to set the far plane far enough away to cover the entire visible range to prevent clipping, with the tradeoff being that further draw distances increase the occurrence of z fighting artifacts. The best practice is to always set the near and far planes to as tight of a range as your content will allow.\n\nTo adjust the near and far plane distance, and values can be given in meters when calling .\n\nPreventing the compositor from using the depth buffer\n\nBy default the depth attachment of an ’s , if present, may be used to assist the XR compositor. For example, the scene’s depth values may be used by advanced reprojection techniques or to help avoid depth conflicts when rendering platform/UA interfaces. This assumes, of course, that the values in the depth buffer are representative of the scene content.\n\nSome applications may violate that assumption, such as when using certain deferred rendering techniques or rendering stereo video. In those cases if the depth buffer’s values are used by the compositor it may result in objectionable artifacts. To avoid this, the compositor can be instructed to ignore the depth values of an by setting the option to at layer creation time:\n\nIf is not set to the The UA is allowed (but not required) to use depth buffer as it sees fit. As a result, barring compositor access to the depth buffer in this way may lead to certain platform or UA features being unavailable or less robust. To detect if the depth buffer is being used by the compositor, check the attribute of the after the layer is created. A value of indicates that the depth buffer will not be utilized by the compositor even if was set to during layer creation.\n\nChanging the Field of View for inline sessions\n\nWhenever possible the matrices given by ’s attribute should make use of physical properties, such as the headset optics or camera lens, to determine the field of view to use. Most inline content, however, won’t have any physically based values from which to infer a field of view. In order to provide a unified render pipeline for inline content an arbitrary field of view must be selected.\n\nBy default a vertical field of view of 0.5π radians (90 degrees) is used for inline sessions. The horizontal field of view can be computed from the vertical field of view based on the width/height ratio of the ’s associated canvas.\n\nIf a different default field of view is desired, it can be specified by passing a new value, in radians, to the method:\n\nThe UA is allowed to clamp the value, and if a physically-based field of view is available it must always be used in favor of the default value.\n\nAttempting to set a value on an immersive session will cause to throw an . must return on immersive sessions.\n\nAppendix A: I don’t understand why this is a new API. Why can’t we use…\n\nThe data provided by an instance is similar to the data provided by the non-standard , with some key differences:\n• It’s an explicit polling interface, which ensures that new input is available for each frame. The event-driven data may skip a frame, or may deliver two updates in a single frame, which can lead to disruptive, jittery motion in an XR application.\n• events do not provide positional data, which is a key feature of high-end XR hardware.\n• More can be assumed about the intended use case of XR device data, so optimizations such as motion prediction can be applied.\n• events are typically not available on desktops.\n\nIt should be noted that events have not been standardized, have behavioral differences between browser, and there are ongoing efforts to change or remove the API. This makes it difficult for developers to rely on for a use case where accurate tracking is necessary to prevent user discomfort.\n\nThe events specification is superceded by Orientation Sensor specification that defines the and interfaces. This next generation API is purpose-built for WebXR Device API polyfill. It represents orientation data in WebGL-compatible formats (quaternion, rotation matrix), satisfies stricter latency requirements, and addresses known interoperability issues that plagued events by explicitly defining which low-level motion sensors are used in obtaining the orientation data.\n\nA local WebSocket service could be set up to relay headset poses to the browser. Some early VR experiments with the browser tried this route, and some tracking devices (most notably Leap Motion) have built their JavaScript SDKs around this concept. Unfortunately, this has proven to be a high-latency route. A key element of a good XR experience is low latency. For head mounted displays, ideally, the movement of your head should result in an update on the device (referred to as “motion-to-photons time”) in 20ms or less. The browser’s rendering pipeline already makes hitting this goal difficult, and adding more overhead for communication over WebSockets only exaggerates the problem. Additionally, using such a method requires users to install a separate service, likely as a native app, on their machine, eroding away much of the benefit of having access to the hardware via the browser. It also falls down on mobile where there’s no clear way for users to install such a service.\n\nSome people have suggested that we try to expose XR data through the Gamepad API, which seems like it should provide enough flexibility through an unbounded number of potential axes. While it would be technically possible, there are a few properties of the API that currently make it poorly suited for this use.\n• Axes are normalized to always report data in a range. That may work sufficiently for orientation reporting, but when reporting position or acceleration, you would have to choose an arbitrary mapping of the normalized range to a physical one (i.e., is equal to 2 meters or similar). However that forces developers to make assumptions about the capabilities of future XR hardware, and the mapping makes for error-prone and unintuitive interpretation of the data.\n• Axes are not explicitly associated with any given input, making it difficult for users to remember if axis is a component of devices’ position, orientation, acceleration, etc.\n• XR device capabilities can differ significantly, and the Gamepad API currently doesn’t provide a way to communicate a device’s features and its optical properties.\n• Gamepad features such as buttons have no clear meaning when describing an XR headset and its periphery.\n\nThere is a related effort to expose motion-sensing controllers through the Gamepad API by adding a attribute and some other related properties. Although these additions would make the API more accommodating for headsets, we feel that it’s best for developers to have a separation of concerns such that devices exposed by the Gamepad API can be reasonably assumed to be gamepad-like and devices exposed by the WebXR Device API can be reasonably assumed to be headset-like.\n\nIt’s important to realize that all of the alternative solutions offer no method of displaying imagery on the headset itself, with the exception of Cardboard-like devices where you can simply render a fullscreen split view. Even so, that doesn’t take into account how to communicate the projection or distortion necessary for an accurate image. Without a reliable presentation method the ability to query inputs from a headset becomes far less valuable.\n\nThere’s understandably some confusion between the WebXR and an API that some browsers have implemented at various points in the past called WebVR. Both handle communication with Virtual Reality hardware, and both have very similar names. So what’s the difference between these two APIs?\n\nWebVR was an API developed in the earliest days of the current generation of Virtual Reality hardware/software, starting around the time that the Oculus DK2 was announced. Native VR APIs were still in their formative stages, and the capabilities of commercial devices were still being determined. As such the WebVR API developed around some assumptions that would not hold true long term. For example, the API assumed that applications would always need to render a single left and right eye view of the scene, that the separation between eyes would only ever involve translation and not rotation, and that only one cannonical tracking space was necessary to support. In addition, the API design made forward compatibility with newer device types, like mobile AR, difficult, to the point that it may have necessitated a separate API. WebVR also made some questionable descisions regarding integration with the rest of the web platform, specifically in terms of how it interacted with WebGL and the Gamepad API. Despite this, it worked well enough in the short term that some UAs, especially those shipped specifically for VR devices, decided to ship the API to their users.\n\nIn the meantime the group that developed WebVR recognized the issues with the initial API, in part through feedback from developers and standards bodies, and worked towards resolving them. Eventually they recognized that in order to create a more scalable and more ergonomic API they would have to break backwards compatibility with WebVR. This new revision of the API was referred to as WebVR 2.0 for a while, but eventually was officially renamed WebXR in recognition of the fact that the new API would support both VR and AR content. Developement of WebXR has been able to benefit not only from the group’s experience with WebVR but also from a more mature landscape of immersive computing devices that now includes multiple commercial headsets, the emergence of both mobile and headset AR, and multiple mature native APIs.\n\nWebXR is intended to completely replace WebVR in the coming years. All browsers that initially shipped WebVR have committed to shipping WebXR in it’s place once the API design is finished. In the meanwhile, developers can code against WebXR, relying on the WebXR Polyfill to ensure their code runs in browsers with only WebVR implementations."
    },
    {
        "link": "https://codewave.com/insights/webxr-virtual-augmented-reality",
        "document": "Discover Hide What Our Win MeansAbout CodewaveWhy Does This Recognition"
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/API/WebXR_Device_API",
        "document": "This feature is not Baseline because it does not work in some of the most widely-used browsers.\n\nWebXR is a group of standards which are used together to support rendering 3D scenes to hardware designed for presenting virtual worlds (virtual reality, or VR), or for adding graphical imagery to the real world, (augmented reality, or AR). The WebXR Device API implements the core of the WebXR feature set, managing the selection of output devices, render the 3D scene to the chosen device at the appropriate frame rate, and manage motion vectors created using input controllers.\n\nWebXR-compatible devices include fully-immersive 3D headsets with motion and orientation tracking, eyeglasses which overlay graphics atop the real-world scene passing through the frames, and handheld mobile phones which augment reality by capturing the world with a camera and augment that scene with computer-generated imagery.\n\nTo accomplish these things, the WebXR Device API provides the following key capabilities:\n• Render a 3D scene to the device at an appropriate frame rate\n\nAt the most basic level, a scene is presented in 3D by computing the perspective to apply to the scene in order to render it from the viewpoint of each of the user's eyes by computing the position of each eye and rendering the scene from that position, looking in the direction the user is currently facing. Each of these two images is rendered into a single framebuffer, with the left eye's rendered image on the left and the right eye's viewpoint rendered into the right half of the buffer. Once both eyes' perspectives on the scene have been rendered, the resulting framebuffer is delivered to the WebXR device to be presented to the user through their headset or other appropriate display device.\n\nWhile the older WebVR API was designed solely to support Virtual Reality (VR), WebXR provides support for both VR and Augmented Reality (AR) on the web. Support for AR functionality is added by the WebXR Augmented Reality Module.\n\nA typical XR device can have either 3 or 6 degrees of freedom and might or might not have an external positional sensor.\n\nThe equipment may also include an accelerometer, barometer, or other sensors which are used to sense when the user moves through space, rotates their head, or the like."
    }
]