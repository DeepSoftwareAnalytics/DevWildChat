[
    {
        "link": "https://developer.apple.com/documentation/http-live-streaming/hls-authoring-specification-for-apple-devices-appendixes",
        "document": ""
    },
    {
        "link": "https://en.wikipedia.org/wiki/HTTP_Live_Streaming",
        "document": "HTTP Live Streaming (also known as HLS) is an HTTP-based adaptive bitrate streaming communications protocol developed by Apple Inc. and released in 2009. Support for the protocol is widespread in media players, web browsers, mobile devices, and streaming media servers. As of 2022 , an annual video industry survey has consistently found it to be the most popular streaming format.[2]\n\nHLS resembles MPEG-DASH in that it works by breaking the overall stream into a sequence of small HTTP-based file downloads, each downloading one short chunk of an overall potentially unbounded transport stream. A list of available streams, encoded at different bit rates, is sent to the client using an extended M3U playlist.[3]\n\nBased on standard HTTP transactions, HTTP Live Streaming can traverse any firewall or proxy server that lets through standard HTTP traffic, unlike UDP-based protocols such as RTP. This also allows content to be offered from conventional HTTP servers and delivered over widely available HTTP-based content delivery networks.[4][5][6] The standard also includes a standard encryption mechanism[7] and secure-key distribution using HTTPS, which together provide a simple DRM system. Later versions of the protocol also provide for trick-mode fast-forward and rewind and for integration of subtitles.\n\nApple has documented HTTP Live Streaming as an Internet Draft (Individual Submission), the first stage in the process of publishing it as a Request for Comments (RFC). As of December 2015, the authors of that document have requested the RFC Independent Stream Editor (ISE) to publish the document as an informational (non-standard) RFC outside of the IETF consensus process.[8] In August 2017, RFC 8216 was published to describe version 7 of the protocol.[9]\n\nHTTP Live Streaming uses a conventional web server, that implements support for HTTP Live Streaming (HLS), to distribute audiovisual content and requires specific software, such as OBS to fit the content into a proper format (codec) for transmission in real time over a network. The service architecture comprises:\n\nHTTP Live Streaming provides mechanisms for players to adapt to unreliable network conditions without causing user-visible playback stalling. For example, on an unreliable wireless network, HLS allows the player to use a lower quality video, thus reducing bandwidth usage. HLS videos can be made highly available by providing multiple servers for the same video, allowing the player to swap seamlessly if one of the servers fails.\n\nTo enable a player to adapt to the bandwidth of the network, the original video is encoded in several distinct quality levels. The server serves an index, called a master playlist, of these encodings, called variant streams. The player can then choose between the variant streams during playback, changing back and forth seamlessly as network conditions change.\n\nAt WWDC 2016 Apple announced[11] the inclusion of byte-range addressing for fragmented MP4 files, or fMP4, allowing content to be played via HLS without the need to multiplex it into MPEG-2 Transport Stream. The industry considered this as a step towards compatibility between HLS and MPEG-DASH.[12][13]\n\nTwo unrelated HLS extensions with a Low Latency name and corresponding acronym exist:\n• Apple Low Latency HLS (ALHLS) which was announced by Apple at WWDC2019 14\n• Community LHLS (LHLS) which predated Apple's publication and is allegedly simpler 15\n\nThe remainder of this section describes Apple's ALHLS. It reduces the glass-to-glass delay when streaming via HLS by reducing the time to start live stream playbacks and maintain that time during a live-streaming event. It works by adding partial media segment files into the mix, much like MPEG-CMAF's fMP4. Unlike CMAF, ALHLS also supports partial MPEG-2 TS transport files. A partial media segment is a standard segment (e.g. 6 seconds) split into equal segments of less than a second (e.g. 200 milliseconds). The standard first segment is replaced by the series of partial segments. Subsequent segments are of the standard size.[16] HTTP/2 is required to push the segments along with the playlist, reducing the overhead of establishing repeated HTTP/TCP connections.\n• Playlist Delta Updates: only sending what changed between playlists, which typically fit in a single MTU making it more efficient to load the playlists which, with large DVR windows, can be quite large.\n• Blocking of playlist reload: when requesting live media playlists, wait until the first segment is also ready, and return both at the same time (saving additional HTTP/TCP requests)\n• Rendition Reports: add metadata to other media renditions to make switching between ABR faster\n\nApple also added new tools: tsrecompressor produces and encodes a continuous low latency stream of audio and video. The mediastreamsegmenter tool is now available in a low-latency version. It is an HLS segmenter that takes in a UDP/MPEG-TS stream from tsrecompressor and generates a media playlist, including the new tags above.\n\nSupport for low-latency HLS is available in tvOS 13 beta, and iOS & iPadOS 14.[17] On April 30, 2020, Apple added the low latency specifications to the second edition of the main HLS specification.[18]\n\nDynamic ad insertion is supported in HLS using splice information based on SCTE-35 specification. The SCTE-35 splice message is inserted into the media playlist file using the EXT-X-DATERANGE tag. Each SCTE-35 splice_info_section() is represented by an EXT-X-DATERANGE tag with a SCTE35-CMD attribute. A SCTE-35 splice out/in pair signaled by the splice_insert() commands is represented by one or more EXT-X-DATERANGE tags carrying the same ID attribute. The SCTE-35 splice out command should have the SCTE35-OUT attribute and the splice in command should have the SCTE35-IN attribute.\n\nBetween the two EXT-X-DATERANGE tags that contain the SCTE35-OUT and SCTE35-IN attributes respectively, there may be a sequence of media segment URIs. These media segments normally represent ad programs that can be replaced by the local or customized ad. The ad replacement does not require the replacement of the media files, only the URIs in the playlist need to be changed to point to different ad programs. The ad replacement can be done on the origin server or on the client's media-playing device.\n• AT&T supports HLS in all formats live or on-demand.\n• Ant Media Server support HLS and Low Latency HLS 19 for live and on-demand streams.\n• Helix Universal Server from RealNetworks supports iPhone OS 3.0 and later for live and on-demand HTTP Live or On-Demand streaming of H.264 and AAC content to iPhone, iPad and iPod.\n• IIS Media Services from Microsoft supports live and on-demand Smooth Streaming and HTTP Live Streaming.\n• Nginx with the nginx-rtmp-module supports HLS in live mode. Commercial version Nginx Plus, which includes ngx_http_hls_module module, also supports HLS/HDS VOD. 22\n• Nimble Streamer supports HLS in live and VOD mode, Apple Low Latency HLS spec is also supported.\n• Node.js with the hls-server package supports hls encoding to live mode and local files conversion. 23\n• OvenMediaEngine is an open source project that supports Low Latency HLS (LL-HLS) and HLS for live streaming.\n• Storm Streaming Server supports HLS as backup mode for its Media Source Extensions player 24\n• TVersity supports HLS in conjunction with on-the-fly transcoding for playback of any video content on iOS devices.\n• Ustream supports HLS delivery of live broadcasts. The ingested stream is re-transcoded if the original audio and video codec falls outside HLS requirements.\n• VLC Media Player supports HLS for serving live and on-demand streams as of version 2.0.\n• Wowza Streaming Engine from Wowza Media Systems supports HLS and encrypted HLS for live (with DVR), on-demand streaming and Apple Low Latency HLS spec.\n• Microsoft added support for HTTP Live Streaming in EdgeHTML rendering engine in Windows 10 in 2015. 28\n• Yospace added HTTP Live Streaming support in Yospace HLS Player and SDK for flash version 1.0.[ ]\n• Sling Media added HTTP Live Streaming support to its Slingboxes and its SlingPlayer apps. 30\n• In 2014/15, the BBC introduced HLS-AAC streams for its live internet radio and on-demand audio services, and supports those streams with its iPlayer Radio clients. 31\n• Twitch uses HTTP Live Streaming (HLS) to transmit and scale the live streaming to many concurrent viewers, also supporting multiple variants (e.g., 1080p, 720p, etc.). 32\n\nHTTP Live Streaming is natively supported in the following operating systems:\n\nWindows 10 used to have native support for HTTP Live Streaming in EdgeHTML, a proprietary browser engine that was used in Microsoft Edge (now referred to as Edge Legacy) before the transition to the Chromium-based Blink browser engine. Edge Legacy was included in Windows 10 up till version 2004. It was replaced by Edge Chromium in version 20H2. Along with Windows 11, Microsoft released an updated Media Player that supports HLS natively."
    },
    {
        "link": "https://dyte.io/blog/hls-in-depth",
        "document": "HLS (HTTP Live Streaming) is a live streaming protocol that leverages current widespread HTTP technologies to deliver live video + audio experiences to large audiences.\n\nInitially developed by Apple in 2009, it has gained wide adoption across devices ranging from desktops and mobile phones to smart TVs. The iPhone drove its initial adoption, and having a significant market share in the mobile space, its supporting HLS by default was an excellent booster.\n\nLet’s understand how HLS works on a high level. Looking from a client’s (any web player's) perspective, these are the steps it follows to understand and play media:\n• It makes a GET request to a server to fetch a master playlist.\n• The client chooses the best bitrate from the list considering the user’s network conditions.\n• Next, it makes a GET request to fetch the playlist corresponding to that bitrate.\n• It understands the playlist, starts fetching different media segments, and starts playing.\n• It repeats the last two mentioned steps in a loop.\n\nThis was a rough plan for how clients work with HLS. Now let’s see what these playlists/segments actually are in more detail. To generate this data, I am going to take the help of the swiss-army knife of media-related stuff:\n\nThis creates a sample video of 30 seconds in duration.\n\nNow we'll create an HLS playlist from the following command:\n\nThis will create a playlist in the same dir, with files:\n\nPuzzled about what they are? Let’s understand what the command did and what these files mean.\n\nWe’ll first break down the command.\n\nWe are telling by to take MP4 video as input, note this can also take RTMP input directly; one just needs to say something like:\n\nThis is another exciting aspect we will discuss further in this article.\n\nNext, understanding and . This is a way of saying copy audio/video data of encoding, etc., from source input to output as it is. This is where you could do encoding in different formats too. For example, if you want more than one quality in your HLS playlist.\n\ntells what our output format will be, which in this case is HLS.\n\ntells what format we want our segment filenames to be in.\n\nAnd finally, the last argument is the name of our master playlist.\n\nNow, moving on to the files:\n\nOpening the master playlist in an editor, it looks like this:\n• : M3U(MP3 URL or Moving Picture Experts Group Audio Layer 3 Uniform Resource Locator in full) is a multimedia playlist format name, and this line indicates that we are using extended M3U format, according to HLS spec: \"It MUST be the first line of every Media Playlist and every Master Playlist.\"\n• : This tag indicates the compatibility version of our playlist file, there has been more than one iteration on the playlist format, and there are older clients who do not understand newer versions, so this becomes compulsory to send for any server-generating playlist format version greater than 1. It being present multiple times should lead to clients rejecting the whole playlist.\n• These were the as specified by the spec.\n• We will now move on to something called . The first among them is . It is a required flag again telling the client what will be the longest segment length it can expect. Its value is in seconds.\n• : This tag mentions the number of the first media segments. It is unnecessary and, by default, considered 0 when not present. Its only restriction is that it should appear before the first media segment.\n• : This tag comes under and indicates the duration and metadata for each media segment. Its format, according to spec, is: . Duration is required to be specified in floating point numbers when using any format above version 3. The title is mostly metadata for humans and optional, seems to have skipped it by default.\n• Each following line after specifies where to find the media segment. In our case, they are in the same folder, so they are directly listed, but they can also be in subfolders like , which would mean their name would be: . Note that is also a format given by us in the above command, and one can change it very easily.\n• : I guess the most difficult-to-understand tag? One would definitely not be able to look and tell what it does. Jokes aside, except for indicating that the list has ended, an exciting property is that it can appear anywhere in the playlist file. If manually building these files, you may want to be careful about placing it in the right place.\n\nFor keen readers, they may have noticed that no multiple bitrates are listed here. Well, I wanted to explain basic structure first with a simple example.\n\nTo understand ABR, let’s take an already hosted URL, something like:\n\nOn downloading the playlist, it looks something like this:\n\nOver here, we see a new tag . This tag gives more info about all the variants available for a given stream. The tag is to be followed by an arbitrary number of attributes.\n• : It is a compulsory attribute whose value tells us how many bits per second will be sent with this variant.\n• : It is another mandatory attribute specifying our codecs for video and audio, respectively.\n• : It is an optional attribute mentioning the variant’s pixel resolution.\n• : We can ignore this tag cause it is not specified in spec, though at the time of writing this article, there’s an open issue on hls.js to add support for it.\n• : It is an optional attribute specified as a quoted string. It must match the value of the attribute on a tag. This tag should also have as its attribute.\n• : It is an optional attribute, mostly the same as . Two things that differentiate it are:\n• Its attribute value can be a quoted string or .\n• Tag value on for closed captions should be .\n\nThe line following these stream-inf tags specifies where to find the corresponding variant stream, a playlist similar to what we saw earlier.\n\nEarlier, we saw an RTMP input command. It helped us understand another property of live HLS playlists. Revisiting the command, it looked like this:\n\nThe command should give an output similar to what we have below, which seems stuck.\n\nNow, let’s satisfy your dream of becoming a streamer! One can use any RTMP source, even , but in our demo, we will use OBS to send data to our server listening for RTMP input. I have created a simple screen share capture input in it and then navigated to section in , here you will see a screen similar to this:\n\nLet’s fill in our Service as Custom and add the Server with an empty Stream Key. Now click on apply and press .\n\nWe will land at the default view of OBS here:\n\nWhen we click on , we will stream to our local RTMP-to-HLS server. You will notice the folder in which you ran the command. There are new files with names:\n\nLet’s check in one instant, I captured this as:\n\nNotice how there’s no end playlist tag? That’s because this is a live playlist that keeps on changing over time with new segments. For example, after some time, my local playlist looks something like this:\n\nOld segments are replaced with new segments to load. This helps one deliver the latest data when live streaming an event.\n\nThis covers playlists to a reasonable extent. Now let’s talk about segment files a bit.\n\nMedia segments use container formats to store actual encoded video/audio data. Discussing container formats in detail would be out of the scope of this particular article. A good starting point to know more about it might be this.\n\nHLS initially only supported MPEG-2 TS containers. This decision was different from container formats used by other HTTP-based protocols. For example, DASH has always used fMP4 (fragmented MP4). Though finally, Apple announced support for fMP4 in HLS, and now it is officially supported by spec.\n\nThe media segment size is not defined. People usually started with ~10 seconds segment size, but another part of the spec caused this number to change. According to the spec, one must fetch and ready at least three segments to start playing. This would mean for 10 seconds segments, it would require 30 seconds of buffer to be loaded before the client could begin playing. One can change it as and when one wants, and this is a common tuning parameter one employs while reducing latency.\n• Ingest: Ingest (relatively less relevant) marks the point where our system takes input from the user, this can be done in various formats with different protocols, but platforms like Twitch and Youtube have popularized RTMP. However, this seems to be changing with the introduction of WebRTC in OBS, making it possible to use WHIP to ingest content much faster at lower latencies.\n• Processing: Media Processing includes transcoding, transmuxing, etc., depending on the ingest format used. Input here is converted into MPEG-TS or fMP4 segments indexed in the playlist and made ready to be consumed.\n• Delivery: Delivery is an essential aspect of ensuring HLS is scalable. A web server hosts the playlists and segments, which are then sent to the end user through a CDN. A CDN caches these GET requests and scales itself to return static data on its side, making protocol highly scalable when asked for it at much higher traffic times.\n• Playback: A smooth playback is vital for a pleasant user-end experience. ABR must be adequately implemented on the player side so that the user experiences minimal experience dips during the stream.\n• Distributes over common format HTTP making it easily reachable everywhere\n• Using HTTP, HLS can leverage advantages from underlying TCP implementation, making retransmissions handled out of the box.\n• Much of the internet relies on HTTP, making it automatically deliverable on all consumer devices.\n• This becomes challenging for UDP-based protocols like RTP, which sometimes need stuff like NAT traversals to work properly.\n• Easy CDN-level caching\n• Scaling HLS becomes super easy because caching HTTP is widely supported, and offloading it to good CDNs can reduce the load on the source server to a minimum while ensuring delivering the same audio + video data to all the consumers.\n• Adaptive Bitrate Streaming (ABR)\n• HLS supports ABR, so users with a poor internet connection can quickly shift to lower bitrates and continue enjoying the stream.\n• Inbuilt ad insertion support\n• HLS provides easy-to-use tags to insert ads during a stream dynamically.\n• Closed captions\n• It also supports embedded closed captions and even DRM content.\n• Using fMP4\n• Using fragmented MP4 helps reduce encoding and delivery costs and increases compatibility with MPEG-DASH.\n• HLS is widely known for its painstakingly higher latencies. One of the simple ways to make it slightly better is by fine-tuning segment size and then changing the ingest protocol to lower latency protocols like RTMP. But still, for crisp live experiences, vanilla HLS is not enough. The computing world has offered community LL-HLS and Apple LL-HLS as the solutions. Both have their way of approaching the problem and can promise some nice latencies.\n• HLS spec specifies that the client needs to at least load three segments before starting. So now, imagine you set the segment size as 10 seconds. You already see 30 seconds of latency in the stream from the start. This also makes tuning segment size very important.\n\nHaving open standards helps adoption and growth while making it much easier to interop between open and closed software/hardware stacks. HLS drives the same in live streaming space — it is widely adopted, simple, yet robust. It did not try to reinvent the wheel from scratch, instead embraced current widely accepted protocols and built on top of it.\n\nIf this article tickled the builder or the hacker in you, then check out Dyte’s live streaming SDK. We provide easy-to-use SDKs to deliver interactive live streams at scale with much lower latency than traditional HLS. Feel at home with the same friendly developer experience and extensibility.\n\nGet better insights on leveraging Dyte’s technology and discover how it can revolutionize your app’s communication capabilities with its SDKs."
    },
    {
        "link": "https://docs.aws.amazon.com/solutions/latest/live-streaming-on-aws/live-streaming-on-aws.pdf",
        "document": ""
    },
    {
        "link": "https://dacast.com/blog/encoder-settings-hls-live-streaming",
        "document": "Live streaming is more prominent and accessible than ever. With ever-increasing popularity, live-streaming video content can prove a lucrative strategic move for businesses. However, live-streaming video content requires the right set-up to do it properly so viewers tune in.\n\nFortunately, starting a successful live stream requires only a few important elements. For live video streams, you need a camera, an encoder, an internet connection and a high-quality video streaming solution. If you’re unfamiliar, an encoder is software or hardware that interfaces between your camera and a live streaming service provider. Furthermore, an HSL encoder is a tool that uses the HLS streaming protocol to ingest the video files and stream them to the live video streaming platform. This is a particularly technical component which requires sound knowledge and guidance.\n\nThis post will help enlighten this process. In this article, we cover everything you need to know about HLS encoding. We’ll discuss HLS encoders and RTMP encoders in the context of HLS streaming. Also, we’ll look at specific encoder settings for HLS streaming. We’ll then break down each HLS encoder setting so you can get a better understanding of all the inner workings. By the end, you’ll be well-equipped to begin turning mere video and audio data into winning live streams.\n\nLet’s dive in and learn more about the HLS encoder and for HTTP live streaming.\n• How to Connect Your Encoder to Your OVP\n\nIn the past, Flash was the de-facto standard for delivering internet video. However, due to security and power consumption issues, Flash has been phased out . The reason that Flash is now obsolete is that it was not compatible with the operating systems of many mobile devices.\n\nhave since replaced it. Access to an HTML5 video player via HLS streaming makes it possible to stream to iOS, Android, desktop browsers, and a variety of other internet-connected devices. HLS streaming is possible due to HLS encoders.\n\n(HLS) has played a major role in the move away from Flash. HLS was developed by Apple to deliver content to an . This protocol is an adaptive bitrate streaming protocol that makes it possible to transport videos over the internet. Adaptive bitrate means that it can deliver content at different quality levels, such as 480p, 720p, or 1080p, depending on the content quality of the original video and the end user’s internet connection.\n\nHTTP live streaming (HLS) is used for both on-demand and live streaming, despite the term “live” in the name. HLS takes large video files and breaks them into smaller downloadable HTTP files, which then allows them to be delivered using the HTTP protocol.\n\nDue to its ultra-compatibility, the dominates the market today. It is the safest bet to deliver your stream to any viewer on any device. That’s why Dacast and most other major deliver online streams in HLS format. HLS streaming is the standard method for sharing videos online.\n\nHTTP Live Streaming (HLS) is a widely used protocol for delivering video content over the internet, offering both on-demand and live streaming capabilities. The HLS streaming process involves several key steps:\n• Encoding: HLS uses H.264 or H.265 encoding. The video data is reformatted using one of these two encoding methods so that other devices can recognize and interpret the data.\n• Segmenting: The video is split into small segments. The average length of each segment is 6 seconds, although it can vary. This makes the content easier and quicker to transmit.\n• None Index File Creation: An index file, known as a playlist or M3U8 file, is generated. This file lists the sequence of video segments and provides the necessary information for playback.\n• Duplicate Segments: To accommodate varying network conditions and device capabilities, duplicate segments are created at different quality levels (e.g., 480p, 720p, 1080p). This allows the streaming client to switch between quality levels dynamically, ensuring an optimal viewing experience, especially if you are offering adaptive bitrate streaming in HLS.\n\nThe M3U8 playlist file plays a crucial role in HLS streaming. It organizes and sequences the multimedia files for playback, guiding the video player to locate and retrieve the appropriate video segments. Additionally, M3U8 files can incorporate security tokens, which are essential for enforcing access controls such as password protection and viewing restrictions. These tokens regularly communicate with the streaming server to verify permissions, thereby safeguarding your content and supporting monetization efforts.\n\nBy implementing HLS with M3U8 playlists and security tokens, you can deliver high-quality, adaptive streaming experiences while maintaining robust content protection measures.\n\nOne of the most significant advantages of working with HLS over other streaming protocols is adaptive bitrate streaming. With adaptive bitrate streaming, the video quality will adjust automatically, without the end-viewer doing anything, if the viewer’s network quality changes.\n\nFor example, if the user’s network slows down, the video player will detect this, and the adaptive bitrate streaming technology will lower the quality of the stream. The video will not stop playing or experience buffering; the video quality will drop to a lower tier to ensure the viewer can continue to watch your content.\n\nOr, if more bandwidth opens up on the user’s network, the video stream quality will increase. This also happens automatically, without the user having to do anything. With adaptive bitrate streaming, the quality of the video will automatically increase.\n\nAdaptive bitrate streaming works because HLS creates duplicate segments, as explained in step four agave, of the stream at different quality levels when it segments. This allows the video player to automatically switch between the different quality segments without the end-user having to do anything.\n\nThis allows you to provide the viewer with a better overall viewing experience.\n\nHLS streaming is used to deliver video content to an . HLS ingest, however, refers to ingesting content to the encoder from the camera or other media source.\n\nIf you are using , you must use an HLS encoder. An HLS encoder is a tool that is used for encoding with HLS ingest. HLS ingest and HLS streaming is two different functions and should not be confused.\n\nAt this point, HLS is not yet the standard protocol for ingesting. That is because HLS ingest has some latency issues. Since HLS is not the primary protocol for this role, HLS encoders are a bit difficult to come by.\n\npaired with HLS streaming is currently the most optimal for a few reasons. This duo gives you access to the compatibility and security of HLS and the low latency and accessibility of RTMP.\n\nThe Dacast , for example, uses the for ingesting live streams. From there, our platform converts that live video content into the\n\nFinally, streaming content reaches your viewers via top-tier CDNs such as Akamai and Limelight. Unlike RTMP, HLS is compatible with most browsers and devices with no need for the Flash plugin.\n\nSince RTMP is the standard, RTMP encoders are affordable and readily accessible.\n\nThe way you configure your settings will affect the outcome of your stream. That said, understanding HLS encoder configurations on a more technical level gives you better insight as a broadcaster.\n\nLet’s take a look at what each of these terms means and how they relate to streaming.\n\nCodec is the shorthand for “coder-decoder,” and it is the technology that makes encoding possible. In live streaming, you’ll use both audio and\n\nThe is currently the most efficient for HLS streaming. The X.264 codec is another implementation of the same protocol, so it is also a viable option. You can use either one. In some cases, X.264 may use less processing power, but the difference is rarely significant.\n\nThere is one additional detail to keep in mind. The is a family of standards, which are called “profiles.” There are a lot of these profiles, but you only need to worry about two.\n\nIf you’re streaming in 720p resolution or lower, with a video bitrate of 350-800 kbps, use the “Main” protocol. If you’re streaming in 1080p full HD, with a video bitrate of 800-4500 kbps, use the “High” protocol.\n\nAs for the best , you should select AAC or AAC-LC.\n\nFor more details on codecs, check out our\n\nis simply the size of your video, measured in pixels. You can choose from the ultra-low definition, low definition, standard definition, high definition, and full high definition.\n\nThe most common video frame sizes today are:\n\nHigh-resolution streaming requires a fast internet connection. When the conditions are right, higher resolution typically equates to higher quality.\n\nThat’s why, most of the time, you’ll want to broadcast in the highest available resolution. Since streaming in high resolution requires faster internet, most broadcasters choose to broadcast in multiple resolutions and multiple bitrates to accommodate viewers with all different internet speeds. That way, every viewer will have the best possible resolution for their situation.\n\nThat is where HLS bitrate settings and multi-bitrate streaming come into play. Check out on how to set up multi-bitrate streaming for more information.\n\nBitrate refers to the amount of data in your video/audio streams per unit of time. This is measured in Kilobits per second (kbps) or Megabits per second (Mbps). One Mbps is equal to 1000 kbps.\n\nHigher video resolution requires more data. To give you a rough sense of the numbers, a low-quality 240p live stream might require around 400 kbps. A full HD 1080p live stream usually requires 4-8 Mbps. Here are some recommended video bitrates at various resolutions:\n\nThe requirements for bitrate for 720p are less than they are for higher resolutions. The amount of bitrate required increases as the resolution of the video increases\n\nAudio bitrates are simpler. We recommend always using at least 128 kbps and an audio sample rate of 48 kHz (48,000 Hz).\n\nMulti-bitrate streaming allows viewers to be served as the best possible quality video for their situation. Check out our tutorial on how to set up multi-bitrate streaming for more information.\n\nIn general, we recommend that your be roughly double the total combined bandwidth of your video and audio. If you’re streaming in multiple bitrates, you should consider the total bandwidth of all combined streams. Multiple bitrate streaming requires a stronger internet connection on your end.\n\nAttempting to stream too much data on an internet connection that isn’t fast enough can cause your live stream to fail.\n\nTo select the correct bit rate, divide the sustained upload speed of your internet connection by two. This is the amount of bandwidth you have to play with. For example, a 10 Mbps upload speed would give you 5 Mbps of bandwidth.\n\nIn this case, we recommend sending out a multi-bitrate stream with the following settings:\n\nThis would ensure that a reliable stream will be available to both people with a fast internet connection and a slow one.\n\nCBR refers to “Constant Bitrate,” and VBR means “Variable Bitrate.” Encoder settings will often include a toggle for for both audio and video.\n\nConstant bitrate is simple. With this setting, streams use the same amount of data regardless of the contents of the stream at any given time.\n\nIn contrast, VBR takes into account the contents of your stream. If, for example, a segment of the video contains a lot of fast-moving action, the bitrate will temporarily increase. This increases perceived quality. However, in some situations, it can also cause excess load on your internet bandwidth.\n\nIn general, we recommend using VBR for the best results. However, CBR will suffice.\n\n“ ” is another aspect to consider as you configure your encoder settings. Simply put, most users should generally set at 30. For people in certain regions of the world, however, 25 frames per second are standard.\n\nHowever, 30 fps will work anywhere. If you are broadcasting sports or another fast-action video, 60 fps may be preferable. Be aware that it may take a higher bit rate to make these videos look high-quality. 1080p60 bitrate will be higher than needed for a 1080p stream.\n\nKeyframe interval, which is also referred to as “keyframe frequency” by some encoders, is the frequency that the full image on the screen changes.\n\nWhen broadcasting over the Dacast OVP, users should always set the keyframe interval to 2 seconds (or 2x frame rate).\n\nAt this time, RTMP encoders are still the most common type of encoder for HLS streaming due to their widespread compatibility with other components of the streaming process.\n\nRTMP encoders come in a wide variety of types. The simplest is software encoders, which users can install on a smartphone, tablet, or computer just like any other app. There are free available, as well as highly complex and expensive suites.\n\nAdditionally, there are hardware encoders for use with live streaming. These are well suited for mobile streaming, studio use, and professional-grade settings. On the other hand, hardware encoders do require more know-how than simple encoder settings for\n\nRegardless, RTMP encoders are all configured in the same basic manner. Whether you are using a hardware encoder or a software encoder, it is important to follow the required settings for your online video platform.\n\nDacast requires specific encoder setting configurations to ensure that the platform works properly and produces the highest quality of content.\n\nThe following settings are required for live streaming with Dacast, regardless of your selected resolution and bitrate:\n\nFor more information on Dacast’s preferred and required encoder settings, please check out our\n\nHow to Connect Your Encoder to Your OVP\n\nNow that you are familiar with the most common HLS encoding settings, it is time to connect your encoder to your streaming solution.\n\nThe exact encoder configuration process varies depending on which encoder you’ve chosen and the streaming solution you’re using. However, the process typically requires gathering a few credentials to connect the two tools.\n\nFor example, with Dacast, the required credentials include\n\nThis information will automatically be generated when you create a new channel within your Dacast account. To access this info, navigate to the “encoder settings” portion of a live channel, and select the requisite RTMP encoder from the list.\n\nFor a detailed walkthrough on how to set up your encoder with Dacast, please check out our\n\nProperly configuring HLS encoder settings can be incredibly daunting at first. The language can seem too technical and abstract. However, with clear instructions and expert guidance, as we’ve provided, you too can get up and running in no time.\n\nWe recommend conducting a live-streaming test to ensure your live-streaming tools work correctly. Your video codec, video quality, bitrate settings and everything else must be aligned and in-check. If you follow our technical and live-streaming guidance, you can quickly set up a successful livestream.\n\nFinally, you should know about Dacast. Dacast is a professional-grade live-streaming platform that will get you up and running, fast. Using Dacast you can set up the perfect live stream that your audience will appreciate. It’s flexible and works seamlessly with HLS, ensuring the highest quality live streaming.\n\nYou can try Dacast completely free with our 14-day free trial.\n\nAny questions? Let us know in the comment section below! We love to hear from our readers and will respond as soon as we can.\n\nFor regular tips and exclusive offers, you can join our"
    },
    {
        "link": "https://github.com/video-dev/hls.js/blob/master/docs/API.md",
        "document": "See API Reference for a complete list of interfaces available in the \"hls.js\" package.\n• Getting started\n• Second step: instantiate Hls object and bind it to element\n\nFirst include (or for unminified) in your web page.\n\nInvoke the following static method: to check whether your browser supports MediaSource Extensions with any baseline codecs.\n\nIf you want to test for MSE support without testing for baseline codecs, use :\n\nYou need to provide manifest URL as below:\n\nHTMLVideoElement control and events could be used seamlessly.\n\nAll errors are signalled through a unique single event.\n\nEach error is categorized by an error type, error details, and whether or not is is :\n• Error Types:\n• for all other errors\n• Error is :\n• if error is not fatal, HLS.js will try to recover.\n• if error is fatal, all attempts to recover have been performed. See LoadPolicies details on how to configure retries.\n\nFull details are described below\n\nSee sample code below to listen to errors:\n\nHLS.js provides several methods for attempting playback recover in the event of a decoding error in the HTMLMediaElement:\n\nResets the MediaSource and restarts streaming from the last known playhead position.\n\ncan be used in the place of when dealing with user agents that have issues handling HE-AAC and AAC audio (mp4a.40.5 and mp4a.40.2) codecs and media.\n\nThis should no longer be required and is not recommended. If you find a case where it is, please file a bug with steps to reproduce.\n\nshould be called to free used resources and destroy hls context.\n\nConfiguration parameters could be provided to HLS.js upon instantiation of object.\n\nThis getter/setter allows retrieval and override of the Hls default configuration. This configuration will be applied by default to all instances.\n• if set to true, the adaptive algorithm with limit levels usable in auto-quality by the HTML video element dimensions (width and height). If dimensions between multiple levels are equal, the cap is chosen as the level with the greatest bandwidth. In some devices, the video element dimensions will be multiplied by the device pixel ratio. Use for a strict level limitation based on the size of the video element.\n• if set to false, levels will not be limited. All available levels could be used in auto-quality mode taking only bandwidth into consideration.\n• when set to true, if the number of dropped frames over the period exceeds the ratio set by , then the quality level is dropped and capped at this lower level.\n• when set to false, levels will not be limited. All available levels could be used in auto-quality mode taking only bandwidth into consideration.\n• when set to true, calculations related to player size will ignore browser .\n• when set to false, calculations related to player size will respect browser .\n• when set, calculations related to player size will limit the browser's to this specified value.\n\nSetting will turn on debug logs on JS console.\n\nA logger object could also be provided for custom logging: .\n• if set to true, start level playlist and first fragments will be loaded automatically, after triggering of event\n• if set to false, an explicit API call ( ) will be needed to start quality level/fragment loading.\n• if set to -1, playback will start from initialTime=0 for VoD and according to config params for Live\n• Otherwise, playback will start from predefined value. (unless stated otherwise in mode : in that case startPosition can be overridden using ).\n\nUse this to override the multi-variant playlist audio codec, or provide one if loading only a media playlist.\n\nHLS.js parses track codecs from mp4 stsd or ADTS object type in the case of AAC in MPEG-TS. If there is an error using the value it finds in the segments, it will fallback to codec found in the multi-variant playlist or .\n\nThis should no longer be required and is not recommended. If you find a case where it is, please file a bug with steps to reproduce.\n\nnumber of segments needed to start a playback of Live stream. Buffering will begin after N chunks are available in the current playlist. If you want playback to begin chunks from the live edge at the beginning of a stream, set to or higher.\n\nMaximum buffer length in seconds. If buffer length is/become less than this value, a new fragment will be loaded. This is the guaranteed buffer length HLS.js will try to reach, regardless of maxBufferSize.\n\nThe maximum duration of buffered media to keep once it has been played, in seconds. Any video buffered past this duration will be evicted. means no restriction on back buffer length; keeps the minimum amount. The minimum amount is equal to the target duration of a segment to ensure that current playback is not interrupted. Keep in mind, the browser can and does evict media from the buffer on its own, so with the setting, HLS.js will let the browser do what it needs to do. (Ref: the MSE spec under coded frame eviction).\n\nThe maximum duration of buffered media, in seconds, from the play position to keep before evicting non-contiguous forward ranges. A value of means no active eviction will take place; This value will always be at least the .\n\n'Minimum' maximum buffer size in bytes. If buffer size upfront is bigger than this value, no fragment will be loaded.\n\n'Maximum' inter-fragment buffer hole tolerance that HLS.js can cope with when searching for the next fragment to load. When switching between quality level, fragments might not be perfectly aligned. This could result in small overlapping or hole in media buffer. This tolerance factor helps cope with this.\n\nABR algorithm will always try to choose a quality level that should avoid rebuffering. In case no quality level with this criteria can be found (lets say for example that buffer length is 1s, but fetching a fragment at lowest quality is predicted to take around 2s ... ie we can forecast around 1s of rebuffering ...) then ABR algorithm will try to find a level that should guarantee less than of buffering.\n\nmax video loading delay used in automatic start level selection : in that mode ABR controller will ensure that video loading time (ie the time to fetch the first fragment at lowest quality level + the time to fetch the fragment at the appropriate quality level is less than )\n\nhas been deprecated. Use instead.\n\nThe amount of time that playback can progress without advancing before HLS.js will report a stall. Note that stalls are detected immediately when the attached HTMLMediaElement dispatched the \"waiting\" event outside of startup and seeking. is used when \"waiting\" is not dispatched and fails to advance without a reasonable interval.\n\nif media element is expected to play and if currentTime has not moved for more than and if there are more than seconds buffered upfront, HLS.js will jump buffer gaps, or try to nudge playhead to recover playback\n\nIn case playback continues to stall after first playhead nudging, currentTime will be nudged evenmore following nudgeOffset to try to restore playback.\n\nWhether or not HLS.js should perform a seek nudge to flush the rendering pipeline upon traversing a gap or hole in video SourceBuffer buffered time ranges. This is only performed when audio is buffered at the point where the hole is detected. For more information see in gap-controller and issues https://issues.chromium.org/issues/40280613#comment10 and #5631.\n\nThis tolerance factor is used during fragment lookup. Instead of checking whether buffered.end is located within [start, end] range, frag lookup will be done by checking within [start-maxFragLookUpTolerance, end-maxFragLookUpTolerance] range.\n\nThis tolerance factor is used to cope with situations like:\n\nis within range, but as we are close to , should be choosen instead\n\nIf , this lookup will be adjusted to\n\nThis time, is within range, and will be the next fragment to be loaded, as expected.\n\nMaximum buffer length in seconds. HLS.js will never exceed this value, even if is not reached yet.\n\nHLS.js tries to buffer up to a maximum number of bytes (60 MB by default) rather than to buffer up to a maximum nb of seconds. this is to mimic the browser behaviour (the buffer eviction algorithm is starting after the browser detects that video buffer size reaches a limit in bytes)\n\nis the minimum guaranteed buffer length that HLS.js will try to achieve, even if that value exceeds the amount of bytes 60 MB of memory. acts as a capping value, as if bitrate is really low, you could need more than one hour of buffer to fill 60 MB.\n\nedge of live delay, expressed in multiple of . if set to 3, playback will start from fragment N-3, N being the last fragment of the live playlist. decreasing this value is likely to cause playback stalls.\n\nincrement to the calculated on each playback stall, expressed in seconds. When is specified in config, is calculated as plus multiplied by number of stalls. Otherwise is calculated as multiplied by plus multiplied by number of stalls. Decreasing this value will mean that each stall will have less affect on .\n\nmaximum delay allowed from edge of live, expressed in multiple of . if set to 10, the player will seek back to whenever the next fragment to be loaded is older than N-10, N being the last fragment of the live playlist. If set, this value must be stricly superior to a value too close from is likely to cause playback stalls.\n\nAlternative parameter to , expressed in seconds vs number of segments. If defined in the configuration object, will take precedence over the default . You can't define this parameter and either or in your configuration object at the same time. A value too low (inferior to ~3 segment durations) is likely to cause playback stalls.\n\nAlternative parameter to , expressed in seconds vs number of segments. If defined in the configuration object, will take precedence over the default . If set, this value must be stricly superior to which must be defined as well. You can't define this parameter and either or in your configuration object at the same time. A value too close from is likely to cause playback stalls.\n\nWhen set to a value greater than , the latency-controller will adjust up to to catch up to target latency in a live stream. is based on or manifest PART-|HOLD-BACK.\n\nThe default value is , which disables playback rate adjustment. Set to a value greater than to enable playback rate adjustment at the live edge.\n\nOverride current Media Source duration to for a live broadcast. Useful, if you are building a player which relies on native UI capabilities in modern browsers. If you want to have a native Live UI in environments like iOS Safari, Safari, Android Google Chrome, etc. set this value to .\n\nhas been deprecated. Use instead.\n\nHLS.js uses the Managed Media Source API ( global) instead of the global by default when present. Setting this to will only use when is undefined.\n\nEnable WebWorker (if available on browser) for TS demuxing/MP4 remuxing, to improve performance and avoid lag/frame drops.\n\nProvide a path to hls.worker.js as an alternative to injecting the worker based on the iife library wrapper function. When is defined as a string, the transmuxer interface will initialize a WebWorker using the resolved URL.\n\nWhen using the ESM version of the library (hls.mjs), this option is required in order for web workers to be used.\n\nEnable to use JavaScript version AES decryption for fallback of WebCrypto API.\n\nWhen set, use this level as the default . Keep in mind that the set with the API takes precedence over config.startLevel configuration parameter. should be set to value between 0 and the maximum index of .\n\nx-LoadingTimeOut settings have been deprecated. Use one of the LoadPolicy settings instead.\n\nx-LoadingMaxRetry settings have been deprecated. Use one of the LoadPolicy settings instead.\n\nx-LoadingMaxRetryTimeout settings have been deprecated. Use one of the LoadPolicy settings instead.\n\nMaximum frag/manifest/key retry timeout (in milliseconds). This value is used as capping value for exponential grow of , i.e. the retry delay can not be bigger than this value, but overall time will be based on the overall number of retries.\n\nx-LoadingRetryDelay settings have been deprecated. Use one of the LoadPolicy settings instead.\n\nInitial delay between error and first load retry (in ms). Any I/O error will trigger retries every 500ms,1s,2s,4s,8s, ... capped to / / value (exponential backoff).\n\nLoadPolicies specify the default settings for request timeouts and the timing and number of retries after a request error or timeout for a particular type of asset.\n• : The for Segment and Part* requests\n\n*Some timeout settings are adjusted for Low-Latency Part requests based on Part duration or target.\n\nEach contains a set of contexts. The property is the only context supported at this time. It contains the for that asset type. Future releases may include support for other policy contexts besides .\n\nHLS.js config defines the following default policies. Each can be overridden on player instantiation in the user configuration:\n\nEach has the following properties:\n\nMaximum time-to-first-byte in milliseconds. If no bytes or readyState change happens in this time, a network timeout error will be triggered for the asset.\n\nNon-finite values and 0 will be ignored, resulting in only a single timeout timer for the entire request.\n\nMaximum time to load the asset in milliseconds. If the request is not completed in time, a network timeout error will be triggered for the asset.\n\nRetry rules for timeout errors. Specifying null results in no retries after a timeout error for the asset type.\n\nRetry rules for network I/O errors. Specifying null results in no retries after a timeout error for the asset type.\n\nEach has the following properties:\n\nMaximum number of retries. After an error, the request will be retried this many times before other recovery measures are taken. For example, after having retried a segment or playlist request this number of times*, if it continues to error, the player will try switching to another level or fall back to another Pathway to recover playback.\n\nWhen no valid recovery options are available, the error will escalate to fatal, and the player will stop loading all media and asset types.\n\n*Requests resulting in a stall may trigger a level switch before all retries are performed.\n\nThe time to wait before performing a retry in milliseconds. Delays are added to prevent the player from overloading servers having trouble responding to requests.\n\nMaximum delay between retries in milliseconds. With each retry, the delay is increased up to .\n\nStart prefetching start fragment although media not attached yet.\n\nYou must also set for this to have any impact. Otherwise, HLS.js will load the first level in the manifest and start playback from there. If you do set , a fragment of the lowest level will be downloaded to establish a bandwidth estimate before selecting the first auto-level. Disable this test if you'd like to provide your own estimate or use the default .\n\nEnable Low-Latency HLS part playlist and segment loading, and start live streams at playlist PART-HOLD-BACK rather than HOLD-BACK.\n\nThe period used by the default to observe .\n\nThe ratio of frames dropped to frames elapsed within needed for the default to emit an event.\n\nMax number of retry upon error. Such error could happen in loop with UHD streams, when internal buffer is full. (Quota Exceeding Error will be triggered). In that case we need to wait for the browser to evict some data before being able to append buffer correctly.\n\nOverride standard URL loader by a custom one. Use composition and wrap internal implementation which could be exported by . Could be useful for P2P or stubbing (testing).\n\nUse this, if you want to overwrite both the fragment and the playlist loader.\n\nNote: If or are used, they overwrite !\n\nThis enables the manipulation of the fragment loader. Note: This will overwrite the default , as well as your own loader function (see above).\n\nThis enables the manipulation of the playlist loader. Note: This will overwrite the default , as well as your own loader function (see above).\n\nif you want to just make slight adjustments to existing loader implementation, you can also eventually override it, see an example below :\n\nshould be a function with two arguments . If is specified, the default loader will invoke it before calling . This allows users to easily modify the instance before sending a request. Optionally, a Promise can be returned to wait before the request is sent.\n\nNote that should be called in if the callback modifies the instance in ways that require it to be opened first. If throws, the error will be caught, and will be called a second time after opening a GET request.\n\nParameter should be a function with two arguments ( and ). If is specified and Fetch loader is used, will be triggered to instantiate Request Object. This allows user to easily tweak Fetch loader. See example below.\n\nThese settings determine whether HDR video should be selected before SDR video. Which VIDEO-RANGE values are allowed, and in what order of priority can also be specified.\n• Allow all video ranges if is unspecified.\n• If is defined, use the value to filter .\n• Else check window for HDR support and set to the result.\n\nWhen is set, skip checking if the window supports HDR and instead use the value provided to determine level selection preference via dynamic range. A value of will attempt to use HDR levels before selecting from SDR levels.\n\ncan restrict playback to a limited set of VIDEO-RANGE transfer functions and set their priority for selection. For example, to ignore all HDR variants, set to . Or, to ignore all HLG variants, set to . To prioritize PQ variants over HLG, set to .\n\nlimits initial selection to a particular code provided a baseline vairant (1080p 30fps or lower in ) is found.\n\nSet a preference used to find and select the best matching audio track on start. The selection can influence starting level selection based on the audio group(s) available to match the preference. accepts a value of an audio track object (MediaPlaylist), AudioSelectionOption (track fields to match), or undefined. If not set or set to a value of , HLS.js will auto select a default track on start.\n\nSet a preference used to find and select the best matching subtitle track on start. accepts a value of a subtitle track object (MediaPlaylist), SubtitleSelectionOption (track fields to match), or undefined. If not set or set to a value of , HLS.js will not enable subtitles unless there is a default or forced option.\n\nParameter should be a class providing a getter/setter and a method:\n• get/set : return next auto-quality level/force next auto-quality level that should be returned (currently used for emergency switch down)\n• : should clean-up all used resources\n\nFor to return an estimate from your custom controller, it will also need to satisfy .\n\nA class in charge of setting to limit ABR level selection based on player size. Enable the default cap level controller by setting to .\n\nA class in charge of monitoring frame rate, that emits events when frames dropped exceeds configured threshold. Enable the default fps controller by setting to .\n\nParameter should be a class with a method:\n• : should clean-up all used resources\n\nwhether or not to add, update, and remove cues from the metadata TextTrack for EXT-X-DATERANGE playlist tags\n\nwhether or not to add, update, and remove cues from the metadata TextTrack for ID3 Timed Metadata found in CMAF Event Message (emsg) boxes\n\nwhether or not to extract KLV Timed Metadata found in CMAF Event Message (emsg) boxes and deliver via\n\nwhether or not to add, update, and remove cues from the metadata TextTrack for ID3 Timed Metadata found in audio and MPEG-TS containers\n\nwhether or not to enable WebVTT captions on HLS\n\nwhether or not to enable IMSC1 captions on HLS\n\nwhether or not to enable CEA-708 captions\n\nLabel for the text track generated for CEA-708 captions track 1. This is how it will appear in the browser's native menu for subtitles and captions.\n\nRFC 3066 language code for the text track generated for CEA-708 captions track 1.\n\nLabel for the text track generated for CEA-708 captions track 2. This is how it will appear in the browser's native menu for subtitles and captions.\n\nRFC 3066 language code for the text track generated for CEA-708 captions track 2.\n\nLabel for the text track generated for CEA-708 captions track 3. This is how it will appear in the browser's native menu for subtitles and captions.\n\nRFC 3066 language code for the text track generated for CEA-708 captions track 3.\n\nLabel for the text track generated for CEA-708 captions track 4. This is how it will appear in the browser's native menu for subtitles and captions.\n\nRFC 3066 language code for the text track generated for CEA-708 captions track 4.\n\nWhether or not render captions natively using the HTMLMediaElement's TextTracks. Disable native captions rendering when you want to handle rending of track and track cues using and events.\n\nIf a segment's video track is shorter than its audio track by > , extend the final video frame's duration to match the audio track's duration. This helps playback continue in certain cases that might otherwise get stuck.\n\nBrowsers are really strict about audio frames timings. They usually play audio frames one after the other, regardless of the timestamps advertised in the fmp4. If audio timestamps are not consistent (consecutive audio frames too close or too far from each other), audio will easily drift. HLS.js is restamping audio frames so that the distance between consecutive audio frame remains constant. if the distance is larger than the max allowed drift, HLS.js will either:\n• drop the next audio frame if distance is too small (if next audio frame timestamp is smaller than expected time stamp - max allowed drift)\n\nparameter should be an integer representing the max number of audio frames allowed to drift. keep in mind that one audio frame is 1024 audio samples (if using AAC), at 44.1 kHz, it gives 1024/44100 = 23ms\n\nWhether or not to force having a key frame in the first AVC sample after a discontinuity. If set to true, after a discontinuity, the AVC samples without any key frame will be dropped until finding one that contains a key frame. If set to false, all AVC samples will be kept, which can help avoid holes in the stream. Setting this parameter to false can also generate decoding weirdness when switching level or seeking.\n\nFast bitrate Exponential moving average half-life, used to compute average bitrate for Live streams. Half of the estimate is based on the last abrEwmaFastLive seconds of sample history. Each of the sample is weighted by the fragment loading duration.\n\nparameter should be a float greater than 0\n\nSlow bitrate Exponential moving average half-life, used to compute average bitrate for Live streams. Half of the estimate is based on the last abrEwmaSlowLive seconds of sample history. Each of the sample is weighted by the fragment loading duration.\n\nparameter should be a float greater than abrEwmaFastLive\n\nFast bitrate Exponential moving average half-life, used to compute average bitrate for VoD streams. Half of the estimate is based on the last abrEwmaFastVoD seconds of sample history. Each of the sample is weighted by the fragment loading duration.\n\nparameter should be a float greater than 0\n\nSlow bitrate Exponential moving average half-life, used to compute average bitrate for VoD streams. Half of the estimate is based on the last abrEwmaSlowVoD seconds of sample history. Each of the sample is weighted by the fragment loading duration.\n\nparameter should be a float greater than abrEwmaFastVoD\n\nLimits value of updated bandwidth estimate taken from first variant found in multivariant playlist on start.\n\nScale factor to be applied against measured bandwidth average, to determine whether we can stay on current or lower quality level. If then ABR can switch to that level providing that it is equal or less than current level.\n\nScale factor to be applied against measured bandwidth average, to determine whether we can switch up to a higher quality level. If then ABR can switch up to that quality level.\n\nmax bitrate used in ABR by avg measured bitrate i.e. if bitrate signaled in variant manifest for a given level is 2Mb/s but average bitrate measured on this level is 2.5Mb/s, then if config value is set to , ABR will use 2.5 Mb/s for this quality level.\n\nReturn the capping/min bandwidth value that could be used by automatic level selection algorithm. Useful when browser or tab of the browser is not in the focus and bandwidth drops\n\nSet to to enable DRM key system access and license retrieval.\n\nhas been deprecated. Use instead.\n\nA pre-processor function for modifying license requests. The license request URL, request headers, and payload can all be modified prior to sending the license request, based on operating conditions, the current key-session, and key-system.\n\nA post-processor function for modifying the license response before passing it to the key-session ( ).\n\nDefine license settings for given key-systems according to your own DRM provider. Ex:\n\nSupported key-systems include 'com.apple.fps', 'com.microsoft.playready', 'com.widevine.alpha', and 'org.w3.clearkey'. Mapping to other values in key-system access requests can be done by customizing .\n\nWhen loading content with DRM Keys, the player will only request access to key-systems for the Session Keys or Playlist Keys for which there are also key-systems defined in .\n\nUsed to map initData or generate initData for playlist keys before MediaKeySession is called.\n\nDefine optional arguments to be passed to . Ex:\n\nWith the default argument, will be specified for each option (i.e. no specific robustness required).\n\nAllows for the customization of . This can be used to map key-system access request to from a supported value to a custom one:\n\nWhen the object is defined, Common Media Client Data (CMCD) data will be passed on all media requests (manifests, playlists, a/v segments, timed text). It's configuration values are:\n• : The CMCD session id. One will be automatically generated if none is provided.\n• : Send CMCD data in request headers instead of as query args. Defaults to .\n• : An optional array of CMCD keys. When present, only these CMCD fields will be included with each each request.\n\nInterstitial playback can be disabled without disabling parsing or schedule update and buffered-to events by setting this to allowing for custom playout and ad managers to use Interstitials data.\n\nUse this option to turn off the appending of interstitials \"in place\" by setting it to .\n\n\"In place\" appending is performed on a single timeline, with the same SourceBuffers and MediaSource as the primary media. The default value is , allowing HLS.js to decide which mode is used based on each interstitial event's scheduled start and resumption and how it aligns with primary playlist media.\n\nEven when , HLS.js may reset the MediaSource and timeline for interstitial playback as necessary. The instance's property indicates the mode used to append assets of the interstitial. Once the first event has triggered for the interstitial, the value of will remain fixed.\n\nThe time (in seconds) ahead of the end of a live playlist to request scheduled Interstitials when playing at the live edge.\n\nThe default value is , meaning that HLS.js will begin requesting interstitial ASSET-LIST and ASSET-URIs whose START-DATE is within 10 seconds of the program-date-time at the end of the primary variant playlist while the forward buffer is within a target duration of the same range.\n• create MediaSource and set it as video source\n• once MediaSource object is successfully created, MEDIA_ATTACHED event will be fired.\n• signal the end of the stream on MediaSource\n\nDetaches and returns MediaSource and SourceBuffers non-destructively in a format that can be passed to . This is used by Interstitial asset players that append the same SourceBuffer as the primary player.\n• get: Return the bound videoElement from the hls instance\n\nBy default, hls.js handles quality switch automatically, using heuristics based on fragment loading bitrate and quality level bandwidth exposed in the variant manifest. It is also possible to manually control quality switch using below API.\n• get: Return array of available quality levels.\n• set: Trigger an immediate quality level switch to new quality level. This will abort the current fragment request if any, flush the whole buffer, and fetch fragment matching with current position and requested quality level.\n• get: Return next playback quality level (playback quality level for next buffered fragment). Return if next fragment not buffered yet.\n• set: Trigger a quality level switch for next fragment. This could eventually flush already buffered next fragment.\n• get: Return quality level that will be used to load next fragment.\n• set: Force quality level for next loaded fragment. Quality level will be forced only for that fragment. After a fragment at this quality level has been loaded, will prevail.\n• get: First level index (index of the first Variant appearing in the Multivariant Playlist).\n• get: Return quality level that will be used to load the first fragment when not overridden by .\n• get/set: Start level index (level of first fragment that will be played back).\n• if not overridden by user: first level appearing in manifest will be used as start level.\n• if -1: automatic start level selection, playback will start from level matching download bandwidth (determined from download of first segment).\n• get: Tell whether auto level selection is enabled or not.\n• get/set: Capping/max level value that could be used by ABR Controller.\n\nDefault value is (no level capping).\n• get/set: The maximum HDCP-LEVEL allowed to be selected by auto level selection. Must be a valid HDCP-LEVEL value ('NONE', 'TYPE-0', 'TYPE-1', 'TYPE-2'), or null (default). is automatically set to the next lowest value when a error occurs. To prevent manual selection of levels with specific HDCP-LEVEL attribute values, use on or on error.\n\nDefault value is null (no level capping based on HDCP-LEVEL)\n• get: Enables or disables level capping. If disabled after previously enabled, will be immediately called.\n\nDefault value is set via in config.\n\nget: Returns the current bandwidth estimate in bits/s, if available. Otherwise, is returned.\n\nset: Reset using the value set as the new default estimate. This will update the value of .\n\nRemove a level from the list of loaded levels. This can be used to remove a rendition or playlist url that errors frequently from the list of levels that a user or HLS.js can choose from.\n\nModifying the levels this way will result in a event being triggered.\n\nBy default, HLS.js will automatically start loading quality level playlists, and fragments after event has been triggered.\n\nHowever, if is set to , then needs to be called to manually start playlist and fragments loading.\n\nStart/restart playlist/fragment loading. This is only effective if MANIFEST_PARSED event has been triggered.\n\nstartPosition is the initial position in the playlist. If startPosition is not set to -1, it allows to override default startPosition to the one you want (it will bypass hls.config.liveSync* config params for Live for example, so that user can start playback from whatever position).\n\nOnce media is appended HLS.js will seek to the start position. Passing in a of allows loading to begin at the start position without seeking on append. This is used when multiple players contribute to buffering media to the same source for Interstitials that overlap primary content.\n\nstop playlist/fragment loading. could be resumed later on by calling\n\nget : Returns the resolved target (number) used for loading before media is buffered, and where playback will begin once media is buffered.\n\nget : Returns a boolean indicating whether fragment loading has been toggled with and .\n\nget : Returns a boolean indicating if EOS has been appended (media is buffered from currentTime to end of stream).\n\nget: Returns an object with each streaming controller's state and in-flight fragment (or null).\n\nget : string of current HLS asset passed to , otherwise null\n\nFind and select the best matching audio track, making a level switch when a Group change is necessary. Updates . Returns the selected track or null when no matching track is found.\n\nget : array of all supported audio tracks found in the Multivariant Playlist\n\nget : array of supported audio tracks in the active audio group ID\n\nFind and select the best matching subtitle track, making a level switch when a Group change is necessary. Updates . Returns the selected track or null when no matching track is found.\n\nget : array of all subtitle tracks found in the Multivariant Playlist\n\nget : array of subtitle tracks in the active subtitle group ID\n\nget/set : index of selected subtitle track in . Returns -1 if no track is visible. Set to -1 to disable all subtitle tracks.\n\nget/set : if set to true the active subtitle track mode will be set to and the browser will display the active subtitles. If set to false, the mode will be set to .\n\nget : position of live sync point (ie edge of live position minus safety delay defined by ). If playback stalls outside the sliding window, or latency exceeds , HLS.js will seek ahead to to get back in sync with the stream stream.\n\nget : estimated position (in seconds) of live edge (ie edge of live playlist plus time sync playlist advanced) returns 0 before first playlist is loaded\n\nget : maximum distance from the edge before the player seeks forward to configured using (multiple of target duration) or returns 0 before first playlist is loaded\n\nget/set : target distance from the edge as calculated by the latency controller\n\nWhen is specified in config, is calculated as plus multiplied by number of stalls. Otherwise is calculated as multiplied by plus multiplied by number of stalls.\n\nSetting resets number of stalls to and sets to the new value. Note: if the initial config specified rather than , setting will assign a new value to . This value will be used to calculate from now on and will be ignored.\n\nget : the rate at which the edge of the current live playlist is advancing or 1 if there is none\n\nget: the datetime value relative to media.currentTime for the active level Program Date Time if present\n\nHLS.js supports playback of X-ASSET-URI and X-ASSET-LIST m3u8 playlists scheduled with Interstitial EXT-X-DATERANGE tags. The provides playback state with seek and skip control. There are a variety of events to notify applications of Interstitials schedule changes and playback state. Here is an overview of how they work.\n• Set to to disable interstitial parsing, events, and playback.\n• Set to to disable interstitial playback, without disabling parsing and events.\n• Set to to disable appending \"in place\".\n• Adjust how far in advance to load interstitials during live playback.\n• get: Returns the (or ) with information about the current program.\n\nThe data includes the list of Interstitial events with their asset lists, the schedule of event and primary segment items, information about which items and assets are buffering and playing, the player instance currently buffering media, and the queue of players responsible for the streaming of assets.\n\nUse to skip the current interstitial. Use and playhead objects to get , and to seek along the respective timeline. Use to get active intersititial info (playing or upcoming buffering break) like , , and within an Interstital break.\n\nThe interstials manager can be used to get various apects of interstitial playback.\n\nThe last watched position of primary content:\n\nis fired following playlist parsing with Interstitial EXT-X-DATERANGE tags and any changes to interstitial asset duration or scheduling. It includes the list of interstitial events, the scheduled playback segments, the durations for the schedule for any chosen timeline, and any removed Interstitial EXT-X-DATERANGE since the last update (Live only).\n\nInterstitials are loaded when the buffer reaches the scheduled date of an event. This will be signalled by .\n\nIf the Interstitial EXT-X-DATERANGE has an X-ASSET-LIST, and will fire (or non-fatal with ).\n\nOnce the asset list/uri are known, player instances will be created to preload the assets signalled by . At this point the asset player is configured and requesting the HLS playlists. HLS.js will transfer the media element to this player when it is its turn to buffer or play media unless another one is attached at this time.\n\nThe property indicates the mode used to append assets of the interstitial.\n\nHLS.js determines if an interstitial will be appended \"in place\" on a single timeline, with the same SourceBuffers and MediaSource as the primary player, or if it will reset the MediaSource and duration for each asset. Attaching additional media elements to asset players results in their reset ahead of playback. When the media element is shared (by default), the mode is determined based on each interstitial event's scheduled start and resumption and how it aligns with primary playlist media.\n\nand mark entering and exiting of a scheduled interstitial event item. These events fire whenever playing or seeking into or out-of an Interstitial DATERANGE.\n\nand mark the entrance and exit of an asset in an interstitial.\n\nAdaptaion control and streaming status should be performed on asset players while assets are active. Use for integrated playback status, seeking, and skipping interstitials.\n\nis fired when playback enters a primary schedule item from an interstitial or the start of playback.\n\nis fired when an error results in an asset not playing or finishing early. Playback is expected to fallback to primary. This should be accompanied by a schedule update an an property present on the and the when all its assets failed.\n• An item or segment of the program schedule. This can be an or an .\n• A parsed and scheduled asset in an 's .\n• A class for wrapping an instance of used to stream Interstitial assets.\n• get: Returns the of last loaded level (variant) or prior to loading a media playlist.\n• get: Returns the object of the selected level (variant) or prior to selecting a level or once the level is removed.\n\nget: Returns the session UUID assigned to the Hls instance. Used as the default CMCD session ID.\n\nhls.js fires a bunch of events, that could be registered and unregistered as below:\n\nFull list of Events is available below:\n• - fired before MediaSource is attaching to media element\n• - fired when MediaSource has been successfully attached to media element\n• - fired when MediaSource has been detached from media element\n• - fired when we buffer is going to be reset\n• - fired when we know about the codecs that we need buffers for to push into\n• - fired when sourcebuffers have been created\n• - fired when we append a segment to the buffer\n• - fired when we are done with appending a media segment to the buffer\n• - fired when the stream is finished and we want to notify the media buffer that there will be no more data\n• - fired when the media buffer should be flushed\n• - fired when the media buffer has been flushed\n• - fired when the back buffer is reached as defined by the backBufferLength config option\n• - fired after manifest has been loaded\n• data: { levels : [available quality levels], audioTracks : [available audio tracks], captions? [available closed-captions media], subtitles?: [available subtitle tracks], url : manifestURL, stats : [LoaderStats], sessionData: [parsed #EXT-X-SESSION-DATA], networkDetails: [Loader specific object for debugging (XMLHttpRequest or fetch Response)]}\n• - fired after manifest has been parsed\n• data: { levels : [ available quality levels ], firstLevel : index of first quality level appearing in Manifest, audioTracks, subtitleTracks, stats, audio: boolean, video: boolean, altAudio: boolean }\n• - fired when the Content Steering Manifest is loaded\n• - fired when a level switch is requested\n• data: { and Level object properties (please see below for more information) }\n• - fired when a level playlist is requested (unless it is the only media playlist loaded via )\n• data: { url : level URL, level : id of level being loaded, deliveryDirectives: LL-HLS delivery directives or when blocking reload is not supported }\n• - fired when a level's details have been updated based on previous details, after it has been loaded\n• - fired when a level's PTS information has been updated after parsing a fragment\n• data: { details : LevelDetails, level : id of updated level, drift: PTS drift observed when parsing last fragment, type, start, end }\n• - fired when a level is removed after calling\n• - fired to notify that audio track lists has been updated\n• - fired when an audio track switching is requested\n• - fired when an audio track switch actually occurs\n• - fired to notify that subtitle track lists has been updated\n• - fired when a subtitle fragment has been processed\n• data: { success : boolean, frag : [the processed fragment object], error?: [error parsing subtitles if any] }\n• - fired when the first timestamp is found\n• - fired when a fragment loading starts\n• data: { frag : fragment object, targetBufferTime: number | null [The unbuffered time that we expect to buffer with this fragment] }\n• - Identifier for fragment load aborting for emergency switch down\n• - fired when Init Segment has been extracted from fragment\n• - fired when parsing metadata is completed (ID3 / CMAF KLV)\n• data: { id: demuxer id, frag : fragment object, samples : [ type field aligns with values from enum. pes - pts and dts timestamp are relative, values are in seconds], details: LevelDetails }\n• - fired when fragment remuxed MP4 boxes have all been appended into SourceBuffer\n• - fired when fragment matching with current video position is changing\n• - triggered when FPS drop in last monitoring period is higher than given threshold\n• data: { curentDropped : nb of dropped frames in last monitoring period, currentDecoded : nb of decoded frames in last monitoring period, totalDroppedFrames : total dropped frames on this video element }\n• - triggered when FPS drop triggers auto level capping\n• data: { level: suggested new auto level capping by fps controller, droppedLevel : level has too many dropped frames and will be restricted }\n• - Identifier for an error event\n• data: { type : error type, details : error details, fatal : is error fatal or not, other error specific data }\n• - fired when hls.js instance starts destroying. Different from as one could want to detach and reattach a video to the instance of hls.js to handle mid-rolls for example\n• - When is , this event will fire when a new captions or subtitle track is found, in the place of adding a TextTrack to the video element.\n• - When is , this event will fire when new captions or subtitle cues are parsed.\n\nYou can use the internal loader definition for your own implementation via the static getter .\n\nAlternatively, environments that support ES6 classes can extends the loader directly:\n\nFull list of errors is described below:\n• - raised when manifest loading fails because of a network error\n• - raised when manifest loading fails because of a timeout\n• - raised when loaded level contains no fragments (applies to levels and audio and subtitle tracks)\n• data: { type : , details : , url: playlist URL, reason: error reason, level: index of the bad level or undefined, parent: PlaylistLevelType }\n• - raised when level loading fails because of a network error\n• - raised when level loading fails because of a timeout\n• - raised when playlist parsing failed or found invalid content (applies to levels and audio and subtitle tracks)\n• - raised when audio playlist loading fails because of a network error\n• - raised when audio playlist loading fails because of a timeout\n• - raised when subtitle playlist loading fails because of a network error\n• - raised when subtitle playlist loading fails because of a timeout\n• - raised when fragment loading fails because of a network error\n• - raised when fragment loading fails because of a timeout\n• - raised when decrypt key loading fails because of a network error\n• - raised when decrypt key loading fails because of a timeout\n• - raised when manifest only contains quality level with codecs incompatible with MediaSource Engine.\n• - raised when segment loading is skipped because a fragment with a GAP tag or part with GAP=YES attribute was encountered\n• - raised when MediaSource fails to add new sourceBuffer\n• data: { type : , details : , fatal : , error : error raised by MediaSource, mimeType: mimeType on which the failure happened }\n• - raised when no MediaSource(s) could be created based on track codec(s)\n• - raised when exception is raised while calling buffer append\n• - raised when exception is raised during buffer appending\n• - raised when playback is stuck because buffer is running out of data\n• - raised when no data can be appended anymore in media buffer because it is full. this error is recovered by reducing the max buffer length.\n• - raised after hls.js seeks over a buffer hole to unstuck the playback,\n• - raised when playback is stuck although currentTime is in a buffered area\n• Not fatal for the first few nudges, but if we reach attempts and the player is still stalled, then is fatal\n• - Key-system license request failed (fails on first status 4xx, or after 3 tries (EMEController MAX_LICENSE_REQUEST_FAILURES))\n• - raised when an exception occurs in an internal hls.js event handler\n\nA object represents a given quality level. It contains quality level related info, retrieved from manifest, such as:\n• is an array that might contains several items if failover/redundant streams are found in the manifest.\n\nA object contains level details retrieved after level playlist parsing, they are specified below:\n• is this level a live playlist or not?\n\nSee sample object below, available after corresponding event has been fired:\n\nThe object contains fragment related info, such as:"
    },
    {
        "link": "https://github.com/video-dev/hls.js",
        "document": "HLS.js is a JavaScript library that implements an HTTP Live Streaming client. It relies on HTML5 video and MediaSource Extensions for playback.\n\nIt works by transmuxing MPEG-2 Transport Stream and AAC/MP3 streams into ISO BMFF (MP4) fragments. Transmuxing is performed asynchronously using a Web Worker when available in the browser. HLS.js also supports HLS + fmp4, as announced during WWDC2016.\n\nHLS.js is written in ECMAScript6 ( ) and TypeScript ( ) (strongly typed superset of ES6), and transpiled in ECMAScript5 using Babel and the TypeScript compiler.\n\nRollup is used to build the distro bundle and serve the local development environment.\n• Timed Metadata for HTTP Live Streaming (ID3 format carried in MPEG-2 TS, Emsg in CMAF/Fragmented MP4, and DATERANGE playlist tags)\n• Alternate Audio Track Rendition (Master Playlist with Alternative Audio) for VoD and Live playlists\n• Adaptive streaming\n• Manual & Auto Quality Switching\n• 3 Quality Switching modes are available (controllable through API means)\n• Bandwidth conservative switching (quality switch change for next loaded fragment, without flushing the buffer)\n• In Auto-Quality mode, emergency switch down in case bandwidth is suddenly dropping to minimize buffering.\n• Accurate Seeking on VoD & Live (not limited to fragment or keyframe boundary)\n• Ability to seek in buffer and back buffer without redownloading segments\n• Built-in Analytics\n• All internal events can be monitored (Network Events, Video Events)\n• Resilience to errors\n• Recovery actions can be triggered fix fatal media or network errors\n\nFor details on the HLS format and these tags' meanings, see https://datatracker.ietf.org/doc/html/draft-pantos-hls-rfc8216bis\n\nFor a complete list of issues, see \"Top priorities\" in the Release Planning and Backlog project tab. Codec support is dependent on the runtime environment (for example, not all browsers on the same OS support HEVC).\n• is not used in variant filtering or selection\n• MP3 elementary stream audio in IE and Edge (<=18) on Windows 10 (See #1641 and Microsoft answers forum)\n\nYou can safely require this library in Node and absolutely nothing will happen. A dummy object is exported so that requiring the library does not throw an error. HLS.js is not instantiable in Node.js. See #1841 for more details.\n\nFirst, checkout the repository and install the required dependencies\n\ngit clone https://github.com/video-dev/hls.js.git hls.js After cloning or pulling from the repository, make sure all dependencies are up-to-date npm install ci Run dev-server for demo page (recompiles on file-watch, but doesn't write to actual dist fs artifacts) npm run dev After making changes run the sanity-check task to verify all checks before committing changes npm run sanity-check\n\nThe dev server will host files on port 8000. Once started, the demo can be found running at http://localhost:8000/demo/.\n\nBefore submitting a PR, please see our contribution guidelines. Join the discussion on Slack via video-dev.org in #hlsjs for updates and questions about development.\n\nBuild and watch (customized dev setups where you'll want to host through another server - for example in a sub-module/project)\n\nOnly specific flavor (known configs are: debug, dist, light, light-dist, demo):\n\nNote: The \"demo\" config is always built.\n\nNOTE: dist files do not include alternate-audio, subtitles, CMCD, EME (DRM), or Variable Substitution support. In addition, the following types are not available in the light build:\n\nRun linter with errors only (no warnings)\n\nRun all tests at once:\n\nAn overview of this project's design, it's modules, events, and error handling can be found here.\n\nNote you can access the docs for a particular version using \"https://github.com/video-dev/hls.js/tree/deployments\"\n\nHLS.js is only compatible with browsers supporting MediaSource extensions (MSE) API with 'video/MP4' mime-type inputs.\n• Safari for iOS 17.1+ since HLS version 1.5.0 using Managed Media Source (MMS) WebKit blog\n\nSafari browsers (iOS, iPadOS, and macOS) have built-in HLS support through the plain video \"tag\" source URL. See the example below (Using HLS.js) to run appropriate feature detection and choose between using HLS.js or natively built-in HLS support.\n\nWhen a platform has neither MediaSource nor native HLS support, the browser cannot play HLS.\n\nKeep in mind that if the intention is to support HLS on multiple platforms, beyond those compatible with HLS.js, the HLS streams need to strictly follow the specifications of RFC8216, especially if apps, smart TVs, and set-top boxes are to be supported.\n\nFind a support matrix of the MediaSource API here: https://developer.mozilla.org/en-US/docs/Web/API/MediaSource\n\nPrepackaged builds are included with each release. Or install the hls.js as a dependency of your project:\n\nA canary channel is also available if you prefer to work off the development branch (master):\n\nDirectly include dist/hls.js or dist/hls.min.js in a script tag on the page. This setup prioritizes HLS.js MSE playback over native browser support for HLS playback in HTMLMediaElements:\n\n=\" \" <!-- Or if you want the latest version from the main branch --> =\" \" // HLS.js is not supported on platforms that do not have Media Source // When the browser has built-in HLS support (check using `canPlayType`), // we can provide an HLS manifest (i.e. .m3u8 URL) directly to the video // element through the `src` property. This is using the built-in support // of the plain video element, without using HLS.js.\n\nTo check for native browser support first and then fallback to HLS.js, swap these conditionals. See this comment to understand some of the tradeoffs.\n\nHLS transcoding of an original video file often pushes the time of the first frame a bit. If you depend on having an exact match of frame times between original video and HLS stream, you need to account for this:\n\nFor more embed and API examples see docs/API.md.\n\nAll HLS resources must be delivered with CORS headers permitting requests.\n\nVideo is controlled through HTML element methods, events and optional UI controls ( ).\n\nThe following players integrate HLS.js for HLS playback:\n• Videojs through videojs-hls.js. hls.js is integrated as a SourceHandler -- new feature in Video.js 5.\n• OpenPlayerJS, as part of the OpenPlayer project\n\nmade by gramk, plays hls from address bar and m3u8 links"
    },
    {
        "link": "https://developer.apple.com/documentation/http-live-streaming/hls-authoring-specification-for-apple-devices",
        "document": "Please turn on JavaScript in your browser and refresh the page to view its content."
    },
    {
        "link": "https://nochev.github.io/hls.js/docs/html",
        "document": "hls.js is a JavaScript library which implements an HTTP Live Streaming client. It relies on HTML5 video and MediaSource Extensions for playback.\n\nIt works by transmuxing MPEG-2 Transport Stream and AAC/MP3 streams into ISO BMFF (MP4) fragments. This transmuxing could be performed asynchronously using Web Worker if available in the browser. hls.js also supports HLS + fmp4, as announced during WWDC2016\n\nhls.js does not need any player, it works directly on top of a standard HTML element.\n\nhls.js is written in ECMAScript6, and transpiled in ECMAScript5 using Babel.\n• None Find a more detailed library usage guide, use-cases and example here\n\nHTMLVideoElement control and events could be used seamlessly.\n\nthey use hls.js in production !\n\nhls.js is (being) integrated in the following players:\n• Videojs through videojs-hls.js. hls.js is integrated as a SourceHandler -- new feature in Video.js 5.\n\nmade by gramk, plays hls from address bar and m3u8 links\n\nNo external JS libs are needed. Prepackaged build is included in the dist folder:\n\nIf you want to bundle the application yourself, use node\n\nNOTE: dist files do not include subtitling and alternate-audio features.\n\nOptionally there is a declaration file available to help with code completion and hinting within your IDE for the hls.js api\n\nhls.js is compatible with browsers supporting MediaSource extensions (MSE) API with 'video/MP4' mimetypes inputs.\n\nFind a support matrix of the MediaSource API here: https://developer.mozilla.org/en-US/docs/Web/API/MediaSource\n\nAs of today, it is supported on:\n\nPlease note: iOS Safari \"Mobile\" does not support the MediaSource API. Safari browsers have however built-in HLS support through the plain video \"tag\" source URL. See the example above (Getting Started) to run appropriate feature detection and choose between using Hls.js or natively built-in HLS support.\n\nWhen a platform has neither MediaSource nor native HLS support, you will not be able to play HLS.\n\nAll HLS resources must be delivered with CORS headers permitting requests.\n• Timed Metadata for HTTP Live Streaming (in ID3 format, carried in MPEG-2 TS)\n• SAMPLE-AES decryption (only supported if using MPEG-2 TS container)\n• Alternate Audio Track Rendition (Master Playlist with alternative Audio) for VoD and Live playlists\n• Adaptive streaming\n• Manual & Auto Quality Switching\n• 3 Quality Switching modes are available (controllable through API means)\n• Bandwidth conservative switching (quality switch change for next loaded fragment, without flushing the buffer)\n• In Auto-Quality mode, emergency switch down in case bandwidth is suddenly dropping to minimize buffering.\n• Accurate Seeking on VoD & Live (not limited to fragment or keyframe boundary)\n• Ability to seek in buffer and back buffer without redownloading segments\n• Built-in Analytics\n• Every internal events could be monitored (Network Events,Video Events)\n• Resilience to errors\n• Recovery actions could be triggered fix fatal media or network errors\n\nPull requests are welcome. Here is a quick guide on how to start.\n• First, checkout the repository and install required dependencies\n• Use EditorConfig or at least stay consistent to the file formats defined in the file.\n• Don't commit the updated file in your PR. We'll take care of generating an updated build right before releasing a new tagged version."
    },
    {
        "link": "https://developer.android.com/media/media3/exoplayer/hls",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nExoPlayer supports HLS with multiple container formats. The contained audio and video sample formats must also be supported (see the sample formats section for details). We strongly encourage HLS content producers to generate high quality HLS streams, as described here.\n\nTo play an HLS stream, you need to depend on the HLS module.\n\nYou can then create a for an HLS playlist URI and pass it to the player.\n\nIf your URI doesn't end with , you can pass to of to explicitly indicate the type of the content.\n\nThe URI of the media item may point to either a media playlist or a multivariant playlist. If the URI points to a multivariant playlist that declares multiple tags, then ExoPlayer will automatically adapt between variants, taking into account both available bandwidth and device capabilities.\n\nFor more customization options, you can create a and pass it directly to the player instead of a .\n\nYou can retrieve the current manifest by calling . For HLS, you should cast the returned object to . The callback of is also called whenever the manifest is loaded. This will happen once for on-demand content and possibly many times for live content. The following code snippet shows how an app can do something whenever the manifest is loaded.\n\nExoPlayer provides multiple ways for you to tailor playback experience to your app's needs. See the Customization page for examples.\n\nBy default, ExoPlayer will use chunkless preparation. This means that ExoPlayer will only use the information in the multivariant playlist to prepare the stream, which works if the tags contain the attribute.\n\nYou may need to disable this feature if your media segments contain muxed closed-caption tracks that are not declared in the multivariant playlist with a tag. Otherwise, these closed-caption tracks won't be detected and played. You can disable chunkless preparation in the as shown in the following snippet. Note that this will increase start up time as ExoPlayer needs to download a media segment to discover these additional tracks and it is preferable to declare the closed-caption tracks in the multivariant playlist instead.\n\nIn order to get the most out of ExoPlayer, there are certain guidelines you can follow to improve your HLS content. Read our Medium post about HLS playback in ExoPlayer for a full explanation. The main points are:\n• Use a continuous media stream; avoid changes in the media structure across segments.\n• Prefer demuxed streams, as opposed to files that include both video and audio.\n• Include all information you can in the Multivariant Playlist.\n\nThe following guidelines apply specifically for live streams:\n• Provide a long live window. One minute or more is great."
    }
]