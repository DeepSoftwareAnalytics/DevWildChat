[
    {
        "link": "https://huggingface.co/blog/sentiment-analysis-python",
        "document": "Getting Started with Sentiment Analysis using Python\n\nIn the past, sentiment analysis used to be limited to researchers, machine learning engineers or data scientists with experience in natural language processing. However, the AI community has built awesome tools to democratize access to machine learning in recent years. Nowadays, you can use sentiment analysis with a few lines of code and no machine learning experience at all! ü§Ø\n\nIn this guide, you'll learn everything to get started with sentiment analysis using Python, including:\n‚Ä¢ How to use pre-trained sentiment analysis models with Python\n‚Ä¢ How to build your own sentiment analysis model\n‚Ä¢ How to analyze tweets with sentiment analysis\n\nSentiment analysis is a natural language processing technique that identifies the polarity of a given text. There are different flavors of sentiment analysis, but one of the most widely used techniques labels data into positive, negative and neutral. For example, let's take a look at these tweets mentioning @VerizonSupport:\n‚Ä¢ None \"dear @verizonsupport your service is straight üí© in dallas.. been with y‚Äôall over a decade and this is all time low for y‚Äôall. i‚Äôm talking no internet at all.\" ‚Üí Would be tagged as \"Negative\".\n‚Ä¢ None \"@verizonsupport ive sent you a dm\" ‚Üí would be tagged as \"Neutral\".\n‚Ä¢ None \"thanks to michelle et al at @verizonsupport who helped push my no-show-phone problem along. order canceled successfully and ordered this for pickup today at the apple store in the mall.\" ‚Üí would be tagged as \"Positive\".\n\nSentiment analysis allows processing data at scale and in real-time. For example, do you want to analyze thousands of tweets, product reviews or support tickets? Instead of sorting through this data manually, you can use sentiment analysis to automatically understand how people are talking about a specific topic, get insights for data-driven decisions and automate business processes.\n\nSentiment analysis is used in a wide variety of applications, for example:\n‚Ä¢ Analyze social media mentions to understand how people are talking about your brand vs your competitors.\n‚Ä¢ Analyze feedback from surveys and product reviews to quickly get insights into what your customers like and dislike about your product.\n‚Ä¢ Analyze incoming support tickets in real-time to detect angry customers and act accordingly to prevent churn.\n\n2. How to Use Pre-trained Sentiment Analysis Models with Python\n\nNow that we have covered what sentiment analysis is, we are ready to play with some sentiment analysis models! üéâ\n\nOn the Hugging Face Hub, we are building the largest collection of models and datasets publicly available in order to democratize machine learning üöÄ. In the Hub, you can find more than 27,000 models shared by the AI community with state-of-the-art performances on tasks such as sentiment analysis, object detection, text generation, speech recognition and more. The Hub is free to use and most models have a widget that allows to test them directly on your browser!\n\nThere are more than 215 sentiment analysis models publicly available on the Hub and integrating them with Python just takes 5 lines of code:\n\nThis code snippet uses the pipeline class to make predictions from models available in the Hub. It uses the default model for sentiment analysis to analyze the list of texts and it outputs the following results:\n\nYou can use a specific sentiment analysis model that is better suited to your language or use case by providing the name of the model. For example, if you want a sentiment analysis model for tweets, you can specify the model id:\n\nYou can test these models with your own data using this Colab notebook:\n‚Ä¢ Twitter-roberta-base-sentiment is a roBERTa model trained on ~58M tweets and fine-tuned for sentiment analysis. Fine-tuning is the process of taking a pre-trained large language model (e.g. roBERTa in this case) and then tweaking it with additional training data to make it perform a second similar task (e.g. sentiment analysis).\n‚Ä¢ Bert-base-multilingual-uncased-sentiment is a model fine-tuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian.\n‚Ä¢ Distilbert-base-uncased-emotion is a model fine-tuned for detecting emotions in texts, including sadness, joy, love, anger, fear and surprise.\n\nAre you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models here and filter at the left according to the language of your interest.\n\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case.\n\nIn this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the ü§óTransformers, an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses AutoNLP, a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience.\n\nIn this tutorial, you'll use the IMDB dataset to fine-tune a DistilBERT model for sentiment analysis.\n\nThe IMDB dataset contains 25,000 movie reviews labeled by sentiment for training a model and 25,000 movie reviews for testing it. DistilBERT is a smaller, faster and cheaper version of BERT. It has 40% smaller than BERT and runs 60% faster while preserving over 95% of BERT‚Äôs performance. You'll use the IMDB dataset to fine-tune a DistilBERT model that is able to classify whether a movie review is positive or negative. Once you train the model, you will use it to analyze new data! ‚ö°Ô∏è\n\nWe have created this notebook so you can use it through this tutorial in Google Colab.\n\nAs a first step, let's set up Google Colab to use a GPU (instead of CPU) to train the model much faster. You can do this by going to the menu, clicking on 'Runtime' > 'Change runtime type', and selecting 'GPU' as the Hardware accelerator. Once you do this, you should check if GPU is available on our notebook by running the following code:\n\nThen, install the libraries you will be using in this tutorial:\n\nYou should also install to use git in our model repository:\n\nYou need data to fine-tune DistilBERT for sentiment analysis. So, let's use ü§óDatasets library to download and preprocess the IMDB dataset so you can then use this data for training your model:\n\nIMDB is a huge dataset, so let's create smaller datasets to enable faster training and testing:\n\nTo preprocess our data, you will use DistilBERT tokenizer:\n\nNext, you will prepare the text inputs for the model for both splits of our dataset (training and test) by using the map method:\n\nTo speed up training, let's use a data_collator to convert your training samples to PyTorch tensors and concatenate them with the correct amount of padding:\n\nNow that the preprocessing is done, you can go ahead and train your model üöÄ\n\nYou will be throwing away the pretraining head of the DistilBERT model and replacing it with a classification head fine-tuned for sentiment analysis. This enables you to transfer the knowledge from DistilBERT to your custom model üî•\n\nFor training, you will be using the Trainer API, which is optimized for fine-tuning Transformersü§ó models such as DistilBERT, BERT and RoBERTa.\n\nFirst, let's define DistilBERT as your base model:\n\nThen, let's define the metrics you will be using to evaluate how good is your fine-tuned model (accuracy and f1 score):\n\nNext, let's login to your Hugging Face account so you can manage your model repositories. will launch a widget in your notebook where you'll need to add your Hugging Face token:\n\nYou are almost there! Before training our model, you need to define the training arguments and define a Trainer with all the objects you constructed up to this point:\n\nNow, it's time to fine-tune the model on the sentiment analysis dataset! üôå You just have to call the method of your Trainer:\n\nAnd voila! You fine-tuned a DistilBERT model for sentiment analysis! üéâ\n\nTraining time depends on the hardware you use and the number of samples in the dataset. In our case, it took almost 10 minutes using a GPU and fine-tuning the model with 3,000 samples. The more samples you use for training your model, the more accurate it will be but training could be significantly slower.\n\nNext, let's compute the evaluation metrics to see how good your model is:\n\nIn our case, we got 88% accuracy and 89% f1 score. Quite good for a sentiment analysis model just trained with 3,000 samples!\n\n4. Analyzing new data with the model\n\nNow that you have trained a model for sentiment analysis, let's use it to analyze new data and get ü§ñ predictions! This unlocks the power of machine learning; using a model to automatically analyze data at scale, in real-time ‚ö°Ô∏è\n\nFirst, let's upload the model to the Hub:\n\nNow that you have pushed the model to the Hub, you can use it pipeline class to analyze two new movie reviews and see how your model predicts its sentiment with just two lines of code ü§Ø:\n\nThese are the predictions from our model:\n\nIn the IMDB dataset, means positive and is negative. Quite good! üî•\n\nAutoNLP is a tool to train state-of-the-art machine learning models without code. It provides a friendly and easy-to-use user interface, where you can train custom models by simply uploading your data. AutoNLP will automatically fine-tune various pre-trained models with your data, take care of the hyperparameter tuning and find the best model for your use case. All models trained with AutoNLP are deployed and ready for production.\n\nTraining a sentiment analysis model using AutoNLP is super easy and it just takes a few clicks ü§Ø. Let's give it a try!\n\nAs a first step, let's get some data! You'll use Sentiment140, a popular sentiment analysis dataset that consists of Twitter messages labeled with 3 sentiments: 0 (negative), 2 (neutral), and 4 (positive). The dataset is quite big; it contains 1,600,000 tweets. As you don't need this amount of data to get your feet wet with AutoNLP and train your first models, we have prepared a smaller version of the Sentiment140 dataset with 3,000 samples that you can download from here. This is how the dataset looks like:\n\nNext, let's create a new project on AutoNLP to train 5 candidate models:\n\nThen, upload the dataset and map the text column and target columns:\n\nOnce you add your dataset, go to the \"Trainings\" tab and accept the pricing to start training your models. AutoNLP pricing can be as low as $10 per model:\n\nAfter a few minutes, AutoNLP has trained all models, showing the performance metrics for all of them:\n\nThe best model has 77.87% accuracy üî• Pretty good for a sentiment analysis model for tweets trained with just 3,000 samples!\n\nAll these models are automatically uploaded to the Hub and deployed for production. You can use any of these models to start analyzing new data right away by using the pipeline class as shown in previous sections of this post.\n\nIn this last section, you'll take what you have learned so far in this post and put it into practice with a fun little project: analyzing tweets about NFTs with sentiment analysis!\n\nFirst, you'll use Tweepy, an easy-to-use Python library for getting tweets mentioning #NFTs using the Twitter API. Then, you will use a sentiment analysis model from the ü§óHub to analyze these tweets. Finally, you will create some visualizations to explore the results and find some interesting insights.\n\nYou can use this notebook to follow this tutorial. Let‚Äôs jump into it!\n\nFirst, let's install all the libraries you will use in this tutorial:\n\nNext, you will set up the credentials for interacting with the Twitter API. First, you'll need to sign up for a developer account on Twitter. Then, you have to create a new project and connect an app to get an API key and token. You can follow this step-by-step guide to get your credentials.\n\nOnce you have the API key and token, let's create a wrapper with Tweepy for interacting with the Twitter API:\n\nAt this point, you are ready to start using the Twitter API to collect tweets üéâ. You will use Tweepy Cursor to extract 1,000 tweets mentioning #NFTs:\n\nNow you can put our new skills to work and run sentiment analysis on your data! üéâ\n\nYou will use one of the models available on the Hub fine-tuned for sentiment analysis of tweets. Like in other sections of this post, you will use the pipeline class to make the predictions with this model:\n\nHow are people talking about NFTs on Twitter? Are they talking mostly positively or negatively? Let's explore the results of the sentiment analysis to find out!\n\nFirst, let's load the results on a dataframe and see examples of tweets that were labeled for each sentiment:\n\nThen, let's see how many tweets you got for each sentiment and visualize these results:\n\nInterestingly, most of the tweets about NFTs are positive (56.1%) and almost none are negative\n\n(2.0%):\n\nFinally, let's see what words stand out for each sentiment by creating a word cloud:\n\nSome of the words associated with positive tweets include Discord, Ethereum, Join, Mars4 and Shroom:\n\nIn contrast, words associated with negative tweets include: cookies chaos, Solana, and OpenseaNFT:\n\nAnd that is it! With just a few lines of python code, you were able to collect tweets, analyze them with sentiment analysis and create some cool visualizations to analyze the results! Pretty cool, huh?\n\nSentiment analysis with Python has never been easier! Tools such as ü§óTransformers and the ü§óHub makes sentiment analysis accessible to all developers. You can use open source, pre-trained models for sentiment analysis in just a few lines of code üî•\n\nDo you want to train a custom model for sentiment analysis with your own data? Easy peasy! You can fine-tune a model using Trainer API to build on top of large language models and get state-of-the-art results. If you want something even easier, you can use AutoNLP to train custom machine learning models by simply uploading data.\n\nIf you have questions, the Hugging Face community can help answer and/or benefit from, please ask them in the Hugging Face forum. Also, join our discord server to talk with us and with the Hugging Face community."
    },
    {
        "link": "https://huggingface.co/transformers/v3.5.1/quicktour.html",
        "document": "Let‚Äôs have a quick look at the ü§ó Transformers library features. The library downloads pretrained models for Natural Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG), such as completing a prompt with new text or translating in another language.\n\nFirst we will see how to easily leverage the pipeline API to quickly use those pretrained models at inference. Then, we will dig a little bit more and see how the library gives you access to those models and helps you preprocess your data.\n\nGetting started on a task with a pipeline¬∂ The easiest way to use a pretrained model on a given task is to use . ü§ó Transformers provides the following tasks out of the box:\n‚Ä¢ None Text generation (in English): provide a prompt and the model will generate what follows.\n‚Ä¢ None Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)\n‚Ä¢ None Question answering: provide the model with some context and a question, extract the answer from the context.\n‚Ä¢ None Filling masked text: given a text with masked words (e.g., replaced by ), fill the blanks. Let‚Äôs see how this work for sentiment analysis (the other tasks are all covered in the task summary): When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached. We will look at both later on, but as an introduction the tokenizer‚Äôs job is to preprocess the text for the model, which is then responsible for making predictions. The pipeline groups all of that together, and post-process the predictions to make them readable. For instance: 'We are very happy to show you the ü§ó Transformers library.' That‚Äôs encouraging! You can use it on a list of sentences, which will be preprocessed then fed to the model as a , returning a list of dictionaries like this one: \"We are very happy to show you the ü§ó Transformers library.\" \"We hope you don't hate it.\" You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is fairly neutral. By default, the model downloaded for this pipeline is called ‚Äúdistilbert-base-uncased-finetuned-sst-2-english‚Äù. We can look at its model page to get more information about it. It uses the DistilBERT architecture and has been fine-tuned on a dataset called SST-2 for the sentiment analysis task. Let‚Äôs say we want to use another model; for instance, one that has been trained on French data. We can search through the model hub that gathers models pretrained on a lot of data by research labs, but also community models (usually fine-tuned versions of those big models on a specific dataset). Applying the tags ‚ÄúFrench‚Äù and ‚Äútext-classification‚Äù gives back a suggestion ‚Äúnlptown/bert-base-multilingual-uncased-sentiment‚Äù. Let‚Äôs see how we can use it. You can directly pass the name of the model to use to : This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! You can also replace that name by a local folder where you have saved a pretrained model (see below). You can also pass a model object and its associated tokenizer. We will need two classes for this. The first is , which we will use to download the tokenizer associated to the model we picked and instantiate it. The second is (or if you are using TensorFlow), which we will use to download the model itself. Note that if we were using the library on an other task, the class of the model would change. The task summary tutorial summarizes which class is used for which task. Now, to download the models and tokenizer we found previously, we just have to use the method (feel free to replace by any other model from the model hub): # This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow. If you don‚Äôt find a model that has been pretrained on some data similar to yours, you will need to fine-tune a pretrained model on your data. We provide example scripts to do so. Once you‚Äôre done, don‚Äôt forget to share your fine-tuned model on the hub with the community, using this tutorial.\n\nLet‚Äôs now see what happens beneath the hood when using those pipelines. As we saw, the model and tokenizer are created using the method: We mentioned the tokenizer is responsible for the preprocessing of your texts. First, it will split a given text in words (or part of words, punctuation symbols, etc.) usually called . There are multiple rules that can govern that process (you can learn more about them in the tokenizer summary, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was pretrained. The second step is to convert those into numbers, to be able to build a tensor out of them and feed them to the model. To do this, the tokenizer has a , which is the part we download when we instantiate it with the method, since we need to use the same as when the model was pretrained. To apply these steps on a given text, we can just feed it to our tokenizer: \"We are very happy to show you the ü§ó Transformers library.\" This returns a dictionary string to list of ints. It contains the ids of the tokens, as mentioned before, but also additional arguments that will be useful to the model. Here for instance, we also have an attention mask that the model will use to have a better understanding of the sequence: You can pass a list of sentences directly to your tokenizer. If your goal is to send them through your model as a batch, you probably want to pad them all to the same length, truncate them to the maximum length the model can accept and get tensors back. You can specify all of that to the tokenizer: \"We are very happy to show you the ü§ó Transformers library.\" \"We hope you don't hate it.\" \"We are very happy to show you the ü§ó Transformers library.\" \"We hope you don't hate it.\" The padding is automatically applied on the side expected by the model (in this case, on the right), with the padding token the model was pretrained with. The attention mask is also adapted to take the padding into account: You can learn more about tokenizers here. Once your input has been preprocessed by the tokenizer, you can send it directly to the model. As we mentioned, it will contain all the relevant information the model needs. If you‚Äôre using a TensorFlow model, you can pass the dictionary keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding . In ü§ó Transformers, all outputs are tuples (with only one element potentially). Here, we get a tuple with just the final activations of the model. The model can return more than just the final activations, which is why the output is a tuple. Here we only asked for the final activations, so we get a tuple with one element. .. note: All ü§ó Transformers models (PyTorch or TensorFlow) return the activations of the model *before* the final activation function (like SoftMax) since this final activation function is often fused with the loss. Let‚Äôs apply the SoftMax activation to get predictions. We can see we get the numbers from before: If you have labels, you can provide them to the model, it will return a tuple with the loss and the final activations. Models are standard torch.nn.Module or tf.keras.Model so you can use them in your usual training loop. ü§ó Transformers also provides a (or if you are using TensorFlow) class to help with your training (taking care of things such as distributed training, mixed precision, etc.). See the training tutorial for more details. Pytorch model outputs are special dataclasses so that you can get autocompletion for their attributes in an IDE. They also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes not set (that have values) are ignored. Once your model is fine-tuned, you can save it with its tokenizer in the following way: You can then load this model back using the method by passing the directory name instead of the model name. One cool feature of ü§ó Transformers is that you can easily switch between PyTorch and TensorFlow: any model saved as before can be loaded back either in PyTorch or TensorFlow. If you are loading a saved PyTorch model in a TensorFlow model, use like this: and if you are loading a saved TensorFlow model in a PyTorch model, you should use the following code: Lastly, you can also ask the model to return all hidden states and all attention weights if you need them: The and classes are just shortcuts that will automatically work with any pretrained model. Behind the scenes, the library has one model class per combination of architecture plus class, so the code is easy to access and tweak if you need to. In our previous example, the model was called ‚Äúdistilbert-base-uncased-finetuned-sst-2-english‚Äù, which means it‚Äôs using the DistilBERT architecture. As (or if you are using TensorFlow) was used, the model automatically created is then a . You can look at its documentation for all details relevant to that specific model, or browse the source code. This is how you would directly instantiate model and tokenizer without the auto magic: If you want to change how the model itself is built, you can define your custom configuration class. Each architecture comes with its own relevant configuration (in the case of DistilBERT, ) which allows you to specify any of the hidden dimension, dropout rate, etc. If you do core modifications, like changing the hidden size, you won‚Äôt be able to use a pretrained model anymore and will need to train from scratch. You would then instantiate the model directly from this configuration. Here we use the predefined vocabulary of DistilBERT (hence load the tokenizer with the method) and initialize the model from scratch (hence instantiate the model from the configuration instead of using the method). For something that only changes the head of the model (for instance, the number of labels), you can still use a pretrained model for the body. For instance, let‚Äôs define a classifier for 10 different labels using a pretrained body. We could create a configuration with all the default values and just change the number of labels, but more easily, you can directly pass any argument a configuration would take to the method and it will update the default configuration with it:"
    },
    {
        "link": "https://huggingface.co/docs/transformers.js/en/pipelines",
        "document": "Just like the transformers Python library, Transformers.js provides users with a simple way to leverage the power of transformers. The function is the easiest and fastest way to use a pretrained model for inference.\n\nStart by creating an instance of and specifying a task you want to use it for. For example, to create a sentiment analysis pipeline, you can do:\n\nWhen running for the first time, the will download and cache the default pretrained model associated with the task. This can take a while, but subsequent calls will be much faster.\n\nYou can now use the classifier on your target text by calling it as a function:\n\nIf you have multiple inputs, you can pass them as an array:\n\nYou can also specify a different model to use for the pipeline by passing it as the second argument to the function. For example, to use a different model for sentiment analysis (like one trained to predict sentiment of a review as a number of stars between 1 and 5), you can do:\n\nTransformers.js supports loading any model hosted on the Hugging Face Hub, provided it has ONNX weights (located in a subfolder called ). For more information on how to convert your PyTorch, TensorFlow, or JAX model to ONNX, see the conversion section.\n\nThe function is a great way to quickly use a pretrained model for inference, as it takes care of all the preprocessing and postprocessing for you. For example, if you want to perform Automatic Speech Recognition (ASR) using OpenAI‚Äôs Whisper model, you can do:\n\nWe offer a variety of options to control how models are loaded from the Hugging Face Hub (or locally). By default, when running in-browser, a quantized version of the model is used, which is smaller and faster, but usually less accurate. To override this behaviour (i.e., use the unquantized model), you can use a custom object as the third parameter to the function:\n\nCheck out the section on quantization to learn more.\n\nYou can also specify which revision of the model to use, by passing a parameter. Since the Hugging Face Hub uses a git-based versioning system, you can use any valid git revision specifier (e.g., branch name or commit hash).\n\nFor the full list of options, check out the PretrainedOptions documentation.\n\nMany pipelines have additional options that you can specify. For example, when using a model that does multilingual translation, you can specify the source and target languages like this:\n\nWhen using models that support auto-regressive generation, you can specify generation parameters like the number of new tokens, sampling methods, temperature, repetition penalty, and much more. For a full list of available parameters, see to the GenerationConfig class.\n\nFor example, to generate a poem using , you can do:\n\nLogging to the console gives:\n\nSome pipelines such as or support streaming output. This is achieved using the class. For example, when using a chat model like , you can specify a callback function that will be called with each generated token text (if unset, new tokens will be printed to the console).\n\nLogging to the console gives:\n\nThis streaming feature allows you to process the output as it is generated, rather than waiting for the entire output to be generated before processing it.\n\nFor more information on the available options for each pipeline, refer to the API Reference. If you would like more control over the inference process, you can use the , , or classes instead."
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/index",
        "document": "and get access to the augmented documentation experience\n\nü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:\n\nüìù Natural Language Processing: text classification, named entity recognition, question answering, language modeling, code generation, summarization, translation, multiple choice, and text generation.\n\n üñºÔ∏è Computer Vision: image classification, object detection, and segmentation.\n\n üó£Ô∏è Audio: automatic speech recognition and audio classification.\n\n üêô Multimodal: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.\n\nü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model‚Äôs life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.\n\nJoin the growing community on the Hub, forum, or Discord today!\n\nIf you are looking for custom support from the Hugging Face team\n\nThe documentation is organized into five sections:\n‚Ä¢ None GET STARTED provides a quick tour of the library and installation instructions to get up and running.\n‚Ä¢ None TUTORIALS are a great place to start if you‚Äôre a beginner. This section will help you gain the basic skills you need to start using the library.\n‚Ä¢ None HOW-TO GUIDES show you how to achieve a specific goal, like finetuning a pretrained model for language modeling or how to write and share a custom model.\n‚Ä¢ None CONCEPTUAL GUIDES offers more discussion and explanation of the underlying concepts and ideas behind models, tasks, and the design philosophy of ü§ó Transformers.\n‚Ä¢ \n‚Ä¢ MAIN CLASSES details the most important classes like configuration, model, tokenizer, and pipeline.\n‚Ä¢ MODELS details the classes and functions related to each model implemented in the library.\n\nThe table below represents the current support in the library for each of those models, whether they have a Python tokenizer (called ‚Äúslow‚Äù). A ‚Äúfast‚Äù tokenizer backed by the ü§ó Tokenizers library, whether they have support in Jax (via Flax), PyTorch, and/or TensorFlow."
    },
    {
        "link": "https://huggingface.co/docs/transformers/main/en/tasks/sequence_classification",
        "document": "and get access to the augmented documentation experience\n\nText classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like üôÇ positive, üôÅ negative, or üòê neutral to a sequence of text.\n\nThis guide will show you how to:\n‚Ä¢ Finetune DistilBERT on the IMDb dataset to determine whether a movie review is positive or negative.\n‚Ä¢ Use your finetuned model for inference.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\nWe encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:\n\nStart by loading the IMDb dataset from the ü§ó Datasets library:\n\nThen take a look at an example:\n\nimdb[ ][ ] { : , : \"I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \\\"Gene Roddenberry's Earth...\\\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\" , }\n\nThere are two fields in this dataset:\n‚Ä¢ : a value that is either for a negative review or for a positive review.\n\nThe next step is to load a DistilBERT tokenizer to preprocess the field:\n\nCreate a preprocessing function to tokenize and truncate sequences to be no longer than DistilBERT‚Äôs maximum input length:\n\nTo apply the preprocessing function over the entire dataset, use ü§ó Datasets map function. You can speed up by setting to process multiple elements of the dataset at once:\n\nNow create a batch of examples using DataCollatorWithPadding. It‚Äôs more efficient to dynamically pad the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n\nIncluding a metric during training is often helpful for evaluating your model‚Äôs performance. You can quickly load a evaluation method with the ü§ó Evaluate library. For this task, load the accuracy metric (see the ü§ó Evaluate quick tour to learn more about how to load and compute a metric):\n\nThen create a function that passes your predictions and labels to compute to calculate the accuracy:\n\nYour function is ready to go now, and you‚Äôll return to it when you setup your training.\n\nBefore you start training your model, create a map of the expected ids to their labels with and :\n\nIf you aren‚Äôt familiar with finetuning a model with the Trainer, take a look at the basic tutorial here! You‚Äôre ready to start training your model now! Load DistilBERT with AutoModelForSequenceClassification along with the number of expected labels, and the label mappings: At this point, only three steps remain:\n‚Ä¢ Define your training hyperparameters in TrainingArguments. The only required parameter is which specifies where to save your model. You‚Äôll push this model to the Hub by setting (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the Trainer will evaluate the accuracy and save the training checkpoint.\n‚Ä¢ Pass the training arguments to Trainer along with the model, dataset, tokenizer, data collator, and function. Trainer applies dynamic padding by default when you pass to it. In this case, you don‚Äôt need to specify a data collator explicitly. Once training is completed, share your model to the Hub with the push_to_hub() method so everyone can use your model: If you aren‚Äôt familiar with finetuning a model with Keras, take a look at the basic tutorial here! To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters: To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters: Then you can load DistilBERT with TFAutoModelForSequenceClassification along with the number of expected labels, and the label mappings: Convert your datasets to the format with prepare_tf_dataset(): Configure the model for training with . Note that Transformers models all have a default task-relevant loss function, so you don‚Äôt need to specify one unless you want to: The last two things to setup before you start training is to compute the accuracy from the predictions, and provide a way to push your model to the Hub. Both are done by using Keras callbacks. Specify where to push your model and tokenizer in the PushToHubCallback: Then bundle your callbacks together: Finally, you‚Äôre ready to start training your model! Call with your training and validation datasets, the number of epochs, and your callbacks to finetune the model: Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!\n\nGreat, now that you‚Äôve finetuned a model, you can use it for inference!\n\nGrab some text you‚Äôd like to run inference on:\n\nThe simplest way to try out your finetuned model for inference is to use it in a pipeline(). Instantiate a for sentiment analysis with your model, and pass your text to it:\n\nYou can also manually replicate the results of the if you‚Äôd like:"
    },
    {
        "link": "https://huggingface.co/blog/sentiment-analysis-python",
        "document": "Getting Started with Sentiment Analysis using Python\n\nIn the past, sentiment analysis used to be limited to researchers, machine learning engineers or data scientists with experience in natural language processing. However, the AI community has built awesome tools to democratize access to machine learning in recent years. Nowadays, you can use sentiment analysis with a few lines of code and no machine learning experience at all! ü§Ø\n\nIn this guide, you'll learn everything to get started with sentiment analysis using Python, including:\n‚Ä¢ How to use pre-trained sentiment analysis models with Python\n‚Ä¢ How to build your own sentiment analysis model\n‚Ä¢ How to analyze tweets with sentiment analysis\n\nSentiment analysis is a natural language processing technique that identifies the polarity of a given text. There are different flavors of sentiment analysis, but one of the most widely used techniques labels data into positive, negative and neutral. For example, let's take a look at these tweets mentioning @VerizonSupport:\n‚Ä¢ None \"dear @verizonsupport your service is straight üí© in dallas.. been with y‚Äôall over a decade and this is all time low for y‚Äôall. i‚Äôm talking no internet at all.\" ‚Üí Would be tagged as \"Negative\".\n‚Ä¢ None \"@verizonsupport ive sent you a dm\" ‚Üí would be tagged as \"Neutral\".\n‚Ä¢ None \"thanks to michelle et al at @verizonsupport who helped push my no-show-phone problem along. order canceled successfully and ordered this for pickup today at the apple store in the mall.\" ‚Üí would be tagged as \"Positive\".\n\nSentiment analysis allows processing data at scale and in real-time. For example, do you want to analyze thousands of tweets, product reviews or support tickets? Instead of sorting through this data manually, you can use sentiment analysis to automatically understand how people are talking about a specific topic, get insights for data-driven decisions and automate business processes.\n\nSentiment analysis is used in a wide variety of applications, for example:\n‚Ä¢ Analyze social media mentions to understand how people are talking about your brand vs your competitors.\n‚Ä¢ Analyze feedback from surveys and product reviews to quickly get insights into what your customers like and dislike about your product.\n‚Ä¢ Analyze incoming support tickets in real-time to detect angry customers and act accordingly to prevent churn.\n\n2. How to Use Pre-trained Sentiment Analysis Models with Python\n\nNow that we have covered what sentiment analysis is, we are ready to play with some sentiment analysis models! üéâ\n\nOn the Hugging Face Hub, we are building the largest collection of models and datasets publicly available in order to democratize machine learning üöÄ. In the Hub, you can find more than 27,000 models shared by the AI community with state-of-the-art performances on tasks such as sentiment analysis, object detection, text generation, speech recognition and more. The Hub is free to use and most models have a widget that allows to test them directly on your browser!\n\nThere are more than 215 sentiment analysis models publicly available on the Hub and integrating them with Python just takes 5 lines of code:\n\nThis code snippet uses the pipeline class to make predictions from models available in the Hub. It uses the default model for sentiment analysis to analyze the list of texts and it outputs the following results:\n\nYou can use a specific sentiment analysis model that is better suited to your language or use case by providing the name of the model. For example, if you want a sentiment analysis model for tweets, you can specify the model id:\n\nYou can test these models with your own data using this Colab notebook:\n‚Ä¢ Twitter-roberta-base-sentiment is a roBERTa model trained on ~58M tweets and fine-tuned for sentiment analysis. Fine-tuning is the process of taking a pre-trained large language model (e.g. roBERTa in this case) and then tweaking it with additional training data to make it perform a second similar task (e.g. sentiment analysis).\n‚Ä¢ Bert-base-multilingual-uncased-sentiment is a model fine-tuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian.\n‚Ä¢ Distilbert-base-uncased-emotion is a model fine-tuned for detecting emotions in texts, including sadness, joy, love, anger, fear and surprise.\n\nAre you interested in doing sentiment analysis in languages such as Spanish, French, Italian or German? On the Hub, you will find many models fine-tuned for different use cases and ~28 languages. You can check out the complete list of sentiment analysis models here and filter at the left according to the language of your interest.\n\nUsing pre-trained models publicly available on the Hub is a great way to get started right away with sentiment analysis. These models use deep learning architectures such as transformers that achieve state-of-the-art performance on sentiment analysis and other machine learning tasks. However, you can fine-tune a model with your own data to further improve the sentiment analysis results and get an extra boost of accuracy in your particular use case.\n\nIn this section, we'll go over two approaches on how to fine-tune a model for sentiment analysis with your own data and criteria. The first approach uses the Trainer API from the ü§óTransformers, an open source library with 50K stars and 1K+ contributors and requires a bit more coding and experience. The second approach is a bit easier and more straightforward, it uses AutoNLP, a tool to automatically train, evaluate and deploy state-of-the-art NLP models without code or ML experience.\n\nIn this tutorial, you'll use the IMDB dataset to fine-tune a DistilBERT model for sentiment analysis.\n\nThe IMDB dataset contains 25,000 movie reviews labeled by sentiment for training a model and 25,000 movie reviews for testing it. DistilBERT is a smaller, faster and cheaper version of BERT. It has 40% smaller than BERT and runs 60% faster while preserving over 95% of BERT‚Äôs performance. You'll use the IMDB dataset to fine-tune a DistilBERT model that is able to classify whether a movie review is positive or negative. Once you train the model, you will use it to analyze new data! ‚ö°Ô∏è\n\nWe have created this notebook so you can use it through this tutorial in Google Colab.\n\nAs a first step, let's set up Google Colab to use a GPU (instead of CPU) to train the model much faster. You can do this by going to the menu, clicking on 'Runtime' > 'Change runtime type', and selecting 'GPU' as the Hardware accelerator. Once you do this, you should check if GPU is available on our notebook by running the following code:\n\nThen, install the libraries you will be using in this tutorial:\n\nYou should also install to use git in our model repository:\n\nYou need data to fine-tune DistilBERT for sentiment analysis. So, let's use ü§óDatasets library to download and preprocess the IMDB dataset so you can then use this data for training your model:\n\nIMDB is a huge dataset, so let's create smaller datasets to enable faster training and testing:\n\nTo preprocess our data, you will use DistilBERT tokenizer:\n\nNext, you will prepare the text inputs for the model for both splits of our dataset (training and test) by using the map method:\n\nTo speed up training, let's use a data_collator to convert your training samples to PyTorch tensors and concatenate them with the correct amount of padding:\n\nNow that the preprocessing is done, you can go ahead and train your model üöÄ\n\nYou will be throwing away the pretraining head of the DistilBERT model and replacing it with a classification head fine-tuned for sentiment analysis. This enables you to transfer the knowledge from DistilBERT to your custom model üî•\n\nFor training, you will be using the Trainer API, which is optimized for fine-tuning Transformersü§ó models such as DistilBERT, BERT and RoBERTa.\n\nFirst, let's define DistilBERT as your base model:\n\nThen, let's define the metrics you will be using to evaluate how good is your fine-tuned model (accuracy and f1 score):\n\nNext, let's login to your Hugging Face account so you can manage your model repositories. will launch a widget in your notebook where you'll need to add your Hugging Face token:\n\nYou are almost there! Before training our model, you need to define the training arguments and define a Trainer with all the objects you constructed up to this point:\n\nNow, it's time to fine-tune the model on the sentiment analysis dataset! üôå You just have to call the method of your Trainer:\n\nAnd voila! You fine-tuned a DistilBERT model for sentiment analysis! üéâ\n\nTraining time depends on the hardware you use and the number of samples in the dataset. In our case, it took almost 10 minutes using a GPU and fine-tuning the model with 3,000 samples. The more samples you use for training your model, the more accurate it will be but training could be significantly slower.\n\nNext, let's compute the evaluation metrics to see how good your model is:\n\nIn our case, we got 88% accuracy and 89% f1 score. Quite good for a sentiment analysis model just trained with 3,000 samples!\n\n4. Analyzing new data with the model\n\nNow that you have trained a model for sentiment analysis, let's use it to analyze new data and get ü§ñ predictions! This unlocks the power of machine learning; using a model to automatically analyze data at scale, in real-time ‚ö°Ô∏è\n\nFirst, let's upload the model to the Hub:\n\nNow that you have pushed the model to the Hub, you can use it pipeline class to analyze two new movie reviews and see how your model predicts its sentiment with just two lines of code ü§Ø:\n\nThese are the predictions from our model:\n\nIn the IMDB dataset, means positive and is negative. Quite good! üî•\n\nAutoNLP is a tool to train state-of-the-art machine learning models without code. It provides a friendly and easy-to-use user interface, where you can train custom models by simply uploading your data. AutoNLP will automatically fine-tune various pre-trained models with your data, take care of the hyperparameter tuning and find the best model for your use case. All models trained with AutoNLP are deployed and ready for production.\n\nTraining a sentiment analysis model using AutoNLP is super easy and it just takes a few clicks ü§Ø. Let's give it a try!\n\nAs a first step, let's get some data! You'll use Sentiment140, a popular sentiment analysis dataset that consists of Twitter messages labeled with 3 sentiments: 0 (negative), 2 (neutral), and 4 (positive). The dataset is quite big; it contains 1,600,000 tweets. As you don't need this amount of data to get your feet wet with AutoNLP and train your first models, we have prepared a smaller version of the Sentiment140 dataset with 3,000 samples that you can download from here. This is how the dataset looks like:\n\nNext, let's create a new project on AutoNLP to train 5 candidate models:\n\nThen, upload the dataset and map the text column and target columns:\n\nOnce you add your dataset, go to the \"Trainings\" tab and accept the pricing to start training your models. AutoNLP pricing can be as low as $10 per model:\n\nAfter a few minutes, AutoNLP has trained all models, showing the performance metrics for all of them:\n\nThe best model has 77.87% accuracy üî• Pretty good for a sentiment analysis model for tweets trained with just 3,000 samples!\n\nAll these models are automatically uploaded to the Hub and deployed for production. You can use any of these models to start analyzing new data right away by using the pipeline class as shown in previous sections of this post.\n\nIn this last section, you'll take what you have learned so far in this post and put it into practice with a fun little project: analyzing tweets about NFTs with sentiment analysis!\n\nFirst, you'll use Tweepy, an easy-to-use Python library for getting tweets mentioning #NFTs using the Twitter API. Then, you will use a sentiment analysis model from the ü§óHub to analyze these tweets. Finally, you will create some visualizations to explore the results and find some interesting insights.\n\nYou can use this notebook to follow this tutorial. Let‚Äôs jump into it!\n\nFirst, let's install all the libraries you will use in this tutorial:\n\nNext, you will set up the credentials for interacting with the Twitter API. First, you'll need to sign up for a developer account on Twitter. Then, you have to create a new project and connect an app to get an API key and token. You can follow this step-by-step guide to get your credentials.\n\nOnce you have the API key and token, let's create a wrapper with Tweepy for interacting with the Twitter API:\n\nAt this point, you are ready to start using the Twitter API to collect tweets üéâ. You will use Tweepy Cursor to extract 1,000 tweets mentioning #NFTs:\n\nNow you can put our new skills to work and run sentiment analysis on your data! üéâ\n\nYou will use one of the models available on the Hub fine-tuned for sentiment analysis of tweets. Like in other sections of this post, you will use the pipeline class to make the predictions with this model:\n\nHow are people talking about NFTs on Twitter? Are they talking mostly positively or negatively? Let's explore the results of the sentiment analysis to find out!\n\nFirst, let's load the results on a dataframe and see examples of tweets that were labeled for each sentiment:\n\nThen, let's see how many tweets you got for each sentiment and visualize these results:\n\nInterestingly, most of the tweets about NFTs are positive (56.1%) and almost none are negative\n\n(2.0%):\n\nFinally, let's see what words stand out for each sentiment by creating a word cloud:\n\nSome of the words associated with positive tweets include Discord, Ethereum, Join, Mars4 and Shroom:\n\nIn contrast, words associated with negative tweets include: cookies chaos, Solana, and OpenseaNFT:\n\nAnd that is it! With just a few lines of python code, you were able to collect tweets, analyze them with sentiment analysis and create some cool visualizations to analyze the results! Pretty cool, huh?\n\nSentiment analysis with Python has never been easier! Tools such as ü§óTransformers and the ü§óHub makes sentiment analysis accessible to all developers. You can use open source, pre-trained models for sentiment analysis in just a few lines of code üî•\n\nDo you want to train a custom model for sentiment analysis with your own data? Easy peasy! You can fine-tune a model using Trainer API to build on top of large language models and get state-of-the-art results. If you want something even easier, you can use AutoNLP to train custom machine learning models by simply uploading data.\n\nIf you have questions, the Hugging Face community can help answer and/or benefit from, please ask them in the Hugging Face forum. Also, join our discord server to talk with us and with the Hugging Face community."
    },
    {
        "link": "https://huggingface.co/docs/transformers/en/main_classes/pipelines",
        "document": "and get access to the augmented documentation experience\n\nThe pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the task summary for examples of use.\n\nThere are two categories of pipeline abstractions to be aware about:\n‚Ä¢ The pipeline() which is the most powerful object encapsulating all other pipelines.\n‚Ä¢ Task-specific pipelines are available for audio, computer vision, natural language processing, and multimodal tasks.\n\nThe pipeline abstraction is a wrapper around all the other available pipelines. It is instantiated as any other pipeline but can provide additional quality of life.\n\nIf you want to use a specific model from the hub you can ignore the task if the model on the hub already defines it:\n\nTo call a pipeline on many items, you can call it with a list.\n\nTo iterate over full datasets it is recommended to use a directly. This means you don‚Äôt need to allocate the whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on GPU. If it doesn‚Äôt don‚Äôt hesitate to create an issue.\n\nFor ease of use, a generator is also possible:\n\nAll pipelines can use batching. This will work whenever the pipeline uses its streaming ability (so when passing lists or or ).\n\nExample where it‚Äôs most a slowdown:\n\nThis is a occasional very long sentence compared to the other. In that case, the whole batch will need to be 400 tokens long, so the whole batch will be [64, 400] instead of [64, 4], leading to the high slowdown. Even worse, on bigger batches, the program simply crashes.\n\nThere are no good (general) solutions for this problem, and your mileage may vary depending on your use cases. Rule of thumb:\n\nFor users, a rule of thumb is:\n‚Ä¢ None Measure performance on your load, with your hardware. Measure, measure, and keep measuring. Real numbers are the only way to go.\n‚Ä¢ None If you are latency constrained (live product doing inference), don‚Äôt batch.\n‚Ä¢ None If you are using CPU, don‚Äôt batch.\n‚Ä¢ None If you are using throughput (you want to run your model on a bunch of static data), on GPU, then:\n‚Ä¢ If you have no clue about the size of the sequence_length (‚Äúnatural‚Äù data), by default don‚Äôt batch, measure and try tentatively to add it, add OOM checks to recover when it will fail (and it will at some point if you don‚Äôt control the sequence_length.)\n‚Ä¢ If your sequence_length is super regular, then batching is more likely to be VERY interesting, measure and push it until you get OOMs.\n‚Ä¢ The larger the GPU the more likely batching is going to be more interesting\n‚Ä¢ None As soon as you enable batching, make sure you can handle OOMs nicely.\n\nand are slightly specific in the sense, that a single input might yield multiple forward pass of a model. Under normal circumstances, this would yield issues with argument.\n\nIn order to circumvent this issue, both of these pipelines are a bit specific, they are instead of regular . In short:\n\nThis should be very transparent to your code because the pipelines are used in the same way.\n\nThis is a simplified view, since the pipeline can handle automatically the batch to ! Meaning you don‚Äôt have to care about how many forward passes you inputs are actually going to trigger, you can optimize the independently of the inputs. The caveats from the previous section still apply.\n\nModels can be run in FP16 which can be significantly faster on GPU while saving memory. Most models will not suffer noticeable performance loss from this. The larger the model, the less likely that it will.\n\nTo enable FP16 inference, you can simply pass or to the pipeline constructor. Note that this only works for models with a PyTorch backend. Your inputs will be converted to FP16 internally.\n\nIf you want to override a specific pipeline.\n\nDon‚Äôt hesitate to create an issue for your task at hand, the goal of the pipeline is to be easy to use and support most cases, so could maybe support your use case.\n\nIf you want to try simply you can:\n\nThat should enable you to do all the custom code you want.\n\nPipelines available for audio tasks include the following.\n\nPipelines available for computer vision tasks include the following.\n\nPipelines available for natural language processing tasks include the following.\n\nPipelines available for multimodal tasks include the following."
    },
    {
        "link": "https://stackoverflow.com/questions/77159136/efficiently-using-hugging-face-transformers-pipelines-on-gpu-with-large-datasets",
        "document": "I think you can ignore this message. I found it being reported on different websites this year, but if I get it correctly, this Github issue on the Huggingface transformers (https://github.com/huggingface/transformers/issues/22387) shows that the warning can be safely ignored. In addition, batching or using might not remove the warning or automatically use the resources in the best way. You can do in here (https://github.com/huggingface/transformers/blob/main/src/transformers/pipelines/base.py#L1100) to ignore the warning, as explained by Martin Weyssow above.\n\nHow can I modify my code to batch my data and use parallel computing to make better use of my GPU resources:\n\nYou can add batching like this:\n\nand most importantly, you can experiment with the batch size that will result to the highest GPU usage possible on your device and particular task.\n\nHuggingface provides here some rules to help users figure out how to batch: https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching. Making the best resource/GPU usage possible might take some experimentation and it depends on the use case you work on every time.\n\nWhat does this warning mean, and why should I use a dataset for efficiency?\n\nThis means the GPU utilization is not optimal, because the data is not grouped together and it is thus not processed efficiently. Using a dataset from the Huggingface library will utilize your resources more efficiently. However, it is not so easy to tell what exactly is going on, especially considering that we don‚Äôt know exactly how the data looks like, what the device is and how the model deals with the data internally. The warning might go away by using the library, but that does not necessarily mean that the resources are optimally used.\n\nWhat code or function or library should be used with hugging face transformers?\n\nHere is a code example with and the library: https://huggingface.co/docs/transformers/v4.27.1/pipeline_tutorial#using-pipelines-on-a-dataset. It mentions that using iterables will fill your GPU as fast as possible and batching might also help with computational time improvements.\n\nIn your case it seems you are doing a relatively small POC (doing inference for under 10,000 documents with a medium size model), so I don‚Äôt think you need to use pipelines. I assume the sentiment analysis model is a classifier and you want to keep using as shown in the post, so here is how you can combine both. This is usually fast enough for my experiments and prints no warnings about the resources.\n\nAs soon as your inference starts, either with this snippet or with the library code, you can run in a terminal and check what the GPU usage is and play around with the parameters to optimize it. Beware that running the code on your local machine with a GPU vs running it on a larger machine, e.g., a Linux server with perhaps a more powerful GPU might lead to different performance and might need different tuning. If you wish to run the code for larger document collections, you can split the data in order to avoid GPU memory errors locally, or in order to speed up the inference with concurrent runs in a server."
    },
    {
        "link": "https://medium.com/@durgeshgurnani/sentiment-analysis-with-hugging-face-a-step-by-step-guide-7a95f38a9bf8",
        "document": "How to Use Hugging Face Transformers to Perform Sentiment Analysis in Python\n\nSentiment analysis is a popular natural language processing (NLP) task that involves determining the sentiment expressed in a piece of text. With the rise of machine learning and deep learning, performing sentiment analysis has become easier than ever, thanks to powerful libraries like Hugging Face‚Äôs Transformers. In this article, we‚Äôll explore how to use the Hugging Face function for sentiment analysis, walking through each line of code and offering practical examples to solidify your understanding.\n‚Ä¢ Importing the pipeline: The function is a powerful feature of the Hugging Face Transformers library. It provides a high-level interface for various NLP tasks like sentiment analysis, text generation, translation, and more. By importing , we gain access to these capabilities without needing to manually load models or write complex code.\n‚Ä¢ Initializing the sentiment analysis pipeline: Here, we create an instance of the specifically for sentiment analysis by passing the argument . The pipeline automatically loads a pre-trained model‚Ä¶"
    },
    {
        "link": "https://stackoverflow.com/questions/66906652/how-to-download-hugging-face-sentiment-analysis-pipeline-to-use-it-offline",
        "document": "How to download hugging face sentiment-analysis pipeline to use it offline? I'm unable to use hugging face sentiment analysis pipeline without internet. How to download that pipeline?\n\nThe basic code for sentiment analysis using hugging face is\n\nAnd the output is"
    }
]