[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html",
        "document": "Read more in the User Guide.\n\nNumber of neighbors to use by default for queries. Weight function used in prediction. Possible values:\n• None ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n• None ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n• None [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. Refer to the example entitled Nearest Neighbors Classification showing the impact of the parameter on the decision boundary. Algorithm used to compute the nearest neighbors:\n• None ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to method. Note: fitting on sparse input will override the setting of this parameter, using brute force. Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected to be positive. Metric to use for distance computation. Default is “minkowski”, which results in the standard Euclidean distance when p = 2. See the documentation of scipy.spatial.distance and the metrics listed in for valid metric values. If metric is “precomputed”, X is assumed to be a distance matrix and must be square during fit. X may be a sparse graph, in which case only “nonzero” elements may be considered neighbors. If metric is a callable function, it takes two arrays representing 1D vectors as inputs and must return one value indicating the distance between those vectors. This works for Scipy’s metrics, but is less efficient than passing the metric name as a string. The number of parallel jobs to run for neighbors search. means 1 unless in a context. means using all processors. See Glossary for more details. Doesn’t affect method. Class labels known to the classifier The distance metric used. It will be same as the parameter or a synonym of it, e.g. ‘euclidean’ if the parameter set to ‘minkowski’ and parameter set to 2. Additional keyword arguments for the metric function. For most metrics will be same with parameter, but may also contain the parameter value if the attribute is set to ‘minkowski’. Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings. Number of samples in the fitted data. False when ’s shape is (n_samples, ) or (n_samples, 1) during fit otherwise True.\n\nSee Nearest Neighbors in the online documentation for a discussion of the choice of and .\n\nRegarding the Nearest Neighbors algorithms, if it is found that two neighbors, neighbor and , have identical distances but different labels, the results will depend on the ordering of the training data.\n\nReturns indices of and distances to the neighbors of each point. The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. Number of neighbors required for each sample. The default is the value passed to the constructor. Whether or not to return the distances. Array representing the lengths to points, only present if return_distance=True. Indices of the nearest points in the population matrix. In the following example, we construct a NearestNeighbors class from an array representing our data set and ask who’s the closest point to [1,1,1] As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:\n\nCompute the (weighted) graph of k-Neighbors for points in X. X {array-like, sparse matrix} of shape (n_queries, n_features), or (n_queries, n_indexed) if metric == ‘precomputed’, default=None The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. For the shape should be (n_queries, n_indexed). Otherwise the shape should be (n_queries, n_features). Number of neighbors for each sample. The default is the value passed to the constructor. Type of returned matrix: ‘connectivity’ will return the connectivity matrix with ones and zeros, in ‘distance’ the edges are distances between points, type of distance depends on the selected metric parameter in NearestNeighbors class. is the number of samples in the fitted data. gives the weight of the edge connecting to . The matrix is of CSR format. Compute the (weighted) graph of Neighbors for points in X.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/neighbors.html",
        "document": "provides functionality for unsupervised and supervised neighbors-based learning methods. Unsupervised nearest neighbors is the foundation of many other learning methods, notably manifold learning and spectral clustering. Supervised neighbors-based learning comes in two flavors: classification for data with discrete labels, and regression for data with continuous labels.\n\nThe principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree).\n\nDespite its simplicity, nearest neighbors has been successful in a large number of classification and regression problems, including handwritten digits and satellite image scenes. Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n\nThe classes in can handle either NumPy arrays or matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.\n\nThere are many learning routines which rely on nearest neighbors at their core. One example is kernel density estimation, discussed in the density estimation section.\n\nimplements unsupervised nearest neighbors learning. It acts as a uniform interface to three different nearest neighbors algorithms: , , and a brute-force algorithm based on routines in . The choice of neighbors search algorithm is controlled through the keyword , which must be one of . When the default value is passed, the algorithm attempts to determine the best approach from the training data. For a discussion of the strengths and weaknesses of each option, see Nearest Neighbor Algorithms. Regarding the Nearest Neighbors algorithms, if two neighbors \\(k+1\\) and \\(k\\) have identical distances but different labels, the result will depend on the ordering of the training data. For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within can be used: Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance of zero. It is also possible to efficiently produce a sparse graph showing the connections between neighboring points: The dataset is structured such that points nearby in index order are nearby in parameter space, leading to an approximately block-diagonal matrix of K-nearest neighbors. Such a sparse graph is useful in a variety of circumstances which make use of spatial relationships between points for unsupervised learning: in particular, see , , and . Alternatively, one can use the or classes directly to find nearest neighbors. This is the functionality wrapped by the class used above. The Ball Tree and KD Tree have the same interface; we’ll show an example of using the KD Tree here: Refer to the and class documentation for more information on the options available for nearest neighbors searches, including specification of query strategies, distance metrics, etc. For a list of valid metrics use and :\n\nNeighbors-based classification is a type of instance-based learning or non-generalizing learning: it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point: a query point is assigned the data class which has the most representatives within the nearest neighbors of the point. scikit-learn implements two different nearest neighbors classifiers: implements learning based on the \\(k\\) nearest neighbors of each query point, where \\(k\\) is an integer value specified by the user. implements learning based on the number of neighbors within a fixed radius \\(r\\) of each training point, where \\(r\\) is a floating-point value specified by the user. The \\(k\\)-neighbors classification in is the most commonly used technique. The optimal choice of the value \\(k\\) is highly data-dependent: in general a larger \\(k\\) suppresses the effects of noise, but makes the classification boundaries less distinct. In cases where the data is not uniformly sampled, radius-based neighbors classification in can be a better choice. The user specifies a fixed radius \\(r\\), such that points in sparser neighborhoods use fewer nearest neighbors for the classification. For high-dimensional parameter spaces, this method becomes less effective due to the so-called “curse of dimensionality”. The basic nearest neighbors classification uses uniform weights: that is, the value assigned to a query point is computed from a simple majority vote of the nearest neighbors. Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the keyword. The default value, , assigns uniform weights to each neighbor. assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied to compute the weights.\n• None Nearest Neighbors Classification: an example of classification using nearest neighbors.\n\nNeighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors. scikit-learn implements two different neighbors regressors: implements learning based on the \\(k\\) nearest neighbors of each query point, where \\(k\\) is an integer value specified by the user. implements learning based on the neighbors within a fixed radius \\(r\\) of the query point, where \\(r\\) is a floating-point value specified by the user. The basic nearest neighbors regression uses uniform weights: that is, each point in the local neighborhood contributes uniformly to the classification of a query point. Under some circumstances, it can be advantageous to weight points such that nearby points contribute more to the regression than faraway points. This can be accomplished through the keyword. The default value, , assigns equal weights to all points. assigns weights proportional to the inverse of the distance from the query point. Alternatively, a user-defined function of the distance can be supplied, which will be used to compute the weights. The use of multi-output nearest neighbors for regression is demonstrated in Face completion with a multi-output estimators. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces.\n• None Nearest Neighbors regression: an example of regression using nearest neighbors.\n• None Face completion with a multi-output estimators: an example of multi-output regression using nearest neighbors.\n\nFast computation of nearest neighbors is an active area of research in machine learning. The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset: for \\(N\\) samples in \\(D\\) dimensions, this approach scales as \\(O[D N^2]\\). Efficient brute-force neighbors searches can be very competitive for small data samples. However, as the number of samples \\(N\\) grows, the brute-force approach quickly becomes infeasible. In the classes within , brute-force neighbors searches are specified using the keyword , and are computed using the routines available in . To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt to reduce the required number of distance calculations by efficiently encoding aggregate distance information for the sample. The basic idea is that if point \\(A\\) is very distant from point \\(B\\), and point \\(B\\) is very close to point \\(C\\), then we know that points \\(A\\) and \\(C\\) are very distant, without having to explicitly calculate their distance. In this way, the computational cost of a nearest neighbors search can be reduced to \\(O[D N \\log(N)]\\) or better. This is a significant improvement over brute-force for large \\(N\\). An early approach to taking advantage of this aggregate information was the KD tree data structure (short for K-dimensional tree), which generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotropic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no \\(D\\)-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only \\(O[\\log(N)]\\) distance computations. Though the KD tree approach is very fast for low-dimensional (\\(D < 20\\)) neighbors searches, it becomes inefficient as \\(D\\) grows very large: this is one manifestation of the so-called “curse of dimensionality”. In scikit-learn, KD tree neighbors searches are specified using the keyword , and are computed using the class .\n• None “Multidimensional binary search trees used for associative searching”, Bentley, J.L., Communications of the ACM (1975) To address the inefficiencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a series of nesting hyper-spheres. This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly structured data, even in very high dimensions. A ball tree recursively divides the data into nodes defined by a centroid \\(C\\) and radius \\(r\\), such that each point in the node lies within the hyper-sphere defined by \\(r\\) and \\(C\\). The number of candidate points for a neighbor search is reduced through use of the triangle inequality: With this setup, a single distance calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node. Because of the spherical geometry of the ball tree nodes, it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure of the training data. In scikit-learn, ball-tree-based neighbors searches are specified using the keyword , and are computed using the class . Alternatively, the user can work with the class directly. The optimal algorithm for a given dataset is a complicated choice, and depends on a number of factors:\n• \n• None KD tree query time changes with \\(D\\) in a way that is difficult to precisely characterise. For small \\(D\\) (less than 20 or so) the cost is approximately \\(O[D\\log(N)]\\), and the KD tree query can be very efficient. For larger \\(D\\), the cost increases to nearly \\(O[DN]\\), and the overhead due to the tree structure can lead to queries which are slower than brute force. For small data sets (\\(N\\) less than 30 or so), \\(\\log(N)\\) is comparable to \\(N\\), and brute force algorithms can be more efficient than a tree-based approach. Both and address this through providing a leaf size parameter: this controls the number of samples at which a query switches to brute-force. This allows both algorithms to approach the efficiency of a brute-force computation for small \\(N\\).\n• None data structure: intrinsic dimensionality of the data and/or sparsity of the data. Intrinsic dimensionality refers to the dimension \\(d \\le D\\) of a manifold on which the data lies, which can be linearly or non-linearly embedded in the parameter space. Sparsity refers to the degree to which the data fills the parameter space (this is to be distinguished from the concept as used in “sparse” matrices. The data matrix may have no zero entries, but the structure can still be “sparse” in this sense).\n• None Ball tree and KD tree query times can be greatly influenced by data structure. In general, sparser data with a smaller intrinsic dimensionality leads to faster query times. Because the KD tree internal representation is aligned with the parameter axes, it will not generally show as much improvement as ball tree for arbitrarily structured data. Datasets used in machine learning tend to be very structured, and are very well-suited for tree-based queries.\n• \n• None Brute force query time is largely unaffected by the value of \\(k\\)\n• None Ball tree and KD tree query time will become slower as \\(k\\) increases. This is due to two effects: first, a larger \\(k\\) leads to the necessity to search a larger portion of the parameter space. Second, using \\(k > 1\\) requires internal queueing of results as the tree is traversed. As \\(k\\) becomes large compared to \\(N\\), the ability to prune branches in a tree-based query is reduced. In this situation, Brute force queries can be more efficient.\n• None number of query points. Both the ball tree and the KD Tree require a construction phase. The cost of this construction becomes negligible when amortized over many queries. If only a small number of queries will be performed, however, the construction can make up a significant fraction of the total cost. If very few query points will be required, brute force is better than a tree-based method. Currently, selects if any of the following conditions are verified:\n• None isn’t in the list for either or Otherwise, it selects the first out of and that has in its list. This heuristic is based on the following assumptions:\n• None the number of query points is at least the same order as the number of training points\n• None is close to its default value of\n• None when \\(D > 15\\), the intrinsic dimensionality of the data is generally too high for tree-based methods As noted above, for small sample sizes a brute force search can be more efficient than a tree-based query. This fact is accounted for in the ball tree and KD tree by internally switching to brute force searches within leaf nodes. The level of this switch can be specified with the parameter . This parameter choice has many effects: A larger leads to a faster tree construction time, because fewer nodes need to be created Both a large or small can lead to suboptimal query cost. For approaching 1, the overhead involved in traversing nodes can significantly slow query times. For approaching the size of the training set, queries become essentially brute force. A good compromise between these is , the default value of the parameter. As increases, the memory required to store a tree structure decreases. This is especially important in the case of ball tree, which stores a \\(D\\)-dimensional centroid for each node. The required storage space for is approximately times the size of the training set. is not referenced for brute force queries. For a list of available metrics, see the documentation of the class and the metrics listed in . Note that the “cosine” metric uses . A list of valid metrics for any of the above algorithms can be obtained by using their attribute. For example, valid metrics for can be generated by:\n\nThe classifier is a simple algorithm that represents each class by the centroid of its members. In effect, this makes it similar to the label updating phase of the algorithm. It also has no parameters to choose, making it a good baseline classifier. It does, however, suffer on non-convex classes, as well as when classes have drastically different variances, as equal variance in all dimensions is assumed. See Linear Discriminant Analysis ( ) and Quadratic Discriminant Analysis ( ) for more complex methods that do not make this assumption. Usage of the default is simple: The classifier has a parameter, which implements the nearest shrunken centroid classifier. In effect, the value of each feature for each centroid is divided by the within-class variance of that feature. The feature values are then reduced by . Most notably, if a particular feature value crosses zero, it is set to zero. In effect, this removes the feature from affecting the classification. This is useful, for example, for removing noisy features. In the example below, using a small shrink threshold increases the accuracy of the model from 0.81 to 0.82.\n• None Nearest Centroid Classification: an example of classification using nearest centroid with different shrink thresholds.\n\nMany scikit-learn estimators rely on nearest neighbors: Several classifiers and regressors such as and , but also some clustering methods such as and , and some manifold embeddings such as and . All these estimators can compute internally the nearest neighbors, but most of them also accept precomputed nearest neighbors sparse graph, as given by and . With mode , these functions return a binary adjacency sparse graph as required, for instance, in . Whereas with , they return a distance sparse graph as required, for instance, in . To include these functions in a scikit-learn pipeline, one can also use the corresponding classes and . The benefits of this sparse graph API are multiple. First, the precomputed graph can be re-used multiple times, for instance while varying a parameter of the estimator. This can be done manually by the user, or using the caching properties of the scikit-learn pipeline: # we use a temporary folder here Second, precomputing the graph can give finer control on the nearest neighbors estimation, for instance enabling multiprocessing though the parameter , which might not be available in all estimators. Finally, the precomputation can be performed by custom estimators to use different implementations, such as approximate nearest neighbors methods, or implementation with special data types. The precomputed neighbors sparse graph needs to be formatted as in output:\n• None a CSR matrix (although COO, CSC or LIL will be accepted).\n• None only explicitly store nearest neighborhoods of each sample with respect to the training data. This should include those at 0 distance from a query point, including the matrix diagonal when computing the nearest neighborhoods between the training data and itself.\n• None each row’s should store the distance in increasing order (optional. Unsorted data will be stable-sorted, adding a computational overhead).\n• None all values in data should be non-negative.\n• None there should be no duplicate in any row (see scipy/scipy#5807).\n• None if the algorithm being passed the precomputed matrix uses k nearest neighbors (as opposed to radius neighborhood), at least k neighbors must be stored in each row (or k+1, as explained in the following note). When a specific number of neighbors is queried (using ), the definition of is ambiguous since it can either include each training point as its own neighbor, or exclude them. Neither choice is perfect, since including them leads to a different number of non-self neighbors during training and testing, while excluding them leads to a difference between and , which is against scikit-learn API. In we use the definition which includes each training point as its own neighbor in the count of . However, for compatibility reasons with other estimators which use the other definition, one extra neighbor will be computed when . To maximise compatibility with all estimators, a safe choice is to always include one extra neighbor in a custom nearest neighbors estimator, since unnecessary neighbors will be filtered by following estimators.\n• None Approximate nearest neighbors in TSNE: an example of pipelining and . Also proposes two custom nearest neighbors estimators based on external packages.\n• None Caching nearest neighbors: an example of pipelining and to enable caching of the neighbors graph during a hyper-parameter grid-search.\n\nNeighborhood Components Analysis (NCA, ) is a distance metric learning algorithm which aims to improve the accuracy of nearest neighbors classification compared to the standard Euclidean distance. The algorithm directly maximizes a stochastic variant of the leave-one-out k-nearest neighbors (KNN) score on the training set. It can also learn a low-dimensional linear projection of data that can be used for data visualization and fast classification. In the above illustrating figure, we consider some points from a randomly generated dataset. We focus on the stochastic KNN classification of point no. 3. The thickness of a link between sample 3 and another point is proportional to their distance, and can be seen as the relative weight (or probability) that a stochastic nearest neighbor prediction rule would assign to this point. In the original space, sample 3 has many stochastic neighbors from various classes, so the right class is not very likely. However, in the projected space learned by NCA, the only stochastic neighbors with non-negligible weight are from the same class as sample 3, guaranteeing that the latter will be well classified. See the mathematical formulation for more details. Combined with a nearest neighbors classifier ( ), NCA is attractive for classification because it can naturally handle multi-class problems without any increase in the model size, and does not introduce additional parameters that require fine-tuning by the user. NCA classification has been shown to work well in practice for data sets of varying size and difficulty. In contrast to related methods such as Linear Discriminant Analysis, NCA does not make any assumptions about the class distributions. The nearest neighbor classification can naturally produce highly irregular decision boundaries. To use this model for classification, one needs to combine a instance that learns the optimal transformation with a instance that performs the classification in the projected space. Here is an example using the two classes: The plot shows decision boundaries for Nearest Neighbor Classification and Neighborhood Components Analysis classification on the iris dataset, when training and scoring on only two features, for visualisation purposes. NCA can be used to perform supervised dimensionality reduction. The input data are projected onto a linear subspace consisting of the directions which minimize the NCA objective. The desired dimensionality can be set using the parameter . For instance, the following figure shows a comparison of dimensionality reduction with Principal Component Analysis ( ), Linear Discriminant Analysis ( ) and Neighborhood Component Analysis ( ) on the Digits dataset, a dataset with size \\(n_{samples} = 1797\\) and \\(n_{features} = 64\\). The data set is split into a training and a test set of equal size, then standardized. For evaluation the 3-nearest neighbor classification accuracy is computed on the 2-dimensional projected points found by each method. Each data sample belongs to one of 10 classes.\n• None Comparing Nearest Neighbors with and without Neighborhood Components Analysis The goal of NCA is to learn an optimal linear transformation matrix of size , which maximises the sum over all samples \\(i\\) of the probability \\(p_i\\) that \\(i\\) is correctly classified, i.e.: with \\(N\\) = and \\(p_i\\) the probability of sample \\(i\\) being correctly classified according to a stochastic nearest neighbors rule in the learned embedded space: where \\(C_i\\) is the set of points in the same class as sample \\(i\\), and \\(p_{i j}\\) is the softmax over Euclidean distances in the embedded space: NCA can be seen as learning a (squared) Mahalanobis distance metric: where \\(M = L^T L\\) is a symmetric positive semi-definite matrix of size . This implementation follows what is explained in the original paper . For the optimisation method, it currently uses scipy’s L-BFGS-B with a full gradient computation at each iteration, to avoid to tune the learning rate and provide stable learning. See the examples below and the docstring of for further information. NCA stores a matrix of pairwise distances, taking memory. Time complexity depends on the number of iterations done by the optimisation algorithm. However, one can set the maximum number of iterations with the argument . For each iteration, time complexity is . Here the operation returns \\(LX^T\\), therefore its time complexity equals . There is no added space complexity in the operation."
    },
    {
        "link": "https://datacamp.com/tutorial/k-nearest-neighbor-classification-scikit-learn",
        "document": "While kNN can be used for classification and regression, this article will focus on building a classification model. Classification in machine learning is a supervised learning task that involves predicting a categorical label for a given input data point. The algorithm is trained on a labeled dataset and uses the input features to learn the mapping between the inputs and the corresponding class labels. We can use the trained model to predict new, unseen data. You can also run the code for this tutorial by opening this DataLab workbook.\n\nThe kNN algorithm can be considered a voting system, where the majority class label determines the class label of a new data point among its nearest ‘k’ (where k is an integer) neighbors in the feature space. Imagine a small village with a few hundred residents, and you must decide which political party you should vote for. To do this, you might go to your nearest neighbors and ask which political party they support. If the majority of your’ k’ nearest neighbors support party A, then you would most likely also vote for party A. This is similar to how the kNN algorithm works, where the majority class label determines the class label of a new data point among its k nearest neighbors.\n\nLet's take a deeper look with another example. Imagine you have data about fruit, specifically grapes and pears. You have a score for how round the fruit is and the diameter. You decide to plot these on a graph. If someone hands you a new fruit, you could plot this on the graph too, then measure the distance to k (a number) nearest points to decide what fruit it is. In the example below, if we choose to measure three points, we can say the three nearest points are pears, so I’m 100% sure this is a pear. If we choose to measure the four nearest points, three are pears while one is a grape, so we would say we are 75% sure this is a pear. We’ll cover how to find the best value for k and the different ways to measure distance later in this article.\n\nTo further illustrate the kNN algorithm, let's work on a case study you may find while working as a data scientist. Let's assume you are a data scientist at an online retailer, and you have been tasked with detecting fraudulent transactions. The only features you have at this stage are:\n• : The distance between the user's home location and where the transaction was made.\n• : the ratio between the price of the item purchased in this transaction to the median purchase price of that user.\n\nThe data has 39 observations which are individual transactions. In this tutorial, we’ve been given the dataset the variable , it looks like this:\n\nTo fit and train this model, we’ll be following The Machine Learning Workflow infographic.\n\nHowever, as our data is pretty clean, we won’t carry out every step. We will do the following:\n\nLet’s start by visualizing our data using Matplotlib; we can plot our two features in a scatterplot.\n\nAs you can see, there is a clear difference between these transactions, with fraudulent transactions being of much higher value, compared to the customers' median order. The trends around distance from home are somewhat hard to interpret, with non-fraudulent transactions typically being closer to home but with several outliers.\n\nWhen training any machine learning model, it is important to split the data into training and test data. The training data is used to fit the model. The algorithm uses the training data to learn the relationship between the features and the target. It tries to find a pattern in the training data that can be used to make predictions on new, unseen data. The test data is used to evaluate the performance of the model. The model is tested on the test data by using it to make predictions and comparing these predictions to the actual target values.\n\nWhen training a kNN classifier, it's essential to normalize the features. This is because kNN measures the distance between points. The default is to use the Euclidean Distance, which is the square root of the sum of the squared differences between two points. In our case, purchase_price_ratio is between 0 and 8 while dist_from_home is much larger. If we didn’t normalize this, our calculation would be heavily weighted by dist_from_home because the numbers are bigger.\n\nWe should normalize the data after splitting it into training and test sets. This is to prevent ‘data leakage’ as the normalization would give the model additional information about the test set if we normalized all the data at once.\n\nThe following code splits the data into train/test splits, then normalizes using scikit-learn’s standard scaler. We first call .fit_transform() on the training data, which fits our scaler to the mean and standard deviation of the training data. We can then apply this to the test data by calling .transform(), which uses the previously learned values.\n\nWe are now ready to train the model. For this, we’ll use a fixed value of 3 for k, but we’ll need to optimize this later on. We first create an instance of the kNN model, then fit this to our training data. We pass both the features and the target variable, so the model can learn.\n\nThe model is now trained! We can make predictions on the test dataset, which we can use later to score the model.\n\nThe simplest way to evaluate this model is by using accuracy. We check the predictions against the actual values in the test set and count up how many the model got right.\n\nThis is a pretty good score! However, we may be able to do better by optimizing our value of k.\n\nUsing Cross Validation to Get the Best Value of k\n\nUnfortunately, there is no magic way to find the best value for k. We have to loop through many different values, then use our best judgment.\n\nIn the below code, we select a range of values for k and create an empty list to store our results. We use cross-validation to find the accuracy scores, which means we don’t need to create a training and test split, but we do need to scale our data. We then loop over the values and add the scores to our list.\n\nTo implement cross-validation, we use scikit-learn’s cross_val_score. We pass an instance of the kNN model, along with our data and a number of splits to make. In the code below, we use five splits which means the model with split the data into five equal-sized groups and use 4 to train and 1 to test the result. It will loop through each group and give an accuracy score, which we average to find the best model.\n\nWe can plot the results with the following code\n\nWe can see from our chart that k = 9, 10, 11, 12, and 13 all have an accuracy score of just under 95%. As these are tied for the best score, it is advisable to use a smaller value for k. This is because when using higher values of k, the model will use more data points that are further away from the original. Another option would be to explore other evaluation metrics.\n\nWe can now train our model using the best k value using the code below.\n\nthen evaluate with accuracy, precision, and recall (note your results may differ due to randomization)\n\nTake it to the Next Level\n• The Supervised Learning with scikit-learn course is the entry point to DataCamp's machine learning in Python curriculum and covers k-nearest neighbors.\n• The Anomaly Detection in Python, Dealing with Missing Data in Python, and Machine Learning for Finance in Python courses all show examples of using k-nearest neighbors.\n• The Decision Tree Classification in Python Tutorial covers another machine learning model for classifying data.\n• The scikit-learn cheat sheet provides a handy reference for popular machine learning functionality."
    },
    {
        "link": "https://digitalocean.com/community/tutorials/python-scikit-learn-tutorial",
        "document": "Scikit-learn is one of the most widely used Python libraries for machine learning. Whether you’re working on classification, regression, or clustering tasks, Scikit-learn provides simple and efficient tools to build and evaluate models.\n\nIt features several regression, classification, and clustering algorithms, including SVMs, gradient boosting, k-means, random forests, and DBSCAN. It is designed to work with Python Numpy and SciPy.\n\nWhat is Scikit-learn in Python?\n\nScikit Learn is written in Python (most of it), and some of its core algorithms are written in Cython for even better performance. Scikit-learn is used to build models and it is not recommended to use it for reading, manipulating and summarizing data as there are better frameworks available for the purpose. It is open source and released under BSD license.\n\nIt provides various tools for:\n\nScikit assumes you have a running Python 2.7 or above platform with NumPY (1.8.2 and above) and SciPY (0.13.3 and above) packages on your device. Once we have these packages installed we can proceed with the installation. For pip installation, run the following command in the terminal:\n\nIf you like , you can also use the conda for package installation, run the following command:\n\nOnce you are done with the installation, you can use scikit-learn easily in your Python code by importing it as:\n\nLet’s start with loading a dataset to play with. Let’s load a simple dataset named Iris. It is a dataset of a flower, it contains 150 observations about different measurements of the flower. Let’s see how to load the dataset using scikit-learn.\n\nWe are printing shape of data for ease, you can also print whole data if you wish so, running the codes gives an output like this:\n\nNow we have loaded data, let’s try learning from it and predict on new data. For this purpose we have to create an estimator and then call its fit method.\n\nHere is what we get when we run this script:\n\nCreating various models is rather simple using scikit-learn. Let’s start with a simple example of regression.\n\nRunning the model should return a point that can be plotted on the same line:\n\nLet’s try a simple classification algorithm. This classifier uses an algorithm based on ball trees to represent the training samples.\n\nLet’s run the classifier and check results, the classifier should return 0. Let’s try the example:\n\nThis is the simplest clustering algorithm. The set is divided into ‘k’ clusters and each observation is assigned to a cluster. This is done iteratively until the clusters converge. We will create one such clustering model in the following program:\n\nOn running the program we’ll see separate clusters in the list. Here is the output for above code snippet: .\n\nScikit-learn is ideal for traditional machine learning models, while TensorFlow and PyTorch excel in deep learning and large-scale AI applications.\n\nScikit-learn is a powerful library for machine learning, but it’s optimized for small to medium-sized datasets. When working with large datasets, you need to handle them efficiently. Here are some strategies:\n• Use : This method supports incremental learning for large datasets. It’s particularly useful when you can’t fit the entire dataset into memory at once.\n• Apply Feature Selection: Reducing the number of features in your dataset can significantly reduce memory usage and computation time.\n• Leverage for Parallel Processing: This library can be used to distribute tasks across multiple cores, which can greatly speed up your computations.\n\nHere’s an example of using :\n\n1. What is Scikit-learn used for?\n\nScikit-learn is used for traditional machine learning tasks such as classification, regression, clustering, and feature selection.\n\n2. How does Scikit-learn compare to TensorFlow and PyTorch?\n\nScikit-learn is better suited for small-scale, traditional machine learning tasks, while TensorFlow and PyTorch are designed for deep learning and large-scale computations.\n\nNo, Scikit-learn is not designed for deep learning. Instead, it integrates well with deep learning libraries when needed.\n\n5. How do I optimize model performance in Scikit-learn?\n\nOptimizing model performance is crucial to achieve the best results in machine learning. Here are some strategies to optimize model performance in Scikit-learn:\n\nHyperparameter Tuning: Use GridSearchCV to perform hyperparameter tuning. This involves searching for the best combination of hyperparameters that result in the best model performance.\n\nFeature Selection: Apply feature selection techniques to reduce the dimensionality of your dataset. This can help in reducing overfitting, improving model interpretability, and enhancing model performance.\n\nEnsemble Methods: Utilize ensemble methods like Random Forests and Gradient Boosting. These methods combine the predictions of multiple models to produce a more accurate and robust prediction model.\n\nIn this tutorial, you learned about the versatility of Scikit-Learn, which simplifies the implementation of various machine learning algorithms. We have delved into examples of Regression, Classification, and Clustering. Despite being in the development phase and maintained by volunteers, Scikit-Learn is widely popular in the community. We encourage you to experiment with your own examples.\n\nYou can also check out these tutorials:"
    },
    {
        "link": "https://activestate.com/resources/quick-reads/how-to-classify-data-in-python",
        "document": "Classification in supervised Machine Learning (ML) is the process of predicting the class or category of data based on predefined classes of data that have been ‘labeled’.\n• Labeled data is data that has already been classified\n• Unlabeled data is data that has not yet been labeled\n\nFor more information about labeled data, refer to: How to label data for machine learning in Python\n\nThere are two main types of classification:\n• Binary Classification – sorts data on the basis of discrete or non-continuous values (usually two values). For example, a medical test may sort patients into those that have a specific disease versus those that do not.\n• Multi-class Classification – sorts data into three or more classes. For example, medical profiling that sorts patients into those with kidney, liver, lung, or bladder infection symptoms.\n\nHow to Do Classification with Scikit-Learn\n\nYou can use scikit-learn to perform classification using any of its numerous classification algorithms (also known as classifiers), including:\n• Decision Tree/Random Forest – the Decision Tree classifier has dataset attributes classed as nodes or branches in a tree. The Random Forest classifier is a meta-estimator that fits a forest of decision trees and uses averages to improve prediction accuracy.\n• K-Nearest Neighbors (KNN) – a simple classification algorithm, where K refers to the square root of the number of training records.\n• Linear Discriminant Analysis – estimates the probability of a new set of inputs for every class.\n• Logistic Regression – a model with an input variable (x) and an output variable (y), which is a discrete value of either 1 (yes) or 0 (no).\n• Naive Bayes – a family of classifiers based on a simple Bayesian model that is comparatively fast and accurate. Bayesian theory explores the relationship between probability and possibility.\n• Support Vector Machines (SVMs) – a model with associated learning algorithms that analyze data for classification. Also known as Support-Vector Networks.\n\nFor more information about SciKit-Learn, as well as how to install it, refer to:\n• What is Scikit-learn in Python?\n\nHow to Run a Classification Task with K-Nearest Neighbour\n\nIn this example, the KNN classifier is used to train data and run classification tasks.\n\nWatch how to use KNN classifier to train and classify data:\n\nHow to Run a Classification Task with Naive Bayes\n\nIn this example, a Naive Bayes (NB) classifier is used to run classification tasks.\n\nThe main difference between classification and regression is that the output variable for classification is discrete, while the output for regression is continuous.\n\nFor information about regression, refer to: How to Run Linear Regression in Python Scikit-Learn"
    },
    {
        "link": "https://scikit-learn.org/stable/modules/preprocessing.html",
        "document": "The package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n\nIn general, many learning algorithms such as linear models benefit from standardization of the data set (see Importance of Feature Scaling). If some outliers are present in the set, robust scalers or other transformers can be more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in Compare the effect of different scalers on data with outliers.\n\nStandardization, or mean removal and variance scaling# Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation. For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) may assume that all features are centered around zero or have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. The module provides the utility class, which is a quick and easy way to perform the following operation on an array-like dataset: Scaled data has zero mean and unit variance: This class implements the API to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. This class is hence suitable for use in the early steps of a : It is possible to disable either centering or scaling by either passing or to the constructor of . An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using or , respectively. The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data. Here is an example to scale a toy data matrix to the range: The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data: It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data: If is given an explicit the full formula is: works in a very similar fashion, but scales in a way that the training data lies within the range by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data. Here is how to use the toy data from the previous example with this scaler: Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales. was specifically designed for scaling sparse data, and is the recommended way to go about this. However, can accept matrices as input, as long as is explicitly passed to the constructor. Otherwise a will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. cannot be fitted to sparse inputs, but you can use the method on sparse inputs. Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see and ). Any other sparse input will be converted to the Compressed Sparse Rows representation. To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream. Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the method of sparse matrices is another option. If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use as a drop-in replacement instead. It uses more robust estimates for the center and range of your data. Further discussion on the importance of centering and scaling data is available on this FAQ: Should I normalize/standardize/rescale the data? It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features. To address this issue you can use with to further remove the linear correlation across features. If you have a kernel matrix of a kernel \\(K\\) that computes a dot product in a feature space (possibly implicitly) defined by a function \\(\\phi(\\cdot)\\), a can transform the kernel matrix so that it contains inner products in the feature space defined by \\(\\phi\\) followed by the removal of the mean in that space. In other words, computes the centered Gram matrix associated to a positive semidefinite kernel \\(K\\). We can have a look at the mathematical formulation now that we have the intuition. Let \\(K\\) be a kernel matrix of shape computed from \\(X\\), a data matrix of shape , during the step. \\(K\\) is defined by \\(\\phi(X)\\) is a function mapping of \\(X\\) to a Hilbert space. A centered kernel \\(\\tilde{K}\\) is defined as: where \\(\\tilde{\\phi}(X)\\) results from centering \\(\\phi(X)\\) in the Hilbert space. Thus, one could compute \\(\\tilde{K}\\) by mapping \\(X\\) using the function \\(\\phi(\\cdot)\\) and center the data in this new space. However, kernels are often used because they allows some algebra calculations that avoid computing explicitly this mapping using \\(\\phi(\\cdot)\\). Indeed, one can implicitly center as shown in Appendix B in [Scholkopf1998]: \\(1_{\\text{n}_{samples}}\\) is a matrix of where all entries are equal to \\(\\frac{1}{\\text{n}_{samples}}\\). In the step, the kernel becomes \\(K_{test}(X, Y)\\) defined as: \\(Y\\) is the test dataset of shape and thus \\(K_{test}\\) is of shape . In this case, centering \\(K_{test}\\) is done as: \\(1'_{\\text{n}_{samples}}\\) is a matrix of shape where all entries are equal to \\(\\frac{1}{\\text{n}_{samples}}\\).\n\nTwo types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature. Quantile transforms put all features into the same desired distribution based on the formula \\(G^{-1}(F(X))\\) where \\(F\\) is the cumulative distribution function of the feature and \\(G^{-1}\\) the quantile function of the desired output distribution \\(G\\). This formula is using the two following facts: (i) if \\(X\\) is a random variable with a continuous cumulative distribution function \\(F\\) then \\(F(X)\\) is uniformly distributed on \\([0,1]\\); (ii) if \\(U\\) is a random variable with uniform distribution on \\([0,1]\\) then \\(G^{-1}(U)\\) has distribution \\(G\\). By performing a rank transformation, a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features. Power transforms are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution. provides a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1: This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined: This can be confirmed on a independent testing set with similar remarks: In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness. currently provides two such power transformations, the Yeo-Johnson transform and the Box-Cox transform. Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by \\(\\lambda\\), which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution: While the above example sets the option to , will apply zero-mean, unit-variance normalization to the transformed output by default. Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation. It is also possible to map data to a normal distribution using by setting . Using the earlier example with the iris dataset: Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input’s minimum and maximum — corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively — do not become infinite under the transformation.\n\nNormalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. This assumption is the base of the Vector Space Model often used in text classification and clustering contexts. The function provides a quick and easy way to perform this operation on a single array-like dataset, either using the , , or norms: The module further provides a utility class that implements the same operation using the API (even though the method is useless in this case: the class is stateless as this operation treats samples independently). This class is hence suitable for use in the early steps of a : The normalizer instance can then be used on sample vectors as any transformer: Note: L2 normalization is also known as spatial sign preprocessing. and accept both dense array-like and sparse matrices from scipy.sparse as input. For sparse input the data is converted to the Compressed Sparse Rows representation (see ) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.\n\nDiscretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models. For more advanced possibilities, in particular smooth ones, see Generating polynomial features further below. By default the output is one-hot encoded into a sparse matrix (See Encoding categorical features) and this can be configured with the parameter. For each feature, the bin edges are computed during and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as: Based on these bin intervals, is transformed as follows: The resulting dataset contains ordinal attributes which can be further used in a . Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins. implements different binning strategies, which can be selected with the parameter. The ‘uniform’ strategy uses constant-width bins. The ‘quantile’ strategy uses the quantiles values to have equally populated bins in each feature. The ‘kmeans’ strategy defines bins based on a k-means clustering procedure performed on each feature independently. Be aware that one can specify custom bins by passing a callable defining the discretization strategy to . For instance, we can use the Pandas function :\n• None Demonstrating the different strategies of KBinsDiscretizer Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the . It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice. As for the , the utility class is meant to be used in the early stages of . The method does nothing as each sample is treated independently of others: It is possible to adjust the threshold of the binarizer: As for the class, the preprocessing module provides a companion function to be used when the transformer API is not necessary. Note that the is similar to the when , and when the bin edge is at the value . and accept both dense array-like and sparse matrices from scipy.sparse as input. For sparse input the data is converted to the Compressed Sparse Rows representation (see ). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.\n\nOften it’s useful to add complexity to a model by considering nonlinear features of the input data. We show two possibilities that are both based on polynomials: The first one uses pure polynomials, the second one uses splines, i.e. piecewise polynomials. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented in : The features of X have been transformed from \\((X_1, X_2)\\) to \\((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\\). In some cases, only interaction terms among features are required, and it can be gotten with the setting : The features of X have been transformed from \\((X_1, X_2, X_3)\\) to \\((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\\). Note that polynomial features are used implicitly in kernel methods (e.g., , ) when using polynomial Kernel functions. See Polynomial and Spline interpolation for Ridge regression using created polynomial features. Another way to add nonlinear terms instead of pure polynomials of features is to generate spline basis functions for each feature with the . Splines are piecewise polynomials, parametrized by their polynomial degree and the positions of the knots. The implements a B-spline basis, cf. the references below. The treats each feature separately, i.e. it won’t give you interaction terms. Some of the advantages of splines over polynomials are:\n• None B-splines are very flexible and robust if you keep a fixed low degree, usually 3, and parsimoniously adapt the number of knots. Polynomials would need a higher degree, which leads to the next point.\n• None B-splines do not have oscillatory behaviour at the boundaries as have polynomials (the higher the degree, the worse). This is known as Runge’s phenomenon.\n• None B-splines provide good options for extrapolation beyond the boundaries, i.e. beyond the range of fitted values. Have a look at the option .\n• None B-splines generate a feature matrix with a banded structure. For a single feature, every row contains only non-zero elements, which occur consecutively and are even positive. This results in a matrix with good numerical properties, e.g. a low condition number, in sharp contrast to a matrix of polynomials, which goes under the name Vandermonde matrix. A low condition number is important for stable algorithms of linear models. The following code snippet shows splines in action: As the is sorted, one can easily see the banded matrix output. Only the three middle diagonals are non-zero for . The higher the degree, the more overlapping of the splines. Interestingly, a of is the same as with and if .\n• None Eilers, P., & Marx, B. (1996). Flexible Smoothing with B-splines and Penalties. Statist. Sci. 11 (1996), no. 2, 89–121.\n• None Perperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. A review of spline function procedures in R. BMC Med Res Methodol 19, 46 (2019)."
    },
    {
        "link": "https://scikit-learn.org/stable/common_pitfalls.html",
        "document": "The purpose of this chapter is to illustrate some common pitfalls and anti-patterns that occur when using scikit-learn. It provides examples of what not to do, along with a corresponding correct example.\n\nscikit-learn provides a library of Dataset transformations, which may clean (see Preprocessing data), reduce (see Unsupervised dimensionality reduction), expand (see Kernel Approximation) or generate (see Feature extraction) feature representations. If these data transforms are used when training a model, they also must be used on subsequent datasets, whether it’s test data or data in a production system. Otherwise, the feature space will change, and the model will not be able to perform effectively. For the following example, let’s create a synthetic dataset with a single feature: The train dataset is scaled, but not the test dataset, so model performance on the test dataset is worse than expected: Instead of passing the non-transformed to , we should transform the test data, the same way we transformed the training data: Alternatively, we recommend using a , which makes it easier to chain transformations with estimators, and reduces the possibility of forgetting a transformation: Pipelines also help avoiding another common pitfall: leaking the test data into the training data.\n\nData leakage occurs when information that would not be available at prediction time is used when building the model. This results in overly optimistic performance estimates, for example from cross-validation, and thus poorer performance when the model is used on actually novel data, for example during production. A common cause is not keeping the test and train data subsets separate. Test data should never be used to make choices about the model. The general rule is to never call on the test data. While this may sound obvious, this is easy to miss in some cases, for example when applying certain pre-processing steps. Although both train and test data subsets should receive the same preprocessing transformation (as described in the previous section), it is important that these transformations are only learnt from the training data. For example, if you have a normalization step where you divide by the average value, the average should be the average of the train subset, not the average of all the data. If the test subset is included in the average calculation, information from the test subset is influencing the model. Below are some tips on avoiding data leakage:\n• None Always split the data into train and test subsets first, particularly before any preprocessing steps.\n• None Never include test data when using the and methods. Using all the data, e.g., , can result in overly optimistic scores. Conversely, the method should be used on both train and test subsets as the same preprocessing should be applied to all the data. This can be achieved by using on the train subset and on the test subset.\n• None The scikit-learn pipeline is a great way to prevent data leakage as it ensures that the appropriate method is performed on the correct data subset. The pipeline is ideal for use in cross-validation and hyper-parameter tuning functions. An example of data leakage during preprocessing is detailed below. We here choose to illustrate data leakage with a feature selection step. This risk of leakage is however relevant with almost all transformations in scikit-learn, including (but not limited to) , , and . A number of Feature selection functions are available in scikit-learn. They can help remove irrelevant, redundant and noisy features as well as improve your model build time and performance. As with any other type of preprocessing, feature selection should only use the training data. Including the test data in feature selection will optimistically bias your model. To demonstrate we will create this binary classification problem with 10,000 randomly generated features: Using all the data to perform feature selection results in an accuracy score much higher than chance, even though our targets are completely random. This randomness means that our and are independent and we thus expect the accuracy to be around 0.5. However, since the feature selection step ‘sees’ the test data, the model has an unfair advantage. In the incorrect example below we first use all the data for feature selection and then split the data into training and test subsets for model fitting. The result is a much higher than expected accuracy score: To prevent data leakage, it is good practice to split your data into train and test subsets first. Feature selection can then be formed using just the train dataset. Notice that whenever we use or , we only use the train dataset. The score is now what we would expect for the data, close to chance: Here again, we recommend using a to chain together the feature selection and model estimators. The pipeline ensures that only the training data is used when performing and the test data is used only for calculating the accuracy score: The pipeline can also be fed into a cross-validation function such as . Again, the pipeline ensures that the correct data subset and estimator method is used during fitting and predicting:\n\nSome scikit-learn objects are inherently random. These are usually estimators (e.g. ) and cross-validation splitters (e.g. ). The randomness of these objects is controlled via their parameter, as described in the Glossary. This section expands on the glossary entry, and describes good practices and common pitfalls w.r.t. this subtle parameter. For an optimal robustness of cross-validation (CV) results, pass instances when creating estimators, or leave to . Passing integers to CV splitters is usually the safest option and is preferable; passing instances to splitters may sometimes be useful to achieve very specific use-cases. For both estimators and splitters, passing an integer vs passing an instance (or ) leads to subtle but significant differences, especially for CV procedures. These differences are important to understand when reporting results. For reproducible results across executions, remove any use of . Using or instances, and repeated calls to and # The parameter determines whether multiple calls to fit (for estimators) or to split (for CV splitters) will produce the same results, according to these rules:\n• None If an integer is passed, calling or multiple times always yields the same results.\n• None If or a instance is passed: and will yield different results each time they are called, and the succession of calls explores all sources of entropy. is the default value for all parameters. We here illustrate these rules for both estimators and CV splitters. Since passing is equivalent to passing the global instance from ( ), we will not explicitly mention here. Everything that applies to instances also applies to using . Passing instances means that calling multiple times will not yield the same results, even if the estimator is fitted on the same data and with the same hyper-parameters: We can see from the snippet above that repeatedly calling has produced different models, even if the data was the same. This is because the Random Number Generator (RNG) of the estimator is consumed (i.e. mutated) when is called, and this mutated RNG will be used in the subsequent calls to . In addition, the object is shared across all objects that use it, and as a consequence, these objects become somewhat inter-dependent. For example, two estimators that share the same instance will influence each other, as we will see later when we discuss cloning. This point is important to keep in mind when debugging. If we had passed an integer to the parameter of the , we would have obtained the same models, and thus the same scores each time. When we pass an integer, the same RNG is used across all calls to . What internally happens is that even though the RNG is consumed when is called, it is always reset to its original state at the beginning of . Randomized CV splitters have a similar behavior when a instance is passed; calling multiple times yields different data splits: We can see that the splits are different from the second time is called. This may lead to unexpected results if you compare the performance of multiple estimators by calling many times, as we will see in the next section. While the rules that govern the parameter are seemingly simple, they do however have some subtle implications. In some cases, this can even lead to wrong conclusions. Different `random_state` types lead to different cross-validation procedures Depending on the type of the parameter, estimators will behave differently, especially in cross-validation procedures. Consider the following snippet: We see that the cross-validated scores of and are different, as should be expected since we didn’t pass the same parameter. However, the difference between these scores is more subtle than it looks, and the cross-validation procedures that were performed by significantly differ in each case:\n• None Since was passed an integer, every call to uses the same RNG: this means that all random characteristics of the random forest estimator will be the same for each of the 5 folds of the CV procedure. In particular, the (randomly chosen) subset of features of the estimator will be the same across all folds.\n• None Since was passed a instance, each call to starts from a different RNG. As a result, the random subset of features will be different for each folds. While having a constant estimator RNG across folds isn’t inherently wrong, we usually want CV results that are robust w.r.t. the estimator’s randomness. As a result, passing an instance instead of an integer may be preferable, since it will allow the estimator RNG to vary for each fold. Here, will use a non-randomized CV splitter (as is the default), so both estimators will be evaluated on the same splits. This section is not about variability in the splits. Also, whether we pass an integer or an instance to isn’t relevant for our illustration purpose: what matters is what we pass to the estimator. Another subtle side effect of passing instances is how will work: Since a instance was passed to , and are not clones in the strict sense, but rather clones in the statistical sense: and will still be different models, even when calling on the same data. Moreover, and will influence each-other since they share the same internal RNG: calling will consume ’s RNG, and calling will consume ’s RNG, since they are the same. This bit is true for any estimators that share a parameter; it is not specific to clones. If an integer were passed, and would be exact clones and they would not influence each other. Even though is rarely used in user code, it is called pervasively throughout scikit-learn codebase: in particular, most meta-estimators that accept non-fitted estimators call internally ( , , , etc.). When passed a instance, CV splitters yield different splits each time is called. When comparing different estimators, this can lead to overestimating the variance of the difference in performance between the estimators: Directly comparing the performance of the estimator vs the estimator on each fold would be a mistake: the splits on which the estimators are evaluated are different. Indeed, will internally call on the same instance, but the splits will be different each time. This is also true for any tool that performs model selection via cross-validation, e.g. and : scores are not comparable fold-to-fold across different calls to , since would have been called multiple times. Within a single call to , however, fold-to-fold comparison is possible since the search estimator only calls once. For comparable fold-to-fold results in all scenarios, one should pass an integer to the CV splitter: . While fold-to-fold comparison is not advisable with instances, one can however expect that average scores allow to conclude whether one estimator is better than another, as long as enough folds and data are used. What matters in this example is what was passed to . Whether we pass a instance or an integer to is not relevant for our illustration purpose. Also, neither nor are randomized estimators. In order to obtain reproducible (i.e. constant) results across multiple program executions, we need to remove all uses of , which is the default. The recommended way is to declare a variable at the top of the program, and pass it down to any object that accepts a parameter: We are now guaranteed that the result of this script will always be 0.84, no matter how many times we run it. Changing the global variable to a different value should affect the results, as expected. It is also possible to declare the variable as an integer. This may however lead to less robust cross-validation results, as we will see in the next section. We do not recommend setting the global seed by calling . See here for a discussion. When we evaluate a randomized estimator performance by cross-validation, we want to make sure that the estimator can yield accurate predictions for new data, but we also want to make sure that the estimator is robust w.r.t. its random initialization. For example, we would like the random weights initialization of a to be consistently good across all folds: otherwise, when we train that estimator on new data, we might get unlucky and the random initialization may lead to bad performance. Similarly, we want a random forest to be robust w.r.t the set of randomly selected features that each tree will be using. For these reasons, it is preferable to evaluate the cross-validation performance by letting the estimator use a different RNG on each fold. This is done by passing a instance (or ) to the estimator initialization. When we pass an integer, the estimator will use the same RNG on each fold: if the estimator performs well (or bad), as evaluated by CV, it might just be because we got lucky (or unlucky) with that specific seed. Passing instances leads to more robust CV results, and makes the comparison between various algorithms fairer. It also helps limiting the temptation to treat the estimator’s RNG as a hyper-parameter that can be tuned. Whether we pass instances or integers to CV splitters has no impact on robustness, as long as is only called once. When is called multiple times, fold-to-fold comparison isn’t possible anymore. As a result, passing integer to CV splitters is usually safer and covers most use-cases."
    },
    {
        "link": "https://geeksforgeeks.org/data-preprocessing-machine-learning-python",
        "document": "Data preprocessing is a important step in the data science transforming raw data into a clean structured format for analysis. It involves tasks like handling missing values, normalizing data and encoding variables. Mastering preprocessing in Python ensures reliable insights for accurate predictions and effective decision-making. Pre-processing refers to the transformations applied to data before feeding it to the algorithm.\n\nYou can download dataset from here.\n\nAs we can see from the above info that the our dataset has 9 columns and each columns has 768 values. There is no Null values in the dataset.\n\nWe can also check the null values using df.isnull()\n\nIn statistical analysis we use df.describe() which will give a descriptive overview of the dataset.\n\nThe above table shows the count, mean, standard deviation, min, 25%, 50%, 75% and max values for each column. When we carefully observe the table we will find that Insulin, Pregnancies, BMI, BloodPressure columns has outliers.\n\nLet’s plot the boxplot for each column for easy understanding.\n\nfrom the above boxplot we can clearly see that every column has some amounts of outliers.\n\nWe can also compare by single columns in descending order\n• None works well when the features have different scales and the algorithm being used is sensitive to the scale of the features, such as k-nearest neighbors or neural networks.\n• None Rescale your data using scikit-learn using the\n• None MinMaxScaler scales the data so that each feature is in the range [0, 1].\n• None is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.\n• None We can standardize data using scikit-learn with the StandardScaler class.\n• None It works well when the features have a normal distribution or when the algorithm being used is not sensitive to the scale of the features\n\nIn conclusion data preprocessing is an important step to make raw data clean for analysis. Using Python we can handle missing values, organize data and prepare it for accurate results. This ensures our model is reliable and helps us uncover valuable insights from data."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn",
        "document": "Practical Guide on Data Preprocessing in Python using Scikit Learn\n\nThis article primarily focuses on data pre-processing techniques in python. Learning algorithms have affinity towards certain data types on which they perform incredibly well. They are also known to give reckless predictions with unscaled or unstandardized features. Algorithm like XGBoost, specifically requires dummy encoded data while algorithm like decision tree doesn’t seem to care at all (sometimes)!\n\nIn simple words, pre-processing refers to the transformations applied to your data before feeding it to the algorithm. In python, scikit-learn library has a pre-built functionality under sklearn.preprocessing. There are many more options for pre-processing which we’ll explore.\n\nAfter finishing this article, you will be equipped with the basic techniques of data pre-processing and their in-depth understanding. For your convenience, I’ve attached some resources for in-depth learning of machine learning algorithms and designed few exercises to get a good grip of the concepts.\n\nFor this article, I have used a subset of the Loan Prediction (missing value observations are dropped) data set from You can download the final training and testing data set from here: Download Data\n\nNote : Testing data that you are provided is the subset of the training data from Loan Prediction problem.\n\nNow, lets get started by importing important packages and the data set.\n\nLets take a closer look at our data set.\n\nFeature scaling is the method to limit the range of variables so that they can be compared on common grounds. It is performed on continuous variables. Lets plot the distribution of all the continuous variables in the data set.\n\nAfter understanding these plots, we infer that and are in similar range (0-50000$) where as is in thousands and it ranges from 0 to 600$. The story for is completely different from other variables because its unit is months as opposed to other variables where the unit is dollars.\n\nIf we try to apply distance based methods such as kNN on these features, feature with the largest range will dominate the outcome results and we’ll obtain less accurate predictions. We can overcome this trouble using feature scaling. Let’s do it practically.\n\nResources : Check out this article on kNN for better understanding.\n\nWow !! we got an accuracy of 63% just by guessing, What is the meaning of this, getting better accuracy than our prediction model ?\n\nThis might be happening because of some insignificant variable with larger range will be dominating the objective function. We can remove this problem by scaling down all the features to a same range. sklearn provides a tool that will scale down all the features between 0 and 1. Mathematical formula for is.\n\nLets try this tool on our problem.\n\nNow, that we are done with scaling, lets apply kNN on our scaled data and check its accuracy.\n\nGreat !! Our accuracy has increased from 61% to 75%. This means that some of the features with larger range were dominating the prediction outcome in the domain of distance based methods(kNN).\n\nIt should be kept in mind while performing distance based methods we must attempt to scale the data, so that the feature with lesser significance might not end up dominating the objective function due to its larger range. In addition, features having different unit should also be scaled thus providing each feature equal initial weightage and at the end we will have a better prediction model.\n\nTry to do the same exercise with a logistic regression model(parameters : penalty=’l2′,C=0.01) and provide your accuracy before and after scaling in the comment section.\n\nBefore jumping to this section I suggest you to complete Exercise 1.\n\nIn the previous section, we worked on the Loan_Prediction data set and fitted a kNN learner on the data set. After scaling down the data, we have got an accuracy of 75% which is very considerably good. I tried the same exercise on Logistic Regression and I got the following result :\n\nThe accuracy we got after scaling is close to the prediction which we made by guessing, which is not a very impressive achievement. So, what is happening here? Why hasn’t the accuracy increased by a satisfactory amount as it increased in kNN?\n\nResources : Go through this article on Logistic Regression for better understanding.\n\nHere is the answer:\n\nIn logistic regression, each feature is assigned a weight or coefficient (Wi). If there is a feature with relatively large range and it is insignificant in the objective function then logistic regression will itself assign a very low value to its co-efficient, thus neutralizing the dominant effect of that particular feature, whereas distance based method such as kNN does not have this inbuilt strategy, thus it requires scaling.\n\nAren’t we forgetting something ? Our logistic model is still predicting with an accuracy almost closer to a guess.\n\nNow, I’ll be introducing a new concept here called standardization. Many machine learning algorithms in sklearn requires standardized data which means having zero mean and unit variance.\n\nStandardization (or Z-score normalization) is the process where the features are rescaled so that they’ll have the properties of a standard normal distribution with μ=0 and σ=1, where μ is the mean (average) and σ is the standard deviation from the mean. Standard scores (also called z scores) of the samples are calculated as follows :\n\nElements such as l1 ,l2 regularizer in linear models (logistic comes under this category) and RBF kernel in SVM in objective function of learners assumes that all the features are centered around zero and have variance in the same order.\n\nFeatures having larger order of variance would dominate on the objective function as it happened in the previous section with the feature having large range. As we saw in the Exercise 1 that without any preprocessing on the data the accuracy was 61%, lets standardize our data apply logistic regression on that. Sklearn provides to standardize the data.\n\nWe again reached to our maximum score that was attained using kNN after scaling. This means standardizing the data when using a estimator having l1 or l2 regularization helps us to increase the accuracy of the prediction model. Other learners like kNN with euclidean distance measure, k-means, SVM, perceptron, neural networks, linear discriminant analysis, principal component analysis may perform better with standardized data.\n\nThough, I suggest you to understand your data and what kind of algorithm you are going to apply on it; over the time you will be able to judge weather to standardize your data or not.\n\nNote : Choosing between scaling and standardizing is a confusing choice, you have to dive deeper in your data and learner that you are going to use to reach the decision. For starters, you can try both the methods and check cross validation score for making a choice.\n\nResources : Go through this article on cross validation for better understanding.\n\nTry to do the same exercise with SVM model and provide your accuracy before and after standardization in the comment section.\n\nResources : Go through this article on support vector machines for better understanding.\n\nIn previous sections, we did the pre-processing for continuous numeric features. But, our data set has other features too such as , , , and . All these categorical features have string values. For example, has two levels either or . Lets feed the features in our logistic regression model.\n\nWe got an error saying that it cannot convert string to float. So, what’s actually happening here is learners like logistic regression, distance based methods such as kNN, support vector machines, tree based methods etc. in sklearn needs numeric arrays. Features having string values cannot be handled by these learners.\n\nSklearn provides a very efficient tool for encoding the levels of a categorical features into numeric values. encode labels with value between 0 and n_classes-1.\n\nAll our categorical features are encoded. You can look at your updated data set using . We are going to take a look at frequency distribution before and after the encoding.\n\nNow that we are done with label encoding, lets now run a logistic regression model on the data set with both categorical and continuous features.\n\nIts working now. But, the accuracy is still the same as we got with logistic regression after standardization from numeric features. This means categorical features we added are not very significant in our objective function.\n\nTry out decision tree classifier with all the features as independent variables and comment your accuracy.\n\nResources : Go through this article on decision trees for better understanding.\n\nOne-Hot Encoding transforms each categorical feature with n possible values into n binary features, with only one active.\n\nMost of the ML algorithms either learn a single weight for each feature or it computes distance between the samples. Algorithms like linear models (such as logistic regression) belongs to the first category.\n\nLets take a look at an example from loan_prediction data set. Feature have 4 possible values 0,1,2 and 3+ which are then encoded without loss of generality to 0,1,2 and 3.\n\nWe, then have a weight “W” assigned for this feature in a linear classifier,which will make a decision based on the constraints or eqivalently .\n\nPossible values that can be attained by the equation are 0, W, 2W and 3W. A problem with this equation is that the weight “W” cannot make decision based on four choices. It can reach to a decision in following ways:\n• All leads to the same decision (all of them <K or vice versa)\n• 3:1 division of the levels (Decision boundary at f(w)>2W)\n• 2:2 division of the levels (Decision boundary at f(w)>W)\n\nHere we can see that we are loosing many different possible decisions such as the case where “0” and “2W” should be given same label and “3W” and “W” are odd one out.\n\nThis problem can be solved by One-Hot-Encoding as it effectively changes the dimensionality of the feature “Dependents” from one to four, thus every value in the feature “Dependents” will have their own weights. Updated equation for the decison would be .\n\nwhere, All four new variable has boolean values (0 or 1).\n\nThe same thing happens with distance based methods such as kNN. Without encoding, distance between “0” and “1” values of is 1 whereas distance between “0” and “3+” will be 3, which is not desirable as both the distances should be similar. After encoding, the values will be new features (sequence of columns is 0,1,2,3+) : [1,0,0,0] and [0,0,0,1] (initially we were finding distance between “0” and “3+”), now the distance would be √2.\n\nFor tree based methods, same situation (more than two values in a feature) might effect the outcome to extent but if methods like random forests are deep enough, it can handle the categorical variables without one-hot encoding.\n\nNow, lets take look at the implementation of one-hot encoding with various algorithms.\n\nNow we are going to encode the data.\n\nHere, again we got the maximum accuracy as 0.75 that we have gotten so far. In this case, logistic regression regularization(C) parameter 1 where as earlier we used C=0.01.\n\nThe aim of this article is to familiarize you with the basic data pre-processing techniques and have a deeper understanding of the situations of where to apply those techniques.\n\nThese methods work because of the underlying assumptions of the algorithms. This is by no means an exhaustive list of the methods. I’d encourage you to experiment with these methods since they can be heavily modified according to the problem at hand.\n\nFor a more comprehensive guide on data preprocessing, check out our course, “How to Preprocess Data.“\n\nI plan to provide more advance techniques of data pre-processing such as pipeline and noise reduction in my next post, so stay tuned to dive deeper into the data pre-processing.\n\nDid you like reading this article ? Do you follow a different approach / package / library to perform these talks. I’d love to interact with you in comments.\n\nYou can test your skills and knowledge. Check out Live Competitions and compete with best Data Scientists from all over the world."
    },
    {
        "link": "https://labex.io/tutorials/ml-preprocessing-techniques-in-scikit-learn-71130",
        "document": "In this lab, we will explore the preprocessing techniques available in scikit-learn. Preprocessing is an essential step in any machine learning workflow as it helps to transform raw data into a suitable format for the learning algorithm. We will cover various preprocessing techniques such as standardization, scaling, normalization, encoding categorical features, imputing missing values, generating polynomial features, and creating custom transformers.\n\nAfter the VM startup is done, click the top left corner to switch to the Notebook tab to access Jupyter Notebook for practice.\n\nSometimes, you may need to wait a few seconds for Jupyter Notebook to finish loading. The validation of operations cannot be automated because of limitations in Jupyter Notebook.\n\nIf you face issues during learning, feel free to ask Labby. Provide feedback after the session, and we will promptly resolve the problem for you.\n\n%%%%{init: {'theme':'neutral'}}%%%% flowchart RL sklearn((\"Sklearn\")) -.-> sklearn/DataPreprocessingandFeatureEngineeringGroup([\"Data Preprocessing and Feature Engineering\"]) ml((\"Machine Learning\")) -.-> ml/FrameworkandSoftwareGroup([\"Framework and Software\"]) sklearn/DataPreprocessingandFeatureEngineeringGroup -.-> sklearn/preprocessing(\"Preprocessing and Normalization\") sklearn/DataPreprocessingandFeatureEngineeringGroup -.-> sklearn/impute(\"Impute\") ml/FrameworkandSoftwareGroup -.-> ml/sklearn(\"scikit-learn\") subgraph Lab Skills sklearn/preprocessing -.-> lab-71130{{\"Preprocessing Techniques in Scikit-Learn\"}} sklearn/impute -.-> lab-71130{{\"Preprocessing Techniques in Scikit-Learn\"}} ml/sklearn -.-> lab-71130{{\"Preprocessing Techniques in Scikit-Learn\"}} end"
    },
    {
        "link": "https://geeksforgeeks.org/k-nearest-neighbor-algorithm-in-python",
        "document": "K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning method. It operates for classification as well as regression:\n• Classification : For a new data point, the algorithm identifies its nearest neighbors based on a distance metric (e.g., Euclidean distance). The predicted class is determined by the majority class among these neighbors.\n• Regression : The algorithm predicts the value for a new data point by averaging the values of its nearest neighbors.\n\nQuick Revision : It works by identifying the ‘k’ nearest data points (neighbors) to a given input and predicting its class or value based on the majority class or the average of its neighbors. In this article, we will explore the concept of the KNN algorithm and demonstrate its implementation using Python’s Scikit-Learn library.\n\nChoosing the optimal k-value is critical before building the model for balancing the model’s performance.\n• smaller k value makes the model sensitive to noise, leading to overfitting (complex models).\n• larger k value results in smoother boundaries, reducing model complexity but possibly underfitting.\n\nIn the example shown above following steps are performed:\n• None The k-nearest neighbor algorithm is imported from the scikit-learn package.\n• None Train or fit the data into the model.\n\nWe have seen how we can use K-NN algorithm to solve the supervised machine learning problem. But how to measure the accuracy of the model?\n\nConsider an example shown below where we predicted the performance of the above model:\n\n\n\nModel Accuracy: So far so good. But how to decide the right k-value for the dataset?\n\nObviously, we need to be familiar to data to get the range of expected k-value, but to get the exact k-value we need to test the model for each and every expected k-value. Refer to the example shown below.\n\n\n\nHere in the example shown above, we are creating a plot to see the k-value for which we have high accuracy.\n\nNote: This is a technique which is not used industry-wide to choose the correct value of n_neighbors. Instead, we do hyperparameter tuning to choose the value that gives the best performance. We will be covering this in future posts.\n\nSummary – \n\nIn this post, we have understood what supervised learning is and what are its categories. After having a basic understanding of Supervised learning we explored the k-nearest neighbor algorithm which is used to solve supervised machine learning problems. We also explored measuring the accuracy of the model."
    },
    {
        "link": "https://arize.com/blog-course/knn-algorithm-k-nearest-neighbor",
        "document": "The K Nearest Neighbor (KNN) algorithm is a simple, non-parametric machine learning algorithm used for both classification and regression tasks. It is based on the assumption that similar items are close to each other in a feature space. KNN works by finding the k-nearest neighbors to a given query point, and predicting the class or value of the query point based on the classes or values of its neighbors.\n\nThis blog post aims to provide a technical overview of the KNN algorithm, and to explain its practical implementation, applications, and limitations. We will also cover some techniques to improve the performance of the KNN algorithm.\n\nWhat is the K Nearest Neighbor Algorithm?\n\nThe KNN algorithm is a type of instance-based learning, or lazy learning. It involves storing all available cases and classifying new cases based on a similarity measure (e.g., distance functions). The basic idea of KNN is to find the k-nearest neighbors to a given query point and use their class labels (in the case of classification) or their values (in the case of regression) to make a prediction for the query point.\n\nKNN works in three main steps: (1) calculating the distance between the query point and each training point, (2) selecting the k-nearest neighbors to the query point, and (3) predicting the class or value of the query point based on the majority class or the mean value of the neighbors, respectively. The choice of the distance metric and the value of k are important parameters in the KNN algorithm.\n\nThere are many different distance metrics you can choose from, with the most common being Euclidean distance. Other possible distance metrics include Manhattan distance, Minkowski distance, Hamming distance, and Cosine distance. These distance metrics define the decision boundaries for the corresponding partitions of data. The “right” distance metric to choose depends on the problem at hand.\n\nWith respect to choosing the appropriate value of k, this will largely depend on the input dataset. The k value determines the number of data points that the algorithm considers when predicting the value/label of a new data point. A larger value of k considers more data points, resulting in a smoother decision boundary, but may lead to underfitting. A smaller value of k considers fewer data points, resulting in a more complex decision boundary and may lead to overfitting. We dive deeper into choosing the value of k in the section on implementation KNN algorithm below.\n\nThe KNN algorithm can be used in image classification tasks by representing each image as a feature vector and then applyig the KNN algorithm to the set of feature vectors.\n\nOne common approach is to use the pixel values of an image as the features. For example, an RGB image with dimensions 100×100 pixels can be represented as a feature vector of length 30,000 (100x100x3), where each element of the vector corresponds to the pixel value of the corresponding RGB channel.\n\nOnce the feature vectors are extracted from the images, the KNN algorithm can be used to classify new images by finding the k nearest neighbors of the new image’s feature vector in the training set and assigning the new image to the majority class among its neighbors.\n\nHowever, using pixel values as features can result in high-dimensional feature vectors that are computationally expensive to process. To address this issue, techniques like Principal Component Analysis (PCA) and Convolutional Neural Networks (CNNs) can be used to reduce the dimensionality of the feature vectors and extract more meaningful features that can improve the accuracy of the KNN algorithm.\n\nThe KNN algorithm can be used in sentiment classification tasks by representing each text document as a feature vector, and then applying the KNN algorithm to the set of feature vectors. The process of transforming text (i.e., unstructured data) into vector representations is known as tokenization (check out our blog post on tokenization here to read more).\n\nOne common approach is to represent each document as a bag-of-words model, where each feature corresponds to a unique word in the vocabulary, and the value of each feature is the frequency of the corresponding word in the document. For example, consider the following two documents:\n\nDocument 2: “The movie was terrible and I do not recommend it.”\n\nThe vocabulary for these documents might be: [“the”, “movie”, “was”, “excellent”, “and”, “i”, “highly”, “recommend”, “terrible”, “do”, “not”]. The feature vectors for these documents would then be:\n\nOnce the feature vectors are extracted from the text documents, the KNN algorithm can be used to classify new documents by finding the k nearest neighbors of the new document’s feature vector in the training set and assigning the new document to the majority class among its neighbors.\n\nHowever, similarly to using pixel values as features for image classification, using the bag-of-words model as features can result in high-dimensional feature vectors that are computationally expensive to process. To address this issue, techniques like Term Frequency-Inverse Document Frequency (TF-IDF) weighting and word embeddings can be used to represent the documents as lower-dimensional feature vectors that can improve the accuracy of the KNN algorithm.\n\nBefore applying KNN, it is essential to preprocess the data to remove any noise, outliers, and missing values. This can involve techniques such as scaling or normalizing the data, imputing missing values, or reducing the dimensionality of the data.\n\nScaling the data is an important preprocessing step for the KNN algorithm because the algorithm is distance-based. If the data is not scaled, features with larger scales will dominate the distance calculation and can result in incorrect classifications. For example, consider a dataset with two features, one of which is age (ranging from 0 to 100) and the other is income (ranging from 0 to 1,000,000). If the data is not scaled, the distance calculation will be dominated by the income feature, and the age feature will have a much smaller impact on the classification decision. This can lead to inaccurate classifications, especially if the features have very different scales. Scaling the data ensures that some features don’t dominate others just because of the magnitude of the values.\n\nScaling the features can improve the accuracy of the KNN algorithm and ensure that it’s robust to changes in the scale of the data. Standardization is one common scaling technique that is often used for KNN, as it transforms the data to have a mean of 0 and a standard deviation of 1, which can help to mitigate the effects of differences in scale between features.\n\nAfter scaling the data, we need to choose our hyperparameters (i.e., the value of k). Hyperparameters refer to parameters that we choose for the algorithm, or in other words, the parameters that aren’t learned during training. Choosing the right value of k is critical for the performance of the KNN algorithm. If k is too small, the algorithm may be overly sensitive to noise in the data, which can lead to overfitting. On the other hand, if k is too large, the algorithm may oversimplify the decision boundaries and fail to capture important patterns in the data, which can lead to underfitting.\n\nThe best value of k depends on the specific dataset and problem that we are working with. Generally, smaller values of k are better suited for datasets with a lot of noise or with complex decision boundaries, while larger values of k are better suited for datasets with smoother decision boundaries.\n\nOne common technique for choosing the optimal value of k is to use cross -validation, where we split the data into training and testing sets multiple times and evaluate the algorithm’s performance for different values of k. This allows us to choose the value of k that results in the best overall performance on the dataset.\n\nIt’s also important to note that the choice of k is not the only hyperparameter that can affect the performance of the KNN algorithm. Other factors, such as the distance metric used to measure distances between data points, can also have a significant impact on the algorithm’s performance. Therefore, it’s important to experiment with different hyperparameters and evaluate their impact on the algorithm’s performance.\n\nAfter choosing hyperparameters and training the model, we now need to evaluate our model’s performance. This comes in 2 steps – choosing a performance metric and evaluating the model on a new dataset with that performance metric.\n\nThe choice of performance metric largely depends on the problem that KNN is trying to solve. Some common classification metrics include accuracy, precision, recall, f1 score, ROC AUC, and PR AUC. Common regression metrics include MSE, RMSE, MAE, and R2. The right metric to choose also depends heavily on the dataset. For example, choosing accuracy as the evaluation metric for highly imbalanced datasets is often not as good of a metric as precision or recall. Take a dataset that has 98% of examples in the majority class, and the other 2% of examples belong to the other class. If our model just predicted the majority class each time, we would achieve roughly 98% accuracy, which is a false representation of how well our model can differentiate between the two classes.\n\nOnce the metric is chosen, there are several ways to evaluate the performance of the KNN algorithm, each with its own benefits and limitations. Two common approaches are cross -validation and training/testing split. Overall, both cross -validation and training/testing split are useful approaches for evaluating the performance of the KNN algorithm, and the choice between them depends on the specific problem and the available resources. Training/testing split is a quick and easy way to evaluate the performance of the algorithm, but it may suffer from overfitting or underfitting. Cross -validation provides a more reliable estimate of performance, but can be computationally expensive.\n\nIn our example above, we captured the average of the cross validation scores for each value of k that we tested. Thus, we already have our cross validation score saved in the cv_scores array.\n\nThe curse of dimensionality is a term used to describe the challenge of working with high-dimensional data. In a high-dimensional space, the distance between points becomes less meaningful as the number of dimensions increases. This is because the volume of the space increases exponentially with the number of dimensions, which leads to a sparse distribution of data points. As a result, the nearest neighbors to a given point may be very far away, making it difficult to find a reliable set of neighbors to use in the KNN algorithm.\n\nIn addition, as the number of dimensions increases, the amount of data required to represent the space effectively also increases. This means that as the number of dimensions grows, the amount of data required to produce reliable results can become prohibitively large. As a result, the computational cost of the KNN algorithm can become very high, making it difficult to use in practical applications.\n\nTo address the curse of dimensionality, various techniques have been developed, such as feature selection, dimensionality reduction, and data preprocessing. These techniques can help to reduce the number of dimensions in the data, making it more manageable and easier to work with. Additionally, other algorithms that are less sensitive to high-dimensional data, such as decision trees, neural networks, or support vector machines, may be used as an alternative to KNN.\n\nImbalanced data can also affect the performance of the KNN algorithm by affecting the distance calculation between samples. Recall that KNN is a distance-based algorithm that computes the distances between samples to find the k-nearest neighbors. In imbalanced datasets, the samples of the minority class are often sparse and scattered, and the majority class dominates the feature space. As a result, the distances between the minority class samples may be higher, and the distances between the majority class samples may be lower, leading to misclassifications.\n\nOne solution is to use resampling techniques to balance the dataset. Resampling techniques can either oversample the minority class by creating synthetic samples or undersample the majority class by removing some samples. This way, the density of the minority class samples can be increased, and the distances between the minority class samples can be reduced, leading to better classification performance. However, it is important to be careful with resampling techniques as they can introduce bias and lead to overfitting. Therefore, it is recommended to use cross -validation or other evaluation metrics to validate the performance of the model.\n\nThe computational complexity of the KNN algorithm mainly depends on the size of the dataset, the number of features, and the value of k. The time complexity of the KNN algorithm for a single query point is O(nd), where n is the number of training examples and d is the number of features. This is because for each query point, the algorithm needs to compute the distance between the query point and every other point in the dataset. As the number of features or the size of the dataset increases, the computational complexity of KNN also increases significantly, making it computationally expensive and impractical for large datasets. Additionally, finding the optimal value of k by brute force can also increase the time complexity of the algorithm.\n\nFeature selection refers to the process of selecting a subset of relevant features from a larger set of features in a dataset. The aim of feature selection is to remove irrelevant and redundant features, thereby improving the performance of the algorithm.\n\nIn the case of KNN, feature selection can be used to improve performance in several ways. One of the main benefits is that it can reduce the dimensionality of the dataset, which in turn can significantly reduce the computational complexity of the algorithm. This is particularly important in large datasets where the number of features can be very high.\n\nBy reducing the dimensionality of the dataset, feature selection can also help to remove noisy and irrelevant features, which can reduce overfitting and improve the accuracy of the algorithm. Furthermore, by reducing the number of features, feature selection can also help to improve the generalization of the algorithm, making it more robust to new data.\n\nOverall, feature selection can be a powerful tool for improving the performance of KNN, particularly in cases where the number of features is high, and computational complexity is a concern.\n\nSimilarly to feature selection, dimensionality reduction algorithms can help with KNN performance by reducing the number of features or variables used in the distance calculation, thus reducing the computational complexity of the algorithm. These algorithms aim to find a lower-dimensional representation of the data that retains most of the information.\n\nFor example, Principal Component Analysis (PCA) is a commonly used dimensionality reduction technique that transforms the data into a lower-dimensional space by projecting the original data onto a smaller number of principal components, which are linear combinations of the original features. The principal components are chosen in such a way that they explain most of the variance in the data. By using PCA, one can reduce the number of features used in the distance calculation while retaining most of the information in the data.\n\nOther dimensionality reduction algorithms include SNE, t-SNE, and UMAP. To read more about these algorithms, check out our blog post on dimensionality reduction.\n\nEnsemble methods can improve the performance of KNN by combining the predictions of multiple KNN models. One way to do this is by using a technique called bagging, where multiple KNN models are trained on different subsets of the data, and their predictions are combined through averaging or voting. Another way is by using boosting, where KNN models are sequentially trained on the data, with each subsequent model focused on correcting the errors of the previous ones. By combining multiple models, ensemble methods can improve the stability and robustness of KNN, and reduce the impact of noisy or irrelevant features in the data.\n\nWhile KNN has several advantages such as ease of implementation, interpretability, and no assumptions about data distribution, it also has its limitations. The algorithm is sensitive to the choice of k, and can be affected by the curse of dimensionality, imbalanced data, and computational complexity issues. However, with appropriate preprocessing steps such as scaling, feature selection, and dimensionality reduction, as well as the use of ensemble methods, these limitations can be mitigated. Overall, KNN is a valuable tool in the machine learning toolbox and can be useful in a variety of real-world applications."
    },
    {
        "link": "https://medium.com/@amirm.lavasani/classic-machine-learning-in-python-k-nearest-neighbors-knn-a06fbfaaf80a",
        "document": "KNN relies on a straightforward principle: when given a new, unknown data point, it looks at the nearest labeled data points and assigns the most common label among them to the new point.\n\nThis closeness is determined by a distance metric, commonly Euclidean or Manhattan distance.\n\nAt the core of proximity-based learning, such as the K-Nearest Neighbors (KNN) algorithm, lies a fundamental concept: closeness dictates similarity.\n\nThe concept translates into the algorithm’s behavior by seeking the closest ‘neighbors’ — data points that share proximity — to make decisions about new, unseen data. By assuming that nearby points are alike, the algorithm infers patterns and assigns labels based on this proximity.\n\nThe algorithm doesn’t just predict outcomes; it also delineates decision boundaries. These boundaries partition the feature space into regions, where each region signifies a particular class or label. Think of it as virtual borders drawn based on the proximity of data points.\n• Neighbor identification: KNN identifies ‘k’ nearest neighbors to a query point based on their proximity.\n• A decision by the majority: It predicts by a majority vote among the neighbors for classification or averaging for regression.\n• No explicit training phase: KNN’s lazy learning means no explicit training; it stores the entire dataset for inference.\n• Impact of k-value: The k parameter influences model complexity and can affect overfitting or underfitting.\n\nChoosing the Right Distance Metrics in KNN\n\nEach distance metric provides a unique perspective in determining proximity and contributes distinct decision boundaries within the KNN algorithm.\n\nUnderstanding their characteristics aids in selecting the most appropriate metric for a given dataset.\n\nLet's explore some key distance metrics used in KNN:\n\nThe most prevalent and straightforward distance measure, exclusively applicable to real-valued vectors.\n\nOften known as a taxicab or city block distance, this metric calculates the absolute value between two points.\n\nA generalized form that encompasses both Euclidean and Manhattan distances, allowing the creation of various distance metrics based on the parameter .\n\nTailored for Boolean or string vectors, identifying discrepancies between vectors. Commonly referred to as the overlap metric.\n\nImplementing the K-Nearest Neighbors (KNN) algorithm from scratch allows a deep dive into its mechanics.\n\nLet’s break down the process into distinct parts and code each step comprehensively.\n\nThe core of KNN involves measuring distances between data points. In this step, we’ll create a function to compute the Euclidean distance between two points.\n\nNext, let’s write a function to find the ‘k’ nearest neighbors of a query point within a dataset.\n\nFinally, let’s create a function to predict the class of a query point based on the majority class among its nearest neighbors.\n\nThe wine quality dataset comprises 11 features like acidity, residual sugar, pH, and alcohol content, aiming to predict wine quality on a scale from 1 to 10. With 4898 samples, this dataset serves as a playground for exploring classification techniques.\n\nIn this phase, we initiate our exploration by reading the dataset, splitting it into training and testing sets, and applying a KNeighborsClassifier from scikit-learn with a static k-value of 15.\n\nThe next step is to find the optimum k-value that yields the best accuracy on the training data.\n\nEmploying GridSearchCV to find the best hyperparameter k-value resulted in a noticeable accuracy surge, hitting 50%. Optimizing the number of neighbors and exploring distance metrics were pivotal in this improvement.\n\nResults:\n\nWith an optimized k-value of 19, the model yields:\n\nBy incorporating distance-based weighted averaging within GridSearchCV, the model’s accuracy jumped to nearly 51.56%. Considering the proximity of neighbors in the prediction significantly contributed to this progress.\n\nWe added both distance and uniform mode in the parameters to find the best k-value.\n\nResults:\n\nWith an optimized k-value of 43, and using weighted distance, the model yields:\n\nImplementing Bagging with KNN led to a substantial leap in accuracy, reaching around 60.31%. Combining multiple KNN models through Bagging brought more robust predictions.\n\nResults:\n\nWith a bagging ensemble model with 100 estimators, each randomly having 0.3 of the data.\n\nThe complete code is accessible on GitHub.\n\nPros and Cons of the kNN Algorithm\n\nK-Nearest Neighbors (KNN) brings forth advantages and limitations, offering interpretability and adaptability while grappling with computational demands and challenges in high-dimensional spaces.\n• Interpretable and Fast Development: KNN offers interpretability, allowing users to comprehend its functioning. Its simplicity facilitates the rapid development of models without the complexity of more advanced techniques.\n• Adaptable to New Data: It easily accommodates new data points without retraining, adjusting its predictions based on new examples added to the dataset.\n• Few Hyperparameters: Requires minimal parameter tuning — mainly ‘k’ and choice of distance metric — simplifying the training process.\n• Robustness to Noisy Data: Demonstrates resilience to noisy training data, aiding in effective classification, especially with a large dataset.\n• Robustness to Large Training Data: Can be more effective with larger training datasets, leveraging the abundance of information for predictions.\n• Computationally Intensive and Resource-Heavy: As a lazy algorithm, KNN requires significant computing power and storage due to storing all data points, making it time and resource-consuming.\n• Curse of Dimensionality: Faces challenges with high-dimensional data, struggling to properly classify data points in higher dimensions, potentially leading to less accurate predictions.\n• Needs Optimal ‘k’ Selection: The choice of ‘k’ can significantly impact performance, requiring careful selection and optimization, which might be complex at times.\n• Slower Performance with Increased Data: The algorithm’s efficiency decreases notably as the dataset size or number of predictors/independent variables grows, affecting its speed.\n\nIn this comprehensive exploration of K-Nearest Neighbors (KNN) in Python, we delved into the algorithm’s fundamentals, its pivotal components, and practical implementation aspects.\n\nAlgorithm Insights: Understanding how KNN classifies based on proximity to neighbors.\n\nSignificance of K-Value: Recognizing the impact of k-value on model performance and the trade-off between bias and variance.\n\nDistance Metrics Importance: Appreciating the role of distance metrics in shaping decision boundaries and influencing predictions.\n\nKNN Implementation from Scratch: Crafting KNN code from the ground up, enhancing comprehension of its inner workings.\n\nFine-Tuning on Wine Quality Dataset: Iteratively optimizing KNN on the wine quality dataset to elevate its predictive capacity.\n\nAdvantages and Disadvantages: Weighing the pros and cons to comprehend where KNN excels and where it faces limitations."
    },
    {
        "link": "https://medium.datadriveninvestor.com/increase-10-accuracy-with-re-scaling-features-in-k-nearest-neighbors-python-code-677d28032a45",
        "document": "This story is a continuation of my previous story about the KNN Algorithm. For those who have not read my previous story, it is highly advised to read first so that you can follow the story at this time. You can read my previous story below.\n\nKNN is a Distance-Based algorithm where KNN classifies data based on proximity to K-Neighbors. Then, often we find that the features of the data we used are not at the same scale/units. An example is when we have features age and height. Obviously these two features have different units, the feature age is in year and the height is in centimeter.\n\nThis unit difference causes Distance-Based algorithms such as KNN to not perform optimally, so it is necessary to rescaling features…"
    },
    {
        "link": "https://github.com/xbeat/Machine-Learning/blob/main/Mastering%20K-Nearest%20Neighbors%20Hyperparameters%20in%20Python.md",
        "document": "K-Nearest Neighbors is a simple yet powerful machine learning algorithm used for classification and regression tasks. It works by finding the K closest data points to a new instance and making predictions based on their labels or values.\n\nKNN operates by calculating the distance between a new data point and all existing points in the dataset. It then selects the K nearest neighbors and uses their labels to make a prediction, either through majority voting (for classification) or averaging (for regression).\n\nSlide 3: Choosing the Right K Value\n\nThe value of K is a crucial hyperparameter in KNN. A small K can lead to overfitting, while a large K can result in underfitting. The optimal K value often depends on the specific dataset and problem at hand.\n\nKNN relies on distance calculations to determine nearest neighbors. Common distance metrics include Euclidean, Manhattan, and Minkowski distances. The choice of metric can significantly impact the algorithm's performance.\n\nWeighted KNN assigns more importance to closer neighbors, potentially improving prediction accuracy. Weights can be uniform (equal importance) or distance-based (closer neighbors have higher weights).\n\nKNN can be used for regression tasks by averaging the target values of the K nearest neighbors. This approach is useful for predicting continuous values based on similar instances in the dataset.\n\nKNN works best with numerical features. For categorical data, one-hot encoding or label encoding can be used to convert them into a suitable format for KNN.\n\nFeature scaling is crucial for KNN as it ensures all features contribute equally to distance calculations. Common scaling techniques include standardization and normalization.\n\nAs the number of features increases, KNN's performance can degrade due to the curse of dimensionality. This phenomenon occurs because the concept of \"nearest\" becomes less meaningful in high-dimensional spaces.\n\nTo mitigate the curse of dimensionality, we can use Principal Component Analysis (PCA) to reduce the number of features while retaining most of the variance in the data.\n\nCross-validation helps in finding optimal hyperparameters for KNN, such as the number of neighbors and weight function, by evaluating the model's performance on different subsets of the data.\n\nKNN can be used for anomaly detection by identifying data points that are far from their nearest neighbors. This approach is particularly useful for detecting outliers in datasets.\n\nKNN can be applied to image classification tasks by treating each pixel as a feature. However, this approach may require dimensionality reduction techniques for larger images.\n• \"Nearest Neighbor Methods in Learning and Vision: Theory and Practice\" - MIT Press (Not an ArXiv source, but a comprehensive book on KNN)\n\nThese resources provide in-depth information on KNN algorithms, their applications, and advanced techniques for improving their performance."
    }
]