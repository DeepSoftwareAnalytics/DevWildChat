[
    {
        "link": "https://geeksforgeeks.org/merge-sort",
        "document": "Merge sort is a sorting algorithm that follows the divide-and-conquer approach. It works by recursively dividing the input array into smaller subarrays and sorting those subarrays then merging them back together to obtain the sorted array.\n\nIn simple terms, we can say that the process of merge sort is to divide the array into two halves, sort each half, and then merge the sorted halves back together. This process is repeated until the entire array is sorted.\n\nMerge sort is a popular sorting algorithm known for its efficiency and stability. It follows the divide-and-conquer approach to sort a given array of elements. \n\nHere’s a step-by-step explanation of how merge sort works:\n\nLet’s sort the array or list [38, 27, 43, 10] using Merge Sort\n\n// begin is for left index and end is right index // of the sub-array of arr to be sorted // l is for left index and r is right index of the // sub-array of arr to be sorted // Sort first and second halves // Find sizes of two subarrays to be merged // Initial indices of first and second subarrays // Copy remaining elements of L[] if any // Copy remaining elements of R[] if any // Sort first and second halves // Sort first and second halves // Copy the remaining elements of L[], if there are any // Copy the remaining elements of R[], if there are any // l is for left index and r is right index of the // sub-array of arr to be sorted // Sort first and second halves //This code is contributed by Susobhan Akhuli\n• None T(n) Represents the total time time taken by the algorithm to sort an array of size n.\n• None 2T(n/2) represents time taken by the algorithm to recursively sort the two halves of the array. Since each half has n/2 elements, we have two recursive calls with input size as (n/2).\n• None O(n) represents the time taken to merge the two sorted halves\n• Time Complexity:\n• Best Case: O(n log n), When the array is already sorted or nearly sorted.\n• Average Case: O(n log n), When the array is randomly ordered.\n• Worst Case: O(n log n), When the array is sorted in reverse order.\n• Auxiliary Space: O(n), Additional space is required for the temporary array used during merging.\n• None Merge Sort and its variations are used in library methods of programming languages.\n• None is used in Python, Java Android and Swift. The main reason why it is preferred to sort non-primitive types is stability which is not there in QuickSort.\n• None It is a preferred algorithm for sorting Linked lists.\n• None It can be easily parallelized as we can independently sort subarrays and then merge.\n• None The merge function of merge sort to efficiently solve the problems like union and intersection of two sorted arrays\n• Stability : Merge sort is a stable sorting algorithm, which means it maintains the relative order of equal elements in the input array.\n• Guaranteed worst-case performance: O(N logN) , which means it performs well even on large datasets.\n• Naturally Parallel : We independently merge subarrays that makes it suitable for parallel processing.\n• Space complexity: Merge sort requires additional memory to store the merged sub-arrays during the sorting process.\n• Not in-place: Merge sort is not an in-place sorting algorithm, which means it requires additional memory to store the sorted data. This can be a disadvantage in applications where memory usage is a concern.\n• Slower than QuickSort in general as QuickSort is more cache friendly because it works in-place."
    },
    {
        "link": "https://stackoverflow.com/questions/8158802/merge-sort-implementation-with-an-array-used-instead-of-vector",
        "document": "I am trying to implement a algorithm with the use of instead of and I am getting some errors in one of my two functions. The code of the two functions is below.\n\nI figured out how to solve the problem I was facing with the code below."
    },
    {
        "link": "https://stackoverflow.com/questions/12030683/implementing-merge-sort-in-c",
        "document": "To answer the question: Creating dynamically sized arrays at run-time is done using . Ideally, you'd get your input using one of these. If not, it is easy to convert them. For example, you could create two arrays like this:\n\nHowever, allocating dynamic arrays is relatively slow and generally should be avoided when possible. For merge sort you can just sort subsequences of the original array and in-place merge them. It seems, asks for bidirectional iterators."
    },
    {
        "link": "https://geeksforgeeks.org/in-place-merge-sort",
        "document": "Implement Merge Sort i.e. standard implementation keeping the sorting algorithm as in-place. \n\nIn-place means it does not occupy extra memory for merge operation as in the standard case.\n• Maintain two pointers that point to the start of the segments which have to be merged.\n• Compare the elements at which the pointers are present.\n• If element1 < element2 then element1 is at right position, simply increase pointer1.\n• Else shift all the elements between element1 and element2(including element1 but excluding element2) right by 1 and then place the element2 in the previous place(i.e. before shifting right) of element1. Increment all the pointers by 1.\n\nBelow is the implementation of the above approach:\n\nNote: Time Complexity of above approach is O(n2 * log(n)) because merge is O(n2). Time complexity of standard merge sort is less, O(n Log n).\n\nApproach 2: The idea: We start comparing elements that are far from each other rather than adjacent. Basically we are using shell sorting to merge two sorted arrays with O(1) extra space.\n• Calculate mid two split the array in two halves(left sub-array and right sub-array)\n• Recursively call merge sort on left sub-array and right sub-array to sort them\n• Call merge function to merge left sub-array and right sub-array\n• For every pass, we calculate the gap and compare the elements towards the right of the gap.\n• Initiate the gap with ceiling value of n/2 where n is the combined length of left and right sub-array.\n• Every pass, the gap reduces to the ceiling value of gap/2.\n• Take a pointer i to pass the array.\n• Swap the ith and (i+gap)th elements if (i+gap)th element is smaller than(or greater than when sorting in decreasing order) ith element.\n\nBelow is the implementation of the above approach:\n\nNote: mergeSort method makes log n recursive calls and each time merge is called which takes n log n time to merge 2 sorted sub-arrays\n\nApproach 3: Here we use the below technique:\n• Calculate mid two split the array into two halves(left sub-array and right sub-array)\n• Recursively call merge sort on left sub-array and right sub-array to sort them\n• Call merge function to merge left sub-array and right sub-array\n• We first find the maximum element of both sub-array and increment it one to avoid collision of 0 and maximum element during modulo operation.\n• The idea is to traverse both sub-arrays from starting simultaneously. One starts from l till m and another starts from m+1 till r. So, We will initialize 3 pointers say i, j, k.\n• i will move from l till m; j will move from m+1 till r; k will move from l till r.\n• Now update value a[k] by adding min(a[i],a[j])*maximum_element.\n• Then also update those elements which are left in both sub-arrays.\n• After updating all the elements divide all the elements by maximum_element so we get the updated array back.\n\nBelow is the implementation of the above approach:\n\nTime Complexity: O(n log n)\n\nNote: Time Complexity of above approach is O(n2) because merge is O(n). Time complexity of standard merge sort is O(n log n).\n\nApproach 4: Here we use the following technique to perform an in-place merge\n\nThe above procedure naturally lends itself to the following implementation of an in-place merge sort.\n• Hereafter, for convenience, we’ll refer to the first sub-array as A, and the second sub-array as B\n• If either A or B are empty, or if the first element B is not less than the last element of A then we’re done\n• If the length of A is small enough and if it’s length is less than the length of B, then use insertion sort to merge A into B and return\n• If the length of B is small enough then use insertion sort to merge B into A and return\n• Find the location in A where we can exchange the remaining portion of A with the first-portion of B, such that the elements in A are less than or equal to any element in B\n• Perform the exchange between A and B\n• Recursively call merge() on the 2 sorted sub-arrays now residing in A\n• Recursively call merge() on the 2 sorted sub-arrays now residing in B\n• Split the array into two halves(left sub-array and right sub-array)\n• Recursively call merge_sort() on left sub-array and right sub-array to sort them\n• Call merge function to merge left sub-array and right sub-array\n\nTime Complexity of merge(): Worst Case: O(n^2), Average: O(n log n), Best: O(1)\n\nTime Complexity of merge_sort() function: Overall: O(log n) for the recursion alone, due to always evenly dividing the array in 2\n\nTime Complexity of merge_sort() overall: Worst Case: O(n^2 log n), Average: O(n (log n)^2), Best: O(log n)\n\nThe worst-case occurs when every sub-array exchange within merge() results in just _INSERT_THRESH-1 elements being exchanged"
    },
    {
        "link": "https://ccbp.in/blog/articles/merge-sort-cpp",
        "document": "Data structures rely on sorting algorithms that organise information effectively. Merge sort in C++ stands out by splitting complex tasks into smaller, manageable parts. The algorithm is based on the principle of the Divide and Conquer strategy and makes data handling easier or faster in many cases. This guide explains the merge sort implementation in C++ with examples and practical uses.\n\nHow does Divide and Conquer Work?\n\nA divide-and-conquer approach solves big problems by splitting them into smaller parts. The method makes each small task simple before putting solutions back together.\n\nThe main steps of divide-and-conquer are\n• Problems get divided into smaller portions. This continues until each portion becomes easy to solve.\n• Each small part receives its solution, often via recursion. Tiny parts get fixed right away.\n• Everything comes together once all parts have solutions to fix the original problem.\n\nA merge sort algorithm splits information into two halves and puts them back in order. It works by:\n\nThe process shows excellent results with big data sets because it maintains O(nlogn) time complexity. Many applications prefer merge sort for this reason.\n\nTo understand the merge sort c++ program. Let’s take an example array such as:\n\nStep 1: Divide the array into two halves\n\nMerge sort starts by dividing the array into two halves:\n\nNext, each of these halves is divided again into smaller parts. We divide the left half into two arrays of size 2:\n\nFinally, we merge the two sorted halves:\n• Time Complexity: O(nlog⁡n) for best, average, and worst cases.\n• Space Complexity: O(n) due to the additional space required for merging.\n\nImplementing merge sort c++ code typically involves defining a merge function combining two sorted arrays and a mergeSort function recursively sorts the array.\n• Merge functions combine two sorted subarrays into one by comparing their elements sequentially.\n• The mergeSort function recursively divides the array until each subarray contains one element and then calls the merge function to combine them.\n\nExample C++ Program for Merge Sort\n\nHere is the example c++ merge sort program:\n\n1. The main function that implements the recursive merge sort algorithm.\n• left: The starting index of the subarray to be sorted.\n• right: The ending index of the subarray to be sorted.\n\n3. If left < right, continue the sorting process; otherwise, return.\n\n4. The array is divided at the middle index.\n\n5. The subarray on the left (arr[left..mid]) and the subarray on the right (arr[mid+1..right]) are recursively sorted using MergeSort.\n\n6. After sorting both subarrays, the Merge function merges them into a single sorted array.\n• The merge function merges two sorted subarrays into a single sorted array. It uses temporary arrays to hold the values from the left and right halves.\n• It calls MergeSort to sort the array.\n\nThe recurrence relation for Merge Sort is a mathematical expression that describes the algorithm's time complexity in terms of the input size. The relation can be derived based on the algorithm's divide-and-conquer approach.\n\nThe recurrence relation for Merge Sort can be expressed as:\n• T(n): Represents the total time complexity for sorting an array of size n.\n• 2T(n/2): This term accounts for the two recursive calls made on subarrays of size n/2​. Each call sorts half of the array.‍\n• O(n): This term represents the linear time required to merge the two sorted subarrays.\n\nMerge Sort is an efficient sorting algorithm with an O(n log n) time complexity. It can be implemented in two ways: Recursive Merge Sort and Iterative Merge Sort. Let’s dive into these methods, explaining their respective processes and offering C++ code examples.\n\nRecursive Merge Sort follows a divide-and-conquer approach, where the array is recursively split into two halves, each half is sorted, and then the two halves are merged back into a sorted array. The recursion continues until each subarray contains a single element (which is trivially sorted), and then the merge process begins.\n• The function recursiveMergeSort recursively divides the array into two halves.\n• The base case occurs when the array cannot be divided further (i.e., the subarray size is 1).\n• The merge function merges two sorted arrays back into a single sorted array.\n\nWhile Recursive Merge Sort divides and conquers the problem by calling itself recursively, Iterative Merge Sort does the same task without recursion. Instead, it progressively merges pairs of adjacent elements or subarrays in a bottom-up manner.\n• The function iterativeMergeSort starts with a subarray size of 1 and doubles the size in each iteration.\n• It uses a loop to merge adjacent pairs of subarrays at each step.\n• The merge function combines the two sorted halves into one sorted array.\n• Merge sort is particularly well-suited for linked lists because it does not require random access to elements. This makes it ideal when you need to merge two sorted arrays in C++, as the linked list structure allows easy splitting and merging without the need for random access.\n• It is used in external sorting algorithms where data to be sorted does not fit into memory, as it efficiently handles large datasets by dividing them into smaller chunks.\n• Merge sort maintains the relative order of equal elements, making it ideal for sorting data structures where stability is essential, such as sorting records in databases.\n• The divide-and-conquer approach allows for easy parallelisation, making it suitable for multi-threaded applications.\n• Merge sort has a consistent time complexity of O(nlogn) for best, average, and worst cases, making it efficient for large datasets.\n• It is a stable sorting algorithm, preserving the relative order of equal elements.\n• Compared to quicksort, which can degrade to O(n2) in the worst case, merge sort guarantees O(nlogn) performance regardless of input.\n• Works Well with Linked Lists: It can be implemented without extra space when sorting linked lists, as merging can be done in place.\n• Merge sort requires additional space proportional to the size of the input array (O(n)), which can be a drawback for large datasets.\n• The recursive nature and merging process introduce overhead compared to simpler algorithms like insertion or selection sort for small datasets.\n• The standard implementation is not an in-place algorithm since it requires extra storage for temporary arrays.\n\nHere are the coding questions with c++ code asked in top companies' assessments on merge sort\n\nGiven an array, count the number of inversions in the array. An inversion is a pair of indices (i, j) such that i < j and arr[i] > arr[j].\n• In the above program a pair (i, j) where i < j and arr[i] > arr[j].\n• It uses modified merge sort to count inversions during merging.\n• Inversions are counted when elements from the right subarray are smaller than those in the left.\n\nGiven k sorted arrays of different sizes, merge them into a single sorted array.\n• The above program merges k sorted arrays into one sorted array.\n• Utilises a min-heap to access the smallest element across arrays efficiently.\n• Extracts the smallest element, add it to the result, and pushes the next element from the originating array.\n• Time complexity is O(Nlogk), where N is the total elements and k is the number of arrays.\n\nGiven two sorted arrays A[] and B[] of sizes N and M, the task is to merge both arrays into a single array in non-decreasing order.\n• In the above program, the arrays compare elements and add the smaller one to a merged array.\n• Adds leftover elements from the other array.\n\nIn conclusion, merge sort c++ is an efficient and stable sorting algorithm that uses the divide-and-conquer strategy, achieving a time complexity of O(nlogn). It is mainly used in handling large datasets and maintains the relative order of equal elements, making it ideal for applications requiring stability. While it is particularly effective for sorting linked lists and external data, its requirement for additional memory can be a drawback in memory-constrained environments. Merge Sort's predictable performance and versatility make it a valuable tool in C++ programming, suitable for various applications where efficient sorting is essential.\n\n1. What is the time complexity of Merge Sort in C++?\n\nThe time complexity of Merge Sort Program in C++:\n\n2. How does Merge Sort Compare to QuickSort?\n• Both have an average time complexity of O(n log n), but QuickSort performs faster in practice due to better locality of reference and fewer memory accesses.\n• Merge Sort requires O(n) extra space, while QuickSort can be implemented with O(log n) space (in the best case).\n• Merge Sort is stable, while QuickSort is not.\n\n3. Can Merge Sort be used for linked lists?\n\nYes, Merge Sort is very efficient for linked lists because linked lists allow easy splitting and merging operations. When sorting linked lists, you don't need extra space for arrays as in the case of arrays, and Merge Sort can be implemented in O(n log n) time with O(1) extra space.\n\n1. What is the time complexity of Merge Sort in the worst case?\n\n2. Which type of algorithm is the merge sort?\n\n3. What is the space complexity of Merge Sort?\n\n4. What is the best case time complexity for Merge Sort?"
    },
    {
        "link": "http://cppreference.com",
        "document": "\n• 7 June 2019: New version of the offline archive\n• 28 October 2018: New version of the offline archive\n• 11 March 2018: New version of the offline archive"
    },
    {
        "link": "https://isocpp.org/wiki/faq/freestore-mgmt",
        "document": "How do I deal with memory leaks?\n\nBy writing code that doesn’t have any. Clearly, if your code has operations, operations, and pointer arithmetic all over the place, you are going to mess up somewhere and get leaks, stray pointers, etc. This is true independently of how conscientious you are with your allocations: eventually the complexity of the code will overcome the time and effort you can afford.\n\nIt follows that successful techniques rely on hiding allocation and deallocation inside more manageable types: For single objects, prefer or . For multiple objects, prefer using standard containers like and as they manage memory for their elements better than you could without disproportionate effort. Consider writing this without the help of and :\n\nWhat would be your chance of getting it right the first time? And how would you know you didn’t have a leak?\n\nNote the absence of explicit memory management, macros, casts, overflow checks, explicit size limits, and pointers. By using a function object and a standard algorithm, the code could additionally have eliminated the pointer-like use of the iterator, but that seemed overkill for such a tiny program.\n\nThese techniques are not perfect and it is not always easy to use them systematically. However, they apply surprisingly widely and by reducing the number of explicit allocations and deallocations you make the remaining examples much easier to keep track of. As early as 1981, Stroustrup pointed out that by reducing the number of objects that he had to keep track of explicitly from many tens of thousands to a few dozens, he had reduced the intellectual effort needed to get the program right from a Herculean task to something manageable, or even easy.\n\nIf your application area doesn’t have libraries that make programming that minimizes explicit memory management easy, then the fastest way of getting your program complete and correct might be to first build such a library.\n\nTemplates and the standard libraries make this use of containers, resource handles, etc., much easier than it was even a few years ago. The use of exceptions makes it close to essential.\n\nIf you cannot handle allocation/deallocation implicitly as part of an object you need in your application anyway, you can use a resource handle to minimize the chance of a leak. Here is an example where you need to return an object allocated on the free store from a function. This is an opportunity to forget to delete that object. After all, we cannot tell just looking at pointer whether it needs to be deallocated and if so who is responsible for that. Using a resource handle, here the standard library , makes it clear where the responsibility lies:\n\nThink about resources in general, rather than simply about memory.\n\nIf systematic application of these techniques is not possible in your environment (you have to use code from elsewhere, part of your program was written by Neanderthals, etc.), be sure to use a memory leak detector as part of your standard development procedure, or plug in a garbage collector.\n\nCan I use just as in Java?\n\nSort of, but don’t do it blindly, if you do want it prefer to spell it as or , and there are often superior alternatives that are simpler and more robust than any of that. Consider:\n\nThe clumsy use of for is unnecessary and slow compared with the idiomatic use of a local variable ( ). You don’t need to use to create an object if you also that object in the same scope; such an object should be a local variable.\n\nShould I use or or ?\n\nYou should use as the null pointer value. The others still work for backward compatibility with older code.\n\nA problem with both and as a null pointer value is that is a special “maybe an integer value and maybe a pointer” value. Use only for integers, and that confusion disappears.\n\nDoes delete the pointer , or the pointed-to-data ?\n\nThe keyword should really be . The same abuse of English occurs when ing the memory pointed to by a pointer in C: really means .\n\nIs it safe to the same pointer twice?\n\nNo! (Assuming you didn’t get that pointer back from in between.)\n\nFor example, the following is a disaster:\n\nThat second line might do some really bad things to you. It might, depending on the phase of the moon, corrupt your heap, crash your program, make arbitrary and bizarre changes to objects that are already out there on the heap, etc. Unfortunately these symptoms can appear and disappear randomly. According to Murphy’s law, you’ll be hit the hardest at the worst possible moment (when the customer is looking, when a high-value transaction is trying to post, etc.).\n\nNote: some runtime systems will protect you from certain very simple cases of double . Depending on the details, you might be okay if you happen to be running on one of those systems and if no one ever deploys your code on another system that handles things differently and if you are deleting something that doesn’t have a destructor and if you don’t do anything significant between the two s and if no one ever changes your code to do something significant between the two s and if your thread scheduler (over which you likely have no control!) doesn’t happen to swap threads between the two s and if, and if, and if. So back to Murphy: since it can go wrong, it will, and it will go wrong at the worst possible moment.\n\nDo NOT email me saying you tested it and it doesn’t crash. Get a clue. A non-crash doesn’t prove the absence of a bug; it merely fails to prove the presence of a bug.\n\nTrust me: double- is bad, bad, bad. Just say no.\n\nCan I pointers allocated with ? Can I pointers allocated with ?\n\nNo! In brief, conceptually and allocate from different heaps, so can’t or each other’s memory. They also operate at different levels – raw memory vs. constructed objects.\n\nYou can use and in the same program. But you cannot allocate an object with and free it using . Nor can you allocate with and with or use on an array allocated by .\n\nThe C++ operators and guarantee proper construction and destruction; where constructors or destructors need to be invoked, they are. The C-style functions , , , and don’t ensure that. Furthermore, there is no guarantee that the mechanism used by and to acquire and release raw memory is compatible with and . If mixing styles works on your system, you were simply “lucky” – for now.\n\nIf you feel the need for – and many do – then consider using a standard library . For example\n\nSee also the examples and discussion in “Learning Standard C++ as a New Language”, which you can download from Stroustrup’s publications list.\n\nWhat is the difference between and ?\n\nFirst, (or ) are nearly always superior to both and and completely eliminate and .\n\nHaving said that, here’s the difference between those two:\n\nis a function that takes a number (of bytes) as its argument; it returns a pointing to unitialized storage. is an operator that takes a type and (optionally) a set of initializers for that type as its arguments; it returns a pointer to an (optionally) initialized object of its type. The difference is most obvious when you want to allocate an object of a user-defined type with non-trivial initialization semantics. Examples:\n\nNote that when you specify a initializer using the “(value)” notation, you get initialization with that value. Often, a is a better alternative to a free-store-allocated array (e.g., consider exception safety).\n\nWhenever you use you must consider initialization and conversion of the return pointer to a proper type. You will also have to consider if you got the number of bytes right for your use. There is no performance difference between and when you take initialization into account.\n\nreports memory exhaustion by returning . reports allocation and initialization errors by throwing exceptions ( ).\n\nObjects created by are destroyed by . Areas of memory allocated by are deallocated by .\n\nWhy should I use instead of trustworthy old ?\n\nFirst, (or ) are nearly always superior to both and and completely eliminate and .\n\nHaving said that, benefits of using instead of are: Constructors/destructors, type safety, overridability.\n• Type safety: returns a which isn’t type safe. returns a pointer of the right type (a ).\n• Overridability: is an that can be overridden by a class, while is not overridable on a per-class basis.\n\nCan I use on pointers allocated via ?\n\nWhen has to copy the allocation, it uses a bitwise copy operation, which will tear many C++ objects to shreds. C++ objects should be allowed to copy themselves. They use their own copy constructor or assignment operator.\n\nBesides all that, the heap that uses may not be the same as the heap that and use!\n\nWhy doesn’t C++ have an equivalent to ?\n\nIf you want to, you can of course use . However, is only guaranteed to work on arrays allocated by (and similar functions) containing objects without user-defined copy constructors. Also, please remember that contrary to naive expectations, occasionally does copy its argument array.\n\nIn C++, a better way of dealing with reallocation is to use a standard library container, such as , and let it grow naturally.\n\nDo I need to check for null after ?\n\nNo! (But if you have an ancient, stone-age compiler, you may have to force the operator to an exception if it runs out of memory.)\n\nIt turns out to be a real pain to always write explicit tests after every allocation. Code like the following is very tedious:\n\nIf your compiler doesn’t support (or if you refuse to use) exceptions, your code might be even more tedious:\n\nTake heart. In C++, if the runtime system cannot allocate bytes of memory during , a exception will be thrown. Unlike , never returns null!\n\nTherefore you should simply write:\n\nOn the second thought. Scratch that. You should simply write:\n\nThere, there… Much better now!\n\nHowever, if your compiler is ancient, it may not yet support this. Find out by checking your compiler’s documentation under “ ”. If it is ancient, you may have to force the compiler to have this behavior.\n\nHow can I convince my (older) compiler to automatically check to see if it returns null?\n\nIf you have an old compiler that doesn’t automagically perform the null test, you can force the runtime system to do the test by installing a “new handler” function. Your “new handler” function can do anything you want, such as an exception, some objects and return (in which case will retry the allocation), print a message and the program, etc.\n\nHere’s a sample “new handler” that prints a message and s an exception. The handler is installed using :\n\nAfter the line is executed, will call your if/when it runs out of memory. This means that will never return null:\n\nNote: If your compiler doesn’t support exception handling, you can, as a last resort, change the line … to:\n\nNote: If some namespace-scope / global / static object’s constructor uses , it might not use the function since that constructor often gets called before begins. Unfortunately there’s no convenient way to guarantee that the will be called before the first use of . For example, even if you put the call in the constructor of a global object, you still don’t know if the module (“compilation unit”) that contains that global object will be elaborated first or last or somewhere inbetween. Therefore you still don’t have any guarantee that your call of will happen before any other namespace-scope / global’s constructor gets invoked.\n\nDo I need to check for null before ?\n\nThe C++ language guarantees that will do nothing if is null. Since you might get the test backwards, and since most testing methodologies force you to explicitly test every branch point, you should not put in the redundant test.\n\nWhat are the two steps that happen when I say ?\n\nis a two-step process: it calls the destructor, then releases the memory. The code generated for is functionally similar to this (assuming is of type ):\n\nThe statement calls the destructor for the object pointed to by .\n\nThe statement calls the memory deallocation primitive, . This primitive is similar in spirit to . (Note, however, that these two are not interchangeable; e.g., there is no guarantee that the two memory deallocation primitives even use the same heap!)\n\nWhy doesn’t null out its operand?\n\nFirst, you should normally be using smart pointers, so you won’t care – you won’t be writing anyway.\n\nFor those rare cases where you really are doing manual memory management and so do care, consider:\n\nIf the part doesn’t touch then the second is a serious error that a C++ implementation cannot effectively protect itself against (without unusual precautions). Since deleting a null pointer is harmless by definition, a simple solution would be for to do a after it has done whatever else is required. However, C++ doesn’t guarantee that.\n\nOne reason is that the operand of need not be an lvalue. Consider:\n\nHere, the implementation of does not have a pointer to which it can null out. These examples may be rare, but they do imply that it is not possible to guarantee that “any pointer to a deleted object is null.” A simpler way of bypassing that “rule” is to have two pointers to an object:\n\nC++ explicitly allows an implementation of to null out an lvalue operand, but that idea doesn’t seem to have become popular with implementers.\n\nIf you consider zeroing out pointers important, consider using a destroy function:\n\nConsider this yet-another reason to minimize explicit use of and by relying on standard library smart pointers, containers, handles, etc.\n\nNote that passing the pointer as a reference (to allow the pointer to be nulled out) has the added benefit of preventing from being called for an rvalue:\n\nWhy isn’t the destructor called at the end of scope?\n\nThe simple answer is “of course it is!”, but have a look at the kind of example that often accompany that question:\n\nThat is, there was some (mistaken) assumption that the object created by would be destroyed at the end of a function.\n\nBasically, you should only use heap allocation if you want an object to live beyond the lifetime of the scope you create it in. Even then, you should normally use or . In those rare cases where you do want heap allocation and you opt to use , you need to use to destroy the object. For example:\n\nIf you want an object to live in a scope only, don’t use heap allocation at all but simply define a variable:\n\nThe variable is implicitly destroyed at the end of the scope.\n\nCode that creates an object using and then s it at the end of the same scope is ugly, error-prone, inefficient, and usually not exception-safe. For example:\n\nIn , does the memory “leak” if the constructor throws an exception?\n\nIf an exception occurs during the constructor of , the C++ language guarantees that the memory bytes that were allocated will automagically be released back to the heap.\n\nHere are the details: is a two-step process:\n• bytes of memory are allocated using the primitive . This primitive is similar in spirit to . (Note, however, that these two are not interchangeable; e.g., there is no guarantee that the two memory allocation primitives even use the same heap!).\n• It constructs an object in that memory by calling the constructor. The pointer returned from the first step is passed as the parameter to the constructor. This step is wrapped in a … block to handle the case when an exception is thrown during this step.\n\nThus the actual generated code is functionally similar to:\n\nThe statement marked “Placement ” calls the constructor. The pointer becomes the pointer inside the constructor, .\n\nHow do I allocate / unallocate an array of things?\n\nAny time you allocate an array of objects via (usually with the n in the expression), you must use in the statement. This syntax is necessary because there is no syntactic difference between a pointer to a thing and a pointer to an array of things (something we inherited from C).\n\nWhat if I forget the when ing an array allocated via ?\n\nAll life comes to a catastrophic end.\n\nIt is the programmer’s —not the compiler’s— responsibility to get the connection between and correct. If you get it wrong, neither a compile-time nor a run-time error message will be generated by the compiler. Heap corruption is a likely result. Or worse. Your program will probably die.\n\nCan I drop the when ing an array of some built-in type ( , , etc)?\n\nSometimes programmers think that the in the only exists so the compiler will call the appropriate destructors for all elements in the array. Because of this reasoning, they assume that an array of some built-in type such as or can be d without the . E.g., they assume the following is valid code:\n\nBut the above code is wrong, and it can cause a disaster at runtime. In particular, the code that’s called for is , but the code that’s called for is . The default behavior for the latter is to call the former, but users are allowed to replace the latter with a different behavior (in which case they would normally also replace the corresponding code in ). If they replaced the code so it wasn’t compatible with the code, and you called the wrong one (i.e., if you said rather than ), you could end up with a disaster at runtime.\n\nAfter , how does the compiler know there are objects to be destructed during ?\n\nLong answer: The run-time system stores the number of objects, , somewhere where it can be retrieved if you only know the pointer, . There are two popular techniques that do this. Both these techniques are in use by commercial-grade compilers, both have tradeoffs, and neither is perfect. These techniques are:\n• Over-allocate the array and put just to the left of the first object.\n• Use an associative array with as the key and as the value.\n\nIs it legal (and moral) for a member function to say ?\n\nAs long as you’re careful, it’s okay (not evil) for an object to commit suicide ( ).\n• You must be absolutely 100% positively sure that object was allocated via (not by , nor by placement , nor a local object on the stack, nor a namespace-scope / global, nor a member of another object; but by plain ordinary ).\n• You must be absolutely 100% positively sure that your member function will be the last member function invoked on object.\n• You must be absolutely 100% positively sure that the rest of your member function (after the line) doesn’t touch any piece of object (including calling any other member functions or touching any data members). This includes code that will run in destructors for any objects allocated on the stack that are still alive.\n• You must be absolutely 100% positively sure that no one even touches the pointer itself after the line. In other words, you must not examine it, compare it with another pointer, compare it with , print it, cast it, do anything with it.\n\nNaturally the usual caveats apply in cases where your pointer is a pointer to a base class when you don’t have a virtual destructor.\n\nHow do I allocate multidimensional arrays using ?\n\nThere are many ways to do this, depending on how flexible you want the array sizing to be. On one extreme, if you know all the dimensions at compile-time, you can allocate multidimensional arrays statically (as in C):\n\nMore commonly, the size of the matrix isn’t known until run-time but you know that it will be rectangular. In this case you need to use the heap (“freestore”), but at least you are able to allocate all the elements in one freestore chunk.\n\nFinally at the other extreme, you may not even be guaranteed that the matrix is rectangular. For example, if each row could have a different length, you’ll need to allocate each row individually. In the following function, is the number of columns in row number , where varies between and inclusive.\n\nNote the funny use of in the deletion process. This prevents wrap-around of the value when goes one step below zero.\n\nFinally, note that pointers and arrays are evil. It is normally much better to encapsulate your pointers in a class that has a safe and simple interface. The following FAQ shows how to do this.\n\nBut the previous FAQ’s code is SOOOO tricky and error prone! Isn’t there a simpler way?\n\nThe reason the code in the previous FAQ was so tricky and error prone was that it used pointers, and we know that pointers and arrays are evil. The solution is to encapsulate your pointers in a class that has a safe and simple interface. For example, we can define a class that handles a rectangular matrix so our user code will be vastly simplified when compared to the the rectangular matrix code from the previous FAQ:\n\nThe main thing to notice is the lack of clean-up code. For example, there aren’t any statements in the above code, yet there will be no memory leaks, assuming only that the destructor does its job correctly.\n\nHere’s the code that makes the above possible:\n\nNote that the above class accomplishes two things: it moves some tricky memory management code from the user code (e.g., ) to the class, and it reduces the overall bulk of program. The latter point is important. For example, assuming is even mildly reusable, moving complexity from the users [plural] of into itself [singular] is equivalent to moving complexity from the many to the few. Anyone who has seen Star Trek 2 knows that the good of the many outweighs the good of the few… or the one.\n\nBut the above class is specific to ! Isn’t there a way to make it generic?\n\nHere’s how this can be used:\n\nNow it’s easy to use for things other than . For example, the following uses a of (where is the standard string class):\n\nYou can thus get an entire family of classes from a template. For example, , , , etc.\n\nHere’s one way that the template can be implemented:\n\nWhat’s another way to build a template?\n\nUse the standard template, and make a of .\n\nThe following uses a .\n\nNote how much simpler this is than the previous: there is no explicit in the constructor, and there is no need for any of The Big Three (destructor, copy constructor or assignment operator). Simply put, your code is a lot less likely to have memory leaks if you use than if you use explicit and .\n\nNote also that doesn’t force you to allocate numerous chunks of memory. If you prefer to allocate only one chunk of memory for the entire matrix, as was done in the previous, just change the type of to and add member variables and . You’ll figure out the rest: initialize using , change to , etc.\n\nDoes C++ have arrays whose length can be specified at run-time?\n\nYes, in the sense that the standard library has a template that provides this behavior.\n\nNo, in the sense that built-in array types need to have their length specified at compile time.\n\nYes, in the sense that even built-in array types can specify the first index bounds at run-time. E.g., comparing with the previous FAQ, if you only need the first array dimension to vary then you can just ask new for an array of arrays, rather than an array of pointers to arrays:\n\nYou can’t do this if you need anything other than the first dimension of the array to change at run-time.\n\nBut please, don’t use arrays unless you have to. Arrays are evil. Use some object of some class if you can. Use arrays only when you have to.\n\nHow can I force objects of my class to always be created via rather than as local, namespace-scope, global, or ?\n\nAs usual with the Named Constructor Idiom, the constructors are all or , and there are one or more methods (the so-called “named constructors”), one per constructor. In this case the methods allocate the objects via . Since the constructors themselves are not , there is no other way to create objects of the class.\n\nNow the only way to create objects is via :\n\nMake sure your constructors are in the section if you expect to have derived classes.\n\nNote also that you can make another class a of if you want to allow a to have a member object of class , but of course this is a softening of the original goal, namely to force objects to be allocated via .\n\nHow do I do simple reference counting?\n\nIf all you want is the ability to pass around a bunch of pointers to the same object, with the feature that the object will automagically get d when the last pointer to it disappears, you can use something like the following “smart pointer” class:\n\nNaturally you can use nested classes to rename to .\n\nNote that you can soften the “never ” rule above with a little more checking in the constructor, copy constructor, assignment operator, and destructor. If you do that, you might as well put a check into the “ ” and “ ” operators (at least as an ). I would recommend against an method, since that would let people accidentally get at the .\n\nOne of the implicit constraints on is that it must only point to objects which have been allocated via . If you want to be really safe, you can enforce this constraint by making all of ’s constructors , and for each constructor have a ( ) method which allocates the object via and returns a (not a ). That way the only way anyone could create a object would be to get a (“ ” would be replaced by “ ”). Thus no one could accidentally subvert the reference counting mechanism.\n\nFor example, if had a and a , the changes to would be:\n\nThe end result is that you now have a way to use simple reference counting to provide “pointer semantics” for a given object. Users of your explicitly use objects, which act more or less like pointers. The benefit is that users can make as many copies of their “smart pointer” objects, and the pointed-to object will automagically get d when the last such object vanishes.\n\nIf you’d rather give your users “reference semantics” rather than “pointer semantics,” you can use reference counting to provide “copy on write”.\n\nHow do I provide reference counting with copy-on-write semantics?\n\nReference counting can be done with either pointer semantics or reference semantics. The previous FAQ shows how to do reference counting with pointer semantics. This FAQ shows how to do reference counting with reference semantics.\n\nThe basic idea is to allow users to think they’re copying your objects, but in reality the underlying implementation doesn’t actually do any copying unless and until some user actually tries to modify the underlying object.\n\nClass houses all the data that would normally go into the . also has an extra data member, , to manage the reference counting. Class ends up being a “smart reference” that (internally) points to a .\n\nIf it is fairly common to call ’s default constructor, you can avoid all those calls by sharing a common object for all s that are constructed via . To avoid initialization order problems, this shared object is created “on first use” inside a function. Here are the changes that would be made to the above code (note that the shared object’s destructor is never invoked; if that is a problem, either hope you don’t have any initialization order problems, or drop back to the approach described above):\n\nNote: You can also provide reference counting for a hierarchy of classes if your class would normally have been a base class.\n\nHow do I provide reference counting with copy-on-write semantics for a hierarchy of classes?\n\nThe previous FAQ presented a reference counting scheme that provided users with reference semantics, but did so for a single class rather than for a hierarchy of classes. This FAQ extends the previous technique to allow for a hierarchy of classes. The basic difference is that is now the root of a hierarchy of classes, which probably cause it to have some functions. Note that class itself will still not have any functions.\n\nThe Virtual Constructor Idiom is used to make copies of the objects. To select which derived class to create, the sample code below uses the Named Constructor Idiom, but other techniques are possible (a statement in the constructor, etc). The sample code assumes two derived classes: and . Methods in the derived classes are unaware of the reference counting.\n\nNaturally the constructors and methods for and will need to be implemented in whatever way is appropriate.\n\nCan I absolutely prevent people from subverting the reference counting mechanism, and if so, should I?\n\nNo, and (normally) no.\n\nThere are two basic approaches to subverting the reference counting mechanism:\n• The scheme could be subverted if someone got a (rather than being forced to use a ). Someone could get a if class has an that returns a : . Yes it’s bizarre and unexpected, but it could happen. This hole could be closed in two ways: overload so it returns a , or change the return type of so it returns a ( would be a class that simulates a reference; it would need to have all the methods that has, and it would need to forward all those method calls to the underlying object; there might be a performance penalty for this second choice depending on how good the compiler is at inlining methods). Another way to fix this is to eliminate — and lose the corresponding ability to get and use a . But even if you did all this, someone could still generate a by explicitly calling : .\n• The scheme could be subverted if someone had a leak and/or dangling pointer to a . Basically what we’re saying here is that is now safe, but we somehow want to prevent people from doing stupid things with objects. (And if we could solve that via objects, we’d have the same problem again with them). One hole here is if someone created a using , then allowed the to leak (worst case this is a leak, which is bad but is usually a little better than a dangling pointer). This hole could be plugged by declaring as , thus preventing someone from saying . Another hole here is if someone creates a local object, then takes the address of that and passed around the . If that lived longer than the , you could have a dangling pointer — shudder. This hole could be plugged by preventing people from taking the address of a (by overloading as ), with the corresponding loss of functionality. But even if you did all that, they could still create a which is almost as dangerous as a , simply by doing this: (or by passing the to someone else).\n\nAnd even if we closed all those holes, C++ has those wonderful pieces of syntax called pointer casts. Using a pointer cast or two, a sufficiently motivated programmer can normally create a hole that’s big enough to drive a proverbial truck through. (By the way, pointer casts are evil.)\n\nSo the lessons here seem to be: (a) you can’t prevent espionage no matter how hard you try, and (b) you can easily prevent mistakes.\n\nSo I recommend settling for the “low hanging fruit”: use the easy-to-build and easy-to-use mechanisms that prevent mistakes, and don’t bother trying to prevent espionage. You won’t succeed, and even if you do, it’ll (probably) cost you more than it’s worth.\n\nSo if we can’t use the C++ language itself to prevent espionage, are there other ways to do it? Yes. I personally use old fashioned code reviews for that. And since the espionage techniques usually involve some bizarre syntax and/or use of pointer-casts and unions, you can use a tool to point out most of the “hot spots.”\n\nCan I use a garbage collector in C++?\n\nIf you want automatic garbage collection, there are good commercial and public-domain garbage collectors for C++. For applications where garbage collection is suitable, C++ is an excellent garbage collected language with a performance that compares favorably with other garbage collected languages. See The C++ Programming Language (4th Edition) for a discussion of automatic garbage collection in C++. See also, Hans-J. Boehm’s site for C and C++ garbage collection.\n\nAlso, C++ supports programming techniques that allows memory management to be safe and implicit without a garbage collector. Garbage collection is useful for specific needs, such as inside the implementation of lock-free data structures to avoid ABA issues, but not as a general-purpose default way of handling for resource management. We are not saying that GC is not useful, just that there are better approaches in many situations.\n\nCompared with the “smart pointer” techniques, the two kinds of garbage collector techniques are:\n• usually more efficient (especially when the average object size is small or in multithreaded environments)\n• able to handle “cycles” in the data (reference counting techniques normally “leak” if the data structures can form a cycle)\n• sometimes leak other objects (since the garbage collectors are necessarily conservative, they sometimes see a random bit pattern that appears to be a pointer into an allocation, especially if the allocation is large; this can allow the allocation to leak)\n• work better with existing libraries (since smart pointers need to be used explicitly, they may be hard to integrate with existing libraries)\n\nWhat are the two kinds of garbage collectors for C++?\n\nIn general, there seem to be two flavors of garbage collectors for C++:\n• Conservative garbage collectors. These know little or nothing about the layout of the stack or of C++ objects, and simply look for bit patterns that appear to be pointers. In practice they seem to work with both C and C++ code, particularly when the average object size is small. Here are some examples, in alphabetical order:\n• Hybrid garbage collectors. These usually scan the stack conservatively, but require the programmer to supply layout information for heap objects. This requires more work on the programmer’s part, but may result in improved performance. Here are some examples, in alphabetical order:\n\nSince garbage collectors for C++ are normally conservative, they can sometimes leak if a bit pattern “looks like” it might be a pointer to an otherwise unused block. Also they sometimes get confused when pointers to a block actually point outside the block’s extent (which is illegal, but some programmers simply must push the envelope; sigh) and (rarely) when a pointer is hidden by a compiler optimization. In practice these problems are not usually serious, however providing the collector with hints about the layout of the objects can sometimes ameliorate these issues.\n\nWhere can I get more info on garbage collectors for C++?\n\nFor more information, see the Garbage Collector FAQ.\n\nWhat is an and why isn’t there an ?\n\nIt’s now spelled , which supports both single objects and arrays.\n\nis an old standard smart pointer that has been deprecated, and is only being kept in the standard for backward compatibility with older code. It should not be used in new code."
    },
    {
        "link": "https://cplusplus.com/reference/array/array",
        "document": ""
    },
    {
        "link": "https://devdocs.io/cpp",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/34967756/where-is-the-official-c-documentation",
        "document": "The official C++ \"documentation\" is the C++ standard, ISO/IEC 14882:2014(E). There is information at ISOCPP how to obtain the document.\n\nI wouldn't necessarily consider the standard good documentation but it does specify the behavior of the standard language and library constructs.\n\nThere isn't any other official document on C++ describing the entire language. There are good derivative works making things more accessible like Bjarne Stroustrup's \"Programming: Principles as Practices Using C++\" and Nicolai Josuttis's \"The C++ Standard Library\"."
    }
]