[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html",
        "document": "The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using or instead, possibly after a transformer or other Kernel Approximation.\n\nThe multiclass support is handled according to a one-vs-one scheme.\n\nFor details on the precise mathematical formulation of the provided kernel functions and how , and affect each other, see the corresponding section in the narrative documentation: Kernel functions.\n\nTo learn how to tune SVC’s hyperparameters, see the following example: Nested versus non-nested cross-validation\n\nRead more in the User Guide.\n\nRegularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty. For an intuitive visualization of the effects of scaling the regularization parameter C, see Scaling the regularization parameter for SVCs. Specifies the kernel type to be used in the algorithm. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape . For an intuitive visualization of different kernel types see Plot classification boundaries with different SVM Kernels. Degree of the polynomial kernel function (‘poly’). Must be non-negative. Ignored by all other kernels.\n• None if (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n• None if float, must be non-negative. Changed in version 0.22: The default value of changed from ‘auto’ to ‘scale’. Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’. Whether to use the shrinking heuristic. See the User Guide. Whether to enable probability estimates. This must be enabled prior to calling , will slow down that method as it internally uses 5-fold cross-validation, and may be inconsistent with . Read more in the User Guide. Specify the size of the kernel cache (in MB). Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as . Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. Hard limit on iterations within solver, or -1 for no limit. Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (‘ovo’) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). However, note that internally, one-vs-one (‘ovo’) is always used as a multi-class strategy to train models; an ovr matrix is only constructed from the ovo matrix. The parameter is ignored for binary classification. Changed in version 0.19: decision_function_shape is ‘ovr’ by default. Changed in version 0.17: Deprecated decision_function_shape=’ovo’ and None. If true, , and number of classes > 2, predict will break ties according to the confidence values of decision_function; otherwise the first class among the tied classes is returned. Please note that breaking ties comes at a relatively high computational cost compared to a simple predict. See SVM Tie Breaking Example for an example of its usage with . Controls the pseudo random number generation for shuffling the data for probability estimates. Ignored when is False. Pass an int for reproducible output across multiple function calls. See Glossary. Multipliers of parameter C for each class. Computed based on the parameter. Weights assigned to the features when . Dual coefficients of the support vector in the decision function (see Mathematical formulation), multiplied by their targets. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the multi-class section of the User Guide for details. 0 if correctly fitted, 1 otherwise (will raise warning) Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings. Number of iterations run by the optimization routine to fit the model. The shape of this attribute depends on the number of models optimized which in turn depends on the number of classes. Support vectors. An empty array if kernel is precomputed. Number of support vectors for each class.\n\nFor a comaprison of the SVC with other classifiers see: Plot classification probability.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/svm.html",
        "document": "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n\nThe advantages of support vector machines are:\n• None Still effective in cases where number of dimensions is greater than the number of samples.\n• None Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n• None Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n• None If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n• None SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\n\nThe support vector machines in scikit-learn support both dense ( and convertible to that by ) and sparse (any ) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered (dense) or (sparse) with .\n\nThe method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression. The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target. There are three different implementations of Support Vector Regression: , and . provides a faster implementation than but only considers the linear kernel, while implements a slightly different formulation than and . Due to its implementation in also regularizes the intercept, if considered. This effect can however be reduced by carefully fine tuning its parameter, which allows the intercept term to have a different regularization behavior compared to the other features. The classification results and score can therefore differ from the other two classifiers. See Implementation details for further details. As with classification classes, the fit method will take as argument vectors X, y, only that in this case y is expected to have floating point values instead of integer values:\n• None Avoiding data copy: For , , and , if the data passed to certain methods is not C-ordered contiguous and double precision, it will be copied before calling the underlying C implementation. You can check whether a given numpy array is C-contiguous by inspecting its attribute. For (and ) any input passed as a numpy array will be copied and converted to the liblinear internal sparse data representation (double precision floats and int32 indices of non-zero components). If you want to fit a large-scale linear classifier without copying a dense numpy C-contiguous double precision array as input, we suggest to use the class instead. The objective function can be configured to be almost the same as the model.\n• None Kernel cache size: For , , and , the size of the kernel cache has a strong impact on run times for larger problems. If you have enough RAM available, it is recommended to set to a higher value than the default of 200(MB), such as 500(MB) or 1000(MB).\n• None Setting C: is by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization. and are less sensitive to when it becomes large, and prediction results stop improving after a certain threshold. Meanwhile, larger values will take more time to train, sometimes up to 10 times longer, as shown in .\n• None Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a : See section Preprocessing data for more details on scaling and normalization.\n• None Regarding the parameter, quoting : We found that if the number of iterations is large, then shrinking can shorten the training time. However, if we loosely solve the optimization problem (e.g., by using a large stopping tolerance), the code without using shrinking may be much faster\n• None Parameter in / / approximates the fraction of training errors and support vectors.\n• None In , if the data is unbalanced (e.g. many positive and few negative), set and/or try different penalty parameters .\n• None Randomness of the underlying implementations: The underlying implementations of and use a random number generator only to shuffle the data for probability estimation (when is set to ). This randomness can be controlled with the parameter. If is set to these estimators are not random and has no effect on the results. The underlying implementation is similar to the ones of and . As no probability estimation is provided for , it is not random. The underlying implementation uses a random number generator to select features when fitting the model with a dual coordinate descent (i.e. when is set to ). It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller parameter. This randomness can also be controlled with the parameter. When is set to the underlying implementation of is not random and has no effect on the results.\n• None Using L1 penalization as provided by yields a sparse solution, i.e. only a subset of feature weights is different from zero and contribute to the decision function. Increasing yields a more complex model (more features are selected). The value that yields a “null” model (all weights equal to zero) can be calculated using .\n\nThe kernel function can be any of the following:\n• None polynomial: \\((\\gamma \\langle x, x'\\rangle + r)^d\\), where \\(d\\) is specified by parameter , \\(r\\) by .\n• None rbf: \\(\\exp(-\\gamma \\|x-x'\\|^2)\\), where \\(\\gamma\\) is specified by parameter , must be greater than 0.\n• None sigmoid \\(\\tanh(\\gamma \\langle x,x'\\rangle + r)\\), where \\(r\\) is specified by . Different kernels are specified by the parameter: See also Kernel Approximation for a solution to use RBF kernels that is much faster and more scalable. When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: and . The parameter , common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low makes the decision surface smooth, while a high aims at classifying all training examples correctly. defines how much influence a single training example has. The larger is, the closer other examples must be to be affected. Proper choice of and is critical to the SVM’s performance. One is advised to use with and spaced exponentially far apart to choose good values. You can define your own kernels by either giving the kernel as a python function or by precomputing the Gram matrix. Classifiers with custom kernels behave the same way as any other classifiers, except that:\n• None Field is now empty, only indices of support vectors are stored in\n• None A reference (and not a copy) of the first argument in the method is stored for future reference. If that array changes between the use of and you will have unexpected results. You can use your own defined kernels by passing a function to the parameter. Your kernel must take as arguments two matrices of shape , and return a kernel matrix of shape . The following code defines a linear kernel and creates a classifier instance that will use that kernel: You can pass pre-computed kernels by using the option. You should then pass Gram matrix instead of X to the and methods. The kernel values between all training vectors and the test vectors must be provided:\n\nA support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”: In general, when the problem isn’t linearly separable, the support vectors are the samples within the margin boundaries. We recommend and as good references for the theory and practicalities of SVMs. Given training vectors \\(x_i \\in \\mathbb{R}^p\\), i=1,…, n, in two classes, and a vector \\(y \\in \\{1, -1\\}^n\\), our goal is to find \\(w \\in \\mathbb{R}^p\\) and \\(b \\in \\mathbb{R}\\) such that the prediction given by \\(\\text{sign} (w^T\\phi(x) + b)\\) is correct for most samples. Intuitively, we’re trying to maximize the margin (by minimizing \\(||w||^2 = w^Tw\\)), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value \\(y_i (w^T \\phi (x_i) + b)\\) would be \\(\\geq 1\\) for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance \\(\\zeta_i\\) from their correct margin boundary. The penalty term controls the strength of this penalty, and as a result, acts as an inverse regularization parameter (see note below). The dual problem to the primal is \\[ \\begin{align}\\begin{aligned}\\min_{\\alpha} \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha\\\\\\begin{split} \\textrm {subject to } & y^T \\alpha = 0\\\\ & 0 \\leq \\alpha_i \\leq C, i=1, ..., n\\end{split}\\end{aligned}\\end{align} \\] where \\(e\\) is the vector of all ones, and \\(Q\\) is an \\(n\\) by \\(n\\) positive semidefinite matrix, \\(Q_{ij} \\equiv y_i y_j K(x_i, x_j)\\), where \\(K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)\\) is the kernel. The terms \\(\\alpha_i\\) are called the dual coefficients, and they are upper-bounded by \\(C\\). This dual representation highlights the fact that training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function \\(\\phi\\): see kernel trick. Once the optimization problem is solved, the output of decision_function for a given sample \\(x\\) becomes: and the predicted class correspond to its sign. We only need to sum over the support vectors (i.e. the samples that lie within the margin) because the dual coefficients \\(\\alpha_i\\) are zero for the other samples. These parameters can be accessed through the attributes which holds the product \\(y_i \\alpha_i\\), which holds the support vectors, and which holds the independent term \\(b\\) While SVM models derived from libsvm and liblinear use as regularization parameter, most other estimators use . The exact equivalence between the amount of regularization of two models depends on the exact objective function optimized by the model. For example, when the estimator used is regression, the relation between them is given as \\(C = \\frac{1}{alpha}\\). The primal problem can be equivalently formulated as where we make use of the hinge loss. This is the form that is directly optimized by , but unlike the dual form, this one does not involve inner products between samples, so the famous kernel trick cannot be applied. This is why only the linear kernel is supported by (\\(\\phi\\) is the identity function). The \\(\n\nu\\)-SVC formulation is a reparameterization of the \\(C\\)-SVC and therefore mathematically equivalent. We introduce a new parameter \\(\n\nu\\) (instead of \\(C\\)) which controls the number of support vectors and margin errors: \\(\n\nu \\in (0, 1]\\) is an upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. A margin error corresponds to a sample that lies on the wrong side of its margin boundary: it is either misclassified, or it is correctly classified but does not lie beyond the margin. Given training vectors \\(x_i \\in \\mathbb{R}^p\\), i=1,…, n, and a vector \\(y \\in \\mathbb{R}^n\\) \\(\\varepsilon\\)-SVR solves the following primal problem: Here, we are penalizing samples whose prediction is at least \\(\\varepsilon\\) away from their true target. These samples penalize the objective by \\(\\zeta_i\\) or \\(\\zeta_i^*\\), depending on whether their predictions lie above or below the \\(\\varepsilon\\) tube. \\[ \\begin{align}\\begin{aligned}\\min_{\\alpha, \\alpha^*} \\frac{1}{2} (\\alpha - \\alpha^*)^T Q (\\alpha - \\alpha^*) + \\varepsilon e^T (\\alpha + \\alpha^*) - y^T (\\alpha - \\alpha^*)\\\\\\begin{split} \\textrm {subject to } & e^T (\\alpha - \\alpha^*) = 0\\\\ & 0 \\leq \\alpha_i, \\alpha_i^* \\leq C, i=1, ..., n\\end{split}\\end{aligned}\\end{align} \\] where \\(e\\) is the vector of all ones, \\(Q\\) is an \\(n\\) by \\(n\\) positive semidefinite matrix, \\(Q_{ij} \\equiv K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)\\) is the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function \\(\\phi\\). These parameters can be accessed through the attributes which holds the difference \\(\\alpha_i - \\alpha_i^*\\), which holds the support vectors, and which holds the independent term \\(b\\) The primal problem can be equivalently formulated as where we make use of the epsilon-insensitive loss, i.e. errors of less than \\(\\varepsilon\\) are ignored. This is the form that is directly optimized by ."
    },
    {
        "link": "https://scikit-learn.org/stable/api/sklearn.svm.html",
        "document": "User guide. See the Support Vector Machines section for further details.\n\nReturn the lowest bound for C."
    },
    {
        "link": "https://scipy-lectures.org/packages/scikit-learn/index.html",
        "document": "Machine Learning is about building programs with tunable parameters that are adjusted automatically so as to improve their behavior by adapting to previously seen data. Machine Learning can be considered a subfield of Artificial Intelligence since those algorithms can be seen as building blocks to make computers learn to behave more intelligently by somehow generalizing rather that just storing and retrieving data items like a database system would do. We’ll take a look at two very simple machine learning tasks here. The first is a classification task: the figure shows a collection of two-dimensional data, colored according to two different class labels. A classification algorithm may be used to draw a dividing boundary between the two clusters of points: By drawing this separating line, we have learned a model which can generalize to new data: if you were to drop another point onto the plane which is unlabeled, this algorithm could now predict whether it’s a blue or a red point. The next simple task we’ll look at is a regression task: a simple best-fit line to a set of data. Again, this is an example of fitting a model to data, but our focus here is that the model can make generalizations about new data. The model has been learned from the training data, and can be used to predict the result of test data: here, we might be given an x-value, and the model would allow us to predict the y value. Machine learning algorithms implemented in scikit-learn expect data to be stored in a two-dimensional array or matrix. The arrays can be either arrays, or in some cases matrices. The size of the array is expected to be\n• n_samples: The number of samples: each sample is an item to process (e.g. classify). A sample can be a document, a picture, a sound, a video, an astronomical object, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\n• n_features: The number of features or distinct traits that can be used to describe each item in a quantitative manner. Features are generally real-valued, but may be boolean or discrete-valued in some cases. The number of features must be fixed in advance. However it can be very high dimensional (e.g. millions of features) with most of them being zeros for a given sample. This is a case where matrices can be useful, in that they are much more memory-efficient than numpy arrays. As an example of a simple dataset, let us a look at the iris data stored by scikit-learn. Suppose we want to recognize species of irises. The data consists of measurements of three different species of irises: If we want to design an algorithm to recognize iris species, what might the data be? Remember: we need a 2D array of size .\n• What would the refer to?\n• What might the refer to? Remember that there must be a fixed number of features for each sample, and feature number must be a similar kind of quantity for each sample. Scikit-learn has a very straightforward set of data on these iris species. The data consist of the following: embeds a copy of the iris CSV file along with a function to load it into numpy arrays: Import sklearn Note that scikit-learn is imported as The features of each sample flower are stored in the attribute of the dataset: The information about the class of each sample is stored in the attribute of the dataset: The names of the classes are stored in the last attribute, namely : This data is four-dimensional, but we can visualize two of the dimensions at a time using a scatter plot: Can you choose 2 features to find a plot where it is easier to seperate the different classes of irises? Hint: click on the figure above to see the code that generates it, and modify this code.\n\nEvery algorithm is exposed in scikit-learn via an ‘’Estimator’’ object. For instance a linear regression is: Estimator parameters: All the parameters of an estimator can be set when it is instantiated: # The input data for sklearn is 2D: (samples == 3 x features == 1) Estimated parameters: When data is fitted with an estimator, parameters are estimated from the data at hand. All the estimated parameters are attributes of the estimator object ending by an underscore: In Supervised Learning, we have a dataset consisting of both features and labels. The task is to construct an estimator which is able to predict the label of an object given the set of features. A relatively simple example is predicting the species of iris given a set of measurements of its flower. This is a relatively simple task. Some more complicated examples are:\n• given a multicolor image of an object through a telescope, determine whether that object is a star, a quasar, or a galaxy.\n• given a photograph of a person, identify the person in the photo.\n• given a list of movies a person has watched and their personal rating of the movie, recommend a list of movies they would like (So-called recommender systems: a famous example is the Netflix Prize). What these tasks have in common is that there is one or more unknown quantities associated with the object which needs to be determined from other observed quantities. Supervised learning is further broken down into two categories, classification and regression. In classification, the label is discrete, while in regression, the label is continuous. For example, in astronomy, the task of determining whether an object is a star, a galaxy, or a quasar is a classification problem: the label is from three distinct categories. On the other hand, we might wish to estimate the age of an object based on such observations: this would be a regression problem, because the label (age) is a continuous quantity. Classification: K nearest neighbors (kNN) is one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class. Let’s try it out on our iris classification problem: # What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal? Regression: The simplest possible regression setting is the linear regression one: Scikit-learn strives to have a uniform interface across all methods, and we’ll see examples of these below. Given a scikit-learn estimator object named , the following methods are available:\n• : fit training data. For supervised learning applications, this accepts two arguments: the data and the labels (e.g. ). For unsupervised learning applications, this accepts only a single argument, the data (e.g. ).\n• : given a trained model, predict the label of a new set of data. This method accepts one argument, the new data (e.g. ), and returns the learned label for each object in the array.\n• : For classification problems, some estimators also provide this method, which returns the probability that a new observation has each categorical label. In this case, the label with the highest probability is returned by .\n• : for classification or regression problems, most (all?) estimators implement a score method. Scores are between 0 and 1, with a larger score indicating a better fit.\n• : given an unsupervised model, transform new data into the new basis. This also accepts one argument , and returns the new representation of the data based on the unsupervised model.\n• : some estimators implement this method, which more efficiently performs a fit and a transform on the same input data. 3.6.2.4. Regularization: what it is and why it is necessary¶ Train errors Suppose you are using a 1-nearest neighbor estimator. How many errors do you expect on your train set?\n• Train set error is not a good measurement of prediction performance. You need to leave out a test set.\n• In general, we should accept errors on the train set. An example of regularization The core idea behind regularization is that we are going to prefer models that are simpler, for a certain definition of ‘’simpler’’, even if they lead to more errors on the train set. As an example, let’s generate with a 9th order polynomial, with noise: And now, let’s fit a 4th order and a 9th order polynomial to the data. With your naked eyes, which model do you prefer, the 4th order one, or the 9th order one? Let’s look at the ground truth: Regularization is ubiquitous in machine learning. Most scikit-learn estimators have a parameter to tune the amount of regularization. For instance, with k-NN, it is ‘k’, the number of nearest neighbors used to make the decision. k=1 amounts to no regularization: 0 error on the training set, whereas large k will push toward smoother decision boundaries in the feature space. For classification models, the decision boundary, that separates the class expresses the complexity of the model. For instance, a linear model, that makes a decision based on a linear combination of features, is more complex than a non-linear one.\n\n3.6.3.1. The nature of the data¶ Python code and Jupyter notebook for this section are found here In this section we’ll apply scikit-learn to the classification of handwritten digits. This will go a bit beyond the iris classification we saw before: we’ll discuss some of the metrics which can be used in evaluating the effectiveness of a classification model. Let us visualize the data and remind us what we’re looking at (click on the figure for the full code): # plot the digits: each image is 8x8 pixels 3.6.3.2. Visualizing the Data on its principal components¶ A good first-step for many problems is to visualize the data using a Dimensionality Reduction technique. We’ll start with the most straightforward one, Principal Component Analysis (PCA). PCA seeks orthogonal linear combinations of the features which show the greatest variance, and as such, can help give you a good idea of the structure of the data set. Given these projections of the data, which numbers do you think a classifier might have trouble distinguishing? For most classification problems, it’s nice to have a simple, fast method to provide a quick baseline classification. If the simple and fast method is sufficient, then we don’t have to waste CPU cycles on more complex models. If not, we can use the results of the simple method to give us clues about our data. One good method to keep in mind is Gaussian Naive Bayes ( ). Gaussian Naive Bayes fits a Gaussian distribution to each training label independantly on each feature, and uses this to quickly give a rough classification. It is generally not sufficiently accurate for real-world data, but can perform surprisingly well, for instance on text data. # split the data into training and validation sets # use the model to predict the labels of the test data As above, we plot the digits with the predicted labels to get an idea of how well the classification is working. Why did we split the data into training and validation sets? We’d like to measure the performance of our estimator without having to resort to plotting examples. A simple method might be to simply compare the number of matches: We see that more than 80% of the 450 predictions match the input. But there are other more sophisticated metrics that can be used to judge the performance of a classifier: several are available in the submodule. One of the most useful metrics is the , which combines several measures and prints a table with the results: Another enlightening metric for this sort of multi-label classification is a confusion matrix: it helps us visualize which labels are being interchanged in the classification errors: We see here that in particular, the numbers 1, 2, 3, and 9 are often being labeled 8.\n\nHere we’ll do a short example of a regression problem: learning a continuous value from a set of features. 3.6.4.1. A quick look at the data¶ Python code and Jupyter notebook for this section are found here We’ll use the California house prices set, available in scikit-learn. This records measurements of 8 attributes of housing markets in California, as well as the median price. The question is: can you predict the price of a new market given its attributes?: We can see that there are just over 20000 data points. The variable has a long description of the dataset: :Number of Attributes: 8 numeric, predictive attributes and the target This dataset was obtained from the StatLib repository. The target variable is the median house value for California districts. This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. It can be downloaded/loaded using the It often helps to quickly visualize pieces of the data using histograms, scatter plots, or other plot types. With matplotlib, let us show a histogram of the target values: the median price in each neighborhood: Let’s have a quick look to see if some features are more relevant than others for our problem: This is a manual version of a technique called feature selection. Sometimes, in Machine Learning it is useful to use feature selection to decide which features are the most useful for a particular problem. Automated methods exist which quantify this sort of exercise of choosing the most informative features. Now we’ll use to perform a simple linear regression on the housing data. There are many possibilities of regressors to use. A particularly simple one is : this is basically a wrapper around an ordinary least squares calculation. We can plot the error: expected as a function of predicted: The prediction at least correlates with the true price, though there are clearly some biases. We could imagine evaluating the performance of the regressor by, say, computing the RMS residuals between the true and predicted price. There are some subtleties in this, however, which we’ll cover in a later section. There are many other types of regressors available in scikit-learn: we’ll try a more powerful one here. Use the GradientBoostingRegressor class to fit the housing data. hint You can copy and paste some of the above code, replacing with : # Instantiate the model, fit the results, and scatter in vs. out Solution The solution is found in the code of this chapter\n\nHere we’ll continue to look at the digits data, but we’ll switch to the K-Neighbors classifier. The K-neighbors classifier is an instance-based classifier. The K-neighbors classifier predicts the label of an unknown point based on the labels of the K nearest points in the parameter space. Apparently, we’ve found a perfect classifier! But this is misleading for the reasons we saw before: the classifier essentially “memorizes” all the samples it has already seen. To really test how well this algorithm does, we need to try some samples it hasn’t yet seen. This problem also occurs with regression models. In the following we fit an other instance-based model named “decision tree” to the California Housing price dataset we introduced previously: Here again the predictions are seemingly perfect as the model was able to perfectly memorize the training set. Performance on test set does not measure overfit (as described above) Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. To avoid over-fitting, we have to define two different sets:\n• a training set X_train, y_train which is used for learning the parameters of a predictive model\n• a testing set X_test, y_test which is used for evaluating the fitted predictive model In scikit-learn such a random split can be quickly computed with the function: Now we train on the training data, and test on the testing data: The averaged f1-score is often used as a convenient measure of the overall performance of an algorithm. It appears in the bottom row of the classification report; it can also be accessed directly: The over-fitting we saw previously can be quantified by computing the f1-score on the training data itself: Regression metrics In the case of regression models, we need to use different metrics, such as explained variance. We have applied Gaussian Naives, support vectors machines, and K-nearest neighbors classifiers to the digits dataset. Now that we have these validation tools in place, we can ask quantitatively which of the three estimators works best for this dataset.\n• None With the default hyper-parameters for each estimator, which gives the best f1 score on the validation set? Recall that hyperparameters are the parameters set when you instantiate the classifier: for example, the in\n• None For each classifier, which value for the hyperparameters gives the best results for the digits data? For , use and . For we use between 1 and 10. Note that does not have any adjustable hyperparameters. Cross-validation consists in repetively splitting the data in pairs of train and test sets, called ‘folds’. Scikit-learn comes with a function to automatically compute score on all these folds. Here we do with k=5. We can use different splitting strategies, such as random splitting: There exists many different cross-validation strategies in scikit-learn. They are often useful to take in account non iid datasets. Consider regularized linear models, such as Ridge Regression, which uses l2 regularlization, and Lasso Regression, which uses l1 regularization. Choosing their regularization parameter is important. Let us set these parameters on the Diabetes dataset, a simple regression problem. The diabetes data consists of 10 physiological variables (age, sex, weight, blood pressure) measure on 442 patients, and an indication of disease progression after one year: With the default hyper-parameters: we compute the cross-validation score: We compute the cross-validation score as a function of alpha, the strength of the regularization for and . We choose 20 values of alpha between 0.0001 and 1: Can we trust our results to be actually useful? is constructed with an estimator, as well as a dictionary of parameter values to be searched. We can find the optimal parameters this way: For some models within scikit-learn, cross-validation can be performed more efficiently on large datasets. In this case, a cross-validated version of the particular model is included. The cross-validated versions of and are and , respectively. Parameter search on these estimators can be performed as follows: We see that the results match those returned by GridSearchCV How do we measure the performance of these estimators? We have used data to set the hyperparameters, so we need to test on actually new data. We can do this by running on our CV objects. Here there are 2 cross-validation loops going on, this is called ‘nested cross validation’: Note that these results do not match the best results of our curves above, and seems to under-perform . The reason is that setting the hyper-parameter is harder for Lasso, thus the estimation error on this hyper-parameter is larger.\n\nUnsupervised learning is applied on X without y: data without labels. A typical use case is to find hidden structure in the data. Dimensionality reduction derives a set of new artificial features smaller than the original feature set. Here we’ll use Principal Component Analysis (PCA), a dimensionality reduction that strives to retain most of the variance of the original data. We’ll use on the iris dataset: computes linear combinations of the original features using a truncated Singular Value Decomposition of the matrix X, to project the data onto a base of the top singular vectors. Once fitted, exposes the singular vectors in the attribute: Other attributes are available as well: Let us project the iris dataset along those first two dimensions:: and the data, which means that the data is now centered on both components with unit variance: Furthermore, the samples components do no longer carry any linear correlation: With a number of retained components 2 or 3, PCA is useful to visualize the dataset: Note that this projection was determined without any information about the labels (represented by the colors): this is the sense in which the learning is unsupervised. Nevertheless, we see that the projection gives us insight into the distribution of the different flowers in parameter space: notably, iris setosa is much more distinct than the other two species. For visualization, more complex embeddings can be useful (for statistical analysis, they are harder to control). is such a powerful manifold learning method. We apply it to the digits dataset, as the digits are vectors of dimension 8*8 = 64. Embedding them in 2D enables visualization: # Take the first 500 data points: it's hard to see 1500 points As cannot be applied to new data, we need to use its method. separates quite well the different classes of digits eventhough it had no access to the class information. has many other non-linear embeddings. Try them out on the digits dataset. Could you judge their quality without knowing the labels ?\n\n3.6.8. The eigenfaces example: chaining PCA and SVMs¶ The goal of this example is to show how an unsupervised method and a supervised one can be chained for better prediction. It starts with a didactic but lengthy way of doing things, and finishes with the idiomatic approach to pipelining in scikit-learn. Here we’ll take a look at a simple facial recognition example. Ideally, we would use a dataset consisting of a subset of the Labeled Faces in the Wild data that is available with . However, this is a relatively large download (~200MB) so we will do the tutorial on a simpler, less rich dataset. Feel free to explore the LFW dataset. Let’s visualize these faces to see what we’re working with Note is that these faces have already been localized and scaled to a common size. This is an important preprocessing piece for facial recognition, and is a process that can require a large collection of training data. This can be done in scikit-learn, but the challenge is gathering a sufficient amount of training data for the algorithm to work. Fortunately, this piece is common enough that it has been done. One good resource is OpenCV, the Open Computer Vision Library. We’ll perform a Support Vector classification of the images. We’ll do a typical train-test split on the images: 1850 dimensions is a lot for SVM. We can use PCA to reduce these 1850 features to a manageable size, while maintaining most of the information in the dataset. One interesting part of PCA is that it computes the “mean” face, which can be interesting to examine: The principal components measure deviations about this mean along orthogonal axes. It is also interesting to visualize these principal components: The components (“eigenfaces”) are ordered by their importance from top-left to bottom-right. We see that the first few components seem to primarily take care of lighting conditions; the remaining components pull out certain identifying features: the nose, eyes, eyebrows, etc. With this projection computed, we can now project our original training and test data onto the PCA basis: These projected components correspond to factors in a linear combination of component images such that the combination approaches the original face. Now we’ll perform support-vector-machine classification on this reduced dataset: Finally, we can evaluate how well this classification did. First, we might plot a few of the test-cases with the labels learned from the training set: The classifier is correct on an impressive number of images given the simplicity of its learning model! Using a linear classifier on 150 features derived from the pixel-level data, the algorithm correctly identifies a large number of the people in the images. Again, we can quantify this effectiveness using one of several measures from . First we can do the classification report, which shows the precision, recall and other measures of the “goodness” of the classification: Another interesting metric is the confusion matrix, which indicates how often any two items are mixed-up. The confusion matrix of a perfect classifier would only have nonzero entries on the diagonal, with zeros on the off-diagonal: Above we used PCA as a pre-processing step before applying our support vector machine classifier. Plugging the output of one estimator directly into the input of a second estimator is a commonly used pattern; for this reason scikit-learn provides a object which automates this process. The above problem can be re-expressed as a pipeline as follows:\n\nThis section is adapted from Andrew Ng’s excellent Coursera course The issues associated with validation and cross-validation are some of the most important aspects of the practice of machine learning. Selecting the optimal model for your data is vital, and is a piece of the problem that is not often appreciated by machine learning practitioners. The central question is: If our estimator is underperforming, how should we move forward?\n• Use simpler or more complicated model?\n• Add more features to each observed data point? The answer is often counter-intuitive. In particular, Sometimes using a more complicated model will give worse results. Also, Sometimes adding training data will not improve your results. The ability to determine what steps will improve your model is what separates the successful machine learning practitioners from the unsuccessful. Python code and Jupyter notebook for this section are found here Let us start with a simple 1D regression problem. This will help us to easily visualize the data and the model, and the results generalize easily to higher-dimensional datasets. We’ll explore a simple linear regression problem, with . Without noise, as linear regression fits the data perfectly In real life situation, we have noise (e.g. measurement noise) in our data: As we can see, our linear model captures and amplifies the noise in the data. It displays a lot of variance. We can use another linear estimator that uses regularization, the estimator. This estimator regularizes the coefficients by shrinking them to zero, under the assumption that very high correlations are often spurious. The alpha parameter controls the amount of shrinkage used. As we can see, the estimator displays much less variance. However it systematically under-estimates the coefficient. It displays a biased behavior. This is a typical example of bias/variance tradeof: non-regularized estimator are not biased, but they can display a lot of variance. Highly-regularized models have little variance, but high bias. This bias is not necessarily a bad thing: what matters is choosing the tradeoff between bias and variance that leads to the best prediction performance. For a specific dataset there is a sweet spot corresponding to the highest complexity that the data can support, depending on the amount of noise and of observations available. Given a particular dataset and a model (e.g. a polynomial), we’d like to understand whether bias (underfit) or variance limits prediction, and how to tune the hyperparameter (here , the degree of the polynomial) to give the best fit. On a given data, let us fit a simple polynomial regression model with varying degrees: In the above figure, we see fits for three different values of . For , the data is under-fit. This means that the model is too simplistic: no straight line will ever be a good fit to this data. In this case, we say that the model suffers from high bias. The model itself is biased, and this will be reflected in the fact that the data is poorly fit. At the other extreme, for the data is over-fit. This means that the model has too many free parameters (6 in this case) which can be adjusted to perfectly fit the training data. If we add a new point to this plot, though, chances are it will be very far from the curve representing the degree-6 fit. In this case, we say that the model suffers from high variance. The reason for the term “high variance” is that if any of the input points are varied slightly, it could result in a very different model. In the middle, for , we have found a good mid-point. It fits the data fairly well, and does not suffer from the bias and variance problems seen in the figures on either side. What we would like is a way to quantitatively identify bias and variance, and optimize the metaparameters (in this case, the polynomial degree d) in order to determine the best algorithm. A polynomial regression is built by pipelining and a : Let us create a dataset like in the example above: Central to quantify bias and variance of a model is to apply it on test data, sampled from the same distribution as the train, but that will capture independent noise: Validation curve A validation curve consists in varying a model parameter that controls its complexity (here the degree of the polynomial) and measures both error of the model on training data, and on test data (eg with cross-validation). The model parameter is then adjusted so that the test error is minimized: We use to compute train and test error, and plot it: # Vary the \"degrees\" on the pipeline step \"polynomialfeatures\" # Plot the mean train score and validation score across folds This figure shows why validation is important. On the left side of the plot, we have very low-degree polynomial, which under-fit the data. This leads to a low explained variance for both the training set and the validation set. On the far right side of the plot, we have a very high degree polynomial, which over-fits the data. This can be seen in the fact that the training explained variance is very high, while on the validation set, it is low. Choosing around 4 or 5 gets us the best tradeoff. The astute reader will realize that something is amiss here: in the above plot, gives the best results. But in the previous plot, we found that vastly over-fits the data. What’s going on here? The difference is the number of training points used. In the previous example, there were only eight training points. In this example, we have 100. As a general rule of thumb, the more training points used, the more complicated model can be used. But how can you determine for a given model whether more training points will be helpful? A useful diagnostic for this are learning curves. A learning curve shows the training and validation score as a function of the number of training points. Note that when we train on a subset of the training data, the training score is computed using this subset, not the full training set. This curve gives a quantitative view into how beneficial it will be to add training samples.\n• As the number of training samples are increased, what do you expect to see for the training score? For the validation score?\n• Would you expect the training score to be higher or lower than the validation score? Would you ever expect this to change? # Plot the mean train score and validation score across folds Note that the validation score generally increases with a growing training set, while the training score generally decreases with a growing training set. As the training size increases, they will converge to a single value. From the above discussion, we know that is a high-bias estimator which under-fits the data. This is indicated by the fact that both the training and validation scores are low. When confronted with this type of learning curve, we can expect that adding more training data will not help: both lines converge to a relatively low score. When the learning curves have converged to a low score, we have a high bias model. A high-bias model can be improved by:\n• Using a more sophisticated model (i.e. in this case, increase )\n• Gather more features for each sample. Increasing the number of samples, however, does not improve a high-bias model. Now let’s look at a high-variance (i.e. over-fit) model: Here we show the learning curve for . From the above discussion, we know that is a high-variance estimator which over-fits the data. This is indicated by the fact that the training score is much higher than the validation score. As we add more samples to this training set, the training score will continue to decrease, while the cross-validation error will continue to increase, until they meet in the middle. Learning curves that have not yet converged with the full training set indicate a high-variance, over-fit model. A high-variance model can be improved by:\n• Using a less-sophisticated model (i.e. in this case, make smaller) In particular, gathering more features for each sample will not help the results. We’ve seen above that an under-performing algorithm can be due to two possible situations: high bias (under-fitting) and high variance (over-fitting). In order to evaluate our algorithm, we set aside a portion of our training data for cross-validation. Using the technique of learning curves, we can train on progressively larger subsets of the data, evaluating the training error and cross-validation error to determine whether our algorithm has high variance or high bias. But what do we do with this information? If a model shows high bias, the following actions might help:\n• Add more features. In our example of predicting home prices, it may be helpful to make use of information such as the neighborhood the house is in, the year the house was built, the size of the lot, etc. Adding these features to the training and test sets can improve a high-bias estimator\n• Use a more sophisticated model. Adding complexity to the model can help improve on bias. For a polynomial fit, this can be accomplished by increasing the degree d. Each learning technique has its own methods of adding complexity.\n• Use fewer samples. Though this will not improve the classification, a high-bias algorithm can attain nearly the same error with a smaller training sample. For algorithms which are computationally expensive, reducing the training sample size can lead to very large improvements in speed.\n• Decrease regularization. Regularization is a technique used to impose simplicity in some machine learning models, by adding a penalty term that depends on the characteristics of the parameters. If a model has high bias, decreasing the effect of regularization can lead to better results. If a model shows high variance, the following actions might help:\n• Use fewer features. Using a feature selection technique may be useful, and decrease the over-fitting of the estimator.\n• Use a simpler model. Model complexity and over-fitting go hand-in-hand.\n• Use more training samples. Adding training samples can reduce the effect of over-fitting, and lead to improvements in a high variance estimator.\n• Increase Regularization. Regularization is designed to prevent over-fitting. In a high-variance model, increasing regularization can lead to better results. These choices become very important in real-world situations. For example, due to limited telescope time, astronomers must seek a balance between observing a large number of objects, and observing a large number of features for each object. Determining which is more important for a particular learning task can inform the observing strategy that the astronomer employs. 3.6.9.4. A last word of caution: separate validation and test set¶ Using validation schemes to determine hyper-parameters means that we are fitting the hyper-parameters to the particular validation set. In the same way that parameters can be over-fit to the training set, hyperparameters can be over-fit to the validation set. Because of this, the validation error tends to under-predict the classification error of new data. For this reason, it is recommended to split the data into three sets:\n• The training set, used to train the model (usually ~60% of the data)\n• The validation set, used to validate the model (usually ~20% of the data)\n• The test set, used to evaluate the expected error of the validated model (usually ~20% of the data) Many machine learning practitioners do not separate test set and validation set. But if your goal is to gauge the error of a model on unknown data, using an independent test set is vital."
    },
    {
        "link": "https://datacamp.com/tutorial/svm-classification-scikit-learn-python",
        "document": "Grow your machine learning skills with scikit-learn in Python. Use real-world datasets in this interactive course and learn how to make powerful predictions!"
    },
    {
        "link": "https://medium.com/@techwithpraisejames/how-to-calculate-confusion-matrix-in-python-using-scikit-learn-e51368716cbe",
        "document": "Imagine you have a machine learning model that can tell the difference between lions and tigers. You show it pictures of lions and tigers, and it tries to guess which is which. Sometimes, the model gets it right, and sometimes, it makes mistakes. A confusion matrix helps you understand these mistakes and how good your model is at telling lions apart from tigers.\n\nConfusion matrix is an important concept in machine learning that helps you to understand how well your classification model is performing. \n\nThis article will teach you what you need to know about confusion matrix and how to calculate it using Scikit-Learn.\n\nWhat is a Confusion Matrix in Machine Learning?\n\nImagine you are a detective trying to catch a thief. The confusion matrix is like your notebook, where you keep track of every clue and every mistake you make throughout your investigation. It’s a way of organizing the results of your model’s predictions so you can see where your model is getting things right and where it’s getting things wrong.\n\nFor a more formal definition, a confusion matrix or table of confusion is an evaluation table that displays the correct and incorrect predictions made by a classification model on a set of test data for which the true values are known. It is used for model evaluation and performance assessment.\n\nA confusion matrix produces four different outcomes: true positive, false positive, true negative, and false negative.\n• True Positive (TP): This is when the model correctly predicts the positive class. From the detective analogy, this is when you correctly identify the thief and catch them red-handed.\n• False Positive (FP): Also known as type 1 error, this is when the model incorrectly predicts the positive class. From the detective analogy, this is when you accuse an innocent person of being the thief.\n• True Negative (TN): This is when the model correctly predicts the negative class. From the detective analogy, this is when you correctly identify someone as innocent and they are indeed innocent.\n• False Negative (FN): Also known as type II error, this is when the model incorrectly predicts the negative class. From the detective analogy, this is when you fail to catch the real thief.\n\nThese four outcomes are represented in rows and columns as shown below, where:\n\nThe size of the confusion matrix depends on the number of classes the model is predicting.\n\nThat means for a binary classification problem (e.g. whether a person has a disease or not), the confusion matrix will be a 2x2 matrix.\n\nThen, for a multi-class classification, (e.g. classification of different colors, such as red, blue, and green), the confusion matrix for this example will be a 3×3 matrix.\n\nWhy is Confusion Matrix Important in Machine Learning?\n\nThe confusion matrix is important in machine learning because it helps you see the bigger picture. It’s not enough to get the right predictions from your model; it is also important to understand where your model is making mistakes. This information can help you refine the model and discover areas for improvement.\n\nFor example, if you have a classification model that predicts whether a patient has diabetes. By using the confusion matrix, you can see if the model is correctly identifying the patients who have diabetes (true positive) and if it’s incorrectly classifying healthy patients as diabetic(false positive). Information like this is crucial for doctors and researchers who rely on these predictions to make decisions about patient care and medical research.\n\nA good classification model will have high True Positives and True Negatives. A poor classification model will have high False Positives and False Negatives.\n\nYou can get other useful information from confusion matrix data, such as accuracy, precision, recall, and F1 score. These are all quality metrics that help you to evaluate your model’s performance. In this article, you will learn how to calculate these metrics for 2×2 and 3×3 confusion matrices.\n\nHow to Calculate Confusion Matrix in Python Using Scikit-Learn\n\nYou can use Scikit-Learn, a powerful open-source Python library for machine learning, to calculate the confusion matrix.\n\nWith the confusion_matrix() function from the metrics module in Scikit-Learn, you can calculate the confusion matrix based on the true and predicted labels. It’s straightforward, and I have shown how to use the module for a 2×2 and 3×3 confusion matrix below:\n\nStep 1: Run the following command to install the Scikit-Learn library (if you already have Python). If not, you will need to install Python first.\n\nStep 2: Import the metrics module. It includes the confusion_matrix function.\n\nI used sample actual and predicted values for the sake of this article. Assume that the binary classification model (2×2) predicts if a patient has diabetes or not. Assume that the 3×3 matrix is a model that classifies three colors: red, blue, and green.\n\nFor the binary classification problem above, notice how the values are only 0 and 1. This is because there are only two possible classes for a binary classification (2×2) problem. For example, in the context of the diabetes prediction that I mentioned above, 0 might represent no diabetes, while 1 might represent diabetes.\n\nAlso, notice that the actual and predicted values of the multi-class classification problem above(in this case, 3×3) range from 0,1,and 2. It follows the same convention as the binary classification. If you count it, 0,1, and 2 make up three different classes.\n\nThat means, for a 10x10 confusion matrix, the values won’t exceed 9. It’s always going to be n-1, where n is the number of classes in the matrix.\n\nFor the 2x2 confusion matrix(using the diabetes assumption):\n• True Positive = 4. This means that the model correctly identified 4 patients as having diabetes.\n• False Negative = 1. This means that the model incorrectly identified 1 patient as not having diabetes when the patient does.\n• False Positive = 0. This means that the model did not incorrectly identify any patient as having diabetes when they do not.\n• True Negative = 5. This means that the model correctly identified 5 patients of not having diabetes.\n\nFor the 3×3 confusion matrix(using the colors assumption):\n\nEach color represents a class. So, red is class 1, blue is class 2, and green is class 3. Each class has its own outcomes.\n• True Positive = 3. This means the model correctly classifies 3 instances as red.\n• False Negative = 0 + 0 = 0. This means the model does not incorrectly classify any instance as blue or green when the true class is red.\n\nYou get the False Negative of a 3×3 confusion matrix by adding the instances left in the column of the class after removing the True Positive.\n• False Positive = 0 + 0 = 0. This means the model does not incorrectly classify any instance as red.\n\nYou get the False Positive of a 3×3 confusion matrix by adding the instances left in the row of the class after removing the True Positive.\n• True Negative = 1 + 2 + 2 + 1 = 6. This means the model correctly predicts 6 instances as blue or green. That is, it accurately predicts the instances as not being red.\n\nYou get the True Negative of a 3×3 confusion matrix by adding the instances left after removing the rows and columns belonging to the specific class you are interpreting (in this case, the Red class).\n• False Negative = 0 + 2 = 2. The model incorrectly classifies 2 instances as red or green when the true class is blue.\n• True Negative = 3 + 0 + 0 + 1 = 4. The model correctly classifies 4 instances as red or green. That is, it accurately predicts the instances as not being blue.\n• False Negative = 0 + 2 = 2. The model incorrectly classifies 2 instances as red or blue when the true class is green.\n• True Negative = 0 + 1 + 0 + 2 = 3. The model correctly classifies 3 instances as red or blue. That is, it accurately predicts the instances as not being green.\n\nAccuracy: Accuracy tells you the proportion of correctly classified instances out of all the instances. So, if the model’s accuracy is 85%, it means it got 85 out of 100 predictions correct.\n\nThe formula for accuracy is:\n\nTo calculate accuracy for the 2×2 confusion matrix above:\n\nFor the 3×3 confusion matrix above, we calculate accuracy class-wise.\n\nPrecision: Precision tells you how many of the instances the model classified as positive, are actually positive.\n\nThe formula for precision is:\n\nTo calculate precison for the 2×2 confusion matrix above:\n\nPrecision = 4/(4+0) = 4/4 = 1. The model has a perfect precision of 100%.\n\nRecall: Recall measures how many of the actual positive instances the model managed to identify.\n\nTo calculate recall for the 2×2 confusion matrix above:\n\nRecall = 4/(4 + 1) = 4/5 = 0.8. The model has correctly identified 80% of individuals who are diabetic.\n\nFor the 3×3 matrix, you calculate class-wise.\n\nClass 1: Red = 3/(3 + 0) = 3/3 = 1. The model has correctly identified 100% of the instances as red.\n\nClass 2: Blue = 1/(1 + 2) = 1/3 = 0.33. The model has correctly identified 33.33% of the instances as blue.\n\nClass 3: Green = 1/(1 + 2) = 1/3 = 0.33. The model has correctly identified 33.33% of the instances as green.\n\nF1-Score: The F1-score is like a combination of precision and recall. It’s a way to balance both precision and recall into a single number. It’s useful when you want to find an optimal balance between precision and recall. As precision increases, recall goes down, and vice versa.\n\nThe formula for F1-Score is:\n\nTo calculate F1-Score for the 2×2 confusion matrix above:\n\nF1-Score = (2 × 1 × 0.8) / (1 + 0.8) = 1.6/1.8 = 0.8889. The model has an F1 score of 88.89%. This means there’s a good balance between precision and recall.\n\nClass 1: Red = (2 × 1 × 1) / ( 1 + 1) = 1. The model has an F1 score of 100%. This means there’s a perfect balance between precision and recall.\n\nClass 2: Blue = (2× 0.33 × 0.33) / (0.33 + 0.33) = 0.2178/0.66 = 0.33. The model has an F1 score of 33%. This means there’s a poor balance between precision and recall.\n\nClass 3: Green = (2 × 0.33 × 0.33) / (0.33 + 0.33) = 0.2178/0.66 = 0.33. The model has an F1 score of 33%. This means there’s a poor balance between precision and recall.\n\nBelow is a tabular compilation of the results for accuracy, precision, recall, and F1-Score, based on the 3×3 confusion matrix.\n\nFrom the table, the results of the second class (blue) and the third class (green) are poor. Hence, the model still needs more training and testing because it still struggles to distinguish instances correctly. The first class’s (red) result was better, but it could also use more training to improve its accuracy.\n\nThe confusion matrix is very important in machine learning. It helps you understand how well your classifiers are performing. By keeping track of your model’s predictions and outcomes, you can gain valuable insights that enable you to improve the accuracy and quality of your models.\n• What is a Confusion Matrix in Machine Learning? — Machine Learning Mastery\n• Machine Learning Classifiers - The Algorithms & How They Work — Monkey Learn\n• How to interpret a confusion matrix for a machine learning model—Evidently AI"
    },
    {
        "link": "https://scikit-learn.org/stable/modules/model_evaluation.html",
        "document": "Metrics and scoring: quantifying the quality of predictions#\n\nIn multilabel learning, each sample can have any number of ground truth labels associated with it. The goal is to give high scores and better rank to the ground truth labels. The function computes the average number of labels that have to be included in the final prediction such that all true labels are predicted. This is useful if you want to know how many top-scored-labels you have to predict in average without missing any true one. The best value of this metrics is thus the average number of true labels. Our implementation’s score is 1 greater than the one given in Tsoumakas et al., 2010. This extends it to handle the degenerate case in which an instance has 0 true labels. Formally, given a binary indicator matrix of the ground truth labels \\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\) and the score associated with each label \\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\), the coverage is defined as with \\(\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|\\). Given the rank definition, ties in are broken by giving the maximal rank that would have been assigned to all tied values. Here is a small example of usage of this function: The function implements label ranking average precision (LRAP). This metric is linked to the function, but is based on the notion of label ranking instead of precision and recall. Label ranking average precision (LRAP) averages over the samples the answer to the following question: for each ground truth label, what fraction of higher-ranked labels were true labels? This performance measure will be higher if you are able to give better rank to the labels associated with each sample. The obtained score is always strictly greater than 0, and the best value is 1. If there is exactly one relevant label per sample, label ranking average precision is equivalent to the mean reciprocal rank. Formally, given a binary indicator matrix of the ground truth labels \\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\) and the score associated with each label \\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\), the average precision is defined as where \\(\\mathcal{L}_{ij} = \\left\\{k: y_{ik} = 1, \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\), \\(\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|\\), \\(|\\cdot|\\) computes the cardinality of the set (i.e., the number of elements in the set), and \\(||\\cdot||_0\\) is the \\(\\ell_0\\) “norm” (which computes the number of nonzero elements in a vector). Here is a small example of usage of this function: The function computes the ranking loss which averages over the samples the number of label pairs that are incorrectly ordered, i.e. true labels have a lower score than false labels, weighted by the inverse of the number of ordered pairs of false and true labels. The lowest achievable ranking loss is zero. Formally, given a binary indicator matrix of the ground truth labels \\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\) and the score associated with each label \\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\), the ranking loss is defined as where \\(|\\cdot|\\) computes the cardinality of the set (i.e., the number of elements in the set) and \\(||\\cdot||_0\\) is the \\(\\ell_0\\) “norm” (which computes the number of nonzero elements in a vector). Here is a small example of usage of this function: # With the following prediction, we have perfect and minimal loss\n• None Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US. Discounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain (NDCG) are ranking metrics implemented in and ; they compare a predicted order to ground-truth scores, such as the relevance of answers to a query. From the Wikipedia page for Discounted Cumulative Gain: “Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks” DCG orders the true targets (e.g. relevance of query answers) in the predicted order, then multiplies them by a logarithmic decay and sums the result. The sum can be truncated after the first \\(K\\) results, in which case we call it DCG@K. NDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so that it is always between 0 and 1. Usually, NDCG is preferred to DCG. Compared with the ranking loss, NDCG can take into account relevance scores, rather than a ground-truth ranking. So if the ground-truth consists only of an ordering, the ranking loss should be preferred; if the ground-truth consists of actual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very relevant), NDCG can be used. For one sample, given the vector of continuous ground-truth values for each target \\(y \\in \\mathbb{R}^{M}\\), where \\(M\\) is the number of outputs, and the prediction \\(\\hat{y}\\), which induces the ranking function \\(f\\), the DCG score is and the NDCG score is the DCG score divided by the DCG score obtained for \\(y\\).\n• None Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May). A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013)\n• None McSherry, F., & Najork, M. (2008, March). Computing information retrieval performance measures efficiently in the presence of tied scores. In European conference on information retrieval (pp. 414-421). Springer, Berlin, Heidelberg.\n\nWhen doing supervised learning, a simple sanity check consists of comparing one’s estimator against simple rules of thumb. implements several such simple strategies for classification:\n• None always predicts the most frequent label in the training set.\n• None always predicts the class that maximizes the class prior (like ) and returns the class prior.\n• None always predicts a constant label that is provided by the user. A major motivation of this method is F1-scoring, when the positive class is in the minority. Note that with all these strategies, the method completely ignores the input data! To illustrate , first let’s create an imbalanced dataset: Next, let’s compare the accuracy of and : We see that doesn’t do much better than a dummy classifier. Now, let’s change the kernel: We see that the accuracy was boosted to almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the Cross-validation: evaluating estimator performance section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator section for details. More generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc… also implements four simple rules of thumb for regression:\n• None always predicts the mean of the training targets.\n• None always predicts the median of the training targets.\n• None always predicts a user provided quantile of the training targets.\n• None always predicts a constant value that is provided by the user. In all these strategies, the method completely ignores the input data."
    },
    {
        "link": "https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case",
        "document": "I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;).\n\nThe weights from the parameter are used to train the classifier. They are not used in the calculation of any of the metrics you are using: with different class weights, the numbers will be different simply because the classifier is different.\n\nBasically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.\n\n How they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it.\n\nOnce you have a classifier, you want to know how well it is performing. Here you can use the metrics you mentioned: , , ...\n\nUsually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class.\n\nI will not detail all these metrics but note that, with the exception of , they are naturally applied at the class level: as you can see in this of a classification report they are defined for each class. They rely on concepts such as or that require defining which class is the positive one.\n\nYou get this warning because you are using the f1-score, recall and precision without defining how they should be computed! The question could be rephrased: from the above classification report, how do you output one global number for the f1-score? You could:\n• Take the average of the f1-score for each class: that's the result above. It's also called macro averaging.\n• Compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class). Aka micro averaging.\n• Compute a weighted average of the f1-score. Using in scikit-learn will weigh the f1-score by the support of the class: the more elements a class has, the more important the f1-score for this class in the computation.\n\nThese are 3 of the options in scikit-learn, the warning is there to say you have to pick one. So you have to specify an argument for the score method.\n\nWhich one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5.\n\nThe whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it.\n\nLast thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier has never seen. This is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant.\n\nHere's a way to do it using , which gives you a random splits of your data (after shuffling) that preserve the label distribution."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning",
        "document": "Have you expected great results from your machine learning model, only to get poor accuracy? You’ve put in the effort, so what went wrong? How can you fix it? There are many ways to assess your classification model, but the confusion matrix is one of the most reliable option. It shows how well your model performed and where it made errors, helping you improve. Beginners often find the confusion matrix confusing, but it’s actually simple and powerful. This tutorial will explain what a confusion matrix in machine learning is and how it provides a complete view of your model’s performance.\n\nDespite its name, you’ll see that a confusion matrix is straightforward and effective. Let’s explore the confusion matrix together!\n\nIn this article, you will explore the confusion matrix formula and its significance in analyzing confusion metrics. We will delve into the role of the confusion matrix in deep learning and its applications in AI, providing a comprehensive understanding of model performance evaluations.\n• Learn what a confusion matrix is and understand the various terms related to it.\n• Learn to use a confusion matrix for multi-class classification.\n• Learn to implement a confusion matrix using scikit-learn in Python.\n\nLearning the ropes in the machine learning field? These courses will get you on your way:\n\nA confusion matrix is a performance evaluation tool in machine learning, representing the accuracy of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives. This matrix aids in analyzing model performance, identifying mis-classifications, and improving predictive accuracy.\n\nA Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the total number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n\nFor a binary classification problem, we would have a 2 x 2 matrix, as shown below, with 4 values:\n• The target variable has two values: Positive or Negative\n• The columns represent the actual values of the target variable\n• The rows represent the predicted values of the target variable\n\nBut wait – what’s TP, FP, FN, and TN here? That’s the crucial part of a confusion matrix. Let’s understand each term below.\n• The predicted value matches the actual value, or the predicted class matches the actual class.\n• The actual value was positive, and the model predicted a positive value.\n• The predicted value matches the actual value, or the predicted class matches the actual class.\n• The actual value was negative, and the model predicted a negative value.\n• The predicted value was falsely predicted.\n• The actual value was negative, but the model predicted a positive value.\n• Also known as the type I error.\n• The predicted value was falsely predicted.\n• The actual value was positive, but the model predicted a negative value.\n• Also known as the type II error.\n\nLet me give you an example to better understand this. Suppose we had a classification dataset with 1000 data points. We fit a classifier (say logistic regression or decision tree) on it and get the below confusion matrix:\n\nThe different values of the Confusion matrix would be as follows:\n• False Positive (FP) = 60, meaning the model incorrectly classified 60 negative class data points as belonging to the positive class.\n• False Negative (FN) = 50, meaning the model incorrectly classified 50 positive class data points as belonging to the negative class.\n\nThis turned out to be a pretty decent classifier for our dataset, considering the relatively larger number of true positive and true negative values.\n\nRemember the Type I and Type II errors. Interviewers love to ask the difference between these two! You can prepare for all this better from our Machine learning Course Online.\n\nWhy Do We Need a Confusion Matrix?\n\nBefore we answer this question, let’s think about a hypothetical classification problem.\n\nLet’s say you want to predict how many people are infected with a contagious virus in times before they show the symptoms and isolate them from the healthy population (ringing any bells, yet?). The two values for our target variable would be Sick and Not Sick.\n\nNow, you must be wondering why we need a confusion matrix when we have our all-weather friend – Accuracy. Well, let’s see where classification accuracy falters.\n\nOur dataset is an example of an imbalanced dataset. There are 947 data points for the negative class and 3 data points for the positive class. This is how we’ll calculate the accuracy:\n\nLet’s see how our model performed:\n\nSo, the accuracy of our model turns out to be:\n\nBut it gives the wrong idea about the result. Think about it.\n\nOur model is saying, “I can predict sick people 96% of the time”. However, it is doing the opposite. It predicts the people who will not get sick with 96% accuracy while the sick are spreading the virus!\n\nDo you think this is a correct metric for our model, given the seriousness of the issue? Shouldn’t we be measuring how many positive cases we can predict correctly to arrest the spread of the contagious virus? Or maybe, out of the correct predictions, how many are positive cases to check the reliability of our model?\n\nThis is where we come across the dual concept of Precision and Recall.\n\nHow to Calculate Confusion Matrix for a 2-class Classification Problem?\n\nTo calculate the confusion matrix for a 2-class classification problem, you will need to know the following:\n• True positives (TP): The number of samples that were correctly predicted as positive.\n• True negatives (TN): The number of samples that were correctly predicted as negative.\n• False positives (FP): The number of samples that were incorrectly predicted as positive.\n• False negatives (FN): The number of samples that were incorrectly predicted as negative.\n\nOnce you have these values, you can calculate the confusion matrix using the following table:\n\nHere is an example of how to calculate the confusion matrix for a 2-class classification problem:\n\nThe confusion matrix can be used to calculate a variety of metrics, such as accuracy, precision, recall, and F1 score.\n\nThis would determine whether our model is reliable or not.\n\nAnd here’s how we can calculate Recall:\n\nWe can easily calculate Precision and Recall for our model by plugging in the values into the above questions:\n\n50% percent of the correctly predicted cases turned out to be positive cases. Whereas 75% of the positives were successfully predicted by our model. Awesome!\n\nPrecision is important in music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.\n\nRecall is important in medical cases where it doesn’t matter whether we raise a false alarm, but the actual positive cases should not go undetected!\n\nIn our example, when dealing with a contagious virus, the Confusion Matrix becomes crucial. Recall, assessing the ability to capture all actual positives, emerges as a better metric. We aim to avoid mistakenly releasing an infected person into the healthy population, potentially spreading the virus. This context highlights why accuracy proves inadequate as a metric for our model’s evaluation. The Confusion Matrix, particularly focusing on recall, provides a more insightful measure in such critical scenarios\n\nBut there will be cases where there is no clear distinction between whether Precision is more important or Recall. What should we do in those cases? We combine them!\n\nIn practice, when we try to increase the precision of our model, the recall goes down, and vice-versa. The F1-score captures both the trends in a single value:\n\nF1-score is a harmonic mean of Precision and Recall, and so it gives a combined idea about these two metrics. It is maximum when Precision is equal to Recall.\n\nBut there is a catch here. The interpretability of the F1-score is poor. This means that we don’t know what our classifier is maximizing – precision or recall. So, we use it in combination with other evaluation metrics, giving us a complete picture of the result.\n\nYou know the theory – now let’s put it into practice. Let’s code a confusion matrix with the Scikit-learn (sklearn) library in Python.\n\nSklearn has two great functions: confusion_matrix() and classification_report().\n• Sklearn confusion_matrix() returns the values of the Confusion matrix. The output is, however, slightly different from what we have studied so far. It takes the rows as Actual values and the columns as Predicted values. The rest of the concept remains the same.\n• Sklearn classification_report() outputs precision, recall, and f1-score for each target class. In addition to this, it also has some extra values: micro avg, macro avg, and weighted avg\n\nMirco average is the precision/recall/f1-score calculated for all the classes.\n\nMacro average is the average of precision/recall/f1-score.\n\n\n\nWeighted average is just the weighted average of precision/recall/f1-score.\n\nHow would a confusion matrix in machine learning work for a multi-class classification problem? Well, don’t scratch your head! We will have a look at that here.\n\nLet’s draw a confusion matrix for a multiclass problem where we have to predict whether a person loves Facebook, Instagram, or Snapchat. The confusion matrix would be a 3 x 3 matrix like this:\n\nThe true positive, true negative, false positive, and false negative for each class would be calculated by adding the cell values as follows:\n\nThat’s it! You are ready to decipher any N x N confusion matrix!\n\nThe Confusion matrix is not so confusing anymore, is it?\n\nHope this article gave you a solid base on how to interpret and use a confusion matrix for classification algorithms in machine learning. The matrix helps in understanding where the model has gone wrong and gives guidance to correct the path and it is a powerful and commonly used tool to evaluate the performance of a classification model in machine learning.\n\nWe will soon come out with an article on the AUC-ROC curve and continue our discussion there. Until next time, don’t lose hope in your classification model; you just might be using the wrong evaluation metric!\n• True Positive and True Negative values mean the predicted value matches the actual value.\n• A Type I Error happens when the model makes an incorrect prediction, as in, the model predicted positive for an actual negative value.\n• A Type II Error happens when the model makes an incorrect prediction of an actual positive value as negative."
    },
    {
        "link": "https://geeksforgeeks.org/confusion-matrix-machine-learning",
        "document": "Machine learning models are increasingly used in various applications to classify data into different categories. However evaluating the performance of these models is crucial to ensure their accuracy and reliability. One essential tool in this evaluation process is the confusion matrix. In this article we will work on confusion matrix, its significance in machine learning and how it can be used to improve the performance of classification models.\n\nA confusion matrix is a simple table that shows how well a classification model is performing by comparing its predictions to the actual results. It breaks down the predictions into four categories: correct predictions for both classes (true positives and true negatives) and incorrect predictions (false positives and false negatives). This helps you understand where the model is making mistakes, so you can improve it.\n\nThe matrix displays the number of instances produced by the model on the test data.\n• True Positive (TP): The model correctly predicted a positive outcome (the actual outcome was positive).\n• True Negative (TN): The model correctly predicted a negative outcome (the actual outcome was negative).\n• False Positive (FP): The model incorrectly predicted a positive outcome (the actual outcome was negative). Also known as a Type I error.\n• False Negative (FN): The model incorrectly predicted a negative outcome (the actual outcome was positive). Also known as a Type II error.\n\nA confusion matrix helps you see how well a model is working by showing correct and incorrect predictions. It also helps calculate key measures like accuracy, precision, and recall, which give a better idea of performance, especially when the data is imbalanced.\n\nAccuracy measures how often the model’s predictions are correct overall. It gives a general idea of how well the model is performing. However, accuracy can be misleading, especially with imbalanced datasets where one class dominates. For example, a model that predicts the majority class correctly most of the time might have high accuracy but still fail to capture important details about other classes.\n\nPrecision focuses on the quality of the model’s positive predictions. It tells us how many of the instances predicted as positive are actually positive. Precision is important in situations where false positives need to be minimized, such as detecting spam emails or fraud.\n\nRecall measures how well the model identifies all actual positive cases. It shows the proportion of true positives detected out of all the actual positive instances. High recall is essential when missing positive cases has significant consequences, such as in medical diagnoses.\n\nF1-score combines precision and recall into a single metric to balance their trade-off. It provides a better sense of a model’s overall performance, particularly for imbalanced datasets. The F1 score is helpful when both false positives and false negatives are important, though it assumes precision and recall are equally significant, which might not always align with the use case.\n\nSpecificity is another important metric in the evaluation of classification models, particularly in binary classification. It measures the ability of a model to correctly identify negative instances. Specificity is also known as the True Negative Rate. Formula is given by:\n• Type 1 Error occurs when the model incorrectly predicts a positive instance, but the actual instance is negative. This is also known as a false positive precision of a model, which measures the accuracy of positive predictions. \n\n[Tex]\\text{Type 1 Error} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} [/Tex]\n• Type 2 error\n\n Type 2 Error occurs when the model fails to predict a positive instance, even though it is actually positive. This is also known as a false negative recall of a model, which measures how well the model identifies all actual positive cases. \n\n[Tex]\\text{Type 2 Error} = \\frac{FN}{TP+FN}[/Tex]\n\nExample: A diagnostic test is used to detect a particular disease in patients.\n• Type 1 Error (False Positive): This occurs when the test predicts a patient has the disease (positive result), but the patient is actually healthy (negative case).\n• Type 2 Error (False Negative): This occurs when the test predicts the patient is healthy (negative result), but the patient actually has the disease (positive case).\n\nA 2X2 Confusion matrix is shown below for the image recognition having a Dog image or Not Dog image:\n• True Positive (TP): It is the total counts having both predicted and actual values are Dog.\n• True Negative (TN): It is the total counts having both predicted and actual values are Not Dog.\n• False Positive (FP): It is the total counts having prediction is Dog while actually Not Dog.\n• False Negative (FN): It is the total counts having prediction is Not Dog while actually, it is Dog.\n\nExample: Confusion Matrix for Dog Image Recognition with Numbers\n\nImplementation of Confusion Matrix for Binary classification using Python\n\nStep 2: Create the NumPy array for actual and predicted labels\n• None represents the true labels or the actual classification of the items. In this case, it’s a list of 10 items, where each entry is either\n• None represents the predicted labels or the classification made by the model.\n• None computes the confusion matrix, which is a table used to evaluate the performance of a classification algorithm.\n\nStep 4: Plot the confusion matrix with the help of the seaborn heatmap\n• Seaborn is used to create a heatmap of the confusion matrix.\n• None Displays the numerical values in each cell of the heatmap.\n\nIn multi-class classification the confusion matrix is expanded to account for multiple classes.\n• None Each cell in the matrix shows how often a specific actual class was predicted as another class.\n\nFor example, in a 3-class problem, the confusion matrix would be a 3×3 table, where each row and column corresponds to one of the classes. It summarizes the model’s performance across all classes in a compact format.\n\nLets consider that example:\n• None The definitions of all the terms (TP, TN, FP and FN) are the same as described in the previous example.\n\nLet’s consider the scenario where the model processed 30 images:\n• Cats: 8 were correctly identified, 1 was misidentified as a dog, and 1 was misidentified as a horse.\n• Dogs: 10 were correctly identified, 2 were misidentified as cats.\n• Horses: 8 were correctly identified, 2 were misidentified as dogs.\n\nTo calculate true negatives, we need to know the total number of images that were NOT cats, dogs, or horses. Let’s assume there were 10 such images, and the model correctly classified all of them as “not cat,” “not dog,” and “not horse.”\n• True Negative (TN) Counts: 10 (for each class, as the model correctly identified each non-cat/dog/horse image as not belonging to that class)\n\nImplementation of Confusion Matrix for Multi-Class classification using Python\n\nStep 2: Create the NumPy array for actual and predicted labels\n• None List of predicted labels by the model.\n\nConfusion matrix is a valuable tool for evaluating how well a classification model works. It provides clear insights into important metrics like accuracy, precision, and recall by analyzing correct and incorrect predictions (true positives, true negatives, false positives, and false negatives). This article explained these metrics with examples and showed how to create confusion matrices in Python for both binary and multi-class classification. By understanding and using these metrics, practitioners can make better decisions about model performance, especially when dealing with imbalanced datasets.\n\nWhy is a confusion matrix useful?\n\nWhat are some examples of confusion matrix application?\n\nWhat does a confusion matrix diagram look like?"
    }
]