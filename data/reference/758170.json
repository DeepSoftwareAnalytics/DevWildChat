[
    {
        "link": "https://learn.microsoft.com/en-us/dotnet/desktop/winforms/input-mouse/how-to-simulate-events?view=netdesktop-9.0",
        "document": "Simulating mouse events in Windows Forms isn't as straight forward as simulating keyboard events. Windows Forms doesn't provide a helper class to move the mouse and invoke mouse-click actions. The only option for controlling the mouse is to use native Windows methods. If you're working with a custom control or a form, you can simulate a mouse event, but you can't directly control the mouse.\n\nMost events have a corresponding method that invokes them, named in the pattern of followed by , such as . This option is only possible within custom controls or forms, because these methods are protected and can't be accessed from outside the context of the control or form. The disadvantage to using a method such as is that it doesn't actually control the mouse or interact with the control, it simply raises the associated event. For example, if you wanted to simulate hovering over an item in a ListBox, and the doesn't visually react with a highlighted item under the cursor.\n\nThese protected methods are available to simulate mouse events.\n\nFor more information about these events, see Using mouse events (Windows Forms .NET)\n\nConsidering most controls do something when clicked, like a button calling user code, or checkbox change its checked state, Windows Forms provides an easy way to trigger the click. Some controls, such as a combobox, don't do anything special when clicked and simulating a click has no effect on the control.\n\nThe System.Windows.Forms.IButtonControl interface provides the PerformClick method which simulates a click on the control. Both the System.Windows.Forms.Button and System.Windows.Forms.LinkLabel controls implement this interface.\n\nWith a form a custom control, use the InvokeOnClick method to simulate a mouse click. This is a protected method that can only be called from within the form or a derived custom control.\n\nFor example, the following code clicks a checkbox from .\n\nWindows provides methods you can call to simulate mouse movements and clicks such as and . The following example moves the mouse cursor to the center of a control:\n• Overview of using the mouse (Windows Forms .NET)\n• How to distinguish between clicks and double-clicks (Windows Forms .NET)"
    },
    {
        "link": "https://learn.microsoft.com/en-us/dotnet/standard/native-interop/pinvoke",
        "document": "P/Invoke is a technology that allows you to access structs, callbacks, and functions in unmanaged libraries from your managed code. Most of the P/Invoke API is contained in two namespaces: and . Using these two namespaces give you the tools to describe how you want to communicate with the native component.\n\nLet's start from the most common example, and that is calling unmanaged functions in your managed code. Let's show a message box from a command-line application:\n\nThe previous example is simple, but it does show off what's needed to invoke unmanaged functions from managed code. Let's step through the example:\n• Line #2 shows the directive for the namespace that holds all the items needed.\n• Line #8 introduces the LibraryImportAttribute attribute. This attribute tells the runtime that it should load the unmanaged binary. The string passed in is the unmanaged binary that contains the target function. Additionally, it specifies the encoding to use for marshalling the strings. Finally, it specifies that this function calls SetLastError and that the runtime should capture that error code so the user can retrieve it via Marshal.GetLastPInvokeError().\n• Line #9 is the crux of the P/Invoke work. It defines a managed method that has the exact same signature as the unmanaged one. The declaration uses the attribute and the keyword to tell a compiler extension to generate code to call into the unmanaged library.\n• Within the generated code and prior to .NET 7, the is used. This declaration uses the keyword to indicate to the runtime this is an external method, and that when you invoke it, the runtime should find it in the unmanaged binary specified in the attribute.\n\nThe rest of the example is invoking the method as you would any other managed method.\n\nThe sample is similar for macOS. The name of the library in the attribute needs to change since macOS has a different scheme of naming dynamic libraries. The following sample uses the function to get the process ID of the application and print it out to the console:\n\nIt is also similar on Linux. The function name is the same, since is a standard POSIX system call.\n\nThe runtime allows communication to flow in both directions, enabling you to call back into managed code from native functions by using function pointers. The closest thing to a function pointer in managed code is a delegate, so this is what is used to allow callbacks from native code into managed code.\n\nThe way to use this feature is similar to the managed to native process previously described. For a given callback, you define a delegate that matches the signature and pass that into the external method. The runtime will take care of everything else.\n\nBefore walking through the example, it's good to review the signatures of the unmanaged functions you need to work with. The function to be called to enumerate all of the windows has the following signature:\n\nThe first parameter is a callback. The said callback has the following signature:\n\nNow, let's walk through the example:\n• Line #9 in the example defines a delegate that matches the signature of the callback from unmanaged code. Notice how the LPARAM and HWND types are represented using in the managed code.\n• Lines #13 and #14 introduce the function from the user32.dll library.\n• Lines #17 - 20 implement the delegate. For this simple example, we just want to output the handle to the console.\n• Finally, in line #24, the external method is called and passed in the delegate.\n\nThe Linux and macOS examples are shown below. For them, we use the function that can be found in , the C library. This function is used to traverse directory hierarchies and it takes a pointer to a function as one of its parameters. The said function has the following signature: .\n\nmacOS example uses the same function, and the only difference is the argument to the attribute, as macOS keeps in a different place.\n\nBoth of the previous examples depend on parameters, and in both cases, the parameters are given as managed types. Runtime does the \"right thing\" and processes these into its equivalents on the other side. Learn about how types are marshalled to native code in our page on Type marshalling."
    },
    {
        "link": "https://stackoverflow.com/questions/7055252/how-to-simulate-a-mouseclick-in-c-i-keep-getting-an-error",
        "document": "Alright guys, well, here is the thing. I used this wonderful website and found a code snippet that helped me create a basic clicking script. The problem is that I keep getting an error when debugging - pointing right at my mouseclick line. Below, I've added the error in code tags.\n\nI've given it my best effort, and gone through a couple google searches .. but I can't find anything, maybe I'm just doing it wrong. Either way, I'd love if you guys could help me. Below is my actual code."
    },
    {
        "link": "https://tevora.com/threat-blog/dynamic-invocation-in-csharp",
        "document": "This post serves as introduction to payload development, following up on concepts in basic C# payload development such as the usage of Windows APIs, platform invocation (P/Invoke), and how basic Windows APIs tie together to perform shellcode execution. There are many approaches to payload/stager development, with multiple options of language, frameworks, etc. This post will focus on C#.\n\nFirst, credit is due to @The Wover and @FuzzySec for the creation of dynamic invocation (D/Invoke) as well as providing a blog post that goes in depth on the fundamentals of D/Invoke. The linked blog post provides an excellent background into the negatives of P/Invoke as a way of using Windows API calls. It states that:\n• Any reference to a Windows API call made through P/Invoke will result in a corresponding entry in the .NET Assembly’s Import Table. When your .NET Assembly is loaded, its Import Address Table will be updated with the addresses of the functions that you are calling.\n• If the endpoint security product running on the target machine is monitoring API calls (such as via API Hooking), then any calls made via P/Invoke may be detected by the product.\n\nThe blog also includes information on how D/Invoke is integrated with SharpSploit, but doesn’t cover how we can implement it into our own C# payloads. While it also provides a Github repo as an example of integrating D/Invoke, the lack of documentation may make it hard to follow.\n\nIn this blog post, we’ll walk through an example of our own, while also looking at the multiple different methods available to us.\n\nTo begin, rather than straight up integrating the original D/Invoke project into our payloads, we can use a fork provided by Rastamouse, which contains only the minimum core DynamicInvoke and ManualMap functionality. This fork omits the default D/Invoke Injection folder as well as several Windows APIs.\n\nWe will use and make edits to this fork in this post.\n\nUsing this fork, rather than the original project, maintains a lower profile and brings us a step toward evading any sort of detection. To go along with this, Rastamouse has provided a wiki to document API usage with D/Invoke.\n\nLet’s go ahead and start building the framework for our payload. We’ll begin by creating a new project in Visual Studio, making sure to select Console App (.NET Framework) C#. Because we are building our payload specifically for Windows machines, we don’t need the cross-platform capability that .NET core brings, and instead use .NET Framework because the capabilities align with what we’re trying to accomplish. Additionally, .NET Core is not installed by default, whereas .NET Framework is, so we are much less likely to run into compatibility issues. Nevertheless, as long as we are using .NET, our application builds as Assemblies, meaning we can tinker with different execution methods and techniques such as reflective loading. A deeper dive into assemblies can be found within the MSDN documentation here: https://docs.microsoft.com/en-us/dotnet/standard/assembly/.\n\nFrom here, we’ll need to create three folders to house the three different D/Invoke namespaces.\n\nWe can name the folders like so:\n\nNow we’ll need to import the C# files into their respective folders. The final result should look something like this:\n\nWe can test that the D/Invoke files have been added correctly by attempting to use the GetApiHash method from the Generic class of the DInvoke.DynamicInvoke namespace.\n\nNo errors indicates we are all set up and can begin playing around with D/Invoke!\n\nIf we do some quick analysis on the D/Invoke library, we find that the DInvoke.DynamicInvoke folder contains the main, core D/Invoke functionality. We can infer and confirm that the Native class contains Windows Native APIs, such as Nt* functions, while the Win32 class contains Win32 APIs. Interestingly, the only Win32 API included in the RastaMouse fork is (CreateRemoteThread, OpenProcess, IsWow64Process for default D/Invoke library). This begs the question: why aren’t common payload development functions like VirtualAlloc, CreateThread, and VirtualProtect included in the D/Invoke library by default?\n\nJust because the D/Invoke library and dinvoke.net don’t include functions that we just mentioned such as , , and does not mean we can’t use them. In fact, we can create these delegates ourselves since we know the function prototypes and can actually just port them over from the P/Invoke library. You can think of a delegate as a pointer to a function, allowing us to pass methods as arguments to other methods. This is how we can perform dynamic address lookups for relevant Windows APIs and pass the function prototype parameters. This concept, Native interoperability, is required because we are working with unmanaged code (Win32 APIs) from a managed language in C# and .NET. An explanation for this can be found here.\n\nTo start, we’ll add our Win32 APIs to the DInvoke.DynamicInvoke.Win32 class. To add an API, we’ll need to create a method and a corresponding delegate. The delegate is essentially the function prototype that we are used to seeing, which we can grab straight from P/Invoke. P/Invoke defines the C# signature of VirtualAlloc as:\n\nIt stands to assume that we can create a delegate with the exact same arguments.\n\nWe are using the calling convention since it is used to call Win32 API functions in C#.\n\nOnce we have the delegate set up, we’ll need to create a managed wrapper method that will be called when we want to use the Win32 API.\n\nThe delegate, which is the delegate we created that houses the function prototype for VirtualAlloc, is passed to DInvoke.DynamicInvoke.Generic.DynamicApiInvoke, which performs a dynamic lookup of VirtualAlloc, grabs the pointer to VirtualAlloc, and passes it to DInvoke.DynamicInvoke.Generic.DynamicFunctionInvoke, where the function wrapped by the delegate is invoked along with the parameters that were passed.\n\nHere is what our Win32 wrapper class looks like:\n\nIf we did everything correctly, we should be able to call VirtualAlloc without any errors like so:\n\nNice! We can now replicate the above steps for and .\n\nWe can call the functions like so:\n\nTo test our code, we can use the Metasploit messagebox payload so we won’t have to stand up a C2 framework for testing purposes.\n\nAlternatively you can copy this byte array here:\n\nNote that since we’re going through a proof of concept on how to utilize D/Invoke, we’re testing our payload with Defender off and using unobfuscated shellcode. Be sure after this to implement some sort of obfuscation to your payload to bypass AV!\n\nWhen attempting to build this solution, we may run into architecture errors.\n\nWe can change the language version to the latest (10 at the time of this writing) by unloading the project, editing the project file, changing LangVersion to 10.0, and reloading the project.\n\nNow we can build the solution, and when we execute it, we see message box popping up!\n\nNow that we effectively recreated a basic C# payload, which uses D/Invoke rather than P/Invoke, let us analyze the differences between the two.\n\nIn order to compare the differences between P/Invoke and D/Invoke, we need a valid P/invoke call, so we will use P/Invoke to import the VirtualAlloc API and include a call to VirtualAlloc, storing the resulting memory address in the parameter. Our D/Invoke call will still point to the parameter. We’ll also print these addresses to the console so we can perform searches on them.\n\nNow that our payload includes both a P/Invoke and D/Invoke call, we can execute it within API Monitor, a free tool that identifies all API calls made within the process. When configuring API monitor, we want to “hook” kernel32.dll to observe the API calls.\n\nWe can to go the API Filter box in the top left, search for kernel32.dll, and check all the categories.\n\nIn the Monitored Process window, we can click Monitor New Process and attach our EXE, using the Remote Thread Attach method.\n\nWe can see our messagebox has popped, the command prompt output gives us pointers to memory addresses that VirtualAlloc has created, and the Summary window in API monitor has been populated.\n\nWe can use the console printed addresses and search for the memory address in the Summary window. Let’s start with the P/Invoke address first.\n\nKeep in mind we are looking for a VirtualAlloc API call with the set to the size of your buffer (in my case 294) as well as set to .\n\nWe can see clearly here that API monitor picks up our P/Invoke VirtualAlloc API call.\n\nLet’s perform the same search with the D/Invoke address. After we replace the “Find what” value and keep clicking through “Find next,” we aren’t able to find any VirtualAlloc API call with a size of 294.\n\nThis confirms that the usage of D/Invoke aids in bypassing AV and EDR tools that monitor suspicious API calls. Let’s also take a look at the PE headers.\n\nWhen executables are built and code is compiled, the result is a Portable Executable (PE), which follows the PE file format. The PE format consists of many different structures and headers, but generally follows the same format between each executable. The aforementioned IAT is one such section of the PE format which is used by the Windows loader to locate DLLs and functions and update the executable with the corresponding addresses. This is apparent in native Window’s applications like calc.exe.\n\nHowever, a .NET executable works a little bit differently. First off, the advantage of using .NET framework and creating .NET executables is usage of the Common Language Runtime (CLR) and access to class libraries included in the framework. In addition, while the compiled .NET executable also follows the PE format, the structures are slightly different. When a .NET program is compiled with a C# compiler, the result is a managed module. A managed module (which is a standard PE file) also contains a CLR header, metadata tables, and IL code. When a .NET program is executed, the Windows loader calls the function from mscoree.dll, which initializes the CLR, checks the CLR header for the managed entry point, and begins execution. The CLR, once loaded, takes care of execution, looking at the Metadata tables and compiling IL code into native CPU instructions. Because of this, a .NET executable won’t have an IAT.\n\nGoing back to native Window’s applications, we can understand that the lack of CLR means the Windows loader must walk through the entire PE format, which includes the data directories containing the IAT.\n\nThe following image from MSDN shows how .NET programs with P/Invoke use Metadata to locate exported functions.\n\nWithin the .NET metadata, there are 45 tables containing various amounts of information.\n\nThe table is particularly interesting:\n\nThis is where our P/Invoke import can be seen, and is likely to be inspected by EDR upon knowledge that our application is a .NET executable.\n\nNow that we understand the differences between Native and .NET executables with regards to the PE format, let’s analyze and see those differences in realtime. We will use CFF explorer, a PE editor, to take a look at the PE between our .NET executable and a native Windows executable.\n\nFirst, looking at the native calc.exe application, we can see the Import Directory folder, with the kernel32.dll imported and the relevant Windows APIs below.\n\nNow let’s take a look at our .NET application. We see the lack of an Import Directory folder, but the inclusion of a .NET directory folder containing the various metadata tables. In the table, we see the one P/Invoke API that we included in our code. But more importantly, we do not see the D/Invoke APIs here.\n\nWhile these changes will help attacking several AV/EDR solutions, it might not bypass all types of API hooking, since API hooks placed on kernel32.dll may still detect suspicious use of API calls. To improve our payload, we can look into Manual Mapping.\n\nManual mapping can help us avoid API hooking. We can begin by mapping of fresh copy of kernel32.dll, so that if there are any hooks placed on originally loaded kernel32.dll, we call VirtualAlloc, CreateThread, and WaitForSingleObject from within our manually mapped kernel32.dll.\n\nTo do this, we’ll create a object containing the kernel32.dll from disk.\n\nD/Invoke offers us a couple of options for manual mapping:\n\nWe can see here that the D/Invoke function here is slightly different, as instead of calling DynamicAPIInvoke (which locates the function pointer to the API through GetLibraryAddress), uses GetExportAddress (which uses the base address of the manually mapped kernel32.dll).\n\nIn the Native class of the main DInvoke.DynamicInvoke namespace, we see comments about how the Delegates structure must be public so they may be used with DynamicFunctionInvoke. Since that is exactly what we will be using, we must change the Delegates structure in our Win32 class to be Public.\n\nBefore we call the function we’ll need to create an object array for the parameters we want to pass to DynamicFunctionInvoke. We can take advantage of the built in structures that are included in the D/Invoke library.\n\nreturns an object, so we’ll need to cast it to an IntPtr (the VirtualAlloc expected return type).\n\nFrom here, we can comment out our original VirtualAlloc call, build and run our payload, and pop a messagebox.\n\nWe should be able to follow the same process for and . Make sure you call before executing CreateThread otherwise you’re not going to be executing anything!\n\nLet’s take a step back and replace our D/Invoke call with P/Invoke and see what happens in API Monitor. I can simply comment out my D/Invoke call, import P/Invoke’s VirtualAlloc, and replace the variable with a VirtualAlloc call.\n\nOnce it’s replaced, we can compile it. In API Monitor, we were previously only monitoring kernel32.dll. Let’s also monitor ntdll.dll this time by searching for it in the API filter and checking all the categories under ntdll.dll.\n\nNow when we execute our test payload in API Monitor, we can once again search for the the VirtualAlloc call with a size of our payload and permissions.\n\nNow that we’re Monitoring kernel32.dll and ntdll.dll exported APIs, we see something interesting here.\n\nThis VirtualAlloc call is the one from our payload, has a of 294, and has permissions. We also see an underlying ntdll.dll call, .\n\nWe can look into the parameters of NtAllocateVirtualMemory to confirm this is a result of our VirtualAlloc code.\n\nA little sidebar here to explain why it works this way. There are 4 different privilege levels, known as rings, that control access to memory and CPU operations. Ring 0 (kernel mode) is most privileged, ring 3 (user mode) is least privileged. A majority of user activity occurs in Ring 3, but applications may cross into Ring 0 when calling variety of APIs (think accessing file system). User applications generally call high-level APIs (kernel32, user32) and those APIs will call low-level APIs (ntdll). Ntdll.dll is considered a “bridge” between user land and kernel land because Nt* functions exported from ntdll.dll are essentially wrappers for system calls (syscall). Syscalls are how a program requests a service from the kernel and are essentially what our code is executing. The following diagram outlines user land and kernel land:\n\nAnd an Nt* function merely acting as a wrapper for a syscall is shown in this unassembled NtAllocateVirtualMemory call:\n\nIn this example, the syscall number (18h) is pushed to the EAX register (function return value). A syscall is then invoked, requesting the kernel to allocate memory.\n\nSince NtAllocateVirtualMemory is an ntdll.dll function, it stands to assume that manually mapping kernel32.dll may bypass any EDR using kernel32.dll API hooks. But if an EDR hooks ntdll.dll (which is extremely common and considered the norm nowadays), even if our VirtualAlloc call may be undetected, we will still get caught by the underlying NtAllocateVirtualMemory call. Since we’re not actually programming this call ourselves manually in the payload, even if we were to manually map ntdll.dll into our payload, there’s no way for us to control this API call.\n\nWe can see this concept come to fruition even if we were replace this P/Invoke call with our manually mapped VirtualAlloc call. For this, we can use Frida. Frida is a Python module that allows us to hook many common runtimes including .NET and allows us to customize how we want to interpret function arguments and return values in JavaScript. To install it on our Windows development box, just ensure Python is installed.\n\nYou may need to add Python to your PATH, in which afterwards, you may invoke Frida like so:\n\nTo let Frida hook into our application, we’ll add a couple lines of code that outputs the process ID (which must be passed to Frida) as well as a simple pause in our application, that resumes upon user input.\n\nWe’ll add this to the beginning of the Main() method.\n\nFrom here, we can compile the application and run it, taking note of the process ID. In another command prompt, we can pass the process ID to Frida, and tell it to hook ntdll.dll and monitor any NtAllocateVirtualMemory API calls.\n\nWe can see a JavaScript handler file created for NtAllocateVirtualMemory, which by default, does not give us all the information we need. At this point, we can browse to that JS file and edit it like so:\n\nWhen the application is about to call NtAllocateVirtualMemory, Frida calls the function, which will store the value for the arguments of NtAllocateVirtualMemory. When returning from the call, the onLeave function is executed, which will log the respective values stored during the API call.\n\nNote that the function prototype of NtAllocateVirtualMemory is slightly different than VirtualAlloc, with the parameter to be a pointer to a variable that stores the size, rather than the actual size itself. For this reason, we add the variable, that reads the value in the pointer variable stored by RegionSize . Since there are numerous NtAllocateVirtualMemory calls, this will aid in identifying if our malicious call is detected. To further help identification, we’ve added a comparison between the RegionValue and a hard-coded value of the hexadecimal representation of the length of our byte array, which will output three DETECTED’s if matched.\n\nFrida hooks are updated as soon as the handler files are saved, so we can press any key in our payload to continue. Since there’s a lot of output, it’d probably be easier to copy the output and paste it all in a text editor.\n\nNow that we know that Nt* functions will be called, as well as the abundance of EDR’s that hook ntdll.dll rather than kernel32.dll, we can turn to RastaMouse’s dinvoke.net to assist us. Dinvoke.net provides us examples of how to use APIs with generic D/Invoke and syscalls (which we will use later), but not with manually mapping. Still these examples can help us build out our payload.\n\nWe’ll first reuse the MapModuleToMemory method to map a fresh copy of ntdll.dll. We’ll declare a uint variable and set it to 1. This status variable equates to the NTSTATUS result of an API call, with a value of 0 equal to success.\n\nNext, we declare an variable, which will be a pointer to our allocated memory.\n\nFrom here, we’ll create the parameters for and call NtAllocateVirtualMemory, which is very similar to VirtualAlloc with the addition of specifying a process handle.\n\nThe documentation states that the macro should be used, which can’t be accessed in C#. NtCurrentProcess and ZwCurrentProcess, which are essentially the same, returns a handle to the current process.\n\nIf we take a look at the function documentation, it “retrieves a pseudo handle for the current process… A pseudo handle is a special constant, currently (HANDLE)-1, that is interpreted as the current process handle.” It stands to reason that we can pass this value as an IntPtr as a parameter for NtAllocateVirtualMemory. In fact, we can see that D/Invoke already uses this concept.\n\nAnother difference between VirtualAlloc and NtAllocateVirtualMemory is the output and return type. Kernel32.dll exported APIs will usually have their own return type. However, most Nt* APIs will have the NTSTATUS return type. With VirtualAlloc, the pointer to the newly allocated memory is the default API return, stored as . But for NtAllocateVirtualMemory, the return type is NTSTATUS, and the newly allocated memory address pointer is actually set to the parameter. So in order to grab the memory address value, we will set the addr variable to the baseAddress parameter by accessing the second index of the parameter object array.\n\nWe can then use the same method to copy our buffer to the newly created memory address. We can use Marshal.Copy here since we are within a single process. On the flip side, if we were attempting to build an injection payload such as DLL injection, process injection, or process hollowing, we would need to use instead.\n\nWe then follow a similar process for using . To do so, we’ll just pass the current process handle value as the fourth parameter of NtCreateThreadEx. Just like our NtAllocateVirtualMemory call, we’ll need to grab the handle to the thread we created (tHandle).\n\nTo keep our shellcode alive, we will use , which isn’t included in D/Invoke since the default library is focused on process injection techniques, where existing processes won’t terminate after the shellcode is run. Similar to what we’ve done with Win32 APIs, we can add a NtWaitForSingleObject delegate in the Delegates structure of the DInvoke.DynamicInvoke.Native namespace. Luckily, the function prototype exists on MSDN, so we can easily create our delegate like so.\n\nFor the parameters, we can simply supply a handle to the thread (tHandle), and supply IntPtr.Zero as the timeout, as a null value will result in an infinite timeout. Our newly written Nt* function code is shown below:\n\nLastly, we’ll talk about syscalls. As mentioned previously, Nt* functions are essentially wrapers for syscalls. Since D/Invoke supports syscalls, we can directly call assembly instructions without having to go through any Windows API calls, which will bypass any hooks placed on userland APIs. Dinvoke.net provides a general idea of how to invoke syscalls. The only changes we’ll make here is using the explicit uint type for status. We can simply copy the parameters we used for the Nt* manual map APIs. The very short syscall code is shown here:\n\nPutting It All Together\n\nFrom the looks of it, syscalls appear to be the preferred method of invoking Windows APIs, and while it may generally be true, D/Invoke syscalls have certain limitations such as not being able to work in WOW64 processes. Of course, having multiple methods at your disposable means you’ll have more flexibility when it comes to dealing with different types of EDR solutions.\n\nI learned a lot about Windows internals as I was writing this, and hopefully you all have learned a lot as well. The examples shown here aren’t the end though, you’ll still need to create obfuscation and encryption of shellcode to bypass static analysis. Consider creating DLL injection, process injection, process hollowing, etc. payloads, adjusting the necessary parameters and using the appropriate APIs (such as using NtWriteVirtualMemory instead of Marshal.Copy). There are also functions within the D/Invoke library that we have not used in our examples to further obfuscate our code. For example, using within the Generic class can help with static analysis, as performing a dumpbin of our application will reveal the static strings of the malicious APIs we used. Another benefit of knowing how to manually setup D/Invoke into your projects is ease of incorporating these techniques into LOLBINS/LOLBAS applications, which can be helpful in bypassing Machine Learning."
    },
    {
        "link": "https://stackoverflow.com/questions/8836093/how-can-i-specify-a-dllimport-path-at-runtime",
        "document": "Contrary to the suggestions by some of the other answers, using the attribute is still the correct approach.\n\nI honestly don't understand why you can't do just like everyone else in the world and specify a relative path to your DLL. Yes, the path in which your application will be installed differs on different people's computers, but that's basically a universal rule when it comes to deployment. The mechanism is designed with this in mind.\n\nIn fact, it isn't even that handles it. It's the native Win32 DLL loading rules that govern things, regardless of whether you're using the handy managed wrappers (the P/Invoke marshaller just calls ). Those rules are enumerated in great detail here, but the important ones are excerpted here:\n\nSo, unless you're naming your DLL the same thing as a system DLL (which you should obviously not be doing, ever, under any circumstances), the default search order will start looking in the directory from which your application was loaded. If you place the DLL there during the install, it will be found. All of the complicated problems go away if you just use relative paths.\n\nBut if that doesn't work for whatever reason, and you need to force the application to look in a different directory for the DLL, you can modify the default search path using the function.\n\n Note that, as per the documentation:\n\nSo as long as you call this function before you call the function imported from the DLL for the first time, you can modify the default search path used to locate DLLs. The benefit, of course, is that you can pass a dynamic value to this function that is computed at run-time. That isn't possible with the attribute, so you will still use a relative path (the name of the DLL only) there, and rely on the new search order to find it for you.\n\nYou'll have to P/Invoke this function. The declaration looks like this:"
    },
    {
        "link": "https://stackoverflow.com/questions/28935983/preprocessing-image-for-tesseract-ocr-with-opencv",
        "document": "I described some tips for preparing images for Tesseract here: Using tesseract to recognize license plates\n\nIn your example, there are several things going on...\n\nYou need to get the text to be black and the rest of the image white (not the reverse). That's what character recognition is tuned on. Grayscale is ok, as long as the background is mostly full white and the text mostly full black; the edges of the text may be gray (antialiased) and that may help recognition (but not necessarily - you'll have to experiment)\n\nOne of the issues you're seeing is that in some parts of the image, the text is really \"thin\" (and gaps in the letters show up after thresholding), while in other parts it is really \"thick\" (and letters start merging). Tesseract won't like that :) It happens because the input image is not evenly lit, so a single threshold doesn't work everywhere. The solution is to do \"locally adaptive thresholding\" where a different threshold is calculated for each neighbordhood of the image. There are many ways of doing that, but check out for example:\n\nAnother problem you have is that the lines aren't straight. In my experience Tesseract can handle a very limited degree of non-straight lines (a few percent of perspective distortion, tilt or skew), but it doesn't really work with wavy lines. If you can, make sure that the source images have straight lines :) Unfortunately, there is no simple off-the-shelf answer for this; you'd have to look into the research literature and implement one of the state of the art algorithms yourself (and open-source it if possible - there is a real need for an open source solution to this). A Google Scholar search for \"curved line OCR extraction\" will get you started, for example:\n\nLastly: I think you would do much better to work with the python ecosystem (ndimage, skimage) than with OpenCV in C++. OpenCV python wrappers are ok for simple stuff, but for what you're trying to do they won't do the job, you will need to grab many pieces that aren't in OpenCV (of course you can mix and match). Implementing something like curved line detection in C++ will take an order of magnitude longer than in python (* this is true even if you don't know python)."
    },
    {
        "link": "https://stackoverflow.com/questions/73462608/how-to-optimal-preprocess-images-for-tesseract-in-c-when-grayscaled-image-text",
        "document": "I'm struggling with finding a optimal binarization as preprocessing step for OCR (tesseract in C#).\n\nThe images are 1624 X 1728 of pixel size and contain car gui elements (Buttons, Sliders, Info Boxes) and corresponding text from a car navigation command interface generation (different use case scenarios like radio control, car control, etc.). The images contain multiple colors, most of images are dark blue, and the text is white/gray or close to white. Unfortunately, I cannot share the images due to data privacy.\n\nProblem: I cannot separate the text from the background in a efficent way (text to be black, everything else to be white), because the text color has a high range and is partialy the same with the background color (speaking of grayscaled images).\n\nActual procedure: First I convert the RGB Image from System.Drawing.Image to OpenCvSharp.Mat. Then I convert the Mat image from colored to gray and then from gray to binarized.\n\nThis is the main code for the binarization:\n\nI use 255 as maxVal. If I use tresh=90, the binarized image looks ok overall (even if tesseract results are bad here), but some pixels of the bottom control elements text (and some other text) are white, because the tresh is too high (so some text characters are unsharp and not complete).\n\nIf I use like tresh = 40, the characters of the bottom control elements become complete and sharp (as the should be), but the background (middle of the image) gets completely black, which means that some text in there disappears inside of a big black chunk. So the problem is a high text pixel color range inside of the grayscaled image that \"interferes\" with the colors of other elements or background, which makes the text extraction hard.\n\nNote: I already tried AdaptiveThresholding like MeanC and GaussianC with different treshholds, kernel sizes and mean substraction constants without good results.\n\nQuestion: What would be a efficient solution for the preprocessing?\n\nI'm thinking about writing a method that binarizas from RGB, not from grayscaled. So the method would take a RGB image as input and binarize that white text color range into black and everything else into white."
    },
    {
        "link": "https://restack.io/p/image-recognition-answer-ai-csharp-cat-ai",
        "document": "Explore the capabilities of image recognition AI in C#. Learn how to implement and optimize algorithms for effective image processing."
    },
    {
        "link": "https://encord.com/blog/realtime-text-recognition-with-tesseract-using-opencv",
        "document": "Real-time text detection is vital for text extraction and Natural Language Processing (NLP). Recent advances in deep learning have ushered in a new age for natural scene text identification. Apart from formats, natural texts show different fonts, colors, sizes, orientations, and languages (English being the most popular). It often overwhelms readers, especially those with visual impairments. Natural texts also include complex backgrounds, multiple photographic angles, and illumination intensities, creating text recognition and detection barriers. Text detection simplifies decoding videos, images, and even handwriting for a machine.\n\nIn this article, you will work on a project to equip a system to perform real-time text detection from a webcam feed. But, for that, your machine must include a real-timeOCR processing feature. The same OCR powers your applications to perform real-time text detection from the input images or videos.\n\nReady? Let’s start by understanding the problem and project scope.\n\nIn this section, you will learn about the use case of text recognition in video streams, its challenges, and, more specifically, how to overcome them.\n\nThe requirement for real-time text detection from camera-based documents is growing rapidly due to its different applications in robotics, image retrieval, intelligent transport systems, and more. The best part is that you can install real-time text detection using the webcam on your computer. OCR-based tools like Tesseract and OpenCV are there to help you out in this regard.\n\nDisplaying the Detected Text on the Screen\n\nThere is no denying the fact that detecting oriented text in natural images is quite challenging, especially for low-grade digital cameras and mobile cameras. The common challenges include blurring effects, sensor noise, viewing angles, resolutions, etc.\n\n\n\nReal-world text detection isn't without hurdles. Blurring effects, sensor noise, and varying viewing angles can pose significant challenges, especially for low-grade digital cameras. Overcoming these obstacles requires advanced techniques and tools.\n\n\n\nText detection methods using Tesseract is simple, quick, and effective. The Tesseract OCR helps extract text specifically from images and documents. Moreover, it generates the output in a PDF, text file, or other popular format. It's open-source Optical Character Recognition (OCR) software that supports multiple programming languages and frameworks. The Tesseract 3x is even more competent as it performs scene text detection using three methods: word finding, line finding, and character classification to produce state-of-the-art results.\n\nFirstly, the tool finds words by organizing the text lines into bubbles. These lines and regions are analyzed as proportional text or fixed pitch. Then, these lines are arranged by word spacing to make word extraction easier. The next step comprises filtering out words through a two-pass process. The first pass checks only if each word is understandable. If the words are recognizable, they will proceed with the second pass. This time, the words use an adaptive classifier where they are recognized more accurately. On the other hand, the Tesseract 4 adopts a neural network subsystem for recognizing text lines. This neural subsystem originated from OCRopus' Python-based LSTM implementation.\n\nOpenCV (Open Source Computer Vision Library) is open-source for computer vision, image processing, and machine learning. Computer vision is a branch of artificial intelligence that focuses on extracting and analyzing useful information from images. This library allows you to perform real-time scene text detection and image and video processing with the scene text detector. This library has more than 2500 in-built algorithms. The function of these algorithms is to identify objects, recognize images, text lines, and more.\n\nSo, let’s learn how Tesseract OCR and OpenCV help with real-time text detection in this tutorial.\n\nThe preprocessing of a video or image consists of noise removal, binarization, rescaling, and more. Thus, preprocessing is necessary for acquiring an accurate output from the OCR.\n\nThe OCR software imposes several techniques to pre-process the images and videos:\n• Binarization is a technique that converts a colorful or grayscale image into a binary or black-and-white image, enhancing the quality of character recognition. It separates text or image components from the background, making identifying and analyzing text characters easier.\n• De-skewing is a technique that ensures proper alignment of text lines during scanning. Despeckling is used for noise reduction, reducing noise from multiple resources. Word and line detection generate a baseline for shaping characters and words. Script recognition is essential for handling multilingual scripts, as they change at the level of the words.\n• Character segmentation or isolation is crucial for proper character isolation and reconnection of single characters due to image artifacts. Techniques for fixed-pitch font segmentation require aligning the image to a standard grid base, which includes fewer intersections in black areas. Techniques for proportional fonts are necessary to address issues like greater whitespace between letters and vertically intersecting more than one character.\n\nTwo basic OCR algorithms for text recognition through computer vision techniques are matrix matching and feature extraction.\n• Matrix matching compares an image to a glyph pixel-by-pixel, known as image correlation or pattern recognition. The output glyph is in the same scale and a similar font.\n• The feature extraction algorithm breaks glyphs into features such as lines, line intersections, and line directions, making the recognition process more efficient and reducing the dimensionality of the texts. Again, the k-nearest neighbors algorithm compares the image features with the stored glyphs to choose the nearest match. The glyphs are symbolic characters or figures recognized as text after an OCR is conducted over an image.\n\nCapturing Video From the Webcam Using OpenCV\n\nOpenCV can detect text in different languages using your computer’s webcam. The video streaming process in OpenCV runs on a dedicated thread. It reads live frames from the webcam and caches the new videos in memory as a class attribute.\n\nThe video script ingests real-time OCR effects by multi-threading. When the OCR runs in the background, the multi-threading improves the processing by enabling real-time video streaming. The OCR thread updates the detected texts and the boxes, giving them prominent visibility.\n\nSet Up Tesseract OCR and Specify its Executable Path\n\n\n\nThere are several reasons to install Python-tesseract to proceed with your real-time text detection. Its OCR feature easily recognizes and encodes texts from your video. Moreover, it can read many images, such as PNG, GIF, and JPEG. Thus, it can be used as an individual script.\n\nTo integrate Tesseract into your Python code, you should use Tesseract’s API. It supports real concurrent execution when you use it with Python’s threading module. Tesseract releases GIL (Generic Image Library) while processing an image.\n\nFirst of all, install the Tesseract OCR in your environment:\n\nSet up the executable path for Tesseract OCR\n\nThe `capture_screen` function captures the screen content using `ImageGrab.grab` from the Pillow library. This function captures a specific screen region defined by the `bbox` parameter. It converts the captured image from RGB to BGR format, which is suitable for OpenCV.\n\nThe code initializes the webcam (if available) by creating a VideoCapture object and setting its resolution to 640x480.\n\nThe output stream for real-time text detection can be a file of characters or a plain text stream. However a sophisticated OCR stores the original layout of a page. The accuracy of an OCR can be boosted when there is a lexicon constraint in the output. Lexicons are lists of words that can be presented in a document. However, it becomes problematic for an OCR to improve detection accuracy when the quantity of non-lexical words increases. It is, however, possible to assume that a few optimizations will speed up OCR in many scenarios, like data extraction from a screen.\n\nAdditionally, the k-nearest-neighbor analysis (KNN) corrects the error from the words that can be used together. For example, it can differentiate between 'United States of America' and 'United States'.\n\nNow, you will learn about automated text extraction after detecting it with Tesseract OCR.\n\nApplying Tesseract OCR to Perform Text Detection on Each Frame\n\nIn the text detection step, the Tesseract OCR will annotate a box around the text in the videos. Then, it will show the detected text above the box. But this technique works by breaking the video frame-by-frame and applying the tesseract detection to the video frame. The caveat here is that sometimes, you may experience difficulties in text detection due to the abrupt movements of the video objects.\n\nThe following code enters a loop to capture frames from the webcam (or screen capture). It performs text detection on each frame using Tesseract OCR irrespective of the frame rate (fps).\n\nTo draw bounding boxes around the detected text, the code utilizes Tesseract's built-in capabilities for bounding box detection. It uses `pytesseract.image_to_data` with the `pytesseract.Output.DICT` option to obtain information about individual text boxes. The code then loops through the detected boxes, and for each box with a confidence level greater than 0, it draws a green rectangle using `cv2.rectangle`\n\nDetected text is displayed on the frame with green and drawn with `cv2.putText`\n\nThe Google Cloud Vision API is an example of a text extraction API. It can detect and extract text from an image. It has two annotation features to support an OCR. The annotations are:\n• TEXT_DETECTION: It detects and extracts text from any type of image. For example, you might consider a photograph related to a signboard about traffic rules. The JSON of the API formats and stores strings and individual words from the text of that image. Also, it creates bounding boxes around the texts.\n• DOCUMENT_TEXT_DETECTION: The vision API uses this annotation to extract text instancess from a document or dense text. The JSON formats and stores the extracted paragraph, page, word, block, and break information. Four vertices form a quadrilateral bounding box with orientation information in the text instance annotations.\n\nThe vision API detects text from a local image file through the feature detection process. So, when you send REST requests to the API, it should be a Base64 encoding string for the image file's contents within the body of your request.\n\n\n\nBase64 is a group of schemes that encodes binary data into readable text for an image. It represents binary data in a 24-bit sequence. This 24-bit sequence can be further represented as four 6-bit Base64 digits. Base64 reliably carries binary data throughout channels that support text contents.\n\n\n\nGenerally, the text in a video appears in multiple frames. So, you need to detect and recognize the texts present in each frame of the video. The OCR software converts the text content from the video into an editable format. The alphanumeric information in the video must be converted into its ASCII equivalent first. Then, they will be converted to readable text. This way, it detects texts from videos and other imagery formats.\n\n\n\nModern OCR systems, such as Tesseract, are designed to automatically extract and recognize text from videos. The OCR identifies the locations of text within the video and proceeds to extract strokes from the segmented text regions, taking into account factors like text height, alignment, and spacing. Subsequently, the OCR processes these extracted strokes to generate bounding boxes, within which the recognized texts are displayed upon completion of the process. Text localization in real time text detection using Tesseract is a crucial step in optical character recognition (OCR) systems. By accurately identifying the location of text within an image or video frame, Tesseract enables the extraction and analysis of textual information. This process involves employing advanced computer vision techniques to detect and outline text regions, allowing for efficient recognition and subsequent interpretation of the detected text. This is often done using an image annotation tool.\n\nThe processed frame with the detected text and bounding boxes is displayed using `cv2.imshow`.\n\nThe model displays real-time video with a detected text overlay until the user presses the 'q' key. Upon pressing 'q', the loop exits, the webcam is released, and the OpenCV window is closed.\n\nMoreover, you can customize your outputs by using white-listing and black-listing characters. When you choose white-listing characters, Tesseract only detects the characters white-listed by your side. It ignores the rest of the characters in the video or image. Also, you can use black-list characters when you don't want to get the output of some specific characters. Tesseract will black-list them. So, it will not produce the output for these characters.\n\n\n\nIf you need real-time text detection from a mobile scanning app, you must have an OCR as part of that scanning app. The best mobile scanning OCR apps, like Image to Text, usually have these features -\n• Scanning efficiency: A mobile OCR app must focus on every region of a document. Even the sensor can accurately detect the borders of the document. Also, it doesn’t take too much time to scan the document.\n• Modes of scanning: You can get different scanning modes through this app, such as IDs, books, documents, passports, and images.\n• Document management: It supports a file management activity by saving, organizing, printing, sharing, and exporting digitized files.\n• Customization: You can customize your document scanning by adding a signature, text, watermark, or password protection.\n• Accuracy: The OCR app emphasizes document digitization. Thus, it produces digitized text from a document without too much delay.\n\n\n\nFor mobile scanning apps, integrating OCR is essential. An ideal OCR app should scan efficiently and offer various scanning modes, robust document management, customization options, and high accuracy in digitizing text from documents.\n\n\n\nSo, you have learned text detection in real-time with Tesseract OCR, OpenCV, and Python. OCR software uses text detection algorithms to implement real-time text detection. Moreover, OCR software can solve other real-world problems, such as - object detection from video and image datasets, text detection from document scanning, face recognition, and more.\n\nReal-time text detection is crucial for applications involving text extraction and NLP applications, dealing with diverse fonts, colors, sizes, orientations, languages, and complex backgrounds.\n• Tesseract OCR and OpenCV are open-source tools for real-time text detection.\n• Preprocessing steps in OCR include binarization, de-skewing, despeckling, word and line detection, script recognition, and character segmentation.\n• OCR accuracy can be enhanced with lexicon constraints and near-neighbor analysis.\n• Video frames can be processed in real-time for text detection and recognition, converting alphanumeric information into editable text.\n• Customization options, such as white-listing and black-listing characters, are available in OCR for tailored text detection."
    },
    {
        "link": "https://geeksforgeeks.org/text-detection-and-extraction-using-opencv-and-ocr",
        "document": "OpenCV (Open source computer vision) is a library of programming functions mainly aimed at real-time computer vision. OpenCV in python helps to process an image and apply various functions like resizing image, pixel manipulations, object detection, etc. In this article, we will learn how to use contours to detect the text in an image and save it to a text file.\n\nRequired Installations:\n\nOpenCV package is used to read an image and perform certain image processing techniques. Python-tesseract is a wrapper for Google’s Tesseract-OCR Engine which is used to recognize text from images.\n\nDownload the tesseract executable file from this link.\n\nApproach: \n\nAfter the necessary imports, a sample image is read using the imread function of opencv.\n\nThe colorspace of the image is first changed and stored in a variable. For color conversion we use the function cv2.cvtColor(input_image, flag). The second parameter flag determines the type of conversion. We can chose among cv2.COLOR_BGR2GRAY and cv2.COLOR_BGR2HSV. cv2.COLOR_BGR2GRAY helps us to convert an RGB image to gray scale image and cv2.COLOR_BGR2HSV is used to convert an RGB image to HSV (Hue, Saturation, Value) color-space image. Here, we use cv2.COLOR_BGR2GRAY. A threshold is applied to the converted image using cv2.threshold function. \n\nThere are 3 types of thresholding:\n\nFor more information on thresholding, refer Thresholding techniques using OpenCV.\n\ncv2.threshold() has 4 parameters, first parameter being the color-space changed image, followed by the minimum threshold value, the maximum threshold value and the type of thresholding that needs to be applied.\n\ncv2.getStructuringElement() is used to define a structural element like elliptical, circular, rectangular etc. Here, we use the rectangular structural element (cv2.MORPH_RECT). cv2.getStructuringElement takes an extra size of the kernel parameter. A bigger kernel would make group larger blocks of texts together. After choosing the correct kernel, dilation is applied to the image with cv2.dilate function. Dilation makes the groups of text to be detected more accurately since it dilates (expands) a text block.\n\ncv2.findContours() is used to find contours in the dilated image. There are three arguments in cv.findContours(): the source image, the contour retrieval mode and the contour approximation method. \n\nThis function returns contours and hierarchy. Contours is a python list of all the contours in the image. Each contour is a Numpy array of (x, y) coordinates of boundary points in the object. Contours are typically used to find a white object from a black background. All the above image processing techniques are applied so that the Contours can detect the boundary edges of the blocks of text of the image. A text file is opened in write mode and flushed. This text file is opened to save the text from the output of the OCR.\n\nLoop through each contour and take the x and y coordinates and the width and height using the function cv2.boundingRect(). Then draw a rectangle in the image using the function cv2.rectangle() with the help of obtained x and y coordinates and the width and height. There are 5 parameters in the cv2.rectangle(), the first parameter specifies the input image, followed by the x and y coordinates (starting coordinates of the rectangle), the ending coordinates of the rectangle which is (x+w, y+h), the boundary color for the rectangle in RGB value and the size of the boundary. Now crop the rectangular region and then pass it to the tesseract to extract the text from the image. Then we open the created text file in append mode to append the obtained text and close the file.\n\nSample image used for the code:"
    },
    {
        "link": "https://stackoverflow.com/questions/6812068/c-sharp-which-is-the-fastest-way-to-take-a-screen-shot",
        "document": "I am implementing a feature that will take screen shot repeatedly and output dirty rectangles between 2 different shots then send re-draw the screen in a window.\n\nI can get it running between 20~30FPS currently. It is already acceptable. But then I made a benchmark and measured its performance. Found out that the takes up to 50% of the processing time. (Yep. Even in the worst case, it still takes longer than find all the dirty rectangles) Then I used native API implementation and get no improvement.\n\nI know there may not be any practical reasons to make it any faster than 30FPS in this case. I am just wondering, is there any faster way to take a screen shot?"
    },
    {
        "link": "https://stackoverflow.com/questions/362986/capture-the-screen-into-a-bitmap",
        "document": "This is the module1.vb code\n\nThis is the ScreenSpec.vb Class I created\n\nThis is the Form1.vb code\n\nWhen program starts it will find all monitors and list them in ComboBox. Set ComboBox to monitor number you want and single-click PictureBox. Your selected monitor will be captured in the PictureBox. The CheckBox is used as a (Show Task Bar/Hide Task Bar). Also as you switch ComboBox the selected monitors specs will show in the forms title bar.\n\nMade change to the picturebox click which will hide the screenshot program."
    },
    {
        "link": "https://cyotek.com/blog/capturing-screenshots-using-csharp-and-p-invoke",
        "document": "Capturing screenshots using C# and p/invoke\n\nI was recently updating some documentation and wanted to programmatically capture some screenshots of the application in different states. This article describes how you can easily capture screenshots in your own applications.\n\nThis article makes use of a number of Win32 API methods. Although you may not have much call to use them directly in day to day .NET (not to mention Microsoft wanting everyone to use universal \"apps\" these days), they are still extraordinarily useful and powerful.\n\nThis article does assume you know the basics of platform invoke so I won't cover it here. In regards to the actual API's I'm using, you can find lots of information about them either on MSDN, or PInvoke.net.\n\nA number of the API's used in this article are GDI calls. Generally, when you're using the Win32 GDI API, you need to do things in pairs. If something is created (pens, brushes, bitmaps, icons etc.), then it usually needs to be explicitly destroyed when finished with (there are some exceptions just to keep you on your toes). Although there haven't been GDI limits in Windows for some time now (as far as I know!), it's still good not to introduce memory leaks. In addition, device contexts always have a number of objects associated with them. If you assign a new object to a context, you must restore the original object when you're done. I'm a little rusty with this so hopefully I'm not missing anything out.\n\nSetting up a device context for use with BitBlt\n\nTo capture a screenshot, I'm going to be using the API. This copies information from one device context to another, meaning I'm going to need a source and destination context to process.\n\nThe source is going to be the desktop, so first I'll use the and calls to obtain this. As calling essentially places a lock on it, I also need to release it when I'm finished with it.\n\nNow for the destination - for this, I'm going to create a memory context using . When you call this API, you pass in an existing DC and the new one will be created based on that.\n\nThere's still one last step to perform - by itself, that memory DC isn't hugely useful. We need to create and assign a GDI bitmap to it. To do this, first create a bitmap using and then attach it to the DC using . will also return the relevant old object which we need to restore (again using ) when we're done. We also use to clean up the bitmap.\n\nAlthough this might seem like a lot of effort, it's not all that different from using objects implementing in C#, just C# makes it a little easier with things like the statement.\n\nWith the above setup out the way, we have a device context which provides access to a bitmap of the desktop, and we have a new device context ready to transfer data to. All that's left to do is make the call.\n\nIf you've ever used the method of a object before, this call should be fairly familiar - we pass in the DC to write too, along with the upper left corner where data will be copied ( in this example), followed by the and of the rectangle - this applies to both the source and destination. Finally, we pass in the source device context, and the upper left corner where data will be copied from, along with flags that detail how the data will be copied.\n\nIn my old VB6 days, I would just use (direct copy), but in those days windows were simpler things. The flag ensures the call works properly with layered windows.\n\nIf the call fails, I throw a new object without any parameters - this will take care of looking up the result code for the failure and filling in an appropriate message.\n\nNow that our destination bitmap has been happily \"painted\" with the specified region from the desktop we need to get it into .NET-land. We can do this via the static method of the class - this method accepts a GDI bitmap handle and return a fully fledged .NET object from it.\n\nPutting it all together\n\nAs the above code is piecemeal, the following helper method will accept a which describes which part of the desktop you want to capture and will then return a object containing the captured information.\n\nNow that we have this method, we can use it in various ways as demonstrated below.\n\nIf you want to capture a window in your application, you could call with the value of the property of your . But if you want to capture an external window then you're going to need to go back to the Win32 API. The function will return any window's boundaries.\n\nWin32 has its own version of .NET's structure, named . This differs slightly from the .NET version in that it has and properties, not and . The class has a helper method, which constructs a from left, top, right and bottom properties which means you don't need to perform the subtraction yourself.\n\nAs a slight variation on the previous section, you can use the API call to get the handle of the active window.\n\n.NET offers the static class which provides access to all monitors on your system via the property. You can use the method to find out which monitor a form is hosted on, and get the region that represents the monitor - with or without areas covered by the task bar and other app bars. This means it trivial to capture the contents of a given monitor.\n\nIt is also quite simple to capture the entire desktop without having to know all the details of monitor arrangements. We just need to enumerate the available monitors and use to merge two rectangles together. When this is complete, you'll have one rectangle which describes all available monitors.\n\nOf course, you could just call with a custom rectangle to pick up some arbitrary part of the desktop. The above helpers are just that, helpers!\n\nAlthough I don't have a high DPI monitor, I did temporarily scale the display to 125% to test that the correct regions were still captured. I tested with a manifest stating that the application supported high DPI and again without, in both cases the correct sized images were captured.\n\nA demonstration program for the techniques in this article is available from the links below. It's also available on GitHub."
    },
    {
        "link": "https://codeproject.com/Articles/9741/Screen-Captures-Window-Captures-and-Window-Icon-Ca",
        "document": "Ever since I can remember, I've been fascinated by the print screen keyboard command. Just exactly how did it capture an image? What strange and powerful API was I missing? Well, after a few months of coding, and getting familiar with the Windows platform, I realized that I could easily code my own screen capture, without relying upon the print screen keyboard command. This article will provide you with the code and knowledge to implement screen capturing in code, without relying upon the print screen keyboard command.\n\nIn addition to wonder about the screen captures, there were these other programs that could capture an image of a particular window. That, I thought was quite cool, so I worked that out too. So, while we're capturing images of things on our desktop, I'll also show you how to capture images of any visible window.\n\nOne last thing, what about the Windows Task Manager? How does it snag the icon from a window, and then display it for us? Well, I know how, and I'm prepared to show you how easy it is. Interested? Keep reading.\n\nWell, I guess it's not really the last thing. The last thing is that, I've updated the source to include a Spy++ style window finder tool that will allow you to hover over a window to snag it's handle, complete with highlighting, and then to capture that window as an image. A few guys were nice enough to motivate me to do this, and to demonstrate how easy it is!\n\nI'm going cut straight to the heart of the material in this article and hand you the code, so I'm going to assume a few things first. One, that you understand C#, or at least can translate it to another CLR language. Two, that you are at least familiar with the fact that there's a whole lot more behind the scenes called the Win32 APIs, if not I'll show you the way. Three, that you will have a lot of fun playing around with the demo, and code, and hopefully enjoy this article!\n\nCapturing the Desktop as an Image\n\nFirst up, the desktop. Obviously it can be captured, but how'd they do that? Easy, using just a few API calls and some GDI+, we can recreate the functionality that the print screen command offers, with one advantage. We're not going to touch the Window's Clipboard. Too many times, I have seen improper usage of the clipboard, and in my opinion, slapping a big fat image onto it, erasing whatever you had there before with no warning, is just a bad idea.\n\nThe command is 'print screen', not 'erase my clipboard data'. So, why do it? Would you really want to write code that has to use the user's workspace to accomplish it's task? I don't think so. We can capture the image, without storing it on the clipboard and making you paste it into your favorite image editor.\n\nSo a little background. Let's start with device contexts. If you are familiar, skip ahead. If not, read on. Device contexts are just generic ways to represent devices, or things you can draw on or interact with graphically. This is hugely over simplified, and I'm going to get flamed, but for the sake of brevity, let's just go with that for now. You can get device context's, or dc's as they are commonly referred to, from images, windows, monitors, and even printers. Every window has them, and you can use them to draw with. Everything you see on your screen is being drawn upon a device context. The desktop, every window, your taskbar, and anything you see. You can draw upon them, or copy what they have drawn upon them. If you can get a hold of them, you can pretty much draw or steal whatever you want graphically speaking. Working with device contexts is fast, and both GDI and GDI+ are based on them.\n\nWhat's all that mean? Well, in the case of capturing the screen, we know that somewhere Windows is drawing upon a device context, so that we can see it. In fact, there's one for each monitor you have attached to your system, and that desktop that you are seeing on it, is being drawn on that monitor's device context. All we have to do is grab a hold of that device context, create another one of our own, and copy the screen's device context image data to our own, and we've got a screen capture.\n\nUsing our knowledge of the .NET framework, and some additional knowledge of the Windows APIs, we can pretty effectively duplicate how the print screen command works. Let's take a look at some code. Here's a method that is used in the example, that will enumerate all of the monitors attached to the system, figure out just how big the combined desktop is, and then copy what each monitor has displayed on it, into a final bitmap image. Instant screen capture. Here's the code.\n\nLooking at the code, the first thing you'll see is that I'm using a mixture of GDI and GDI+. This is due largely to the fact that there is a bug present in GDI+ and the API. I have spent many hours on the phone with Microsoft Developer Support to confirm this. This issue only manifests itself on systems with multiple monitors, and if I remember correctly, the system had to have a NVida display adapter on the non-primary monitor, and of course, our old friend Windows 98 running as the OS. What happens is the primary monitor captures fine, the secondary (or any non-primary) monitor stands a chance of returning garbage for an image. It looks like cable channel with no signal. Call it snow, call it ant races, I call it a pain in butt. But that's life, and here's the work around.\n\nInstead of relying on purely managed code, do copy the images, or backing up to the API, we instead fall back to it's somewhat slower cousin, . That made me angry when I heard it, but supposedly a fix is in store for the 2.0 framework, so I'll just take my free phone call and wait to see if that's true. In the mean time I need screen captures. You don't have to take my word for it, code it up yourself and when you find out I was right, just remember that I told you so.\n\nBack on the code, first up we just grab all of the monitors using the class' property. This does two things for us. First it allows us to figure out how big the entire desktop is, and create an image just big enough to hold all of the screens inside. And secondly, it allows us to figure out just where each monitor is positioned in relation to the other. Remember, with multiple monitor support you can \"arrange\" your monitors in different ways, and with different resolutions, so don't think in terms of a pure rectangle when you think of how your monitors are positioned.\n\nOnce we have those screens, it's a trivial matter to calculate the size of the entire bitmap by using the method to build up the size of the overall image. After we've figured out the size of the final image, we'll grab a Graphics object from the image. The GDI+ Graphics object is just the .NET wrapper around a device context. Using that graphics context, we can draw on the bitmap with the graphics object.\n\nNext, we'll enumerate through each monitor, and draw what that monitor has on it's device context, upon the image we just created that will hold the final screen shot. Well draw it using it's coordinates so that in case the monitors have different resolutions or positioning we'll be able to see them as the Display Control Panel applet sees them. Go check it out if you have multiple monitors, and you didn't know you could move them. Chances are there if you have multiple monitors, you know this already, but if not so harm no foul. Open the settings tab and drag one of the monitors around and you'll see you can reposition it in relation to the other monitors.\n\nFor each monitor, we'll simply use the API to copy that monitor's device context contents, to the bitmap that will serve as the screen capture of the desktop. Notice that I'm creating a device context each time, this gives us access to that monitor's device context so that we can copy from it. Keep in mind that if we create it, we must destroy it, so we delete the device context when we are finished with it. If you don't, you'll have a memory leak, so keep a watchful eye on your dc's and make sure to release or destroy them. A simple rule is, if you \"acquire\" it, you're required to \"release\" it. And if you \"create\" it, then you must \"destroy\" it. I quote those because if you look at the GDI APIs, with that in mind you'll find the necessary APIs to do exactly what you want.\n\nFinally, after copying the contents of each device context to that bitmap we created, we'll release the Graphics object we acquired from the bitmap, and dispose it. That's the proper way to clean up a graphics object, if you've acquired a device context from it.\n\nThat's it, now we've got a bitmap that any .NET language can use, and we did it without faking keyboard commands, or trashing the contents of the user's clipboard in the process. Try it out, use the print screen command on the keyboard, open an image editor, and paste it into the app. The image was stored on the clipboard. This is ok, but not for us. We're too slick for that.\n\nOk, so we did the desktop, and we can handle multiple monitors, but what about capturing images from a single window? You've seen the apps that use some sort of zoom tool to identify a window and then let you capture just that window. How's that got down?\n\nUsing device contexts of course, and a little guy known as a Window Handle. Every window in the system is identified with a unique number, known as a handle. Windows likes to identify things with \"handles\", so why should a window be any different, right? Right. Using a window handle, we can figure out the size any window, create a bitmap of that size, snag the window's device context using the same handle, and then copy the window's device context contents to a bitmap. Here's how we can accomplish such a feat.\n\nWell, that wasn't so bad, was it? Nah, I've done things more complicated with for loop statements. Let's see what's going on here. First, let me note that the method takes an as a parameter type, and then I create an from it, this stems from the fact that s are serializable, and this source code was taken from a larger project, that just so happened was communicating over a network connection, which required me to make objects that were serializable. Well, I got lazy and made a property that was an , and a method that took that property so you're stuck with it.\n\nOnce you have the handle to the window you want to capture, we'll first figure out how big it is, you can do this using the API. Using that rectangle we can create a bitmap just large enough to hold the window's image, using a straight one to one copy. We'll not rely on this time, but the old standard . It's faster, and for this straight copy, it's all we need.\n\nFrom the bitmap we'll create a graphics object. We'll use that graphics object's device context, and one we can acquire from the window using it's window handle, to copy the contents of the window's device context directly to the bitmap. Once we've done that, a few lines to release the device contexts that we acquired, and another to dispose of that graphics object, and we're home free. We've just created an image of that window, stored in the bitmap. Pretty slick eh? Yeah, it's been done a thousand times, and I'll probably get more heat, acting like I came up with this. Obviously I didn't, I just wanted to show you how to do it.\n\nFinally, the last little fun topic. How does the Window's Task Manager display the icon a window displays in it's title bar? This one, is pretty easy. We'll send the window the message, and wait for it to hand a handle to it's icon back to us. If that doesn't work, I've delved deep into the heart of Litestep, and implemented their technique for backup icon retrieval. Let's look at the code for snagging a window's icon as an image.\n\nFirst thing to note is the use of the API, you want to be careful here as the window might not respond with an answer. It might be locked up for whatever reason, and then you are screwed. Your app, in turn will be screwed. So we'll send the message and specify a timeout to save us if the window doesn't respond in that amount of time. If that fails, we'll try snagging a handle to the icon from the window's bit information. Each window stores handles to things in it's class data. You can query that with the API. And finally, if both fail, we'll try asking the window for the icon it'd display if it were being drug about. Again, for the last two methods, credit must go to the Litestep development team, they are super smart, and thought that one up. I just ported it to C#.\n\nAgain, this method takes a handle to a window, and then returns an image containing that window's icon. If you wanted to snag all of the icon's for every top level window in the system, try using the API. Creating a task manager is beyond the scope of this article, so I just passed the main form's window handle to this method, and retrieve my own icon. Try different window handles, break out Spy++ to find them, or code in some other retrieval methods like or if you are after a single window. If you want advice on that, I'll be happy to help, just give me a yell. I've written task manager and a shell replacement, so I know my way around when it comes to get what I want from a window handle.\n\nOk, this isn't the first time you've seen it, and it probably won't be the last. I didn't think it up, so I really don't deserve a lot of credit. This is just also cool. A few comments from the first posting of this article motivated me to include a window finder tool, just like Spy++.\n\nSo anyway, here's what's up. Just open the spy window, and left click and drag out the finder tool over any window. If you haven't figured it out by now, that everything you see is a window, whether it's a button or , it's got a window handle. I guess that's why it's called Windows, and not GUIWidgets or something eh?\n\nThe code is pretty simple. It just uses the / APIs to snag the mouse movements in or out of the spy window, as long as one of the mouse buttons are down. I went with the standard left-click-drag-over-a-window technique most of us are familiar with because of Spy++. I tried to mimic it as close as I could, but hey, this was a quick hack in the last two nights, so don't flame the crap out of me because it's not perfect.\n\nThe window highlighting was pretty simple too, just snagged the device context from the window under the cursor, and drew a rectangle around it. When the highlighting is done, it just forces the window to redraw itself with a few other handy APIs.\n\nThe window highlighting was by far the most interesting bit of the code, as Spy++ has always fascinated me, leaving me with countless hours of my coding life spent \"highlighting\" windows around on the desktop just to see that stupid rectangle show me where the window was that I was hovering over. Try it out, it's pretty fun for some reason to get a look at how various windows are composed of child windows.\n\nThe sample code provides a mixture of GDI and GDI+ methods. All of the Windows APIs have been declared in the Win32.cs class. Stay out of there if you are squeamish. Nothing but declarations and other fun things for the guys that like that sort of thing.\n\nThe real fun code is in the class. If you want to play around, try changing the main form's icon and see what is returned. Or try saving the images to a file. The main focus of the article was to help you understand how taking screen captures can be accomplished. For some, this is no big news, for others just starting out, this kind of a thing was big fun, but always seemed like I couldn't find any good examples to learn from. I hope, this will shed some light on this subject if it was dark and mysterious before!\n\nEnjoy the code, and give me a shout if you like it! Maybe even a vote or two, a little thanks goes a long way in motivating me to write more articles! Thanks for reading!\n• Sometime in Februrary 2005 I posted the article.\n• Sometime in March 2005 I updated it."
    },
    {
        "link": "https://vbforums.com/showthread.php?680036-Capturing-Part-of-Screen-gt-Bitmap",
        "document": "Click Here to Expand Forum to Full Width"
    }
]