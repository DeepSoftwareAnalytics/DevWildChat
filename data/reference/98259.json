[
    {
        "link": "https://docs.opencv.org/4.x/db/deb/tutorial_display_image.html",
        "document": "Prev Tutorial: Building OpenCV for Tegra with CUDA \n\n Next Tutorial: Writing documentation for OpenCV \n\n\n\nIn this tutorial you will learn how to:\n• Read an image from file (using cv::imread)\n• Display an image in an OpenCV window (using cv::imshow)\n• Write an image to a file (using cv::imwrite)\n\nIn OpenCV 3 we have multiple modules. Each one takes care of a different area or approach towards image processing. You could already observe this in the structure of the user guide of these tutorials itself. Before you use any of them you first need to include the header files where the content of each individual module is declared. You'll almost always end up using the:\n• core section, as here are defined the basic building blocks of the library\n• imgcodecs module, which provides functions for reading and writing\n• highgui module, as this contains the functions to show an image in a window We also include the iostream to facilitate console line output and input. By declaring , in the following, the library functions can be accessed without explicitly stating the namespace.\n\nAs a first step, the OpenCV python library is imported. The proper way to do this is to additionally assign it the name cv, which is used in the following to reference the library.\n\nNow, let's analyze the main code. As a first step, we read the image \"starry_night.jpg\" from the OpenCV samples. In order to do so, a call to the cv::imread function loads the image using the file path specified by the first argument. The second argument is optional and specifies the format in which we want the image. This may be:\n• IMREAD_COLOR loads the image in the BGR 8-bit format. This is the default that is used here.\n• IMREAD_UNCHANGED loads the image as is (including the alpha channel if present)\n• IMREAD_GRAYSCALE loads the image as an intensity one\n\nAfter reading in the image data will be stored in a cv::Mat object.\n\nAfterwards, a check is executed, if the image was loaded correctly.\n\nThen, the image is shown using a call to the cv::imshow function. The first argument is the title of the window and the second argument is the cv::Mat object that will be shown.\n\nBecause we want our window to be displayed until the user presses a key (otherwise the program would end far too quickly), we use the cv::waitKey function whose only parameter is just how long should it wait for a user input (measured in milliseconds). Zero means to wait forever. The return value is the key that was pressed.\n\nIn the end, the image is written to a file if the pressed key was the \"s\"-key. For this the cv::imwrite function is called that has the file path and the cv::Mat object as an argument."
    },
    {
        "link": "https://docs.opencv.org/3.4/d4/da8/group__imgcodecs.html",
        "document": "Reads an image from a buffer in memory. The function imdecode reads an image from the specified buffer in the memory. If the buffer is too short or contains invalid data, the function returns an empty matrix ( Mat::data==NULL ). See cv::imread for the list of supported formats and flags description. In the case of color images, the decoded images will have the channels stored in B G R order. The same flags as in cv::imread, see cv::ImreadModes.\n\nThis is an overloaded member function, provided for convenience. It differs from the above function only in what argument(s) it accepts. The optional output placeholder for the decoded matrix. It can save the image reallocations when the function is called repeatedly for images of the same size.\n\nThe function imencode compresses the image and stores it in the memory buffer that is resized to fit the result. See cv::imwrite for the list of supported formats and flags description. File extension that defines the output format. Must include a leading period.\n\nThe function imread loads an image from the specified file and returns it. If the image cannot be read (because of missing file, improper permissions, unsupported or invalid format), the function returns an empty matrix ( Mat::data==NULL ). Currently, the following file formats are supported:\n• Raster and Vector geospatial data supported by GDAL (see the Note section)\n• The function determines the type of an image by the content, not by the file extension.\n• In the case of color images, the decoded images will have the channels stored in B G R order.\n• When using IMREAD_GRAYSCALE, the codec's internal grayscale conversion will be used, if available. Results may differ to the output of cvtColor()\n• On Microsoft Windows* OS and MacOSX*, the codecs shipped with an OpenCV image (libjpeg, libpng, libtiff, and libjasper) are used by default. So, OpenCV can always read JPEGs, PNGs, and TIFFs. On MacOSX, there is also an option to use native MacOSX image readers. But beware that currently these native image loaders give images with different pixel values because of the color management embedded into MacOSX.\n• On Linux*, BSD flavors and other Unix-like open-source operating systems, OpenCV looks for codecs supplied with an OS image. Install the relevant packages (do not forget the development files, for example, \"libjpeg-dev\", in Debian* and Ubuntu*) to get the codec support or turn on the OPENCV_BUILD_3RDPARTY_LIBS flag in CMake.\n• In the case you set WITH_GDAL flag to true in CMake and IMREAD_LOAD_GDAL to load the image, then the GDAL driver will be used in order to decode the image, supporting the following formats: Raster, Vector.\n• If EXIF information is embedded in the image file, the EXIF orientation will be taken into account and thus the image will be rotated accordingly except if the flags IMREAD_IGNORE_ORIENTATION or IMREAD_UNCHANGED are passed.\n• By default number of pixels must be less than 2^30. Limit can be set using system variable OPENCV_IO_MAX_IMAGE_PIXELS Name of file to be loaded. Flag that can take values of cv::ImreadModes\n\nSaves an image to a specified file. The function imwrite saves the image to the specified file. The image format is chosen based on the filename extension (see cv::imread for the list of extensions). In general, only 8-bit unsigned (CV_8U) single-channel or 3-channel (with 'BGR' channel order) images can be saved using this function, with these exceptions:\n• With OpenEXR encoder, only 32-bit float (CV_32F) images can be saved.\n• With Radiance HDR encoder, non 64-bit float (CV_64F) images can be saved.\n• All images will be converted to 32-bit float (CV_32F).\n• With JPEG 2000 encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n• With PAM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n• With PNG encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n• PNG images with an alpha channel can be saved using this function. To do this, create 8-bit (or 16-bit) 4-channel image BGRA, where the alpha channel goes last. Fully transparent pixels should have alpha set to 0, fully opaque pixels should have alpha set to 255/65535 (see the code sample below).\n• With PGM/PPM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n• With TIFF encoder, 8-bit unsigned (CV_8U), 16-bit unsigned (CV_16U), 32-bit float (CV_32F) and 64-bit float (CV_64F) images can be saved.\n• Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).\n• 32-bit float 3-channel (CV_32FC3) TIFF images will be saved using the LogLuv high dynamic range encoding (4 bytes per pixel) If the image format is not supported, the image will be converted to 8-bit unsigned (CV_8U) and saved that way. If the format, depth or channel order is different, use Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O functions to save the image to XML or YAML format. The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file. It also demonstrates how to save multiple images in a TIFF file: Name of the file. (Mat or vector of Mat) Image or Images to be saved."
    },
    {
        "link": "https://docs.opencv.org/4.x/d4/da8/group__imgcodecs.html",
        "document": "Checks if the specified image file can be decoded by OpenCV. The function haveImageReader checks if OpenCV is capable of reading the specified file. This can be useful for verifying support for a given image format before attempting to load an image. The name of the file to be checked. true if an image reader for the specified file is available and the file can be opened, false otherwise. The function checks the availability of image codecs that are either built into OpenCV or dynamically loaded. It does not check for the actual existence of the file but rather the ability to read the specified file type. If the file cannot be opened or the format is unsupported, the function will return false.\n\nChecks if the specified image file or specified file extension can be encoded by OpenCV. The function haveImageWriter checks if OpenCV is capable of writing images with the specified file extension. This can be useful for verifying support for a given image format before attempting to save an image. The name of the file or the file extension (e.g., \".jpg\", \".png\"). It is recommended to provide the file extension rather than the full file name. true if an image writer for the specified extension is available, false otherwise. The function checks the availability of image codecs that are either built into OpenCV or dynamically loaded. It does not check for the actual existence of the file but rather the ability to write files of the given type.\n\nReads an image from a buffer in memory. The function imdecode reads an image from the specified buffer in the memory. If the buffer is too short or contains invalid data, the function returns an empty matrix ( Mat::data==NULL ). See cv::imread for the list of supported formats and flags description. In the case of color images, the decoded images will have the channels stored in B G R order. The same flags as in cv::imread, see cv::ImreadModes.\n\nThe function imdecodemulti reads a multi-page image from the specified buffer in the memory. If the buffer is too short or contains invalid data, the function returns false. See cv::imreadmulti for the list of supported formats and flags description. In the case of color images, the decoded images will have the channels stored in B G R order. The same flags as in cv::imread, see cv::ImreadModes. A vector of Mat objects holding each page, if more than one.\n\nSaves an image to a specified file. The function imwrite saves the image to the specified file. The image format is chosen based on the filename extension (see cv::imread for the list of extensions). In general, only 8-bit unsigned (CV_8U) single-channel or 3-channel (with 'BGR' channel order) images can be saved using this function, with these exceptions:\n• With OpenEXR encoder, only 32-bit float (CV_32F) images can be saved.\n• With Radiance HDR encoder, non 64-bit float (CV_64F) images can be saved.\n• All images will be converted to 32-bit float (CV_32F).\n• With JPEG 2000 encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n• With JPEG XL encoder, 8-bit unsigned (CV_8U), 16-bit unsigned (CV_16U) and 32-bit float(CV_32F) images can be saved.\n• JPEG XL images with an alpha channel can be saved using this function. To achieve this, create an 8-bit 4-channel (CV_8UC4) / 16-bit 4-channel (CV_16UC4) / 32-bit float 4-channel (CV_32FC4) BGRA image, ensuring the alpha channel is the last component. Fully transparent pixels should have an alpha value of 0, while fully opaque pixels should have an alpha value of 255/65535/1.0.\n• With PAM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n• With PNG encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n• PNG images with an alpha channel can be saved using this function. To achieve this, create an 8-bit 4-channel (CV_8UC4) / 16-bit 4-channel (CV_16UC4) BGRA image, ensuring the alpha channel is the last component. Fully transparent pixels should have an alpha value of 0, while fully opaque pixels should have an alpha value of 255/65535(see the code sample below).\n• With PGM/PPM encoder, 8-bit unsigned (CV_8U) and 16-bit unsigned (CV_16U) images can be saved.\n• With TIFF encoder, 8-bit unsigned (CV_8U), 8-bit signed (CV_8S), 16-bit unsigned (CV_16U), 16-bit signed (CV_16S), 32-bit signed (CV_32S), 32-bit float (CV_32F) and 64-bit float (CV_64F) images can be saved.\n• Multiple images (vector of Mat) can be saved in TIFF format (see the code sample below).\n• 32-bit float 3-channel (CV_32FC3) TIFF images will be saved using the LogLuv high dynamic range encoding (4 bytes per pixel)\n• With GIF encoder, 8-bit unsigned (CV_8U) images can be saved.\n• GIF images with an alpha channel can be saved using this function. To achieve this, create an 8-bit 4-channel (CV_8UC4) BGRA image, ensuring the alpha channel is the last component. Fully transparent pixels should have an alpha value of 0, while fully opaque pixels should have an alpha value of 255.\n• 8-bit single-channel images (CV_8UC1) are not supported due to GIF's limitation to indexed color formats. If the image format is not supported, the image will be converted to 8-bit unsigned (CV_8U) and saved that way. If the format, depth or channel order is different, use Mat::convertTo and cv::cvtColor to convert it before saving. Or, use the universal FileStorage I/O functions to save the image to XML or YAML format. The sample below shows how to create a BGRA image, how to set custom compression parameters and save it to a PNG file. It also demonstrates how to save multiple images in a TIFF file: Returns a reference to the specified array element. the number of rows and columns or (-1, -1) when the matrix has more than 2 dimensions Checks a condition at runtime and throws exception if it fails. Saves an image to a specified file. Name of the file. (Mat or vector of Mat) Image or Images to be saved."
    },
    {
        "link": "https://docs.opencv.org/3.4/db/deb/tutorial_display_image.html",
        "document": "In this tutorial you will learn how to:\n• Read an image from file (using cv::imread)\n• Display an image in an OpenCV window (using cv::imshow)\n• Write an image to a file (using cv::imwrite)\n\nIn OpenCV 3 we have multiple modules. Each one takes care of a different area or approach towards image processing. You could already observe this in the structure of the user guide of these tutorials itself. Before you use any of them you first need to include the header files where the content of each individual module is declared. You'll almost always end up using the:\n• core section, as here are defined the basic building blocks of the library\n• imgcodecs module, which provides functions for reading and writing\n• highgui module, as this contains the functions to show an image in a window We also include the iostream to facilitate console line output and input. By declaring , in the following, the library functions can be accessed without explicitly stating the namespace.\n\nAs a first step, the OpenCV python library is imported. The proper way to do this is to additionally assign it the name cv, which is used in the following to reference the library.\n\nNow, let's analyze the main code. As a first step, we read the image \"starry_night.jpg\" from the OpenCV samples. In order to do so, a call to the cv::imread function loads the image using the file path specified by the first argument. The second argument is optional and specifies the format in which we want the image. This may be:\n• IMREAD_COLOR loads the image in the BGR 8-bit format. This is the default that is used here.\n• IMREAD_UNCHANGED loads the image as is (including the alpha channel if present)\n• IMREAD_GRAYSCALE loads the image as an intensity one\n\nAfter reading in the image data will be stored in a cv::Mat object.\n\nAfterwards, a check is executed, if the image was loaded correctly.\n\nThen, the image is shown using a call to the cv::imshow function. The first argument is the title of the window and the second argument is the cv::Mat object that will be shown.\n\nBecause we want our window to be displayed until the user presses a key (otherwise the program would end far too quickly), we use the cv::waitKey function whose only parameter is just how long should it wait for a user input (measured in milliseconds). Zero means to wait forever. The return value is the key that was pressed.\n\nIn the end, the image is written to a file if the pressed key was the \"s\"-key. For this the cv::imwrite function is called that has the file path and the cv::Mat object as an argument."
    },
    {
        "link": "https://stackoverflow.com/questions/74051197/imread-function-of-opencv-cant-open-my-images-what-im-doing-wrong",
        "document": "as the title suggest i have problems with opencv and in particulare with imread function. This is the simple program I'm trying to run\n\nMy CMakeLists looks like this\n\nThe message error that I'm getting is: [ WARN:[email protected]] global /opt/opencv/modules/imgcodecs/src/loadsave.cpp (244) findDecoder imread_('Images/NewYork.jpg'): can't open/read file: check file path/integrity terminate called after throwing an instance of 'cv::Exception'\n\nI know that the second error is due to the fact that the Mat object is empty and that's becuase my imread function does'nt work. I already checked if my path is correct, i saved my image in different directories, i checked the spelling, i checked evertyhing and i can't make it work. Please help."
    },
    {
        "link": "https://registry.khronos.org/OpenGL-Refpages/gl4/html/glTexImage2D.xhtml",
        "document": "Specifies the data type of the pixel data. The following symbolic values are accepted: GL_UNSIGNED_BYTE , GL_BYTE , GL_UNSIGNED_SHORT , GL_SHORT , GL_UNSIGNED_INT , GL_INT , GL_HALF_FLOAT , GL_FLOAT , GL_UNSIGNED_BYTE_3_3_2 , GL_UNSIGNED_BYTE_2_3_3_REV , GL_UNSIGNED_SHORT_5_6_5 , GL_UNSIGNED_SHORT_5_6_5_REV , GL_UNSIGNED_SHORT_4_4_4_4 , GL_UNSIGNED_SHORT_4_4_4_4_REV , GL_UNSIGNED_SHORT_5_5_5_1 , GL_UNSIGNED_SHORT_1_5_5_5_REV , GL_UNSIGNED_INT_8_8_8_8 , GL_UNSIGNED_INT_8_8_8_8_REV , GL_UNSIGNED_INT_10_10_10_2 , and GL_UNSIGNED_INT_2_10_10_10_REV .\n\nSpecifies the format of the pixel data. The following symbolic values are accepted: GL_RED , GL_RG , GL_RGB , GL_BGR , GL_RGBA , GL_BGRA , GL_RED_INTEGER , GL_RG_INTEGER , GL_RGB_INTEGER , GL_BGR_INTEGER , GL_RGBA_INTEGER , GL_BGRA_INTEGER , GL_STENCIL_INDEX , GL_DEPTH_COMPONENT , GL_DEPTH_STENCIL .\n\nSpecifies the height of the texture image, or the number of layers in a texture array, in the case of the GL_TEXTURE_1D_ARRAY and GL_PROXY_TEXTURE_1D_ARRAY targets. All implementations support 2D texture images that are at least 1024 texels high, and texture arrays that are at least 256 layers deep.\n\nSpecifies the width of the texture image. All implementations support texture images that are at least 1024 texels wide.\n\nSpecifies the number of color components in the texture. Must be one of base internal formats given in Table 1, one of the sized internal formats given in Table 2, or one of the compressed internal formats given in Table 3, below.\n\nSpecifies the level-of-detail number. Level 0 is the base image level. Level n is the nth mipmap reduction image. If target is GL_TEXTURE_RECTANGLE or GL_PROXY_TEXTURE_RECTANGLE , level must be 0.\n\nTexturing allows elements of an image array to be read by shaders.\n\nTo define texture images, call . The arguments describe the parameters of the texture image, such as height, width, width of the border, level-of-detail number (see glTexParameter), and number of color components provided. The last three arguments describe how the image is represented in memory.\n\nIf is , , , or , no data is read from , but all of the texture image state is recalculated, checked for consistency, and checked against the implementation's capabilities. If the implementation cannot handle a texture of the requested texture size, it sets all of the image state to 0, but does not generate an error (see glGetError). To query for an entire mipmap array, use an image array level greater than or equal to 1.\n\nIf is , or one of the targets, data is read from as a sequence of signed or unsigned bytes, shorts, or longs, or single-precision floating-point values, depending on . These values are grouped into sets of one, two, three, or four values, depending on , to form elements. Each data byte is treated as eight 1-bit elements, with bit ordering determined by (see glPixelStore).\n\nIf is , data is interpreted as an array of one-dimensional images.\n\nIf a non-zero named buffer object is bound to the target (see glBindBuffer) while a texture image is specified, is treated as a byte offset into the buffer object's data store.\n\nThe first element corresponds to the lower left corner of the texture image. Subsequent elements progress left-to-right through the remaining texels in the lowest row of the texture image, and then in successively higher rows of the texture image. The final element corresponds to the upper right corner of the texture image.\n\ndetermines the composition of each element in . It can assume one of these symbolic values:\n\nIf an application wants to store the texture at a certain resolution or in a certain format, it can request the resolution and format with . The GL will choose an internal representation that closely approximates that requested by , but it may not match exactly. (The representations specified by , , , and must match exactly.)\n\nmay be one of the base internal formats shown in Table 1, below\n\nmay also be one of the sized internal formats shown in Table 2, below\n\nFinally, may also be one of the generic or compressed texture formats shown in Table 3 below\n\nIf the parameter is one of the generic compressed formats, , , , or , the GL will replace the internal format with the symbolic constant for a specific internal format and compress the texture before storage. If no corresponding internal format is available, or the GL can not compress that image for any reason, the internal format is instead replaced with a corresponding base internal format.\n\nIf the parameter is , , , or , the texture is treated as if the red, green, or blue components are encoded in the sRGB color space. Any alpha component is left unchanged. The conversion from the sRGB encoded component to a linear component is:\n\nAssume is the sRGB component in the range [0,1].\n\nUse the , , , or target to try out a resolution and format. The implementation will update and recompute its best match for the requested storage resolution and format. To then query this state, call glGetTexLevelParameter. If the texture cannot be accommodated, texture state is set to 0.\n\nA one-component texture image uses only the red component of the RGBA color extracted from . A two-component image uses the R and G values. A three-component image uses the R, G, and B values. A four-component image uses all of the RGBA components.\n\nImage-based shadowing can be enabled by comparing texture r coordinates to depth texture values to generate a boolean result. See glTexParameter for details on texture comparison."
    },
    {
        "link": "https://stackoverflow.com/questions/9863969/updating-a-texture-in-opengl-with-glteximage2d",
        "document": "Are glTexImage2D and glTexSubImage2D the only ways to pass a buffer of pixels to a texture?\n\nAt the moment I use in a setup function glTexImage2D passing null as the buffer, and then on the render loop I call glTexSubImage2D with the new buffer data on each iteration.\n\nBut knowing that the texture will not change any property such as the dimensions, is there any more efficient way to pass the actual pixel data to the rendering texture?"
    },
    {
        "link": "https://developer.apple.com/library/archive/documentation/GraphicsImaging/Conceptual/OpenGL-MacProgGuide/opengl_texturedata/opengl_texturedata.html",
        "document": "Best Practices for Working with Texture Data\n\nTextures add realism to OpenGL objects. They help objects defined by vertex data take on the material properties of real-world objects, such as wood, brick, metal, and fur. Texture data can originate from many sources, including images.\n\nMany of the same techniques your application uses on vertex data can also be used to improve texture performance.\n\nTextures start as pixel data that flows through an OpenGL program, as shown in Figure 11-2.\n\nThe precise route that texture data takes from your application to its final destination can impact the performance of your application. The purpose of this chapter is to provide techniques you can use to ensure optimal processing of texture data in your application. This chapter\n• None shows how to use OpenGL extensions to optimize performance\n• None provides information on working with textures whose dimensions are not a power of two\n\nWithout any optimizations, texture data flows through an OpenGL program as shown in Figure 11-3. Data from your application first goes to the OpenGL framework, which may make a copy of the data before handing it to the driver. If your data is not in a native format for the hardware (see Optimal Data Formats and Types), the driver may also make a copy of the data to convert it to a hardware-specific format for uploading to video memory. Video memory, in turn, can keep a copy of the data. Theoretically, there could be four copies of your texture data throughout the system. Data flows at different rates through the system, as shown by the size of the arrows in Figure 11-3. The fastest data transfer happens between VRAM and the GPU. The slowest transfer occurs between the OpenGL driver and VRAM. Data moves between the application and the OpenGL framework, and between the framework and the driver at the same \"medium\" rate. Eliminating any of the data transfers, but the slowest one in particular, will improve application performance. There are several extensions you can use to eliminate one or more data copies and control how texture data travels from your application to the GPU:\n• None allows your application to use OpenGL buffer objects to manage texture and image data. As with vertex buffer objects, they allow your application to hint how a buffer is used and to decide when data is copied to OpenGL.\n• None allows you to prevent OpenGL from copying your texture data into the client. Instead, OpenGL keeps the memory pointer you provided when creating the texture. Your application must keep the texture data at that location until the referencing OpenGL texture is deleted.\n• None , along with a storage hint, either or , allows you to specify a single block of texture memory and manage it as you see fit. Here are some recommendations:\n• None If your application requires optimal texture upload performance, use and together to manage your textures.\n• None If your application requires optimal texture download performance, use pixel buffer objects.\n• None If your application requires cross-platform techniques, use pixel buffer objects for both texture uploads and texture downloads.\n• None Use when your source images are not aligned to a power-of-2 size. The sections that follow describe the extensions and show how to use them. Pixel buffer objects are a core feature of OpenGL 2.1 and also available through the extension. The procedure for setting up a pixel buffer object is almost identical to that of vertex buffer objects.\n• None Call the function to create a new name for a buffer object. is the number of buffers you wish to create identifiers for. specifies a pointer to memory to store the buffer names.\n• None Call the function to bind an unused name to a buffer object. After this call, the newly created buffer object is initialized with a memory buffer of size zero and a default state. (For the default setting, see the OpenGL specification for ARB_vertex_buffer_object.) should be be set to to use the buffer as the source of pixel data. specifies the unique name for the buffer object.\n• None Create and initialize the data store of the buffer object by calling the function . Essentially, this call uploads your data to the GPU. must be set to . specifies the size of the data store. points to the source data. If this is not , the source data is copied to the data store of the buffer object. If , the contents of the data store are undefined. is a constant that provides a hint as to how your application plans to use the data store. For more details on buffer hints, see Buffer Usage Hints\n• None Whenever you call , or similar functions that read pixel data from the application, those functions use the data in the bound pixel buffer object instead.\n• None To update the data in the buffer object, your application calls . Mapping the buffer prevents the GPU from operating on the data, and gives your application a pointer to memory it can use to update the buffer. must be set to . indicates the operations you plan to perform on the data. You can supply , , or .\n• None Modify the texture data using the pointer provided by map buffer.\n• None When you have finished modifying the texture, call the function . You should supply . Once the buffer is unmapped, your application can no longer access the buffer’s data through the pointer, and the buffer’s contents are uploaded again to the GPU. normally blocks until previous commands have completed, which includes the slow process of copying the pixel data to the application. However, if you call while a pixel buffer object is bound, the function returns immediately. It does not block until you actually map the pixel buffer object to read its content.\n• None Call the function to create a new name for a buffer object. is the number of buffers you wish to create identifiers for. specifies a pointer to memory to store the buffer names.\n• None Call the function to bind an unused name to a buffer object. After this call, the newly created buffer object is initialized with a memory buffer of size zero and a default state. (For the default setting, see the OpenGL specification for ARB_vertex_buffer_object.) should be be set to to use the buffer as the destination for pixel data. specifies the unique name for the buffer object.\n• None Create and initialize the data store of the buffer object by calling the function . must be set to . specifies the size of the data store. points to the source data. If this is not , the source data is copied to the data store of the buffer object. If , the contents of the data store are undefined. is a constant that provides a hint as to how your application plans to use the data store. For more details on buffer hints, see Buffer Usage Hints\n• None Call or a similar function. The function inserts a command to read the pixel data into the bound pixel buffer object and then returns.\n• None To take advantage of asynchronous pixel reads, your application should perform other work.\n• None To retrieve the data in the pixel buffer object, your application calls . This blocks OpenGL until the previously queued command completes, maps the data, and provides a pointer to your application. must be set to . indicates the operations you plan to perform on the data. You can supply , , or .\n• None Write vertex data to the pointer provided by map buffer.\n• None When you no longer need the vertex data, call the function . You should supply . Once the buffer is unmapped, the data is no longer accessible to your application. Using Pixel Buffer Objects to Keep Data on the GPU There is no difference between a vertex buffer object and a pixel buffer object except for the target to which they are bound. An application can take the results in one buffer and use them as another buffer type. For example, you could use the pixel results from a fragment shader and reinterpret them as vertex data in a future pass, without ever leaving the GPU:\n• None Set up your first pass and submit your drawing commands.\n• None Bind a pixel buffer object and call to fetch the intermediate results into a buffer.\n• None Bind the same buffer as a vertex buffer.\n• None Set up the second pass of your algorithm and submit your drawing commands. Keeping your intermediate data inside the GPU when performing multiple passes can result in great performance increases. The Apple client storage extension ( ) lets you provide OpenGL with a pointer to memory that your application allocates and maintains. OpenGL retains a pointer to your data but does not copy the data. Because OpenGL references your data, your application must retain its copy of the data until all referencing textures are deleted. By using this extension you can eliminate the OpenGL framework copy as shown in Figure 11-4. Note that a texture width must be a multiple of 32 bytes for OpenGL to bypass the copy operation from the application to the OpenGL framework. The Apple client storage extension defines a pixel storage parameter, , that you pass to the OpenGL function to specify that your application retains storage for textures. The following code sets up client storage: For detailed information, see the OpenGL specification for the Apple client storage extension. The Apple texture range extension ( ) lets you define a region of memory used for texture data. Typically you specify an address range that encompasses the storage for a set of textures. This allows the OpenGL driver to optimize memory usage by creating a single memory mapping for all of the textures. You can also provide a hint as to how the data should be stored: cached or shared. The cached hint specifies to cache texture data in video memory. This hint is recommended when you have textures that you plan to use multiple times or that use linear filtering. The shared hint indicates that data should be mapped into a region of memory that enables the GPU to access the texture data directly (via DMA) without the need to copy it. This hint is best when you are using large images only once, perform nearest-neighbor filtering, or need to scale down the size of an image. The texture range extension defines the following routine for making a single memory mapping for all of the textures used by your application: is a valid texture target, such as . specifies the number of bytes in the address space referred to by the parameter. points to the address space that your application provides for texture storage. You provide the hint parameter and a parameter value to to the OpenGL function . The possible values for the storage hint parameter ( ) are or . Some hardware requires texture dimensions to be a power-of-two before the hardware can upload the data using DMA. The rectangle texture extension ( ) was introduced to allow texture targets for textures of any dimensions—that is, rectangle textures ( ). You need to use the rectangle texture extension together with the Apple texture range extension to ensure OpenGL uses DMA to access your texture data. These extensions allow you to bypass the OpenGL driver, as shown in Figure 11-5. Note that OpenGL does not use DMA for a power-of-two texture target ( ). So, unlike the rectangular texture, the power-of-two texture will incur one additional copy and performance won't be quite as fast. The performance typically isn't an issue because games, which are the applications most likely to use power-of-two textures, load textures at the start of a game or level and don't upload textures in real time as often as applications that use rectangular textures, which usually play video or display images. The next section has code examples that use the texture range and rectangle textures together with the Apple client storage extension. For detailed information on these extensions, see the OpenGL specification for the Apple texture range extension and the OpenGL specification for the ARB texture rectangle extension. You can use the Apple client storage extension along with the Apple texture range extension to streamline the texture data path in your application. When used together, OpenGL moves texture data directly into video memory, as shown in Figure 11-6. The GPU directly accesses your data (via DMA). The set up is slightly different for rectangular and power-of-two textures. The code examples in this section upload textures to the GPU. You can also use these extensions to download textures, see Downloading Texture Data. Listing 11-1 shows how to use the extensions for a rectangular texture. After enabling the texture rectangle extension you need to bind the rectangular texture to a target. Next, set up the storage hint. Call to set up the Apple client storage extension. Finally, call the function with a with a rectangular texture target and a pointer to your texture data. Note: The texture rectangle extension limits what can be done with rectangular textures. To understand the limitations in detail, read the OpenGL extension for texture rectangles. See Working with Non–Power-of-Two Textures for an overview of the limitations and an alternative to using this extension. Setting up a power-of-two texture to use these extensions is similar to what's needed to set up a rectangular texture, as you can see by looking at Listing 11-2. The difference is that the texture target replaces the texture target.\n\nOpenGL is often used to process video and images, which typically have dimensions that are not a power-of-two. Until OpenGL 2.0, the texture rectangle extension ( ) provided the only option for a rectangular texture target. This extension, however, imposes the following restrictions on rectangular textures:\n• None You can't use mipmap filtering with them.\n• None You can use only these wrap modes: , , and .\n• None The texture cannot have a border.\n• None The texture uses non-normalized texture coordinates. (See Figure 11-7.) OpenGL 2.0 adds another option for a rectangular texture target through the extension, which supports these textures without the limitations of the extension. Before using it, you must check to make sure the functionality is available. You'll also want to consult the OpenGL specification for the non—power-of-two extension. If your code runs on a system that does not support either the or extensions you have these options for working with with rectangular images:\n• None Use the OpenGL function to scale the image so that it fits in a rectangle whose dimensions are a power of two. The image undoes the scaling effect when you draw the image from the properly sized rectangle back into a polygon that has the correct aspect ratio for the image. Note: This option can result in the loss of some data. But if your application runs on hardware that doesn't support the extension, you may need to use this option.\n• None Segment the image into power-of-two rectangles, as shown in Figure 11-8 by using one image buffer and different texture pointers. Notice how the sides and corners of the image shown in Figure 11-8 are segmented into increasingly smaller rectangles to ensure that every rectangle has dimensions that are a power of two. Special care may be needed at the borders between each segment to avoid filtering artifacts if the texture is scaled or rotated.\n\nOpenGL on the Macintosh provides several options for creating high-quality textures from image data. OS X supports floating-point pixel values, multiple image file formats, and a variety of color spaces. You can import a floating-point image into a floating-point texture. Figure 11-9 shows an image used to texture a cube. Figure 11-9 Using an image as a texture for a cube For Cocoa, you need to provide a bitmap representation. You can create an object from the contents of an object. You can use the Image I/O framework (see CGImageSource Reference). This framework has support for many different file formats, floating-point data, and a variety of color spaces. Furthermore, it is easy to use. You can import image data as a texture simply by supplying a object that specifies the location of the texture. There is no need for you to convert the image to an intermediate integer RGB format. You can use the class or a subclass of it for texturing in OpenGL. The process is to first store the image data from an object in an object so that the image data is in a format that can be readily used as texture data by OpenGL. Then, after setting up the texture target, you supply the bitmap data to the OpenGL function . Note that you must have a valid, current OpenGL context set up. Note: You can't create an OpenGL texture from image data that's provided by a view created from the following classes: , , and . This is because these views do not use the window backing store, which is what the method reads from. Listing 11-3 shows a routine that uses this process to create a texture from the contents of an object. A detailed explanation for each numbered line of code appears following the listing. Listing 11-3 Building an OpenGL texture from an object Here's what the code does:\n• None Initializes the object with bitmap data from the current view.\n• None Gets the number of samples per pixel.\n• None Sets the appropriate unpacking row length for the bitmap.\n• None Sets the byte-aligned unpacking that's needed for bitmaps that are 3 bytes per pixel.\n• None If a texture object is not passed in, generates a new texture object.\n• None Binds the texture name to the texture target.\n• None Sets filtering so that it does not use a mipmap, which would be redundant for the texture rectangle extension.\n• None Checks to see if the bitmap is nonplanar and is either a 24-bit RGB bitmap or a 32-bit RGBA bitmap. If so, retrieves the pixel data using the method, passing it along with other appropriate parameters to the OpenGL function for specifying a 2D texture image. Quartz images ( data type) are defined in the Core Graphics framework ( ) while the image source data type for reading image data and creating Quartz images from an image source is declared in the Image I/O framework ( ). Quartz provides routines that read a wide variety of image data. To use a Quartz image as a texture source, follow these steps:\n• None Create a Quartz image source by supplying a object to the function .\n• None Create a Quartz image by extracting an image from the image source, using the function .\n• None Extract the image dimensions using the function and . You'll need these to calculate the storage required for the texture.\n• None Create a Quartz bitmap graphics context for drawing. Make sure to set up the context for pre-multiplied alpha.\n• None Draw the image to the bitmap context.\n• None Set the pixel storage mode by calling the function .\n• None Set up the appropriate texture parameters. Listing 11-4 shows a code fragment that performs these steps. Note that you must have a valid, current OpenGL context. For more information on using Quartz, see Quartz 2D Programming Guide, CGImage Reference, and CGImageSource Reference. You can use the Image I/O framework together with a Quartz data provider to obtain decompressed raw pixel data from a source image, as shown in Listing 11-5. You can then use the pixel data for your OpenGL texture. The data has the same format as the source image, so you need to make sure that you use a source image that has the layout you need. Alpha is not premultiplied for the pixel data obtained in Listing 11-5, but alpha is premultiplied for the pixel data you get when using the code described in Creating a Texture from a Cocoa View and Creating a Texture from a Quartz Image Source.\n\nA texture download operation uses the same data path as an upload operation except that the data path is reversed. Downloading transfers texture data, using direct memory access (DMA), from VRAM into a texture that can then be accessed directly by your application. You can use the Apple client range, texture range, and texture rectangle extensions for downloading, just as you would for uploading. To download texture data using the Apple client storage, texture range, and texture rectangle extensions:\n• None Call the function to copy a texture subimage from the specified window coordinates. This call initiates an asynchronous DMA transfer to system memory the next time you call a flush routine. The CPU doesn't wait for this call to complete.\n• None Call the function to transfer the texture into system memory. Note that the parameters must match the ones that you used to set up the texture when you called the function . This call is the synchronization point; it waits until the transfer is finished. Listing 11-6 shows a code fragment that downloads a rectangular texture that uses cached memory. Your application processes data between the and calls. How much processing? Enough so that your application does not need to wait for the GPU. // Do other work processing here, using a double or triple buffer\n\nWhen you use any technique that allows the GPU to access your texture data directly, such as the texture range extension, it's possible for the GPU and CPU to access the data at the same time. To avoid such a collision, you must synchronize the GPU and the CPU. The simplest way is shown in Figure 11-10. Your application works on the data, flushes it to the GPU and waits until the GPU is finished before working on the data again. One technique for ensuring that the GPU is finished executing commands before your application sends more data is to insert a token into the command stream and use that to determine when the CPU can touch the data again, as described in Use Fences for Finer-Grained Synchronization. Figure 11-10 uses the fence extension command to synchronize buffer updates for a stream of single-buffered texture data. Notice that when the CPU is processing texture data, the GPU is idle. Similarly, when the GPU is processing texture data, the CPU is idle. It's much more efficient for the GPU and CPU to work asynchronously than to work synchronously. Double buffering data is a technique that allows you to process data asynchronously, as shown in Figure 11-11. To double buffer data, you must supply two sets of data to work on. Note in Figure 11-11 that while the GPU is rendering one frame of data, the CPU processes the next. After the initial startup, neither processing unit is idle. Using the function provided by the fence extension ensures that buffer updating is synchronized."
    },
    {
        "link": "https://github.com/fendevel/Guide-to-Modern-OpenGL-Functions",
        "document": "\n• Ideal Way Of Retrieving All Uniform Names\n\nWhat this is:\n• A guide on how to apply modern OpenGL functionality.\n\nWhat this is not:\n\nWhen I say modern I'm talking DSA modern, not VAO modern, because that's old modern or \"middle\" GL (however I will be covering some from it), I can't tell you what minimal version you need to make use of DSA because it's not clear at all but you can check if you support it yourself with something like glew's or checking your API version.\n\nWith DSA we, in theory, can keep our bind count outside of drawing operations at zero. Great right? Sure, but if you were to research how to use all the new DSA functions you'd have a hard time finding anywhere where it's all explained, which is what this guide is all about.\n\nThe wiki page does a fine job comparing the DSA naming convention to the traditional one so I stole their table:\n• The texture related calls aren't hard to figure out so let's jump right in.\n• is the equivalent of + (for initialization).\n\nUnlike will create the handle and initialize the object which is why the field is listed as the internal initialization depends on knowing the type.\n• is the equivalent of\n\nThere isn't much to say about this family of functions; they're used exactly the same but take in the texture name rather than the texture target.\n• is semi-equivalent to (Where is glTextureImage?).\n\nThe and families are the same exact way.\n• is the equivalent of +\n\nDefeats the need for:\n\nAnd replaces it with a simple:\n• is the equivalent of\n\nTakes in the texture name instead of the texture target.\n\nI should briefly point out that in order to upload cube map textures you need to use .\n\nIf you look at the OpenGL function listing you will see a lack of and here's why:\n\nHaving to build up a valid texture object piecewise as you would with left plenty of room for mistakes and required the driver to do validation as late as possible, and so when it came time to specify a new set of texture functions they took the opportunity to address this rough spot. The result was .\n\nStorage provides a way to create complete textures with checks done on-call, which means less room for error, it solves most if not all problems brought on by mutable textures.\n\ntl;dr \"Immutable textures are a more robust approach to handle textures\"\n• However be mindful as allocating immutable textures requires physical video memory to be available upfront rather than having the driver deal with when and where the data goes, this means it's very possible to unintentionally exceed your card's capacity.\n\nSources: ARB_texture_storage, \"What does glTexStorage do?\", \"What's the DSA version of glTexImage2D?\"\n• is the equivalent of\n\nis used exactly the same but initializes the object for you.\n\nEverything else is pretty much the same but takes in the framebuffer handle instead of the target.\n• is the equivalent of\n\nThe difference here is that we no longer need to bind the two framebuffers and specify which is which through the and enums.\n• is the equivalent of\n\nThere are two ways to go about clearing a framebuffer:\n\nThe more familar way\n\nand the more versatile per-attachment way\n\nis the attachment index, so it would be equivalent to , and the draw buffer index for depth is always .\n\nAs you can see with we can clear the texels of any attachment to some value, both methods are similar enough that you could reimplement the functions of method 1 using those of method 2.\n\nDespite the name it has nothing to do with buffer objects and this gets cleared up with the DSA version:\n\nSo the DSA version looks like this:\n\ncan be if you're clearing the default framebuffer.\n\nNone of the DSA glBuffer functions ask for the buffer target and is only required to be specified whilst drawing.\n• is the equivalent of + (the initialization part)\n\nis used exactly like its traditional equivalent and automatically initializes the object.\n• is the equivalent of\n\nis just like but instead of requiring the buffer target it takes in the buffer handle itself.\n• and are the equivalent of\n\nIf you aren't familiar with the application of it is used like so:\n\nisn't much different, the main thing with it is that it's one out of a two-parter with .\n\nIn order to get out the same effect as the previous snippet we first need to make a call to . Despite Bind being in the name it isn't the same kind as for instance: .\n\nHere's how they're both put into action:\n\nAlthough this is the newer way of going about it this isn't fully DSA as we still need to make that VAO bind call, to go all the way we need to transform , , , and into , , , and .\n\nThe version that takes in the VAO for binding the VBO, , has an equivalent for the IBO: .\n\nAll together this is how uploading an indexed model with only DSA should look:\n\nhas been in core since version 4.3 and it's a big step up from how we used to do error polling.\n\nWith Debug Output we can receive meaningful messages on the state of the GL through a callback function that we'll be providing.\n\nAll it takes to get running are two calls: & .\n\nYour callback must have the signature .\n\nThere will be times when you want to filter your messages, maybe you're interested in anything but notifications. OpenGL has a function for this: .\n\nHere's how we use it to disable notifications:\n\nSomething we can do is have messages fire synchronously where it will call on the same thread as the context and from within the OpenGL call. This way we can guarantee function call order and this means if we were to add a breakpoint into the definition of our callback we could traverse the call stack and locate the origin of the error.\n\nAll it takes is another call to with the value of , so you end up with this:\n\nMost material on OpenGL that touch on indexed drawing will separate vertex and index data between buffers, this is because the vertex_buffer_object spec strongly recommends to do so. The reasoning for this is that different GL implementations may have different memory type requirements, so having the index data in its own buffer allows the driver to decide the optimal storage strategy.\n\nThis was useful when there were were several ways to attach GPUs to the main system, technically there still are, but AGP was completely phased out by PCIe about a decade ago and regular PCI ports aren't really used for this anymore save a few cases.\n\nThe overhead that comes with managing an additional buffer for indexed geometry isn't as justifiable a trade off as it used to be.\n\nWe store the vertices and indices at known byte offsets and pass the information to OpenGL. For the vertices we do this with 's parameter, and indices through 's parameter.\n\nAnd then when it's time to render:\n\nIf you're curious why the pointer cast is necessary it's because in immediate mode you'd pass in your index buffer directly from host memory but in retained mode it's an offset into the buffer store.\n\nThere is material out there that teach beginners to retrieve uniform information by manually parsing the shader source strings, please don't do this.\n\nHere is how it should be done:\n\nNote that the parameter refers to the number of locations the uniform takes up with , , , etc. being 1 and arrays having it be the number of elements, the locations are arranged in a way that allows you to do to find the location of an element, so if you wanted to write to element it would be done like this:\n\nor if you want to modify the whole array:\n\nIdeally UBOs would be used when dealing with collections of data larger than 16K as it may be slower than packing the data into vec4s and using .\n\nWith this you can store the uniform datatype and check it within your uniform update functions.\n\nArray textures are a great way of managing collections of textures of the same size and format. They allow for using a set of textures without having to bind between them.\n\nThey turn out to be good as an alternative to atlases as long as some criteria are met, that being all the sub-textures, or swatches, fit under the same dimensions and levels.\n\nThe advantages of using this over an atlas is that each layer is treated as a separate texture in terms of wrapping and mipmapping.\n\nArray textures come with three targets: , , and .\n\n2D array textures and 3d textures are similar but are semantically different, the differences come in where the mipmap level are and how layers are filtered.\n\nThere is no built-in filtering for interpolation between layers where the Z part of a 3d texture will have filtering available. The same goes for 1d arrays and 2d textures.\n\nTo allocate a 2D texture array we do this:\n\nhas been modified to accommodate 2d array textures which I imagine is confusing at first but there's a pattern: the last dimension parameter acts as the layer specifier, so if you were to allocate a 1D texture array you would have to use with height as the layer capacity.\n\nAnyway, uploading to individual layers is very straightforward:\n\nThe most notable difference between arrays and atlases in terms of implementation lies in the shader.\n\nTo bind a texture array to the context you need a specialized sampler called . We will also need a uniform to store the layer id.\n\nIdeally you should calculate the layer coordinate outside of the shader.\n\nYou can take this way further and set up a little UBO/SSBO system of arrays containing layer id and texture array id pairs and update which layer id is used with regular uniforms.\n\nAlso, I advise against using ubos and ssbos for per object/draw stuff without a plan otherwise you will end up with everything not working as you'd like because the command queue has no involvement during the reads and writes.\n\nAs a bonus let me tell you an easy way to populate a texture array with parts of an atlas, in our case a basic rpg tileset.\n\nModern OpenGL comes with two generic memory copy functions: and . Here we'll be dealing with , this function allows us to copy sections of a source image to a region of a destination image. We're going to take advantage of its offset and size parameters so that we can copy tiles from every location and paste them in the appropriate layers within our texture array.\n\nHere it is:\n\nTexture views allow us to share a section of a texture's storage with an object of a different texture target and/or format. Share as in there's no copying of the texture data, any changes you make to a view's data is visible to all views that share the storage along with the original texture object.\n\nViews are mostly indistinguishable from regular texture objects so you can use them as if they are.\n\nThe original storage is only freed once all references to it are deleted, if you are familiar with C++'s it's very similar.\n\nWe can only make views if we use a target and format which is compatible with our original texture, you can read the format tables on the wiki.\n\nMaking the view itself is simple, it's only two function calls: & .\n\nDespite this being about modern OpenGL has an important role here: we need only an available texture name and nothing more; this needs to be a completely empty uninitialized object and because this function only handles the generation of a valid handle it's perfect for this.\n\nis how we'll create the view.\n\nIf we were to have a typical 2d texture array and needed a view of layer 5 in isolation this is how it would look:\n\nWith this you can bind layer 5 alone as a texture.\n\nThis is the exact same when dealing with cube maps, the layer parameters will correspond to the cube faces with the layer params of cube map arrays being .\n\nTexture views can be of other views as well, so there could be a texture array, and a view of a section of that array, and another view of a specific layer within that array view. The parameters are relative to the properties of the source.\n\nThe fact that we can specify which mipmaps we want in the view means that we can have views which are just of those specific mipmap levels, so for example you could make textures views of the Nth mipmap level of a bunch of textures and use only those for expensive texture dependant lighting calculations.\n• Nvidia drivers have spotty performance as they lean towards monolithic shader programs, so it may be better suited for non-performance-critical applications.\n\nProgram Pipeline objects allow us to change shader stages on the fly without having to relink them.\n\nTo create and set up a simple program pipeline without any debugging looks like this:\n\ngenerates the handle and initializes the object, generates, initializes, compiles, and links a shader program using the sources given, and attaches the program's stage(s) to the pipeline object. as you can tell binds the pipeline to the context.\n\nBecause our shaders are now looser and flexible we need to get stricter with our input and output variables. Either we declare the input and output in the same order with the same names or we make their locations explicitly match through the location qualifier.\n\nI greatly suggest the latter option for non-blocks, this will allow us to set up a well-defined interface while also being flexible with the naming and ordering. Interface blocks also need to match members.\n\nAs collateral for needing a stricter interface we also need to declare the built-in input and output blocks we wish to use for every stage.\n\nThe built-in block interfaces are defined as (from the wiki):\n\nAn extremely basic vertex shader enabled for use in a pipeline object looks like this:\n\nWith we can get a pointer to a region of memory that OpenGL will be using as a sort of intermediate buffer zone, this will allow us to make reads and writes to this area and let the driver decide when to use the contents.\n\nFirst we need the right flags for both buffer storage creation and the mapping itself:\n\nGL_MAP_COHERENT_BIT: This flag ensures writes will be seen by the server when done from the client and vice versa.\n\nGL_MAP_PERSISTENT_BIT: This tells our driver you wish to keep the mapping through subsequent OpenGL operations.\n\nGL_MAP_READ_BIT: Tells OpenGL we wish to read from the buffer.\n\nGL_MAP_WRITE_BIT: Lets OpenGL know we're gonna write to it, if you don't specify this anything could happen.\n\nIf we don't use these flags for the storage creation GL will reject your mapping request with scorn. What's worse is that you absolutely won't know unless you're doing some form of error checking.\n\nSetting up our immutable storage is straight forward:\n\nWhatever we put in the parameter is arbitrary and marking it as specifies we wish not to copy any data into it.\n\nHere is how we get that pointer we're after:\n\nI recommend mapping it as infrequently as possible because the process of mapping the buffer isn't particularly fast. In most cases you only need to do it just the once.\n\nMake sure to unmap the buffer before deleting it:\n\nIf C++20 is available you can drop it into a std::span."
    },
    {
        "link": "https://docs.gl/gl3/glTexImage2D",
        "document": "Specifies the data type of the pixel data. The following symbolic values are accepted: GL_UNSIGNED_BYTE , GL_BYTE , GL_UNSIGNED_SHORT , GL_SHORT , GL_UNSIGNED_INT , GL_INT , GL_FLOAT , GL_UNSIGNED_BYTE_3_3_2 , GL_UNSIGNED_BYTE_2_3_3_REV , GL_UNSIGNED_SHORT_5_6_5 , GL_UNSIGNED_SHORT_5_6_5_REV , GL_UNSIGNED_SHORT_4_4_4_4 , GL_UNSIGNED_SHORT_4_4_4_4_REV , GL_UNSIGNED_SHORT_5_5_5_1 , GL_UNSIGNED_SHORT_1_5_5_5_REV , GL_UNSIGNED_INT_8_8_8_8 , GL_UNSIGNED_INT_8_8_8_8_REV , GL_UNSIGNED_INT_10_10_10_2 , and GL_UNSIGNED_INT_2_10_10_10_REV .\n\nSpecifies the format of the pixel data. The following symbolic values are accepted: GL_RED , GL_RG , GL_RGB , GL_BGR , GL_RGBA , and GL_BGRA .\n\nSpecifies the height of the texture image, or the number of layers in a texture array, in the case of the GL_TEXTURE_1D_ARRAY and GL_PROXY_TEXTURE_1D_ARRAY targets. All implementations support 2D texture images that are at least 1024 texels high, and texture arrays that are at least 256 layers deep.\n\nSpecifies the width of the texture image. All implementations support texture images that are at least 1024 texels wide.\n\nSpecifies the number of color components in the texture. Must be one of the following symbolic constants: GL_RGBA32F , GL_RGBA32I , GL_RGBA32UI , GL_RGBA16 , GL_RGBA16F , GL_RGBA16I , GL_RGBA16UI , GL_RGBA8 , GL_RGBA8UI , GL_SRGB8_ALPHA8 , GL_RGB10_A2 , GL_RGB10_A2UI , GL_R11F_G11F_B10F , GL_RG32F , GL_RG32I , GL_RG32UI , GL_RG16 , GL_RG16F , GL_RGB16I , GL_RGB16UI , GL_RG8 , GL_RG8I , GL_RG8UI , GL_R32F , GL_R32I , GL_R32UI , GL_R16F , GL_R16I , GL_R16UI , GL_R8 , GL_R8I , GL_R8UI , GL_RGBA16_SNORM , GL_RGBA8_SNORM , GL_RGB32F , GL_RGB32I , GL_RGB32UI , GL_RGB16_SNORM , GL_RGB16F , GL_RGB16I , GL_RGB16UI , GL_RGB16 , GL_RGB8_SNORM , GL_RGB8 , GL_RGB8I , GL_RGB8UI , GL_SRGB8 , GL_RGB9_E5 , GL_RG16_SNORM , GL_RG8_SNORM , GL_COMPRESSED_RG_RGTC2 , GL_COMPRESSED_SIGNED_RG_RGTC2 , GL_R16_SNORM , GL_R8_SNORM , GL_COMPRESSED_RED_RGTC1 , GL_COMPRESSED_SIGNED_RED_RGTC1 , GL_DEPTH_COMPONENT32F , GL_DEPTH_COMPONENT24 , GL_DEPTH_COMPONENT16 , GL_DEPTH32F_STENCIL8 , GL_DEPTH24_STENCIL8 .\n\nSpecifies the level-of-detail number. Level 0 is the base image level. Level n is the nth mipmap reduction image. If target is GL_TEXTURE_RECTANGLE or GL_PROXY_TEXTURE_RECTANGLE , level must be 0.\n\nTexturing allows elements of an image array to be read by shaders.\n\nTo define texture images, call . The arguments describe the parameters of the texture image, such as height, width, width of the border, level-of-detail number (see glTexParameter), and number of color components provided. The last three arguments describe how the image is represented in memory.\n\nIf is , , , or , no data is read from , but all of the texture image state is recalculated, checked for consistency, and checked against the implementation's capabilities. If the implementation cannot handle a texture of the requested texture size, it sets all of the image state to 0, but does not generate an error (see glGetError). To query for an entire mipmap array, use an image array level greater than or equal to 1.\n\nIf is , or one of the targets, data is read from as a sequence of signed or unsigned bytes, shorts, or longs, or single-precision floating-point values, depending on . These values are grouped into sets of one, two, three, or four values, depending on , to form elements. Each data byte is treated as eight 1-bit elements, with bit ordering determined by (see glPixelStore).\n\nIf is , data is interpreted as an array of one-dimensional images.\n\nIf a non-zero named buffer object is bound to the target (see glBindBuffer) while a texture image is specified, is treated as a byte offset into the buffer object's data store.\n\nThe first element corresponds to the lower left corner of the texture image. Subsequent elements progress left-to-right through the remaining texels in the lowest row of the texture image, and then in successively higher rows of the texture image. The final element corresponds to the upper right corner of the texture image.\n\ndetermines the composition of each element in . It can assume one of these symbolic values:\n\nIf an application wants to store the texture at a certain resolution or in a certain format, it can request the resolution and format with . The GL will choose an internal representation that closely approximates that requested by , but it may not match exactly. (The representations specified by , , , and must match exactly.)\n\nIf the parameter is one of the generic compressed formats, , , , or , the GL will replace the internal format with the symbolic constant for a specific internal format and compress the texture before storage. If no corresponding internal format is available, or the GL can not compress that image for any reason, the internal format is instead replaced with a corresponding base internal format.\n\nIf the parameter is , , , or , the texture is treated as if the red, green, or blue components are encoded in the sRGB color space. Any alpha component is left unchanged. The conversion from the sRGB encoded component to a linear component is:\n\nAssume is the sRGB component in the range [0,1].\n\nUse the , , , or target to try out a resolution and format. The implementation will update and recompute its best match for the requested storage resolution and format. To then query this state, call glGetTexLevelParameter. If the texture cannot be accommodated, texture state is set to 0.\n\nA one-component texture image uses only the red component of the RGBA color extracted from . A two-component image uses the R and G values. A three-component image uses the R, G, and B values. A four-component image uses all of the RGBA components.\n\nImage-based shadowing can be enabled by comparing texture r coordinates to depth texture values to generate a boolean result. See glTexParameter for details on texture comparison."
    }
]