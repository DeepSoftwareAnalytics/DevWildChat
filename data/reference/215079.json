[
    {
        "link": "https://docs.aws.amazon.com/cli/latest/reference/kafkaconnect/update-connector.html",
        "document": "You are viewing the documentation for an older major version of the AWS CLI (version 1).\n\nAWS CLI version 2, the latest major version of AWS CLI, is now stable and recommended for general use. To view this page for the AWS CLI version 2, click here. For more information see the AWS CLI version 2 installation instructions and migration guide."
    },
    {
        "link": "https://awscli.amazonaws.com/v2/documentation/api/latest/reference/kafkaconnect/update-connector.html",
        "document": "Did you find this page useful? Do you have a suggestion to improve the documentation? Give us feedback. \n\n If you would like to suggest an improvement or fix for the AWS CLI, check out our contributing guide on GitHub."
    },
    {
        "link": "https://awscli.amazonaws.com/v2/documentation/api/2.8.7/reference/kafkaconnect/update-connector.html",
        "document": "Did you find this page useful? Do you have a suggestion to improve the documentation? Give us feedback. \n\n If you would like to suggest an improvement or fix for the AWS CLI, check out our contributing guide on GitHub."
    },
    {
        "link": "https://docs.aws.amazon.com/cli/latest/reference/transfer/update-connector.html",
        "document": "You are viewing the documentation for an older major version of the AWS CLI (version 1).\n\nAWS CLI version 2, the latest major version of AWS CLI, is now stable and recommended for general use. To view this page for the AWS CLI version 2, click here. For more information see the AWS CLI version 2 installation instructions and migration guide."
    },
    {
        "link": "https://docs.aws.amazon.com/it_it/cli/latest/reference/kafkaconnect/index.html",
        "document": "You are viewing the documentation for an older major version of the AWS CLI (version 1).\n\nAWS CLI version 2, the latest major version of AWS CLI, is now stable and recommended for general use. To view this page for the AWS CLI version 2, click here. For more information see the AWS CLI version 2 installation instructions and migration guide."
    },
    {
        "link": "https://docs.aws.amazon.com/msk/latest/developerguide/msk-connect-capacity.html",
        "document": "The total capacity of a connector depends on the number of workers that the connector has, as well as on the number of MSK Connect Units (MCUs) per worker. Each MCU represents 1 vCPU of compute and 4 GiB of memory. The MCU memory pertains to the total memory of a worker instance and not the heap memory in use.\n\nMSK Connect workers consume IP addresses in the customer-provided subnets. Each worker uses one IP address from one of the customer-provided subnets. You should ensure that you have enough available IP addresses in the subnets provided to a CreateConnector request to account for their specified capacity, especially when autoscaling connectors where the number of workers can fluctuate.\n\nTo create a connector, you must choose between one of the following two capacity modes.\n• Provisioned - Choose this mode if you know the capacity requirements for your connector. You specify two values:\n• The number of MCUs per worker.\n• Autoscaled - Choose this mode if the capacity requirements for your connector are variable or if you don't know them in advance. When you use autoscaled mode, Amazon MSK Connect overrides your connector's property with a value that is proportional to the number of workers running in the connector and the number of MCUs per worker. You specify three sets of values:\n• The minimum and maximum number of workers.\n• The scale-in and scale-out percentages for CPU utilization, which is determined by the metric. When the metric for the connector exceeds the scale-out percentage, MSK Connect increases the number of workers that are running in the connector. When the metric goes below the scale-in percentage, MSK Connect decreases the number of workers. The number of workers always remains within the minimum and maximum numbers that you specify when you create the connector.\n• The number of MCUs per worker.\n\nFor more information about workers, see Understand MSK Connect workers. To learn about MSK Connect metrics, see Monitoring MSK Connect."
    },
    {
        "link": "https://docs.aws.amazon.com/msk/latest/developerguide/msk-connect-examples.html",
        "document": "Did this page help you? - Yes\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of it."
    },
    {
        "link": "https://docs.aws.amazon.com/msk/latest/developerguide/msk-connect-connectors.html",
        "document": "A connector integrates external systems and Amazon services with Apache Kafka by continuously copying streaming data from a data source into your Apache Kafka cluster, or continuously copying data from your cluster into a data sink. A connector can also perform lightweight logic such as transformation, format conversion, or filtering data before delivering the data to a destination. Source connectors pull data from a data source and push this data into the cluster, while sink connectors pull data from the cluster and push this data into a data sink.\n\nThe following diagram shows the architecture of a connector. A worker is a Java virtual machine (JVM) process that runs the connector logic. Each worker creates a set of tasks that run in parallel threads and do the work of copying the data. Tasks don't store state, and can therefore be started, stopped, or restarted at any time in order to provide a resilient and scalable data pipeline."
    },
    {
        "link": "https://docs.aws.amazon.com/msk/latest/developerguide/bestpractices.html",
        "document": "This topic outlines some best practices to follow when using Amazon MSK. For information about Amazon MSK Replicator best practices, see Best practices for using MSK Replicator.\n\nYour application's availability and performance depends not only on server-side settings but on client settings as well.\n\nRight-size your cluster: Number of partitions per Standard broker\n\nThe following table shows the recommended number of partitions (including leader and follower replicas) per Standard broker. The recommended number of partitions are not enforced and are a best practice for scenarios where you are sending traffic across all provisioned topic partitions.\n\nIf you have high partition, low throughput use cases where you have higher partition counts, but you are not sending traffic across all partitions, you can pack more partitions per broker, as long as you have performed sufficient testing and performance testing to validate that your cluster remains healthy with the higher partition count. If the number of partitions per broker exceeds the maximum allowed value and your cluster becomes overloaded, you will be prevented from performing the following operations:\n\nA high number of partitions can also result in missing Kafka metrics on CloudWatch and on Prometheus scraping.\n\nFor guidance on choosing the number of partitions, see Apache Kafka Supports 200K Partitions Per Cluster . We also recommend that you perform your own testing to determine the right size for your brokers. For more information about the different broker sizes, see Amazon MSK broker types.\n\nRight-size your cluster: Number of Standard brokers per cluster\n\nTo determine the right number of Standard brokers for your MSK Provisioned cluster and understand costs, see the MSK Sizing and Pricing spreadsheet. This spreadsheet provides an estimate for sizing an MSK Provisioned cluster and the associated costs of Amazon MSK compared to a similar, self-managed, EC2-based Apache Kafka cluster. For more information about the input parameters in the spreadsheet, hover over the parameter descriptions. Estimates provided by this sheet are conservative and provide a starting point for a new MSK Provisioned cluster. Cluster performance, size, and costs are dependent on your use case and we recommend that you verify them with actual testing.\n\nTo understand how the underlying infrastructure affects Apache Kafka performance, see Best practices for right-sizing your Apache Kafka clusters to optimize performance and cost in the AWS Big Data Blog. The blog post provides information about how to size your clusters to meet your throughput, availability, and latency requirements. It also provides answers to questions, such as when you should scale up versus scale out, and guidance about how to continuously verify the size of your production clusters. For information about tiered storage based clusters, see Best practices for running production workloads using Amazon MSK tiered storage .\n\nWhen using m5.4xl, m7g.4xl, or larger instances, you can optimize the MSK Provisioned cluster throughput by tuning the num.io.threads and num.network.threads configurations.\n\nNum.io.threads is the number of threads that a Standard broker uses for processing requests. Adding more threads, up to the number of CPU cores supported for the instance size, can help improve cluster throughput.\n\nNum.network.threads is the number of threads the Standard broker uses for receiving all incoming requests and returning responses. Network threads place incoming requests on a request queue for processing by io.threads. Setting num.network.threads to half the number of CPU cores supported for the instance size allows for full usage of the new instance size.\n\nThe ID of a topic is lost (Error: does not match the topic Id for partition) when you use a Kafka AdminClient version lower than 2.8.0 with the flag to increase or reassign topic partitions for an MSK Provisioned cluster using Kafka version 2.8.0 or higher. Note that the flag is deprecated in Kafka 2.5 and is removed starting with Kafka 3.0. See Upgrading to 2.5.0 from any version 0.8.x through 2.4.x .\n\nTo prevent topic ID mismatch, use a Kafka client version 2.8.0 or higher for Kafka admin operations. Alternatively, clients 2.5 and higher can use the flag instead of the flag.\n\nUse the following recommendations so that your MSK Provisioned clusters can be highly available during an update (such as when you're updating the broker size or Apache Kafka version, for example) or when Amazon MSK is replacing a broker.\n\nAmazon MSK strongly recommends that you maintain the total CPU utilization for your brokers (defined as ) under 60%. When you have at least 40% of your cluster's total CPU available, Apache Kafka can redistribute CPU load across brokers in the cluster when necessary. One example of when this is necessary is when Amazon MSK detects and recovers from a broker fault; in this case, Amazon MSK performs automatic maintenance, like patching. Another example is when a user requests a broker-size change or version upgrade; in these two cases, Amazon MSK deploys rolling workflows that take one broker offline at a time. When brokers with lead partitions go offline, Apache Kafka reassigns partition leadership to redistribute work to other brokers in the cluster. By following this best practice you can ensure you have enough CPU headroom in your cluster to tolerate operational events like these.\n\nYou can use Amazon CloudWatch metric math to create a composite metric that is . Set an alarm that gets triggered when the composite metric reaches an average CPU utilization of 60%. When this alarm is triggered, scale the cluster using one of the following options:\n\nTo avoid running out of disk space for messages, create a CloudWatch alarm that watches the metric. When the value of this metric reaches or exceeds 85%, perform one or more of the following actions:\n\nFor information on how to set up and use alarms, see Using Amazon CloudWatch Alarms. For a full list of Amazon MSK metrics, see Monitor an Amazon MSK Provisioned cluster.\n\nConsuming messages doesn't remove them from the log. To free up disk space regularly, you can explicitly specify a retention time period, which is how long messages stay in the log. You can also specify a retention log size. When either the retention time period or the retention log size are reached, Apache Kafka starts removing inactive segments from the log.\n\nTo specify a retention policy at the cluster level, set one or more of the following parameters: , , , or . For more information, see Custom Amazon MSK configurations.\n\nYou can also specify retention parameters at the topic level:\n\nThe retention parameters that you specify at the topic level take precedence over cluster-level parameters.\n\nAfter an unclean shutdown, a broker can take a while to restart as it does log recovery. By default, Kafka only uses a single thread per log directory to perform this recovery. For example, if you have thousands of partitions, log recovery can take hours to complete. To speed up log recovery, it's recommended to increase the number of threads using configuration property . You can set it to the number of CPU cores.\n\nWe recommend that you monitor the memory that Apache Kafka uses. Otherwise, the cluster may become unavailable.\n\nTo determine how much memory Apache Kafka uses, you can monitor the metric. is the percentage of total heap memory that is in use after garbage collection. We recommend that you create a CloudWatch alarm that takes action when increases above 60%.\n\nThe steps that you can take to decrease memory usage vary. They depend on the way that you configure Apache Kafka. For example, if you use transactional message delivery, you can decrease the value in your Apache Kafka configuration from ms to ms (from 7 days to 1 day). This decreases the memory footprint of each transaction.\n\nFor ZooKeeper-based MSK Provisioned clusters, if you use Apache ZooKeeper commands to add brokers, these brokers don't get added to your MSK Provisioned cluster, and your Apache ZooKeeper will contain incorrect information about the cluster. This might result in data loss. For supported MSK Provisioned cluster operations, see Amazon MSK key features and concepts.\n\nFor information about encryption in transit and how to enable it, see Amazon MSK encryption in transit.\n\nTo move partitions to different brokers on the same MSK Provisioned cluster, you can use the partition reassignment tool named . We recommend that you don't reassign more than 10 partitions in a single call for safe operations. For example, after you add new brokers to expand a cluster or to move partitions in order to removing brokers, you can rebalance that cluster by reassigning partitions to the new brokers. For information about how to add brokers to an MSK Provisioned cluster, see Expand the number of brokers in an Amazon MSK cluster. For information about how to remove brokers from an MSK Provisioned cluster, see Remove a broker from an Amazon MSK cluster. For information about the partition reassignment tool, see Expanding your cluster in the Apache Kafka documentation."
    },
    {
        "link": "https://docs.aws.amazon.com/en_us/msk/latest/developerguide/msk-connect-kafka-connect-topics.html",
        "document": "An Apache Kafka Connect application that’s running in distributed mode stores its state by using internal topics in the Kafka cluster and group membership. The following are the configuration values that correspond to the internal topics that are used for Kafka Connect applications:\n• In the configuration topic, Kafka Connect stores the configuration of all the connectors and tasks that have been started by users. Each time users update the configuration of a connector or when a connector requests a reconfiguration (for example, the connector detects that it can start more tasks), a record is emitted to this topic. This topic is compaction enabled, so it always keeps the last state for each entity.\n• In the offsets topic, Kafka Connect stores the offsets of the source connectors. Like the configuration topic, the offsets topic is compaction enabled. This topic is used to write the source positions only for source connectors that produce data to Kafka from external systems. Sink connectors, which read data from Kafka and send to external systems, store their consumer offsets by using regular Kafka consumer groups.\n• In the status topic, Kafka Connect stores the current state of connectors and tasks. This topic is used as the central place for the data that is queried by users of the REST API. This topic allows users to query any worker and still get the status of all running plugins. Like the configuration and offsets topics, the status topic is also compaction enabled.\n\nIn addition to these topics, Kafka Connect makes extensive use of Kafka’s group membership API. The groups are named after the connector name. For example, for a connector named file-sink, the group is named connect-file-sink. Each consumer in the group provides records to a single task. These groups and their offsets can be retrieved by using regular consumer groups tools, such as . For each sink connector, the Connect runtime runs a regular consumer group that extracts records from Kafka."
    }
]