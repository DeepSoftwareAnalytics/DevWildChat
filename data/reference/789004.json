[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html",
        "document": "Read more in the User Guide.\n\nThe number of clusters to form as well as the number of centroids to generate. For an example of how to choose an optimal value for refer to Selecting the number of clusters with silhouette analysis on KMeans clustering.\n• None ‘k-means++’ : selects initial cluster centroids using sampling based on an empirical probability distribution of the points’ contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is “greedy k-means++”. It differs from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them.\n• None ‘random’: choose observations (rows) at random from data for the initial centroids.\n• None If an array is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n• None If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization. For an example of how to use the different strategies, see A demo of K-Means clustering on the handwritten digits data. For an evaluation of the impact of initialization, see the example Empirical evaluation of the impact of k-means initialization. Number of times the k-means algorithm is run with different centroid seeds. The final results is the best output of consecutive runs in terms of inertia. Several runs are recommended for sparse high-dimensional problems (see Clustering sparse data with k-means). When , the number of runs depends on the value of init: 10 if using or is a callable; 1 if using or is an array-like. Changed in version 1.4: Default value for changed to . Maximum number of iterations of the k-means algorithm for a single run. Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence. Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See Glossary. When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. Note that if the original data is not C-contiguous, a copy will be made even if copy_x is False. If the original data is sparse, but not in CSR format, a copy will be made even if copy_x is False. K-means algorithm to use. The classical EM-style algorithm is . The variation can be more efficient on some datasets with well-defined clusters, by using the triangle inequality. However it’s more memory intensive due to the allocation of an extra array of shape . Changed in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”. Changed “auto” to use “lloyd” instead of “elkan”. Coordinates of cluster centers. If the algorithm stops before fully converging (see and ), these will not be consistent with . Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided. Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nThe k-means problem is solved using either Lloyd’s or Elkan’s algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of samples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. Refer to “How slow is the k-means method?” D. Arthur and S. Vassilvitskii - SoCG2006. for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That’s why it can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of or ), and will not be consistent, i.e. the will not be the means of the points in each cluster. Also, the estimator will reassign after the last iteration to make consistent with on the training set.\n\nFor examples of common problems with K-Means and how to address them see Demonstration of k-means assumptions.\n\nFor a demonstration of how K-Means can be used to cluster text documents see Clustering text documents using k-means.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example Comparison of the K-Means and MiniBatchKMeans clustering algorithms.\n\nFor a comparison between K-Means and BisectingKMeans refer to example Bisecting K-Means and Regular K-Means Performance Comparison.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://scikit-learn.org/1.0/auto_examples/cluster/plot_cluster_iris.html",
        "document": ""
    },
    {
        "link": "https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/cluster/_kmeans.py",
        "document": ""
    },
    {
        "link": "https://scikit-learn.org/stable/modules/clustering.html",
        "document": "Clustering of unlabeled data can be performed with the module .\n\nEach clustering algorithm comes in two variants: a class, that implements the method to learn the clusters on train data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the attribute.\n\ncreates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given. Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the preference, which controls how many exemplars are used, and the damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages. The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \\(O(N^2 T)\\), where \\(N\\) is the number of samples and \\(T\\) is the number of iterations until convergence. Further, the memory complexity is of the order \\(O(N^2)\\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets. The messages sent between points belong to one of two categories. The first is the responsibility \\(r(i, k)\\), which is the accumulated evidence that sample \\(k\\) should be the exemplar for sample \\(i\\). The second is the availability \\(a(i, k)\\) which is the accumulated evidence that sample \\(i\\) should choose sample \\(k\\) to be its exemplar, and considers the values for all other samples that \\(k\\) should be an exemplar. In this way, exemplars are chosen by samples if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves. More formally, the responsibility of a sample \\(k\\) to be the exemplar of sample \\(i\\) is given by: Where \\(s(i, k)\\) is the similarity between samples \\(i\\) and \\(k\\). The availability of sample \\(k\\) to be the exemplar of sample \\(i\\) is given by: To begin with, all values for \\(r\\) and \\(a\\) are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor \\(\\lambda\\) is introduced to iteration process: where \\(t\\) indicates the iteration times.\n• None Demo of affinity propagation clustering algorithm: Affinity Propagation on a synthetic 2D datasets with 3 classes\n• None Visualizing the stock market structure Affinity Propagation on financial time series to find groups of companies\n\nThe algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, and , which define formally what we mean when we say dense. Higher or lower indicate higher density necessary to form a cluster. More formally, we define a core sample as being a sample in the dataset such that there exist other samples within a distance of , which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster. Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least in distance from any core sample, is considered an outlier by the algorithm. While the parameter primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below). In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below. The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering. The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see . This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g., with sparse matrices). This matrix will consume \\(n^2\\) floats. A couple of mechanisms for getting around this are:\n• None Use OPTICS clustering in conjunction with the method. OPTICS clustering also calculates the full pairwise matrix, but only keeps one row in memory at a time (memory complexity n).\n• None A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with . See .\n• None The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a when fitting DBSCAN.\n• None A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n• None DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.\n\nThe algorithm shares many similarities with the algorithm, and can be considered a generalization of DBSCAN that relaxes the requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a distance, and a spot within the cluster attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for , then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given value using the method. Setting to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points. The reachability distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining reachability distances and data set produces a reachability plot, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. ‘Cutting’ the reachability plot at a single value produces DBSCAN like results; all points above the ‘cut’ are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter . There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster. The results from OPTICS method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise. Note that for any single value of , DBSCAN will tend to have a shorter run time than OPTICS; however, for repeated runs at varying values, a single run of OPTICS may require less cumulative runtime than DBSCAN. It is also important to note that OPTICS’ output is close to DBSCAN’s only if and are close. Spatial indexing trees are used to avoid calculating the full distance matrix, and allow for efficient memory usage on large sets of samples. Different distance metrics can be supplied via the keyword. For large datasets, similar (but not identical) results can be obtained via . The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain \\(n\\) (as opposed to \\(n^2\\)) memory scaling; however, tuning of the parameter will likely need to be used to give a solution in a reasonable amount of wall time.\n• None “OPTICS: ordering points to identify the clustering structure.” Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999."
    },
    {
        "link": "https://stackoverflow.com/questions/26903091/sklearn-pipeline-how-to-build-for-kmeans-clustering-text",
        "document": "The above will be training set for clustering text using kmeans.\n\nThe above is my test set.\n\nI have built a vectorizer and the model as shown below:\n\nIf I try to predict the cluster for my test set of \"list2\":\n\nI get the error below:\n\nI was told to use to solve this issue. So I wrote the code below:\n\nBut I get the error:\n\nI think that maybe the output of does not implement a fit and transform, but how do I do that in this particular case? I'm new to Machine Learning. Also, how to get the labels from the kmeans model? When I run kmeans, I can access the cluster labels by using . How to do something similar in Pipeline?"
    },
    {
        "link": "https://codesignal.com/learn/courses/k-means-clustering-decoded/lessons/visualizing-k-means-clustering-on-an-iris-dataset-with-matplotlib",
        "document": "Let's see how to apply KMeans from sklearn and visualize results using Matplotlib. Horizontal and vertical indicators alone may not provide all the clarity we need in a plot. That's where Python's Matplotlib shines — its assortment of plot customization capabilities. Let's delve into customizing the colormap and other parameters for your plots for improving visualization and readability.\n\nBefore jumping into plotting, let's cluster our data first:\n\nA colormap is like an artist's palette, essentially mapping values to colors on a plot. Matplotlib offers a variety of default colormaps. The function uses a colormap to define the colors of markers.\n\nThe colormap can be specified using the parameter of the method.\n\nThe parameter passed to the method is a list of the same length as that specifies the color of each point.\n\nWith Sklearn's implementation, we instantiate the KMeans model with the desired number of clusters, then fit it to our data. The labels of the clusters for each data point are obtained via the attribute of the model, while the attribute gives us the centroid of these clusters. These are then visualized through the function in Matplotlib just as we did before. Check out the plot presented below:"
    },
    {
        "link": "https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html",
        "document": "In the previous few sections, we have explored one category of unsupervised machine learning models: dimensionality reduction. Here we will move on to another class of unsupervised machine learning models: clustering algorithms. Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points. Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering, which is implemented in . We begin with the standard imports:\n\nThe k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:\n• The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n• Each point is closer to its own cluster center than to other cluster centers. Those two assumptions are the basis of the k-means model. We will soon dive into exactly how the algorithm reaches this solution, but for now let's take a look at a simple dataset and see the k-means result. First, let's generate a two-dimensional dataset containing four distinct blobs. To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization\n\nExpectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science. k-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here. In short, the expectation–maximization approach here consists of the following procedure:\n• Repeat until converged\n• M-Step: set the cluster centers to the mean Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to. The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster. The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics. We can visualize the algorithm as shown in the following figure. For the particular initialization shown here, the clusters converge in just three iterations. For an interactive version of this figure, refer to the code in the Appendix."
    },
    {
        "link": "https://w3schools.com/python/python_ml_k-means.asp",
        "document": "On this page, W3schools.com collaborates with NYC Data Science Academy, to deliver digital training content to our students.\n\nK-means is an unsupervised learning method for clustering data points. The algorithm iteratively divides data points into K clusters by minimizing the variance in each cluster.\n\nHere, we will show you how to estimate the best value for K using the elbow method, then use K-means clustering to group the data points into clusters.\n\nHow does it work?\n\nFirst, each data point is randomly assigned to one of the K clusters. Then, we compute the centroid (functionally the center) of each cluster, and reassign each data point to the cluster with the closest centroid. We repeat this process until the cluster assignments for each data point are no longer changing.\n\nK-means clustering requires us to select K, the number of clusters we want to group the data into. The elbow method lets us graph the inertia (a distance-based metric) and visualize the point at which it starts decreasing linearly. This point is referred to as the \"elbow\" and is a good estimate for the best value for K based on our data.\n\nNow we utilize the elbow method to visualize the intertia for different values of K:\n\nThe elbow method shows that 2 is a good value for K, so we retrain and visualize the result:\n\nImport the modules you need.\n\nYou can learn about the Matplotlib module in our \"Matplotlib Tutorial.\n\nCreate arrays that resemble two variables in a dataset. Note that while we only use two variables here, this method will work with any number of variables:\n\nTurn the data into a set of points:\n\nIn order to find the best value for K, we need to run K-means across our data for a range of possible values. We only have 10 data points, so the maximum number of clusters is 10. So for each value K in range(1,11), we train a K-means model and plot the intertia at that number of clusters:\n\nWe can see that the \"elbow\" on the graph above (where the interia becomes more linear) is at K=2. We can then fit our K-means algorithm one more time and plot the different clusters assigned to the data:"
    },
    {
        "link": "https://askpython.com/python/examples/plot-k-means-clusters-python",
        "document": "In this article we’ll see how we can plot K-means Clusters.\n\nK-means Clustering is an iterative clustering method that segments data into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centroid).\n\nThis article demonstrates how to visualize the clusters. We’ll use the digits dataset for our cause.\n\nFirst Let’s get our data ready.\n\nDigits dataset contains images of size 8×8 pixels, which is flattened to create a feature vector of length 64. We used PCA to reduce the number of dimensions so that we can visualize the results using a 2D Scatter plot.\n\nNow, let’s apply K-mean to our data to create clusters.\n\nHere in the digits dataset we already know that the labels range from 0 to 9, so we have 10 classes (or clusters).\n\nBut in real-life challenges when performing K-means the most challenging task is to determine the number of clusters.\n\nThere are various methods to determine the optimum number of clusters, i.e. Elbow method, Average Silhouette method. But determining the number of clusters will be the subject of another talk.\n\nmethod returns the array of cluster labels each data point belongs to.\n\nNow, it’s time to understand and see how can we plot individual clusters.\n\nThe array of labels preserves the index or sequence of the data points, so we can utilize this characteristic to filter data points using Boolean indexing with numpy.\n\nLet’s visualize cluster with label 0 using the matplotlib library.\n\nThe code above first filters and keeps the data points that belong to cluster label 0 and then creates a scatter plot.\n\nSee how we passed a Boolean series to filter [label == 0]. Indexed the filtered data and passed to as (x,y) to plot. x = filtered_label0[:, 0] , y = filtered_label0[:, 1].\n\nNow, that we have some idea, let’s plot clusters with label 2 and 8.\n\nNow, that we got the working mechanism let’s apply it to all the clusters.\n\nThe above code iterates filtering the data according to each unique class one iteration at a time. The result we get is the final visualization of all the clusters.\n\nHere’s the complete code of what we just saw above.\n\nIn this article, we saw how we can visualize the clusters formed by the k-means algorithm. Till we meet next time, Happy Learning!"
    },
    {
        "link": "https://stackoverflow.com/questions/43541187/how-can-i-plot-a-kmeans-text-clustering-result-with-matplotlib",
        "document": "Here is a longer, better answer with more data:\n\nThere are two clear clusters in the data: one is a description of the crash, the other is a summary of the fatalities. It is easy to comment out lines and tune the cluster sizes up and down a little. As written the code should show two blue clusters, one larger and one smaller, with two orange centroids. There are more items of data than there are markers: some of the rows of data are transformed onto identical points in space."
    }
]