[
    {
        "link": "https://github.com/elastic/go-elasticsearch",
        "document": "The official Go client for Elasticsearch.\n\nDownload the latest version of Elasticsearch or sign-up for a free trial of Elastic Cloud.\n\nStarting from version , this library follow the Go language policy. Each major Go release is supported until there are two newer major releases. For example, Go 1.5 was supported until the Go 1.7 release, and Go 1.6 was supported until the Go 1.8 release.\n\nLanguage clients are forward compatible; meaning that clients support communicating with greater or equal minor versions of Elasticsearch. Elasticsearch language clients are only backwards compatible with default distributions and without guarantees made.\n\nWhen using Go modules, include the version in the import path, and specify either an explicit version or a branch:\n\nIt's possible to use multiple versions of the client in a single project:\n\nThe branch of the client is compatible with the current branch of Elasticsearch.\n\nRefer to the Installation section of the getting started documentation.\n\nRefer to the Connecting section of the getting started documentation.\n\nThe package provides convenience helpers for working with the client. At the moment, it provides the and the helpers.\n\nThe folder contains a number of recipes and comprehensive examples to get you started with the client, including configuration and customization of the client, using a custom certificate authority (CA) for security (TLS), mocking the transport for unit tests, embedding the client in a custom type, building queries, performing requests individually and in bulk, and parsing the responses.\n\nThis software is licensed under the Apache 2 license. See NOTICE."
    },
    {
        "link": "https://elastic.co/guide/en/elasticsearch/client/go-api/current/index.html",
        "document": "The official Go client provides one-to-one mapping with Elasticsearch REST APIs.\n\nGet to know the Go client\n\nThe Go gopher available here is created by Renee French and released under the Creative Commons Attribution-Share Alike 4.0 International license."
    },
    {
        "link": "https://elastic.co/guide/en/elasticsearch/client/go-api/current/examples.html",
        "document": "This sections lists a series of frequent use cases that will help you start with this new API.\n\nFor this example on how to create an index, lets create an index named and provide a mapping for the field which will be an integer. Notice how using the builder for the will automatically apply the correct value for the field.\n\nThe standard way of indexing a document is to provide a to the method, the standard will be run on your structure and the result will be sent to Elasticsearch. Alternatively, you can use the method and provide the already serialized JSON:\n\nRetrieving a document follows the API as part of the argument of the endpoint. In order you provide the , the and then run the query:\n\nIf you do not wish to retrieve the content of the document and want only to check if it exists in your index, we provide the shortcut: if exists, err := es.Exists(\"index_name\", \"doc_id\").IsSuccess(nil); exists { // The document exists ! } else if err != nil { // Handle error. } Result is if everything succeeds, if the document doesn’t exist. If an error occurs during the request, you will be granted with a and the relevant error.\n\nBuilding a search query can be done with structs or builder. As an example, let’s search for a document with a field with a value of in the index named . The targeted index for this search. The request part of the search. The query is run with a and returns the response. It produces the following JSON:"
    },
    {
        "link": "https://pkg.go.dev/github.com/elastic/go-elasticsearch/v7",
        "document": "The official Go client for Elasticsearch.\n\nLanguage clients are forward compatible; meaning that clients support communicating with greater or equal minor versions of Elasticsearch. Elasticsearch language clients are only backwards compatible with default distributions and without guarantees made.\n\nWhen using Go modules, include the version in the import path, and specify either an explicit version or a branch:\n\nIt's possible to use multiple versions of the client in a single project:\n\nThe branch of the client is compatible with the current branch of Elasticsearch.\n\nAdd the package to your file:\n\nThe package ties together two separate packages for calling the Elasticsearch APIs and transferring data over HTTP: and , respectively.\n\nUse the function to create the client with the default settings.\n\nWhen you export the environment variable, it will be used to set the cluster endpoint(s). Separate multiple adresses by a comma.\n\nTo set the cluster endpoint(s) programatically, pass a configuration object to the function.\n\nTo set the username and password, include them in the endpoint URL, or use the corresponding configuration options.\n\nTo set a custom certificate authority used to sign the certificates of cluster nodes, use the configuration option.\n\nTo configure other HTTP settings, pass an object in the configuration object.\n\nSee the and files for more examples of configuration and customization of the client. See the for an example of a security configuration.\n\nThe following example demonstrates a more complex usage. It fetches the Elasticsearch version from the cluster, indexes a couple of documents concurrently, and prints the search results, using a lightweight wrapper around the response body.\n\nAs you see in the example above, the package allows to call the Elasticsearch APIs in two distinct ways: either by creating a struct, such as , and calling its method by passing it a context and the client, or by calling the function on the client directly, using the option functions such as . See more information and examples in the package documentation.\n\nThe package handles the transfer of data to and from Elasticsearch, including retrying failed requests, keeping a connection pool, discovering cluster nodes and logging.\n\nRead more about the client internals and usage in the following blog posts:\n\nThe package provides convenience helpers for working with the client. At the moment, it provides the and the helpers.\n\nThe folder contains a number of recipes and comprehensive examples to get you started with the client, including configuration and customization of the client, using a custom certificate authority (CA) for security (TLS), mocking the transport for unit tests, embedding the client in a custom type, building queries, performing requests individually and in bulk, and parsing the responses.\n\n(c) 2019 Elasticsearch. Licensed under the Apache License, Version 2.0."
    },
    {
        "link": "https://codingexplorations.com/blog/indexing-documents-elasticsearch-go",
        "document": "Elasticsearch has become a cornerstone technology for developers looking to implement quick and powerful search capabilities into their applications. This distributed, RESTful search and analytics engine allows you to store, search, and analyze big volumes of data quickly and in near real time. When paired with the Go programming language and the Go-Elasticsearch library, it offers a robust setup for managing scalable search applications.\n\nGetting Started with Elasticsearch and Go\n\nBefore we dive into the nuts and bolts of indexing documents, let’s set up our environment. Here’s what you need to get rolling:\n• None Elasticsearch: Ensure you have Elasticsearch installed and running on your machine. You can download it from the official Elasticsearch website.\n• None Go Environment: If you haven’t already, set up your Go environment. You can download and install Go from the official Go website.\n• None Go-Elasticsearch Library: Install the Go-Elasticsearch library, which provides a robust interface to interact with your Elasticsearch cluster. Install it using Go’s package manager:"
    },
    {
        "link": "https://logit.io/blog/post/using-index-patterns-to-search-your-logs-and-metrics-with-kibana",
        "document": "Some of the leading benefits of using Logit.io for ELK Stack hosting include:\n\nAfter your trial period ends you can choose from a range of cost-efficient pricing plans that are most suitable for their requirements with overage protection being included as standard across all plans.\n\nAll new Logit.io stacks come pre-configured with a number of useful index patterns to help you get started. To get started with using Logit.io simply sign up to our platform and experience 14-days of free access to create stacks backed by ELK, Grafana or Open Distro for Elasticsearch (ODFE). Also with OpenTelemetry (OTel) and Logit.io, you can gain end-to-end observability effortlessly. Essentially, OpenTelemetry standardizes all data from any source, enabling automated analysis and as a result, saving your engineers hours every month.\n\nAn index pattern is a string with optional wildcards. The index pattern can match the name of a single index or include wildcards ( * ) to match multiple indices.\n\nAll logs and metrics that you send to Logit.io belong to an index pattern. To search your data with Kibana you have to select which Elasticsearch index or indices that you want to explore. You can do this in Kibana by configuring index patterns.\n\nOnce you have signed up and have selected \"launch Kibana\" from your provisioning dashboard, you will need to complete the following steps to view your index patterns:\n\nAll new Logit.io ELK stacks provide you with default indexes, including:\n\nThe index pattern enables you to search all fields for any logs sent to Logit.io using the Filebeat shipper, this is an example of an index pattern matching on a single index.\n\nTip: When you access Kibana for the very first time the default index pattern is set to search log data from all indices being sent to Elasticsearch (a multiple indices match), the pattern is .\n\nHow To Create an Index\n\nThe \"Create Index Pattern\" button is found above the list of existing index patterns as shown below:\n\nSelect this and Kibana will display the list of indices for which logs are available. You will see that the \"Next Step\" button is disabled and will only become available when the specified index name that you have entered matches any indices.\n\nWhen you are setting up a new index pattern, if your index contains one or more timestamp fields you will be asked to select one. This is the field that will be used to filter your data by time. If you do not wish to filter your search by timestamp you can select the \"I don't want to use the Time Filter\" option.\n\nTip: You can give fields a timestamp during mapping by using Index Templates, read more about Index Templates and Mappings.\n\nIf you want the new index pattern to be designated as your default pattern to load whenever you select the \"Discover\" tab then click the favourite star button after the index has finished being created. This is located in the top right-hand side of the screen.\n\nIf you add any further index mapping, Kibana automatically scans the indices that match each pattern to display a list of the new fields. It does not however, automatically pick them up.\n\nYou can refresh the index pattern to pick up any newly-added fields by selecting the index pattern and then clicking the \"refresh\" icon in the top right-hand side of the screen.\n\nWhen refreshing you will be prompted to reset the popularity counters for each field. Kibana keeps track of the fields that you've used the most often and the place where this data is stored is called a popularity counter. The data is used to sort fields within lists, by refreshing the index pattern this data and ability to sort will be reset.\n\nHow To Delete An Index Pattern\n\nTo delete an index pattern, select the index from the Index Patterns page and then click the \"delete\" icon in the top right-hand side of the screen (next to the default and refresh icons previously mentioned).\n\nYou can recreate an index pattern again at any time in the future but you will also lose all visualisations, saved searches, and other saved objects that reference the pattern as well as all data in any popularity counters, so be careful!\n\nIn the event that you need any help with configuring Kibana index patterns, our support team are always on hand to help, simply reach out to us via live chat and we'll be able to help answer any of your questions and help you get started.\n\nIf you want to continue learning more about Kibana then why not review this helpful Kibana query language cheat sheet that covers most of the common queries you’ll encounter on your analysis journey.\n\nIf you enjoyed this guide on using index patterns to search logs and metrics with Kibana then why not check out our article on Github vs Gitlab."
    },
    {
        "link": "https://elastic.co/guide/en/kibana/current/data-views.html",
        "document": "Kibana requires a data view to access the Elasticsearch data that you want to explore. A data view can point to one or more indices, data streams, or index aliases. For example, a data view can point to your log data from yesterday, or all indices that contain your data.\n\nIf you collected data using one of the Kibana ingest options, uploaded a file, or added sample data, you get a data view for free, and can start exploring your data. If you loaded your own data, follow these steps to create a data view.\n\nWant to explore your data or create a visualization without saving it as a data view? Select Use without saving in the Create data view form in Discover or Lens. With a temporary data view, you can add fields and create an Elasticsearch query alert, just like you would a regular data view. Your work won’t be visible to others in your space.\n\nA temporary data view remains in your space until you change apps, or until you save it.\n\nA data view can match one rollup index. For a combination rollup data view with both raw and rolled up data, use the standard notation:\n\nFor an example, refer to Create and visualize rolled up data.\n\nIf your Elasticsearch clusters are configured for cross-cluster search, you can create a data view to search across the clusters of your choosing. Specify data streams, indices, and aliases in a remote cluster using the following syntax:\n\nTo query Logstash indices across two Elasticsearch clusters that you set up for cross-cluster search, named and :\n\nUse wildcards in your cluster names to match any number of clusters. To search Logstash indices across clusters named , , and so on:\n\nTo query across all Elasticsearch clusters that have been configured for cross-cluster search, use a standalone wildcard for your cluster name:\n\nTo match indices starting with , but exclude those starting with , from all clusters having a name starting with :\n\nExcluding a cluster avoids sending any network calls to that cluster. To exclude a cluster with the name :\n\nOnce you configure a data view to use the cross-cluster search syntax, all searches and aggregations using that data view in Kibana take advantage of cross-cluster search.\n\nFor more information, refer to Excluding clusters or indicies from cross-cluster search.\n\nWhen you delete a data view, you cannot recover the associated field formatters, runtime fields, source filters, and field popularity data. Deleting a data view does not remove any indices or data documents from Elasticsearch.\n\nThe browser caches data view field lists for increased performance. This is particularly impactful for data views with a high field count that span a large number of indices and clusters. The field list is updated every couple of minutes in typical Kibana usage. Alternatively, use the refresh button on the data view management detail page to get an updated field list. A force reload of Kibana has the same effect.\n\nThe field list may be impacted by changes in indices and user permissions."
    },
    {
        "link": "https://forum.opensearch.org/t/kibana-discover-custom-index-pattern/4913",
        "document": "Can you suggest some security model to me.\n\nI have few groups of users with custom indinces access rights. For example fliebeat-nginx-a-* filebeat-nginx-b-* filebeat-java-a-* filebeat-java-b-*\n\nThen such a user logs in to kibana and trying to discover indices (default index pattern is “asterisk”) he receives no data error.\n\n If i creating index pattern like fliebeat-nginx-a-* filebeat-nginx-b-* filebeat-java-a-* filebeat-java-b-* it works, but it is not very useful, as i have to create as many index patterns as datasets i have.\n\nI think my problem can be solved by creating tenant for every dataset i have.\n\nMaybe you have suggestions how to create default index pattern * and every user can see own dataset in discover depending on his access rights"
    },
    {
        "link": "https://stackoverflow.com/questions/42711986/how-to-create-index-pattern-for-kibana",
        "document": "I am new to Kibana. I am learning it from Docs. It is written in docs that \"Specify an index pattern that matches the name of one or more of your Elasticsearch indices.\" I am not able to understand how to create Elasticsearch indices and where to save it so as to specify an index pattern in Kibana."
    },
    {
        "link": "https://educba.com/kibana-index-pattern",
        "document": "As the Elasticsearch server index has been created and therefore the Apache logs are becoming pushed thereto, our next task is to configure Kibana to read Elasticsearch index data. First, we’d like to open Kibana using its default port number: http://localhost:5601. To add the Elasticsearch index data to Kibana, we’ve to configure the index pattern. This will be the first step to work with Elasticsearch data. In this topic, we are going to learn about Kibana Index Pattern.\n\nTo create a new index pattern, we have to follow steps:\n• First, click on the Management link, which is on the left side menu.\n• After that, click on the Index Patterns tab, which is just on the Management tab. This will open a new window screen like the following screen:\n\nThe above screenshot shows us the basic metricbeat index pattern fields, their data types, and additional details. This metricbeat index pattern is already created just as a sample.\n• Now, we have to click on the index pattern option, which is just below the tab of the Index pattern, to create a new pattern. This will open the new window screen like the following screen:\n\nThe preceding screenshot shows step 1 of 2 for the index creating a pattern.\n• On this screen, we need to provide the keyword for the index name in the search box. Below the search box, it shows different Elasticsearch index names.\n• Now, if you want to add the server-metrics index of Elasticsearch, you need to add this name in the search box, which will give the success message, as shown in the following screenshot:\n• Click on the Next Step button to move to the next step. The given screenshot shows the next screen:\n\nThe preceding screen in step 2 of 2, where we need to configure settings.\n• Now pick the time filter field name and click on Create index pattern. This will open the following screen:\n\nThe preceding screenshot shows the field names and data types with additional attributes.\n• Now we can check the index pattern data using Kibana Discover. So click on Discover on the left menu and choose the server-metrics index pattern. This will show the index data.\n\nSo, this way, we can create a new index pattern, and we can see the Elasticsearch index data in Kibana.\n\nKibana, by default, on every option shows an index pattern, so we don’t care about changing the index pattern on the visualize timeline, discover, or dashboard page. There, an asterisk sign is shown on every index pattern just before the name of the index.\n\nTo set another index pattern as default, we tend to need to click on the index pattern name then click on the top-right aspect of the page on the star image link. this may modification the opt for index pattern to default:\n\nAll fields of the Elasticsearch index are mapped in Kibana when we add the index pattern, as the Kibana index pattern scans all fields of the Elasticsearch index. However, whenever any new field is added to the Elasticsearch index, it will not be shown automatically, and for these cases, we need to refresh the Kibana index fields.\n\nTo refresh the particular index pattern field, we need to click on the index pattern name and then on the refresh link in the top-right of the index pattern page:\n\nThe preceding screenshot shows that when we click on the refresh link, it shows a pop-up box with a message. This action resets the popularity counter of each field. It also shows two buttons: Cancel and Refresh. Clicking on the Refresh button refreshes the fields.\n\nIf we want to delete an index pattern from Kibana, we can do that by clicking on the delete icon in the top-right corner of the index pattern page. It asks for confirmation before deleting and deletes the pattern after confirmation. The following screenshot shows the delete operation:\n\nThis delete will only delete the index from Kibana, and there will be no impact on the Elasticsearch index.\n\nUnder the index pattern, we can get the tabular view of all the index fields. We can sort the values by clicking on the table header. We have the filter option, through which we can filter the field name by typing it. After filter the textbox, we have a dropdown to filter the fields according to field type; it has the following options:\n\nUnder the controls column, against each row, we have the pencil symbol, using which we can edit the field’s properties. The given screenshot shows us the field listing of the index pattern:\n\nAfter clicking on the edit control for any field, we can manually set the format for that field using the format selection dropdown. Thus, for every type of data, we have a different set of formats that we can change after editing the field.\n\nOn the edit screen, we can set the field popularity using the popularity textbox. After making all these changes, we can save it by clicking on the Update field button. We can cancel those changes by clicking on the Cancel button. Under Kibana’s Management option, we have a field formatter for the following types of fields:\n\nAt the bottom of the page, we have a link scroll to the top, which scrolls the page up.\n\nString fields have support for two formatters: String and URL. For example, in the String field formatter, we can apply the following transformations to the content of the field:\n\nThis screenshot shows the string type format and the transform options:\n\nIn the URL field formatter, we can apply the following transformations to the content of the field:\n\nThe date field has support for the date, string, and URL formatters. For the string and the URL type formatter, we have already discussed it in the previous string type. The following screen shows the date type field with an option to change the\n\nformat and popularity of the field:\n\nThe date formatter enables us to use the display format of the date stamps, using the moment.js standard definition for date-time.\n\nNumber fields are used in different areas and support the Percentage, Bytes, Duration, Duration, Number, URL, String, and formatters of Color.\n\nThe below screenshot shows the type filed, with the option of setting the format and the very popular number field.\n\nWe can use the duration field formatter to displays the numeric value of a field in the following ways:\n\nThe color field option giving us the power to choose colors with specific ranges of numeric values. We can choose the Color formatted, which shows the Font, Color, Range, Background Color, and also shows some Example fields, after which we can choose the color.\n\nNumber, Bytes, and Percentage formatters enables us to pick the display formats of numbers using the numeral.js standard format definitions.\n\nWe covered the index pattern where first we created the index pattern by taking the server-metrics index of Elasticsearch. After creating an index pattern, we covered the set as the default index pattern feature of Management, through which we can set any index pattern as a default. This is quite helpful. As for discovering, visualize, and dashboard, we need not worry about the index pattern selection in case we want to work on any particular index.\n\nThis is a guide to Kibana Index Pattern. Here we discuss the index pattern in which we created the index pattern by taking the server-metrics index of Elasticsearch. You may also have a look at the following articles to learn more –"
    },
    {
        "link": "https://elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html",
        "document": "You can index a new JSON document with the or resource. Using guarantees that the document is only indexed if it does not already exist. To update an existing document, you must use the resource.\n\nIf request’s target doesn’t exist and matches an index template with a definition, the index operation automatically creates the data stream. See Set up a data stream.\n\nIf the target doesn’t exist and doesn’t match a data stream template, the operation automatically creates the index and applies any matching index templates.\n\nIf no mapping exists, the index operation creates a dynamic mapping. By default, new fields and objects are automatically added to the mapping if needed. For more information about field mapping, see mapping and the update mapping API.\n\nAutomatic index creation is controlled by the setting. This setting defaults to , which allows any index to be created automatically. You can modify this setting to explicitly allow or block automatic creation of indices that match specified patterns, or set it to to disable automatic index creation entirely. Specify a comma-separated list of patterns you want to allow, or prefix each pattern with or to indicate whether it should be allowed or blocked. When a list is specified, the default behaviour is to disallow.\n\nYou can force a create operation by using the resource or setting the parameter to create. In this case, the index operation fails if a document with the specified ID already exists in the index.\n\nWhen using the request format, the is automatically set to and the index operation generates a unique ID for the document.\n\nThe API returns the following result:\n\nIndex operations can be made conditional and only be performed if the last modification to the document was assigned the sequence number and primary term specified by the and parameters. If a mismatch is detected, the operation will result in a and a status code of 409. See Optimistic concurrency control for more details.\n\nBy default, shard placement — or — is controlled by using a hash of the document’s id value. For more explicit control, the value fed into the hash function used by the router can be directly specified on a per-operation basis using the parameter. For example:\n\nIn this example, the document is routed to a shard based on the parameter provided: \"kimchy\".\n\nWhen setting up explicit mapping, you can also use the field to direct the index operation to extract the routing value from the document itself. This does come at the (very minimal) cost of an additional document parsing pass. If the mapping is defined and set to be , the index operation will fail if no routing value is provided or extracted.\n\nThe index operation is directed to the primary shard based on its route (see the Routing section above) and performed on the actual node containing this shard. After the primary shard completes the operation, if needed, the update is distributed to applicable replicas.\n\nTo improve the resiliency of writes to the system, indexing operations can be configured to wait for a certain number of active shard copies before proceeding with the operation. If the requisite number of active shard copies are not available, then the write operation must wait and retry, until either the requisite shard copies have started or a timeout occurs. By default, write operations only wait for the primary shards to be active before proceeding (i.e. ). This default can be overridden in the index settings dynamically by setting . To alter this behavior per operation, the request parameter can be used.\n\nValid values are or any positive integer up to the total number of configured copies per shard in the index (which is ). Specifying a negative value or a number greater than the number of shard copies will throw an error.\n\nFor example, suppose we have a cluster of three nodes, , , and and we create an index with the number of replicas set to 3 (resulting in 4 shard copies, one more copy than there are nodes). If we attempt an indexing operation, by default the operation will only ensure the primary copy of each shard is available before proceeding. This means that even if and went down, and hosted the primary shard copies, the indexing operation would still proceed with only one copy of the data. If is set on the request to (and all 3 nodes are up), then the indexing operation will require 3 active shard copies before proceeding, a requirement which should be met because there are 3 active nodes in the cluster, each one holding a copy of the shard. However, if we set to (or to , which is the same), the indexing operation will not proceed as we do not have all 4 copies of each shard active in the index. The operation will timeout unless a new node is brought up in the cluster to host the fourth copy of the shard.\n\nIt is important to note that this setting greatly reduces the chances of the write operation not writing to the requisite number of shard copies, but it does not completely eliminate the possibility, because this check occurs before the write operation commences. Once the write operation is underway, it is still possible for replication to fail on any number of shard copies but still succeed on the primary. The section of the write operation’s response reveals the number of shard copies on which replication succeeded/failed.\n\nControl when the changes made by this request are visible to search. See refresh.\n\nWhen updating a document using the index API a new version of the document is always created even if the document hasn’t changed. If this isn’t acceptable use the API with set to true. This option isn’t available on the index API because the index API doesn’t fetch the old source and isn’t able to compare it against the new source.\n\nThere isn’t a hard and fast rule about when noop updates aren’t acceptable. It’s a combination of lots of factors like how frequently your data source sends updates that are actually noops and how many queries per second Elasticsearch runs on the shard receiving the updates.\n\nThe primary shard assigned to perform the index operation might not be available when the index operation is executed. Some reasons for this might be that the primary shard is currently recovering from a gateway or undergoing relocation. By default, the index operation will wait on the primary shard to become available for up to 1 minute before failing and responding with an error. The parameter can be used to explicitly specify how long it waits. Here is an example of setting it to 5 minutes:\n\nEach indexed document is given a version number. By default, internal versioning is used that starts at 1 and increments with each update, deletes included. Optionally, the version number can be set to an external value (for example, if maintained in a database). To enable this functionality, should be set to . The value provided must be a numeric, long value greater than or equal to 0, and less than around 9.2e+18.\n\nWhen using the external version type, the system checks to see if the version number passed to the index request is greater than the version of the currently stored document. If true, the document will be indexed and the new version number used. If the value provided is less than or equal to the stored document’s version number, a version conflict will occur and the index operation will fail. For example:\n\nIn the previous example, the operation will succeed since the supplied version of 2 is higher than the current document version of 1. If the document was already updated and its version was set to 2 or higher, the indexing command will fail and result in a conflict (409 http status code).\n\nA nice side effect is that there is no need to maintain strict ordering of async indexing operations executed as a result of changes to a source database, as long as version numbers from the source database are used. Even the simple case of updating the Elasticsearch index using data from a database is simplified if external versioning is used, as only the latest version will be used if the index operations arrive out of order for whatever reason.\n\nIn addition to the version type, Elasticsearch also supports other types for specific use cases:"
    },
    {
        "link": "https://elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html",
        "document": "For more information regarding all the different index level settings that can be set when creating an index, please check the index modules section.\n\nYou do not have to explicitly specify index section inside the settings section.\n\nEach index created can have specific settings associated with it, defined in the body:\n\nBy default, index creation will only return a response to the client when the primary copies of each shard have been started, or the request times out. The index creation response will indicate what happened:\n\nindicates whether the index was successfully created in the cluster, while indicates whether the requisite number of shard copies were started for each shard in the index before timing out. Note that it is still possible for either or to be , but the index creation was successful. These values simply indicate whether the operation completed before the timeout. If is , then we timed out before the cluster state was updated with the newly created index, but it probably will be created sometime soon. If is , then we timed out before the requisite number of shards were started (by default just the primaries), even if the cluster state was successfully updated to reflect the newly created index (i.e. ).\n\nWe can change the default of only waiting for the primary shards to start through the index setting (note that changing this setting will also affect the value on all subsequent write operations):\n\nor through the request parameter :\n\nA detailed explanation of and its possible values can be found here."
    },
    {
        "link": "https://elastic.co/guide/en/elasticsearch/reference/current/rest-apis.html",
        "document": "Elasticsearch exposes REST APIs that are used by the UI components and can be called directly to configure and access Elasticsearch features.\n\nFor the most up-to-date API details, refer to For the most up-to-date API details, refer to Elasticsearch APIs"
    },
    {
        "link": "https://logz.io/blog/elasticsearch-api",
        "document": ""
    },
    {
        "link": "https://elastic.co/guide/en/elasticsearch/reference/current/getting-started.html",
        "document": "This quick start guide is a hands-on introduction to the fundamental concepts of Elasticsearch: indices, documents and field type mappings.\n\nYou’ll learn how to create an index, add data as documents, work with dynamic and explicit mappings, and perform your first basic searches.\n\nYou’ll need a running Elasticsearch cluster, together with Kibana to use the Dev Tools API Console. Run the following command in your terminal to set up a single-node local cluster in Docker:\n\nThe following response indicates the index was created successfully.\n\nYou add data to Elasticsearch as JSON objects called documents. Elasticsearch stores these documents in searchable indices.\n\nSubmit the following indexing request to add a single document to the index.\n\nThe response includes metadata that Elasticsearch generates for the document, including a unique for the document within the index.\n\nUse the endpoint to add multiple documents in one request. Bulk data must be formatted as newline-delimited JSON (NDJSON).\n\nYou should receive a response indicating there were no errors.\n\nMappings define how data is stored and indexed in Elasticsearch, like a schema in a relational database.\n\nWhen using dynamic mapping, Elasticsearch automatically creates mappings for new fields by default. The documents we’ve added so far have used dynamic mapping, because we didn’t specify a mapping when creating the index.\n\nTo see how dynamic mapping works, add a new document to the index with a field that doesn’t appear in the existing documents.\n\nView the mapping for the index with the Get mapping API. The new field has been added to the mapping with a data type.\n\nCreate an index named with explicit mappings. Pass each field’s properties as a JSON object. This object should contain the field data type and any additional mapping parameters.\n\nExplicit mappings are defined at index creation, and documents must conform to these mappings. You can also use the Update mapping API. When an index has the flag set to , you can add new fields to documents without updating the mapping.\n\nThis allows you to combine explicit and dynamic mappings. Learn more about managing and updating mappings.\n\nIndexed documents are available for search in near real-time, using the API.\n\nRun the following command to search the index for all documents:\n\nYou can use the query to search for documents that contain a specific value in a specific field. This is the standard query for full-text searches.\n\nRun the following command to search the index for documents containing in the field:\n\nWhen following along with examples, you might want to delete an index to start from scratch. You can delete indices using the Delete index API.\n\nFor example, run the following command to delete the indices created in this tutorial:"
    }
]