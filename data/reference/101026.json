[
    {
        "link": "https://wiki.facepunch.com/gmod/Beginner_Tutorial_Intro",
        "document": "A Lua file is called a script. Lua scripts are plain text files. To create and edit Lua scripts, you need a plain text editor such as notepad.\n\nYou can use any plain text editor, but you are going to have a hard time if you aren't using one of the many available Lua Editors, such as Sublime Text or Visual Studio Code.\n\nDuring these tutorials, we'll refer to your preferred text editor as \"notepad\".\n\nFor our first script, we're not going to do anything too complex. We'll learn how to print a message into the console.\n\nThe print function does exactly that. Type the following code into your chosen editor :\n\nYou're done! Wasn't that easy? It should have been.\n\nis a function. A function is a command that does something when you call it. Many functions can take arguments, which is data you give the function to change exactly what it does. In this case, takes only one argument, which is a string (a series of letters, numbers, spaces, and so on), and when is called, it puts that string into the console in Garry's Mod.\n\nYou're now ready to save the actual Lua file. To find your Lua folder, follow the following path - depending on where you have installed Garry's Mod on your computer, this path may be different, but will look something like this:\n\nC:\\Program Files\\Steam\\steamapps\\common\\GarrysMod\\garrysmod\\lua\\ This may be different!\n\nIn the filename box, type (Note that you must specify ), and in the Save As type box, select All Files. Now, just press enter (or press the save button) to save your script.\n\nTo run any of your scripts, you need to be playing a map. Once in game, open the developer console and type the following:\n\nPress enter. If you did everything correctly up to this point, you should see a message in the console:\n\nCongratulations, you have written and run your first Lua script.\n\nTo make your script run automatically when the server starts, you simply put it into one of these folders. Again, if you have Garry's Mod installed somewhere else, navigate there instead.\n• For shared scripts ( Loaded on either client or server )\n• For client-only scripts ( Loaded only on client )\n• For server-only scripts ( Loaded only on server )\n\nFor this specific tutorial, any of those paths will work just fine.\n\nLoading portions of a script on the client or server\n\nIf you want to use a single file for clientside and serverside code, but want separate parts of the code between client and server, we can do something like this:\n• To run code only on the client\n• To run code only on the server\n• To load onto shared (either Client or Server), don't specifically check for or , just put your code down without the block.\n\nThe and variables you see up there are explained on the States page.\n\nNext we will learn about variables.\n\nClientside, Serverside, and Shared are explained here: States"
    },
    {
        "link": "https://maurits.tv/data/garrysmod/wiki/wiki.garrysmod.com/indexde03.html",
        "document": "Scripted npcs are NPCs that the user has total control of, in this example I've taken garry's fighter npc and made it fight the player instead of just shooting props, and it dies properly.\n\nWhat program do I use for these, and where do I put them?\n\nFirst of all, all these files (as of the beta) go into: garrysmod/lua/entities or the respective equal gamemode folder. Second of all, you can use notepad for editing/writing lua, though it's suggested that you google notepad++ and get that.\n\nI'll try to fit information on how it works into the comments.\n\ninclude ENT.RenderGroup RENDERGROUP_BOTH ENT:Draw self:DrawModel ENT:DrawTranslucent // This is here just to make it backwards compatible. // You shouldn't really be drawing your model here unless it's translucent self:Draw ENT:BuildBonePositions NumBones, NumPhysBones // You can use this section to position the bones of // This will override any animation data and isn't meant as a // replacement for animations. We're using this to position the limbs ENT:SetRagdollBones bIn // If this is set to true then the engine will call // DoRagdollBone (below) for each ragdoll bone. // It will then automatically fill in the rest of the bones self.m_bRagdollSetup bIn ENT:DoRagdollBone PhysBoneNum, BoneNum\n\nAddCSLuaFile AddCSLuaFile include schdChase ai_schedule.New //creates the schedule used for this npc // Run away randomly (first objective in task) schdChase:EngTask , schdChase:EngTask , schdChase:EngTask , schdChase:AddTask , Name , Speed // Find an enemy and run to it (second objectives in task) schdChase:AddTask , Class , Radius schdChase:EngTask , schdChase:EngTask , schdChase:EngTask , // Shoot it (third objective in task) schdChase:EngTask , schdChase:EngTask , schdChase:EngTask , schdChase:EngTask , schdChase:EngTask , //schedule is looped till you give it a different schedule ENT:Initialize self:SetModel self:SetHullType HULL_HUMAN self:SetHullSizeNormal self:SetSolid SOLID_BBOX self:SetMoveType MOVETYPE_STEP self:CapabilitiesAdd CAP_MOVE_GROUND CAP_OPEN_DOORS CAP_ANIMATEDFACE CAP_TURN_HEAD CAP_USE_SHOT_REGULATOR CAP_AIM_GUN self:SetMaxYawSpeed self:SetHealth self:Give ENT:OnTakeDamage dmg self:SetHealth self:Health - dmg:GetDamage self:Health self:SetSchedule SCHED_FALL_TO_GROUND //because it's given a new schedule, the old one will end. ENT:SelectSchedule self:StartSchedule schdChase\n\nENT.Base ENT.Type ENT.PrintName ENT.Author ENT.Contact //fill in these if you want it to be in the spawn menu ENT.Purpose ENT.Instructions ENT.AutomaticFrameAdvance /*--------------------------------------------------------- Name: OnRemove Desc: Called just before entity is deleted ---------------------------------------------------------*/ ENT:OnRemove /*--------------------------------------------------------- Name: PhysicsCollide Desc: Called when physics collides. The table contains data on the collision ---------------------------------------------------------*/ ENT:PhysicsCollide data, physobj /*--------------------------------------------------------- Name: PhysicsUpdate Desc: Called to update the physics .. or something. ---------------------------------------------------------*/ ENT:PhysicsUpdate physobj /*--------------------------------------------------------- Name: SetAutomaticFrameAdvance Desc: If you're not using animation you should turn this off - it will save lots of bandwidth. ---------------------------------------------------------*/ ENT:SetAutomaticFrameAdvance bUsingAnim self.AutomaticFrameAdvance bUsingAnim\n\nUnless you got it in the spawn menu, you can use this command to create it (without the quotes):\n\n\"ent_create folder_name\" please note:Do not use capitals in your folder name, and do not use spaces.\n\nWhere can I find a list of schedules for my SNPC?\n\nThe full list of schedules can be found here: http://developer.valvesoftware.com/wiki/Shared_schedules\n\nBut as garrysmod has it's own enumeration file for schedules, you can probably only use these\n\nWhere can I find a list of tasks for my SNPC?\n\nA full list can be found here: http://developer.valvesoftware.com/wiki/Shared_tasks"
    },
    {
        "link": "https://steamcommunity.com/sharedfiles/filedetails?id=940122060",
        "document": ""
    },
    {
        "link": "https://unknowncheats.me/forum/garry-s-mod/231946-basic-lua-script.html",
        "document": "INTRO/TOOLS\n\n You will need a text editor of some sort\n\n Garrys Mod if you don't own it just pirate it or something ( )\n\n A Basic Understanding of Lua *it's easy don't worry You will need a text editor of some sortGarrys Mod if you don't own it just pirate it or something (A Basic Understanding of Lua *it's easy don't worry Okay So now That you have all that stuff you will need to first locate your Gmod Folder, I have mine for steam and on my main drive so its Located at\n\n Now there are a few ways to do this like making a file in Notepad then saving it, or i like to make a .txt file and replace the .txt to .lua\n\n http://prntscr.com/gisefb\n\n Now that thats over we can now begin making the script!\n\n So lets start by putting in some basic code that will make our script!\n\n --Sound Playing surface.PlaySound (\"npc/scanner/scanner_scan1.wav\") --This Will play a sound upon Script loading! --Command Adding function Console_Print_test () --Anything inside of here is The function for SoundPlay print (\" Test Worked!\" ) end concommand.Add( \"Console_Print_test\", Console_Print_test ) --This just adds a command to the console. --Console Printing local UCuser = \"UnkownCheats User!\" --This Defines the variable UCuser so we can print it and use it later on print ( \" Hello,\", UCuser ) --This will print things to the console Anything in \"will be Printed normally\" however if something is like UCuser it will look for a variable called UCuser --Chat Printing! chat.AddText( Color( 255, 0, 0 ) ,\"Test: \" , Color( 0, 255, 0 ),\"Lua Test Worked OMG\") --chat.AddText( Color( 0, 0, 0 ) ,\"1st part \" , Color( 0, 0, 0 ),\"2nd Part\") To Run this open gmod, and type print ( \"\" ) Prints stuff to console\n\n surface.PlaySound (\"\") Plays Sounds\n\n concommand.Add ( \"command here\", Function here ) Makes a command with a function\n\n chat.AddText ( Color( 0(R), 0(G), 0(B) ) , \"Text here\") Prints stuff to chat (others cant see it unless its a server side lua script).\n\n local Var name here = \"\" A basic Variable A quick Word Of advice 1st there is no vac for gmod, 2nd You can upload cheats on the workshop 3rd Lua is easy and you can get help from UC or the gmod wiki. Okay So now That you have all that stuff you will need to first locate your Gmod Folder, I have mine for steam and on my main drive so its Located atNow there are a few ways to do this like making a file in Notepad then saving it, or i like to make a .txt file and replace the .txt to .luaNow that thats over we can now begin making the script!So lets start by putting in some basic code that will make our script!To Run this open gmod, and typeA quick Word Of advice 1st there is no vac for gmod, 2nd You can upload cheats on the workshop 3rd Lua is easy and you can get help from UC or the gmod wiki. Last edited by Slash Element; 8th September 2017 at . Reason: Incorrect Formatting"
    },
    {
        "link": "https://wiki.facepunch.com/gmod/NPC",
        "document": ""
    },
    {
        "link": "https://medium.com/@www.seymour/training-an-ai-to-play-a-game-using-deep-reinforcement-learning-b63534cfdecd",
        "document": "This article builds on tutorials on Reinforcement Learning (DQN, or Deep Q Network), such as this one. I recommend checking that out for the basics of DQNs. That tutorial uses a pre-made “gymnasium” environment in which a DQN agent can learn to act to maximise reward / minimise punishment. However, I wanted to try and build my own environment and DQN from scratch. This article details my efforts.\n\nThe video above compares the agent’s performance at episode (i.e. epoch) -1, 50, and 130. Before training, the agent responds to the environment randomly. After some training, the agent catches some fruit but still achieves a negative score. After much training, the agent is thinking ahead to catch fruit, seems to understand when pursuing fruit will prove fruitless, and exhibits signs of making sensible decisions between complex paths. More compute, and a more complex model, is likely required to perfect play.\n\nThese results might seem unimpressive, but, as we’ll see, getting a neural network to “see” the game and make good choices is more difficult than it seems.\n\nNB: I provide complete code chunks below for ease of replication. The downside of this is that they are quite long. Feel free to scroll past without guilt.\n\nYou can also find the complete code here.\n\nI coded a basic fruit catching game in Python. The game is human-playable with the arrow keys when launching game.py, but is also playable by an AI agent when imported as a module.\n\nFruit randomly spawn at the top of the game field and fall. If the player catches a fruit, the score is incremented by one. Missed fruit cost the player one point each.\n\nFor each tick of the game, the AI passes one of three actions to the game: {stay still, move left, move right}. In return it gets some information: the game state, the reward/punishment, whether the game has ended, and the score. With this information, the AI chooses its next move and learns from past experience.\n\nThis game is deceptively difficult for an AI. First, the AI has to “see” what is being represented in two dimensions, then it has to predict the rewards associated with different actions. Rewards are usually the delayed consequence of previous actions. Sometimes there is much fruit visible, other times very little. Fruit also fall frequently enough that catching them all is impossible, and so tough decisions need to be made. Perfect strategy will necessitate ignoring some fruit in order to catch more fruit in the future.\n\nThe agent is initiated with the following parameters:\n• The number of actions possible — i.e. 3\n• Gamma — i.e. how much to discount future rewards (1=no discount)\n• Memory size — i.e. how many examples to keep in memory before discarding the oldest\n• Exploration max, min, and decay — exploration rate is the proportion of actions made by random chance\n\nThe DQN has functions for remembering past states with their rewards, and for replaying and training from its memory.\n\nThe DQN also has two versions of its own model. The base model is updated every time the model trains, but the the target model is only updated after set intervals and is used for long term reward estimation. This is to ensure that long term reward predictions are less affected by short-term changes in the model, which could otherwise introduce instability.\n\nBasic DQNs replay scenarios from a simple buffer. Sparse environments, however, can make it very challenging for agents to learn. My environment is pretty sparse — rewards are not available every tick. I found it necessary to implement a Prioritised Replay Buffer that prioritises learning from memories with high error associated with them. This means that the DQN will spend more time learning from events it doesn’t understand well. With alpha set to 0.8, this Buffer quite aggressively favours memories the DQN currently predicts badly.\n\nTraining is relatively straightforward. The agent plays a number of games until they are lost, committing each game state to memory, and retraining from a sample of memories every tick.\n\nEach episode, the learning rate falls, as does the exploration rate. Lowering the learning rate helps the model converge, and starting off with a high exploration rate helps the agent discover strategies and also prevents the model from locking itself into a bad strategy. Models are also saved for future reference.\n\nIf you want to see how your agent performs at any particular point, you can load it into a fresh environment like this:\n\nGetting things working well is often a frustrating case of trial and error. With enough compute, one could grid search for ideal parameters, but I proceeded mainly by feel.\n\nModels in well-designed spaces converged on decent parameters after around 100 episodes. In each episode, the agent might train around 50–100 times on batches of 64 or more examples — that’s the better part of a million examples processed. While still far less than the theoretical number of possible states, that is still a lot of training to master very simple environments.\n\nThis is a computer vision + game-playing task, so requires a fairly complex neural network. With some trial and error I was able to find a model spec that trained quickly enough on an old laptop CPU, but better performance undoubtedly requires more complex neural networks.\n\nDQNs find it difficult to learn in sparse reward environments. In most ticks of my game, no rewards are available. In addition, actions are often rewarded only in several moves’ time. Some of this is addressed below, but I also made things as easy as possible for the agent in early prototypes, while getting to grips with the code and parameters.\n\nIn the first proof-of-concept, the agent was told how far away it was from the fruit in the x-axis — so it had very little work to do to predict the best direction of movement. In the next iteration, the agent received the x and y coordinates of the lowest fruit, and its own x coordinate. Only when I was satisfied that these were working did I move onto passing the agent the full state of the board.\n\nI also tweaked the environment to assist with learning, for example by ensuring there is always at least one fruit available. I found that higher batch sizes led to more stable learning, as they were more likely to contain a representative sample of rewards/punishments.\n\nAn auxiliary task can be given to the agent’s model, so that it does not just predict the reward for each action — in this case, it also predicts the next state.\n\nI experimented with auxiliary outputs and found that this helped the agent learn in some cases, but proved a distraction in more complex cases.\n\nA feature of DQNs is that they can predict future rewards. However, this is only possible if the network is predicting immediate rewards properly. The agent uses its own predictions to account for future rewards, which obviously fails when rewards are not being predicted accurately.\n\nOne solution to this is to use “curriculum learning” — i.e. initially training the model with a simpler version of the problem. In simpler versions of the game, I found that this was key to success. An agent initially learns only from instances in which the fruit is on the row above it. This vastly simplifies the problem, and greatly increases the frequency of rewards/punishments in memory. Once the agent learns how to catch fruit directly above it, it can start to figure out how to move towards fruit that is further away. At first, the agent’s memory contains only occasions where fruit is very low. Gradually other examples are introduced and slowly the agent’s memory becomes representative of the whole game.\n\nThe Prioritised Replay Buffer is another solution to this problem, with the advantage that it requires no manipulation of the agent’s experience of the environment. If found this to be the much more performant solution in the final case.\n\nAgents exhibit sensitivity to reward structure in ways that can be unexpected, and that can hinder learning. Earlier versions of my game would prevent the player from off the screen to the left or right. The inadvertent side-effect of this is that holding down one of the keys appears to generate rewards, because you are actually staying still while fruit falls on you. Agents would therefore often get stuck on the sides, and get into a learning hole. The solution to this was to let the player wrap around the x axis.\n\nInterestingly, the agent learns how to take full advantage of the wrap-around mechanic and successfully chases fruit off each side. The downside is that the agent will have to choose more frequently between moving left and right if fruit can be caught off the sides of the field.\n\nI found it useful to gradually adjust the learning rate downwards using a decay function. Some trial and error is required to get a feel for when the model starts to get stuck.\n\nIt is common to start training with a very high exploration rate (e.g. 1) so that the agent is acting at random. The rate is gradually lowered as the agent trains. Remember to set this to 0 when observing agent behaviour, otherwise the agent will act randomly a proportion of the time."
    },
    {
        "link": "https://vocal.media/gamers/how-to-implement-ai-in-game-development-from-simple-to-complex-behaviors",
        "document": ""
    },
    {
        "link": "https://quora.com/What-are-some-tips-on-programming-an-AI-for-a-game",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://news.lift.co/index.jsp/uploaded-files/4020086/LearningGameAiProgrammingWithLua.pdf",
        "document": ""
    },
    {
        "link": "https://gamedev.stackexchange.com/questions/49240/how-should-i-manage-the-ai-using-lua-scripts",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    }
]