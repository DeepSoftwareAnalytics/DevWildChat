[
    {
        "link": "https://alglib.net/eigen/symmetric/symmbisectionandinverseiteration.php",
        "document": "Real number λ and vector z are called an eigen pair of matrix A, if Az = λz. If matrix A of size NxN is symmetric, it has N eigenvalues (not necessarily distinctive) and N corresponding eigenvectors which form an orthonormal basis (generally, eigenvectors are not orthogonal, and their number could be lower than N).\n\nFor a real matrix A, there might be both the problem of finding all the eigenvalues and eigenvectors (the so-called matrix spectrum) and the problem of finding part of a spectrum. If not all the eigenvalues are required, we can use the bisection method to find the eigenvalues from a given interval (or having given numbers). After that, we can find the eigenvectors by using the inverse iteration method. If we only have to find a small part of the spectrum, we can increase the performance considerably in comparison with QL/QR algorithm.\n\nAs when using QL/QR algorithm, the symmetric matrix is reduced to tridiagonal form and then the bisection and inverse iteration methods which are working with the tridiagonal matrix are called.\n\nLet's consider the reliability of such a calculation procedure. This methods are rather reliable when dealing with matrices with well-separated eigenvalues. If some eigenvalues are close to each other, the inverse iteration method can return an inaccurate set of eigenvectors (the closer the eigenvalues are, the less accurate the eigenvectors will be).\n\nIt should be understood that such difficulties are typical of all methods of finding eigenvalues and no satisfactory solution has yet been found. Nevertheless, this method is applicable to the majority of real problems. If the algorithm is compared with the calculation problem which it couldn't solve, the subroutine returns False. In this case, it is reasonable to use QL/QR algorithm which is more stable than the inverse iteration method, and then select a required vector from the full set of eigenvectors found (it is rather a slow way but you will get a solution).\n\nTo find the eigenvalues (and their corresponding eigenvectors) from the given half-interval (A, B], use the subroutine. The subroutine finds the eigen pairs having given numbers (the spectrum is considered as being sorted in ascending order).\n\nIt should be noted that the algorithm is effective only when finding a small part of the spectrum. If it is required to find all eigenvalues (or the majority of them), the QL/QR algorithm is more effective.\n\nThis algorithm is transferred from the LAPACK library.\n\nThis article is licensed for personal use only."
    },
    {
        "link": "https://alglib.net/eigen/symmetric/tdbisectionandinverseiteration.php",
        "document": "There are several ways of finding eigenvalues except for reducing the matrix to tridiagonal form by orthogonal transformations. For example, the number of eigenvalues less than a given number could be easily determined for a symmetric tridiagonal matrix. Let's introduce the function N(w) which is equal to the number of elements of matrix T which are not greater than w. This function is piecewise constant, non-strictly monotone increasing, having discontinuities in points w which are eigenvalues. Thus, the problem of finding eigenvalues is reduced to the problem of finding discontinuities of some piecewise constant function. This can be easily performed by using the bisection of the intervals which contain these discontinuities.\n\nThe bisection method allows to find both eigenvalues in a given interval and with given numbers (in ascending order). If it is required to find all eigenvalues, this method is significantly slower than the others, but it can be used to find a small part of eigenvalues.\n\nThe bisection method is often used along with the inverse iteration method which allows to find an eigenvector by its corresponding eigenvalue. If it is required to find a small part of a full set of eigenvectors, the inverse iteration is faster than finding all eigenvectors using QL/QR algorithm.\n\nThe bisection and inverse iteration methods could be both used on their own and applied to a tridiagonal matrix obtained from a symmetric matrix (this is done by the algorithm which gets a part of eigenvalues and eigenvectors of a symmetric matrix.\n\nLet's consider the reliability of such a calculation procedure. This methods are rather reliable when dealing with matrices with well-separated eigenvalues. If some eigenvalues are close to each other, the inverse iteration method can return an inaccurate set of eigenvectors (the closer the eigenvalues are, the less accurate the eigenvectors will be).\n\nIt should be understood that such difficulties are typical of all methods of finding eigenvalues and no satisfactory solution has yet been found. Nevertheless, this method is applicable to the majority of real problems. If the algorithm is applied to the problem which it couldn't solve, the subroutine returns False. In this case, it is reasonable to use QL/QR algorithm which is more stable than the inverse iteration method, and then select a required vector from the full set of eigenvectors found (it is rather a slow way but you will get a solution).\n\nThere are two subroutines for finding eigenvalues and eigenvectors. The first, , allows to find the eigenvalues from a given interval. The second subroutine, , allows to find the eigenvalues (and the corresponding eigenvectors) having given indexes.\n\nThis algorithm is transferred from the LAPACK library.\n\nThis article is licensed for personal use only."
    },
    {
        "link": "https://arxiv.org/abs/1212.0417",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/7592508/solving-linear-equations-during-inverse-iteration",
        "document": "I am using OpenCL to calculate the eigenvectors of a matrix. AMD has an example of eigenvalue calculation so I decided to use inverse iteration to get the eigenvectors.\n\nI was following the algorithm described here and I noticed that in order to solve step 4 I need to solve a system of linear equations (or calculate the inverse of a matrix).\n\nWhat is the best way to do this on a GPU using OpenCL? Are there any examples/references that I should look into?\n\nEDIT: I'm sorry, I should have mentioned that my matrix is symmetric tridiagonal. From what I have been reading this could be important and maybe simplifies the whole process a lot"
    },
    {
        "link": "https://github.com/ammahmoudi/Iterative-Methods-for-Eigenvalue-Problem",
        "document": "The eigenvalue problem is one of linear algebra's most essential and valuable problems. It deals with finding a square matrix's special values and vectors that satisfy a certain equation. The equation is of the form , where is a given matrix, is an unknown vector, and is an unknown scalar. The vector is called an eigenvector of , and the scalar is called an eigenvalue of . The word \"eigen\" comes from the German word for \"own\" or \"characteristic\", meaning that the eigenvectors and eigenvalues capture some essential properties of the matrix.\n\nThe eigenvalue problem arises in many applications, such as modeling vibrations, stability, and data analysis. For example, suppose we have a system of masses and springs, and we want to find the natural frequencies and modes of vibration of the system. We can represent the system by a matrix , where each entry indicates the stiffness of the spring connecting the -th and the -th mass. Then, the equation describes how the displacements of the masses change over time, and the eigenvalues are the squares of the natural frequencies, and the eigenvectors are the modes of vibration. By finding the eigenvalues and eigenvectors of , we can understand how the system behaves and how to control it.\n\nTo solve the eigenvalue problem, we must find all the possible values of and the corresponding vectors of that make the equation true. There are various methods to do this, depending on the type and the size of the matrix . One of the most common methods is finding the roots of the characteristic polynomial of , defined as . The determinant of is a polynomial function of , and its roots are precisely the eigenvalues of . Once we find the eigenvalues, we can find them by plugging them into the equation and solving for . However, this method can be difficult or impossible to carry out by hand, especially for large or complicated matrices. Therefore, we often use numerical algorithms, such as the power method, the QR algorithm, or the Jacobi method, to approximate the eigenvalues and eigenvectors of .\n\nThe eigenvalue problem reveals important properties and insights about the matrix and the system it represents. For example, the number of eigenvalues of is equal to the dimension of , and the sum of the eigenvalues of is equal to the trace of , which is the sum of the diagonal entries of . The product of the eigenvalues of equals the determinant of , which measures the volume change of . The eigenvalues and eigenvectors of also determine the rank, the nullity, the inverse, and the powers of . Moreover, some special matrices have special eigenvalues and eigenvectors. For example, a symmetric matrix has only real eigenvalues and orthogonal eigenvectors, meaning that the eigenvectors are perpendicular. A positive definite matrix has only positive eigenvalues and positive definite eigenvectors, meaning that the eigenvectors point in the same direction as . These properties make the eigenvalue problem a powerful tool for analyzing and solving linear systems.\n\nA common way to find eigenvalues in basic linear algebra courses is to solve a characteristic polynomial. However, this method has a drawback: the solutions of the characteristic polynomial can change drastically if the polynomial coefficients are slightly altered, even for eigenvalues that are not ill-conditioned. Instead of using this approach, we will use different techniques. Iterative methods help solve eigenvalue problems for large matrices, which are matrices that have a high number of rows and columns. Large matrices are often sparse, meaning that most entries are zero and have unique structures, such as symmetry, positive definiteness, or diagonal dominance. These properties can be exploited by iterative methods to reduce the computational cost and storage requirements of finding the eigenvalues and eigenvectors of large matrices.\n\nSome of the most common iterative methods for eigenvalue problems are:\n• Power method: This is the simplest iterative method. It computes the largest eigenvalue in absolute value and its corresponding eigenvector by repeatedly multiplying a random vector by the matrix and normalizing it. The power method is easy to implement and only requires matrix-vector multiplication, but it converges slowly and may fail if there are multiple eigenvalues of the same magnitude.\n• Inverse iteration method: This is a variant of the power method, which computes the eigenvalue closest to a given value and its corresponding eigenvector by repeatedly solving a linear system with the matrix shifted by the given value and normalizing the solution. The inverse iteration method can be used to find any eigenvalue, but it requires solving a linear system at each step, which can be costly and unstable.\n• Rayleigh quotient iteration method: This is another variant of the power method, which computes the eigenvalue and eigenvector of a symmetric matrix by using the Rayleigh quotient, which is the ratio of and , as the shift value for the inverse iteration method. The Rayleigh quotient iteration method can converge faster than the inverse iteration method, but it is not guaranteed to converge to the desired eigenvalue and may oscillate or diverge. = Arnoldi method: This is a generalization of the power method, which computes a few eigenvalues and eigenvectors of a matrix by constructing an orthogonal basis for a Krylov subspace, which is the span of successive powers of the matrix applied to a random vector, and then finding the eigenvalues and eigenvectors of a smaller matrix that preserves the action of the original matrix on the subspace. The Arnoldi method can find eigenvalues of any magnitude and multiplicity, but it requires storing and orthogonalizing the basis vectors, which can be expensive and ill-conditioned.\n• Davidson method: This is an improvement of the Arnoldi method, which computes a few eigenvalues and eigenvectors of a symmetric matrix by using a preconditioner, which is a matrix that approximates the inverse of the matrix, to accelerate the convergence of the Krylov subspace and reduce the size of the smaller matrix. The Davidson method can be more efficient and robust than the Arnoldi method, but it depends on the choice of the preconditioner, which can be challenging to construct and apply.\n• Jacobi-Davidson method: This is a further improvement of the Davidson method, which computes a few eigenvalues and eigenvectors of a symmetric matrix by using a correction equation, which is a linear system that updates the approximate eigenvector by minimizing the residual, to refine the Krylov subspace and the preconditioner. The Jacobi-Davidson method can be more accurate and flexible than the Davidson method, but it requires solving a correction equation at each step, which can be challenging and time-consuming.\n• Lanczos method: This is a special case of the Arnoldi method, which computes a few eigenvalues and eigenvectors of a symmetric matrix by constructing a tridiagonal matrix that preserves the action of the original matrix on the Krylov subspace. The Lanczos method can be faster and more stable than the Arnoldi method, but it may suffer from loss of orthogonality and spurious eigenvalues due to round-off errors.\n\nvarious iterative methods for finding eigenvalues and eigenvectors of matrices in Python are presented. The following methods are implemented:\n\nThe performance and accuracy of the implementation are tested, and the results are compared with the built-in functions in SciPy and numpy."
    },
    {
        "link": "https://stackoverflow.com/questions/4400203/calculating-eigenvectors-in-c-sharp-using-advanced-matrix-library-in-c-net",
        "document": "Ok guys, I am using the following library: http://www.codeproject.com/KB/recipes/AdvancedMatrixLibrary.aspx\n\nAnd I wish to calculate the eigenvectors of certain matrices I have. I do not know how to formulate the code.\n\nSo far I have attempted:\n\nHowever it says that the best overloaded method match has some invalid arguments. I know the library works but I just do not know how to formulate my c# code.\n\nAny help would be fantastic!"
    },
    {
        "link": "https://visualstudiomagazine.com/Articles/2024/01/17/principal-component-analysis.aspx",
        "document": "Principal Component Analysis (PCA) from Scratch Using the Classical Technique with C#\n\nTransforming a dataset into one with fewer columns is more complicated than it might seem, explains Dr. James McCaffrey of Microsoft Research in this full-code, step-by-step machine learning tutorial.\n\nPrincipal component analysis (PCA) is a classical machine learning technique. The goal of PCA is to transform a dataset into one with fewer columns. This is called dimensionality reduction. The transformed data can be used for visualization or as the basis for prediction using ML techniques that can't deal with a large number of predictor columns.\n\nThere are two main techniques to implement PCA. The first technique computes eigenvalues and eigenvectors from a covariance matrix derived from the source data. The second PCA technique sidesteps the covariance matrix and computes a singular value decomposition (SVD) of the source data. Both techniques give the same results, subject to very small differences.\n\nThis article presents a from-scratch C# implementation of the first technique: compute eigenvalues and eigenvectors from the covariance matrix. If you're not familiar with PCA, most of the terminology -- covariance matrix, eigenvalues and eigenvectors and so on -- sounds quite mysterious. But the ideas will become clear shortly.\n\nA good way to see where this article is headed is to take a look at the screenshot of a demo program in Figure 1.\n\nFor simplicity, the demo uses just nine data items:\n\nThe nine data items are one-based items 1, 10, 20, 51, 61, 71, 101, 111 and 121 of the 150-item Iris dataset. Each line represents an iris flower. The four columns are sepal length, sepal width, petal length and petal width (a sepal is a green leaf-like structure). Because there are four columns, the data is said to have dimension = 4.\n\nThe leading index values in brackets are not part of the data. Each data item is one of three species: 0 = setosa, 1 = versicolor 2 = virginica. The first three items are setosa, the second three items are versicolor and the last three items are virginica. The species labels were removed because they're not used for PCA dimensionality reduction.\n\nPCA is intended for use with strictly numeric data. Mathematically, the technique works with Boolean variables (0-1 encoded) and for one-hot encoded categorical data. Conceptually, however, applying PCA to non-numeric data is questionable, and there is very little research on the topic.\n\nThe demo program applies z-score normalization to the source data. Then a 4-by-4 covariance matrix is computed from the normalized data. Next, four eigenvalues and four eigenvectors are computed from the covariance matrix (using the Jacobi algorithm). Next, the demo uses the eigenvectors to compute transformed data:\n\nAt this point, in a non-demo scenario, you could extract the first two or first three of the columns of the transformed data to use as a surrogate for the source data. For example, if you used the first two columns, you could graph the data with the first column acting as x-values and the second column acting as y-values.\n\nThe demo program concludes by programmatically reconstructing the source data from the transformed data. The idea is to verify the PCA worked correctly and also to illustrate that the transformed data contains all of the information in the source data.\n\nThis article assumes you have intermediate or better programming skill but doesn't assume you know anything about principal component analysis. The demo is implemented using C#, but you should be able to refactor the demo code to another C-family language if you wish. All normal error checking has been removed to keep the main ideas as clear as possible.\n\nThe source code for the demo program is too long to be presented in its entirety in this article. The complete code is available in the accompanying file download, and is also available online.\n\nUnderstanding Principal Component Analysis\n\nThe best way to understand PCA is to take a closer look at the demo program. The demo has eight steps:\n\nThe demo loads the source data using a helper MatLoad() function:\n\nThe arguments to MatLoad() mean fetch columns 0,1,2,3 of the comma-separated data, where a \"#\" character indicates a comment line. The arguments to MatShow() mean display using four decimals, with a field width of nine per element, and display indices.\n\nNormalizing the Data\n\nEach column of the data is normalized using these statements:\n\nThe four means and standard deviations for each column are saved because they are needed to reconstruct the original source data. The terms standardization and normalization are technically different but are often used interchangeably. Standardization is a specific type of normalization (z-score). PCA documentation tends to use the term standardization, but I sometimes use the more general term normalization.\n\nEach value is normalized using the formula x' = (x - u) / s, where x' is the normalized value, x is the original value, u is the column mean and s is the column biased standard deviation (divide sum of squares by n rather than n-1 where n is the number of values in the column).\n\nFor example, suppose a column of some data has just n = 3 values: (4, 15, 8). The mean of the column is (4 + 15 + 8) / 3 = 9.0. The biased standard deviation of the column is sqrt( ((4 - 9.0)^2 + (15 - 9.0)^2 + (8 - 9.0)^2) / n) ) = sqrt( (25.0 + 36.0 + 1.0) / 3 ) = sqrt(62.0 / 3) = 4.55.\n\nThe normalized values for (4, 15, 8) are:\n\nNormalized values that are negative are less than the column mean, and normalized values that are positive are greater than the mean. For the demo data, the normalized data is:\n\nAfter normalization, each value in a column will typically be between -3.0 and +3.0. Normalized values that are less than -6.0 or greater than +6.0 are extreme and should be investigated. The idea behind normalization is to prevent columns with large values (such as employee annual salaries) from overwhelming columns with small values (such as employee age).\n\nComputing the Covariance Matrix\n\nThe covariance matrix of the normalized source data is computed like so:\n\nThe \"false\" argument in the call to CovarMatrix() means to treat the data as if it is organized so that each variable (sepal length, sepal width, petal length and petal width) is in a column. A \"true\" argument means the data variables are organized as rows. I use this interface to match that of the Python numpy.cov() library function.\n\nThe covariance of two vectors is a measure of the joint variability of the vectors. For example, if v1 = [4, 9, 8] and v2 = [6, 8, 1], then covariance(v1, v2) = -0.50. Loosely speaking, the closer the covariance is to 0, the less associated the two vectors are. The larger the covariance, the greater the association. The sign of the covariance indicates the direction of the association. There's no upper limit on the magnitude of a covariance because it will increase as the number of elements in the vectors increases. See my article, \"Computing the Covariance of Two Vectors Using C#\", for a worked example.\n\nA covariance matrix stores the covariance between all pairs of columns in the normalized data. For example, cell [0][3] of the covariance matrix holds the covariance between column 0 and column 3. For the demo data, the covariance matrix is:\n\nNotice that the covariance matrix is symmetric because covariance(x, y) = covariance(y, x). The values on the diagonal of the covariance matrix are all the same (1.1250) due to the z-score normalization.\n\nComputing the Eigenvalues and Eigenvectors\n\nThe eigenvalues and eigenvectors of the covariance matrix are computed using this code:\n\nFor the demo data that has dim = 4, the covariance matrix of the normalized data has shape 4-by-4. There are four eigenvalues (single values like 1.234) and four eigenvectors, each with four values. The eigenvalues and eigenvectors are paired.\n\nFor the demo data, the four eigenvectors (as columns) are:\n\nThe program-defined Eigen() function returns the eigenvectors as columns, so the first eigenvector is (0.5498, -0.0438, 0.5939, 0.5857). The associated eigenvector is 3.1167. For PCA it doesn't help to overthink the deep mathematics of eigenvalues and eigenvectors. You can think of eigenvalues and eigenvectors as a kind of factorization of a matrix that captures all the information in the matrix in a sequential way.\n\nComputing eigenvalues and eigenvectors is one of the most difficult problems in numerical programming. Because a covariance matrix is mathematically symmetric positive semidefinite (think \"relatively simple structure\"), it is possible to use a technique called the Jacobi algorithm to find the eigenvalues and eigenvectors. The majority of the demo code is the Eigen() function and its helpers.\n\nEigenvectors from a matrix are not unique in the sense that the sign is arbitrary. For example, an eigenvector (0.20, -0.50, 0.30) is equivalent to (-0.20, 0.50, -0.30). The eigenvector sign issue does not affect PCA.\n\nSorting the Eigenvalues and Eigenvectors\n\nThe next step in PCA is to sort the eigenvalues and their associated eigenvectors from largest to smallest. Some library eigen() functions return eigenvalues and eigenvectors already sorted, and some eigen() functions do not. The demo program assumes that the eigenvalues and eigenvectors are not necessarily sorted.\n\nFirst, the four eigenvalues are sorted in a way that saves the ordering so that the paired eigenvectors can be sorted in the same order:\n\nFor the demo data, the sorted eigenvalues are (3.1167, 1.1930, 0.1868, 0.0036). The program-defined ArgSort() function doesn't sort but returns the order in which to sort from smallest to largest. The eigenvalues are sorted from smallest to largest using the built-in Array.Sort() function and then reversed to largest to smallest using the built-in Array.Reverse() function.\n\nThe eigenvectors are sorted using these statements:\n\nThe MatExtractCols() function pulls out each column in the eigenvectors, ordered by the idxs array, which holds the order in which to sort from largest to smallest. The now-sorted eigenvectors are converted from columns to rows using the MatTranspose() function only because that makes them a bit easier to read. The eigenvectors will have to be converted back to columns later. The point is that when you're looking at PCA documentation or working with PCA code, it's important to keep track of whether the eigenvectors are stored as columns (usually) or rows (sometimes)."
    },
    {
        "link": "https://stackoverflow.com/questions/53469969/algorithm-for-finding-eigenvectors-given-eigenvalues-of-a-3x3-matrix-in-c-sharp",
        "document": "I am trying to find the best OOBB hitboxes for my meshes using PCA. In order to do this, I need the eigenvectors but I am kind of lost how to compute them without using a huge library.\n\nI implemented an algorithm that computes three eigenvalues given a 3x3 Matrix. The code for this originally is from Wikipedia:\n\nHowever, the Wikipedia article has no code for calculating the eigenvectors for the three eigenvalues. I tried to understand this topic, but my math skills are quite limited. I would have to google every second word in every tutorial.\n\nSo my question is:\n\nIf I have the 3x3 matrix and three eigenvalues, is there any simple way to compute the corresponding eigenvectors without using external libraries?"
    },
    {
        "link": "https://thuvienso.dau.edu.vn:88/bitstream/DHKTDN/6142/1/Numerical%20Methods%2C%20Algorithms%20and%20Tools%20in%20C%23.pdf",
        "document": ""
    },
    {
        "link": "https://forums.codeguru.com/showthread.php?512477-Eigenvalues-and-Eigenvectors-Calculation",
        "document": "\n• Hi,\n\n \n\n I'm doing a project for computers class in C#.\n\n \n\n In that project I need to calculate the eigenvalues and eigenvectors of a square matrix.\n\n \n\n Does someone have an algorithm for that?\n\n \n\n Yuval Lewi\n• Does this help? http://en.wikipedia.org/wiki/Eigenva...d_eigenvectors I was thrown out of college for cheating on the metaphysics exam; I looked into the soul of the boy sitting next to me.\n\n \n\n This is a snakeskin jacket! And for me it's a symbol of my individuality, and my belief... in personal freedom.\n• I recommend the QR algorithm: http://en.wikipedia.org/wiki/QR_algorithm\n\n \n\n Calculating eigenvalues programatically is a bit mathematically complicated though, so ... hang on to your hat.\n\n \n\n To be honest, if you don't HAVE to write your own algorithm to do it, I would try to find some good numerical software that can do it (you can even find some libraries with C# wrappers).\n\n \n\n Good luck! \n\n \n\n \n\n http://blog.biophysengr.net\n\n --\n\n All advice is offered in good faith only. You are ultimately responsible for effects of your programs and the integrity of the machines they run on. Best Regards,--All advice is offered in good faith only. You are ultimately responsible for effects of your programs and the integrity of the machines they run on.\n• Hate to bring up such an old post, but in case it should be useful to those searching the archive: The Math.NET Numerics package (see http://numerics.mathdotnet.com/) has an eigensolver that is relatively easy to call and use. However, I seem to think it was taking a long time on very large sparse matrices (compared to the Octave [Matlab clone] eigensolver on the same matrix). However, for moderate-size dense (or sparse) matrices, it worked just fine. I haven't been able to find C# code that was good for large, sparse matrices. Presumably, such a thing would be a port of ARPACK, but I have had no luck finding a library like that. \n\n \n\n \n\n http://blog.biophysengr.net\n\n --\n\n All advice is offered in good faith only. You are ultimately responsible for effects of your programs and the integrity of the machines they run on. Best Regards,--All advice is offered in good faith only. You are ultimately responsible for effects of your programs and the integrity of the machines they run on.\n• You may not post new threads\n• You may not post replies\n• You may not post attachments\n• You may not edit your posts\n\n\n\n\n\n\n\n \n\n \n\n Click Here to Expand Forum to Full Width \n\n \n\n * The Best Reasons to Target Windows 8\n\n Learn some of the best reasons why you should seriously consider bringing your Android mobile development expertise to bear on the Windows 8 platform.\n• * Porting from Android to Windows 8: The Real Story\n\n Do you have an Android application? How hard would it really be to port to Windows 8?\n• * Guide to Porting Android Applications to Windows 8\n\n If you've already built for Android, learn what do you really need to know to port your application to Windows Phone 8.\n• * HTML5 Development Center\n\n Our portal for articles, videos, and news on HTML5, CSS3, and JavaScript\n• * Windows App Gallery\n\n See the Windows 8.x apps we've spotlighted or submit your own app to the gallery!"
    }
]