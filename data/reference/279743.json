[
    {
        "link": "https://sitepoint.com/javascript-3d-minecraft-editor",
        "document": "This article was peer reviewed by Paul O’Brien. Thanks to all of SitePoint’s peer reviewers for making SitePoint content the best it can be!\n\nI’ve always wanted to build a 3D game. I’ve just never had the time and energy to learn the intricacies of 3D programming. Then I discovered I didn’t need to…\n\nWhile tinkering one day, I got to thinking that maybe I could simulate a 3D environment using CSS transformations. I stumbled across an old article about creating 3D worlds with HTML and CSS.\n\nI wanted to simulate a Minecraft world (or a tiny part of it at least). Minecraft is a sandbox game, in which you can break and place blocks. I wanted the same kind of functionality, but with HTML, JavaScript, and CSS.\n\nCome along as I describe what I learned, and how it can help you to be more creative with your CSS transformations!\n\nThis is just half of the adventure. If you’d like to know how to persist the designs to an actual server, check out the sister post, PHP Minecraft Mod. There we explore ways to interact with a Minecraft server, to manipulate it in real time and respond to user input.\n• Utilize CSS transformations to simulate a 3D environment, bypassing the need for complex 3D programming skills.\n• Employ JavaScript and HTML to mimic Minecraft’s block manipulation capabilities, allowing users to create and interact with a 3D world.\n• Understand and apply CSS properties like `transform-origin` and `transform` to control element positioning and orientation in three dimensions.\n• Learn to build and manipulate 3D block structures using JavaScript classes and methods, enhancing the visual complexity of scenes.\n• Explore dynamic interaction in the 3D space, such as adding, removing, and highlighting blocks with mouse events.\n• Enhance user experience by implementing viewport controls like zoom and rotation, enabling better navigation and visibility of the 3D space.\n• Leverage open-source resources and community contributions to expand and customize the functionality of the 3D Minecraft editor.\n\nThe Things We’re Already Doing\n\nI’ve written my fair share of CSS and I’ve come to understand it quite well, for the purpose of building websites. But that understanding is predicated on the assumption that I’m going to be working in a 2D space.\n\nLet’s consider an example:\n\nHere we have a canvas element, starting at the top left corner of the page, and stretching all the way to the bottom right. On top of that, we’re adding a tools element. It starts from the left and from the top of the page, and measures wide by high.\n\nDepending on the order and are added to the markup, it’s entirely possible that could overlap . That is except for the styles applied to each.\n\nYou’re probably used to thinking of elements, styled in this way, as 2D surfaces with the potential to overlap each other. But that overlapping is essentially a third dimension. , , and may as well be renamed to , , and . So long as we assume every element has a fixed depth of , and has an implicit unit, we’re already thinking in 3D terms.\n\nWhat some of us tend to struggle with are the concepts of rotation and translation in this third dimension…\n\nCSS translations duplicate this familiar functionality, in an API that extends beyond the limitations , , and place on us. It’s possible to replace some of our previous styles with translations:\n\nInstead of defining and offsets (with an assumed origin of from the left and from the top), we can declare an explicit origin. We can perform all sorts of transformations on this element, for which use as the centre. moves the element to the right and down. We can use negative values to move the element left and/or up.\n\nWith the ability to define an origin for our transformations, we can start to do other interesting things as well. For example, we can rotate and scale elements:\n\nEvery element starts with a default of , but a value of sets , , and to the equivalent of . We can scale our element to a value between and , and rotate it (clockwise) by degrees or radians. And we can convert between the two with:\n\nTo rotate an element anti-clockwise, we just need to use a negative or value.\n\nWhat’s even more interesting, about these transformations, is that we can use 3D versions of them.\n\nLet’s begin to create our 3D world. We’ll start by making a space in which to place our blocks. Create a new file, called :\n\nHere we stretch the body to the full width and height, resetting padding to . Then we create a smallish , which we’ll use to hold various blocks. We use 50% and , as well as a negative left and top (equal to half the and ) to horizontally and vertically centre it. Then we tilt it slightly (using 3D rotation) so that we have a perspective view of where the blocks will be.\n\nThe result should look something like this:\n\nSee the Pen Empty Scene by SitePoint (@SitePoint) on CodePen.\n\nNow, let’s start to add a block shape to the scene. We’ll need to create a new JavaScript file, called :\n\nEach block needs to be a 6-sided, 3D shape. We can break the different parts of construction into methods to (1) build the whole block, (2) build each surface, and (3) get the texture of each surface.\n\nEach of these behaviours (or methods) are contained within an ES6 class. It’s a neat way to group data structures and the methods that operate on them together. You may be familiar with the traditional form:\n\nThis may look a little different, but it’s much the same. In addition to shorter syntax, ES6 classes also provide shortcuts for extending prototypes and calling overridden methods. But I digress…\n\nLet’s work from the bottom up:\n\nEach surface (or face) consists of a rotated and translated div. We can’t make elements thicker than , but we can simulate depth by covering up all the holes and using multiple elements parallel to each other. We can give the block the illusion of depth, even though it is hollow.\n\nTo that end, the method takes a set of coordinates: , , and for the position of the face. We also provide rotations for each axis, so that we can call with any configuration and it will translate and rotate the face just how we want it to.\n\nWe’re used to thinking in terms of single pixel positions, but a game like Minecraft works at a larger scale. Every block is bigger, and the coordinate system deals with the position of the block, not individual pixels that make it up. I want to convey the same sort of idea here…\n\nWhen someone creates a new block, at × × , I want that to mean × × . So we multiply each coordinate by the default size (in this case , because that’s the size of the textures in the texture pack we’ll be using).\n\nThen we create a container div (which we call ). Inside it we place another 3 divs. These will show us the axis of our block – they’re like guides in a 3D rendering program. We should also add some new CSS for our block:\n\nThis styling is similar to what we’ve seen before. We need to remember to set on the , so that the axis are rendered in their own 3D space. We give each a different colour, and make them slightly bigger than the block they’re contained in. This is so that they’ll be visible even when the block has sides.\n\nLet’s create a new block, and add it to the :\n\nThe result should look something like this:\n\nSee the Pen Basic 3D Block by SitePoint (@SitePoint) on CodePen.\n\nI found this code to be a bit of trial and error (owing to my limited experience with 3D perspective). Each element starts off in exactly the same position as the element. That is, in the vertical centre of the and facing the top.\n\nSo, for the “top” element, I had to translate it “up” by half the size of the block, but I didn’t have to rotate it in any way. For the “bottom” element, I had to rotate it 180 degrees (along the x or y axis), and move it down by half the size of the block.\n\nUsing similar thinking, I rotated and translated each of the remaining sides. I also had to add corresponding CSS for them:\n\nAdding prevents the “bottom” side of the elements from being rendered. Usually they would just appear the same (only mirrored) no matter how they were rotated. With hidden back faces, only the “top” side is rendered. Take care when turning this on: your surfaces need to be rotated the right way around or sides of the block will just disappear. That’s the reason for the 90/270/-90/-270 rotations I’ve given the sides.\n\nSee the Pen 3D Block Sides by SitePoint (@SitePoint) on CodePen.\n\nLet’s make this block look a bit more realistic. We need to create a new file, called , and override the method:\n\nWe begin by defining a look-up table for the textures of the sides and top of the block. The texture pack doesn’t specify which textures should be used for the bottom, so we’ll just reuse the top textures.\n\nIf the side needing a texture is “top” or “bottom”, then we fetch a random texture from the “top” list. The random method doesn’t exist, until we define it:\n\nSimilarly, if we need a texture for a side, we fetch a random one. These textures are seamless, so the randomisation works in our favour.\n\nThe result should look something like this:\n\nSee the Pen 3D Block Textures by SitePoint (@SitePoint) on CodePen.\n\nHow do we make this interactive? Well, a good place to start is with a scene. We’ve already been placing blocks in the scene, so now we just have to enable dynamic placement!\n\nTo begin with, we can render a flat surface of blocks:\n\nGreat, that gives us a flat surface to start adding blocks to. Now, let’s highlight surfaces as we hover over them with our cursor:\n\nSomething strange is going on, though:\n\nThis is because the surfaces are clipping through each other randomly. There’s no nice way to fix this problem, but we can prevent it from happening by scaling the blocks slightly:\n\nWhile this does make things look better, it will affect performance the more blocks there are in the scene. Tread lightly when scaling many elements at a time…\n\nLet’s tag each surface with the block and type that belong to it:\n\nThen, as we click on a surface, we can derive a new set of coordinates and create a new block:\n\nhas a simple but important task. Given the type of side, and the coordinates for the block to which it belongs, should return a new set of coordinates. These are where the new block will be placed.\n\nThen we’ve attached an event listener. It will be triggered for each that gets clicked. As this happens, we get the block to which the side belongs, and derive a new set of coordinates for the next block. Once we have those, we create the block and append it to the scene.\n\nSee the Pen Pre-populated Scene by SitePoint (@SitePoint) on CodePen.\n\nIt would be helpful to see an outline of the block that we’re about to place, before we place it. This is sometimes referred to as “showing a ghost” of the thing we’re about to do.\n\nThe code to enable this is quite similar to that which we’ve already seen:\n\nThe main difference is that we maintain a single instance of the ghost block. As each new one is created, the old one is removed. This could benefit from a few additional styles:\n\nLeft active, the pointer events associated with the elements of the ghost would counteract the and events of the side underneath. Since we don’t need to interact with the ghost elements, we can disable these pointer events.\n\nSee the Pen 3D Block Ghosts by SitePoint (@SitePoint) on CodePen.\n\nThe more interactivity we add, the harder it is to see what’s going on. It seems like a good time to do something about that. It’d be awesome if we could zoom and rotate the viewport, to be able to see what’s going on a little better…\n\nLet’s start with zoom. Many interfaces (and games) allow viewport zooming by scrolling the mouse wheel. Different browsers handle mouse wheel events in different ways, so it make sense to use an abstraction library.\n\nOnce that’s installed, we can hook into the events:\n\nNow we can control the scale of the entire scene, just by scrolling the mouse wheel. Unfortunately, the moment we do, the rotations are overridden. We need to take the rotation into account, as we allow dragging the viewport with the mouse to adjust it:\n\nThis function won’t only account for the scale factor of the scene, but also the x, y, and z rotations factors. We also need to change our zooming event listener:\n\nNow, we can start to rotate the scene. We need:\n• An event listener for when the drag action starts\n• An event listener for when the mouse moves (while dragging)\n• An event listener for when the drag action stops\n\nSomething like these should do the trick:\n\nOn we capture the initial mouse and coordinates. As the mouse moves (if the button is still being pressed) we adjust the and by a scaled amount. There’s no harm in letting the values go over degrees or below degrees, but these would look terrible if we wanted to render them onscreen.\n\nWhen the mouse button is released, we unset the and , so that the listener stops computing things. We could just clear , but clearing both feels cleaner to me.\n\nUnfortunately, the mousedown event can interfere with the click event on block sides. We can get around this by preventing event bubbling:\n\nSee the Pen Zoom And Rotation by SitePoint (@SitePoint) on CodePen.\n\nLet’s round out the experiment by adding the ability to remove blocks. We need to do a couple of subtle but important things:\n\nIt’ll be easier to do these with CSS, as long as we have a body class to indicate whether we’re in addition (normal) mode or subtraction mode:\n\nWhen a modifier key is pressed ( , , or ), this code will make sure has a class. This makes it easier to target various elements using this class:\n\nWe’re checking for a number of modifier keys, since different operating systems intercept different modifiers. For example, and work on macOS, whereas works on Ubuntu.\n\nIf we click on a block, when we’re in subtraction mode, we should remove it:\n\nThis is the same click event listener we had before, but instead of just adding new blocks when a side is clicked, we first check if we’re in subtraction mode. If we are, the block we just clicked is removed from the scene.\n\nThe final demo is wondrous to play with:\n\nSee the Pen Removing Blocks by SitePoint (@SitePoint) on CodePen.\n\nThere’s a long way to go before we support as many blocks and interactions as Minecraft, but this is a good start. What’s more, we managed to achieve this without needing to study advanced 3D techniques. It’s an unconventional (and creative) use of CSS transformations!\n\nIf you’re keen to do more with this code, head over to the other half of this adventure. You don’t have to be a PHP expert to interact with Minecraft servers. And just imagine the awesome things you can do with that knowledge…\n\nAnd don’t forget: this is just half of the adventure. If you’d like to know how to persist the designs to an actual server, check out the sister post, PHP Minecraft Mod. There we explore ways to interact with a Minecraft server, to manipulate it in real time and respond to user input."
    },
    {
        "link": "https://medium.com/@cassycassy/how-to-build-a-javascript-3d-minecraft-editor-d4db1fba9b94",
        "document": "I’ve always wanted to build a 3D game. I’ve just never had the time and energy to learn the intricacies of 3D programming. Then I discovered I didn’t need to…\n\nWhile tinkering one day, I got to thinking that maybe I could simulate a 3D environment using CSS transformations. I stumbled across an old article about creating 3D worlds with HTML and CSS.\n\nI wanted to simulate a Minecraft world (or a tiny part of it at least). Minecraft is a sandbox game, in which you can break and place blocks. I wanted the same kind of functionality, but with HTML, JavaScript, and CSS.\n\nCome along as I describe what I learned, and how it can help you to be more creative with your CSS transformations!\n\nNote: Most of the code for this tutorial can be found on Github. I’ve tested it in the latest version of Chrome. I can’t promise it will look exactly the same in other browsers, but the core concepts are universal.\n\nThis is just half of the adventure. If you’d like to know how to persist the designs to an actual server, check out the sister post, PHP Minecraft Mod. There we explore ways to interact with a Minecraft server, to manipulate it in real time and respond to user input.\n\nThe Things We’re Already Doing\n\nI’ve written my fair share of CSS and I’ve come to understand it quite well, for the purpose of building websites. But that understanding is predicated on the assumption that I’m going to be working in a 2D space.\n\nLet’s consider an example:\n\nHere we have a canvas element, starting at the top left corner of the page, and stretching all the way to the bottom right. On top of that, we’re adding a tools element. It starts from the left and from the top of the page, and measures wide by high.\n\nDepending on the order and are added to the markup, it’s entirely possible that could overlap . That is except for the styles applied to each.\n\nYou’re probably used to thinking of elements, styled in this way, as 2D surfaces with the potential to overlap each other. But that overlapping is essentially a third dimension. , , and may as well be renamed to , , and . So long as we assume every element has a fixed depth of , and has an implicit unit, we’re already thinking in 3D terms.\n\nWhat some of us tend to struggle with are the concepts of rotation and translation in this third dimension…\n\nCSS translations duplicate this familiar functionality, in an API that extends beyond the limitations , , and place on us. It’s possible to replace some of our previous styles with translations:\n\nInstead of defining and offsets (with an assumed origin of from the left and from the top), we can declare an explicit origin. We can perform all sorts of transformations on this element, for which use as the centre. moves the element to the right and down. We can use negative values to move the element left and/or up.\n\nWith the ability to define an origin for our transformations, we can start to do other interesting things as well. For example, we can rotate and scale elements:\n\nEvery element starts with a default of , but a value of sets , , and to the equivalent of . We can scale our element to a value between and , and rotate it (clockwise) by degrees or radians. And we can convert between the two with:\n\nTo rotate an element anti-clockwise, we just need to use a negative or value.\n\nWhat’s even more interesting, about these transformations, is that we can use 3D versions of them.\n\nEvergreen browsers have pretty good support for these styles, though they may require vendor prefixes. CodePen has a neat “autoprefix” option, but you can add libraries like PostCSS to your local code to achieve the same thing.\n\nLet’s begin to create our 3D world. We’ll start by making a space in which to place our blocks. Create a new file, called :\n\nHere we stretch the body to the full width and height, resetting padding to . Then we create a smallish , which we’ll use to hold various blocks. We use 50% and , as well as a negative left and top (equal to half the and ) to horizontally and vertically centre it. Then we tilt it slightly (using 3D rotation) so that we have a perspective view of where the blocks will be.\n\nNotice how we define . This is so that child elements can also be manipulated in a 3D space.\n\nNow, let’s start to add a block shape to the scene. We’ll need to create a new JavaScript file, called :\n\nEach block needs to be a 6-sided, 3D shape. We can break the different parts of construction into methods to (1) build the whole block, (2) build each surface, and (3) get the texture of each surface.\n\nEach of these behaviours (or methods) are contained within an ES6 class. It’s a neat way to group data structures and the methods that operate on them together. You may be familiar with the traditional form:\n\nThis may look a little different, but it’s much the same. In addition to shorter syntax, ES6 classes also provide shortcuts for extending prototypes and calling overridden methods. But I digress…\n\nLet’s work from the bottom up:\n\nEach surface (or face) consists of a rotated and translated div. We can’t make elements thicker than , but we can simulate depth by covering up all the holes and using multiple elements parallel to each other. We can give the block the illusion of depth, even though it is hollow.\n\nTo that end, the method takes a set of coordinates: , , and for the position of the face. We also provide rotations for each axis, so that we can call with any configuration and it will translate and rotate the face just how we want it to.\n\nWe’re used to thinking in terms of single pixel positions, but a game like Minecraft works at a larger scale. Every block is bigger, and the coordinate system deals with the position of the block, not individual pixels that make it up. I want to convey the same sort of idea here…\n\nWhen someone creates a new block, at × × , I want that to mean × × . So we multiply each coordinate by the default size (in this case , because that’s the size of the textures in the texture pack we’ll be using).\n\nThen we create a container div (which we call ). Inside it we place another 3 divs. These will show us the axis of our block – they’re like guides in a 3D rendering program. We should also add some new CSS for our block:\n\nThis styling is similar to what we’ve seen before. We need to remember to set on the , so that the axis are rendered in their own 3D space. We give each a different colour, and make them slightly bigger than the block they’re contained in. This is so that they’ll be visible even when the block has sides.\n\nLet’s create a new block, and add it to the :\n\nI found this code to be a bit of trial and error (owing to my limited experience with 3D perspective). Each element starts off in exactly the same position as the element. That is, in the vertical centre of the and facing the top.\n\nSo, for the “top” element, I had to translate it “up” by half the size of the block, but I didn’t have to rotate it in any way. For the “bottom” element, I had to rotate it 180 degrees (along the x or y axis), and move it down by half the size of the block.\n\nUsing similar thinking, I rotated and translated each of the remaining sides. I also had to add corresponding CSS for them:\n\nAdding prevents the “bottom” side of the elements from being rendered. Usually they would just appear the same (only mirrored) no matter how they were rotated. With hidden back faces, only the “top” side is rendered. Take care when turning this on: your surfaces need to be rotated the right way around or sides of the block will just disappear. That’s the reason for the 90/270/-90/-270 rotations I’ve given the sides.\n\nLet’s make this block look a bit more realistic. We need to create a new file, called , and override the method:\n\nWe’re going to use a popular texture pack, called Sphax PureBDCraft. It’s free to download and use (provided you’re not try to sell it on), and it comes in a variety of sizes. I’m using the version.\n\nWe begin by defining a look-up table for the textures of the sides and top of the block. The texture pack doesn’t specify which textures should be used for the bottom, so we’ll just reuse the top textures.\n\nIf the side needing a texture is “top” or “bottom”, then we fetch a random texture from the “top” list. The random method doesn’t exist, until we define it:\n\nSimilarly, if we need a texture for a side, we fetch a random one. These textures are seamless, so the randomisation works in our favour.\n\nHow do we make this interactive? Well, a good place to start is with a scene. We’ve already been placing blocks in the scene, so now we just have to enable dynamic placement!\n\nTo begin with, we can render a flat surface of blocks:\n\nGreat, that gives us a flat surface to start adding blocks to. Now, let’s highlight surfaces as we hover over them with our cursor:\n\nSomething strange is going on, though:\n\nThis is because the surfaces are clipping through each other randomly. There’s no nice way to fix this problem, but we can prevent it from happening by scaling the blocks slightly:\n\nWhile this does make things look better, it will affect performance the more blocks there are in the scene. Tread lightly when scaling many elements at a time…\n\nLet’s tag each surface with the block and type that belong to it:\n\nThen, as we click on a surface, we can derive a new set of coordinates and create a new block:\n\nhas a simple but important task. Given the type of side, and the coordinates for the block to which it belongs, should return a new set of coordinates. These are where the new block will be placed.\n\nThen we’ve attached an event listener. It will be triggered for each that gets clicked. As this happens, we get the block to which the side belongs, and derive a new set of coordinates for the next block. Once we have those, we create the block and append it to the scene.\n\nIt would be helpful to see an outline of the block that we’re about to place, before we place it. This is sometimes referred to as “showing a ghost” of the thing we’re about to do.\n\nThe code to enable this is quite similar to that which we’ve already seen:\n\nThe main difference is that we maintain a single instance of the ghost block. As each new one is created, the old one is removed. This could benefit from a few additional styles:\n\nLeft active, the pointer events associated with the elements of the ghost would counteract the and events of the side underneath. Since we don’t need to interact with the ghost elements, we can disable these pointer events.\n\nThe more interactivity we add, the harder it is to see what’s going on. It seems like a good time to do something about that. It’d be awesome if we could zoom and rotate the viewport, to be able to see what’s going on a little better…\n\nLet’s start with zoom. Many interfaces (and games) allow viewport zooming by scrolling the mouse wheel. Different browsers handle mouse wheel events in different ways, so it make sense to use an abstraction library.\n\nOnce that’s installed, we can hook into the events:\n\nNow we can control the scale of the entire scene, just by scrolling the mouse wheel. Unfortunately, the moment we do, the rotations are overridden. We need to take the rotation into account, as we allow dragging the viewport with the mouse to adjust it:\n\nThis function won’t only account for the scale factor of the scene, but also the x, y, and z rotations factors. We also need to change our zooming event listener:\n\nNow, we can start to rotate the scene. We need:\n• An event listener for when the drag action starts\n• An event listener for when the mouse moves (while dragging)\n• An event listener for when the drag action stops\n\nSomething like these should do the trick:\n\nOn we capture the initial mouse and coordinates. As the mouse moves (if the button is still being pressed) we adjust the and by a scaled amount. There’s no harm in letting the values go over degrees or below degrees, but these would look terrible if we wanted to render them onscreen.\n\nComputation inside a event listener can be computationally expensive due to how much this even listener may be triggered. There are potentially millions of pixels on the screen, and this listener could be triggered as the mouse moves to each one. That is why we exit early if the mouse button isn’t being held down.\n\nWhen the mouse button is released, we unset the and , so that the listener stops computing things. We could just clear , but clearing both feels cleaner to me.\n\nUnfortunately, the mousedown event can interfere with the click event on block sides. We can get around this by preventing event bubbling:\n\nLet’s round out the experiment by adding the ability to remove blocks. We need to do a couple of subtle but important things:\n\nIt’ll be easier to do these with CSS, as long as we have a body class to indicate whether we’re in addition (normal) mode or subtraction mode:\n\nWhen a modifier key is pressed ( , , or ), this code will make sure has a class. This makes it easier to target various elements using this class:\n\nWe’re checking for a number of modifier keys, since different operating systems intercept different modifiers. For example, and work on macOS, whereas works on Ubuntu.\n\nIf we click on a block, when we’re in subtraction mode, we should remove it:\n\nThis is the same click event listener we had before, but instead of just adding new blocks when a side is clicked, we first check if we’re in subtraction mode. If we are, the block we just clicked is removed from the scene.\n\nThere’s a long way to go before we support as many blocks and interactions as Minecraft, but this is a good start. What’s more, we managed to achieve this without needing to study advanced 3D techniques. It’s an unconventional (and creative) use of CSS transformations!\n\nIf you’re keen to do more with this code, head over to the other half of this adventure. You don’t have to be a PHP expert to interact with Minecraft servers. And just imagine the awesome things you can do with that knowledge…"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/js-drawing-shapes-canvas-api",
        "document": "In this article we’ll be looking at the HTML canvas element and the JavaScript canvas API to render complex shapes onto our web pages.\n\nAll we need to start is an HTML page with a canvas tag and a JavaScript file to manipulate it with.\n\nWith our canvas element in place, we now need to create a new variable with it and a canvas context, which adds a bunch of functionality onto our canvas. To keep things simple we’ll stick with 2D shapes, but with the context, 3D is also possible.\n\nFor our example we’ll need our canvas to be fullscreen but setting the size using CSS creates a strange blurry effect, which we obviously don’t want, so we’ll have to set it here.\n\nTo draw rectangles, on our context variable ( ), we can start adding what we want, measured in pixels:\n• : Sets the location and dimensions of our rectangle, and needs to be called before or .\n• : Renders an outline of everything before it.\n• : Renders the whole shape as a solid color.\n• and : Sets the outline and shape color. They are not functions like the others and need to be assigned a string.\n• and : Same as and but only for that item, works the same as .\n• : Clears everything inside of a certain area. Very useful when we get into animations where we’re constantly rendering new elements and don’t want the old ones to stick around.\n\nAnd here are a few examples where we draw some lines:\n\nThe only method we really need for drawing circles is . The angles are taken in radians and not degrees so for our end-angle we can just use , since that’s equal to 360 degrees, and the starting angle can be left at 0. We’re not going to need to specify a value for , so we can just leave it off since it defaults to false.\n\nIf you’ve ever used graphic design tools like Photoshop or Affinity Designer, these will seem very similar to some of their line tools.\n\nEssentially, quadratic and bezier curves are just free form lines with different methods of control. Quadratic curves are simpler in that they just have a start, endpoint, and what’s known as the control point, which acts as a handle for curving the line. You can see a wonderful interactive example here. Bezier curves, on the other hand, have two control points, at each end of the curve for more complex shapes. Another great example here.\n\nText works very similarly to rectangles with a few CSS-like options for styling:\n• Takes a string with the size in pixels and font family; like ‘ ’.\n• : Takes a string with the same options as its CSS counterpart; , , , , and .\n\nWhile there is still an enormous amount that can be done with HTML canvas like animations and interactivity, hopefully this was a good first introduction to some of its possibilities."
    },
    {
        "link": "https://github.com/alexbol99/flatten-js",
        "document": "flatten-js is a javascript library for manipulating abstract geometrical shapes like point, vector, line, ray, segment, circle, arc and polygon. Shapes may be organized into Planar Set - searchable container which support spatial queries.\n\nflatten-js provides a lot of useful methods and algorithms like finding intersections, checking inclusion, calculating distance, applying affine transformations, performing boolean operations and more.\n\nPackages are distributed in 3 formats: commonjs, umd and es6 modules. Package.json file provides various entry points suitable for different targets.\n\nTypeScript users may take advantage of static type checking with typescript definition file index.d.ts included into the package.\n\nflatten-js does not concern too much about visualization. Anyway, all classes implement svg() method, that returns a string which may be inserted into SVG container. It works pretty well together with d3js library, but it is definitely possible to create bridges to other graphic libraries.\n\nThe best way to start working with FlattenJS is to use awesome Observable javascript interactive notebooks. Check out collection of Tutorials published in Observable Notebooks.\n\nFull documentation may be found here: https://alexbol99.github.io/flatten-js/index.html\n\nIt is possible to import Flatten namespace as default import, and then destruct all classes from it.\n\nSome classes have shortcuts to avoid annoying new constructor:\n\nAfter module imported, it is possible to create some construction:\n\nYou may test the code above also in NPM RunKit\n\nYou may also check out examples section in the code which illustrate different use cases:\n• in a browser using tag with unpkg.com loader\n• Box (may be used as rectangle)\n\nPolygon in flatten-js library is actually a multi-polygon. Polygon is a collection of faces - closed oriented chains of edges, which may be of type Segment or Arc. The most external face called island, a face included into it is called hole. Holes in turn may have inner islands, number of inclusion levels is unlimited.\n\nOrientation of islands and holes is matter for calculation of relationships and boolean operations, holes should have orientation opposite to islands. It means that for proper results faces in a polygon should be orientable: they should not have self-intersections. Faces also should not overlap each other. Method checks if polygon fit these rules.\n\nConstructor of the polygon object accept various inputs:\n• Array of shapes (instances of Flatten.Segment or Flatten.Arc) that represent closed chains\n• Array of shapes as json objects that represent closed chains\n• Array of points (Flatten.Point) that represent vertices of the polygon\n• Array of numeric pairs [x,y] that represent vertices of the polygon\n\nPolygon provides various useful methods:\n• - split an edge of polygon adn create new vertex\n• - test if polygon contains shape (point, segment or arc)\n• - split to array of islands with holes\n\nMultiline represent an unclosed chain of edges of type Segment or Arc\n\nPlanar Set is a container of shapes that enables spatial seach by rectangular query.\n\nAll the classes have methods , and which may be chained. \n\nExample:\n\nAll classes have method that return array of intersection points, if two shapes intersect each other, or empty array otherwise. The is no predefined order of intersection points in the array.\n\nPlease don't be confused, there are another two methods that performs boolean intersection of polygons and logical predicate that check if two shapes intersected or not.\n\nAll basic classes and polygon have method that calculate distance to other shape. Together with the distance function returns the shortest segment between two shapes - segment between two closest point, where the first point lays on shape, and the second - on the other shape, see example:\n\nThe Dimensionally Extended nine-Intersection Model (DE-9IM) is a topological model and a standard used to describe the spatial relations of two geometries in 2-dimensional plane.\n\nFirst, for every shape we define:\n\nFor polygons, the interior, boundary and exterior are obvious, other types have some exclusions:\n\nThe DE-9IM model based on a 3×3 intersection matrix with the form:\n\nwhere and are two shapes (geometries),\n\ndenotes operation of intersection. Dimension of intersection result depends on the dimension of shapes, for example,\n• intersection between an interior of the line and an interior of the polygon is an array of segments\n• intersection between an interior of the line and boundary polygon is an array of points (may include segments in case of touching)\n• intersection between interiors of two polygons (if exists) will be a polygon.\n\nDE-9IM matrix describes any possible relationships between two shapes on the plane.\n\nDE-9IM matrix is available via method under namespace .\n\nEach element of DE-9IM matrix is an array of the objects representing corresponding intersection. Empty array represents case of no intersection. If intersection is not applicable (i.e. intersection with a boundary for a line which has no boundary), correspondent cell left undefined.\n\nIntersection between two exteriors not calculated because usually it is meaningless.\n\nAnother common way to represent DE-9IM matrix is a string where\n• represent intersection where array is not impty\n• means not relevant or not applicable\n\nString may be obtained with method.\n\nThe spatial relationships between two shapes exposed via namespace . The spatial predicates return if relationship match and otherwise.\n• - shapes a and b have at least one common point\n• - shapes a and b have at least one point in common but their interiors not intersect\n• - shape a lies in the interior of shape b\n• - shape b lies in the interior of shape a\n• - every point of a lies or in the interior or on the boundary of shape b\n• - every point of b lies or in the interior or on the boundary of shape a\n\nBoolean operations on polygons available via namespace BooleanOperations. Polygons in boolean operation should be valid: both operands should have same meaning of face orientation, faces should not overlap each other and should not have self-intersections.\n\nUser is responsible to provide valid polygons, boolean operation methods do not check validity.\n• - subtract second polygon from the first and return resulted polygon\n• - intersect two polygons and return boundary of intersection as 2 arrays. The first aray contains edges of the first polygon, the second - the edges of the second\n• - clip boundary of the first polygon with the interior of the second polygon\n\nImplementation based on Weiler-Atherton clipping algorithm, described in the article Hidden Surface Removal Using Polygon Area Sorting\n\nAll flatten-js shape objects may be serialized using method. transforms object to string using formatter implemented in the class. restore object from a string, and then constructor can use this object to create Flatten object.\n\nAll classes provide method, that create svg string that may be inserted into svg container element in a very straightforward way:\n\nMethod may accept as a parameter an object that enables to define several basic attributes of svg element: , , , , , and . If attributes not provided, method use default values."
    },
    {
        "link": "https://learn.microsoft.com/en-us/minecraft/creator/scriptapi?view=minecraft-bedrock-stable",
        "document": "This browser is no longer supported.\n\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support."
    },
    {
        "link": "https://medium.com/@aidobreen/js-promises-async-await-and-functional-programming-f2e5fa66b4ef",
        "document": "Unlike a lot of the internet, I happen to enjoy writing Javascript. I’m also a fan of functional programming; from a practical point of view, and from an aesthetic point of view. In the art of code, functional is beautiful.\n\nUnfortunately, like most JS developers, my love of functional code is at odds with the asynchronous nature of the language.\n• The web is inherently asynchronous. Waiting for network requests, user input or animations — they all need to happen without holding up the rest of our code. \n\n(Aside: Async is not unique to the web of course. And as any programmer who has dealt with multithreaded systems with tell you: It’s never simple.)\n• Functional programming doesn’t naturally map to asynchronous tasks. Why? A pure function should have two attributes: \n\nA) It should be deterministic: For a specific input, it should always produce the same output.\n\nB) It should not have side effects: that is, a function should not affect anything outside itself. Further, it should not rely on anything in the global state.\n\nHere are some examples:\n\nHow does this conflict with asynchronous programming?\n\nLet’s assume you want to calculate something based on an asynchronous request: the value of a user’s ETH wallet (Ethereum crypto currency tokens) in USD. You would take the total number of ETH, and multiply it by the most recent exchange rate — which you get from an API somewhere.\n\nThere are a few issues here.\n\nFirst and foremost, event though there isn’t a side-effect, is still a global variable, which we want to avoid. Instead, let’s pass it to the function as a parameter.\n\nSecond, assuming the function is asynchronous, this code will probably just throw an error. This is because will not have a value until finishes, but there is nothing telling our code to wait for , so the function will return the result of , before has a value.\n\nThere are two ways to fix this. Promises, or async/await.\n\nI’m not going to go into detail on promises here, there are much better resources out there. But to remind you:\n\nIn a nutshell, Promises either complete (resolve) or fail (reject). We can do something when a promise completes by using the function, and do something when it fails by using the function.\n\nLet’s assume returns a Promise. In the last example, would become a Promise object, which would have a function. \n\nWe could call with a callback function called fn which will receive when the promise completes. \n\nOR\n\nWe could implicitly treat as a promise, and use directly:\n\nPromises also do this funky thing called chaining. The function also returns a promise, which means we could do something else asynchronously inside that inner function and have a after that too. This chain also allows us to handle errors nicely. We can just put one at the end, and it will handle any asynchronous failures:\n\nLovely, but there’s still one problem: It’s not even close to functional.\n\nDeterministic? will return a different value at different times — even with the same input. That’s OK if we expect the USD exchange rate to change over time, but that’s not all. If the api request fails, we don’t get the same value, but we don’t even get the same data type!\n\nSide-effects? The function itself does not affect our global state, but what about the inner function passed to ? It relies on the value of , which cannot be passed directly. \n\nAnd what about the api call itself? How do we know this function isn’t affecting the global state of our app through some other method? Maybe by requesting the ETH price, it updates a request counter for our user state, limiting the number of requests we can make, preventing api access after X requests, or just invalidating our credentials and effectively logging us out of the whole api?\n\nThere is another approach that involves creating an asynchronous function. Again, the details are beyond the scope of this article, but are described excellently elsewhere. Again, to jog your memory:\n\nKurzgesagt, we can user the keyword to identify a function that will need to wait for something in it’s body to complete. Within the function, we use the keyword to identify what we want to wait for.\n\nOnce again, starting with:\n\nWe identify as an asynchronous function, and we wait for the result of :\n\nThis already looks really nice — right? \n\nWhat happens if the api fails? Well, we have to use a good old try/catch.\n\nIs this deterministic? Not if the we assume the api returns a different value over time and never fails.\n\nAre there side effects? Possibly, but at least visually, we don’t see the same inner function accessing the parent function scope. However, because async/await still uses promises under the hood, it’s technically still happening — it just doesn’t affect us.\n\nThe api may still have side effects, but we don’t have a choice about that.\n• The Promise version is longer in terms of lines of code by a whopping single line. I’ve seen many blog posts calling this a win for async/await, but to be honest, that is not going to make much difference in a codebase of 10,000+ lines.\n• The async/await version is still using promises, they are just hidden in the syntax.\n• The async/await version looks more like our synchronous version, without handling the asynchronous api call. Is this a win? Many blogs seem to think so. I personally don’t. \n\nAt a glance, it is easy to tell that there is something odd happening with the Promise version. The difference is subtle, but straight away, we know that something asynchronous is happening — By structure rather than syntax, we know will not have an immediate value. \n\nThe argument could, of course, be made that the literal inclusion of the word makes it obvious enough.\n• In a longer chain of asynchronous operations, what are these two approaches going to look like? \n\nIn my current day-job codebase, we have a few key controller functions with many asynchronous steps. \n\nImagine creating a new widget:\n\na) You need to validate the user (async)\n\nb) then You need to add the new widget to the database.\n\nc) then You need to update 3 gadgets that reference the new widget.\n\nd) then You need to notify 2 users that are using each gadget (6 in total) that a widget has been created.\n\ne) then You need to log that this has all happened successfully. \n\nIf any one of those steps fails, we want to log a different message and role back as much as possible.\n\nUsing async/await, we would probably put each call in a single block and handle the catch at the end. But what if we wanted separate catch blocks for some and not others? Imagine what that would look like! \n\nNow consider each step happens in a neat then() function, and separate failures can be handled by individual catch()’s, all chained together.\n\nWhat about being Functional?\n\nThe fact is that if you have a non-deterministic api call, you simply cannot write a pure function. However, in JavaScript, functional programming is always going to be a best-effort situation. The language is not designed to be truly functional.\n\nNo matter what approach you use, you can separate your function code from your non-functional code. Be functional where you can to maintain an understandable, maintainable codebase.\n\nWhen you think functional javascript, you probably also think function chaining. Promises allow us to chain asynchronous sections of code in a functional way. We can treat each like a function returned by the previous (even if it’s actually a method of a Promise object…).\n\nSee the similarity? Using await encourages you not to chain asynchronous requests, rather to use intermediate values.\n\nIt’s procedural. \n\n(Not a bad thing! But the topic of this post is being functional…)\n\nThis is useful in certain situations, such as using as an argument of . You end up storing the result in an intermediate variable outside of the promise chain anyway. This is a huge benefit of async/await — but it’s less functional in my opinion.\n\nOf course, because returns an implicit promise anyway, you could turn it into a promise chain:\n\nBest of both worlds, or unholy hackery? I’ll let you decide for yourself.\n\nThis might look like I have a preference for Promises, but to be honest, it all boils down to one thing: Use the right tool for the job!\n• If your code base already uses one approach exclusively, continue using that. It’ll save somebody a good deal of confusion.\n• If one approach is clearly easier to understand in a given context, use the more obvious solution!\n• If in doubt, ask a colleague. If you don’t have a colleague (lucky you) try both and see which feels better.\n\nIf you enjoyed this post, consider signing upto my personal mailing list for less than monthly updates.\n\nMany thanks to Mike van Rossum for proof reading this post and providing really valuable and insightful feedback."
    },
    {
        "link": "https://blog.bitsrc.io/asynchronous-programming-with-promises-and-async-await-in-javascript-bb78ca61f227",
        "document": "Asynchronous Programming with Promises and Async/Await in JavaScript\n\nIn this article, we will explore the concept of promises and how async/await simplifies promises in JavaScript. async/await approach can efficiently handle asynchronous operations such as fetching data from a server or lazy loading.\n\nTo begin with, it's important to understand that promises are objects. They can be constructed using an executor property. The structure of the executor should look like this:\n\nLet me explain the executor function more simply. An executor function is a function that takes two inputs and . The body of this function contains the asynchronous task that we want to perform, such as fetching data from a server.\n\nThe and inputs are functions that are used to handle the completion of the task. The function is called when the task is completed successfully, and the function is called when there's an error while executing the task.\n\nThe and are attributes of each promise. They are functions that help us to figure out whether our async task has been successfully done or failed, their inputs are parameters passed by or function.\n\nNow let’s see two examples:\n\nHere are the results of the execution of these examples:\n\nIt is crucial to note that in JavaScript, every promise is constructed and executed asynchronously. The success or failure of these promises is captured using the functions created for them. If we do not provide a catch callback, JavaScript will raise an error to notify us that the rejection was not handled properly. For example:\n\nThis is very important to catch our defined promises to prevent these errors in our apps.\n\nWhen you need to use certain parameters to execute something asynchronously, you can create a wrapper function that returns a promise. For example:\n\nNow let’s see another example:\n\nAssume that you want to do something whether the promise was fulfilled or rejected you don’t care about it. you can use . This function gets a callback as its parameter to execute, For example:\n\nI'll demonstrate how to handle a promise by catching it for monitoring purposes. The aim is to extract data from the error without actually catching it. For instance, if you want to keep track of your request status when it fails, you can use the following example (this isn't the best way. I want to show you how the promises work):\n\nAs I mentioned in the code, it is very important to use because if we don't use it, the promise won't be rejected in our business logic.\n\nWe can use the keyword to create a promise with the necessary parameters, without manually creating any promises. For example:\n\nThe above function is equivalent to the previous . You can use or use the keyword to resolve the result of this function.\n\nYou can simply use to handle errors coming from our promise.\n\nSome useful methods for Promises\n\nIn this section, I will share some useful Promise functions I have used in my previous work experiences.\n\nIf you are familiar with lazy loading, you know that sometimes you need to load certain libraries in browsers only when required. You don't want to load a huge library unnecessarily, especially if you don't end up using it at all. Therefore, you only want to load a library when it is needed. In this particular situation, you will need to load all of these libraries:\n\nWhen working with Webpack or other bundlers that support lazy loading, you can lazy load them like this:\n\nI am mocking modules to show you how it works. The input array is an array of promises. The returns a promise that resolves an array of results from the input array.\n\nYou are working on a project involving a bidding system that requires fetching products from all providers and displaying them to potential bidders. Your focus is on obtaining results, rather than ensuring that providers respond to your request or that the request is successful. You can use to reach this purpose:\n\nThe method returns an array of objects, each containing a and another key. If the status is \"fulfilled\", the other key is , holds the returned value and you can access the fulfilled data using it. However, if the status is \"rejected\", the other key is which is the reason why the promise was rejected.\n\nIf you have a microservice system that needs to fetch data from another resource, you may need to implement a time-out mechanism. This would allow your system to stop waiting for a response after a certain amount of time has elapsed, thus preventing it from being overloaded by the other system.\n\nWhen one of the input promises is fulfilled or rejected, the output promise follows suit.\n\nAssuming you have a system that displays content to its users, which is provided by multiple sources. Your system needs to retrieve this data from the providers as quickly as possible, without showing any preference toward a specific provider.\n\nWe collect data from the first fulfilled provider and display it to the user. The reason why the was ignored is that it was rejected. Replacing with may result in rejection."
    },
    {
        "link": "https://antondevtips.com/blog/mastering-async-await-in-javascript-for-asynchronous-programming",
        "document": "Asynchronous programming is a must-have in modern JavaScript development, allowing developers to perform non-blocking operations, such as fetching data from a server, reading files, or executing time-consuming operations. ES2017 introduced async functions and the keyword that are a complete game changer in asynchronous development. This blog post is a guide to using async/await to handle asynchronous tasks in an elegant way.\n\nstatements allow developers to write asynchronous code that looks and behaves a like synchronous code:\n\nHere an function returns a , that holds a data received from the API call. By using keyword we get this data as a promise result.\n\nAfter the line we can simply write more code as if all operations were executed synchronously.\n\noffer an elegant way for executing asynchronous operations represented by JavaScript Promises.\n\nTo learn more about promises read my blog post.\n\nkeyword is only allowed to be used in the functions.\n\nWith the introduction of ECMAScript 2022, JavaScript supports top-level statements in modules. This allows you to use outside of async functions within modules, simplifying the initialization of resources.\n\nOutside modules or in older versions of web browsers, you can use the following trick with anonymous async function to use in top-level statements:\n\nYou can encapsulate asynchronous logic within objects by defining async methods in JS classes. It is a good practise to add suffix when naming asynchronous functions.\n\nError handling when using is straightforward by using statement:\n\nWhen a method is called while awaiting a promise - an exception is thrown, that can be handled in the block:\n\nPromise class has few utility static methods for asynchronous programming:\n\nYou can use function to wait for multiple promises to resolve. This function takes an array of promises and returns a new promise that resolves when all of the promises have resolved, or rejects if any promise is rejected.\n\nThis method is particularly useful when you have multiple asynchronous operations that can be executed in parallel. Let's explore an example, where we fetch posts, comments and todos using :\n\nIt will be more efficient to fetch this data in parallel than fetching posts, comments and todos one by one.\n\nYou can use function to wait for one of multiple promises to resolve. This function takes an array of promises and returns a single promise that resolves when the first of the promises is resolved. If all the promises are rejected, then the returned promise is rejected with an AggregateError, an exception type that groups together individual errors.\n\nis similar to , but it completes as soon as one of the promises is either resolved or rejected. This method is useful for timeout patterns when you need to cancel request after a certain time.\n\nYou can use function to wait for all the promises to complete, regardless of whether they resolve or reject. It returns a promise that resolves after all the given promises have either resolved or rejected. This promise contains an array of objects where each describes the result of each promise.\n\nIn JavaScript, a thenable is an object or function that defines a method. This method behaves similarly to the method found in native promises. That way can handle these objects just like regular promises.\n\nThenables objects can be useful for integrating with systems that don't use native promises but have promise-like behavior. However, thenable objects can introduce confusion to the source code as their behavior is not straightforward.\n\nNative promises are preferable due to their comprehensive feature set and better integration with the JavaScript ecosystem.\n\nHope you find this newsletter useful. See you next time."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Learn_web_development/Extensions/Async_JS/Promises",
        "document": "Promises are the foundation of asynchronous programming in modern JavaScript. A promise is an object returned by an asynchronous function, which represents the current state of the operation. At the time the promise is returned to the caller, the operation often isn't finished, but the promise object provides methods to handle the eventual success or failure of the operation. A solid understanding of JavaScript fundamentals and asynchronous concepts, as covered in previous lessons in this module.\n• The concepts and fundamentals of using promises in JavaScript.\n• and : how they relate to promises, and why they are useful. In the previous article, we talked about the use of callbacks to implement asynchronous functions. With that design, you call the asynchronous function, passing in your callback function. The function returns immediately and calls your callback when the operation is finished. With a promise-based API, the asynchronous function starts the operation and returns a object. You can then attach handlers to this promise object, and these handlers will be executed when the operation has succeeded or failed.\n\nNote: In this article, we will explore promises by copying code samples from the page into your browser's JavaScript console. To set this up:\n• in that tab, open the JavaScript console in your browser's developer tools\n• when we show an example, copy it into the console. You will have to reload the page each time you enter a new example, or the console will complain that you have redeclared . In this example, we'll download the JSON file from https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/products.json, and log some information about it. To do this, we'll make an HTTP request to the server. In an HTTP request, we send a request message to a remote server, and it sends us back a response. In this case, we'll send a request to get a JSON file from the server. Remember in the last article, where we made HTTP requests using the API? Well, in this article, we'll use the API, which is the modern, promise-based replacement for . Copy this into your browser's JavaScript console:\n• calling the API, and assigning the return value to the variable\n• immediately after, logging the variable. This should output something like: , telling us that we have a object, and it has a whose value is . The state means that the fetch operation is still going on.\n• passing a handler function into the Promise's method. When (and if) the fetch operation succeeds, the promise will call our handler, passing in a object, which contains the server's response.\n• logging a message that we have started the request. The complete output should be something like: Note that is logged before we receive the response. Unlike a synchronous function, returns while the request is still going on, enabling our program to stay responsive. The response shows the (OK) status code, meaning that our request succeeded. This probably seems a lot like the example in the last article, where we added event handlers to the object. Instead of that, we're passing a handler into the method of the returned promise.\n\nWith the API, once you get a object, you need to call another function to get the response data. In this case, we want to get the response data as JSON, so we would call the method of the object. It turns out that is also asynchronous. So this is a case where we have to call two successive asynchronous functions. In this example, as before, we add a handler to the promise returned by . But this time, our handler calls , and then passes a new handler into the promise returned by . This should log \"baked beans\" (the name of the first product listed in \"products.json\"). But wait! Remember the last article, where we said that by calling a callback inside another callback, we got successively more nested levels of code? And we said that this \"callback hell\" made our code hard to understand? Isn't this just the same, only with calls? It is, of course. But the elegant feature of promises is that itself returns a promise, which will be completed with the result of the function passed to it. This means that we can (and certainly should) rewrite the above code like this: Instead of calling the second inside the handler for the first , we can return the promise returned by , and call the second on that return value. This is called promise chaining and means we can avoid ever-increasing levels of indentation when we need to make consecutive asynchronous function calls. Before we move on to the next step, there's one more piece to add. We need to check that the server accepted and was able to handle the request, before we try to read it. We'll do this by checking the status code in the response and throwing an error if it wasn't \"OK\":\n\nThis brings us to the last piece: how do we handle errors? The API can throw an error for many reasons (for example, because there was no network connectivity or the URL was malformed in some way) and we are throwing an error ourselves if the server returned an error. In the last article, we saw that error handling can get very difficult with nested callbacks, making us handle errors at every nesting level. To support error handling, objects provide a method. This is a lot like : you call it and pass in a handler function. However, while the handler passed to is called when the asynchronous operation succeeds, the handler passed to is called when the asynchronous operation fails. If you add to the end of a promise chain, then it will be called when any of the asynchronous function calls fail. So you can implement an operation as several consecutive asynchronous function calls, and have a single place to handle all errors. Try this version of our code. We've added an error handler using , and also modified the URL so the request will fail. const fetchPromise = fetch( \"bad-scheme://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/products.json\", ); fetchPromise .then((response) => { if (!response.ok) { throw new Error(`HTTP error: ${response.status}`); } return response.json(); }) .then((data) => { console.log(data[0].name); }) .catch((error) => { console.error(`Could not get products: ${error}`); }); Try running this version: you should see the error logged by our handler.\n\nPromises come with some quite specific terminology that it's worth getting clear about. First, a promise can be in one of three states:\n• pending: the promise has been created, and the asynchronous function it's associated with has not succeeded or failed yet. This is the state your promise is in when it's returned from a call to , and the request is still being made.\n• fulfilled: the asynchronous function has succeeded. When a promise is fulfilled, its handler is called.\n• rejected: the asynchronous function has failed. When a promise is rejected, its handler is called. Note that what \"succeeded\" or \"failed\" means here is up to the API in question. For example, rejects the returned promise if (among other reasons) a network error prevented the request being sent, but fulfills the promise if the server sent a response, even if the response was an error like 404 Not Found. Sometimes, we use the term settled to cover both fulfilled and rejected. A promise is resolved if it is settled, or if it has been \"locked in\" to follow the state of another promise. The article Let's talk about how to talk about promises gives a great explanation of the details of this terminology.\n\nThe promise chain is what you need when your operation consists of several asynchronous functions, and you need each one to complete before starting the next one. But there are other ways you might need to combine asynchronous function calls, and the API provides some helpers for them. Sometimes, you need all the promises to be fulfilled, but they don't depend on each other. In a case like that, it's much more efficient to start them all off together, then be notified when they have all fulfilled. The method is what you need here. It takes an array of promises and returns a single promise. The promise returned by is:\n• fulfilled when and if all the promises in the array are fulfilled. In this case, the handler is called with an array of all the responses, in the same order that the promises were passed into .\n• rejected when and if any of the promises in the array are rejected. In this case, the handler is called with the error thrown by the promise that rejected. const fetchPromise1 = fetch( \"https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/products.json\", ); const fetchPromise2 = fetch( \"https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/not-found\", ); const fetchPromise3 = fetch( \"https://mdn.github.io/learning-area/javascript/oojs/json/superheroes.json\", ); Promise.all([fetchPromise1, fetchPromise2, fetchPromise3]) .then((responses) => { for (const response of responses) { console.log(`${response.url}: ${response.status}`); } }) .catch((error) => { console.error(`Failed to fetch: ${error}`); }); Here, we're making three requests to three different URLs. If they all succeed, we will log the response status of each one. If any of them fail, then we're logging the failure. With the URLs we've provided, all the requests should be fulfilled, although for the second, the server will return (Not Found) instead of (OK) because the requested file does not exist. So the output should be: If we try the same code with a badly formed URL, like this: const fetchPromise1 = fetch( \"https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/products.json\", ); const fetchPromise2 = fetch( \"https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/not-found\", ); const fetchPromise3 = fetch( \"bad-scheme://mdn.github.io/learning-area/javascript/oojs/json/superheroes.json\", ); Promise.all([fetchPromise1, fetchPromise2, fetchPromise3]) .then((responses) => { for (const response of responses) { console.log(`${response.url}: ${response.status}`); } }) .catch((error) => { console.error(`Failed to fetch: ${error}`); }); Then we can expect the handler to run, and we should see something like: Sometimes, you might need any one of a set of promises to be fulfilled, and don't care which one. In that case, you want . This is like , except that it is fulfilled as soon as any of the array of promises is fulfilled, or rejected if all of them are rejected: Note that in this case we can't predict which fetch request will complete first. These are just two of the extra functions for combining multiple promises. To learn about the rest, see the reference documentation.\n\nThe keyword gives you a simpler way to work with asynchronous promise-based code. Adding at the start of a function makes it an async function: async function myFunction() { // This is an async function } Inside an async function, you can use the keyword before a call to a function that returns a promise. This makes the code wait at that point until the promise is settled, at which point the fulfilled value of the promise is treated as a return value, or the rejected value is thrown. This enables you to write code that uses asynchronous functions but looks like synchronous code. For example, we could use it to rewrite our fetch example: async function fetchProducts() { try { // after this line, our function will wait for the `fetch()` call to be settled // the `fetch()` call will either return a Response or throw an error const response = await fetch( \"https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/products.json\", ); if (!response.ok) { throw new Error(`HTTP error: ${response.status}`); } // after this line, our function will wait for the `response.json()` call to be settled // the `response.json()` call will either return the parsed JSON object or throw an error const data = await response.json(); console.log(data[0].name); } catch (error) { console.error(`Could not get products: ${error}`); } } fetchProducts(); Here, we are calling , and instead of getting a , our caller gets back a fully complete object, just as if were a synchronous function! We can even use a block for error handling, exactly as we would if the code were synchronous. Note though that async functions always return a promise, so you can't do something like: async function fetchProducts() { try { const response = await fetch( \"https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/products.json\", ); if (!response.ok) { throw new Error(`HTTP error: ${response.status}`); } const data = await response.json(); return data; } catch (error) { console.error(`Could not get products: ${error}`); } } const promise = fetchProducts(); console.log(promise[0].name); // \"promise\" is a Promise object, so this will not work Instead, you'd need to do something like: async function fetchProducts() { const response = await fetch( \"https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/products.json\", ); if (!response.ok) { throw new Error(`HTTP error: ${response.status}`); } const data = await response.json(); return data; } const promise = fetchProducts(); promise .then((data) => { console.log(data[0].name); }) .catch((error) => { console.error(`Could not get products: ${error}`); }); Here, we moved the back to the handler on the returned promise. This means our handler doesn't have to deal with the case where an error got caught inside the function, causing to be . Handle errors as the last step of your promise chain. Also, note that you can only use inside an function, unless your code is in a JavaScript module. That means you can't do this in a normal script: try { // using await outside an async function is only allowed in a module const response = await fetch( \"https://mdn.github.io/learning-area/javascript/apis/fetching-data/can-store/products.json\", ); if (!response.ok) { throw new Error(`HTTP error: ${response.status}`); } const data = await response.json(); console.log(data[0].name); } catch (error) { console.error(`Could not get products: ${error}`); throw error; } You'll probably use functions a lot where you might otherwise use promise chains, and they make working with promises much more intuitive. Keep in mind that just like a promise chain, forces asynchronous operations to be completed in series. This is necessary if the result of the next operation depends on the result of the last one, but if that's not the case then something like will be more performant."
    },
    {
        "link": "https://medium.com/cstech/asynchronous-programming-in-javascript-with-async-await-24b599a394cb",
        "document": "JavaScript is a scripting language primarily based on the concept of single-threaded execution.\n\nIf a long-running operation is performed, the browser or application cannot respond to the user until the operation is completed, resulting in a frozen or unresponsive appearance, which is not desirable in terms of user experience.\n\nFor example, let’s consider a scenario where you want to add a product to your shopping cart on an e-commerce website, and the addition to the cart is proceed through an API. Let’s assume that we send a request to the backend for each product added.\n\nEarlier, we mentioned that the code is processed step by step in a sequential manner, and we cannot move on to the next task until the current one is completed. So, does that mean we have to wait without doing anything until the request is completed? And what if it takes 30 seconds or more to receive a response from this process? Will we just sit and wait for those 30 seconds? This is where the need for asynchronous programming comes into play.\n\nWhy do we need asynchronous operations, and what benefits do they provide?\n\nAsynchronous operations, in its simplest definition, allow other operations to continue executing regardless of the progress or completion of a specific operation. This enables other tasks to run simultaneously while waiting for the result of a long-running operation, preventing the browser or application from freezing or becoming unresponsive.\n\nIn JavaScript, there are several ways to achieve this syntactically:\n\nIn asynchronous JavaScript functions, callback functions will be called when an operation is completed, instead of waiting for the completion of that operation. When an operation is asynchronous, the program flow is not completely blocked, and the secondary function continues its execution while the operation is being completed. Once the operation is finished, the specified callback function is executed.\n\nIn this example, we have a function called asyncFunction. This function waits for 2 seconds and then completes an operation, followed by calling the specified callback function. The setTimeout function delays the operation by 2 seconds and then executes the callback function when the operation is completed.\n\nThe callbackFunction is the callback function in this example, which simply prints some messages to the console.\n\nThe output of the code will be like,\n\nLet’s approach the same scenario using Promises.\n\nPromise objects are a JavaScript feature used to handle asynchronous operations more effectively. Promise objects provide us with two important keys which are resolve and reject.\n\nWhen the operation is successfully completed, resolve is called, while in case of errors, reject is called.\n\nIn addition to the mentioned information above, the then method is used to specify the callback function that will be executed when a Promise is successfully resolved. The catch method, on the other hand, is used to specify the callback function that will be executed when a Promise is rejected with an error.\n\nThe output of the code will be like,\n\n“In this case, the program flow continues immediately after the asyncFunction call, and the message “Program continues…” is printed to the console. After 2 seconds, the operation is completed, and resolve is called. Consequently, the callbackFunction specified with the then method is executed, printing the messages “Promise resolved successfully.” and “Operation finished, you can proceed.” to the console.\n\nHowever, the code appears to be a bit lengthy and complex, right? In JavaScript, there is always a cleaner way of writing code :)\n\nNow, we come to the main topic of our discussion, which is the Async & Await structure.\n\nIn any case, Async & Await helps us with performing asynchronous operations, just like callbacks and promises. In terms of syntax structure, they are not significantly different from the functions we are familiar with. MDN — Async Function\n\nAs we know with Promises, the success or failure of an operation is determined by the resolve and reject keywords, and we need to continue the process using .then() if the operation is successful or .catch() if it fails.\n\nAnd here comes the help of await.\n\nThe await keyword is used to pause the execution of a function until a Promise is resolved, and it can only be used inside an async function. MDN — Await\n\nIn this example, we used async/await to fetch user data from an API, parse the response as JSON, and handle any potential errors that may occur during the process.\n\nAdditionally, to handle any error that may occur in the Promise result, we wrapped the entire operation inside a try/catch block. We will delve into this topic in more detail later.\n\nLet’s examine why we prefer working with async/await instead of callbacks through a simple example.\n\nThey both accomplish the same task. Which one do you prefer?\n\nAnother benefit of using Async/Await is that it provides us with an easy and understandable structure for error handling. Additionally, with Try & Catch, we can handle errors in an asynchronous operation as if they were occurring in synchronous code.\n\nIf we consider our previous example in case of error inside the Catch block,\n• Through currying, we can repeatedly invoke a function until it executes,\n• We can redirect the user to a different page and perform many other operations that I haven’t mentioned.\n\nLet's see another Asynchronous usage for handling multi requests with Parallel and Sequential Execution\n\nPromise.all() is a built-in method in JavaScript. It allows us to run multiple Promises in parallel and returns the results of all the Promises in a single response. MDN — Promise.all()\n\nIn this example, requests are processed in parallel using Promise.all(), and the retrieved data is then sequentially processed using async/await.\n\nBy default, async/await executes asynchronous tasks sequentially, meaning the next task won’t start until the previous one is completed. This can be useful in scenarios where you need tasks to be executed in a specific order or dependent on the result of a previous task.\n\nFor a transaction, we will request the registered user to enter a One-Time Password (OTP).\n• Check if the user is a registered user in the system,\n• Check if the user’s email address is valid based on the result of the first step,\n• Send an OTP to the user’s email address based on the result of the second step,\n• Return the result of the third step.\n\nThen, if the process is successful, trigger the function that displays the OTP login screen.\n\nIt is indeed possible to use these two structures together, but I won’t go into the details as it would make the article too long.\n\nSince we have talked about the principles of synchronous and asynchronous operation, we can now take a look at how the process works in the Event Loop.\n\nFirst of all, let me clarify that the relationship of the Event Loop with Events in HTML is as much as the relationship between JavaScript and Java :) So there is no relationship.\n\nJavaScript has a runtime model based on the Event Loop, which is responsible for executing code, handling events, and progressing queued sub-tasks in the stack.\n\nThe Event Loop continuously operates by managing two main components: the Call Stack and the Event Queue.\n\nThe Call Stack keeps track of the functions being executed at any given time, while the Event Queue holds the pending tasks or events to be processed.\n\nComparison of an Event Loop with Synchronous and Asynchronous Code\n\nHere, I would like to provide you with two simple code examples to visualize the concept together and help us understand it better.\n\nTo sum up, JavaScript, with its single-threaded execution model, manages asynchronous operations through its Event Loop, providing us with the ability to work asynchronously.\n\nDon’t want to prolong it further as Event Loop is a topic that can be addressed separately, so I will conclude my article here.\n\nThank you for taking the time to read it! Your thoughts and comments are invaluable. Until we meet again, Arrivederci!\n\nHere is my favorite video about Event Loop on Youtube: https://youtu.be/MJeofIcEWLo\n\nYou can reach me through the channels below,"
    },
    {
        "link": "https://medium.com/@deathcap1/six-months-of-voxel-js-494be64dd1cc",
        "document": "Welcome to my first blog post, where I discuss my experience developing with voxel.js. For those unfamiliar, voxel.js is an open source platform for creating voxel-style games in your browser using WebGL and JavaScript, founded by @maxogden and @substack in December 2012.\n\nCheck out maxogden’s one-month status report Bringing Minecraft-style games to the Open Web (January 2013) and his presentation at NodePDX (May 17th, 2013) for a detailed introduction to voxel.js.\n\nLater in 2013, voxel.js progress sort of slowed down; the end of 2013 and early 2014 is where I got involved. I am a big fan of Minecraft by Mojang Specifications and especially the extensive “modding” community focused on enhancing it to create a richer and more advanced game, such as in the Feed The Beast packs.\n\nThe prospect of a completely open source and highly modular system for creating voxel-style games like modded Minecraft, right in your browser, was very appealing to me so I signed up for GitHub and began playing around with voxel.js:\n\nThis article is written based on summarizing my commits over this five or six month period. More details if you are interested are available in the actual commits themselves, all publicly available on GitHub.\n\nDay 1. I began on November 9th, creating a repository named ‘voxpopuli’ (internal codename; subject to change UPDATE 2015/02/04: renamed project to ‘voxelmetaverse’) as a testbed for development (live demo). Just for fun I decided to use CoffeeScript, a nice and clean language that compiles down to JavaScript, to replicate the voxel-hello-world demo. Here’s how it first looked with voxel-engine 0.18.4:\n\nNot much to look at it, but it was a great feeling to see this blank canvas unfurl beneath me, the prospect of countless enhancements on the horizon. It was the start of something, for sure. I initially focused on small miscellaneous tweaks and control improvements (right-click to place blocks, continuous firing, hide avatar in first-person mode, hold tab to sprint, double-tap to exit fly mode, etc.), slowly shaping it up into how I envisioned a voxel game might be.\n\nArtwork. Soon I ran into a problem: what graphics should I use for textures?\n\nThere is a painterly-textures module on NPM, used in a few examples, but it hasn’t been updated in a while and I couldn’t find clear licensing terms on the official Painterly Pack website. isabella-texture-pack looks promising, licensed Creative Commons 3.0 Attribution, but it is also outdated (although Isabella II: FTB Edition is actively updated, couldn’t find licensing information either). If anyone knows of an actively developed permissively-licensed set of original textures for voxel-style games, I’d be interested — I also asked on the Minecraft Forums — but I couldn’t find any at the time so I decided to make my own, essentially as a placeholder which could be freely redistributed:\n\nThat’s how it looks when ProgrammerArt, as I call it, is loaded into Minecraft 1.7. It took a while to even haphazardly draw placeholders for everything, but textures for all items and blocks are included. The pack is distributed in several formats: as a Resource Pack (compatible with MC 1.7, 1.6), Texture Pack (MC 1.5), and a pre-built “Stitch Pack” (MC 1.4 and earlier):\n\nThe latter format can be read directly into a GL texture atlas, but has the disadvantage of limiting flexibility when combining multiple textures from different sources or of higher resolutions. Zipped archives of individual textures have a clear advantage, so I wrote artpacks to load this format (compatible with MC’s resource packs). The loader cascades, like CSS, allowing multiple packs to be loaded and organized with user-defined priority:\n\nThis allows the player to easily customize the appearance of the game to their liking, improving from the default included ProgrammerArt pack. Even high-res packs can be loaded, shown here are Faithful32 and Sphax PureBDCraft 64x64, installed simply by downloading and dragging the .zip into the browser.\n\nMeshing. With textures out of the way, another problem became apparent: severe lag spikes when breaking or placing blocks. After some investigation, finally isolated it to the chunk remeshing algorithm, which is used to generate the vertices and faces given voxel data to send to WebGL for rendering.\n\nThe voxel module provides a handful of meshers, most notably ‘culled’ and ‘greedy’. The former is an improvement of the simplest possible algorithm, culling the hidden faces, but still providing only one face per voxel; the latter is a bigger improvement, meshing together adjacent voxels into one face therefore significantly reducing the number of vertices to upload. Changing the mesher from culled to greedy fixed the block lag, but introduced a visual problem:\n\nThe effect may be appear subtle in these screenshots, so here’s another block configuration which makes it more obvious:\n\nThe textures are stretched! Instead of the texture (grass side, in this case) repeating for each voxel, there is one texture across the entire greedily-meshed face.\n\nAs a workaround, I tried reverting to an older version of shama’s voxel-texture before it began to use texture atlases and set UV texture coordinates, but then texture upload bandwidth became the new bottleneck. Turns out there is a solution giving the best of both worlds, @mikolalysenko eloquently explained on his seminal 0fps blog posts, Texture atlases, wrapping and mip mapping (highly recommend reading this post and the others on his blog, as they are extremely informative).\n\nUnfortunately the greedy meshing + texture atlases demonstration was not compatible with the three.js library used by voxel-engine, so I forked voxel-texture into voxel-texture-shader to port it. Using a custom three.js ShaderMaterial to set the proper texture coordinates from the fragment position within the voxel and optional four-tap sampling to fix the bleeding seams. voxel-texture-shader also supports artpacks texture loading as described above. Though the three.js integration is a bit of hack, it solved the problem at the time (revisited it in the future with the ndarray/gl-now based voxel-shader and voxel-mesher derived from mikolalysenko’s ao-shader and ao-mesher, but now we’re getting ahead of ourselves).\n\nMining. Now with rendering down (at least for now), the lack of gameplay elements in the demo became more noticeable. Blocks could be “mined” (broken) instantly, just by left-clicking once, for example. So I added a finite mining time and block break overlay:\n\nHolding down the left mouse button increases the block break progress over time:\n\nuntil it eventually ‘breaks’. The voxel-mine plugin shown here relies on a new module, voxel-reach, to listen for mining events. Later it became possible to use different tools to mine certain blocks faster, and to configure the relative hardness per block type.\n\nPlugins. With the infinite possibilities in a voxel-style game, how to best manage extensibility? The Node Packaged Modules (NPM) system voxel.js is built on (via browserify) encourages small reusable modules, a breath of fresh air in comparison to the monolithic design of other platforms, but NPM itself does not provide a “plugin” system, per se. Modules can specify “peer dependencies”, but it does not provide the full functionality I was looking for in a plugin system (and there is talk about removing it). Hence, voxel-plugins, a simple modular system for loading, enabling, and disabling addons to voxel.js.\n\nvoxel-plugins builds on the existing voxel.js convention of exporting a factory constructor which takes (game, opts) arguments, where game is a global game object (voxel-engine instance), and opts is a hash of options (think of it as the plugin configuration file). Modules already supporting this convention can be loaded without changes, and others can be loaded with only minor modification. Furthermore, plugins can optionally implement an API to support runtime enable/disable support, allowing the player to toggle the plugins on or off:\n\nNot all plugins would make sense to support enabling/disabling, but many do. For example, disabling the fly plugin completely removes the event handlers making flight possible. With some refactoring, I was able to get voxpopuli down to consist entirely of plugins (~50 at this time).\n\nTerrain. With modularity nailed down, time to get started on writing plugins. voxel-mine etc. were soon refactored into plugins, before tackling chunk generation.\n\nIn voxel.js, the world is segmented into 32x32x32 cubic chunks, by default. Interestingly, Minecraft also uses chunking, but in the shape of a rectangular prism instead: 16x256x16 (or 16x128x16 in earlier versions), that is, vertical columns with limited fixed height. The X and Z dimensions in MC are practically infinite (modulo the Far Lands, due to precision limits — voxel.js has a similar issue), but Y cannot normally exceed 256 — though there is an unofficial Cubic Chunks modification to change this. With voxel.js, chunks are already cubic. This may have novel gameplay implications later on.\n\nAs for the terrain itself, to move beyond a more interesting landscape than a valley, some kind of continuous noise can be used. maxogden’s voxel-perlin-terrain and voxel-simplex-terrain served as good starting points, showing Perlin and Simplex noise, respectively (very useful algorithms both invented by Ken Perlin in 2001 and earlier). substack’s voxel-trees added some vegetation, and rachel-carvalho’s voxel-mars (which lets you explore a the topology of another planet using data captured by NASA in 1996 using voxel.js, check it out at voxelmars.com) used web workers to efficiently generate the chunks. Combining several of these techniques, we now have:\n\nThe surface is all grass, below that is dirt, and further below solid stone, occasionally interspersed with coal ore and iron ore. By this point, we are using quite a few blocks, leading to…\n\nRegistry. To keep track of all the block types in the game, you can use voxel-registry. Registering a block allocates a 16-bit numerical voxel index and sets optional properties, which can be later looked up by name, during terrain generation or otherwise. No fancy screenshots here, but this added level of indirection is a huge benefit in avoiding hardcoded static “magic numbers”, providing more flexibility when adding new blocks.\n\nThis same plugin also now handles items, which have no numerical index (not needed), and metablocks — blocks with additional state (rotation or so on, analogous to MC’s metadata, except it is not limited to 4-bits (at the time of this writing)), which will become important later.\n\nInventory. Now that players mine blocks and plugin developers can register their own blocks, having a means to store said blocks is only logical. The itempile module represents items which can group together (pile/stack), and inventory holds a fixed number of slots for these item piles. These modules implement the low-level merging/splitting logic and have no UI, but can be interacted with through an inventory-window:\n\nYou can click to pick up and drop the items in an inventory window as you would expect (well, not in the above static screenshot, but you can in the interactive demo). An inventory window is used for the player’s hotbar:\n\nwhich shows a subset of the items the player is currently carrying, including their currently “held” item; in this screenshot, an iron pickaxe about to mine coal ore. Pressing ‘E’ (can be rebound) will open your full inventory:\n\nBesides letting you rearrange your inventory, this dialog lets you do something else:\n\nCrafting. The built-in inventory screen pictured above provides a 2x2 crafting grid, backed by the low-level craftingrecipes module. Recipe types supported include amorphous (given ingredients in any order) and positional (ingredients in a specified shape). A larger, 3x3 grid can be accessed by building a workbench, and then right-clicking to open the interface:\n\nThere are a couple things worth noting going on here. voxel-workbench registered a workbench block using voxel-registry, and added a crafting recipe for it (not shown: 4 planks, amorphous) using voxel-recipes. Right-clicking the block triggers voxel-reach, and then voxel-use which recognizes you are attempting to use the block placed in the world (the workbench), so it opens the registered onUse handler. [Note that right-clicking is also used to place blocks from your inventory; but GUI opening overrides this behavior, unless the player is crouching (by holding shift — in case you want to place a block in the world against a workbench, for example, instead of opening the workbench GUI itself).] The recipe pictured above was added by voxel-pickaxe, which in turn registers pickaxe items with the appropriate registry properties to provide a mining speedup for voxel-mine, and when you mine with it the blocks make it to items in your inventory (backed by voxel-carry) using voxel-harvest.\n\nWhew, that’s a lot of plugins! To better support seamlessly integrating numerous plugins, craftingrecipes also supports a crafting thesaurus, where items can be registered as equivalent for crafting purposes. For example, recipes with wooden logs can use oak or birch logs equivalently, not unsimilar to the functionality provided by the Forge ore dictionary.\n\nChests. Need more space to store your items? A chest is for exactly that:\n\nCrafted with planks and placed in the world, right-clicking the chest opens up an interface showing both your player inventory (here, it is empty) and the chest contents, 10 by 3. As a handy convenience, shift-clicking the items will quickly transfer them between the chest and player inventory (using the inventory-window concept of ‘linked’ inventories).\n\nHow are the items stored within the chest, you might be wondering? The <16-bit address space of voxel-registry metablocks is not nearly enough, so instead the data is stored out-of-band, using voxel-blockdata. Using this plugin, arbitrary data is stored within the voxel chunk format, keyed by the block location. This blockdata scheme can be compared to MC’s concept of a block entity.\n\nFurnaces. Another GUI-enabled block, allows you to smelt ores:\n\nLike the workbench, furnaces also use voxel-blockdata and voxel-recipes.\n\nClientMC. Now for something a bit different. What if you could use voxel.js to connect to an actual Minecraft server?\n\nvoxel-clientmc is the start of such a plugin, using node-minecraft-protocol to parse the MC protocol in the browser, and wsmc to proxy the MC server to and from a WebSocket where your browser can reach it. The above screenshot is from voxel.js connecting to a real MC 1.7 server.\n\nImplementing a full MC client is a lot of work, and is somewhat of a moving target; voxel-clientmc is therefore very incomplete, more of a proof-of-concept than anything. See also, the more complete projects: mineflayer (client API using node-minecraft-protocol; mineflayer-voxel), and ThinkMap (3D map viewer using WebGL and haxe). voxel-clientmc may not reach a usable level of completeness, but it was a fun experiment at least.\n\nConsole. With the possibility of remote server connectivity, a chat/command console could be useful:\n\nPlugins can register their own commands, and voxel-commands provides a few handy commands itself, for: teleporting to specific coordinates or to the player home position, cheating in items into the player inventory, setting/getting blocks (including with optional voxel-blockdata tags), and listing or enabling/disabling plugins (CLI alternative voxel-plugins-ui GUI).\n\nVoila. Continuing the trend of providing more debugging/testing/informative functionality, and in addition to the possible difficulty of recognizing my quick-and-dirty ProgrammerArt textures, wouldn’t it be nice if there was a label showing the name of the currently-highlighted block?\n\nUsing voxel-registry to supply the display names, voxel-voilà provides this much-needed feature. Note: the name is phonentically similar to the popular Waila (What Am I Looking At) modification for MC.\n\nIn case this and other UI elements become too distracting, you can toggle them off (default F1 key) using voxel-zen (the name inspired by GitHub’s Zen Writing Mode, which similarly removes distractions):"
    },
    {
        "link": "https://reddit.com/r/VoxelGameDev/comments/ox965e/how_can_a_newbie_coder_build_there_own_voxel_game",
        "document": "Hi, I am a relatively rookie programmer when it comes to things like creating a voxel game/engine. I am learning how to use THREE.js and the basics of shaders (fragment/vertex), however I have a very limited knowledge of things like that as well as vertex buffer objects and the sort. THREE.js is easy enough to wrap my head around right now seeing as it sits a little higher up then something like a graphics API such as OpenGL. The problem for me has been following tutorials on building a voxel game engine because the ones I have seen all require knowledge of OpenGL or other lower level programming that I just don't really understand. I was wondering if there are any resources available for someone with limited 3d graphics programming to start to build a voxel engine in something like THREE.js? I do understand the basics such as geometry, material, meshes, rendering loop. Its just when a lot of stuff with OpenGL/ other API's and usually code written in C++ as well as shaders comes in to play, I find this very confusing. Any help is much appreciated, thanks!"
    },
    {
        "link": "https://reddit.com/r/gamedev/comments/7mq4jf/creating_my_own_voxel_engine_in_javascript",
        "document": "Hello everybody! Long time lurker first time poster, so be gentle.\n\nA while ago I decided to build my own game from scratch. I wanted it to be a isometric turn based strategy, and I really wanted destructible environments. After some thinking and testing I decided to go with a voxel based system, and use JavaScript to show off my mediocre web development skills. I first decided to create my own 3d engine, but that is a lot of math and work. So I went with Three.js.\n\nBasically this post has 2 purposes; First, I would like to share what I have developed so far in case someone else would like to learn from what I've done (its no where near the best). Second, I would like your opinion on what I have done so far. I'm not looking for play testers or anything, mostly because there isn't anything to \"play\", just what do you think or if you have a suggestion.\n\nFirst is my level editor: chrispcr.com/map_editor/\n\nIts a little slow to get started (something I need to work on. It creates and renders all the chunks before starting the game loop), it can take a lot of ram too :(\n\nBasically I needed a level editor to build the levels for testing path finding, gravity, etc... At first the chunks were png files, but editing those pixel per pixel is just annoying. Then I thought, its a bunch of blocks, I like building things on Minecraft, I'll make it like Minecraft. A problem that I have run into is the frame rate drops when too many faces are on the screen (web browsers aren't the best platform for games, who knew), and slows further when you start to add and remove blocks. I've optimized it as best as I could, but I'm out of ideas. Recently a update to windows seems to have broken the pointer lock controls in Chrome. So now the pointer will arbitrarily jump back to some random point, and its so annoying. I started to build a house with a fence and driveway, but the slow downs and the pointer jumping when I'm trying to move started to get really annoying.\n\nI developed using Chrome, but I've tested it in Edge and Firefox and it seems to work ok, right clicking only seems to work in Chrome. Internet explore 11 gives the error that pointer lock is not supported.\n\nSecond, if your wondering what I hope the game to look like I hastily setup this: chrispcr.com/voxel/ a few months ago to show friends my progress. This version still uses the pngs to hold the chunk data, if you are interested to see how I did that. The code is a bit ugly (again it was hastily setup).\n\nIf you have any questions feel free to ask, I'll answer to the best of my ability."
    },
    {
        "link": "https://mdpi.com/2220-9964/13/12/461",
        "document": "2,3,6, Path planning plays a critical role in optimizing travel efficiency, conserving resources, and enhancing safety, and is widely applied in various fields, such as personal travel, logistics, and emergency response. Early path-planning algorithms primarily focused on finding the shortest path between a starting point and an endpoint in static and structured environments. These algorithms are typically optimized for factors such as distance or time, using simple geometric or topological computations to quickly generate a feasible path. However, with the continuous advancement of computer technology and data-processing capabilities, path planning has evolved to address dynamic and complex real-world scenarios. In autonomous driving systems, path planning now considers not only traditional metrics like distance and time but also integrates multiple factors such as traffic conditions, fuel consumption, and environmental impact. This enables improved travel efficiency, reduced resource consumption, and the promotion of sustainable development [ 1 4 ]. In pedestrian navigation, the focus has expanded to include urban environmental factors, such as aesthetics, natural elements, and the built environment [ 5 7 ]. Incorporating these elements into navigation design not only enhances the enjoyment and comfort of walking but also effectively promotes healthier and more user-friendly travel options, supporting the creation of livable urban environments [ 8 ]. 10, Environmental models are crucial in path planning, as they provide the necessary spatial information and environmental data, directly affecting the accuracy, efficiency, and safety of path planning. However, constructing high-precision environmental models poses numerous challenges, including accurately representing complex 3D spaces, dynamically updating environmental information, and managing and processing large-scale data with significant resource demands. One promising approach is the use of voxels, as illustrated in Figure 1 . Voxel, short for “Volume Pixel”, can be seen as the 3D counterpart of a pixel in a 2D image. As a fundamental unit in 3D space, voxels are defined on a regular 3D grid and are used to represent spatial positions and their associated properties. Typically, a voxel is represented by its center point or a corner point rather than being directly stored as a geometric cube. Each voxel is associated with one or more values that describe measurable attributes or independent variables of real-world objects or phenomena, such as density, color, or other characteristics [ 9 11 ]. Table 1 compares several 3D-representation methods for navigation. As shown in the table, compared to other methods, voxel models not only support complex 3D path-planning algorithms but also allow for flexible resolution adjustments and easy dynamic information updates, adapting to various application scenarios. Moreover, their structured data format simplifies model processing, further enhancing their potential and value in navigation applications. Currently, there is still a lack of comprehensive summaries and discussions on voxel-based navigation, as well as on their advantages and disadvantages in practical applications. Therefore, this paper provides a systematic review of the existing applications of voxel-based modeling in research related to navigation and path planning, focusing on the characteristics, strengths, and weaknesses of voxel-based path planning. The contributions of this work are as follows:\n• None We provide a detailed discussion of the history and characteristics of voxel-based navigation. To the best of our knowledge, this is the first review focusing on voxel-based navigation.\n• None We review the key technologies required for voxel-based navigation, considering four critical aspects: voxel modeling, voxel segmentation, voxel analysis, and voxel management.\n• None We investigate various applications of voxel-based navigation and discuss their advantages and disadvantages in these applications.\n• None Based on the above, we analyze the potential and limitations of voxel-based navigation. The remainder of this paper is organized as follows: In Section 2 , we describe the methods used in this literature review. In Section 3 , we discuss the advantages of voxel-based representation. In Section 4 , we outline the key technologies required for voxel-based navigation. In Section 5 , we review existing studies on voxel-based path planning. Section 6 discusses the potential applications of voxel-based navigation. After that, Section 7 delves into the limitations of voxel-based navigation and unresolved issues. Concluding insights and future research directions are summarized in Section 8\n\nTo identify the literature relevant to this review, we employed a widely used review methodology following previous studies [ 21 22 ]. First, we selected a set of keywords and data sources to gather papers potentially related to our review. Next, we refined the selection based on specific eligibility criteria and expanded the literature pool through citation tracking. Finally, we classified the papers in the final collection. Figure 2 illustrates our literature screening flowchart, presenting the specific steps of the screening process: This review selected three data sources: Web of Science ( www.isiknowledge.com , accessed on 25 June 2024) Scopus ( www.scopus.com , accessed on 25 June 2024), and Google Scholar ( www.scholar.google.com , accessed on 25 June 2024). The initial search was conducted using a core keyword (“voxel”) to capture a wide range of the literature related to the research topic. We identified common themes and high-frequency terms by analyzing the search results, which were then used to optimize the keyword combinations. The optimized keywords mainly focused on the following research directions: “voxel AND 3D mapping”, “voxel AND pathfinding/navigation”, and “voxel AND GIS AND routing”. This process effectively helped us focus on the latest research findings related to voxel technology and its applications, providing more precise references for future research. Using the abovementioned keywords and data sources, we initially obtained 157 relevant papers. These papers spanned multiple disciplines, including computer vision, computing, engineering, and geology, demonstrating the wide range of applications and developments of voxel technology in various fields. We established a set of criteria for selecting the literature: (1) written in English; (2) published in 2010 or later; (3) related to navigation or addressing navigation issues; (4) using voxel models or voxel-based approaches. Based on these criteria, we initially selected 48 papers from the 157 retrieved documents, and through citation analysis we ultimately included 87 papers for the final review. The selection of 2010 as the starting point was made to focus on more contemporary research, reflecting the rapid progress in voxel-based techniques, computational methods, and their integration with emerging technologies like autonomous systems, which have gained considerable attention in recent years. From the final pool of 83 papers, these publications were published between 2010 and 2024, including 32 conference papers and 51 journal papers. The top three sources of these publications are the following journals: ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences (7), International Journal of Applied Earth Observation and Geoinformation (6), and Automation in Construction (4). Figure 3 a shows the number of relevant articles published each year. The number of articles from 2010 to 2015 was relatively low and exhibited minimal fluctuation. From 2016 to 2018, there was a significant increase in the number of articles, and from 2019 to 2024, the overall number remained at a high level, indicating a continued growth trend. 24, By referring to previous studies [ 23 25 ], we used VOSviewer for co-occurrence analysis of keywords, clustering, and visualizing 79 keywords that appeared at least three times, as shown in Figure 4 . The size of each node represented the frequency of keyword occurrence, and the lines connecting two nodes indicated whether they co-occurred, with the line width reflecting the frequency of co-occurrence. These keywords were divided into 5 clusters, based on their association strength. Cluster 1 (colored in red) primarily involved topics related to drone navigation and sensor-based path planning. Core keywords included map, environment, sensor, path, UAV, planning, and obstacle avoidance. This cluster focused on drone path planning, sensor data fusion, and obstacle detection, with an emphasis on dynamic obstacle avoidance and path optimization in drone navigation. Cluster 2 (colored in blue) primarily involved topics related to 3D reconstruction, voxel modeling, and path navigation. Core keywords included voxel, navigation, point cloud, structure, and ground. This cluster emphasized 3D environmental reconstruction and voxel modeling, using voxelization of point cloud data to achieve spatial structure extraction and environmental representation while supporting robot path navigation in 3D environments. Cluster 3 (colored in green) primarily involved topics related to indoor real-time navigation and performance optimization. Core keywords include object, scene, accuracy, indoor environment, position, performance, and real time. This cluster emphasized precise object and scene recognition in 3D spatial modeling and navigation systems, particularly achieving high-accuracy object localization and navigation performance evaluation in indoor environments to enhance the navigation performance of robots or autonomous systems. Cluster 4 (colored in yellow) primarily involved topics related to algorithm development and optimization methods. Core keywords included algorithm, approach, time, accuracy, and density. This cluster emphasized algorithm design and method optimization in 3D modeling and path planning, focusing on time efficiency, accuracy improvement, and data density handling in algorithms, exploring efficient methods to improve the performance of environmental reconstruction, path optimization, and navigation tasks. Cluster 5 (colored in purple) primarily involved topics related to SLAM (Simultaneous Localization and Mapping) and real-time environmental mapping. Core keywords included slam, robot, real-time, and mapping. This cluster highlighted the application of SLAM technology in robot navigation, focusing on real-time environmental mapping and localization estimation, aiming to achieve efficient spatial mapping and localization synchronization, and optimizing robot performance in dynamic environments. Based on the above analysis, we first selected papers related to voxel navigation technologies. Further, we categorized them into voxel-based modeling, voxel segmentation, voxel-based analysis, and voxel data storage and management, with the number and proportion of papers shown in Figure 3 b. In the second step, we selected papers related to the application of voxel navigation and categorized them based on the scenarios into indoor navigation and outdoor navigation. In the following sections, we will delve deeper into the examination of these research papers.\n\nBased on a review of the literature, we found that voxel-based representation, a flexible and precise method for three-dimensional space representation, has been successfully applied in various fields, such as fire modeling [ 26 ], indoor navigation [ 27 ], and manufacturing simulation [ 28 ]. The following discusses the advantages of voxel-based representation: Firstly, voxel models have significant advantages in storing and managing three-dimensional geographic information, supporting more comprehensive 3D spatial analysis. Compared to traditional 2D GIS data, voxel models not only represent terrain changes accurately but also convert urban elements traditionally modeled as 2D surfaces (such as buildings and roads) into 3D voxel structures. This transformation not only reveals urban features and spatial connections that are difficult to detect with 2D data but also significantly benefits navigation-related activities. Voxel models provide finer-grained information for path planning and obstacle detection, enhancing the decision-making capability of navigation systems in complex environments, helping researchers and urban planners better understand and analyze complex urban spaces [ 29 30 ]. Secondly, voxel models excel at handling spatial topology. Each voxel has a well-defined topological relationship with its surrounding voxels, which provides reliable data support for path planning, spatial connectivity analysis, and material propagation studies. In navigation applications, voxel representation can effectively describe the 3D shape of obstacles, allowing path-planning algorithms to avoid obstacles and optimize travel paths more accurately. Moreover, voxel models also have strong adaptability, enabling real-time environmental updates and path adjustments, enhancing navigation systems’ robustness in complex and dynamic environments [ 26 27 ]. Finally, voxel models’ flexibility makes them particularly well-suited to navigation tasks that require dynamic updates and rapid responses. By simplifying the representation of complex 3D environments, voxel models can quickly adapt to constantly changing environments, particularly in systems like autonomous driving and drone navigation, where real-time perception and decision making are essential. Voxel technology significantly improves the efficiency and accuracy of path planning and obstacle avoidance. These advantages highlight the important role of voxel integration in navigation-related activities, improving the precision, safety, and real-time responsiveness of path planning. With the continued advancement of computational power and big data technologies, voxel technology is expected to provide more efficient and precise solutions for future intelligent navigation systems.\n\nBased on our investigation into voxel-based navigation, we have identified four key aspects—modeling, segmentation, analysis, and data storage—that are closely linked to the efficient operation of the navigation system. As shown in Figure 5 , voxel-based modeling serves as the foundation of the navigation system. Constructing voxel maps in three-dimensional space effectively represents the geometric information of the environment and provides a spatial framework for subsequent path-planning and navigation algorithms. Voxel segmentation is used to identify different types of spatial areas (such as corridors, obstacles, or passable areas), providing the navigation system with precise data on accessibility and the location of obstacles. The voxel-based analysis involves further processing of voxel data to extract richer spatial information, including connectivity analysis, visibility analysis, spatial analysis, etc., which provides decision-making support for navigation algorithms. Voxel data storage and management is crucial for ensuring the efficient operation of the navigation system, relying on efficient storage structures and update mechanisms to ensure stable operation under different operating conditions, especially in dynamic environments. We briefly overview the applications and related literature on voxel-based modeling, voxel segmentation, voxel-based analysis, and voxel storage and management (see Table 2 ). In the following sections, we discussed these four relevant issues in detail. Voxel modeling is a crucial step in acquiring environmental information. Various data sources, such as Building Information Modeling (BIM), LiDAR, Computer-Aided Design (CAD), and CityGML, can be used to construct voxel models. Researchers have developed different voxelization algorithms and techniques to adapt to different application scenarios. In indoor scene modeling, the primary data sources include BIM and LiDAR. BIM models usually contain highly detailed geometric information, building materials, system components, etc., but these complex details can impose a computational burden during path planning. Converting BIM models to voxel models can simplify data processing, retaining only the necessary geometric and spatial information, thereby optimizing computational efficiency. Ref. [ 33 ] proposed a method to discretize BIM models into voxel sets for path planning, significantly reducing the time required for navigable analysis. Ref. [ 34 ] proposed a method to automatically extract structures and boundaries from complex building models and convert them into voxel models based on geometric and semantic relationships. This method overcomes the complexity of extracting multi-story indoor spaces, but its accuracy and reliability depend on the quality of the input 3D building models. For buildings lacking CAD floor plans or BIM models, such as historical buildings or newly constructed buildings, LiDAR data become an important data source. Ref. [ 80 ] utilized LiDAR data to build voxel models, particularly suitable for 3D point cloud analysis in indoor environments. Ref. [ 27 ] presented a 3D indoor point cloud processing method based on octrees for path planning in multistory buildings. The method improves the efficiency of point cloud data processing by applying octree segmentation. Through semantic recognition of architectural elements (such as floors, walls, stairs, and obstacles), it accurately delineates navigable space and computes paths based on user constraints. However, this method relies on the Manhattan world assumption for model construction, which limits its generalizability. Ref. [ 81 ] proposed a voxel-modeling method for indoor spaces combining point clouds and scanning trajectories, which is capable of identifying walkable spaces in various rooms without any constraints. Unlike indoor environments, outdoor scene modeling involves larger spatial areas and more diverse data sources. Ref. [ 30 ] provided a set of algorithms that can generate voxels from point clouds, curves, or surface objects, obtaining voxel connectivity based on topological voxelization methods [ 82 ], which helps in voxelized modeling of 3D city models. An example of voxelization of a surface is shown in Figure 6 . Ref. [ 38 ] proposed a voxelization technique that combines BIM and GIS, achieving three-dimensional modeling and management of geological information, meeting the needs of different scales. Ref. [ 40 ] proposed a voxel modeling and visualization framework for large-scale urban environments, effectively reducing memory consumption and supporting large urban models’ generation, storage, and real-time rendering. The regular cubic grid structure of voxel models exhibits extremely high efficiency in operations such as spatial queries and neighborhood searches, making them particularly suitable for applications in complex indoor navigation and robotic path planning. Moreover, voxel models support three-dimensional topological analysis, clearly illustrating the adjacency and connectivity of different parts of the space, making them highly applicable in path-planning scenarios. Semantic segmentation is crucial in voxel-based modeling for navigation tasks. While geometric segmentation divides 3D data, such as point clouds, meshes, or surfaces, into smaller, meaningful regions, semantic segmentation assigns each voxel a semantic label (e.g., “car”, “road”, “pedestrian”). This classification is essential for identifying navigable spaces and obstacles, which greatly enhances environmental understanding. In addition, semantic segmentation can also identify dynamic obstacles, further enhancing the system’s understanding of the environment and enabling real-time updates. This allows the navigation system to adapt to dynamic environment changes, ensuring more robust and reliable navigation in constantly changing scenarios. As a result, semantic segmentation improves the efficiency, safety, and adaptability of voxel-based navigation systems, supporting their ability to navigate complex and changing environments. Traditional voxel-based semantic-segmentation methods mainly rely on handcrafted and extracted features (such as geometric features, color information, texture, etc.) and are then used in machine-learning algorithms (such as Conditional Random Fields (CRF) [ 41 ]) for semantic segmentation. Ref. [ 42 ] proposed a supervoxel-based 3D urban scene-segmentation-and-classification method, where supervoxels are first clustered using a chained-linking method and then classified based on surface normals, geometric centroids, color intensity, and geometric shape. Ref. [ 27 ] performed semantic segmentation of indoor spaces by combining histogram analysis and region-growing methods based on an octree indoor space model, ultimately identifying walkable paths for pedestrians. This is shown in Figure 7 , which depicts the classification of different indoor components that are critical for determining navigable pathways. Traditional methods have strong interpretability but rely on expert knowledge for feature selection, making it challenging to handle high-dimensional features in complex and large-scale environments. With the rise of deep learning, more research is leveraging deep-learning techniques for voxel labeling. Through multi-layer structures and convolution operations, neural networks can automatically learn features from high-dimensional data and capture complex relationships. Ref. [ 43 ] introduced VoxNet, a framework based on Convolutional Neural Networks (CNNs) for real-time object recognition, converting 3D point clouds into voxel grids and using 3D CNNs for feature extraction. Ref. [ 44 ] proposed OctNet, a 3D CNN architecture utilizing an octree structure for sparse 3D data, improving computational efficiency and storage. Ref. [ 45 ] developed SpoxelNet, which transforms voxels into global descriptor vectors, enhancing feature capture and reducing errors in occluded directions, especially in complex indoor spaces. However, these methods perform less effectively in outdoor environments. Recently, new approaches combining voxel-based techniques with other methods have significantly improved handling of complex outdoor environments. Table 3 compares their performance on the SemanticKITTI test set: For navigation purposes, identifying static obstacles (such as furniture and walls) is relatively straightforward, while detecting and tracking dynamic obstacles is more complex. This process requires continuous monitoring of the environment and effective tracking of moving objects, to ensure safe and efficient navigation. Ref. [ 51 ] proposed a method for detecting and tracking dynamic objects based on an octree structure, first converting point cloud data into voxels, then constructing a short-term map through the octree, comparing the current scan with the short-term map, to detect dynamic voxels, and performing clustering and tracking. Ref. [ 52 ] developed a 3D perception system for dynamic urban environments, which is capable of detecting and classifying static and dynamic voxel obstacles. Ref. [ 53 ] proposed a model-free tracking method that can perceive and understand the motion in dynamic environments in real time. With the development of deep learning, methods combining voxel models with deep learning frameworks have emerged, effectively handling static and dynamic objects and classifying moving objects, such as cars and pedestrians, especially in dynamic road scenes. Here, we present the performance of these methods on the SemanticKITTI dataset (see Table 4 ): 59,60, Voxel-based spatial analysis is the core of voxel navigation. The intuitive description of spatial topology by voxels gives them a significant advantage in analyzing complex spatial relationships, such as topology, direction, and connectivity. The voxel model discretizes continuous 3D space into regular cubic units and clearly represents the connections and interactions between each voxel and its surrounding voxels through defined neighborhood relationships (6-neighbor, 18-neighbor, or 26-neighbor) and voxel indexing. This structure allows complex spatial relationships to be expressed and analyzed through simple and efficient computational methods without the need to compute and restore the spatial coordinates of the voxels. Additionally, by quantifying the geometric features of each unit in space, voxels provide an accurate description of spatial directional information. The directionality of each voxel in space can be obtained by calculating its normal vector or gradient, and this directional information helps in understanding the arrangement and distribution of elements in space. By combining the above information, the similarity between voxels can be measured, enabling the assessment of spatial connectivity, which is commonly used in geometric space segmentation [ 58 61 ]. Visibility analysis, which focuses on evaluating of line-of-sight, obstructions, and spatial relationships within a voxelized environment, is a specialized subcomponent of 3D spatial analysis and is also essential for voxel-based navigation. This type of analysis has widespread applications in generating path-planning networks and assessing urban environment safety. Ref. [ 62 ] proposed a voxel-based visibility-analysis technique, using voxel models and Unity software to simulate fields of view and areas of interest. Ref. [ 19 ] calculated the volume of visible space from specific viewpoints, using Digital Elevation Models (DEMs) to partition urban environments into voxels, providing a comprehensive evaluation of visible areas. Additionally, ref. [ 63 ] proposed a quantification method using 3D isovists and voxels to measure the visible and occluded portions of the view volume for landmark-based navigation, focusing on visual pollution, especially from outdoor billboards. Ref. [ 64 ] focused on the impact of vegetation on the visibility of traffic lights and signs in urban street environments, using highly detailed 3D city models and ray-tracing techniques to assess the visibility of signals and signs while considering visibility differences for various subjects (e.g., cars, bicycles, or pedestrians) (see Figure 8 ). In summary, these visibility-analysis methods not only have significant implications for urban safety and visual pollution assessment but also provide valuable insights for voxel-based navigation, helping to build more efficient and safe navigation solutions. Voxel data structure plays a crucial role in 3D spatial representation, especially when dealing with complex scenes. However, due to the large number of voxels, especially in large-scale environments, storing and managing voxel data becomes challenging. To address these challenges, researchers have proposed various voxel data structures. We provide an overview of voxel structures based on the frequency of data structure updates, dividing them into static and dynamic grids. 85,86,71, Static grids, once generated, typically do not change or only undergo minimal updates. They are suitable for scenarios requiring fixed spatial partitioning and are appropriate for handling static or minimally changing environments. The regular grid structure [ 66 ] is straightforward, storing voxels in a three-dimensional array. The topological relationships between voxels enable rapid queries and operations. However, regular grids are memory-inefficient, particularly for large scenes or high-resolution models, requiring substantial storage space. Sparse Voxel Octrees (SVOs) [ 67 ] enhance memory efficiency by recursively subdividing space into smaller blocks and storing only occupied voxels. SVOs also support Level of Detail (LoD), allowing flexible rendering at various resolutions. While SVOs have been applied successfully across different fields [ 27 87 ], they still demand significant memory and bandwidth for huge scenes. Sparse Voxel Directed Acyclic Graphs (SVDAGs) further reduce storage by merging identical subtrees, significantly compressing voxel data while maintaining performance. Unlike traditional SVOs, an SVDAG shares pointers to identical subtrees, cutting down memory usage. It also eliminates decompression, providing compact and efficient voxel representation. This method is ideal for large-scale, high-resolution voxel data applications, such as city-scale 3D models and complex volumetric rendering. Several improvements have been made to SVDAGs, such as [ 70 72 ], making voxel data structures more effective in managing large-scale models. Dynamic grid structures need to include real-time updating capabilities, flexible data structures, and support for high-frequency updates, to reflect changes in the environment and adapt to dynamic scenes. The main challenges lie in handling large amounts of dynamic data and maintaining consistency. Dynamic Tiling Grid (DT-Grid) [ 73 ] introduces dynamic-grid segmentation methods, allowing flexible adjustment of voxel grids based on different resolutions and data densities, improving visual effects in dynamic scenes but also adding complexity to dynamic adjustments. Hierarchical Run-Length Encoding (HRLE) [ 74 ] reduces voxel data storage needs through systematic run-length encoding while maintaining spatial hierarchy, making access and manipulation of voxel data feasible. VDB uses a hierarchical sparse voxel data structure, dividing volumetric data into small chunks and managing these chunks through a tree structure (typically an octree). Nodes containing only valid data are rapidly stored, significantly reducing unnecessary storage and computational overhead while allowing quick access and manipulation of voxel data. With advancements in hardware technology, GPU methods such as GVDB [ 75 ] and GSPGrid [ 78 ] are becoming dominant, also focusing on compatibility with more APIs [ 79 ].\n\nBased on the different navigation scenarios, we categorize the literature into indoor navigation, outdoor navigation, and autonomous driving scenarios (see Table 5 ) and discuss the performance and advantages of voxel models in each scenario. The complexity and the diversity of indoor environments pose numerous challenges for indoor navigation, including intricate spatial layouts and multi-layered building structures. At the same time, the need for emergency evacuation is also critical. In such scenarios, traditional 2D maps often fail to provide sufficient information. With their detailed representation in 3D space and support for multi-story structures, voxel models have become a key tool in solving these problems. Indoor environments typically feature complex layouts, including rooms, corridors, and furniture. These complex structures require navigation systems capable of handling various spatial relationships, particularly in navigating effectively among numerous obstacles and details. Ref. [ 20 ] focuses on developing a strategy that allows drones to navigate using voxel models in known indoor environments. They designed a method to generate three-dimensional buffer zones around obstacles through an algorithm based on distance propagation, enabling drones to plot general paths that ensure safe navigation in complex indoor spaces by avoiding obstacles (see Figure 9 ). The rich geometric and semantic information in BIM models provides prior knowledge for indoor navigation. Ref. [ 88 ] combines voxel models with BIM, designing the BIM-based Indoor 3D Map model (BI3DM) that achieves indoor routing, laying the foundation for automated indoor navigation. In indoor navigation, multi-story building structures present unique challenges. Multi-story buildings typically include vertical connections between different floors, such as stairs, elevators, and ramps, making applying traditional 2D navigation methods difficult. Ref. [ 27 ] developed a method based on octree processing of 3D indoor point clouds, generating multi-level navigation maps suitable for multi-story path planning in complex indoor environments, considering the walkable spaces necessary for floor connections. Additionally, ref. [ 89 ] simulated different movement behaviors (such as walking, rolling, and flying) in an octree-based indoor building model and integrated these behaviors into the path-planning algorithm. This method provides accurate 3D path navigation for users with different mobility needs, such as pedestrians, wheelchair users, and drones. Emergency evacuation is of paramount importance in indoor navigation. In emergencies, such as fires, earthquakes, and other critical situations, effective evacuation planning not only improves escape efficiency but also reduces the risks associated with chaos and panic. Voxel models can analyze the passability of the interior of a building, identifying the most suitable evacuation paths and avoiding congestion and hazardous areas. Ref. [ 97 ] classified indoor spaces into Pedestrian Free Passable Space (P-Space), Conditional Passable Space (C-Space), and Non-Passable Space (N-Space) in voxel models, further subdividing C-Space according to pedestrian movement patterns (crawling, knee–hand crawling, stooping, or upright walking). This contributed to effectively identifying and extracting passable spaces for pedestrians in indoor evacuation simulations. Ref. [ 32 ] analyzed the passability of emergency exits in buildings, assessing the effectiveness of emergency exit routes and the overall safety of building evacuation plans. In summary, voxel-based representations bring numerous benefits for indoor path planning. The advantage of voxel models lies in their ability to accurately represent the three-dimensional spatial structure of buildings, containing detailed geometric and semantic information, thereby addressing the inadequacies of traditional flat maps in complex environments. Compared to other 3D navigation methods, voxel models offer greater expressiveness and can more precisely reflect the details of buildings, particularly in multi-story indoor spaces and complex scenarios. Unlike grid-based or point cloud-based 3D navigation methods, voxel models handle local spatial variations more effectively, reducing computational complexity and improving path-planning efficiency. Additionally, voxel models demonstrate greater flexibility and adaptability in dealing with complex situations such as emergency evacuations, significantly enhancing navigation effectiveness. Outdoor and indoor navigation differ significantly. Outdoor navigation involves diverse terrain features, such as city streets, forests, and mountains, and requires handling complex obstacles. We divide outdoor navigation into two scenarios: real-time navigation in unknown environments and path planning in pre-established environments, each with unique challenges and methods. In real-time navigation, sensors like cameras, radar, and LiDAR play a crucial role in generating real-time maps for safe navigation. For example, Ref. [ 99 ] developed a navigation system for mobile robots in agricultural environments, using 3D camera data to build a three-dimensional voxel map of the agricultural environment. This voxel map allows the robot to identify open spaces and navigate efficiently within them. The ability to create and update such maps is crucial in dynamic and complex environments. Ref. [ 103 ] used scanners mounted on rotating mechanisms to divide scanned spaces into voxels for pedestrian recognition and tracking, minimizing the likelihood of collisions between mobile robots and crowds. Ref. [ 100 ] introduced a camera–LiDAR fusion method to address forest occlusion and lighting variation issues. They constructed a three-dimensional voxel map of the forest and introduced adjacency feature assessments based on the voxel map to estimate the navigability of the forest environment, enabling ground autonomous vehicles to traverse complex and dense forest terrains. In the second scenario, path planning in established environments depends on existing maps of the surrounding area. Ref. [ 104 ] combined 2D and 3D datasets collected by robot sensors and developed a ground-segmentation method to detect non-ground objects, helping to create detailed and accurate environmental maps. Ref. [ 105 ] proposed an integrated path-planning method combining the three-dimensional voxel Jump Point Search (JPS) and the Markov Decision Process (MDP) to address static obstacles and dynamic threats. The 3D voxel JPS is used to generate global reference paths, ensuring collision-free flight paths around known static obstacles, thereby guiding drones safely through urban environments. By dividing complex three-dimensional spaces into regular voxel grids, voxel models can describe obstacles and terrain features in detail, which is crucial for handling complex terrains, such as forests, city streets, or mountainous regions. Additionally, real-time updates to voxel data can address moving obstacles and emergent situations, providing stable and reliable path-planning support for outdoor navigation tasks of varying scales and complexities. 110, Compared to other outdoor scenarios, autonomous driving presents unique challenges, due to the complexity and dynamic nature of the driving environment. Autonomous systems must contend with dynamic obstacles, such as other vehicles, pedestrians, and cyclists, as well as navigate through dense urban structures and narrow roads. These factors place higher demands on environmental perception and path planning, warranting a separate discussion of autonomous driving scenarios. Ref. [ 52 ] proposed a voxel-based method to segment static and dynamic obstacles. This system integrates time-dependent data to construct a voxel grid model, and it uses discriminant analysis to detect these obstacles (see Figure 10 ). Ref. [ 107 ] utilized LiDAR sensors on Autonomous Vehicles (AVs) to perceive the surrounding environment by voxelizing the 3D space with a fixed unit size and applying deep-learning techniques to identify voxels that exhibit key dynamic features. This approach enables rapid identification of objects that may impact AV safety and tracks their movement in the 3D observation space around the AV, providing a comprehensive safety assessment of the driving scenario. One critical aspect of vehicle navigation is accurately detecting road features, such as sidewalks, curbs, and lane markings. To this end, ref. [ 108 ] developed a method that automatically extracts road features in urban environments, using dense Airborne Laser Scanning (ALS) data. By leveraging the high-resolution data provided by ALS, the method creates a detailed and accurate representation of the urban road network, significantly enhancing the reliability and safety of autonomous vehicle navigation systems. Recent studies [ 109 111 ] have increasingly focused on combining Bird’s-Eye View (BEV) and voxel representations to improve the performance of autonomous driving systems and computer vision applications. This combination leverages the intuitive representation of BEV in the planar view and the detailed modeling capability of voxels in 3D space, leading to a more accurate and comprehensive understanding of the environment.\n\nAlthough this paper mainly focuses on voxel-based navigation, other research fields would benefit from advancements in these studies. In this section, we list a set of applications of voxel-based navigation techniques that could serve as a basic function and lead to the generation of new models and algorithms. Autonomous driving has become a highly active research field. However, in autonomous driving, voxel-based navigation systems still face significant challenges in managing and processing large-scale voxel data. One key issue during voxelization is the loss of detail, especially when modeling complex environments. The complexity of urban roads, building details, and terrain variations require high-resolution voxel models. Low-resolution voxels may fail to accurately represent the position or structure of obstacles, thus reducing the precision of path planning. While reducing voxel size can mitigate the loss of detail, it significantly increases the computational load and memory consumption, posing challenges for real-time navigation tasks, where rapid updates and environmental analysis are crucial. Overly fine-grained voxel models can overwhelm computational resources, hindering real-time processing and even compromising safety. To address these challenges, common solutions include sampling techniques and parallel computing. For example, ref. [ 112 ] proposed a collaborative framework where multiple devices work together to construct voxel-based indoor maps. However, the applicability of this framework to both indoor and outdoor modeling remains limited. With the rapid development of quantum computing, future research could explore its application in voxel-based urban environment modeling to further improve data-processing efficiency. In disaster response scenarios, quickly providing safe and reliable navigation routes for emergency responders is critical. This becomes particularly challenging in complex public buildings, such as train stations, hospitals, airports, or shopping centers, where dynamic hazards like fires and toxic smoke significantly increase the difficulty of navigation. Voxel-based modeling and navigation techniques demonstrate unique advantages in handling the uncertainties in such environments. These models not only provide detailed spatial layout information but also track real-time environmental changes, such as fire spread, smoke diffusion, and structural damage, to support optimal path planning for rescue efforts. 98, Voxel models, with their discrete and regularized representation, can effectively adapt to dynamic changes in complex scenarios, such as corridor planning, obstacle recognition, and emergency evacuation route selection. By continuously updating environmental data, these models can capture emerging hazards, including fire sources, collapsed walls, or expanding smoke areas, ensuring that navigation paths are adjusted in real time to avoid dangers and safeguard emergency personnel. In addition to representing static structures, voxel models are adept at handling dynamic obstacles like falling debris or the spread of toxic gases, integrating these changes into path planning to generate safe, adaptable routes. Furthermore, voxel models excel in emergency simulations by recreating dynamic scenarios, such as fire and smoke spread [ 96 113 ], allowing for real-time evaluation of the safety and feasibility of evacuation routes. This capability enables responders to make faster, more informed decisions, thereby improving the efficiency and effectiveness of disaster response efforts. 115, With the continuous advancement of 3D data-acquisition and model-construction technologies [ 114 116 ], 3D city models have been increasingly applied in various fields, such as urban design, spatial planning, facility management, and emergency response [ 22 ]. In pedestrian navigation research, the influence of urban environmental features on route choice has become an important focus. Factors such as street width, green coverage, building height distribution, and the proportion of open views can significantly affect pedestrian route preferences. In recent years, there have been numerous explorations into the application of 3D city models in urban studies. For instance, [ 117 ] utilized 3D models to extract environmental indicators, such as line-of-sight distance and sky view factor, integrating them into a walkability assessment model. Similarly, ref. [ 118 ] developed a spatial index based on 3D building models to measure compactness. These studies highlight environmental features’ critical role in shaping pedestrian navigation behaviors. As noted by ref. [ 119 ], 3D city models provide an integrated and comprehensive platform for analyzing the impact of natural and built environments on human activities. By discretizing and quantifying urban environmental attributes, voxel-based models offer robust technical support for analyzing how environmental factors influence pedestrians in choosing safer, faster, or more comfortable routes. However, research on developing 3D voxel-based urban models that support human–environment interaction analysis still needs to be completed, indicating a need for further exploration and refinement in this domain.\n\nDespite the advantages of voxel models in representing complex environments, handling dynamic scenarios, and supporting path planning, several challenges remain that need to be addressed. These issues not only affect the accuracy and efficiency of navigation systems but also point to future directions for research. Firstly, voxel granularity is a fundamental challenge in voxel-based navigation. The size of a voxel directly determines the level of detail in environmental representation and the computational complexity of the system. Smaller voxels can capture fine details of the environment, which is especially useful in high-precision navigation scenarios, such as indoor environments or robot navigation. However, fine-grained voxel models significantly increase computational and storage costs, potentially affecting the system’s real-time performance. On the other hand, larger voxels simplify computation but may lead to the loss of important environmental details, reducing the accuracy of the navigation paths. Thus, selecting the appropriate voxel granularity for different application scenarios to balance accuracy and efficiency remains a key challenge. Future improvements may include adaptive voxelization methods, where voxel size is adjusted based on environmental complexity. Larger voxels can improve efficiency in simpler areas, while smaller voxels can ensure precision in more complex regions. Another potential approach is using multi-resolution voxel models, with coarse-grained voxels for global path planning and fine-grained voxels for local optimization, ensuring both precision and efficiency. Secondly, path optimization is another major challenge in voxel-based navigation. While traditional pathfinding algorithms like A* perform well in voxel models, the resulting paths often exhibit rigid or “stepped” shapes, especially in highly dynamic or irregular environments. Such paths may lack smoothness and continuity, leading to suboptimal navigation performance. This rigidity not only increases travel or movement costs but can also impact the safety and efficiency of the navigation system, particularly in scenarios that require high agility, such as drones or robots navigating narrow indoor spaces. Furthermore, an additional challenge in path optimization is effectively avoiding dynamic obstacles. Complex 3D environments often involve unpredictable dynamic factors, which can render previously planned paths unusable. To address these uncertainties, navigation systems need to be highly flexible and capable of real-time adjustments, allowing them to generate new, safe routes when the environment changes. Future research should focus on exploring more advanced path-optimization algorithms, such as those leveraging reinforcement learning or intelligent search strategies, to dynamically adjust the search process and generate more flexible and smoother paths. The third challenge lies in integrating voxel models with other existing data models. While voxel models excel in 3D geometric representation, other data models, such as CityGML, IndoorGML, and IFC, offer rich semantic information, including building functionality, usage, and floor structure. Seamlessly integrating voxel models with these existing data models, allowing navigation systems to perform path planning based not only on geometry but also on semantic information, is a crucial direction for future research. This integration could enhance the adaptability of navigation systems and support multi-functional navigation applications, such as urban planning and building management.\n\nThis paper presents the first systematic review of voxel-based navigation technology, covering key technologies, specific applications, development potential, and associated challenges. We have detailed the latest advancements in voxel-based navigation for both indoor and outdoor environments and explored the significant advantages of voxel models. These models excel in representing complex environments, handling dynamic scenarios, and supporting path planning. For instance, voxel models offer precise environmental representation and are dynamically updated to accommodate real-time changes, significantly enhancing the flexibility and reliability of navigation systems. Additionally, voxel models possess strong emergency-simulation capabilities, effectively supporting decision making in complex scenarios. However, despite the extensive application of voxel modeling in navigation studies, several significant challenges still need to be solved, such as voxel granularity, path optimization, and model integration. Addressing these issues will require further advancements in algorithms and model improvements. Furthermore, this paper primarily focuses on voxel-based navigation models and does not delve into other related aspects, such as route communication, user interface, and localization. Our review aims to provide not only deep insights into voxel-based navigation technology but also valuable guidance for researchers in other fields considering the application of voxel models in their studies."
    },
    {
        "link": "https://buzzcoder.gitbooks.io/codecraft-javascript/content/codecraft-game/3d-coordinates.html",
        "document": "The graph shows a 3 dimensional coordinate system with origin O and the three axes. In the CodeCraft game world, the x-axis and z-axis both lay horizontally, but the x-axis points to the right while the z-axis points towards you. The y-axis is vertical, pointing up.\n\nIn object, the 3 numbers represent the 3D coordinates x, y and z:\n\nand is the origin, your initial location. includes everything you see in front of you, while is behind you.\n\nindicates the vertical location, is above the ground where we can see;\n\nYou can create objects to represent specific locations in the CodeCraft world:\n\nWant to see where these points are in the 3D world? We will place blocks at those locations in the next lesson."
    }
]