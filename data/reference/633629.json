[
    {
        "link": "https://st.com/en/development-tools/stm32cubeide.html",
        "document": ""
    },
    {
        "link": "https://wiki.st.com/stm32mcu/wiki/STM32StepByStep:Getting_started_with_STM32_:_STM32_step_by_step",
        "document": "This message will disappear after all relevant tasks have been resolved. There are 1 incomplete or There are 1 incomplete or pending task to finish installation of Semantic MediaWiki . An administrator or user with sufficient rights can complete it. This should be done before adding new data to avoid inconsistencies."
    },
    {
        "link": "https://01001000.xyz/2020-05-11-Tutorial-STM32CubeIDE-Getting-started",
        "document": "Getting started with an 32-bit ARM-based microcontroller is always a little daunting. There’s a plethora of available microcontrollers, platforms, development boards, tools, and software. Everyone seems to have their own opinion on what is best to use, and debates between the available options range in tone from pleasant and insightful to divisive and vitriolic. It can be quite confusing to work out exactly how one should get going - I certainly remember it being so, at any rate!\n\nWell, in this tutorial I’m aiming to de-mystify things a little, targeting the very challenges that I myself faced when I first got started. I’ll walk through project initialisation, introducing the peripheral and clock configuration options, and illustrate the ST CubeMX code generator traps. I’ll then show off tools for debugging, something that everybody should know a little about. Finally, I’ll demonstrate the ST-Link’s integrated Virtual COM Port, which is a handy feature built in to my development kit.\n\nWhile my normal ARM programming environment is currently based on Visual Studio Code, for this tutorial blogpost I’m actually going to use the STM32CubeIDE, as it’s free, it was only semi-recently released, and I’m interested in its capabilities — especially with the debugger. Debugging for me in the past has been through the nightmare-inducing (but indisputably powerful) command line interface for OpenOCD.\n\nHey, is this blog post an advertisment?\n\nNo, but I wish it was! How does one get into that gig? Seriously, let me know. :-)\n\nSadly, I’m just writing about ST offerings since I currently only own three ARM-based development boards. Two of them are made by ST, are quite cheap, and are reasonably friendly to hobbyists and beginners. My other board is the Terasic DE10-Standard, which is none of those things - however, as the DE10-Standard is freaking awesome, I will no doubt blog about it in future.\n\nI’ve also worked extensively with ST products throughout my academic and industrial career, so I have a bit of institutional knowledge to go along with these dev boards.\n\nFirst, before you download and instally anything, make sure you actually have something to program! As mentioned, I have two ST dev boards; the stm32f4-discovery series and the stm32nucleo series , as seen here:\n\nIf you’re looking for a review of these, I like both of them quite a lot. The discovery board is great since it has an integrated LCD screen and a big on-device SDRAM chip, and the nucleo board is neat since it is quite cost effective and has both Arduino-compatible headers as well as its own larger array of I/O to work with.\n\nAs it’s the cheaper board, this tutorial will focus on the development board. However, as the steps can easily be generalised, you should be able to follow along with almost any other STM32-based development board.\n\nFor your convenience these are two purchase links for the dev boards, for the Nucleo-F303RE and the STM32F429I-DISC1. Note that these are Amazon referral links. I’ve found Amazon usually has cheaper prices than anyone else, but you can also shop around for a deal that looks good to you. Also, while the official ST boards are nice, but you might also be able to find third party ones that are cheaper (although they often come with less documentation / worked examples).\n\nThere are a lot of different software development environments that will work with the STM32 line of microcontrollers. Today I’m going to focus on ST’s own IDE: STM32CubeIDE. It includes the necessary drivers, compilers, and the debugger all in a nice eclipse package.\n\nSo, let’s get started. You can download and install CubeIDE from their website. It’s free, although you do have to sign up an account / give them your email address. My first impressions are very positive - ST have provided binaries for every platform (yay Debian/Linux).\n\nAnyway, here’s the steps, I’m sure I don’t need to go into detail here:\n• Download the correct version for your operating system (I grabbed the Debian bundle since I’m running an Ubuntu system)\n\nBefore we can start writing code we need to create a project. This is similar to most other IDEs - projects are used to bundle together all of your settings, code, and definitions into a single collection all managed from the same application (hence the name IDE, which stands for Integrated Development Environment). The alternative would be to have multiple programs to handle your development, for instance using CubeMX for your chip configuration, or to code, standalone ST-Link drivers for programming, OpenOCD for debugging, and so on. This is a more fiddly approach, but some developers prefer it.\n\nAt any rate, here’s what CubeIDE looks like - yep, that sure looks like an eclipse-based environment.\n\nAs CubeIDE is eclipse-based, I know to look under the top left icon (Or under the menu ) to get started:\n\nThis will now think for a moment (a popup may appear briefly) and then show you the chip/board selector. This menu appears to be copied from their standalone STM32CubeMX program.\n\nSince I have the Nucleo-F303RE, that’s what I’m going to search for, after selecting at the top:\n\nI like that it shows me the picture of the kit, that’s kind of neat. Now, pick a sensible project name:\n\nI didn’t change anything in the final window. Hit finish and let’s move on!\n\nI then got a popup asking me if I wanted to initialise peripherals to their default configuration.\n\nI assume this means the peripherals should be set up for the hardware on my development board. I’ll hit yes.\n\nI now get the CubeMX configuration menu view. This looks like this:\n\nIn here, we can set up our peripherals. By choosing a moment ago it appears to have pre-populated some settings for me:\n• and PC14/15, PF1/0 are the high and low speed oscillators (I’m not entirely sure why these are enabled by default since my dev kit does not have one of the crystals populated).\n\nQuick pause for a sanity check, let’s open up the schematic for this development board (schematic available in menu to the right of that page). Has it put the LED in the right place?\n\nCool cool. We can check the switch in the same way, and that looks correct as well.\n\nIf you click on in the top menu bar, you’ll now see an intricate clock system diagram:\n\nUsing the radio buttons embedded into the multiplexers throughout this diagram you can change how different available clock sources propagate through the various PLLs and clock dividers in the device in order to generate all the different frequencies you need.\n\nThe default suggestion seems pretty good to me - It’s using the on-chip high and low speed RC clocks, which are perfectly acceptable for little demo applications such as this. Hopefully you can see from this diagram that it’s then using a PLL to multiply the internal 8MHz up to 72MHz. The 72MHz is then passed as-is to everything except the APB1 peripheral bus.\n\nIn theory, the clock system does its best to stop you from breaking anything. Observe, if I change the APB1 divider to instead of , the system detects that this will create a faulty system (it exceeds the 36MHz maximum for this bus), and thus flags the inappropriate setting in red.\n\nI returned the setting to the original, and saved. It may ask you if you want to generate code - if you haven’t already done so, hit yes. Now, onto the programming!\n\nLet’s C what we can do here\n\nCubeMX, which this functionality is developed upon, generates C files to work with under a Src directory, and puts a HAL (Hardware Abstraction Layer) into an Includes directory. It appears CubeIDE works the exact same way. Expand the folders on the right under the project view and see what it has generated to work for you:\n\nFor the purposes of this tutorial we’ll just focus on the main section, starting with main.c.\n\nYou’ll see that main.c is already quite large, containing a fair amount of autogenerated code. A key piece of information to remember here is that main.c can be edited by the code generator, so it’s important to only write code in the USER sections. What does this mean? Let’s take a look under :\n\nObserve in this block of code that there are a number of and sections marked out by comments. Any code written inside these blocks is safe and will not be deleted by the code autogenerator. Any code written outside those blocks is unsafe and will be deleted by the code autogenerator any time you edit the CubeMX settings that we looked at earlier.\n\nOf course, files that you yourself add to the project (e.g. ) are also totally safe. It’s just this set of autogenerated (and ) files that you must be careful with when adding code.\n\nTake a brief moment now to look at the function names used in the autogenerated code.\n• Any function call beginning with is from the STM32 HAL, and is provided in the library files. There’s HAL functions to do all sorts of things, including using the UART, writing a Pin, etc.\n• Any function call beginning with is autogenerated by CubeMX/CubeIDE. These functions tend to be used to initialise functions.\n• There are exceptions to these rules, including, annoyingly, , which is also an autogenerated function.\n\nLet’s add a smidge of C code of our own now! After the area, we’re going to add code to toggle the LED under section 3. I initially couldn’t get the autosuggest to show up on its own, but then worked out it appears after you press :\n\nWe’re going to change the overall loop to:\n• Indentation by the code generator is 2 spaces per level (It’s not my favourite, but you do get used to it)\n• Under the CubeMX view, the Led GPIO pin was named LD2 - observe how a name has automatically been generated for both the Pin and the Port\n• I’ve used two HAL functions, one to toggle a GPIO pin, and one to cause a delay of 1000 milliseconds.\n\nAlright! This tiny project is all we needed to blink that on-board LED at around once per second. What’s next?\n\nCompiling the project and downloading it to the board\n\nSTM32CubeIDE actually makes it pretty easy to compile our work and get it onto the STM32 chip. We only need to tell it what we want to do once, right at the start, and every subsequent time it will copy what we asked. The first step is to produce the first version of the compiled (a binary version of our code). We need this so that we can point the download tool to it.\n\nTo generate the , we need to do a build. This is as easy as pressing the button on the toolbar:\n\nNow, build information is presented in the console at the bottom of the screen:\n\nNow what we want to do is send this compiled binary onto the STM32 microcontroller.\n\nThe Red power LED (to the left of the blue switch) is lit, as is the larger communication LED (by the USB cable). Not much else is happening - that’s fine though, we’ll soon get a bit more going.\n\nThis will open the Run dialog (as it’s the first time we’ve run it). The settings we choose now will be saved as a run configuration which we can re-use or edit later.\n\nFortunately, the default settings (at least on Ubuntu) are perfect for this tutorial, so we don’t need to change anything. Feel free to take a look under the other tabs (the debugger one is interesting - note how it expects to be using the SWD interface, which was some of the pins that were set by default under the CubeMX project when we first created a project for our board).\n\nSimply press Apply and then OK and the download will proceed.\n\nThe Console will now fill with some interesting text:\n\nInterestingly, I didn’t get the option to choose a board or USB port or anything during this process, it all just happened automagically, so I’m not sure what would happen if you had multiple ST dev boards / ST-LINK programmers connected all at the same time.\n\nAt any rate, my dev kit’s communication LED lit up during this time, and after that it seems that the board is running the program!\n\n(Yes, I went to the effort of recording, editing, and compressing a 2 second video of a blinking LED).\n\nIt really is as easy as that - you’ve got everything set up now. Any new code from here, you simply just need to hit the button - it will compile it for you automatically.\n\nTooling for analysing and debugging embedded systems largely comes in two main categories. The first of these is in the hardware domain: tools like oscilloscopes, logic analysers, datasheets, and horrifying math equations. These can all work together to help you benchmark your systems and solve issues with your designs. These tools are frequently used.\n\nThe second category of tools focuses on software debugging, with breakpoints, watch lists, trace analysers, and so forth. These all work together to help you diagnose and understand what your embedded software is doing. Yet, it has been my experience that these formal tools are much less frequently used - even though there is a trend throughout industry to integrate more and more software into our designs. Instead, designers rely on simpler ‘tried-and-true’ approaches such as blinking LEDs or outputting debug messages to UART peripherals. Now, it’s not to say that these techniques don’t work - they do, given time! - but they can often get in your way and be a major barrier to understanding when trying to diagnose why something is or is not working.\n\nIt’s hard to say why the proper tooling for software debugging gets less attention than the proper tooling for hardware debugging. Perhaps it is because there is a lack of awareness of the tools and their capabilities. Perhaps they are not properly taught in our apprenticeships and academic institutions. Perhaps it is because historically these tools have been fiercely difficult to use with high barriers to entry.\n\nAt any rate, the state of the art these days for software debugging tools seems to be pretty good, with reasonably well made and accessible tooling available for us to use. STM32CubeIDE seems to be no exception, and in this section, I aim to introduce you to some of the things we can do with its integrated debugger.\n\nNote: You may need to enable debugging via your CubeMX config. I had it enabled already since it’s included in the default configuration for my development board. If you need to enable it, do this in the CubeMX view:\n\nSave your config using and regenerate the code.\n\nLet’s have a play by debugging our code! But first, let’s add something to debug, since there’s not much going on here. I choose to add a few snippets of code to calculate some prime numbers!\n\nThus, I’m going to add a few lines of code to a few of those USER code blocks in , as detailed here:\n\nOkay! If we compile and execute this, we won’t observe any changes (except for maybe the slightest of delays before the LED starts blinking the first time). So how do we know if the list of primes is correct?\n\nWell, how about we just ask the debugger? Let’s give it a go!\n\nLet’s first look at the end result, and then look at observing the looping calculation using a breakpoint.\n\nTo simply observe the end result, we launch the program using the Debug mode, with the button.\n\nFirstly, you’ll see STM32CubeIDE change into its debug perspective. You may get a popup asking you about this.\n\nYou’ll notice that the dev kit in front of you has the communication LED blinking continously, but that the user LED isn’t blinking yet. This is because the program is not actually running yet. In order to run the program, we need to press the ‘Resume’ key, which will get us started: (the resume key, as well as the other debug control keys, will have appeared due to the debug perspective activating).\n\nOnce you notice the dev board’s LED light is blinking, pause the execution by pressing the suspend key:\n\nYou’ll notice that the IDE immediately throws you somewhere in the C code for your project - almost certainly in somewhere to do with the HAL_Delay function. That’s where your program was when you hit suspend!\n\nWe can get back home by going to the left Debug panel and selecting .\n\nNote that the IDE highlights the function that is currently being executed (2) as well as presents a list of variables in the current scope (3).\n\nWe can also examine the contents of our global variable by double clicking it:\n\nOr in a more convenient manner (since this is a big array) by right clicking on the variable and selecting (and then pressing OK)\n\nThat then adds it to the menu on the right (I’ve expanded it for this screenshot):\n\nThat’s really cool! Press the Red Stop button now in the top menu, and we’ll have a go at watching execution of the prime calculation loop.\n\nIn addition to running and suspending execution, we can also ask the program to suspend at a point of our choosing. This is known as creating a breakpoint.\n\nIn STM32CubeIDE you do this by double clicking on the red bar next to the line numbers, which will cause a small blue breakpoint indicator dot to appear.\n\nNow, without changing anything else, launch the debug mode again.\n\nThis time, when you hit resume, you’ll notice that the program executes and then automatically halts when it reaches your breakpoint.\n\nNow if I press suspend again, it will loop and stop at this same function. And, oh, what’s this?\n\nThe first index of primes has changed, and it’s highlighted the changed variable!\n\nNow the second index of primes has changed, and the changed variable is highlighted once more!\n\nYou can keep pressing the resume button and you’ll see it slowly calculate the array.\n\nLet’s now quit the debugger and move on. You can delete the breakpoint again by double clicking the blue dot. Note that if you right click on the red column, you can also toggle and create breakpoints this way. This also brings up advanced breakpoint options, including breakpoint conditions and breakpoint types.\n\nIf properly configured, you can output arbitrary strings directly to the debugger via the programmer, rather than sending them via any other peripherals. It’s a bit like a virtual UART that you can send data to.\n\nThis is a little involved to set up, and it can be worth simply using a UART if you must send out strings of characters to help your debugging, but I’ll step through the basics here using STM32CubeIDE.\n\nFirst, we must configure the reception clock rate. We do this via the debug configuration menu.\n\nGo to the Debugger tab, Enable SWV (Serial Wire Viewer), then change your clock rate to the FCLK from earlier (remember when we chose the clock rate for all of our components?)\n\nYou may leave the SWO Clock drop down set to its maximum.\n\nLet’s add a test to send some characters. In the main loop, add the following:\n\nis a special function which sends a character to the debugger’s serial viewer. You shouldn’t need to anything new or special to use this.\n\nYou can now launch the debug session as before. But, before you press to get it started, we need to enable a few more debugging options.\n\nFirst, open the ITM data console through the menu:\n\nThis will bring up the ITM data console window. Now, enter the configuration menu:\n\nPort 0 will appear in the console view. Now press - this actually contacts your microcontroller and adjusts some registers internally to enable this mechanism:\n\nNow, and only now, can you press Resume. You’ll notice your Port 0 terminal slowly start filling with exclamation marks (since that’s the character we’re sending!)\n\nAs you can imagine, this is pretty handy when your design might not have a free UART for debugging.\n\nWithout too much difficulty, we can also spin up a custom function for debugging. There’s a few options for this, but my preferred approach is to actually create my own function, like so:\n\nAdding more to , as detailed:\n\nNow we save, build, and that. Remember to the ITM trace before pressing !\n\nThe very last thing I want to talk about today is a neat feature included in my Nucleo board. The inbuilt ST-Link v2.1 interface that we’ve been using for programming and debugging also includes a virtual COM port. As of the time of this tutorial, the COM port uses drivers which are included natively in most operating systems (including Windows and Ubuntu Linux).\n\nIndeed, if I run in my terminal, I can see it has been made available as thus:\n\nThis is really handy for your user applications, as this virtual COM port is wired directly onto one of the USART peripherals on the nucleo board! Recall from the CubeMX view that pins PA2 and PA3 were automatically configured as a USART for us.\n\nA quick sanity check to make sure this makes sense by looking on the schematic:\n\nSure looks like they’re connected to a USART (and indeed, tracing it through the rest of the schematic shows them connected to the ST-Link V2 programmer). So let’s quickly jump back into the CubeMX view and see how the port was set up:\n\nIt’s configured as asynchronous, at 38400 baud, 8 data bits, no parity, 1 stop bit.\n\nWe could change these settings now if we wanted to. The virtual COM port works the same as any other USB to serial adapter, and so any baud rate and config can work with it. For now I’m happy with the defaults though.\n\nLet’s have a go at sending some characters to it.\n\nWe now build and download that, before running Minicom:\n\nIf we wanted, we could now make a like our from earlier, but I’ll leave that as an exercise for the reader.\n\nHopefully across this tutorial you’ve managed to gain some familiarity with the tools and environment available for your STM32-based device. My intention was to show you a little bit of everything! Now that you know how to make a project and get code working, why not try out some of the features of your microcontroller?\n• Have a go at reading the input push button\n• Try out timer interrupts or timer PWM to get a true 1 second LED blink\n• Have a go at using USART with DMA (direct memory access)\n\nIn addition, practice your C a little more:\n• Split functionality out into multiple files so you don’t need to worry about the code autogenerator deleting things\n• Practice string manipulation (maybe make a basic chatbot which sends and receives via UART?)\n• Try out pointers to functions (rather than pointers to variables) and have a google of dependency injection\n\nI for one am pleasantly surprised at the capabilities and stability of STM32CubeIDE running on Ubuntu. While creating this tutorial I found that it ran smoothly and every feature that I wanted was present. I’m pleased with the capabilities of the integrated debugger - it was a damn sight easier than my previous method, that’s for sure.\n\nIf you made it this far, thanks for reading, and hopefully this blogpost was at least somewhat helpful and interesting!"
    },
    {
        "link": "https://deepbluembedded.com/stm32-ecosystem-development-environment-setup",
        "document": "At the beginning of this series of tutorials, we’ll set up the development environment which we’ll be using throughout the entire course tutorials, LABs, and projects. In this short tutorial, I’ll list down the required software tools. And where to get them.\n\nThere are some different options for IDEs that can be used to develop firmware projects using the STM32 ARM-Based microcontrollers. Here are a few of them:\n\nSTM32CUBE IDE is the software tool we’ll be using. It’s a free eclipse-based IDE officially from STMicroelectronics, the hardware manufacturer for the STM32 microcontrollers itself. And it’s looking like a re-branded newer version of their older tool (Atollic TrueStudio).\n\nThe toolchain provides so many features to ease and accelerate the development, debugging, and testing tasks. The whole experience is just so good to be our starting point.\n\nNote: You’ll need to register for a free account using your email address to be able to download the software from their website. This step is required in order to get the download link for the version that fits your operating system condition (Windows, MAC, or Linux).\n\nIt’s important to decide on the level of abstraction which we’ll stick to throughout this course just at the beginning. We won’t be developing LL drivers at the register level as we’ve done in the Microchip PIC tutorials. However, we’ll be using the LL+HAL device drivers provided by STMicroelectronics. So we can dedicate the development effort to the application layer and middleware, mostly the ECU abstraction layer (ECUAL) drivers. As you can see in the software layered architecture diagram below.\n\nDoing this will have 2 major advantages that are considered to be goals for the entire series of tutorials. First of which is that you’ll learn how to develop reusable configurable firmware drivers for different modules, sensors, and interfaces. All of which will be dependent on the STM’s HAL+LL drivers that have uniformed APIs across the entire portfolio of STM’s STM32 microcontrollers families.\n\nThis brings us to the second advantage which is having an embedded software stack that can potentially run on any STM32 microcontroller with very little effort. This is really helpful if you’re designing your own STM32-based PCB boards and projects with any microcontroller part, having portable reusable firmware drivers is key in shortening the development time.\n\nThis is the second software tool you need to download and install. Obviously, we’ll use the CubeMX GUI app to setup and configure the low-level hardware and peripherals. It also helps you configures the clock tree of the microcontroller to decide on the various clock speed for the system, buses, and peripherals.\n\nAt the end of the configuration process, it generates the project folder in the specified directory. Then you click a button in order to launch it in the Cube IDE and start developing your project right away.\n\nYou’ll finally need to make sure that the driver for the ST-Link v2 programmer/debugger is correctly installed and it’s assigned a virtual COM port by your operating system.\n\nFor the nucleo32 board (any Nucleo or discovery board as well) you’ll not be in need to do this step. As it should be installed automatically the first time you connect it to your USB port in the PC. It’ll install the required drivers for the ST-Link debugger on the board itself.\n\nHowever, for the blue pill, you’ll need an external USB ST-Link v2 debugger and it may not install the driver automatically once connected to your USB port. So here is the link to download the ST-Link v2 Debugger Windows Drivers.\n\nAnd That’s It For This First Tutorial .. Next, We’ll Get Started With STM32 Microcontrollers Architecture"
    },
    {
        "link": "https://iies.in/blog/a-guide-to-setting-up-stm32-development-environment",
        "document": "Before diving into the setup process, it’s crucial to understand the STM32 ecosystem. The STM32 family consists of a wide variety of ARM Cortex-M-based microcontrollers that cater to different application needs. From low-power devices to high-performance models, STM32 microcontrollers are used in various domains, including automotive, industrial, consumer electronics, and more.\n• STM32CubeMX: A graphical software configuration tool that allows developers to configure peripherals and generate initialization code for STM32 microcontrollers.\n• STM32CubeIDE: An all-in-one integrated development environment (IDE) that combines the STM32CubeMX configuration tool with a powerful editor, compiler, and debugger.\n• STM32Cube Firmware Packages: These packages provide drivers, middleware, and example projects for various STM32 microcontrollers.\n• ST-LINK: An in-circuit debugger and programmer for the STM32 microcontroller families.\n\nSelecting the appropriate development tools is the first step in setting up your STM32 environment. Here are the primary tools you will need:\n• Integrated Development Environment (IDE): While there are several options available, STM32CubeIDE is the most recommended as it integrates seamlessly with other STM32 tools.\n• Compiler: STM32CubeIDE comes with the GNU Arm Embedded Toolchain (GCC), which is the most commonly used compiler for STM32 development.\n• Debugger/Programmer: The ST-LINK/V2 or ST-LINK/V3 debugger/programmer is necessary for loading your code onto the microcontroller and debugging it.\n\nSTM32CubeIDE is a one-stop solution that simplifies the setup process. It combines the configuration capabilities of STM32CubeMX with the development and debugging features of an IDE. Follow these steps to install STM32CubeIDE:\n• Download STM32CubeIDE: Visit the STMicroelectronics website and download the latest version of STM32 CubeIDE compatible with your operating system (Windows, macOS, or Linux).\n• Installation: Follow the on-screen instructions to install the IDE. Ensure you have administrative privileges if required by your operating system.\n• Initial Setup: Upon launching STM32CubeIDE, you may need to configure your workspace, which is the directory where your projects will be stored.\n\nSTM32CubeMX is integrated into STM32CubeIDE, making it easier to configure microcontroller settings. Use STM32CubeMX to:\n• Select Your STM32 Microcontroller: Choose the specific STM32 microcontroller or board you are using from the extensive list provided.\n• Configure Peripherals: Use the graphical interface to enable and configure the microcontroller peripherals you need for your application, such as GPIOs, timers, ADCs, etc.\n• Generate Code: After configuring the peripherals, STM32CubeMX generates initialization code, which is automatically imported into your STM32CubeIDE project.\n\nC. Setting Up the Debugger\n\nFor successful debugging and programming, ensure that your ST-LINK debugger is correctly set up:\n• Connect the ST-LINK Programmer: Connect the ST-LINK to your development board and your computer via USB.\n• Install ST-LINK Drivers: Ensure that the ST-LINK drivers are installed. These are usually included with STM32CubeIDE or can be downloaded separately from the STMicroelectronics website.\n• Debug Configuration: In STM32CubeIDE, create a new debug configuration for your project, selecting the appropriate debugger (ST-LINK) and target microcontroller.\n• HAL (Hardware Abstraction Layer) and LL (Low Layer) drivers: These libraries simplify hardware interfacing and are crucial for developing reliable applications.\n• Middleware Components: Middleware such as USB, TCP/IP, and file systems are included in these packages.\n• Example Projects: These are pre-configured projects that can serve as a starting point or reference for your own development.\n• Stay Updated: Regularly update STM32CubeIDE, STM32CubeMX, and firmware packages to ensure you have the latest features and bug fixes.\n• Organize Your Workspace: Keep your project directories well-organized and use version control systems like Git for better project management.\n• Leverage Community Resources: The STM32 community is active and extensive. Utilize forums, GitHub repositories, and ST’s own resources for troubleshooting and learning."
    },
    {
        "link": "https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html",
        "document": "\n• Learn to change images between different color spaces. Plus learn to track a colored object in a video.\n• Learn to apply different geometric transformations to images like rotation, translation etc.\n• Learn to convert images to binary images using global thresholding, Adaptive thresholding, Otsu's binarization etc\n• Learn to blur the images, filter the images with custom kernels etc.\n• Learn about morphological transformations like Erosion, Dilation, Opening, Closing etc\n• Learn about image pyramids and how to use them for image blending\n• All about Contours in OpenCV\n• All about histograms in OpenCV\n• Meet different Image Transforms in OpenCV like Fourier Transform, Cosine Transform etc.\n• Learn to search for an object in an image using Template Matching\n• Learn to detect lines in an image\n• Learn to detect circles in an image"
    },
    {
        "link": "https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html",
        "document": "\n• You will see these functions : cv.findContours(), cv.drawContours()\n\nContours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition.\n• For better accuracy, use binary images. So before finding contours, apply threshold or canny edge detection.\n• Since OpenCV 3.2, findContours() no longer modifies the source image but returns a modified image as the first of three return parameters.\n• In OpenCV, finding contours is like finding white object from black background. So remember, object to be found should be white and background should be black.\n\nLet's see how to find contours of a binary image:\n\nSee, there are three arguments in cv.findContours() function, first one is source image, second is contour retrieval mode, third is contour approximation method. And it outputs a modified image, the contours and hierarchy. contours is a Python list of all the contours in the image. Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.\n\nHow to draw the contours?\n\nTo draw the contours, cv.drawContours function is used. It can also be used to draw any shape provided you have its boundary points. Its first argument is source image, second argument is the contours which should be passed as a Python list, third argument is index of contours (useful when drawing individual contour. To draw all contours, pass -1) and remaining arguments are color, thickness etc.\n• To draw all the contours in an image:\n• To draw an individual contour, say 4th contour:\n• But most of the time, below method will be useful:\n\nThis is the third argument in cv.findContours function. What does it denote actually?\n\nAbove, we told that contours are the boundaries of a shape with same intensity. It stores the (x,y) coordinates of the boundary of a shape. But does it store all the coordinates ? That is specified by this contour approximation method.\n\nIf you pass cv.CHAIN_APPROX_NONE, all the boundary points are stored. But actually do we need all the points? For eg, you found the contour of a straight line. Do you need all the points on the line to represent that line? No, we need just two end points of that line. This is what cv.CHAIN_APPROX_SIMPLE does. It removes all redundant points and compresses the contour, thereby saving memory.\n\nBelow image of a rectangle demonstrate this technique. Just draw a circle on all the coordinates in the contour array (drawn in blue color). First image shows points I got with cv.CHAIN_APPROX_NONE (734 points) and second image shows the one with cv.CHAIN_APPROX_SIMPLE (only 4 points). See, how much memory it saves!!!"
    },
    {
        "link": "https://docs.opencv.org/4.x/d7/d4d/tutorial_py_thresholding.html",
        "document": "\n• In this tutorial, you will learn simple thresholding, adaptive thresholding and Otsu's thresholding.\n• You will learn the functions cv.threshold and cv.adaptiveThreshold.\n\nHere, the matter is straight-forward. For every pixel, the same threshold value is applied. If the pixel value is smaller than or equal to the threshold, it is set to 0, otherwise it is set to a maximum value. The function cv.threshold is used to apply the thresholding. The first argument is the source image, which should be a grayscale image. The second argument is the threshold value which is used to classify the pixel values. The third argument is the maximum value which is assigned to pixel values exceeding the threshold. OpenCV provides different types of thresholding which is given by the fourth parameter of the function. Basic thresholding as described above is done by using the type cv.THRESH_BINARY. All simple thresholding types are:\n\nSee the documentation of the types for the differences.\n\nThe method returns two outputs. The first is the threshold that was used and the second output is the thresholded image.\n\nThis code compares the different simple thresholding types:\n\nIn the previous section, we used one global value as a threshold. But this might not be good in all cases, e.g. if an image has different lighting conditions in different areas. In that case, adaptive thresholding can help. Here, the algorithm determines the threshold for a pixel based on a small region around it. So we get different thresholds for different regions of the same image which gives better results for images with varying illumination.\n\nIn addition to the parameters described above, the method cv.adaptiveThreshold takes three input parameters:\n\nThe adaptiveMethod decides how the threshold value is calculated:\n• cv.ADAPTIVE_THRESH_MEAN_C: The threshold value is the mean of the neighbourhood area minus the constant C.\n• cv.ADAPTIVE_THRESH_GAUSSIAN_C: The threshold value is a gaussian-weighted sum of the neighbourhood values minus the constant C.\n\nThe blockSize determines the size of the neighbourhood area and C is a constant that is subtracted from the mean or weighted sum of the neighbourhood pixels.\n\nThe code below compares global thresholding and adaptive thresholding for an image with varying illumination:\n\nIn global thresholding, we used an arbitrary chosen value as a threshold. In contrast, Otsu's method avoids having to choose a value and determines it automatically.\n\nConsider an image with only two distinct image values (bimodal image), where the histogram would only consist of two peaks. A good threshold would be in the middle of those two values. Similarly, Otsu's method determines an optimal global threshold value from the image histogram.\n\nIn order to do so, the cv.threshold() function is used, where cv.THRESH_OTSU is passed as an extra flag. The threshold value can be chosen arbitrary. The algorithm then finds the optimal threshold value which is returned as the first output.\n\nCheck out the example below. The input image is a noisy image. In the first case, global thresholding with a value of 127 is applied. In the second case, Otsu's thresholding is applied directly. In the third case, the image is first filtered with a 5x5 gaussian kernel to remove the noise, then Otsu thresholding is applied. See how noise filtering improves the result.\n\nThis section demonstrates a Python implementation of Otsu's binarization to show how it actually works. If you are not interested, you can skip this.\n\nSince we are working with bimodal images, Otsu's algorithm tries to find a threshold value (t) which minimizes the weighted within-class variance given by the relation:\n\nIt actually finds a value of t which lies in between two peaks such that variances to both classes are minimal. It can be simply implemented in Python as follows:\n• There are some optimizations available for Otsu's binarization. You can search and implement it."
    },
    {
        "link": "https://learnopencv.com/contour-detection-using-opencv-python-c",
        "document": "Using contour detection, we can detect the borders of objects, and localize them easily in an image. It is often the first step for many interesting applications, such as image-foreground extraction, simple-image segmentation, detection and recognition.\n\nSo let’s learn about contours and contour detection, using OpenCV, and see for ourselves how they can be used to build various applications.\n• Steps for finding and drawing contours using OpenCV.\n\nSome really cool applications have been built, using contours for motion detection or segmentation. Here are some examples:\n• Motion Detection: In surveillance video, motion detection technology has numerous applications, ranging from indoor and outdoor security environments, traffic control, behaviour detection during sports activities, detection of unattended objects, and even compression of video. In the figure below, see how detecting the movement of people in a video stream could be useful in a surveillance application. Notice how the group of people standing still in the left side of the image are not detected. Only those in motion are captured. Do refer to this paper to study this approach in detail.\n• Unattended object detection: Any unattended object in public places is generally considered as a suspicious object. An effective and safe solution could be: (Unattended Object Detection through Contour Formation using Background Subtraction).\n• Background / Foreground Segmentation: To replace the background of an image with another, you need to perform image-foreground extraction (similar to image segmentation). Using contours is one approach that can be used to perform segmentation. Refer to this post for more details. The following images show simple examples of such an application:\n\nWhen we join all the points on the boundary of an object, we get a contour. Typically, a specific contour refers to boundary pixels that have the same color and intensity. OpenCV makes it really easy to find and draw contours in images. It provides two simple functions:\n\nAlso, it has two different algorithms for contour detection:\n\nWe will cover these in detail, in the examples below. The following figure shows how these algorithms can detect the contours of simple objects.\n\nNow that you have been introduced to contours, let’s discuss the steps involved in their detection.\n\nSteps for Detecting and Drawing Contours in OpenCV\n\nOpenCV makes this a fairly simple task. Just follow these steps:\n• Read the Image and convert it to Grayscale Format\n\nRead the image and convert the image to grayscale format. Converting the image to grayscale is very important as it prepares the image for the next step. Converting the image to a single channel grayscale image is important for thresholding, which in turn is necessary for the contour detection algorithm to work properly.\n\nWhile finding contours, first always apply binary thresholding or Canny edge detection to the grayscale image. Here, we will apply binary thresholding.\n\nThis converts the image to black and white, highlighting the objects-of-interest to make things easy for the contour-detection algorithm. Thresholding turns the border of the object in the image completely white, with all pixels having the same intensity. The algorithm can now detect the borders of the objects from these white pixels.\n\nNote: The black pixels, having value 0, are perceived as background pixels and ignored.\n\nAt this point, one question may arise. What if we use single channels like R (red), G (green), or B (blue) instead of grayscale (thresholded) images? In such a case, the contour detection algorithm will not work well. As we discussed previously, the algorithm looks for borders, and similar intensity pixels to detect the contours. A binary image provides this information much better than a single (RGB) color channel image. In a later portion of the blog, we have resultant images when using only a single R, G, or B channel instead of grayscale and thresholded images.\n\nUse the function to detect the contours in the image.\n\nOnce contours have been identified, use the function to overlay the contours on the original RGB image.\n\nThe above steps will make much more sense, and become even clearer when we will start to code.\n\nStart by importing OpenCV, and reading the input image.\n\nWe assume that the image is inside the input folder of the current project directory. The next step is to convert the image into a grayscale image (single channel format).\n\nNote: All the C++ code is contained within the main() function.\n\nNext, use the function to convert the original RGB image to a grayscale image.\n\nNow, use the function to apply a binary threshold to the image. Any pixel with a value greater than 150 will be set to a value of 255 (white). All remaining pixels in the resulting image will be set to 0 (black). The threshold value of 150 is a tunable parameter, so you can experiment with it.\n\nAfter thresholding, visualize the binary image, using the function as shown below.\n\nCheck out the image below! It is a binary representation of the original RGB image. You can clearly see how the pen, the borders of the tablet and the phone are all white. The contour algorithm will consider these as objects, and find the contour points around the borders of these white objects.\n\nNote how the background is completely black, including the backside of the phone. Such regions will be ignored by the algorithm. Taking the white pixels around the perimeter of each object as similar-intensity pixels, the algorithm will join them to form a contour based on a similarity measure.\n\nNow, let’s find and draw the contours, using the method.\n\nStart with the function. It has three required arguments, as shown below. For optional arguments, please refer to the documentation page here.\n• : The binary input image obtained in the previous step.\n• : This is the contour-retrieval mode. We provided this as , which means the algorithm will retrieve all possible contours from the binary image. More contour retrieval modes are available, we will be discussing them too. You can learn more details on these options here.\n• : This defines the contour-approximation method. In this example, we will use .Though slightly slower than , we will use this method here tol store ALL contour points.\n\nIt’s worth emphasizing here that refers to the type of contours that will be retrieved, while refers to which points within a contour are stored. We will be discussing both in more detail below.\n\nIt is easy to visualize and understand results from different methods on the same image.\n\nIn the code samples below, we therefore make a copy of the original image and then demonstrate the methods (not wanting to edit the original).\n\nNext, use the function to overlay the contours on the RGB image. This function has four required and several optional arguments. The first four arguments below are required. For the optional arguments, please refer to the documentation page here.\n• : This is the input RGB image on which you want to draw the contour.\n• : Indicates the obtained from the function.\n• : The pixel coordinates of the contour points are listed in the obtained contours. Using this argument, you can specify the index position from this list, indicating exactly which contour point you want to draw. Providing a negative value will draw all the contour points.\n• : This indicates the color of the contour points you want to draw. We are drawing the points in green.\n• : This is the thickness of contour points.\n\nExecuting the above code will produce and display the image shown below. We also save the image to disk.\n\nThe following figure shows the original image (on the left), as well as the original image with the contours overlaid (on the right).\n\nAs you can see in the above figure, the contours produced by the algorithm do a nice job of identifying the boundary of each object. However, if you look closely at the phone, you will find that it contains more than one contour. Separate contours have been identified for the circular areas associated with the camera lens and light. There are also ‘secondary’ contours, along portions of the edge of the phone.\n\nKeep in mind that the accuracy and quality of the contour algorithm is heavily dependent on the quality of the binary image that is supplied (look at the binary image in the previous section again, you can see the lines associated with these secondary contours). Some applications require high quality contours. In such cases, experiment with different thresholds when creating the binary image, and see if that improves the resulting contours.\n\nThere are other approaches that can be used to eliminate unwanted contours from the binary maps prior to contour generation. You can also use more advanced features associated with the contour algorithm that we will be discussing here.\n\nJust to get an idea, the following are some results when using red, green and blue channels separately, while detecting contours. We discussed this in the contour detection steps previously. The following are the Python and C++ code for the same image as above.\n\nThe following figure shows the contour detection results for all the three separate color channels.\n\nIn the above image we can see that the contour detection algorithm is not able to find the contours properly. This is because it is not able to detect the borders of the objects properly, and also the intensity difference between the pixels is not well defined. This is the reason we prefer to use a grayscale, and binary thresholded image for detecting contours.\n\nLet’s find out now how the algorithm works and what makes it different from the algorithm.\n\nHere’s the code for it:\n\nThe only difference here is that we specify the for as instead of .\n\nThe algorithm compresses horizontal, vertical, and diagonal segments along the contour and leaves only their end points. This means that any of the points along the straight paths will be dismissed, and we will be left with only the end points. For example, consider a contour, along a rectangle. All the contour points, except the four corner points will be dismissed. This method is faster than the because the algorithm does not store all the points, uses less memory, and therefore, takes less time to execute.\n\nThe following image shows the results.\n\nIf you observe closely, there are almost no differences between the outputs of CHAIN_APPROX_NONE and CHAIN_APPROX_SIMPLE.\n\nNow, why is that?\n\nThe credit goes to the function. Although the method typically results in fewer points, the function automatically connects adjacent points, joining them even if they are not in the list.\n\nSo, how do we confirm that the algorithm is actually working?\n• The most straightforward way is to loop over the contour points manually, and draw a circle on the detected contour coordinates, using OpenCV.\n• Also, we use a different image that will actually help us visualize the results of the algorithm.\n\nThe following code uses the above image to visualize the effect of the algorithm. Almost everything is the same as in the previous code example, except the two additional for loops and some variable names.\n• The first loop cycles over each contour area present in the list.\n• The second loops over each of the coordinates in that area.\n• We then draw a green circle on each coordinate point, using the function from OpenCV.\n• Finally, we visualize the results and save it to disk.\n\nExecuting the code above, produces the following result:\n\nObserve the output image, which is on the right-hand side in the above figure. Note that the vertical and horizontal sides of the book contain only four points at the corners of the book. Also observe that the letters and bird are indicated with discrete points and not line segments.\n\nHierarchies denote the parent-child relationship between contours. You will see how each contour-retrieval mode affects contour detection in images, and produces hierarchical results.\n\nObjects detected by contour-detection algorithms in an image could be:\n• Single objects scattered around in an image (as in the first example), or\n• Objects and shapes inside one another\n\nIn most cases, where a shape contains more shapes, we can safely conclude that the outer shape is a parent of the inner shape.\n\nTake a look at the following figure, it contains several simple shapes that will help demonstrate contour hierarchies.\n\nNow see below figure, where the contours associated with each shape in Figure 10 have been identified. Each of the numbers in Figure 11 have a significance.\n• All the individual numbers, i.e., 1, 2, 3, and 4 are separate objects, according to the contour hierarchy and parent-child relationship.\n• We can say that the 3a is a child of 3. Note that 3a represents the interior portion of contour 3.\n• Contours 1, 2, and 4 are all parent shapes, without any associated child, and their numbering is thus arbitrary. In other words, contour 2 could have been labeled as 1 and vice-versa.\n\nYou’ve seen that the function returns two outputs: The contours list, and the hierarchy. Let’s now understand the contour hierarchy output in detail.\n\nThe contour hierarchy is represented as an array, which in turn contains arrays of four values. It is represented as:\n\nSo, what do all these values mean?\n\n: Denotes the next contour in an image, which is at the same hierarchical level. So,\n• For contour 1, the next contour at the same hierarchical level is 2. Here, will be 2.\n• Accordingly, contour 3 has no contour at the same hierarchical level as itself. So, it’s value will be -1.\n\n: Denotes the previous contour at the same hierarchical level. This means that contour 1 will always have its value as -1.\n\n: Denotes the first child contour of the contour we are currently considering.\n• Contours 1 and 2 have no children at all. So, the index values for their will be -1.\n• But contour 3 has a child. So, for contour 3, the position value will be the index position of 3a.\n\n: Denotes the parent contour’s index position for the current contour.\n• Contours 1 and 2, as is obvious, do not have any contour.\n• For the contour 3a, its is going to be contour 3\n• For contour 4, the parent is contour 3a\n\nThe above explanations make sense, but how do we actually visualize these hierarchy arrays? The best way is to:\n• Use a simple image with lines and shapes like the previous image\n• Detect the contours and hierarchies, using different retrieval modes\n• Then print the values to visualize them\n\nThus far, we used one specific retrieval technique, to find and draw contours, but there are three more contour-retrieval techniques in OpenCV, namely, , and .\n\nSo let’s now use the image in Figure 10 to review each of these four methods, along with their associated code to get the contours.\n\nThe following code reads the image from disk, converts it to grayscale, and applies binary thresholding.\n\nThe contour retrieval method does not create any parent child relationship between the extracted contours. So, for all the contour areas that are detected, the and index position values are always -1.\n\nAll the contours will have their corresponding and contours as discussed above.\n\nSee how the method is implemented in code.\n\nExecuting the above code produces the following output:\n\nYou can clearly see that the 3rd and 4th index positions of all the detected contour areas are -1, as expected.\n\nThe contour retrieval method is a really interesting one. It only detects the parent contours, and ignores any child contours. So, all the inner contours like 3a and 4 will not have any points drawn on them.\n\nThe above code produces the following output:\n\nThe above output image shows only the points drawn on contours 1, 2, and 3. Contours 3a and 4 are omitted as they are child contours.\n\nUnlike retrieves all the contours in an image. Along with that, it also applies a 2-level hierarchy to all the shapes or objects in the image.\n• All the outer contours will have hierarchy level 1\n• All the inner contours will have hierarchy level 2\n\nBut what if we have a contour inside another contour with hierarchy level 2? Just like we have contour 4 after contour 3a.\n• Again, contour 4 will have hierarchy level 1.\n• If there are any contours inside contour 4, they will have hierarchy level 2.\n\nIn the following image, the contours have been numbered according to their hierarchy level, as explained above.\n\nThe above image shows the hierarchy level as HL-1 or HL-2 for levels 1 and 2 respectively. Now, let us take a look at the code and the output hierarchy array also.\n\nExecuting the above code produces the following output:\n\nHere, we see that all the , , , and relationships are maintained, according to the contour-retrieval method, as all the contours are detected. As expected, the of the first contour area is -1. And the contours which do not have any , also have the value -1\n\nJust like also retrieves all the contours. It also creates a complete hierarchy, with the levels not restricted to 1 or 2. Each contour can have its own hierarchy, in line with the level it is on, and the corresponding parent-child relationship that it has.\n\nFrom the above figure, it is clear that:\n• Contours 1, 2, and 3 are at the same level, that is level 0.\n• Contour 3a is present at hierarchy level 1, as it is a child of contour 3.\n• Contour 4 is a new contour area, so its hierarchy level is 2.\n\nThe following code uses mode to retrieve contours.\n\nExecuting the above code produces the following output:\n\nFinally, let’s look at the complete image with all the contours drawn when using mode.\n\nAll the contours are drawn as expected, and the contour areas are clearly visible. You also infer that contours 3 and 3a are two separate contours, as they have different contour boundaries and areas. At the same time, it is very evident that contour 3a is a child of contour 3.\n\nNow that you are familiar with all the contour algorithms available in OpenCV, along with their respective input parameters and configurations, go experiment and see for yourself how they work.\n\nIt’s not enough to know the contour-retrieval methods. You should also be aware of their relative processing time. The following table compares the runtime for each method discussed above.\n\nSome interesting conclusions emerge from the above table:\n• and take the least amount of time to execute, since does not define any hierarchy and only retrieves the parent contours\n• takes the second highest time to execute. It retrieves all the contours and defines a two-level hierarchy.\n• takes the maximum time to execute for it retrieves all the contours, and defines the independent hierarchy level for each parent-child relationship as well.\n\nAlthough the above times may not seem significant, it is important to be aware of the differences for applications that may require a significant amount of contour processing. It is also worth noting that this processing time may vary, depending to an extent on the contours they extract, and the hierarchy levels they define.\n\nSo far, all the examples we explored seemed interesting, and their results encouraging. However, there are cases where the contour algorithm might fail to deliver meaningful and useful results. Let’s consider such an example too.\n• When the objects in an image are strongly contrasted against their background, you can clearly identify the contours associated with each object. But what if you have an image, like Figure 16 below. It not only has a bright object (puppy), but also a background cluttered with the same value (brightness) as the object of interest (puppy). You find that the contours in the right-hand image are not even complete. Also, there are multiple unwanted contours standing out in the background area.\n• Contour detection can also fail, when the background of the object in the image is full of lines.\n\nIf you think that you have learned something interesting in this article and would like to expand your knowledge, then you may like the Computer Vision 1 course offered by OpenCV. This is a great course to get started with OpenCV and Computer Vision which will be very hands-on and perfect to get you started and up to speed with OpenCV. The best part, you can take it in either Python or C++, whichever you choose. You can visit the course page here to know more about it.\n\nYou started with contour detection, and learned to implement that in OpenCV. Saw how applications use contours for mobility detection and segmentation. Next, we demonstrated the use of four different retrieval modes and two contour-approximation methods. You also learned to draw contours. We concluded with a discussion of contour hierarchies, and how different contour-retrieval modes affect the drawing of contours on an image.\n• The contour-detection algorithms in OpenCV work very well, when the image has a dark background and a well-defined object-of-interest.\n• But when the background of the input image is cluttered or has the same pixel intensity as the object-of-interest, the algorithms don’t fare so well.\n\nYou have all the code here, why not experiment with different images now! Try images containing varied shapes, and experiment with different threshold values. Also, explore different retrieval modes, using test images that contain nested contours. This way, you can fully appreciate the hierarchical relationships between objects."
    },
    {
        "link": "https://bhavikjikadara.medium.com/image-processing-using-opencv-a-step-by-step-guide-e589b0acbbf3",
        "document": "Image processing is a crucial part of modern fields like AI, computer vision, and robotics. OpenCV, a powerful open-source library, allows developers to handle complex image tasks with ease. This blog will guide you through essential image processing techniques using Python, covering everything from reading and displaying images, converting color spaces, and resizing images, to more advanced tasks like edge detection, contour detection, and thresholding. With hands-on examples, you’ll learn to manipulate and enhance images effortlessly.\n\nOpenCV is a widely used open-source computer vision library that allows developers to manipulate images and video streams with minimal effort. It’s the go-to solution for many tasks such as image recognition, filtering, edge detection, and more. OpenCV is also cross-platform, making it a perfect choice for large-scale AI and machine-learning applications.\n\nBefore we start, let’s make sure OpenCV is installed. You can easily install it using :"
    }
]