[
    {
        "link": "https://docs.gitlab.com/runner/install",
        "document": "GitLab Runner runs the CI/CD jobs defined in GitLab. GitLab Runner can run as a single binary and has no language-specific requirements.\n\nFor security and performance reasons, install GitLab Runner on a machine separate from the machine that hosts your GitLab instance.\n\nYou can install GitLab Runner on:\n\nBleeding-edge binaries are also available.\n\nTo use a different operating system, ensure the operating system can compile a Go binary.\n\nYou can install GitLab Runner with:\n\nGitLab Runner is available for the following architectures:\n\nThe system requirements for GitLab Runner depend on the:\n• Number of developers expected to work in parallel\n\nFor more information about the machine types available for GitLab.com, see GitLab-hosted runners.\n\nA GitLab Runner binary compliant with FIPS 140-2 is available for Red Hat Enterprise Linux (RHEL) distributions and the AMD64 architecture. Support for other distributions and architectures is proposed in issue 28814.\n\nThis binary is built with the Red Hat Go compiler and calls into a FIPS 140-2 validated cryptographic library. A UBI-8 minimal image is used as the base for creating the GitLab Runner FIPS image.\n\nFor more information about using FIPS-compliant GitLab Runner in RHEL, see Switching RHEL to FIPS mode."
    },
    {
        "link": "https://about.gitlab.com/blog/2016/03/01/gitlab-runner-with-docker",
        "document": "Build software faster with AI at every stage of development"
    },
    {
        "link": "https://about.gitlab.com/blog/gitlab-runner-with-docker",
        "document": "Build software faster with AI at every stage of development"
    },
    {
        "link": "https://forum.gitlab.com/t/configure-gitlab-runner-ci-on-my-private-gitlab-server/39167",
        "document": "I want to configure GitLab runner for my Android applications, to run CI jobs, on my private GitLab server, but with no success so far.\n\n I have created a very simple .gitlab-ci.yml file, which uses a local docker image that I have created.\n• I tested building applications with the docker and it worked perfectly.\n\n Also, I added a pull policy in my /etc/gitlab-runner/config.yoml file to never so, if I understand the documentation correctly, should prevent the runner from searching the image in DockerHub.\n\nI would really appreciate any help that you guys can give me, I’m really stuck.\n\n thank you and I hope someday I will be able to help someone else :)).\n\nThis is a screenshot of the error that I see in my GildLab Jobs:"
    },
    {
        "link": "https://stackoverflow.com/questions/49090675/how-can-i-test-gitlab-ci-yml",
        "document": "This approach is no longer supported since was removed in 16.0.\n\nI recommend checking out some of the more recent answers, and vote up the ones that work for you.\n\nIf you want to go beyond mere linting and actually run your CI script, you can do so using . Here's how to do it.\n\nWhen I run the above locally, I get the following output:"
    },
    {
        "link": "https://docs.gitlab.com/ci/yaml/yaml_optimization",
        "document": "You can reduce complexity and duplicated configuration in your GitLab CI/CD configuration files by using:\n• YAML-specific features like anchors ( ), aliases ( ), and map merging ( ). Read more about the various YAML features.\n• The keyword, which is more flexible and readable. You should use where possible.\n\nYAML has a feature called ‘anchors’ that you can use to duplicate content across your document.\n\nUse anchors to duplicate or inherit properties. Use anchors with hidden jobs to provide templates for your jobs. When there are duplicate keys, the latest included key wins, overriding the other keys.\n\nIn certain cases (see YAML anchors for scripts), you can use YAML anchors to build arrays with multiple components defined elsewhere. For example:\n\nYou can’t use YAML anchors across multiple files when using the keyword. Anchors are only valid in the file they were defined in. To reuse configuration from different YAML files, use tags or the keyword.\n\nThe following example uses anchors and map merging. It creates two jobs, and , that inherit the configuration, each with their own custom defined:\n\nsets up the name of the anchor ( ), means “merge the given hash into the current one,” and includes the named anchor ( again). The expanded version of this example is:\n\nYou can use anchors to define two sets of services. For example, and share the defined in , but use different , defined in and :\n\nYou can see that the hidden jobs are conveniently used as templates, and overwrites .\n\nYou can use YAML anchors with script, , and to use predefined commands in multiple jobs:\n\nYou can use the keyword to reuse configuration in multiple jobs. It is similar to YAML anchors, but simpler and you can use with .\n\nsupports multi-level inheritance. You should avoid using more than three levels, due to the additional complexity, but you can use as many as eleven. The following example has two levels of inheritance:\n\nTo exclude a key from the extended content, you must assign it to , for example:\n\nUse and together\n\nTo reuse configuration from different configuration files, combine and .\n\nIn the following example, a is defined in the file. Then, in the file, refers to the contents of the :\n\nYou can use to merge hashes but not arrays. The algorithm used for merge is “closest scope wins”. When there are duplicate keys, GitLab performs a reverse deep merge based on the keys. Keys from the last member always override anything defined on other levels. For example:\n\nThe result is this job:\n\nIn this example:\n• does not merge, but overwrites . You can use YAML anchors to merge arrays.\n\nUse the custom YAML tag to select keyword configuration from other job sections and reuse it in the current section. Unlike YAML anchors, you can use tags to reuse configuration from included configuration files as well.\n\nIn the following example, a and an from two different locations are reused in the job:\n\nIn the following example, reuses all the variables in , while selects a specific variable and reuses it as a new variable.\n\nThere’s a known issue when using tags with the keyword.\n\nYou can nest tags up to 10 levels deep in , , and sections. Use nested tags to define reusable sections when building more complex scripts. For example:\n\nIn this example, the job runs all three commands.\n\nThe pipeline editor supports tags. However, the schema rules for custom YAML tags like might be treated as invalid by your editor by default. You can configure some editors to accept tags. For example:\n• None In VS Code, you can set to parse in your file:\n• None In Sublime Text, if you are using the package, you can set in your user settings:"
    },
    {
        "link": "https://docs.gitlab.com/ci/yaml",
        "document": "This document lists the configuration options for the GitLab file. This file is where you define the CI/CD jobs that make up your pipeline.\n• If you are already familiar with basic CI/CD concepts, try creating your own file by following a tutorial that demonstrates a simple or complex pipeline.\n• For a collection of examples, see GitLab CI/CD examples.\n• To view a large file used in an enterprise, see the file for .\n\nWhen you are editing your file, you can validate it with the CI Lint tool.\n\nIf you are editing content on this page, follow the instructions for documenting keywords.\n• None The names and order of the pipeline stages.\n• None Override a set of commands that are executed after job. Allow job to fail. A failed job does not cause the pipeline to fail. List of files and directories to attach to a job on success. Override a set of commands that are executed before job. List of files that should be cached between subsequent runs. Use configuration from DAST profiles on a job level. Restrict which artifacts are passed to a specific job by providing a list of jobs to fetch artifacts from. Name of an environment to which the job deploys. Configuration entries that this job inherits from. Authenticate with third party services using identity federation. Defines if a job can be canceled when made redundant by a newer run. Upload the result of a job to use with GitLab Pages. How many instances of a job should be run in parallel. When and how many times a job can be auto-retried in case of a failure. List of conditions to evaluate and determine selected attributes of a job, and whether or not it’s created. Shell script that is executed by a runner. Run configuration that is executed by a runner. The CI/CD secrets the job needs. List of tags that are used to select a runner. Define a custom job-level timeout that takes precedence over the project-wide setting.\n• None Define default CI/CD variables for all jobs in the pipeline.\n\nSome keywords are not defined in a job. These keywords control pipeline behavior or import additional pipeline configuration.\n\nYou can set global defaults for some keywords. Each default keyword is copied to every job that doesn’t already have it defined. If the job already has a keyword defined, that default is not used.\n\nSupported values: These keywords can have custom defaults:\n• , though due to issue 213634 this keyword has no effect.\n\nIn this example:\n• and are the default keywords for all jobs in the pipeline.\n• The job does not have or defined, so it uses the defaults of and .\n• The job does not have defined, but it does have explicitly defined. It uses the default , but ignores the default and uses the defined in the job.\n• Control inheritance of default keywords in jobs with .\n• Global defaults are not passed to downstream pipelines, which run independently of the upstream pipeline that triggered the downstream pipeline.\n\nUse to include external YAML files in your CI/CD configuration. You can split one long file into multiple files to increase readability, or reduce duplication of the same configuration in multiple places.\n\nYou can also store template files in a central repository and include them in projects.\n• Merged with those in the file.\n• Always evaluated first and then merged with the content of the file, regardless of the position of the keyword.\n\nThe time limit to resolve all files is 30 seconds.\n• Only certain CI/CD variables can be used with keywords.\n• Use merging to customize and override included CI/CD configurations with local\n• You can override included configuration by having the same job name or global keyword in the file. The two configurations are merged together, and the configuration in the file takes precedence over the included configuration.\n• If you rerun a:\n• Job, the files are not fetched again. All jobs in a pipeline use the configuration fetched when the pipeline was created. Any changes to the source files do not affect job reruns.\n• Pipeline, the files are fetched again. If they changed after the last pipeline run, the new pipeline uses the changed configuration.\n• You can have up to 150 includes per pipeline by default, including nested. Additionally:\n• In GitLab 16.0 and later users on GitLab Self-Managed can change the maximum includes value.\n• In GitLab 15.10 and later you can have up to 150 includes. In nested includes, the same file can be included multiple times, but duplicated includes count towards the limit.\n• From GitLab 14.9 to GitLab 15.9, you can have up to 100 includes. The same file can be included multiple times in nested includes, but duplicates are ignored.\n\nUse to add a CI/CD component to the pipeline configuration.\n\nSupported values: The full address of the CI/CD component, formatted as .\n\nUse to include a file that is in the same repository and branch as the configuration file containing the keyword. Use instead of symbolic links.\n• The YAML file must have the extension or .\n• You can use and wildcards in the file path.\n• You can use certain CI/CD variables.\n\nYou can also use shorter syntax to define the path:\n• The file and the local file must be on the same branch.\n• configuration is always evaluated based on the location of the file containing the keyword, not the project running the pipeline. If a nested is in a configuration file in a different project, checks that other project for the file.\n\nTo include files from another private project on the same GitLab instance, use and .\n• A full file path, or array of file paths, relative to the root directory ( ). The YAML files must have the or extension.\n• : Optional. The ref to retrieve the file from. Defaults to the of the project when not specified.\n• You can use certain CI/CD variables.\n\nYou can also specify a :\n• configuration is always evaluated based on the location of the file containing the keyword, not the project running the pipeline. If a nested is in a configuration file in a different project, checks that other project for the file.\n• When the pipeline starts, the file configuration included by all methods is evaluated. The configuration is a snapshot in time and persists in the database. GitLab does not reflect any changes to the referenced file configuration until the next pipeline starts.\n• When you include a YAML file from another private project, the user running the pipeline must be a member of both projects and have the appropriate permissions to run pipelines. A error may be displayed if the user does not have access to any of the included files.\n• Be careful when including another project’s CI/CD configuration file. No pipelines or notifications trigger when CI/CD configuration files change. From a security perspective, this is similar to pulling a third-party dependency. For the , consider:\n• Using a specific SHA hash, which should be the most stable option. Use the full 40-character SHA hash to ensure the desired commit is referenced, because using a short SHA hash for the might be ambiguous.\n• Applying both protected branch and protected tag rules to the in the other project. Protected tags and branches are more likely to pass through change management before changing.\n\nUse with a full URL to include a file from a different location.\n• Authentication with the remote URL is not supported.\n• The YAML file must have the extension or .\n• You can use certain CI/CD variables.\n• All nested includes are executed without context as a public user, so you can only include public projects or templates. No variables are available in the section of nested includes.\n• Be careful when including another project’s CI/CD configuration file. No pipelines or notifications trigger when the other project’s files change. From a security perspective, this is similar to pulling a third-party dependency. To verify the integrity of the included file, consider using the keyword. If you link to another GitLab project you own, consider the use of both protected branches and protected tags to enforce change management rules.\n• All templates can be viewed in . Not all templates are designed to be used with , so check template comments before using one.\n• You can use certain CI/CD variables.\n• All nested includes are executed without context as a public user, so you can only include public projects or templates. No variables are available in the section of nested includes.\n\nUse to set the values for input parameters when the included configuration uses and is added to the pipeline.\n\nIn this example:\n• The configuration contained in is added to the pipeline, with a input set to a value of for the included configuration.\n• If the included configuration file uses , the input value must match the defined type.\n• If the included configuration file uses , the input value must match one of the listed options.\n\nYou can use with to conditionally include other configuration files.\n\nIn this example, if the variable is:\n• , the configuration is included in the pipeline.\n• Not or does not exist, the configuration is not included in the pipeline.\n• Examples of using with:\n\nUse with to specifiy a SHA256 hash of the included remote file. If does not match the actual content, the remote file is not processed and the pipeline fails.\n\nUse to define stages that contain groups of jobs. Use in a job to configure the job to run in a specific stage.\n\nIf is not defined in the file, the default pipeline stages are:\n\nThe order of the items in defines the execution order for jobs:\n• Jobs in the same stage run in parallel.\n• Jobs in the next stage run after the jobs from the previous stage complete successfully.\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nIn this example:\n• All jobs in execute in parallel.\n• If all jobs in succeed, the jobs execute in parallel.\n• If all jobs in succeed, the jobs execute in parallel.\n• If all jobs in succeed, the pipeline is marked as .\n\nIf any job fails, the pipeline is marked as and jobs in later stages do not start. Jobs in the current stage are not stopped and continue to run.\n• If a job does not specify a , the job is assigned the stage.\n• If a stage is defined but no jobs use it, the stage is not visible in the pipeline, which can help compliance pipeline configurations:\n• Stages can be defined in the compliance configuration but remain hidden if not used.\n• The defined stages become visible when developers use them in job definitions.\n• To make a job start earlier and ignore the stage order, use the keyword.\n\nYou can use some predefined CI/CD variables in configuration, but not variables that are only defined when jobs start.\n\nUse to configure the behavior of the auto-cancel redundant pipelines feature.\n• : Cancel the pipeline, but only if no jobs with have started yet. Default when not defined.\n• : Do not auto-cancel any jobs.\n\nIn this example:\n• When a new commit is pushed to a branch, GitLab creates a new pipeline and and start.\n• If a new commit is pushed to the branch before the jobs complete, only is canceled.\n\nUse to configure which jobs should be canceled as soon as one job fails.\n• : Cancel the pipeline and all running jobs as soon as one job fails.\n• : Do not auto-cancel any jobs.\n\nIn this example, if fails, is canceled if it is still running and does not start.\n\nYou can use in to define a name for pipelines.\n\nAll pipelines are assigned the defined name. Any leading or trailing spaces in the name are removed.\n\nA configuration with different pipeline names depending on the pipeline conditions:\n• If the name is an empty string, the pipeline is not assigned a name. A name consisting of only CI/CD variables could evaluate to an empty string if all the variables are also empty.\n• become default variables available in all jobs, including jobs which forward variables to downstream pipelines by default. If the downstream pipeline uses the same variable, the variable is overwritten by the upstream variable value. Be sure to either:\n• Use a unique variable name in every project’s pipeline configuration, like .\n• Use in the trigger job and list the exact variables you want to forward to the downstream pipeline.\n\nThe keyword in is similar to defined in jobs, but controls whether or not a whole pipeline is created.\n\nWhen no rules evaluate to true, the pipeline does not run.\n\nSupported values: You can use some of the same keywords as job-level :\n• , can only be or when used with .\n\nIn this example, pipelines run if the commit title (first line of the commit message) does not end with and the pipeline is for either:\n• If your rules match both branch pipelines (other than the default branch) and merge request pipelines, duplicate pipelines can occur.\n• , , and are not supported in , but do not cause a syntax violation. Though they have no effect, do not use them in as it could cause syntax failures in the future. See issue 436473 for more details.\n\nYou can use in to define variables for specific pipeline conditions.\n\nWhen the condition matches, the variable is created and can be used by all jobs in the pipeline. If the variable is already defined at the top level as a default variable, the variable takes precedence and overrides the default variable.\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ).\n• The value must be a string.\n\nWhen the branch is the default branch:\n\nWhen the branch is :\n• job1’s is , and is .\n• job2’s is , and is .\n\nWhen the branch is something else:\n• become default variables available in all jobs, including jobs which forward variables to downstream pipelines by default. If the downstream pipeline uses the same variable, the variable is overwritten by the upstream variable value. Be sure to either:\n• Use unique variable names in every project’s pipeline configuration, like .\n• Use in the trigger job and list the exact variables you want to forward to the downstream pipeline.\n\nUse to configure the behavior of the or the features.\n\nIn this example, is set to and is set to for all jobs by default. But if a pipeline runs for a protected branch, the rule overrides the default with and . For example, if a pipeline is running for:\n• A non-protected branch and a new commit is pushed, continues to run and is canceled.\n• A protected branch and a new commit is pushed, both and continue to run.\n\nSome keywords must be defined in a header section of a YAML configuration file. The header must be at the top of the file, separated from the rest of the configuration with .\n\nAdd a section to the header of a YAML file to configure the behavior of a pipeline when a configuration is added to the pipeline with the keyword.\n\nYou can use to define input parameters for the CI/CD configuration you intend to add to a pipeline with . Use to define the values to use when the pipeline runs.\n\nUse the inputs to customize the behavior of the configuration when included in CI/CD configuration.\n\nUse the interpolation format to reference the values outside of the header section. Inputs are evaluated and interpolated when the configuration is fetched during pipeline creation, but before the configuration is merged with the contents of the file.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n• Inputs are mandatory unless you use to set a default value.\n• Inputs expect strings unless you use to set a different input type.\n• A string containing an interpolation block must not exceed 1 MB.\n• The string inside an interpolation block must not exceed 1 KB.\n\nInputs are mandatory when included, unless you set a default value with .\n\nUse to have no default value.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nSupported values: A string representing the default value, or .\n\nIn this example:\n• is mandatory and must be defined.\n• is optional. If not defined, the value is .\n• is optional. If not defined, it has no value.\n• The pipeline fails with a validation error when the input:\n• Uses both and , but the default value is not one of the listed options.\n• Uses both and , but the default value does not match the regular expression.\n• Value does not match the .\n\nUse to give a description to a specific input. The description does not affect the behavior of the input and is only used to help users of the file understand the input.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nInputs can use to specify a list of allowed values for an input. The limit is 50 options per input.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nIn this example:\n• is mandatory and must be defined with one of the values in the list.\n• The pipeline fails with a validation error when:\n• The input uses both and , but the default value is not one of the listed options.\n• Any of the input options do not match the , which can be either or , but not when using .\n\nUse to specify a regular expression that the input must match.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nIn this example, inputs of or match the regular expression and pass validation. An input of does not match the regular expression and fails validation.\n• can only be used with a of , not or .\n• Do not enclose the regular expression with the character. For example, use , not .\n\nBy default, inputs expect strings. Use to set a different required type for inputs.\n\nKeyword type: Header keyword. must be declared at the top of the configuration file, in a header section.\n\nSupported values: Can be one of:\n• , to accept an array of inputs.\n• , to accept string inputs (default when not defined).\n• , to only accept or inputs.\n\nThe following topics explain how to use keywords to configure CI/CD pipelines.\n\nUse to define an array of commands to run last, after a job’s and sections complete. commands also run when:\n• The job is canceled while the or sections are still running.\n• The job fails with failure type of , but not other failure types.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nScripts you specify in execute in a new shell, separate from any or commands. As a result, they:\n• Have the current working directory set back to the default (according to the variables which define how the runner processes Git requests).\n• Don’t have access to changes done by commands defined in the or , including:\n• Changes outside of the working tree (depending on the runner executor), like software installed by a or script.\n• Have a separate timeout. For GitLab Runner 16.4 and later, this defaults to 5 minutes, and can be configured with the variable. In GitLab 16.3 and earlier, the timeout is hard-coded to 5 minutes.\n• Don’t affect the job’s exit code. If the section succeeds and the times out or fails, the job exits with code ( ).\n• There is a known issue with using CI/CD job tokens with . You can use a job token for authentication in commands, but the token immediately becomes invalid if the job is canceled. See issue for more details.\n\nFor jobs that time out:\n• commands do not execute by default.\n• You can configure timeout values to ensure runs by setting appropriate and values that don’t exceed the job’s timeout.\n• Use with to define a default array of commands that should run after all jobs.\n• You can configure a job to skip commands if the job is canceled.\n• Use color codes with to make job logs easier to review.\n• You can ignore errors in .\n\nUse to determine whether a pipeline should continue running when a job fails.\n• To let the pipeline continue running subsequent jobs, use .\n• To stop the pipeline from running subsequent jobs, use .\n\nWhen jobs are allowed to fail ( ) an orange warning ( ) indicates that a job failed. However, the pipeline is successful and the associated commit is marked as passed with no warnings.\n\nThis same warning is displayed when:\n• All other jobs in the stage are successful.\n• All other jobs in the pipeline are successful.\n\nThe default value for is:\n• for jobs that use inside .\n• in all other cases.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nIn this example, and run in parallel:\n• If fails, jobs in the stage do not start.\n• If fails, jobs in the stage can still start.\n• You can use as a subkey of .\n• If is set, the job is always considered successful, and later jobs with don’t start if this job fails.\n• You can use with a manual job to create a blocking manual job. A blocked pipeline does not run any jobs in later stages until the manual job is started and completes successfully.\n\nUse to control when a job should be allowed to fail. The job is for any of the listed exit codes, and false for any other exit code.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nUse to specify which files to save as job artifacts. Job artifacts are a list of files and directories that are attached to the job when it succeeds, fails, or always.\n\nThe artifacts are sent to GitLab after the job finishes. They are available for download in the GitLab UI if the size is smaller than the maximum artifact size.\n\nBy default, jobs in later stages automatically download all the artifacts created by jobs in earlier stages. You can control artifact download behavior in jobs with .\n\nWhen using the keyword, jobs can only download artifacts from the jobs defined in the configuration.\n\nJob artifacts are only collected for successful jobs by default, and artifacts are restored after caches.\n\nPaths are relative to the project directory ( ) and can’t directly link outside it.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of file paths, relative to the project directory.\n• You can use Wildcards that use glob patterns and:\n• In GitLab Runner 13.0 and later, .\n• For GitLab Pages job:\n• In GitLab 17.10 and later, the path is automatically appended to , so you don’t need to specify it again.\n• In GitLab 17.10 and later, when the path is not specified, the directory is automatically appended to .\n\nThis example creates an artifact with and all the files in the directory.\n• If not used with , the artifacts file is named , which becomes when downloaded.\n• To restrict which jobs a specific job fetches artifacts from, see .\n\nUse to prevent files from being added to an artifacts archive.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of file paths, relative to the project directory.\n• You can use Wildcards that use glob or patterns.\n\nThis example stores all files in , but not files located in subdirectories of .\n• Files matched by can be excluded using too.\n\nUse to specify how long job artifacts are stored before they expire and are deleted. The setting does not affect:\n• Artifacts from the latest job, unless keeping the latest job artifacts is disabled at the project level or instance-wide.\n\nAfter their expiry, artifacts are deleted hourly by default (using a cron job), and are not accessible anymore.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The expiry time. If no unit is provided, the time is in seconds. Valid values include:\n• The expiration time period begins when the artifact is uploaded and stored on GitLab. If the expiry time is not defined, it defaults to the instance wide setting.\n• To override the expiration date and protect artifacts from being automatically deleted:\n• Select Keep on the job page.\n• Set the value of to .\n• If the expiry time is too short, jobs in later stages of a long pipeline might try to fetch expired artifacts from earlier jobs. If the artifacts are expired, jobs that try to fetch them fail with a could not retrieve the needed artifacts error. Set the expiry time to be longer, or use in later jobs to ensure they don’t try to fetch expired artifacts.\n\nUse the keyword to expose job artifacts in the merge request UI.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• The name to display in the merge request UI for the artifacts download link. Must be combined with .\n• Artifacts are saved, but do not display in the UI if the values:\n• Define a directory, but do not end with . For example, works with , but does not.\n• Start with . For example, works with , but does not.\n• A maximum of 10 job artifacts per merge request can be exposed.\n• If a directory is specified and there is more than one file in the directory, the link is to the job artifacts browser.\n• If GitLab Pages is enabled, GitLab automatically renders the artifacts when the artifacts is a single file with one of these extensions:\n\nUse the keyword to define the name of the created artifacts archive. You can specify a unique name for every archive.\n\nIf not defined, the default name is , which becomes when downloaded.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• The name of the artifacts archive. CI/CD variables are supported. Must be combined with .\n\nTo create an archive with a name of the current job:\n• Use CI/CD variables to define the artifacts configuration\n\nUse to determine whether the job artifacts should be publicly available.\n\nWhen is (default), the artifacts in public pipelines are available for download by anonymous, guest, and reporter users.\n\nTo deny read access to artifacts in public pipelines for anonymous, guest, and reporter users, set to :\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default if not defined) or .\n\nUse to determine who can access the job artifacts from the GitLab UI or API. This option does not prevent you from forwarding artifacts to downstream pipelines.\n\nYou cannot use and in the same job.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default): Artifacts in a job in public pipelines are available for download by anyone, including anonymous, guest, and reporter users.\n• : Artifacts in the job are only available for download by users with the Developer role or higher.\n• : Artifacts in the job are not available for download by anyone.\n• affects all too, so you can also restrict access to artifacts for reports.\n\nUse to collect artifacts generated by included templates in jobs.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• See list of available artifacts reports types.\n• Combining reports in parent pipelines using artifacts from child pipelines is not supported. Track progress on adding support in this issue.\n• To be able to browse and download the report output files, include the keyword. This uploads and stores the artifact twice.\n• Artifacts created for are always uploaded, regardless of the job results (success or failure). You can use to set an expiration date for the artifacts.\n\nUse to add all Git untracked files as artifacts (along with the paths defined in ). ignores configuration in the repository’s , so matching artifacts in are included.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• or (default if not defined).\n\nUse to upload artifacts on job failure or despite the failure.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default): Upload artifacts only when the job succeeds.\n• : Upload artifacts only when the job fails.\n• : Always upload artifacts (except when jobs time out). For example, when uploading artifacts required to troubleshoot failing tests.\n• The artifacts created for are always uploaded, regardless of the job results (success or failure). does not change this behavior.\n\nUse to define an array of commands that should run before each job’s commands, but after artifacts are restored.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• Scripts you specify in are concatenated with any scripts you specify in the main . The combined scripts execute together in a single shell.\n• Using at the top level, but not in the section, is deprecated.\n• Use with to define a default array of commands that should run before the commands in all jobs.\n• Use color codes with to make job logs easier to review.\n\nUse to specify a list of files and directories to cache between jobs. You can only use paths that are in the local working copy.\n• By default, not shared between protected and unprotected branches.\n• Limited to a maximum of four different caches.\n\nYou can disable caching for specific jobs, for example to override:\n• The configuration for a job added with .\n\nFor more information about caches, see Caching in GitLab CI/CD.\n\nUse the keyword to choose which files or directories to cache.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of paths relative to the project directory ( ). You can use wildcards that use glob patterns:\n• In GitLab Runner 13.0 and later, .\n\nCache all files in that end in and the file:\n• The keyword includes files even if they are untracked or in your file.\n• See the common use cases for more examples.\n\nUse the keyword to give each cache a unique identifying key. All jobs that use the same cache key use the same cache, including in different pipelines.\n\nIf not set, the default key is . All jobs with the keyword but no share the cache.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• None If you use Windows Batch to run your shell scripts you must replace with . For example:\n• None The value can’t contain:\n• The character, or the equivalent URI-encoded .\n• Only the character (any number), or the equivalent URI-encoded .\n• None The cache is shared between jobs, so if you’re using different paths for different jobs, you should also set a different . Otherwise cache content can be overwritten.\n• You can specify a fallback cache key to use if the specified is not found.\n• You can use multiple cache keys in a single job.\n• See the common use cases for more examples.\n\nUse the keyword to generate a new key when one or two specific files change. lets you reuse some caches, and rebuild them less often, which speeds up subsequent pipeline runs.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of one or two file paths.\n\nThis example creates a cache for Ruby and Node.js dependencies. The cache is tied to the current versions of the and files. When one of these files changes, a new cache key is computed and a new cache is created. Any future job runs that use the same and with use the new cache, instead of rebuilding the dependencies.\n• The cache is a SHA computed from the most recent commits that changed each listed file. If neither file is changed in any commits, the fallback key is .\n\nUse to combine a prefix with the SHA computed for .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nFor example, adding a of causes the key to look like . If a branch changes , that branch has a new SHA checksum for . A new cache key is generated, and a new cache is created for that key. If is not found, the prefix is added to , so the key in the example would be .\n• If no file in is changed in any commits, the prefix is added to the key.\n\nUse to cache all files that are untracked in your Git repository. Untracked files include files that are:\n• Created, but not added to the checkout with .\n\nCaching untracked files can create unexpectedly large caches if the job downloads:\n• Dependencies, like gems or node modules, which are usually untracked.\n• Artifacts from a different job. Files extracted from the artifacts are untracked by default.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• None You can combine with to cache all untracked files, as well as files in the configured paths. Use to cache any specific files, including tracked files, or files that are outside of the working directory, and use to also cache all untracked files. For example: In this example, the job caches all untracked files in the repository, as well as all the files in . If there are untracked files in , they are covered by both keywords.\n\nUse to set a cache to be shared between protected and unprotected branches.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nUse to define when to save the cache, based on the status of the job.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• (default): Save the cache only when the job succeeds.\n• : Save the cache only when the job fails.\n\nThis example stores the cache whether or not the job fails or succeeds.\n\nTo change the upload and download behavior of a cache, use the keyword. By default, the job downloads the cache when the job starts, and uploads changes to the cache when the job ends. This caching style is the policy (default).\n\nTo set a job to only download the cache when the job starts, but never upload changes when the job finishes, use .\n\nTo set a job to only upload a cache when the job finishes, but never download the cache when the job starts, use .\n\nUse the policy when you have many jobs executing in parallel that use the same cache. This policy speeds up job execution and reduces load on the cache server. You can use a job with the policy to build the cache.\n\nMust be used with , or nothing is cached.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• You can use a variable to control a job’s cache policy.\n\nUse to specify a list of keys to try to restore cache from if there is no cache found for the . Caches are retrieved in the order specified in the section.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nUse with a custom regular expression to configure how code coverage is extracted from the job output. The coverage is shown in the UI if at least one line in the job output matches the regular expression.\n\nTo extract the code coverage value from the match, GitLab uses this smaller regular expression: .\n• An RE2 regular expression. Must start and end with . Must match the coverage number. May match surrounding text as well, so you don’t need to use a regular expression character group to capture the exact number. Because it uses RE2 syntax, all groups must be non-capturing.\n\nIn this example:\n• GitLab checks the job log for a match with the regular expression. A line like would match.\n• GitLab then checks the matched fragment to find a match to . The sample matching line above gives a code coverage of .\n• You can find regex examples in Code Coverage.\n• If there is more than one matched line in the job output, the last line is used (the first result of reverse search).\n• If there are multiple matches in a single line, the last match is searched for the coverage number.\n• If there are multiple coverage numbers found in the matched fragment, the first number is used.\n• Coverage output from child pipelines is not recorded or displayed. Check the related issue for more details.\n\nUse the keyword to specify a site profile and scanner profile to be used in a CI/CD configuration. Both profiles must first have been created in the project. The job’s stage must be .\n\nKeyword type: Job keyword. You can use only as part of a job.\n\nSupported values: One each of and .\n• Use to specify the site profile to be used in the job.\n• Use to specify the scanner profile to be used in the job.\n\nIn this example, the job extends the configuration added with the keyword to select a specific site profile and scanner profile.\n• Settings contained in either a site profile or scanner profile take precedence over those contained in the DAST template.\n\nUse the keyword to define a list of specific jobs to fetch artifacts from. The specified jobs must all be in earlier stages. You can also set a job to download no artifacts at all.\n\nWhen is not defined in a job, all jobs in earlier stages are considered dependent and the job fetches all artifacts from those jobs.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The names of jobs to fetch artifacts from.\n• An empty array ( ), to configure the job to not download any artifacts.\n\nIn this example, two jobs have artifacts: and . When is executed, the artifacts from are downloaded and extracted in the context of the build. The same thing happens for and artifacts from .\n\nThe job downloads artifacts from all previous jobs because of the stage precedence.\n• The job status does not matter. If a job fails or it’s a manual job that isn’t triggered, no error occurs.\n• If the artifacts of a dependent job are expired or deleted, then the job fails.\n• To fetch artifacts from a job in the same stage, you must use . You should not combine with in the same job.\n\nUse to define the environment that a job deploys to.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The name of the environment the job deploys to, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n• If you specify an and no environment with that name exists, an environment is created.\n\nSet a name for an environment.\n\nCommon environment names are , , and , but you can use any name.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The name of the environment the job deploys to, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A single URL, in one of these formats:\n• CI/CD variables, including predefined, project, group, instance, or variables defined in the file. You can’t use variables defined in a section.\n• After the job completes, you can access the URL by selecting a button in the merge request, environment, or deployment pages.\n\nClosing (stopping) environments can be achieved with the keyword defined under . It declares a different job that runs to close the environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• See for more details and an example.\n\nUse the keyword to specify how the job interacts with the environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: One of the following keywords:\n\nThe keyword specifies the lifetime of the environment. When an environment expires, GitLab automatically stops it.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A period of time written in natural language. For example, these are all equivalent:\n\nWhen the environment for is created, the environment’s lifetime is set to . Every time the review app is deployed, that lifetime is also reset to .\n\nThe keyword can be used for all environment actions except . Some actions can be used to reset the scheduled stop time for the environment. For more information, see Access an environment for preparation or verification purposes.\n\nUse the keyword to configure the dashboard for Kubernetes for an environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : A string specifying the GitLab agent for Kubernetes. The format is .\n• : A string representing the Kubernetes namespace. It needs to be set together with the keyword.\n• : A string representing the path to the Flux resource. This must be the full resource path. It needs to be set together with the and keywords.\n\nThis configuration sets up the job to deploy to the environment, associates the agent named with the environment, and configures the dashboard for Kubernetes for an environment with the namespace and the set to .\n• To use the dashboard, you must install the GitLab agent for Kubernetes and configure for the environment’s project or its parent group.\n• The user running the job must be authorized to access the cluster agent. Otherwise, it will ignore , and attributes.\n\nUse the keyword to specify the tier of the deployment environment.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: One of the following:\n• Environments created from this job definition are assigned a tier based on this value.\n• Existing environments don’t have their tier updated if this value is added later. Existing environments must have their tier updated via the Environments API.\n\nUse CI/CD variables to dynamically name environments.\n\nThe job is marked as a deployment to dynamically create the environment. is a CI/CD variable set by the runner. The variable is based on the environment name, but suitable for inclusion in URLs. If the job runs in a branch named , this environment would be accessible with a URL like .\n\nThe common use case is to create dynamic environments for branches and use them as review apps. You can see an example that uses review apps at https://gitlab.com/gitlab-examples/review-apps-nginx/.\n\nUse to reuse configuration sections. It’s an alternative to YAML anchors and is a little more flexible and readable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The name of another job in the pipeline.\n• A list (array) of names of other jobs in the pipeline.\n\nIn this example, the job uses the configuration from the template job. When creating the pipeline, GitLab:\n• Merges the content with the job.\n• Doesn’t merge the values of the keys.\n\nThe combined configuration is equivalent to these jobs:\n• You can use multiple parents for .\n• The keyword supports up to eleven levels of inheritance, but you should avoid using more than three levels.\n• In the example above, is a hidden job, but you can extend configuration from regular jobs as well.\n• Use to reuse configuration from included configuration files.\n\nUse to specify lists of commands to execute on the runner at certain stages of job execution, like before retrieving the Git repository.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A hash of hooks and their commands. Available hooks: .\n\nUse to specify a list of commands to execute on the runner before cloning the Git repository and any submodules. You can use it for example to:\n\nThis feature is in beta.\n\nUse to authenticate with third party services using identity federation.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• : Google Cloud. Must be configured with the Google Cloud IAM integration.\n\nUse to create JSON web tokens (JWT) to authenticate with third party services. All JWTs created this way support OIDC authentication. The required sub-keyword is used to configure the claim for the JWT.\n\nUse to specify a Docker image that the job runs in.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the image, including the registry path if needed, in one of these formats:\n\nIn this example, the image is the default for all jobs in the pipeline. The job does not use the default, because it overrides the default with a job-specific section.\n\nThe name of the Docker image that the job runs in. Similar to used by itself.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the image, including the registry path if needed, in one of these formats:\n\nCommand or script to execute as the container’s entry point.\n\nWhen the Docker container is created, the is translated to the Docker option. The syntax is similar to the Dockerfile directive, where each shell token is a separate string in the array.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• Override the entrypoint of an image.\n\nUse to pass options to the Docker executor runner. This keyword does not work with other executor types.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nA hash of options for the Docker executor, which can include:\n• : Selects the architecture of the image to pull. When not specified, the default is the same platform as the host runner.\n• : Specify the username or UID to use when running the container.\n\nThe pull policy that the runner uses to fetch the Docker image.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single pull policy, or multiple pull policies in an array. Can be , , or .\n• If the runner does not support the defined pull policy, the job fails with an error similar to: ERROR: Job failed (system failure): the configured PullPolicies ([always]) are not allowed by AllowedPullPolicies ([never]) .\n\nUse to control inheritance of default keywords and variables.\n\nUse to control the inheritance of default keywords.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default) or to enable or disable the inheritance of all default keywords.\n• You can also list default keywords to inherit on one line:\n\nUse to control the inheritance of default variables keywords.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• (default) or to enable or disable the inheritance of all default variables.\n• You can also list default variables to inherit on one line:\n\nUse to configure the auto-cancel redundant pipelines feature to cancel a job before it completes if a new pipeline on the same ref starts for a newer commit. If the feature is disabled, the keyword has no effect. The new pipeline must be for a commit with new changes. For example, the Auto-cancel redundant pipelines feature has no effect if you select New pipeline in the UI to run a pipeline for the same commit.\n\nThe behavior of the Auto-cancel redundant pipelines feature can be controlled by the setting.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nExample of with the default behavior:\n\nIn this example, a new pipeline causes a running pipeline to be:\n• Canceled, if only is running or pending.\n\nExample of with the setting:\n\nIn this example, a new pipeline causes a running pipeline to cancel and if they are running or pending.\n• Only set if the job can be safely canceled after it has started, like a build job. Deployment jobs usually shouldn’t be canceled, to prevent partial deployments.\n• When using the default behavior or :\n• A job that has not started yet is always considered , regardless of the job’s configuration. The configuration is only considered after the job starts.\n• Running pipelines are only canceled if all running jobs are configured with or no jobs configured with have started at any time. After a job with starts, the entire pipeline is no longer considered interruptible.\n• If the pipeline triggered a downstream pipeline, but no job with in the downstream pipeline has started yet, the downstream pipeline is also canceled.\n• You can add an optional manual job with in the first stage of a pipeline to allow users to manually prevent a pipeline from being automatically canceled. After a user starts the job, the pipeline cannot be canceled by the Auto-cancel redundant pipelines feature.\n• When using with a trigger job:\n• The triggered downstream pipeline is never affected by the trigger job’s configuration.\n• If is set to , the trigger job’s configuration has no effect.\n• If is set to , a trigger job with can be automatically canceled.\n\nUse to execute jobs out-of-order. Relationships between jobs that use can be visualized as a directed acyclic graph.\n\nYou can ignore stage ordering and run some jobs without waiting for others to complete. Jobs in multiple stages can run concurrently.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• An array of jobs (maximum of 50 jobs).\n• An empty array ( ), to set the job to start as soon as the pipeline is created.\n\nThis example creates four paths of execution:\n• Linter: The job runs immediately without waiting for the stage to complete because it has no needs ( ).\n• Linux path: The job runs as soon as the job finishes, without waiting for to finish.\n• macOS path: The jobs runs as soon as the job finishes, without waiting for to finish.\n• The job runs as soon as all previous jobs finish: , , , , .\n• The maximum number of jobs that a single job can have in the array is limited:\n• For GitLab.com, the limit is 50. For more information, see issue 350398.\n• For GitLab Self-Managed, the default limit is 50. This limit can be changed.\n• If refers to a job that uses the keyword, it depends on all jobs created in parallel, not just one job. It also downloads artifacts from all the parallel jobs by default. If the artifacts have the same name, they overwrite each other and only the last one downloaded is saved.\n• To have refer to a subset of parallelized jobs (and not all of the parallelized jobs), use the keyword.\n• You can refer to jobs in the same stage as the job you are configuring.\n• If refers to a job that might not be added to a pipeline because of , , or , the pipeline might fail to create. Use the keyword to resolve a failed pipeline creation.\n• If a pipeline has jobs with and jobs in the stage, they will all start as soon as the pipeline is created. Jobs with start immediately, and jobs in the stage also start immediately.\n\nWhen a job uses , it no longer downloads all artifacts from previous stages by default, because jobs with can start before earlier stages complete. With you can only download artifacts from the jobs listed in the configuration.\n\nUse (default) or to control when artifacts are downloaded in jobs that use .\n\nKeyword type: Job keyword. You can use it only as part of a job. Must be used with .\n\nIn this example:\n• The job does not download the artifacts.\n• The job downloads the artifacts from all three , because is , or defaults to , for all three needed jobs.\n• You should not combine with in the same job.\n\nUse to download artifacts from up to five jobs in other pipelines. The artifacts are downloaded from the latest successful specified job for the specified ref. To specify multiple jobs, add each as separate array items under the keyword.\n\nIf there is a pipeline running for the ref, a job with does not wait for the pipeline to complete. Instead, the artifacts are downloaded from the latest successful run of the specified job.\n\nmust be used with , , and .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : The job to download artifacts from.\n• : The ref to download artifacts from.\n• : Must be to download artifacts.\n\nIn this example, downloads the artifacts from the latest successful and jobs on the branches in the and projects.\n\nYou can use CI/CD variables in , for example:\n• To download artifacts from a different pipeline in the current project, set to be the same as the current project, but use a different ref than the current pipeline. Concurrent pipelines running on the same ref could override the artifacts.\n• The user running the pipeline must have at least the Reporter role for the group or project, or the group/project must have public visibility.\n• You can’t use in the same job as .\n• When using to download artifacts from another pipeline, the job does not wait for the needed job to complete. Using to wait for jobs to complete is limited to jobs in the same pipeline. Make sure that the needed job in the other pipeline completes before the job that needs it tries to download the artifacts.\n• You can’t download artifacts from jobs that run in .\n• To download artifacts between parent-child pipelines, use .\n\nA child pipeline can download artifacts from a job in its parent pipeline or another child pipeline in the same parent-child pipeline hierarchy.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : A pipeline ID. Must be a pipeline present in the same parent-child pipeline hierarchy.\n• : The job to download artifacts from.\n\nIn this example, the job in the parent pipeline creates some artifacts. The job triggers a child pipeline, and passes the variable to the child pipeline as a new variable. The child pipeline can use that variable in to download artifacts from the parent pipeline.\n• The attribute does not accept the current pipeline ID ( ). To download artifacts from a job in the current pipeline, use .\n• You cannot use in a trigger job, or to fetch artifacts from a multi-project pipeline. To fetch artifacts from a multi-project pipeline use .\n\nTo need a job that sometimes does not exist in the pipeline, add to the configuration. If not defined, is the default.\n\nJobs that use , , or and that are added with might not always be added to a pipeline. GitLab checks the relationships before starting a pipeline:\n• If the entry has and the needed job is present in the pipeline, the job waits for it to complete before starting.\n• If the needed job is not present, the job can start when all other needs requirements are met.\n• If the section contains only optional jobs, and none are added to the pipeline, the job starts immediately (the same as an empty entry: ).\n• If a needed job has , but it was not added to the pipeline, the pipeline fails to start with an error similar to: 'job1' job needs 'job2' job, but it was not added to the pipeline .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nIn this example:\n• When the branch is the default branch, is added to the pipeline, so:\n• waits for both and to complete.\n• When the branch is not the default branch, is not added to the pipeline, so:\n• waits for only to complete, and does not wait for the missing .\n• has no other needed jobs and starts immediately (at the same time as ), like .\n\nYou can mirror the pipeline status from an upstream pipeline to a job by using the keyword. The latest pipeline status from the default branch is replicated to the job.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A full project path, including namespace and group. If the project is in the same group or namespace, you can omit them from the keyword. For example: or .\n• If you add the keyword to , the job no longer mirrors the pipeline status. The behavior changes to .\n\nJobs can use to run a job multiple times in parallel in a single pipeline, but with different variable values for each instance of the job.\n\nUse to execute jobs out-of-order depending on parallelized jobs.\n\nKeyword type: Job keyword. You can use it only as part of a job. Must be used with .\n\nSupported values: An array of hashes of variables:\n• The variables and values must be selected from the variables and values defined in the job.\n\nThe above example generates the following jobs:\n\nThe job runs as soon as the job finishes.\n• Specify a parallelized job using needs with multiple parallelized jobs.\n• None The order of the matrix variables in must match the order of the matrix variables in the needed job. For example, reversing the order of the variables in the job in the earlier example above would be invalid: - : app1 # The variable order does not match `linux:build` and is invalid.\n\nUse to define a GitLab Pages job that uploads static content to GitLab. The content is then published as a website.\n• Alternatively, define if want to use a different content directory.\n\nKeyword type: Job keyword or Job name (deprecated). You can use it only as part of a job.\n• A boolean. Uses the default configuration when set to\n• A hash of configuration options, see the following sections for details.\n\nThis example renames the directory to . This directory is exported as an artifact and published with GitLab Pages.\n\nThis example does not move the directory, but uses the property directly. It also configures the pages deployment to be unpublished after a week.\n\nDeprecated: Use as a job name\n\nUsing as a job name results in the same behavior as specifying the Pages property . This method is available for backwards compatibility, but might not receive all future improvements to the Pages job configuration.\n\nExample using as a job name:\n\nTo use as a job name without triggering a Pages deployment, set the property to false:\n\nUse to configure the content directory of a job. The top-level keyword is deprecated as of GitLab 17.9 and must now be nested under the keyword.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A path to a directory containing the Pages content. In GitLab 17.10 and later, if not specified, the default directory is used and if specified, this path is automatically appended to .\n\nThis example uses Eleventy to generate a static website and output the generated HTML files into a the directory. This directory is exported as an artifact and published with GitLab Pages.\n\nIt is also possible to use variables in the field. For example:\n\nThe publish path specified must be relative to the build root.\n\nUse to configure a path prefix for parallel deployments of GitLab Pages.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe given value is converted to lowercase and shortened to 63 bytes. Everything except alphanumeric characters or periods is replaced with a hyphen. Leading and trailing hyphens or periods are not permitted.\n\nIn this example, a different pages deployment is created for each branch.\n\nUse to specify how long a deployment should be available before it expires. After the deployment is expired, it’s deactivated by a cron job running every 10 minutes.\n\nBy default, parallel deployments expire automatically after 24 hours. To disable this behavior, set the value to .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: The expiry time. If no unit is provided, the time is in seconds. Valid values include:\n\nUse to run a job multiple times in parallel in a single pipeline.\n\nMultiple runners must exist, or a single runner must be configured to run multiple jobs concurrently.\n\nParallel jobs are named sequentially from to .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A numeric value from to .\n\nThis example creates 5 jobs that run in parallel, named to .\n• Every parallel job has a and predefined CI/CD variable set.\n• A pipeline with jobs that use might:\n• Create more jobs running in parallel than available runners. Excess jobs are queued and marked while waiting for an available runner.\n• Create too many jobs, and the pipeline fails with a error. The maximum number of jobs that can exist in active pipelines is limited at the instance-level.\n\nUse to run a job multiple times in parallel in a single pipeline, but with different variable values for each instance of the job.\n\nMultiple runners must exist, or a single runner must be configured to run multiple jobs concurrently.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array of hashes of variables:\n• The variable names can use only numbers, letters, and underscores ( ).\n• The values must be either a string, or an array of strings.\n• The number of permutations cannot exceed 200.\n\nThe example generates 10 parallel jobs, each with different values for and :\n• None jobs add the variable values to the job names to differentiate the jobs from each other, but large values can cause names to exceed limits:\n• Job names must be 255 characters or fewer.\n• When using , job names must be 128 characters or fewer.\n• None You cannot create multiple matrix configurations with the same variable values but different variable names. Job names are generated from the variable values, not the variable names, so matrix entries with identical values generate identical job names that overwrite each other. For example, this configuration would try to create two series of identical jobs, but the versions overwrite the versions:\n• There’s a known issue when using tags with .\n• Select different runner tags for each parallel matrix job.\n\nThe release job must have access to the , which must be in the .\n\nIf you use the Docker executor, you can use this image from the GitLab container registry:\n\nIf you use the Shell executor or similar, install on the server where the runner is registered.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• When you add a Git tag in the UI at Code > Tags.\n• None All release jobs, except trigger jobs, must include the keyword. A release job can use the output from script commands. If you don’t need the script, you can use a placeholder: An issue exists to remove this requirement.\n• None The section executes after the keyword and before the .\n• None A release is created only if the job’s main script succeeds.\n• None If the release already exists, it is not updated and the job with the keyword fails.\n• CI/CD example of the keyword.\n\nRequired. The Git tag for the release.\n\nIf the tag does not exist in the project yet, it is created at the same time as the release. New tags use the SHA associated with the pipeline.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nTo create a release when a new tag is added to the project:\n• Use the CI/CD variable as the .\n• Use to configure the job to run only for new tags.\n\nTo create a release and a new tag at the same time, your should not configure the job to run only for new tags. A semantic versioning example:\n\nIf the tag does not exist, the newly created tag is annotated with the message specified by . If omitted, a lightweight tag is created.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe release name. If omitted, it is populated with the value of .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nThe long description of the release.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to a file that contains the description.\n• The file location must be relative to the project directory ( ).\n• If the file is a symbolic link, it must be in the .\n• The and filename can’t contain spaces.\n• The is evaluated by the shell that runs . You can use CI/CD variables to define the description, but some shells use different syntax to reference variables. Similarly, some shells might require special characters to be escaped. For example, backticks ( ) might need to be escaped with a backslash ( ).\n\nThe for the release, if the doesn’t exist yet.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• A commit SHA, another tag name, or a branch name.\n\nThe title of each milestone the release is associated with.\n\nThe date and time when the release is ready.\n• A date enclosed in quotes and expressed in ISO 8601 format.\n• If it is not defined, the current date and time is used.\n\nUse to include asset links in the release.\n\nUse to create a resource group that ensures a job is mutually exclusive across different pipelines for the same project.\n\nFor example, if multiple jobs that belong to the same resource group are queued simultaneously, only one of the jobs starts. The other jobs wait until the is free.\n\nResource groups behave similar to semaphores in other programming languages.\n\nYou can choose a process mode to strategically control the job concurrency for your deployment preferences. The default process mode is . To change the process mode of a resource group, use the API to send a request to edit an existing resource group.\n\nYou can define multiple resource groups per environment. For example, when deploying to physical devices, you might have multiple physical devices. Each device can be deployed to, but only one deployment can occur per device at any given time.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• Only letters, digits, , , , , , , , and spaces. It can’t start or end with . CI/CD variables are supported.\n\nIn this example, two jobs in two separate pipelines can never run at the same time. As a result, you can ensure that concurrent deployments never happen to the production environment.\n\nUse to configure how many times a job is retried if it fails. If not defined, defaults to and jobs do not retry.\n\nWhen a job fails, the job is processed up to two more times, until it succeeds or reaches the maximum number of retries.\n\nBy default, all failure types cause the job to be retried. Use or to select which failures to retry on.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nwill be retried up to 2 times if the exit code is or if it had a runner system failure.\n\nUse with to retry jobs for only specific failure cases. is the maximum number of retries, like , and can be , , or .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single failure type, or an array of one or more failure types:\n• : Retry when the failure reason is unknown.\n• : Retry when:\n• The runner failed to pull the Docker image. For , , executors.\n• : Retry when the job got stuck or timed out.\n• : Retry if there is a runner system failure (for example, job setup failed).\n• : Retry if the runner is unsupported.\n• : Retry if a delayed job could not be executed.\n• : Retry if the script exceeded the maximum execution time set for the job.\n• : Retry if the job is archived and can’t be run.\n• : Retry if the job failed to complete prerequisite tasks.\n• : Retry if the scheduler failed to assign the job to a runner.\n• : Retry if there is an unknown job problem.\n\nIf there is a failure other than a runner system failure, the job is not retried.\n\nExample of (array of failure types):\n\nUse with to retry jobs for only specific failure cases. is the maximum number of retries, like , and can be , , or .\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nYou can specify the number of retry attempts for certain stages of job execution using variables.\n\nUse to include or exclude jobs in pipelines.\n\nRules are evaluated when the pipeline is created, and evaluated in order. When a match is found, no more rules are checked and the job is either included or excluded from the pipeline depending on the configuration. If no rules match, the job is not added to the pipeline.\n\naccepts an array of rules. Each rules must have at least one of:\n\nRules can also optionally be combined with:\n\nYou can combine multiple keywords together for complex rules.\n\nThe job is added to the pipeline:\n• If an , , or rule matches, and is configured with (default if not defined), , or .\n• If a rule is reached that is only , , or .\n\nThe job is not added to the pipeline:\n• If a rule matches and has .\n\nFor additional examples, see Specify when jobs run with .\n\nUse clauses to specify when to add a job to a pipeline:\n• If an statement is true, add the job to the pipeline.\n• If an statement is true, but it’s combined with , do not add the job to the pipeline.\n• If an statement is false, check the next item (if any more exist).\n• Based on the values of CI/CD variables or predefined CI/CD variables, with some exceptions.\n\nKeyword type: Job-specific and pipeline-specific. You can use it as part of a job to configure the job behavior, or with to configure the pipeline behavior.\n• You cannot use nested variables with . See issue 327780 for more details.\n• If a rule matches and has no defined, the rule uses the defined for the job, which defaults to if not defined.\n• You can mix at the job-level with in rules. configuration in takes precedence over at the job-level.\n• Unlike variables in sections, variables in rules expressions are always formatted as .\n• You can use with to conditionally include other configuration files.\n• CI/CD variables on the right side of and expressions are evaluated as regular expressions.\n\nUse to specify when to add a job to a pipeline by checking for changes to specific files.\n\nFor new branch pipelines or when there is no Git event, always evaluates to true and the job always runs. Pipelines like tag pipelines, scheduled pipelines, and manual pipelines, all do not have a Git event associated with them. To cover these cases, use to specify the branch to compare against the pipeline ref.\n\nIf you do not use , you should use only with branch pipelines or merge request pipelines, though still evaluates to true when creating a new branch. With:\n• Merge request pipelines, compares the changes with the target MR branch.\n• Branch pipelines, compares the changes with the previous commit on the branch.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nAn array including any number of:\n• Paths to files. The file paths can include CI/CD variables.\n• Wildcard paths for:\n• A directory and all its subdirectories, for example .\n• Wildcard glob paths for all files with the same extension or multiple extensions, for example or .\n• Wildcard paths to files in the root directory, or all directories, wrapped in double quotes. For example or .\n\nIn this example:\n• If the pipeline is a merge request pipeline, check and the files in for changes.\n• If has changed, add the job to the pipeline as a manual job, and the pipeline continues running even if the job is not triggered ( ).\n• If a file in has changed, add the job to the pipeline.\n• If no listed files have changed, do not add either job to any pipeline (same as ).\n• Glob patterns are interpreted with Ruby’s with the flags .\n• A maximum of 50 patterns or file paths can be defined per section.\n• resolves to if any of the matching files are changed (an operation).\n• For additional examples, see Specify when jobs run with .\n• You can use the character for both variables and paths. For example, if the variable exists, its value is used. If it does not exist, the is interpreted as being part of a path.\n• Jobs or pipelines can run unexpectedly when using .\n\nUse to specify that a job only be added to a pipeline when specific files are changed, and use to specify the files.\n\nis the same as using without any subkeys. All additional details and related topics are the same.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• Same as above.\n\nIn this example, both jobs have the same behavior.\n\nUse to specify which ref to compare against for changes to the files listed under .\n\nKeyword type: Job keyword. You can use it only as part of a job, and it must be combined with .\n• A branch name, like , , or .\n• A tag name, like or .\n\nIn this example, the job is only included when the has changed relative to and the pipeline source is a merge request event.\n• Using with merged results pipelines can cause unexpected results, because the comparison base is an internal commit that GitLab creates.\n• You can use to skip a job if the branch is empty.\n\nUse to run a job when certain files exist in the repository.\n\nKeyword type: Job keyword. You can use it as part of a job or an .\n• An array of file paths. Paths are relative to the project directory ( ) and can’t directly link outside it. File paths can use glob patterns and CI/CD variables.\n\nIn this example:\n• runs if a exists in the root directory of the repository.\n• runs if a exists anywhere in the repository.\n• Glob patterns are interpreted with Ruby’s with the flags .\n• For performance reasons, GitLab performs a maximum of 50,000 checks against patterns or file paths. After the 50,000th check, rules with patterned globs always match. In other words, the rule always assumes a match in projects with more than 50,000 files, or if there are fewer than 50,000 files but the rules are checked more than 50,000 times.\n• If there are multiple patterned globs, the limit is 50,000 divided by the number of globs. For example, a rule with 5 patterned globs has file limit of 10,000.\n• A maximum of 50 patterns or file paths can be defined per section.\n• resolves to if any of the listed files are found (an operation).\n• With job-level , GitLab searches for the files in the project and ref that runs the pipeline. When using with , GitLab searches for the files in the project and ref of the file that contains the section. The project containing the section can be different than the project running the pipeline when using:\n• cannot search for the presence of artifacts, because evaluation happens before jobs run and artifacts are fetched.\n\nis the same as using without any subkeys. All additional details are the same.\n\nKeyword type: Job keyword. You can use it as part of a job or an .\n\nIn this example, both jobs have the same behavior.\n• In some cases you cannot use or in a CI/CD variable with . See issue 386595 for more details.\n\nUse to specify the location in which to search for the files listed under . Must be used with .\n\nKeyword type: Job keyword. You can use it as part of a job or an , and it must be combined with .\n• : Optional. The commit ref to use to search for the file. The ref can be a tag, branch name, or SHA. Defaults to the of the project when not specified.\n\nIn this example, the job is only included when the exists in the project on the commit tagged with .\n\nUse alone or as part of another rule to control conditions for adding a job to a pipeline. is similar to , but with slightly different input options.\n\nIf a rule is not combined with , , or , it always matches if reached when evaluating a job’s rules.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• (default): Run the job only when no jobs in earlier stages fail.\n• : Run the job only when at least one job in an earlier stage fails.\n• : Don’t run the job regardless of the status of jobs in earlier stages.\n• : Run the job regardless of the status of jobs in earlier stages.\n• : Add the job to the pipeline as a manual job. The default value for changes to .\n• : Add the job to the pipeline as a delayed job.\n\nIn this example, is added to pipelines:\n• For the default branch, with which is the default behavior when is not defined.\n• In all other cases as a manual job.\n• When evaluating the status of jobs for and :\n• Jobs with in earlier stages are considered successful, even if they failed.\n• Skipped jobs in earlier stages, for example manual jobs that have not been started, are considered successful.\n• When using to add a manual job:\n• becomes by default. This default is the opposite of using to add a manual job.\n• To achieve the same behavior as defined outside of , set to .\n\nUse in to allow a job to fail without stopping the pipeline.\n\nYou can also use with a manual job. The pipeline continues running without waiting for the result of the manual job. combined with in rules causes the pipeline to wait for the manual job to run before continuing.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• or . Defaults to if not defined.\n\nIf the rule matches, then the job is a manual job with .\n• The rule-level overrides the job-level , and only applies when the specific rule triggers the job.\n\nUse in rules to update a job’s for specific conditions. When a condition matches a rule, the job’s configuration is completely replaced with the in the rule.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• An array of job names as strings.\n• A hash with a job name, optionally with additional attributes.\n• An empty array ( ), to set the job needs to none when the specific condition is met.\n\nIn this example:\n• If the pipeline runs on a branch that is not the default branch, and therefore the rule matches the first condition, the job needs the job.\n• If the pipeline runs on the default branch, and therefore the rule matches the second condition, the job needs the job.\n• in rules override any defined at the job-level. When overridden, the behavior is same as job-level .\n• in rules can accept and .\n\nUse in to define variables for specific conditions.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• A hash of variables in the format .\n\nUse in rules to update a job’s value for specific conditions.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• The rule-level overrides the job-level , and only applies when the specific rule triggers the job.\n\nUse to define a series of steps to be executed in a job. Each step can be either a script or a predefined step.\n\nYou can also provide optional environment variables and inputs.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• An array of hashes, where each hash represents a step with the following possible keys:\n• : A string representing the name of the step.\n• : A string or array of strings containing shell commands to execute.\n• : Optional. A hash of environment variables specific to this step.\n\nEach array entry must have a , and one or (but not both).\n\nIn this example, the job has two steps:\n• uses a predefined step with an environment variable and an input parameter.\n• A step can have either a or a key, but not both.\n• A configuration cannot be used together with existing keyword.\n• Multi-line scripts can be defined using YAML block scalar syntax.\n\nUse to specify commands for the runner to execute.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• When you use these special characters in , you must use single quotes ( ) or double quotes ( ).\n• Use color codes with to make job logs easier to review.\n\nUse to specify CI/CD secrets to:\n• Make available in the job as CI/CD variables ( type by default).\n\nUse to specify secrets provided by a HashiCorp Vault.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secrets engine. Can be one of (default), , or .\n• : Name of the field where the password is stored.\n\nTo specify all details explicitly and use the KV-V2 secrets engine:\n\nYou can shorten this syntax. With the short syntax, and both default to :\n\nTo specify a custom secrets engine path in the short syntax, add a suffix that starts with :\n\nUse to specify secrets provided by GCP Secret Manager.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secret.\n\nUse to specify secrets provided by a Azure Key Vault.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• : Name of the secret.\n\nUse to configure the secret to be stored as either a or type CI/CD variable\n\nBy default, the secret is passed to the job as a type CI/CD variable. The value of the secret is stored in the file and the variable contains the path to the file.\n\nIf your software can’t use type CI/CD variables, set to store the secret value directly in the variable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The keyword is a setting for the CI/CD variable and must be nested under the CI/CD variable name, not in the section.\n\nUse to explicitly select a token to use when authenticating with Vault by referencing the token’s CI/CD variable.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The name of an ID token\n• When the keyword is not set, the first ID token is used to authenticate.\n\nUse to specify any additional Docker images that your scripts require to run successfully. The image is linked to the image specified in the keyword.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: The name of the services image, including the registry path if needed, in one of these formats:\n\nCI/CD variables are supported, but not for .\n\nIn this example, GitLab launches two containers for the job:\n• A PostgreSQL container. The commands in the Ruby container can connect to the PostgreSQL database at the hostname.\n\nUse to pass options to the Docker executor of a GitLab Runner.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nA hash of options for the Docker executor, which can include:\n• : Selects the architecture of the image to pull. When not specified, the default is the same platform as the host runner.\n• : Specify the username or UID to use when running the container.\n\nThe pull policy that the runner uses to fetch the Docker image.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• A single pull policy, or multiple pull policies in an array. Can be , , or .\n• If the runner does not support the defined pull policy, the job fails with an error similar to: ERROR: Job failed (system failure): the configured PullPolicies ([always]) are not allowed by AllowedPullPolicies ([never]) .\n\nUse to define which stage a job runs in. Jobs in the same can execute in parallel (see Additional details).\n\nIf is not defined, the job uses the stage by default.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: A string, which can be a:\n• The stage name must be 255 characters or fewer.\n• Jobs can run in parallel if they run on different runners.\n• If you have only one runner, jobs can run in parallel if the runner’s setting is greater than .\n\nUse the stage to make a job run at the start of a pipeline. By default, is the first stage in a pipeline. User-defined stages execute after . You do not have to define in .\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nKeyword type: You can only use it with a job’s keyword.\n• If a pipeline has jobs with and jobs in the stage, they will all start as soon as the pipeline is created. Jobs with start immediately, ignoring any stage configuration.\n• A pipeline execution policy can define a stage which runs before .\n\nUse the stage to make a job run at the end of a pipeline. By default, is the last stage in a pipeline. User-defined stages execute before . You do not have to define in .\n\nIf a pipeline contains only jobs in the or stages, it does not run. There must be at least one other job in a different stage.\n\nKeyword type: You can only use it with a job’s keyword.\n• A pipeline execution policy can define a stage which runs after .\n\nUse to select a specific runner from the list of all runners that are available for the project.\n\nWhen you register a runner, you can specify the runner’s tags, for example , , or . To pick up and run a job, a runner must be assigned every tag listed in the job.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n• An array of tag names, which are case-sensitive.\n\nIn this example, only runners with both the and tags can run the job.\n• The number of tags must be less than .\n• Use tags to control which jobs a runner can run\n• Select different runner tags for each parallel matrix job\n\nUse to configure a timeout for a specific job. If the job runs for longer than the timeout, the job fails.\n\nThe job-level timeout can be longer than the project-level timeout, but can’t be longer than the runner’s timeout.\n\nKeyword type: Job keyword. You can use it only as part of a job or in the section.\n\nSupported values: A period of time written in natural language. For example, these are all equivalent:\n\nUse to declare that a job is a “trigger job” which starts a downstream pipeline that is either:\n\nTrigger jobs can use only a limited set of GitLab CI/CD configuration keywords. The keywords available for use in trigger jobs are:\n• (only with a value of , , or ).\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• For multi-project pipelines, the path to the downstream project. CI/CD variables are supported in GitLab 15.3 and later, but not job-only variables. Alternatively, use .\n• You can use in the same job as , but you cannot use the API to start trigger jobs. See issue 284086 for more details.\n• You cannot manually specify CI/CD variables before running a manual trigger job.\n• CI/CD variables defined in a top-level section (globally) or in the trigger job are forwarded to the downstream pipeline as trigger variables.\n• Pipeline variables are not passed to downstream pipelines by default. Use trigger:forward to forward these variables to downstream pipelines.\n• Job-only variables are not available in trigger jobs.\n• Environment variables defined in the runner’s are not available to trigger jobs and are not passed to downstream pipelines.\n• You cannot use in a trigger job.\n• To run a pipeline for a specific branch, tag, or commit, you can use a trigger token to authenticate with the pipeline triggers API. The trigger token is different than the keyword.\n\nUse to declare that a job is a “trigger job” which starts a child pipeline.\n• to set the inputs when the downstream pipeline configuration uses .\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to the child pipeline’s configuration file.\n\nUse to declare that a job is a “trigger job” which starts a multi-project pipeline.\n\nBy default, the multi-project pipeline triggers for the default branch. Use to specify a different branch.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n• The path to the downstream project. CI/CD variables are supported in GitLab 15.3 and later, but not job-only variables.\n\nExample of for a different branch:\n• To run a pipeline for a specific branch, tag, or commit, you can also use a trigger token to authenticate with the pipeline triggers API. The trigger token is different than the keyword.\n\nUse to force the job to wait for the downstream pipeline to complete before it is marked as success.\n\nThis behavior is different than the default, which is for the job to be marked as success as soon as the downstream pipeline is created.\n\nThis setting makes your pipeline execution linear rather than parallel.\n\nIn this example, jobs from subsequent stages wait for the triggered pipeline to successfully complete before starting.\n• Optional manual jobs in the downstream pipeline do not affect the status of the downstream pipeline or the upstream trigger job. The downstream pipeline can complete successfully without running any optional manual jobs.\n• Blocking manual jobs in the downstream pipeline must run before the trigger job is marked as successful or failed. The trigger job shows pending ( ) if the downstream pipeline status is waiting for manual action ( ) due to manual jobs. By default, jobs in later stages do not start until the trigger job completes.\n• If the downstream pipeline has a failed job, but the job uses , the downstream pipeline is considered successful and the trigger job shows success.\n\nUse to set the inputs when the downstream pipeline configuration uses .\n\nUse to specify what to forward to the downstream pipeline. You can control what is forwarded to both parent-child pipelines and multi-project pipelines.\n\nForwarded variables do not get forwarded again in nested downstream pipelines by default, unless the nested downstream trigger job also uses .\n• : (default), or . When , variables defined in the trigger job are passed to downstream pipelines.\n• : or (default). When , pipeline variables are passed to the downstream pipeline.\n\nRun this pipeline manually, with the CI/CD variable :\n• CI/CD variables forwarded to downstream pipelines with are pipeline variables, which have high precedence. If a variable with the same name is defined in the downstream pipeline, that variable is usually overwritten by the forwarded variable.\n\nUse to configure the conditions for when jobs run. If not defined in a job, the default value is .\n\nKeyword type: Job keyword. You can use it as part of a job. and can also be used in .\n• (default): Run the job only when no jobs in earlier stages fail.\n• : Run the job only when at least one job in an earlier stage fails.\n• : Don’t run the job regardless of the status of jobs in earlier stages. Can only be used in a section or .\n• : Run the job regardless of the status of jobs in earlier stages.\n• : Add the job to the pipeline as a manual job.\n• : Add the job to the pipeline as a delayed job.\n\nIn this example, the script:\n• Always executes as the last step in pipeline regardless of success or failure.\n• Executes when you run it manually in the GitLab UI.\n• When evaluating the status of jobs for and :\n• Jobs with in earlier stages are considered successful, even if they failed.\n• Skipped jobs in earlier stages, for example manual jobs that have not been started, are considered successful.\n• The default value for is with . The default value changes to with .\n• can be used with for more dynamic job control.\n• can be used with to control when a pipeline can start.\n\nUse with to define a custom confirmation message for manual jobs. If there is no manual job defined with , this keyword has no effect.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nVariables can be defined in a CI/CD job, or as a top-level (global) keyword to define default CI/CD variables for all jobs.\n• All YAML-defined variables are also set to any linked Docker service containers.\n• YAML-defined variables are meant for non-sensitive project configuration. Store sensitive information in protected variables or CI/CD secrets.\n• Manual pipeline variables and scheduled pipeline variables are not passed to downstream pipelines by default. Use trigger:forward to forward these variables to downstream pipelines.\n• Predefined variables are variables the runner automatically creates and makes available in the job.\n• You can configure runner behavior with variables.\n\nYou can use job variables in commands in the job’s , , or sections, and also with some job keywords. Check the Supported values section of each job keyword to see if it supports variables.\n\nYou cannot use job variables as values for global keywords like .\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ). In some shells, the first character must be a letter.\n• The value must be a string.\n\nIn this example:\n• has and job variables defined. Both job variables can be used in the section.\n\nVariables defined in a top-level section act as default variables for all jobs.\n\nEach default variable is made available to every job in the pipeline, except when the job already has a variable defined with the same name. The variable defined in the job takes precedence, so the value of the default variable with the same name cannot be used in the job.\n\nLike job variables, you cannot use default variables as values for other global keywords, like .\n\nSupported values: Variable name and value pairs:\n• The name can use only numbers, letters, and underscores ( ). In some shells, the first character must be a letter.\n• The value must be a string.\n\nIn this example:\n• has no variables defined. The default variable is copied to the job and can be used in the section.\n• already has a variable defined, so the default is not copied to the job. The job also has a job variable defined. Both job variables can be used in the section.\n\nUse the keyword to define a description for a default variable. The description displays with the prefilled variable name when running a pipeline manually.\n\nKeyword type: You can only use this keyword with default , not job .\n• When used without , the variable exists in pipelines that were not triggered manually, and the default value is an empty string ( ).\n\nUse the keyword to define a pipeline-level (default) variable’s value. When used with , the variable value is prefilled when running a pipeline manually.\n\nKeyword type: You can only use this keyword with default , not job .\n• If used without , the behavior is the same as .\n\nUse to define an array of values that are selectable in the UI when running a pipeline manually.\n\nMust be used with , and the string defined for :\n• Must also be one of the strings in the array.\n\nIf there is no , this keyword has no effect.\n\nKeyword type: You can only use this keyword with default , not job .\n\nUse the keyword to configure a variable to be expandable or not.\n\nKeyword type: You can use this keyword with both default and job .\n• : The variable is not expandable.\n• The result of is .\n• The result of is .\n• The keyword can only be used with default and job keywords. You can’t use it with or .\n\nThe following keywords are deprecated.\n\nDefining , , , , and globally is deprecated. Using these keywords at the top level is still possible to ensure backwards compatibility, but could be scheduled for removal in a future milestone.\n\nUse instead. For example:\n\nYou can use and to control when to add jobs to pipelines.\n• Use to define when a job runs.\n• Use to define when a job does not run.\n\nYou can use the and keywords to control when to add jobs to a pipeline based on branch names or pipeline types.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array including any number of:\n• Branch names, for example or .\n• Regular expressions that match against branch names, for example .\n• The following keywords: For pipelines triggered by the pipelines API. When the Git reference for a pipeline is a branch. For pipelines created by using a GitLab ChatOps command. When you use CI services other than GitLab. When an external pull request on GitHub is created or updated (See Pipelines for external pull requests). For pipelines created when a merge request is created or updated. Enables merge request pipelines, merged results pipelines, and merge trains. For multi-project pipelines created by using the API with , or the keyword. For pipelines triggered by a event, including for branches and tags. When the Git reference for a pipeline is a tag. For pipelines created by using a trigger token. For pipelines created by selecting New pipeline in the GitLab UI, from the project’s Build > Pipelines section.\n\nExample of and :\n• None Scheduled pipelines run on specific branches, so jobs configured with run on scheduled pipelines too. Add to prevent jobs with from running on scheduled pipelines.\n• None or used without any other keywords are equivalent to or . For example, the following two jobs configurations have the same behavior:\n• None If a job does not use , , or , then is set to and by default. For example, and are equivalent:\n\nYou can use the or keywords to control when to add jobs to a pipeline, based on the status of CI/CD variables.\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nUse the keyword with to run a job, or with to skip a job, when a Git push event modifies a file.\n\nUse in pipelines with the following refs:\n\nKeyword type: Job keyword. You can use it only as part of a job.\n\nSupported values: An array including any number of:\n• Wildcard paths for:\n• A directory and all its subdirectories, for example .\n• Wildcard glob paths for all files with the same extension or multiple extensions, for example or .\n• Wildcard paths to files in the root directory, or all directories, wrapped in double quotes. For example or .\n• resolves to if any of the matching files are changed (an operation).\n• Glob patterns are interpreted with Ruby’s with the flags .\n• If you use refs other than , , or , can’t determine if a given file is new or old and always returns .\n• If you use with other refs, jobs ignore the changes and always run.\n• If you use with other refs, jobs ignore the changes and never run.\n• Jobs or pipelines can run unexpectedly when using .\n\nUse or to control if jobs are added to the pipeline when the Kubernetes service is active in the project.\n\nKeyword type: Job-specific. You can use it only as part of a job.\n• The strategy accepts only the keyword.\n\nIn this example, the job runs only when the Kubernetes service is active in the project."
    },
    {
        "link": "https://stackoverflow.com/questions/68878612/yaml-file-validation-and-integration-with-ci-cd",
        "document": "I want to add a feature where my YAML file is validated in my pipeline before execution of commands and running the file on my kubernetes cluster. Are there any tools or products that would support this?"
    },
    {
        "link": "https://docs.gitlab.com/ci/yaml/includes",
        "document": "You can use to include external YAML files in your CI/CD jobs.\n\nTo include a single configuration file, use by itself with a single file with either of these syntax options:\n• None On the same line:\n• None As a single item in an array:\n\nIf the file is a local file, the behavior is the same as . If the file is a remote file, it is the same as .\n\nYou can include an array of configuration files:\n• None If you do not specify an type, each array item defaults to or , as needed:\n• None You can define an array and explicitly specify multiple types:\n• None You can define an array that combines both default and specific types:\n\nUse configuration from an included configuration file\n\nYou can define a section in a configuration file. When you use a section with the keyword, the defaults apply to all jobs in the pipeline.\n\nFor example, you can use a section with .\n\nThe default commands execute in both jobs, before the commands.\n\nWhen you use the keyword, you can override the included configuration values to adapt them to your pipeline requirements.\n\nThe following example shows an file that is customized in the file. Specific YAML-defined variables and details of the job are overridden.\n\nThe and variables and the of the job defined in the file override the values defined in the file. The other keywords do not change. This method is called merging.\n\nThe configuration merges with the main configuration file with this process:\n• Included files are read in the order defined in the configuration file, and the included configuration is merged together in the same order.\n• If an included file also uses , that nested configuration is merged first (recursively).\n• If parameters overlap, the last included file takes precedence when merging the configuration from the included files.\n• After all configuration added with is merged together, the main configuration is merged with the included configuration.\n\nThis merge method is a deep merge, where hash maps are merged at any depth in the configuration. To merge hash map “A” (that contains the configuration merged so far) and “B” (the next piece of configuration), the keys and values are processed as follows:\n• When the key only exists in A, use the key and value from A.\n• When the key exists in both A and B, and their values are both hash maps, merge those hash maps.\n• When the key exists in both A and B, and one of the values is not a hash map, use the value from B.\n• Otherwise, use the key and value from B.\n\nFor example, with a configuration that consists of two files:\n\nIn this example:\n• Variables are only evaluated after all the files are merged together. A job in an included file might end up using a variable value defined in a different file.\n• is an array so it cannot be merged. The top-level file takes precedence.\n• is a hash map so it can be deep merged.\n\nYou can use merging to extend and override configuration in an included template, but you cannot add or modify individual items in an array. For example, to add an additional command to the extended job’s array:\n\nIf and are not repeated in the file, the job would have only in the script.\n\nYou can nest sections in configuration files that are then included in another configuration. For example, for keywords nested three deep:\n\nYou can include the same configuration file multiple times in the main configuration file and in nested includes.\n\nIf any file changes the included configuration using overrides, then the order of the entries might affect the final configuration. The last time the configuration is included overrides any previous times the file was included. For example:\n\nWith these three files, the order they are included changes the final configuration. With:\n• None included first, the contents of the file is: The final configuration would be:\n• None included last, the contents of the file is:\n• None The final configuration would be:\n\nIf no file overrides the included configuration, the order of the entries does not affect the final configuration\n\nIn sections in your file, you can use:\n\nYou cannot use variables defined in jobs, or in a global section which defines the default variables for all jobs. Includes are evaluated before jobs, so these variables cannot be used with .\n\nFor an example of how you can include predefined variables, and the variables’ impact on CI/CD jobs, see this CI/CD variable demo.\n\nYou cannot use CI/CD variables in an section in a dynamic child pipeline’s configuration. Issue 378717 proposes fixing this issue.\n\nYou can use with to conditionally include other configuration files.\n\nYou can only use with certain variables, and these keywords:\n\nUse to conditionally include other configuration files based on the status of CI/CD variables. For example:\n\nUse to conditionally include other configuration files based on the existence of files. For example:\n\nIn this example, GitLab checks for the existence of in the current project.\n\nReview your configuration carefully if you use with in an include file from a different project. GitLab checks for the existence of the file in the other project. For example:\n\nIn this example, GitLab searches for the existence of in on commit ref , not the project/ref in which the pipeline runs.\n\nTo change the search context you can use with . For example:\n\nUse to conditionally include other configuration files based on changed files. For example:\n\nIn this example:\n• is included when has changed.\n• is included when has changed relative to .\n• is included when has changed and the pipeline source is a merge request event. The jobs in must also be configured to run for merge request pipelines.\n\nYou can use wildcard paths ( and ) with .\n• None Adds all files in the directory into the pipeline configuration.\n• None Does not add files in subfolders of the directory. To allow this, add the following configuration: # This matches all `.yml` files in `configs` and any subfolder in it. # This matches all `.yml` files only in subfolders of `configs`.\n\nThe maximum number of nested included files for a pipeline is 150. If you receive the error message in your pipeline, it’s likely that either:\n• Some of the nested configuration includes an overly large number of additional nested configuration.\n• There is an accidental loop in the nested includes. For example, includes which includes , creating a recursive loop.\n\nTo help reduce the risk of this happening, edit the pipeline configuration file with the pipeline editor, which validates if the limit is reached. You can remove one included file at a time to try to narrow down which configuration file is the source of the loop or excessive included files.\n\nIn GitLab 16.0 and later users on GitLab Self-Managed can change the maximum includes value.\n\nWhen using , GitLab tries to fetch the remote file through HTTP(S). This process can fail because of a variety of connectivity issues.\n\nThe error happens when GitLab can’t establish an HTTPS connection to the remote host. This issue can be caused if the remote host has rate limits to prevent overloading the server with requests.\n\nFor example, the GitLab Pages server for GitLab.com is rate limited. Repeated attempts to fetch CI/CD configuration files hosted on GitLab Pages can cause the rate limit to be reached and cause the error. You should avoid hosting CI/CD configuration files on a GitLab Pages site.\n\nWhen possible, use to fetch configuration files from other projects within the GitLab instance without making external HTTP(S) requests."
    },
    {
        "link": "https://codefresh.io/learn/gitlab-ci/what-is-the-gitlab-ci-yml-file-and-how-to-work-with-it",
        "document": "GitLab is an open source code repository and continuous integration / continuous delivery (CI/CD) platform. There are two basic requirements to use GitLab CI/CD: application code hosted in a Git repository, and a file called .gitlab-ci.yml in the root of the repository, which specifies the GitLab configuration.\n\nThe .gitlab-ci.yml file defines scripts that should be run during the CI/CD pipeline and their scheduling, additional configuration files and templates, dependencies, caches, commands GitLab should run sequentially or in parallel, and instructions on where the application should be deployed to.\n\nGitLab makes it possible to group scripts into jobs that run as part of a larger pipeline, and run them in a specific order. This can be defined within the .gitlab-ci.yml file. When you add the file to your repository, GitLab detects it and an application called GitLab Runner runs the scripts defined in the jobs within it.\n\nThis is part of a series of articles about GitLab CI.\n\nThe main way of editing a CI/CD configuration in GitLab is through its provided editor. The editor modifies the .gitlab-ci.yml in the repository’s root directory, and can be accessed via the CI/CD > Editors menu.\n\nYou can do the following through the Pipeline Editor page:\n• Select the branch to work on\n• Check syntax validity and correctness while working on the file\n• View any CI/CD configurations that got added through the include keyword\n• Commit any changes to a particular branch\n• Commit any changes to a particular branch\n\nYou can validate the pipeline’s configuration against the GitLab CI/CD pipeline’s architecture through the pipeline editor. The validation can happen while you edit it. First, the feature checks YAML configuration syntax and performs fundamental logic validation. Then, the results are displayed at the top of the editor page.\n\nThe editor also has a lint tool to check the CI/CD configuration’s validity. Users can access it by going to CI/CD, Editor, and then the Lint tab, as shown below. It checks for syntax and logical errors and has deeper checking functionality than the editor. Changes to the configuration and results from the CI Lint tool are updated in real-time.\n\nHere is what a .gitlab-ci.yml file might have inside it:\n\nHere, runs first since it has a stage, and the order is specified in . The job outputs the version of Ruby it is running and builds project files. Once it completes, the next two jobs with as their stage run in parallel and run tests on the file.\n\nThe entire pipeline consists of three jobs categorized into two stages specified at the start, and . This pipeline gets triggered every time a change gets pushed to a branch associated with the project.\n\nHere are a few techniques that can help you optimize GitLab YAML configurations.\n\nAnchors in YAML are useful for duplicating content at different parts of the document. They are also useful for merging YAML arrays. For example, the following YAML code creates two jobs. Both inherit the same configuration and have their separately defined scripts:\n\nHere, the part specifies the anchors, and the part with the alias specified above gets inserted. Anchors are also handy in defining and easily inserting different sets of services:\n\nThe keyword works similarly to anchors but is simpler to use. It supports multi-level inheritance but using it for more than three levels should be avoided. For example, here there are two levels of inheritance:\n\nIt also comes in handy when reusing YAML code from other scripts. For example, the file has a script. For example, the following script in the file refers to the contents in uses :\n\nThe YAML tag is useful for reusing selected keyword configuration, which can come from other job sections. tags also allow for reusing configuration from files that are included. For example, here is how to use tags to reuse scripts from two different locations while creating a job\n\nCombine GitLab with Codefresh to Support GitOps and Kubernetes Deployments\n\nGitLab is a very powerful platform but it is focused mostly on CI and does not support GitOps and native Kubernetes deployments. Codefresh is created specifically for GitOps and Cloud native applications and includes native support for using GitLab as a Git provider.\n\nThis means that you can get the best of both worlds by keeping all your CI workflows in GitLab, while using Codefresh for advanced features such as:\n\nIn case you are new to Codefresh – we have made it our mission since 2014 to help teams accelerate their pace of innovation. Codefresh recently released a completely rebuilt GitOps CI/CD toolset. Powered by Argo, Codefresh now combines the best of open source with an enterprise-grade runtime allowing you to fully tap the power of Argo Workflows, Events, CD, and Rollouts. It provides teams with a unified GitOps experience to build, test, deploy, and scale their applications."
    }
]