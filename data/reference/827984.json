[
    {
        "link": "https://docs.pyrogram.org/index",
        "document": "The project is no longer maintained or supported. Thanks for appreciating it.\n\nPyrogram is a modern, elegant and asynchronous MTProto API framework. It enables you to easily interact with the main Telegram API through a user account (custom client) or a bot identity (bot API alternative) using Python.\n\nHow the Documentation is Organized#\n\nContents are organized into sections composed of self-contained topics which can be all accessed from the sidebar, or by following them in order using the Next button at the end of each page. You can also switch to Dark or Light theme or leave on Auto (follows system preferences) by using the dedicated button in the top left corner.\n\nHere below you can, instead, find a list of the most relevant pages for a quick access."
    },
    {
        "link": "https://github.com/pyrogram/pyrogram",
        "document": "The project is no longer maintained or supported. Thanks for appreciating it.\n\nElegant, modern and asynchronous Telegram MTProto API framework in Python for users and bots\n\nPyrogram is a modern, elegant and asynchronous MTProto API framework. It enables you to easily interact with the main Telegram API through a user account (custom client) or a bot identity (bot API alternative) using Python.\n• Ready: Install Pyrogram with pip and start building your applications right away.\n• Easy: Makes the Telegram API simple and intuitive, while still allowing advanced usages.\n• Elegant: Low-level details are abstracted and re-presented in a more convenient way.\n• Fast: Boosted up by TgCrypto, a high-performance cryptography library written in C.\n• Type-hinted: Types and methods are all type-hinted, enabling excellent editor support.\n• Async: Fully asynchronous (also usable synchronously if wanted, for convenience).\n• Powerful: Full access to Telegram's API to execute any official client action and more.\n• Check out the docs at https://docs.pyrogram.org to learn more about Pyrogram, get started right away and discover more in-depth material for building your client applications.\n• Join the official channel at https://t.me/pyrogram and stay tuned for news, updates and announcements."
    },
    {
        "link": "https://pypi.org/project/Pyrogram",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://docs.pyrogram.org/topics/mtproto-vs-botapi",
        "document": "Pyrogram is a framework written from the ground up that acts as a fully-fledged Telegram client based on the MTProto API. This means that Pyrogram is able to execute any official client and bot API action and more. This page will therefore show you why Pyrogram might be a better choice for your project by comparing the two APIs, but first, let’s make it clear what actually is the MTProto and the Bot API.\n\nWhat is the MTProto API?# MTProto, took alone, is the name of the custom-made, open and encrypted communication protocol created by Telegram itself — it’s the only protocol used to exchange information between a client and the actual Telegram servers. The MTProto API on the other hand, is what people for convenience call the main Telegram API in order to distinguish it from the Bot API. The main Telegram API is able to authorize both users and bots and is built on top of the MTProto encryption protocol by means of binary data serialized in a specific way, as described by the TL language, and delivered using UDP, TCP or even HTTP as transport-layer protocol. Clients that make use of Telegram’s main API, such as Pyrogram, implement all these details.\n\nWhat is the Bot API?# The Bot API is an HTTP(S) interface for building normal bots using a sub-set of the main Telegram API. Bots are special accounts that are authorized via tokens instead of phone numbers. The Bot API is built yet again on top of the main Telegram API, but runs on an intermediate server application that in turn communicates with the actual Telegram servers using MTProto.\n\nHere is a non-exhaustive list of all the advantages in using MTProto-based libraries – such as Pyrogram – instead of the official HTTP Bot API. Using Pyrogram you can:\n• None – The Bot API only allows bot accounts\n• None + Upload & download any file, up to 2000 MiB each (~2 GB)\n• None – The Bot API allows uploads and downloads of files only up to 50 MB / 20 MB in size (respectively).\n• None + Has less overhead due to direct connections to Telegram\n• None – The Bot API uses an intermediate server to handle HTTP requests before they are sent to the actual Telegram servers.\n• None + Run multiple sessions at once (for both user and bot identities)\n• None – The Bot API intermediate server will terminate any other session in case you try to use the same bot again in a parallel connection.\n• None + Has much more detailed types and powerful methods\n• None – The Bot API types often miss some useful information about Telegram entities and some of the methods are limited as well.\n• None + Obtain information about any message existing in a chat using their ids\n• None + Retrieve the whole chat members list of either public or private chats\n• None + Receive extra updates, such as the one about a user name change\n• None + Has more meaningful errors in case something went wrong\n• None + Get API version updates, and thus new features, sooner\n• None – The Bot API is simply slower in implementing new features"
    },
    {
        "link": "https://docs.python-telegram-bot.org",
        "document": "We have made you a wrapper you can’t refuse\n\nWe have a vibrant community of developers helping each other in our Telegram group. Join us!\n\nStay tuned for library updates and new releases on our Telegram Channel.\n\nYou can install or upgrade via To install a pre-release, use the flag in addition. You can also install from source, though this is usually not necessary. To enable you to verify that a release file that you downloaded was indeed provided by the team, we have taken the following measures. Starting with v21.4, all releases are signed via sigstore. The corresponding signature files are uploaded to the GitHub releases page. To verify the signature, please install the sigstore Python client and follow the instructions for verifying signatures from GitHub Actions. As input for the parameter, please use the value . Earlier releases are signed with a GPG key. The signatures are uploaded to both the GitHub releases page and the PyPI project and end with a suffix . Please find the public keys here. The keys are named in the format . In addition, the GitHub release page also contains the sha1 hashes of the release files in the files with the suffix . tries to use as few 3rd party dependencies as possible. However, for some features using a 3rd party library is more sane than implementing the functionality again. As these features are optional, the corresponding 3rd party dependencies are not installed by default. Instead, they are listed as optional dependencies. This allows to avoid unnecessary dependency conflicts for users who don’t need the optional features. The only required dependency is httpx ~= 0.27 for , the default networking backend. is most useful when used along with additional libraries. To minimize dependency conflicts, we try to be liberal in terms of version requirements on the (optional) dependencies. On the other hand, we have to ensure stability of , which is why we do apply version bounds. If you encounter dependency conflicts due to these bounds, feel free to reach out. PTB can be installed with optional dependencies:\n• None installs the cryptography>=39.0.1 library. Use this, if you want to use Telegram Passport related functionality.\n• None installs httpx[socks]. Use this, if you want to work behind a Socks5 server.\n• None installs httpx[http2]. Use this, if you want to use HTTP/2.\n• None installs aiolimiter~=1.1,<1.3. Use this, if you want to use .\n• None installs the tornado~=6.4 library. Use this, if you want to use / .\n• None installs the cachetools>=5.3.3,<5.6.0 library. Use this, if you want to use arbitrary callback_data.\n• None installs the APScheduler>=3.10.4,<3.12.0 library. Use this, if you want to use the . To install multiple optional dependencies, separate them by commas, e.g. .\n• None installs all optional dependencies that are related to , i.e. .\n\nOnce you have installed the library, you can begin working with it - so let’s get started! Our Wiki contains an Introduction to the API explaining how the pure Bot API can be accessed via . Moreover, the Tutorial: Your first Bot gives an introduction on how chatbots can be easily programmed with the help of the module.\n• None The package documentation is the technical reference for . It contains descriptions of all available classes, modules, methods and arguments as well as the changelog.\n• None The wiki is home to number of more elaborate introductions of the different features of and other useful resources that go beyond the technical documentation.\n• None Our examples section contains several examples that showcase the different features of both the Bot API and . Even if it is not your approach for learning, please take a look at . It is the de facto base for most of the bots out there. The code for these examples is released to the public domain, so you can start by grabbing the code and building on top of it.\n• None The official Telegram Bot API documentation is of course always worth a read. If the resources mentioned above don’t answer your questions or simply overwhelm you, there are several ways of getting help.\n• None We have a vibrant community of developers helping each other in our Telegram group. Join us! Asking a question here is often the quickest way to get a pointer in the right direction.\n• None You can even ask for help on Stack Overflow using the python-telegram-bot tag. Since v20.0, is built on top of Pythons module. Because is in general single-threaded, does currently not aim to be thread-safe. Noteworthy parts of API that are likely to cause issues (e.g. race conditions) when used in a multi-threaded setting include:\n• None all classes in the module that allow to add/remove allowed users/chats at runtime\n\nOccasionally we are asked if we accept donations to support the development. While we appreciate the thought, maintaining PTB is our hobby, and we have almost no running costs for it. We therefore have nothing set up to accept donations. If you still want to donate, we kindly ask you to donate to another open source project/initiative of your choice instead.\n\nYou may copy, distribute and modify the software provided that modifications are described and licensed for free under LGPL-3. Derivative works (including modifications or anything statically linked to the library) can only be redistributed under LGPL-3, but applications that use the library don’t have to be."
    },
    {
        "link": "https://stackoverflow.com/questions/78848918/how-to-handle-audio-messages-in-python-telegram-bot",
        "document": "I want bot to check if user answer have audio and then decide what to do but bot only handle text message and just ignores audio.\n\nThen here new should be check if message have text ot not(because audio should do not have text i believe), but bit just ignores if I send audio-message and will only reply after text message\n\nAnd this in ConversationHandler\n\nI didnt found any handler to handle audio thus I just did"
    },
    {
        "link": "https://stackoverflow.com/questions/72743713/speech-recognition-with-python-telegram-bot-without-downloading-an-audio-file",
        "document": "Funny is uses and it has to get but documentation for shows that it can works also with and few other formats. See all supported audio encodings\n\nWhen I checked source code for then I saw it gets but it converts it to before sending to .\n\nYou can try to use directly but this may need to register own application on Google to get . See more Speech-To-Text\n\nI took source code in which uses and and I took some code from Google documentation and I created own version which can send directly .\n\nIt use API Key from -\n\nIn code I read file from disk but using probably you get data from bot without writing on disk.\n\nMinimal working bot code - which I tested with uploaded files (not with voice)"
    },
    {
        "link": "https://github.com/kkroening/ffmpeg-python/issues/855",
        "document": "I get OPUS-WEBM audio chunks from web-browser which I need convert it to PCM16 format. I tried using ffmpeg-python with the continuous chunks of data. The code actually converts the first chunk of the stream which contains the EBML header and throws an error when the subsequent chunks with no headers arrive like: EBML header not found, Invalid data.\n\nIs there anyother way to handle the headerless chunks coming from a websocket chunks with ffmpeg-python or any other alternative to ffmpeg-python that handles continuous stream of audio data as chunks"
    },
    {
        "link": "https://bannerbear.com/blog/how-to-use-ffmpeg-in-python-with-examples",
        "document": "Integrating FFmpeg's powerful media manipulation capabilities into your Python code becomes easy with the “ffmpeg-python” library. With a few lines of code, you can convert media formats, edit videos, extract audio, and more within your Python applications.\n\nPython is one of the most popular programming languages due to its simplicity and versatility. You can use it for various types of projects including web development, data analysis, web scraping, automation, and more. When it comes to editing or manipulating media files like videos, images, and audio files, you will need to use libraries that provide these functionalities as they don’t come with Python.\n\nIn this article, we are going to learn how to use FFmpeg, a popular media manipulation tool in Python to work with media files, showing some practical examples.\n\nFFmpeg is a complete, cross-platform solution to record, convert, and stream audio and video. It can decode, encode, transcode, mux, demux, stream, filter, and play media files in any format. It is also highly portable—it compiles and runs in a wide variety of build environments, machine architectures, and configurations like Linux, Mac OS X, Microsoft Windows, etc.\n\nFFmpeg contains multiple tools for end-users to convert, play, and analyze media files and libraries for developers to use in different applications. Libraries like libavcodec, libavutil, libavformat, libavfilter, libavdevice, libswscale, and libswresample will be downloaded automatically when you download FFmpeg to your machine.\n\nAlthough FFmpeg is a command-line tool, you can use it in your Python project with the Python library ffmpeg-python.\n\nYou need to have the tools below installed and the versions used in this tutorial are included as a reference:\n\nThe ffmpeg-python library is only a Python wrapper for the FFmpeg installed on your machine and does not work independently. Therefore, you must have FFmpeg installed before using ffmpeg-python in your Python project.\n\nOnce you have the tools above installed, open up the terminal/command prompt and run the command below in your working directory to install ffmpeg-python in your Python project using pip:\n\nThen, create a Python file (eg. index.py), import in the code, and use it to perform various operations on media files.\n\nLet’s look at some examples…\n\nUsing the ffmpeg-python library typically includes specifying the input video file using the function, output file using the function, and executing the command using function:\n\nThe basic command above will convert the input MP4 video into a WMV video. To do more with FFmpeg, we can apply various filters to the input file after the function and also pass different parameters like , , , etc. to the and functions.\n\nConverting an MP4 video to an MP3 audio or extracting audio from a video is simple. It works similarly to converting the format of a video as shown in the previous example. Instead of naming the output file in a video file extension, name it in an audio file extension like “.mp3”. It will convert the video to MP3 automatically using the default audio codec (libmp3lame):\n\nYou can also use a custom audio codec by specifying it in the parameter of the function:\n\nThis will use the libshine encoder to encode the output file:\n\nTrimming a video using ffmpeg-python only requires adding some parameters in the function. In the function, specify the start time in the parameter and the end time in the parameter.\n\nThis will trim the input video from 00:00:10 to 00:00:20 and save it as a new “output.mp4” video:\n\nHere’s a screenshot of the input video at the 00:00:10 timestamp:\n\nAnd this is the result of the output video:\n\nAnother thing you can do with FFmpeg by adding parameters to the functions is to extract frames from the input video. You can specify the number of frames that you want to extract using the parameter :\n\nIt will extract the first three frames from the input video at 25fps, which is the default frame rate:\n\nYou can also use the fps filter to change the frame rate of the video. The code below will extract frames from the whole video (since is not specified) at 1fps instead of 25fps:\n\nFor both methods, you use the parameter to specify the start time:\n\nThe code above will extract the first three frames starting from 00:00:15:\n\nCreating a thumbnail image from the input video is technically the same as extracting frames from the video, but only a single frame:\n\nHere’s the thumbnail created from the executing the code above:\n\nBesides using the parameter to select a particular frame from the video, you can also use a filter. The “Thumbnail” filter selects the most representative frame in a given sequence of consecutive frames automatically from the video and saves it as the thumbnail image.\n\nYou can use the “Thumbnail” filter by specifying the filter name in the function following :\n\nBy default, FFmpeg analyzes the frames in a batch of 100 and picks the most representative frame out of them. The process continues until the end of the video and we will get the best frame among all the batches.\n\nHere’s the thumbnail selected automatically using the “Thumbnail” filter in the default batch size:\n\nYou can also change the batch size by using the parameter :\n\nIf you’re using FFmpeg in Python to automate the video editing process like adding subtitles, logos, or other elements to them, you should try an alternative tool, which is Bannerbear. It provides a simple REST API that offers easy integration into any existing platform or app to generate designed visual content including images and videos automatically.\n\nIt’s easy to add it to your Python project to automate the video editing process. Firstly, you need a design template in your Bannerbear account like the one in the screenshot below:\n\nThen, make an HTTP request in your Python code to call the Bannerbear API and trigger the video generation process. You will get the URL of the resulting video in the response and here’s a screenshot of the video:\n\nYou can refer to this tutorial and the API Reference to learn how to do it in detail.\n\nThe ffmpeg-library enables you to use FFmpeg in Python to manipulate various media files for different purposes like building comprehensive multimedia applications, preprocessing media files for machine learning projects, etc. However, learning the FFmpeg commands has a learning curve. If you prefer a tool that is more visual and with a user-friendly UI, use Bannerbear. Signing up for an account is free and you can choose a design from the Template Library to start immediately!"
    },
    {
        "link": "https://kkroening.github.io/ffmpeg-python",
        "document": "Represents the outgoing edge of an upstream node; may be used to create more downstream nodes. Some ffmpeg filters drop audio streams, and care must be taken to preserve the audio in the final output. The and operators can be used to reference the audio/video portions of a stream so that they can be processed separately and then re-combined later in the pipeline. This dilemma is intrinsic to ffmpeg, and ffmpeg-python tries to stay out of the way while users may refer to the official ffmpeg documentation as to why certain filters drop audio. Process the audio and video portions of a stream independently: Some ffmpeg filters drop audio streams, and care must be taken to preserve the audio in the final output. The and operators can be used to reference the audio/video portions of a stream so that they can be processed separately and then re-combined later in the pipeline. This dilemma is intrinsic to ffmpeg, and ffmpeg-python tries to stay out of the way while users may refer to the official ffmpeg documentation as to why certain filters drop audio. Process the audio and video portions of a stream independently:\n\nAny supplied keyword arguments are passed to ffmpeg verbatim (e.g. , , , , etc.). Some keyword-arguments are handled specially, as shown below. If multiple streams are provided, they are mapped to the same output. To tell ffmpeg to write to stdout, use as the filename.\n\nConcatenate audio and video streams, joining them together one after the other. The filter works on segments of synchronized video and audio streams. All segments must have the same number of streams of each type, and that will also be the number of streams at output. unsafe – Activate unsafe mode: do not fail if segments have a different format. Related streams do not always have exactly the same duration, for various reasons including codec frame size or sloppy authoring. For that reason, related synchronized streams (e.g. a video and its audio track) should be concatenated at once. The concat filter will use the duration of the longest stream in each segment (except the last one), and if necessary pad shorter audio streams with silence. For this filter to work correctly, all segments must start at timestamp 0. All corresponding streams must have the same parameters in all segments; the filtering system will automatically select a common pixel format for video streams, and a common sample format, sample rate and channel layout for audio streams, but other settings, such as resolution, must be converted explicitly by the user. Different frame rates are acceptable but will result in variable frame rate at output; be sure to configure the output file to handle it.\n\nDraw a text string or text from a specified file on top of a video, using the libfreetype library. To enable compilation of this filter, you need to configure FFmpeg with . To enable default font fallback and the font option you need to configure FFmpeg with . To enable the text_shaping option, you need to configure FFmpeg with .\n• None box – Used to draw a box around text using the background color. The value must be either 1 (enable) or 0 (disable). The default value of box is 0.\n• None boxborderw – Set the width of the border to be drawn around the box using boxcolor. The default value of boxborderw is 0.\n• None boxcolor – The color to be used for drawing box around text. For the syntax of this option, check the “Color” section in the ffmpeg-utils manual. The default value of boxcolor is “white”.\n• None line_spacing – Set the line spacing in pixels of the border to be drawn around the box using box. The default value of line_spacing is 0.\n• None borderw – Set the width of the border to be drawn around the text using bordercolor. The default value of borderw is 0.\n• None bordercolor – Set the color to be used for drawing border around text. For the syntax of this option, check the “Color” section in the ffmpeg-utils manual. The default value of bordercolor is “black”.\n• None expansion – Select how the text is expanded. Can be either none, strftime (deprecated) or normal (default). See the Text expansion section below for details.\n• None basetime – Set a start time for the count. Value is in microseconds. Only applied in the deprecated strftime expansion mode. To emulate in normal expansion mode use the pts function, supplying the start time (in seconds) as the second argument.\n• None fix_bounds – If true, check and fix text coords to avoid clipping.\n• None fontcolor – The color to be used for drawing fonts. For the syntax of this option, check the “Color” section in the ffmpeg-utils manual. The default value of fontcolor is “black”.\n• None fontcolor_expr – String which is expanded the same way as text to obtain dynamic fontcolor value. By default this option has empty value and is not processed. When this option is set, it overrides fontcolor option.\n• None font – The font family to be used for drawing text. By default Sans.\n• None fontfile – The font file to be used for drawing text. The path must be included. This parameter is mandatory if the fontconfig support is disabled.\n• None alpha – Draw the text applying alpha blending. The value can be a number between 0.0 and 1.0. The expression accepts the same variables x, y as well. The default value is 1. Please see fontcolor_expr.\n• None fontsize – The font size to be used for drawing text. The default value of fontsize is 16.\n• None text_shaping – If set to 1, attempt to shape the text (for example, reverse the order of right-to-left text and join Arabic characters) before drawing it. Otherwise, just draw the text exactly as given. By default 1 (if supported).\n• None The flags to be used for loading the fonts. The flags map the corresponding flags supported by libfreetype, and are a combination of the following values: Default value is “default”. For more information consult the documentation for the FT_LOAD_* libfreetype flags.\n• None shadowcolor – The color to be used for drawing a shadow behind the drawn text. For the syntax of this option, check the “Color” section in the ffmpeg-utils manual. The default value of shadowcolor is “black”.\n• None shadowx – The x offset for the text shadow position with respect to the position of the text. It can be either positive or negative values. The default value is “0”.\n• None shadowy – The y offset for the text shadow position with respect to the position of the text. It can be either positive or negative values. The default value is “0”.\n• None start_number – The starting frame number for the n/frame_num variable. The default value is “0”.\n• None tabsize – The size in number of spaces to use for rendering the tab. Default value is 4.\n• None timecode – Set the initial timecode representation in “hh:mm:ss[:;.]ff” format. It can be used with or without text parameter. timecode_rate option must be specified.\n• None tc24hmax – If set to 1, the output of the timecode option will wrap around at 24 hours. Default is 0 (disabled).\n• None text – The text string to be drawn. The text must be a sequence of UTF-8 encoded characters. This parameter is mandatory if no file is specified with the parameter textfile.\n• None textfile – A text file containing text to be drawn. The text must be a sequence of UTF-8 encoded characters. This parameter is mandatory if no text string is specified with the parameter text. If both text and textfile are specified, an error is thrown.\n• None reload – If set to 1, the textfile will be reloaded before each frame. Be sure to update it atomically, or it may be read partially, or even fail.\n• None x – The expression which specifies the offset where text will be drawn within the video frame. It is relative to the left border of the output image. The default value is “0”.\n• None y – The expression which specifies the offset where text will be drawn within the video frame. It is relative to the top border of the output image. The default value is “0”. See below for the list of accepted constants and functions. The parameters for x and y are expressions containing the following constants and functions:\n• None dar: input display aspect ratio, it is the same as\n• None hsub: horizontal chroma subsample values. For example for the pixel format “yuv422p” hsub is 2 and vsub is 1.\n• None vsub: vertical chroma subsample values. For example for the pixel format “yuv422p” hsub is 2 and vsub is 1.\n• None line_h: the height of each text line\n• None ascent: the maximum distance from the baseline to the highest/upper grid coordinate used to place a glyph outline point, for all the rendered glyphs. It is a positive value, due to the grid’s orientation with the Y axis upwards.\n• None descent: the maximum distance from the baseline to the lowest grid coordinate used to place a glyph outline point, for all the rendered glyphs. This is a negative value, due to the grid’s orientation, with the Y axis upwards.\n• None max_glyph_h: maximum glyph height, that is the maximum height for all the glyphs contained in the rendered text, it is equivalent to ascent - descent.\n• None max_glyph_w: maximum glyph width, that is the maximum width for all the glyphs contained in the rendered text.\n• None n: the number of input frame, starting from 0\n• None t: timestamp expressed in seconds, NAN if the input timestamp is unknown\n• None text_h: the height of the rendered text\n• None text_w: the width of the rendered text\n• None x: the x offset coordinates where the text is drawn.\n• None y: the y offset coordinates where the text is drawn. These parameters allow the x and y expressions to refer each other, so you can for example specify .\n\nOverlay one video on top of another.\n• None x – Set the expression for the x coordinates of the overlaid video on the main video. Default value is 0. In case the expression is invalid, it is set to a huge value (meaning that the overlay will not be displayed within the output visible area).\n• None y – Set the expression for the y coordinates of the overlaid video on the main video. Default value is 0. In case the expression is invalid, it is set to a huge value (meaning that the overlay will not be displayed within the output visible area).\n• None The action to take when EOF is encountered on the secondary input; it accepts one of the following values:\n• None : Repeat the last frame (the default).\n• None Set when the expressions for x, and y are evaluated. It accepts the following values:\n• None : only evaluate expressions once during the filter initialization or when a command is\n• None shortest – If set to 1, force the output to terminate when the shortest input terminates. Default value is 0.\n• None Set the format for the output video. It accepts the following values:\n• None rgb (deprecated) – If set to 1, force the filter to accept inputs in the RGB color space. Default value is 0. This option is deprecated, use format instead.\n• None repeatlast – If set to 1, force the filter to draw the last overlay frame over the main input until the end of the stream. A value of 0 disables this behavior. Default value is 1.\n\nTrim the input so that the output contains one continuous subpart of the input.\n• None start – Specify the time of the start of the kept section, i.e. the frame with the timestamp start will be the first frame in the output.\n• None end – Specify the time of the first frame that will be dropped, i.e. the frame immediately preceding the one with the timestamp end will be the last frame in the output.\n• None start_pts – This is the same as start, except this option sets the start timestamp in timebase units instead of seconds.\n• None end_pts – This is the same as end, except this option sets the end timestamp in timebase units instead of seconds.\n• None duration – The maximum duration of the output in seconds.\n• None start_frame – The number of the first frame that should be passed to the output.\n• None end_frame – The number of the first frame that should be dropped."
    }
]