[
    {
        "link": "https://geeksforgeeks.org/python-program-for-bubble-sort",
        "document": "Bubble Sort is the simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in the wrong order.\n• None Bubble Sort algorithm, sorts an array by repeatedly comparing adjacent elements and swapping them if they are in the wrong order.\n• None The algorithm iterates through the array multiple times, with each pass pushing the largest unsorted element to its correct position at the end.\n• None Code includes an optimization: if no swaps are made during a pass, the array is already sorted, and the sorting process stops.\n\nBelow is the Python Implementation of Bubble Sort:\n\nTime Complexity: O(n2). However in practice, this optimized version might take less time as when array gets sorted, function would return.\n\nAuxiliary Space: O(1).\n\nPlease refer complete article on Bubble Sort for more details!\n\nWhen should I use Bubble Sort?\n\nWhat are the limitations of Bubble Sort?"
    },
    {
        "link": "https://analyticsvidhya.com/blog/2023/12/bubble-sort-in-python",
        "document": "Bubble Sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. While it may not be the most efficient sorting algorithm for large datasets, it is easy to understand and implement, making it a valuable learning tool for beginners. In this comprehensive guide, we’ll explore the principles behind Bubble Sort and provide a step-by-step implementation in Python.\n\nBubble Sort is a comparison-based algorithm that repeatedly compares adjacent elements and swaps them if they are in the wrong order. This process is repeated until the entire list is sorted. The algorithm gets its name from how smaller elements “bubble” to the top of the list during each iteration.\n\nTo implement Bubble Sort in Python, we follow a step-by-step algorithm. First, we compare adjacent elements and swap them if necessary. We repeat this process for each pair of adjacent elements until the entire list is sorted. We provide a code example below to illustrate the implementation.\n\nBelow is a simple implementation of the Bubble Sort algorithm in Python:\n\nThe time complexity of Bubble Sort is O(n^2), where n is the number of elements in the list. This means the algorithm’s performance decreases significantly as the list size increases. However, Bubble Sort can be efficient for small datasets or partially sorted lists.\n\nTo implement Bubble Sort in Python, we compare and swap the first two elements if necessary. We then move to the next pair of elements and continue this process until the end of the list. This constitutes one pass. We repeat this process for multiple passes until the list is completely sorted.\n\nHere is the illustration of how Bubble Sort works\n\nWhile Bubble Sort is not the most efficient sorting algorithm, several optimization techniques can improve its performance. Here, you will learn about the 3 bubble sorting techniques: Flagged Bubble Sort, Recursive Bubble Sort, and Cocktail Shaker Sort.\n• Flagged Bubble Sort uses a flag to track whether swaps were made during a pass. If no swaps occur, the list is sorted, and we can terminate the algorithm early.\n• Recursive Bubble Sort is a variation of Bubble Sort that uses recursion to sort the list. It divides the list into two parts: the first element and the remaining unsorted list. It then recursively sorts the remaining list until the entire list is sorted.\n• Cocktail Shaker Sort, also known as Bidirectional Bubble Sort, is an optimized version of Bubble Sort that sorts the list in both directions. This reduces the required passes and can improve the algorithm’s performance.\n\nBubble Sort can be applied in various scenarios where sorting is required. It is commonly used for sorting lists, arrays, and data structures such as linked lists and binary trees. Additionally, Bubble Sort can handle large datasets, although its performance may be slower than that of more efficient sorting algorithms. Here are the applications of Bubble Sort:\n• Sorting Lists and Arrays: Bubble Sort can sort lists and arrays of any data type. It is particularly useful when the list is small or partially sorted.\n• Sorting Data Structures: Bubble Sort can be applied to sort data structures like linked lists and binary trees. However, its performance may not be optimal for large data structures.\n• Sorting Large Datasets: While Bubble Sort is not the most efficient algorithm for sorting large datasets, it can still be used if performance is not critical. However, it is recommended that more efficient sorting algorithms for large datasets be considered.\n\nSorting algorithms are essential in computer science, and various algorithms have been developed to organize data efficiently. To understand their differences and performance characteristics, let’s compare Bubble Sort with a few other popular sorting algorithms.\n• Bubble Sort vs Selection Sort: Both algorithms have a time complexity of O(n^2), but Selection Sort performs fewer swaps, making it more efficient in practice.\n• Bubble Sort vs Insertion Sort: Insertion Sort is another comparison-based sorting algorithm that builds the final sorted list one element at a time. It performs better than Bubble Sort for small datasets and partially sorted lists.\n• Bubble Sort vs Merge Sort: Merge Sort is a divide-and-conquer algorithm that divides the list into smaller sublists, sorts them, and then merges them. It has a time complexity of O(n log n) and is more efficient than Bubble Sort for large datasets.\n• Bubble Sort vs Quick Sort: Quick Sort is another divide-and-conquer algorithm that partitions the list into two sublists, sorts them independently, and then combines them. It has an average time complexity of O(n log n) and is generally faster than Bubble Sort.\n\nIn this comprehensive guide, we explored the Bubble Sort algorithm in Python. Its working principle analyzed its time complexity and explored various optimization techniques. We also compared Bubble Sort with other sorting algorithms and provided a step-by-step implementation in Python. We also discussed the applications and use cases of Bubble Sort, as well as best practices and tips for efficient usage. Following the guidelines and techniques outlined in this guide, you can effectively apply Bubble Sort to your projects and achieve the desired sorting results. If you want a complete AI & ML program, register yourself @Certified AI & ML BlackBelt Plus Program today."
    },
    {
        "link": "https://geeksforgeeks.org/time-and-space-complexity-analysis-of-bubble-sort",
        "document": "The time complexity of Bubble Sort is O(n^2) in the worst-case scenario and the space complexity of Bubble sort is O(1). Bubble Sort only needs a constant amount of additional space during the sorting process.\n\nThe best case occurs when the array is already sorted. So the number of comparisons required is N-1 and the number of swaps required = 0. Hence the best case complexity is O(N).\n\nThe worst-case condition for bubble sort occurs when elements of the array are arranged in decreasing order.\n\nIn the worst case, the total number of iterations or passes required to sort a given array is (N-1). where ‘N’ is the number of elements present in the array.\n\nAt pass 1:\n\nNumber of comparisons = (N-1)\n\nNumber of swaps = (N-1) At pass 2:\n\nNumber of comparisons = (N-2)\n\nNumber of swaps = (N-2) At pass 3:\n\nNumber of comparisons = (N-3)\n\nNumber of swaps = (N-3)\n\n.\n\n.\n\n. At pass N-1:\n\nNumber of comparisons = 1\n\nNumber of swaps = 1 Now, calculating total number of comparison required to sort the array\n\n= (N-1) + (N-2) + (N-3) + . . . 2 + 1\n\n= (N-1)*(N-1+1)/2 { by using sum of N natural Number formula }\n\n= (N * (N-1)) / 2 In worst case, Total number of swaps = Total number of comparison\n\nTotal number of comparison (Worst case) = N(N-1)/2\n\nTotal number of swaps (Worst case) = N(N-1)/2 So worst case time complexity is O(N2) as N2 is the highest order term.\n\nThe number of comparisons is constant in Bubble Sort. So in average case, there are O(N2) comparisons. This is because irrespective of the arrangement of elements, the number of comparisons C(N) is same.\n\nFor the number of swaps, consider the following points:\n• None If an element is in index I1 but it should be in index I2, then it will take a minimum of (I2-I1) swaps to bring the element to the correct position.\n• None Consider an element E is at a distance of I3 from its position in sorted array. Then the maximum value of I3 will be (N-1) for the edge elements and N/2 for the elements at the middle. The sum of maximum difference in position across all elements will be: Therefore, in average case the number of comparisons is O(N2)\n\nThe space complexity of Bubble Sort is O(1). This means that the amount of extra space (memory) required by the algorithm remains constant regardless of the size of the input array being sorted. Bubble Sort only needs a constant amount of additional space to store temporary variables or indices during the sorting process. Therefore, the space complexity of Bubble Sort is considered to be very efficient as it does not depend on the input size and does not require additional space proportional to the input size."
    },
    {
        "link": "https://linkedin.com/pulse/bubblesort-algorithm-python-alex-weinberg-p1g4c",
        "document": "Bubble sort is one of the most intuitive sorting algorithms and a perfect starting point for anyone interested in the world of algorithms. Despite its simplicity, bubble sort provides a clear example of the fundamental concepts of sorting. In this post, we’ll walk through the mechanics of bubble sort by breaking down a Python implementation of the algorithm.\n\nBubble sort is a comparison-based algorithm that sorts a list by repeatedly stepping through the list, comparing adjacent elements, and swapping them if they are in the wrong order. The pass through the list is repeated until the list is sorted. The name “bubble sort” comes from the way smaller elements “bubble” to the top of the list (beginning of the array) while larger ones “sink” to the bottom (end of the array).\n• Start with the first two elements and compare them.\n• If the first element is greater than the second element, they are swapped.\n• Move to the next pair of elements and repeat the process.\n• Continue swapping until the largest element is “bubbled” to the correct position at the end of the list.\n• Reduce the range of elements by one (since the largest is now sorted) and repeat the process for the remaining elements.\n\ndef bubblesort(arr): n = len(arr) # Traverse through all array elements for i in range(n): # Last i elements are already in place, so the inner loop can skip them for j in range(n - i - 1): # Traverse the array from 0 to n-i-1 and swap if the element found is greater than the next element if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] return arr\n\nHow Does This Code Work?\n• We define a function bubblesort that takes an array arr as an argument.\n• We store the length of the array in a variable n because we need to know when to stop the sorting process.\n• We use a nested loop, where the outer loop represents the number of passes needed to sort the array and the inner loop goes through the elements to be compared.\n• Inside the inner loop, we perform the comparison: if arr[j] > arr[j+1]: checks if the current element is greater than the next element.\n• If the current element is greater, we swap the two elements using tuple unpacking: arr[j], arr[j+1] = arr[j+1], arr[j].\n• After each pass, the largest element in the current sub-list “bubbles up” to its correct position, and hence, the next iteration can ignore it, which we achieve by reducing the range of the inner loop.\n• The process continues until the array is sorted, and we return the sorted array.\n\nBubble sort, while not the most efficient algorithm for large datasets due to its O(n²) complexity, is an excellent educational tool for understanding the fundamental concepts of sorting. Its simplicity makes it perfect for small lists and for teaching purposes.\n\nIn our Python implementation, we saw how nested loops can be used to apply the algorithm’s logic and how simple swaps can lead to a fully sorted list. I hope this walk-through of bubble sort helps you understand not just how it works, but also the principles that underpin many other sorting algorithms."
    },
    {
        "link": "https://stackoverflow.com/questions/21272497/is-this-most-efficient-to-bubble-sort-a-list-in-python",
        "document": "Your algorithm is technically a bubble sort in that it does exactly the swaps that it should. However, it's a very inefficient bubble sort, in that it does a lot more compares than are necessary.\n\nHow can you know that? It's pretty easy to instrument your code to count the number of compares and swaps. And meanwhile, Wikipedia gives implementations of a simple bubble sort, and one with the skip-sorted-tail optimization, in a pseudocode language that's pretty easy to port to Python and similarly instrument. I'll show the code at the bottom.\n\nFor a perfect bubble sort, given a random list of length 100, you should expect a bit under 10000 compares (100 * 100), and a bit under 2500 swaps. And the Wikipedia implementation does exactly that. The \"skip-sorted-tail\" version should have just over half as many compares, and it does.\n\nYours, however, has 10x as many compares as it should. The reason your code is inefficient is that it starts over at the beginning over and over, instead of starting where it swapped whenever possible. This causes an extra factor of .\n\nMeanwhile, almost any sort algorithm is better than bubble sort for almost any input, so even an efficient bubble sort is not an efficient sort.\n\nI've made one minor change to your code: replacing the four-line swap with a more idiomatic single-line swap. Otherwise, nothing is changed but adding the and variables, and returning the result instead of printing it.\n\nThis is the Psuedocode implementation from Wikipedia, translated to Python. I had to replace the with a while True… if not …: break , but everything else is trivial.\n\nThis is the Optimizing bubble sort, which does the simple version of the skip-sorted-tail optimization, but not the more elaborate version (which comes right after it)."
    },
    {
        "link": "https://docs.python.org/3/howto/sorting.html",
        "document": "Python lists have a built-in method that modifies the list in-place. There is also a built-in function that builds a new sorted list from an iterable.\n\nIn this document, we explore the various techniques for sorting data using Python.\n\nA simple ascending sort is very easy: just call the function. It returns a new sorted list: You can also use the method. It modifies the list in-place (and returns to avoid confusion). Usually it’s less convenient than - but if you don’t need the original list, it’s slightly more efficient. Another difference is that the method is only defined for lists. In contrast, the function accepts any iterable.\n\nBoth and have a key parameter to specify a function (or other callable) to be called on each list element prior to making comparisons. \"This is a test string from Andrew\" ['a', 'Andrew', 'from', 'is', 'string', 'test', 'This'] The value of the key parameter should be a function (or other callable) that takes a single argument and returns a key to use for sorting purposes. This technique is fast because the key function is called exactly once for each input record. A common pattern is to sort complex objects using some of the object’s indices as keys. For example: The same technique works for objects with named attributes. For example: Objects with named attributes can be made by a regular class as shown above, or they can be instances of or a named tuple.\n\nThe key function patterns shown above are very common, so Python provides convenience functions to make accessor functions easier and faster. The module has , , and a function. Using those functions, the above examples become simpler and faster: The operator module functions allow multiple levels of sorting. For example, to sort by grade then by age: The module provides another helpful tool for making key-functions. The function can reduce the arity of a multi-argument function making it suitable for use as a key-function.\n\nSorts are guaranteed to be stable. That means that when multiple records have the same key, their original order is preserved. Notice how the two records for blue retain their original order so that is guaranteed to precede . This wonderful property lets you build complex sorts in a series of sorting steps. For example, to sort the student data by descending grade and then ascending age, do the age sort first and then sort again using grade: This can be abstracted out into a wrapper function that can take a list and tuples of field and order to sort them on multiple passes. The Timsort algorithm used in Python does multiple sorts efficiently because it can take advantage of any ordering already present in a dataset.\n\nThis idiom is called Decorate-Sort-Undecorate after its three steps:\n• None First, the initial list is decorated with new values that control the sort order.\n• None Second, the decorated list is sorted.\n• None Finally, the decorations are removed, creating a list that contains only the initial values in the new order. For example, to sort the student data by grade using the DSU approach: This idiom works because tuples are compared lexicographically; the first items are compared; if they are the same then the second items are compared, and so on. It is not strictly necessary in all cases to include the index i in the decorated list, but including it gives two benefits:\n• None The sort is stable – if two items have the same key, their order will be preserved in the sorted list.\n• None The original items do not have to be comparable because the ordering of the decorated tuples will be determined by at most the first two items. So for example the original list could contain complex numbers which cannot be sorted directly. Another name for this idiom is Schwartzian transform, after Randal L. Schwartz, who popularized it among Perl programmers. Now that Python sorting provides key-functions, this technique is not often needed.\n\nUnlike key functions that return an absolute value for sorting, a comparison function computes the relative ordering for two inputs. For example, a balance scale compares two samples giving a relative ordering: lighter, equal, or heavier. Likewise, a comparison function such as will return a negative value for less-than, zero if the inputs are equal, or a positive value for greater-than. It is common to encounter comparison functions when translating algorithms from other languages. Also, some libraries provide comparison functions as part of their API. For example, is a comparison function. To accommodate those situations, Python provides to wrap the comparison function to make it usable as a key function:\n• None For locale aware sorting, use for a key function or for a comparison function. This is necessary because “alphabetical” sort orderings can vary across cultures even if the underlying alphabet is the same.\n• None The reverse parameter still maintains sort stability (so that records with equal keys retain the original order). Interestingly, that effect can be simulated without the parameter by using the builtin function twice:\n• None The sort routines use when making comparisons between two objects. So, it is easy to add a standard sort order to a class by defining an method: However, note that can fall back to using if is not implemented (see for details on the mechanics). To avoid surprises, PEP 8 recommends that all six comparison methods be implemented. The decorator is provided to make that task easier.\n• None Key functions need not depend directly on the objects being sorted. A key function can also access external resources. For instance, if the student grades are stored in a dictionary, they can be used to sort a separate list of student names:"
    },
    {
        "link": "https://realpython.com/sorting-algorithms-python",
        "document": "Sorting is a basic building block that many other algorithms are built upon. It’s related to several exciting ideas that you’ll see throughout your programming career. Understanding how sorting algorithms in Python work behind the scenes is a fundamental step toward implementing correct and efficient algorithms that solve real-world problems.\n• How different sorting algorithms in Python work and how they compare under different circumstances\n• How Python’s built-in sort functionality works behind the scenes\n• How different computer science concepts like recursion and divide and conquer apply to sorting\n• How to measure the efficiency of an algorithm using Big O notation and Python’s module\n\nBy the end of this tutorial, you’ll understand sorting algorithms from both a theoretical and a practical standpoint. More importantly, you’ll have a deeper understanding of different algorithm design techniques that you can apply to other areas of your work. Let’s get started!\n\nThis tutorial covers two different ways to measure the runtime of sorting algorithms:\n• For a practical point of view, you’ll measure the runtime of the implementations using the module.\n• For a more theoretical perspective, you’ll measure the runtime complexity of the algorithms using Big O notation. When comparing two sorting algorithms in Python, it’s always informative to look at how long each one takes to run. The specific time each algorithm takes will be partly determined by your hardware, but you can still use the proportional time between executions to help you decide which implementation is more time efficient. In this section, you’ll focus on a practical way to measure the actual time it takes to run to your sorting algorithms using the module. For more information on the different ways you can time the execution of code in Python, check out Python Timer Functions: Three Ways to Monitor Your Code. Here’s a function you can use to time your algorithms: # Set up the context and prepare the call to the specified # algorithm using the supplied array. Only import the # algorithm function if it's not the built-in `sorted()`. \\ # Execute the code ten different times and return the time # in seconds that each execution took # Finally, display the name of the algorithm and the # minimum time it took to run In this example, receives the name of the algorithm and the input array that needs to be sorted. Here’s a line-by-line explanation of how it works:\n• Line 8 imports the name of the algorithm using the magic of Python’s f-strings. This is so that knows where to call the algorithm from. Note that this is only necessary for the custom implementations used in this tutorial. If the algorithm specified is the built-in , then nothing will be imported.\n• Line 11 prepares the call to the algorithm with the supplied array. This is the statement that will be executed and timed.\n• Line 15 calls with the setup code and the statement. This will call the specified sorting algorithm ten times, returning the number of seconds each one of these executions took.\n• Line 19 identifies the shortest time returned and prints it along with the name of the algorithm. Note: A common misconception is that you should find the average time of each run of the algorithm instead of selecting the single shortest time. Time measurements are noisy because the system runs other processes concurrently. The shortest time is always the least noisy, making it the best representation of the algorithm’s true runtime. Here’s an example of how to use to determine the time it takes to sort an array of ten thousand integer values using : # of random integer values between 0 and 999 # Call the function using the name of the sorting algorithm # and the array you just created If you save the above code in a file, then you can run it from the terminal and see its output: Remember that the time in seconds of every experiment depends in part on the hardware you use, so you’ll likely see slightly different results when running the code. Note: You can learn more about the module in the official Python documentation. The specific time an algorithm takes to run isn’t enough information to get the full picture of its time complexity. To solve this problem, you can use Big O (pronounced “big oh”) notation. Big O is often used to compare different implementations and decide which one is the most efficient, skipping unnecessary details and focusing on what’s most important in the runtime of an algorithm. The time in seconds required to run different algorithms can be influenced by several unrelated factors, including processor speed or available memory. Big O, on the other hand, provides a platform to express runtime complexity in hardware-agnostic terms. With Big O, you express complexity in terms of how quickly your algorithm’s runtime grows relative to the size of the input, especially as the input grows arbitrarily large. Assuming that n is the size of the input to an algorithm, the Big O notation represents the relationship between n and the number of steps the algorithm takes to find a solution. Big O uses a capital letter “O” followed by this relationship inside parentheses. For example, O(n) represents algorithms that execute a number of steps proportional to the size of their input. Although this tutorial isn’t going to dive very deep into the details of Big O notation, here are five examples of the runtime complexity of different algorithms: The runtime is constant regardless of the size of the input. Finding an element in a hash table is an example of an operation that can be performed in constant time. The runtime grows linearly with the size of the input. A function that checks a condition on every item of a list is an example of an O(n) algorithm. The runtime is a quadratic function of the size of the input. A naive implementation of finding duplicate values in a list, in which each item has to be checked twice, is an example of a quadratic algorithm. The runtime grows exponentially with the size of the input. These algorithms are considered extremely inefficient. An example of an exponential algorithm is the three-coloring problem. The runtime grows linearly while the size of the input grows exponentially. For example, if it takes one second to process one thousand elements, then it will take two seconds to process ten thousand, three seconds to process one hundred thousand, and so on. Binary search is an example of a logarithmic runtime algorithm. This tutorial covers the Big O runtime complexity of each of the sorting algorithms discussed. It also includes a brief explanation of how to determine the runtime on each particular case. This will give you a better understanding of how to start using Big O to classify other algorithms. Note: For a deeper understanding of Big O, together with several practical examples in Python, check out Big O Notation and Algorithm Analysis with Python Examples.\n\nBubble Sort is one of the most straightforward sorting algorithms. Its name comes from the way the algorithm works: With every new pass, the largest element in the list “bubbles up” toward its correct position. Bubble sort consists of making multiple passes through a list, comparing elements one by one, and swapping adjacent items that are out of order. Here’s an implementation of a bubble sort algorithm in Python: # Create a flag that will allow the function to # terminate early if there's nothing left to sort # Start looking at each item of the list one by one, # comparing it with its adjacent value. With each # iteration, the portion of the array that you look at # shrinks because the remaining items have already been # If the item you're looking at is greater than its # adjacent value, then swap them # Since you had to swap two elements, # set the `already_sorted` flag to `False` so the # If there were no swaps during the last iteration, # the array is already sorted, and you can terminate Since this implementation sorts the array in ascending order, each step “bubbles” the largest element to the end of the array. This means that each iteration takes fewer steps than the previous iteration because a continuously larger portion of the array is sorted. The loops in lines 4 and 10 determine the way the algorithm runs through the list. Notice how initially goes from the first element in the list to the element immediately before the last. During the second iteration, runs until two items from the last, then three items from the last, and so on. At the end of each iteration, the end portion of the list will be sorted. As the loops progress, line 15 compares each element with its adjacent value, and line 18 swaps them if they are in the incorrect order. This ensures a sorted list at the end of the function. Note: The flag in lines 13, 23, and 27 of the code above is an optimization to the algorithm, and it’s not required in a fully functional bubble sort implementation. However, it allows the function to skip unnecessary steps if the list ends up wholly sorted before the loops have finished. As an exercise, you can remove the use of this flag and compare the runtimes of both implementations. To properly analyze how the algorithm works, consider a list with values . Assume you’re using from above. Here’s a figure illustrating what the array looks like at each iteration of the algorithm: Now take a step-by-step look at what’s happening with the array as the algorithm progresses:\n• The code starts by comparing the first element, , with its adjacent element, . Since , the values are swapped, resulting in the following order: .\n• The algorithm then compares the second element, , with its adjacent element, . Since , the values are swapped, resulting in the following order: .\n• Next, the algorithm compares the third element, , with its adjacent element, . Since , it swaps the values as well, resulting in the following order: .\n• Finally, the algorithm compares the fourth element, , with its adjacent element, , and swaps them as well, resulting in . At this point, the algorithm completed the first pass through the list ( ). Notice how the value bubbled up from its initial location to its correct position at the end of the list.\n• The second pass ( ) takes into account that the last element of the list is already positioned and focuses on the remaining four elements, . At the end of this pass, the value finds its correct position. The third pass through the list positions the value , and so on until the list is sorted. Your implementation of bubble sort consists of two nested loops in which the algorithm performs n - 1 comparisons, then n - 2 comparisons, and so on until the final comparison is done. This comes at a total of (n - 1) + (n - 2) + (n - 3) + … + 2 + 1 = n(n-1)/2 comparisons, which can also be written as ½n2 - ½n. You learned earlier that Big O focuses on how the runtime grows in comparison to the size of the input. That means that, in order to turn the above equation into the Big O complexity of the algorithm, you need to remove the constants because they don’t change with the input size. Doing so simplifies the notation to n2 - n. Since n2 grows much faster than n, this last term can be dropped as well, leaving bubble sort with an average- and worst-case complexity of O(n2). In cases where the algorithm receives an array that’s already sorted—and assuming the implementation includes the flag optimization explained before—the runtime complexity will come down to a much better O(n) because the algorithm will not need to visit any element more than once. O(n), then, is the best-case runtime complexity of bubble sort. But keep in mind that best cases are an exception, and you should focus on the average case when comparing different algorithms. Using your from earlier in this tutorial, here’s the time it takes for bubble sort to process an array with ten thousand items. Line 8 replaces the name of the algorithm and everything else stays the same: # of random integer values between 0 and 999 # Call the function using the name of the sorting algorithm # and the array you just created You can now run the script to get the execution time of : It took seconds to sort the array with ten thousand elements. This represents the fastest execution out of the ten repetitions that runs. Executing this script multiple times will produce similar results. Note: A single execution of bubble sort took seconds, but the algorithm ran ten times using . This means that you should expect your code to take around seconds to run, assuming you have similar hardware characteristics. Slower machines may take much longer to finish. Analyzing the Strengths and Weaknesses of Bubble Sort The main advantage of the bubble sort algorithm is its simplicity. It is straightforward to both implement and understand. This is probably the main reason why most computer science courses introduce the topic of sorting using bubble sort. As you saw before, the disadvantage of bubble sort is that it is slow, with a runtime complexity of O(n2). Unfortunately, this rules it out as a practical candidate for sorting large arrays.\n\nLike bubble sort, the insertion sort algorithm is straightforward to implement and understand. But unlike bubble sort, it builds the sorted list one element at a time by comparing each item with the rest of the list and inserting it into its correct position. This “insertion” procedure gives the algorithm its name. An excellent analogy to explain insertion sort is the way you would sort a deck of cards. Imagine that you’re holding a group of cards in your hands, and you want to arrange them in order. You’d start by comparing a single card step by step with the rest of the cards until you find its correct position. At that point, you’d insert the card in the correct location and start over with a new card, repeating until all the cards in your hand were sorted. The insertion sort algorithm works exactly like the example with the deck of cards. Here’s the implementation in Python: # Loop from the second element of the array until # This is the element we want to position in its # Initialize the variable that will be used to # find the correct position of the element referenced # Run through the list of items (the left # portion of the array) and find the correct position # of the element referenced by `key_item`. Do this only # if `key_item` is smaller than its adjacent values. # Shift the value one position to the left # and reposition j to point to the next element # When you finish shifting the elements, you can position Unlike bubble sort, this implementation of insertion sort constructs the sorted list by pushing smaller items to the left. Let’s break down line by line:\n• Line 4 sets up the loop that determines the that the function will position during each iteration. Notice that the loop starts with the second item on the list and goes all the way to the last item.\n• Line 7 initializes with the item that the function is trying to place.\n• Line 12 initializes a variable that will consecutively point to each element to the left of . These are the elements that will be consecutively compared with .\n• Line 18 compares with each value to its left using a loop, shifting the elements to make room to place .\n• Line 27 positions in its correct place after the algorithm shifts all the larger values to the right. Here’s a figure illustrating the different iterations of the algorithm when sorting the array : Now here’s a summary of the steps of the algorithm when sorting the array:\n• The algorithm starts with and goes through the subarray to its left to find the correct position for it. In this case, the subarray is .\n• Since , the algorithm shifts element one position to its right. The resultant array at this point is .\n• Since there are no more elements in the subarray, the is now placed in its new position, and the final array is .\n• The second pass starts with and goes through the subarray located to its left, in this case .\n• Since , the algorithm shifts 8 to its right. The resultant array at this point is .\n• Since , the algorithm doesn’t need to keep going through the subarray, so it positions and finishes the second pass. At this time, the resultant array is .\n• The third pass through the list puts the element in its correct position, and the fourth pass places element in the correct spot, leaving the array sorted. Similar to your bubble sort implementation, the insertion sort algorithm has a couple of nested loops that go over the list. The inner loop is pretty efficient because it only goes through the list until it finds the correct position of an element. That said, the algorithm still has an O(n2) runtime complexity on the average case. The worst case happens when the supplied array is sorted in reverse order. In this case, the inner loop has to execute every comparison to put every element in its correct position. This still gives you an O(n2) runtime complexity. The best case happens when the supplied array is already sorted. Here, the inner loop is never executed, resulting in an O(n) runtime complexity, just like the best case of bubble sort. Although bubble sort and insertion sort have the same Big O runtime complexity, in practice, insertion sort is considerably more efficient than bubble sort. If you look at the implementation of both algorithms, then you can see how insertion sort has to make fewer comparisons to sort the list. To prove the assertion that insertion sort is more efficient than bubble sort, you can time the insertion sort algorithm and compare it with the results of bubble sort. To do this, you just need to replace the call to with the name of your insertion sort implementation: # of random integer values between 0 and 999 # Call the function using the name of the sorting algorithm # and the array we just created You can execute the script as before: Notice how the insertion sort implementation took around fewer seconds than the bubble sort implementation to sort the same array. Even though they’re both O(n2) algorithms, insertion sort is more efficient. Analyzing the Strengths and Weaknesses of Insertion Sort Just like bubble sort, the insertion sort algorithm is very uncomplicated to implement. Even though insertion sort is an O(n2) algorithm, it’s also much more efficient in practice than other quadratic implementations such as bubble sort. There are more powerful algorithms, including merge sort and Quicksort, but these implementations are recursive and usually fail to beat insertion sort when working on small lists. Some Quicksort implementations even use insertion sort internally if the list is small enough to provide a faster overall implementation. Timsort also uses insertion sort internally to sort small portions of the input array. That said, insertion sort is not practical for large arrays, opening the door to algorithms that can scale in more efficient ways.\n\nMerge sort is a very efficient sorting algorithm. It’s based on the divide-and-conquer approach, a powerful algorithmic technique used to solve complex problems. To properly understand divide and conquer, you should first understand the concept of recursion. Recursion involves breaking a problem down into smaller subproblems until they’re small enough to manage. In programming, recursion is usually expressed by a function calling itself. Note: This tutorial doesn’t explore recursion in depth. To better understand how recursion works and see it in action using Python, check out Thinking Recursively in Python and Recursion in Python: An Introduction.\n• The original input is broken into several parts, each one representing a subproblem that’s similar to the original but simpler.\n• The solutions to all the subproblems are combined into a single overall solution. In the case of merge sort, the divide-and-conquer approach divides the set of input values into two equal-sized parts, sorts each half recursively, and finally merges these two sorted parts into a single sorted list. The implementation of the merge sort algorithm needs two different pieces:\n• A function that recursively splits the input in half Here’s the code to merge two different arrays: # If the first array is empty, then nothing needs # to be merged, and you can return the second array as the result # If the second array is empty, then nothing needs # to be merged, and you can return the first array as the result # Now go through both arrays until all the elements # make it into the resultant array # The elements need to be sorted to add them to the # resultant array, so you need to decide whether to get # the next element from the first or the second array # If you reach the end of either array, then you can # add the remaining elements from the other array to # the result and break the loop receives two different sorted arrays that need to be merged together. The process to accomplish this is straightforward:\n• Lines 4 and 9 check whether either of the arrays is empty. If one of them is, then there’s nothing to merge, so the function returns the other array.\n• Line 17 starts a loop that ends whenever the result contains all the elements from both of the supplied arrays. The goal is to look into both arrays and combine their items to produce a sorted list.\n• Line 21 compares the elements at the head of both arrays, selects the smaller value, and appends it to the end of the resultant array.\n• Lines 31 and 35 append any remaining items to the result if all the elements from either of the arrays were already used. With the above function in place, the only missing piece is a function that recursively splits the input array in half and uses to produce the final result: # If the input array contains fewer than two elements, # then return it as the result of the function # Sort the array by recursively splitting the input # into two equal halves, sorting each half and merging them # together into the final result\n• Line 44 acts as the stopping condition for the recursion. If the input array contains fewer than two elements, then the function returns the array. Notice that this condition could be triggered by receiving either a single item or an empty array. In both cases, there’s nothing left to sort, so the function should return.\n• Line 47 computes the middle point of the array.\n• Line 52 calls , passing both sorted halves as the arrays. Notice how this function calls itself recursively, halving the array each time. Each iteration deals with an ever-shrinking array until fewer than two elements remain, meaning there’s nothing left to sort. At this point, takes over, merging the two halves and producing a sorted list. Take a look at a representation of the steps that merge sort will take to sort the array : The figure uses yellow arrows to represent halving the array at each recursion level. The green arrows represent merging each subarray back together. The steps can be summarized as follows:\n• The first call to with defines as . The is used to halve the input array into and , producing and , respectively. is then recursively called for each half to sort them separately.\n• The call to with produces and . The process repeats for each of these halves.\n• The call to with returns since that’s the only element. The same happens with the call to with .\n• At this point, the function starts merging the subarrays back together using , starting with and as input arrays, producing as the result.\n• On the other side, is recursively broken down and merged using the same procedure, producing as the result.\n• In the final step, and are merged back together with , producing the final result: . To analyze the complexity of merge sort, you can look at its two steps separately:\n• has a linear runtime. It receives two arrays whose combined length is at most n (the length of the original input array), and it combines both arrays by looking at each element at most once. This leads to a runtime complexity of O(n).\n• The second step splits the input array recursively and calls for each half. Since the array is halved until a single element remains, the total number of halving operations performed by this function is log n. Since is called for each half, we get a total runtime of O(n log n). Interestingly, O(n log n) is the best possible worst-case runtime that can be achieved by a sorting algorithm. To compare the speed of merge sort with the previous two implementations, you can use the same mechanism as before and replace the name of the algorithm in line 8: # of random integer values between 0 and 999 # Call the function using the name of the sorting algorithm # and the array you just created You can execute the script to get the execution time of : Compared to bubble sort and insertion sort, the merge sort implementation is extremely fast, sorting the ten-thousand-element array in less than a second! Analyzing the Strengths and Weaknesses of Merge Sort Thanks to its runtime complexity of O(n log n), merge sort is a very efficient algorithm that scales well as the size of the input array grows. It’s also straightforward to parallelize because it breaks the input array into chunks that can be distributed and processed in parallel if necessary. That said, for small lists, the time cost of the recursion allows algorithms such as bubble sort and insertion sort to be faster. For example, running an experiment with a list of ten elements results in the following times: Both bubble sort and insertion sort beat merge sort when sorting a ten-element list. Another drawback of merge sort is that it creates copies of the array when calling itself recursively. It also creates a new list inside to sort and return both input halves. This makes merge sort use much more memory than bubble sort and insertion sort, which are both able to sort the list in place. Due to this limitation, you may not want to use merge sort to sort large lists in memory-constrained hardware.\n\nJust like merge sort, the Quicksort algorithm applies the divide-and-conquer principle to divide the input array into two lists, the first with small items and the second with large items. The algorithm then sorts both lists recursively until the resultant list is completely sorted. Dividing the input list is referred to as partitioning the list. Quicksort first selects a element and partitions the list around the , putting every smaller element into a array and every larger element into a array. Putting every element from the list to the left of the and every element from the list to the right positions the precisely where it needs to be in the final sorted list. This means that the function can now recursively apply the same procedure to and then until the entire list is sorted. # If the input array contains fewer than two elements, # then return it as the result of the function # Elements that are smaller than the `pivot` go to # the `low` list. Elements that are larger than # `pivot` go to the `high` list. Elements that are # equal to `pivot` go to the `same` list. # with the `same` list and the sorted `high` list\n• Line 6 stops the recursive function if the array contains fewer than two elements.\n• Line 12 selects the element randomly from the list and proceeds to partition the list.\n• Lines 19 and 20 put every element that’s smaller than into the list called .\n• Lines 21 and 22 put every element that’s equal to into the list called .\n• Lines 23 and 24 put every element that’s larger than into the list called .\n• Line 28 recursively sorts the and lists and combines them along with the contents of the list. Here’s an illustration of the steps that Quicksort takes to sort the array : The yellow lines represent the partitioning of the array into three lists: , , and . The green lines represent sorting and putting these lists back together. Here’s a brief explanation of the steps:\n• The element is selected randomly. In this case, is .\n• The first pass partitions the input array so that contains , contains , and contains .\n• is then called recursively with as its input. This selects a random and breaks the array into as , as , and as .\n• The process continues, but at this point, both and have fewer than two items each. This ends the recursion, and the function puts the array back together. Adding the sorted and to either side of the list produces .\n• On the other side, the list containing has fewer than two elements, so the algorithm returns the sorted array, which is now . Merging it with ( ) and ( ) produces the final sorted list. Why does the implementation above select the element randomly? Wouldn’t it be the same to consistently select the first or last element of the input list? Because of how the Quicksort algorithm works, the number of recursion levels depends on where ends up in each partition. In the best-case scenario, the algorithm consistently picks the median element as the . That would make each generated subproblem exactly half the size of the previous problem, leading to at most log n levels. On the other hand, if the algorithm consistently picks either the smallest or largest element of the array as the , then the generated partitions will be as unequal as possible, leading to n-1 recursion levels. That would be the worst-case scenario for Quicksort. As you can see, Quicksort’s efficiency often depends on the selection. If the input array is unsorted, then using the first or last element as the will work the same as a random element. But if the input array is sorted or almost sorted, using the first or last element as the could lead to a worst-case scenario. Selecting the at random makes it more likely Quicksort will select a value closer to the median and finish faster. Another option for selecting the is to find the median value of the array and force the algorithm to use it as the . This can be done in O(n) time. Although the process is little bit more involved, using the median value as the for Quicksort guarantees you will have the best-case Big O scenario. With Quicksort, the input list is partitioned in linear time, O(n), and this process repeats recursively an average of log n times. This leads to a final complexity of O(n log n). That said, remember the discussion about how the selection of the affects the runtime of the algorithm. The O(n) best-case scenario happens when the selected is close to the median of the array, and an O(n2) scenario happens when the is the smallest or largest value of the array. Theoretically, if the algorithm focuses first on finding the median value and then uses it as the element, then the worst-case complexity will come down to O(n log n). The median of an array can be found in linear time, and using it as the guarantees the Quicksort portion of the code will perform in O(n log n). By using the median value as the , you end up with a final runtime of O(n) + O(n log n). You can simplify this down to O(n log n) because the logarithmic portion grows much faster than the linear portion. Note: Although achieving O(n log n) is possible in Quicksort’s worst-case scenario, this approach is seldom used in practice. Lists have to be quite large for the implementation to be faster than a simple randomized selection of the . Randomly selecting the makes the worst case very unlikely. That makes random selection good enough for most implementations of the algorithm. By now, you’re familiar with the process for timing the runtime of the algorithm. Just change the name of the algorithm in line 8: # of random integer values between 0 and 999 # Call the function using the name of the sorting algorithm # and the array you just created You can execute the script as you have before: Not only does Quicksort finish in less than one second, but it’s also much faster than merge sort ( seconds versus seconds). Increasing the number of elements specified by from to and running the script again ends up with merge sort finishing in seconds, whereas Quicksort sorts the list in a mere seconds. Analyzing the Strengths and Weaknesses of Quicksort True to its name, Quicksort is very fast. Although its worst-case scenario is theoretically O(n2), in practice, a good implementation of Quicksort beats most other sorting implementations. Also, just like merge sort, Quicksort is straightforward to parallelize. One of Quicksort’s main disadvantages is the lack of a guarantee that it will achieve the average runtime complexity. Although worst-case scenarios are rare, certain applications can’t afford to risk poor performance, so they opt for algorithms that stay within O(n log n) regardless of the input. Just like merge sort, Quicksort also trades off memory space for speed. This may become a limitation for sorting larger lists. A quick experiment sorting a list of ten elements leads to the following results: The results show that Quicksort also pays the price of recursion when the list is sufficiently small, taking longer to complete than both insertion sort and bubble sort.\n\nThe Timsort algorithm is considered a hybrid sorting algorithm because it employs a best-of-both-worlds combination of insertion sort and merge sort. Timsort is near and dear to the Python community because it was created by Tim Peters in 2002 to be used as the standard sorting algorithm of the Python language. The main characteristic of Timsort is that it takes advantage of already-sorted elements that exist in most real-world datasets. These are called natural runs. The algorithm then iterates over the list, collecting the elements into runs and merging them into a single sorted list. In this section, you’ll create a barebones Python implementation that illustrates all the pieces of the Timsort algorithm. If you’re interested, you can also check out the original C implementation of Timsort. The first step in implementing Timsort is modifying the implementation of from before: # Loop from the element indicated by # `left` until the element indicated by `right` # This is the element we want to position in its # Initialize the variable that will be used to # find the correct position of the element referenced # Run through the list of items (the left # portion of the array) and find the correct position # of the element referenced by `key_item`. Do this only # if the `key_item` is smaller than its adjacent values. # Shift the value one position to the left # and reposition `j` to point to the next element # When you finish shifting the elements, position # the `key_item` in its correct location This modified implementation adds a couple of parameters, and , that indicate which portion of the array should be sorted. This allows the Timsort algorithm to sort a portion of the array in place. Modifying the function instead of creating a new one means that it can be reused for both insertion sort and Timsort. Now take a look at the implementation of Timsort: # Start by slicing and sorting small portions of the # input array. The size of these slices is defined by # Now you can start merging the sorted slices. # Start from `min_run`, doubling the size on # each iteration until you surpass the length of # Determine the arrays that will # Compute the `midpoint` (where the first array ends # and the second starts) and the `endpoint` (where # The `left` array should go from `start` to # `midpoint + 1`, while the `right` array should # go from `midpoint + 1` to `end + 1`. # Each iteration should double the size of your arrays Although the implementation is a bit more complex than the previous algorithms, we can summarize it quickly in the following way:\n• Lines 8 and 9 create small slices, or runs, of the array and sort them using insertion sort. You learned previously that insertion sort is speedy on small lists, and Timsort takes advantage of this. Timsort uses the newly introduced and parameters in to sort the list in place without having to create new arrays like merge sort and Quicksort do.\n• Line 16 merges these smaller runs, with each run being of size initially. With each iteration, the size of the runs is doubled, and the algorithm continues merging these larger runs until a single sorted run remains. Notice how, unlike merge sort, Timsort merges subarrays that were previously sorted. Doing so decreases the total number of comparisons required to produce a sorted list. This advantage over merge sort will become apparent when running experiments using different arrays. Finally, line 2 defines . There are two reasons for using as the value here:\n• Sorting small arrays using insertion sort is very fast, and has a small value to take advantage of this characteristic. Initializing with a value that’s too large will defeat the purpose of using insertion sort and will make the algorithm slower.\n• Merging two balanced lists is much more efficient than merging lists of disproportionate size. Picking a value that’s a power of two ensures better performance when merging all the different runs that the algorithm creates. Combining both conditions above offers several options for . The implementation in this tutorial uses as one of the possibilities. Note: In practice, Timsort does something a little more complicated to compute . It picks a value between 32 and 64 inclusive, such that the length of the list divided by is exactly a power of 2. If that’s not possible, it chooses a value that’s close to, but strictly less than, a power of 2. If you’re curious, you can read the complete analysis on how to pick under the Computing minrun section. On average, the complexity of Timsort is O(n log n), just like merge sort and Quicksort. The logarithmic part comes from doubling the size of the run to perform each linear merge operation. However, Timsort performs exceptionally well on already-sorted or close-to-sorted lists, leading to a best-case scenario of O(n). In this case, Timsort clearly beats merge sort and matches the best-case scenario for Quicksort. But the worst case for Timsort is also O(n log n), which surpasses Quicksort’s O(n2). You can use to see how Timsort performs sorting the ten-thousand-element array: # of random integer values between 0 and 999 # Call the function using the name of the sorting algorithm # and the array you just created Now execute the script to get the execution time of : At seconds, this Timsort implementation is a full seconds, or 17 percent, faster than merge sort, though it doesn’t match the of Quicksort. It’s also a ridiculous 11,000 percent faster than insertion sort! Now try to sort an already-sorted list using these four algorithms and see what happens. You can modify your section as follows: # Call each of the functions If you execute the script now, then all the algorithms will run and output their corresponding execution time: This time, Timsort comes in at a whopping thirty-seven percent faster than merge sort and five percent faster than Quicksort, flexing its ability to take advantage of the already-sorted runs. Notice how Timsort benefits from two algorithms that are much slower when used by themselves. The genius of Timsort is in combining these algorithms and playing to their strengths to achieve impressive results. Analyzing the Strengths and Weaknesses of Timsort The main disadvantage of Timsort is its complexity. Despite implementing a very simplified version of the original algorithm, it still requires much more code because it relies on both and . One of Timsort’s advantages is its ability to predictably perform in O(n log n) regardless of the structure of the input array. Contrast that with Quicksort, which can degrade down to O(n2). Timsort is also very fast for small arrays because the algorithm turns into a single insertion sort. For real-world usage, in which it’s common to sort arrays that already have some preexisting order, Timsort is a great option. Its adaptability makes it an excellent choice for sorting arrays of any length."
    },
    {
        "link": "https://wiki.python.org/moin/TimeComplexity",
        "document": "This page documents the time-complexity (aka \"Big O\" or \"Big Oh\") of various operations in current CPython. Other Python implementations (or older or still-under development versions of CPython) may have slightly different performance characteristics. However, it is generally safe to assume that they are not slower by more than a factor of O(log n).\n\nGenerally, 'n' is the number of elements currently in the container. 'k' is either the value of a parameter or the number of elements in the parameter.\n\nInternally, a list is represented as an array; the largest costs come from growing beyond the current allocation size (because everything must move), or from inserting or deleting somewhere near the beginning (because everything after that must move). If you need to add/remove at both ends, consider using a collections.deque instead.\n\nA deque (double-ended queue) is represented internally as a doubly linked list. (Well, a list of arrays rather than objects, for greater efficiency.) Both ends are accessible, but even looking at the middle is slow, and adding to or removing from the middle is slower still.\n\nSee dict -- the implementation is intentionally very similar.\n• None As seen in the source code the complexities for set difference s-t or s.difference(t) ( ) and in-place set difference s.difference_update(t) ( ) are different! The first one is O(len(s)) (for every element in s add it to the new set, if not in t). The second one is O(len(t)) (for every element in t remove it from s). So care must be taken as to which is preferred, depending on which one is the longest set and whether a new set is needed.\n• To perform set operations like s-t, both s and t need to be sets. However you can do the method equivalents even if t is any iterable, for example s.difference(l), where l is a list.\n\nThe Average Case times listed for dict objects assume that the hash function for the objects is sufficiently robust to make collisions uncommon. The Average Case assumes the keys used in parameters are selected uniformly at random from the set of all keys.\n\nNote that there is a fast-path for dicts that (in practice) only deal with str keys; this doesn't affect the algorithmic complexity, but it can significantly affect the constant factors: how quickly a typical program finishes.\n\n[1] = These operations rely on the \"Amortized\" part of \"Amortized Worst Case\". Individual actions may take surprisingly long, depending on the history of the container.\n\n[2] = Popping the intermediate element at index k from a list of size n shifts all elements after k by one slot to the left using memmove. n - k elements have to be moved, so the operation is O(n - k). The best case is popping the second to last element, which necessitates one move, the worst case is popping the first element, which involves n - 1 moves. The average case for an average value of k is popping the element the middle of the list, which takes O(n/2) = O(n) operations.\n\n[3] = For these operations, the worst case n is the maximum size the container ever achieved, rather than just the current size. For example, if N objects are added to a dictionary, then N-1 are deleted, the dictionary will still be sized for N objects (at least) until another insertion is made."
    },
    {
        "link": "https://stackoverflow.com/questions/14434490/what-is-the-complexity-of-the-sorted-function",
        "document": "I have a list of lists and I am sorting them using the following\n\nWas wondering what is the runtime complexity of this python function?"
    },
    {
        "link": "https://geeksforgeeks.org/sorting-algorithms-in-python",
        "document": "Sorting is defined as an arrangement of data in a certain order. Sorting techniques are used to arrange data(mostly numerical) in an ascending or descending order. It is a method used for the representation of data in a more comprehensible format.\n• None Sorting a large amount of data can take a substantial amount of computing resources if the methods we use to sort the data are inefficient.\n• None The efficiency of the algorithm is proportional to the number of items it is traversing.\n• None For a small amount of data, a complex sorting method may be more trouble than it is worth.\n• None On the other hand, for larger amounts of data, we want to increase the efficiency and speed as far as possible.\n\nWe will now discuss the several sorting techniques and compare them with respect to their time complexity.\n\nSome of the real-life examples of sorting are:\n• Telephone Directory: It is a book that contains telephone numbers and addresses of people in alphabetical order.\n• Dictionary: It is a huge collection of words along with their meanings in alphabetical order.\n• Contact List: It is a list of contact numbers of people in alphabetical order on a mobile phone.\n\nBefore discussing the different algorithms used to sort the data given to us, we should think about the operations which can be used for the analysis of a sorting process. First, we need to compare the values to see which one is smaller and which one is larger so that they can be sorted into an order, it will be necessary to have an organized way to compare values to see that if they are in order.\n\nThe different types of order are:\n• Increasing Order: A set of values are said to be increasing order when every successive element is greater than its previous element. For example: 1, 2, 3, 4, 5. Here, the given sequence is in increasing order.\n• Decreasing Order: A set of values are said to be in decreasing order when the successive element is always less than the previous one. For Example: 5, 4, 3, 2, 1. Here the given sequence is in decreasing order.\n• Non-Increasing Order: A set of values are said to be in non-increasing order if every i element present in the sequence is less than or equal to its (i-1) element. This order occurs whenever there are numbers that are being repeated. For Example: 5, 4, 3, 2, 2, 1. Here 2 repeated two times.\n• Non-Decreasing Order: A set of values are said to be in non-decreasing order if every i element present in the sequence is greater than or equal to its (i-1) element. This order occurs whenever there are numbers that are being repeated. For Example: 1, 2, 2, 3, 4, 5. Here 2 repeated two times.\n\nThe different implementations of sorting techniques in Python are:\n\nBubble Sort is the simplest sorting algorithm that works by repeatedly swapping the adjacent elements if they are in the wrong order.\n• None Bubble Sort algorithm, sorts an array by repeatedly comparing adjacent elements and swapping them if they are in the wrong order.\n• None The algorithm iterates through the array multiple times, with each pass pushing the largest unsorted element to its correct position at the end.\n• None Code includes an optimization: if no swaps are made during a pass, the array is already sorted, and the sorting process stops.\n\nSelection Sort is a comparison-based sorting algorithm. It sorts an array by repeatedly selecting the smallest (or largest) element from the unsorted portion and swapping it with the first unsorted element. This process continues until the entire array is sorted.\n• None First we find the smallest element and swap it with the first element. This way we get the smallest element at its correct position.\n• None Then we find the smallest among remaining elements (or second smallest) and swap it with the second element.\n• None We keep doing this until we get all elements moved to correct position.\n\nInsertion sort is a simple sorting algorithm that works by iteratively inserting each element of an unsorted list into its correct position in a sorted portion of the list.\n• None The insertionSort function takes an array arr as input. It first calculates the length of the array (n). If the length is 0 or 1, the function returns immediately as an array with 0 or 1 element is considered already sorted.\n• None For arrays with more than one element, We start with second element of the array as first element in the array is assumed to be sorted.\n• None Compare second element with the first element and check if the second element is smaller then swap them.\n• None Move to the third element and compare it with the first two elements and put at its correct position\n• None Repeat until the entire array is sorted.\n\nMerge Sort is a Divide and Conquer algorithm. It divides input array in two halves, calls itself for the two halves and then merges the two sorted halves. The merge() function is used for merging two halves. The merge(arr, l, m, r) is key process that assumes that arr[l..m] and arr[m+1..r] are sorted and merges the two sorted sub-arrays into one.\n\nQuickSort is a sorting algorithm based on the Divide and Conquer that picks an element as a pivot and partitions the given array around the picked pivot by placing the pivot in its correct position in the sorted array.\n\nHeapsort is a comparison-based sorting technique based on a Binary Heap data structure. It is similar to selection sort where we first find the maximum element and place the maximum element at the end. We repeat the same process for the remaining element.\n\nCycle sort is an in-place, unstable sorting algorithm that is particularly useful when sorting arrays containing elements with a small range of values. It is optimal in terms of several memory writes. It minimizes the number of memory writes to sort (Each value is either written zero times, if it’s already in its correct position or written one time to its correct position.)\n\nIt is based on the idea that the array to be sorted can be divided into cycles. Cycles can be visualized as a graph. We have n nodes and an edge directed from node i to node j if the element at i-th index must be present at j-th index in the sorted array.\n\nCycle in arr[] = {2, 4, 5, 1, 3}\n\nMerge Sort is a divide-and-conquer algorithm that recursively splits an array into two halves, sorts each half, and then merges them. A variation of this is 3-way Merge Sort, where instead of splitting the array into two parts, we divide it into three equal parts.\n\nIn traditional Merge Sort, the array is recursively divided into halves until we reach subarrays of size 1. In 3-way Merge Sort, the array is recursively divided into three parts, reducing the depth of recursion and potentially improving efficiency.\n\nCounting Sort is a non-comparison-based sorting algorithm. It is particularly efficient when the range of input values is small compared to the number of elements to be sorted. The basic idea behind Counting Sort is to count the frequency of each distinct element in the input array and use that information to place the elements in their correct sorted positions. For example, for input [1, 4, 3, 2, 2, 1], the output should be [1, 1, 2, 2, 3, 4]. The important thing to notice is that the range of input elements is small and comparable to the size of the array.\n\nRadix Sort is a linear sorting algorithm that sorts elements by processing them digit by digit. It is an efficient sorting algorithm for integers or strings with fixed-size keys.\n\nRather than comparing elements directly, Radix Sort distributes the elements into buckets based on each digit’s value. By repeatedly sorting the elements by their significant digits, from the least significant to the most significant, Radix Sort achieves the final sorted order.\n\nBucket sort is a sorting technique that involves dividing elements into various groups, or buckets. These buckets are formed by uniformly distributing the elements. Once the elements are divided into buckets, they can be sorted using any other sorting algorithm. Finally, the sorted elements are gathered together in an ordered fashion.\n\nTim Sort is a hybrid sorting algorithm derived from merge sort and insertion sort. It is designed to perform well on many kinds of real-world data. Tim Sort’s efficiency comes from its ability to exploit the structure present in the data, such as runs (consecutive sequences that are already ordered) and merges these runs using a modified merge sort approach. It was Created by Tim Peters in 2002, Tim Sort is the default sorting algorithm in Python and is renowned for its speed and efficiency in real-world data scenarios.\n\nComb Sort is an improvement over Bubble Sort, and it aims to eliminate the problem of small values near the end of the list, which causes Bubble Sort to take more time than necessary. Comb Sort uses a larger gap for comparison, which gradually reduces until it becomes 1 (like the gap in Shell Sort). By doing this, it helps in more efficient sorting by “jumping over” some unnecessary comparisons and swaps.\n• None The shrink factor has been empirically found to be 1.3 (by testing Comb sort on over 200,000 random lists)\n• None Although it works better than Bubble Sort on average, the worst case remains O (n2)\n\nPigeonhole Sort is a sorting algorithm that is suitable for sorting lists of elements where the number of elements and the number of possible key values are approximately the same. It requires O(n + Range) time where n is number of elements in input array and ‘Range’ is number of possible values in array.\n\nShell Sort is an advanced version of the insertion sort algorithm that improves its efficiency by comparing and sorting elements that are far apart. The idea behind Shell Sort is to break the original list into smaller sublists, sort these sublists, and gradually reduce the gap between the sublist elements until the list is sorted."
    }
]