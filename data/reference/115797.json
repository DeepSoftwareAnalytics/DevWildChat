[
    {
        "link": "https://docs.docker.com/reference/dockerfile",
        "document": "Docker can build images automatically by reading the instructions from a Dockerfile. A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image. This page describes the commands you can use in a Dockerfile.\n\nThe Dockerfile supports the following instructions:\n\nHere is the format of the Dockerfile:\n\nThe instruction is not case-sensitive. However, convention is for them to be UPPERCASE to distinguish them from arguments more easily.\n\nDocker runs instructions in a Dockerfile in order. A Dockerfile must begin with a instruction. This may be after parser directives, comments, and globally scoped ARGs. The instruction specifies the base image from which you are building. may only be preceded by one or more instructions, which declare arguments that are used in lines in the Dockerfile.\n\nBuildKit treats lines that begin with as a comment, unless the line is a valid parser directive. A marker anywhere else in a line is treated as an argument. This allows statements like:\n\nComment lines are removed before the Dockerfile instructions are executed. The comment in the following example is removed before the shell executes the command.\n\nThe following examples is equivalent.\n\nParser directives are optional, and affect the way in which subsequent lines in a Dockerfile are handled. Parser directives don't add layers to the build, and don't show up as build steps. Parser directives are written as a special type of comment in the form . A single directive may only be used once.\n\nThe following parser directives are supported:\n\nOnce a comment, empty line or builder instruction has been processed, BuildKit no longer looks for parser directives. Instead it treats anything formatted as a parser directive as a comment and doesn't attempt to validate if it might be a parser directive. Therefore, all parser directives must be at the top of a Dockerfile.\n\nParser directive keys, such as or , aren't case-sensitive, but they're lowercase by convention. Values for a directive are case-sensitive and must be written in the appropriate case for the directive. For example, is invalid because the check name must use Pascal case, not lowercase. It's also conventional to include a blank line following any parser directives. Line continuation characters aren't supported in parser directives.\n\nDue to these rules, the following examples are all invalid:\n\nTreated as a comment because it appears after a builder instruction:\n\nTreated as a comment because it appears after a comment that isn't a parser directive:\n\nThe following is treated as a comment because it isn't recognized. The known directive is treated as a comment because it appears after a comment that isn't a parser directive.\n\nNon line-breaking whitespace is permitted in a parser directive. Hence, the following lines are all treated identically:\n\nUse the parser directive to declare the Dockerfile syntax version to use for the build. If unspecified, BuildKit uses a bundled version of the Dockerfile frontend. Declaring a syntax version lets you automatically use the latest Dockerfile version without having to upgrade BuildKit or Docker Engine, or even use a custom Dockerfile implementation.\n\nMost users will want to set this parser directive to , which causes BuildKit to pull the latest stable version of the Dockerfile syntax before the build.\n\nFor more information about how the parser directive works, see Custom Dockerfile syntax .\n\nThe directive sets the character used to escape characters in a Dockerfile. If not specified, the default escape character is .\n\nThe escape character is used both to escape characters in a line, and to escape a newline. This allows a Dockerfile instruction to span multiple lines. Note that regardless of whether the parser directive is included in a Dockerfile, escaping is not performed in a command, except at the end of a line.\n\nSetting the escape character to is especially useful on , where is the directory path separator. is consistent with Windows PowerShell .\n\nConsider the following example which would fail in a non-obvious way on Windows. The second at the end of the second line would be interpreted as an escape for the newline, instead of a target of the escape from the first . Similarly, the at the end of the third line would, assuming it was actually handled as an instruction, cause it be treated as a line continuation. The result of this Dockerfile is that second and third lines are considered a single instruction:\n\nOne solution to the above would be to use as the target of both the instruction, and . However, this syntax is, at best, confusing as it is not natural for paths on Windows, and at worst, error prone as not all commands on Windows support as the path separator.\n\nBy adding the parser directive, the following Dockerfile succeeds as expected with the use of natural platform semantics for file paths on Windows:\n\nThe directive is used to configure how build checks are evaluated. By default, all checks are run, and failures are treated as warnings.\n\nYou can disable specific checks using . To specify multiple checks to skip, separate them with a comma:\n\nTo disable all checks, use .\n\nBy default, builds with failing build checks exit with a zero status code despite warnings. To make the build fail on warnings, set .\n\nTo combine both the and options, use a semi-colon to separate them:\n\nTo see all available checks, see the build checks reference . Note that the checks available depend on the Dockerfile syntax version. To make sure you're getting the most up-to-date checks, use the directive to specify the Dockerfile syntax version to the latest stable version.\n\nEnvironment variables (declared with the statement) can also be used in certain instructions as variables to be interpreted by the Dockerfile. Escapes are also handled for including variable-like syntax into a statement literally.\n\nEnvironment variables are notated in the Dockerfile either with or . They are treated equivalently and the brace syntax is typically used to address issues with variable names with no whitespace, like .\n\nThe syntax also supports a few of the standard modifiers as specified below:\n• indicates that if is set then the result will be that value. If is not set then will be the result.\n• indicates that if is set then will be the result, otherwise the result is the empty string.\n\nThe following variable replacements are supported in a pre-release version of Dockerfile syntax, when using the syntax directive in your Dockerfile:\n• None removes the shortest match of from , seeking from the start of the string.\n• None removes the longest match of from , seeking from the start of the string.\n• None removes the shortest match of from , seeking backwards from the end of the string.\n• None removes the longest match of from , seeking backwards from the end of the string.\n• None replace the first occurrence of in with\n• None replaces all occurrences of in with\n\nIn all cases, can be any string, including additional environment variables.\n\nis a glob pattern where matches any single character and any number of characters (including zero). To match literal and , use a backslash escape: and .\n\nYou can escape whole variable names by adding a before the variable: or , for example, will translate to and literals respectively.\n\nExample (parsed representation is displayed after the ):\n\nEnvironment variables are supported by the following list of instructions in the Dockerfile:\n\nYou can also use environment variables with , , and instructions, but in those cases the variable substitution is handled by the command shell, not the builder. Note that instructions using the exec form don't invoke a command shell automatically. See Variable substitution.\n\nEnvironment variable substitution use the same value for each variable throughout the entire instruction. Changing the value of a variable only takes effect in subsequent instructions. Consider the following example:\n• The value of becomes\n• The value of becomes\n\nYou can use file to exclude files and directories from the build context. For more information, see .dockerignore file .\n\nThe , , and instructions all have two possible forms:\n\nThe exec form makes it possible to avoid shell string munging, and to invoke commands using a specific command shell, or any other executable. It uses a JSON array syntax, where each element in the array is a command, flag, or argument.\n\nThe shell form is more relaxed, and emphasizes ease of use, flexibility, and readability. The shell form automatically uses a command shell, whereas the exec form does not.\n\nThe exec form is parsed as a JSON array, which means that you must use double-quotes (\") around words, not single-quotes (').\n\nThe exec form is best used to specify an instruction, combined with for setting default arguments that can be overridden at runtime. For more information, see ENTRYPOINT.\n\nUsing the exec form doesn't automatically invoke a command shell. This means that normal shell processing, such as variable substitution, doesn't happen. For example, won't handle variable substitution for .\n\nIf you want shell processing then either use the shell form or execute a shell directly with the exec form, for example: . When using the exec form and executing a shell directly, as in the case for the shell form, it's the shell that's doing the environment variable substitution, not the builder.\n\nIn exec form, you must escape backslashes. This is particularly relevant on Windows where the backslash is the path separator. The following line would otherwise be treated as shell form due to not being valid JSON, and fail in an unexpected way:\n\nThe correct syntax for this example is:\n\nUnlike the exec form, instructions using the shell form always use a command shell. The shell form doesn't use the JSON array format, instead it's a regular string. The shell form string lets you escape newlines using the escape character (backslash by default) to continue a single instruction onto the next line. This makes it easier to use with longer commands, because it lets you split them up into multiple lines. For example, consider these two lines:\n\nThey're equivalent to the following line:\n\nYou can also use heredocs with the shell form to break up supported commands.\n\nFor more information about heredocs, see Here-documents.\n\nYou can change the default shell using the command. For example:\n\nFor more information, see SHELL.\n\nThe instruction initializes a new build stage and sets the base image for subsequent instructions. As such, a valid Dockerfile must start with a instruction. The image can be any valid image.\n• is the only instruction that may precede in the Dockerfile. See Understand how ARG and FROM interact.\n• can appear multiple times within a single Dockerfile to create multiple images or use one build stage as a dependency for another. Simply make a note of the last image ID output by the commit before each new instruction. Each instruction clears any state created by previous instructions.\n• Optionally a name can be given to a new build stage by adding to the instruction. The name can be used in subsequent , , and instructions to refer to the image built in this stage.\n• The or values are optional. If you omit either of them, the builder assumes a tag by default. The builder returns an error if it can't find the value.\n\nThe optional flag can be used to specify the platform of the image in case references a multi-platform image. For example, , , or . By default, the target platform of the build request is used. Global build arguments can be used in the value of this flag, for example automatic platform ARGs allow you to force a stage to native build platform ( ), and use it to cross-compile to the target platform inside the stage.\n\nUnderstand how ARG and FROM interact\n\ninstructions support variables that are declared by any instructions that occur before the first .\n\nAn declared before a is outside of a build stage, so it can't be used in any instruction after a . To use the default value of an declared before the first use an instruction without a value inside of a build stage:\n\nThe instruction will execute any commands to create a new layer on top of the current image. The added layer is used in the next step in the Dockerfile. has two forms:\n\nFor more information about the differences between these two forms, see shell or exec forms.\n\nThe shell form is most commonly used, and lets you break up longer instructions into multiple lines, either using newline escapes, or with heredocs:\n\nThe available for the instruction are:\n\nThe cache for instructions isn't invalidated automatically during the next build. The cache for an instruction like will be reused during the next build. The cache for instructions can be invalidated by using the flag, for example .\n\nSee the Dockerfile Best Practices guide for more information.\n\nThe cache for instructions can be invalidated by and instructions.\n\nallows you to create filesystem mounts that the build can access. This can be used to:\n• Create bind mount to the host filesystem or other build stages\n• Use a persistent package management cache to speed up your build\n\nThis mount type allows binding files or directories to the build container. A bind mount is read-only by default.\n\nThis mount type allows the build container to cache directories for compilers and package managers.\n\nContents of the cache directories persists between builder invocations without invalidating the instruction cache. Cache mounts should only be used for better performance. Your build should work with any contents of the cache directory as another build may overwrite the files or GC may clean it if more storage space is needed.\n\nApt needs exclusive access to its data, so the caches use the option , which will make sure multiple parallel builds using the same cache mount will wait for each other and not access the same cache files at the same time. You could also use if you prefer to have each build create another cache directory in this case.\n\nThis mount type allows mounting in the build container.\n\nThis mount type allows the build container to access secret values, such as tokens or private keys, without baking them into the image.\n\nBy default, the secret is mounted as a file. You can also mount the secret as an environment variable by setting the option.\n\nThe following example takes the secret and mounts it as an environment variable with the same name.\n\nAssuming that the environment variable is set in the build environment, you can build this with the following command:\n\nThis mount type allows the build container to access SSH keys via SSH agents, with support for passphrases.\n\nYou can also specify a path to file on the host directly instead of . However, pem files with passphrases are not supported.\n\nallows control over which networking environment the command is run in.\n\nEquivalent to not supplying a flag at all, the command is run in the default network for the build.\n\nThe command is run with no network access ( is still available, but is isolated to this process)\n\nwill only be able to install the packages provided in the tarfile, which can be controlled by an earlier build stage.\n\nThe command is run in the host's network environment (similar to , but on a per-instruction basis)\n\nThe default security mode is . With , the builder runs the command without sandbox in insecure mode, which allows to run flows requiring elevated privileges (e.g. containerd). This is equivalent to running .\n\nDefault sandbox mode can be activated via , but that is no-op.\n\nThe instruction sets the command to be executed when running a container from an image.\n\nYou can specify instructions using shell or exec forms:\n\nThere can only be one instruction in a Dockerfile. If you list more than one , only the last one takes effect.\n\nThe purpose of a is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an instruction as well.\n\nIf you would like your container to run the same executable every time, then you should consider using in combination with . See . If the user specifies arguments to then they will override the default specified in , but still use the default .\n\nIf is used to provide default arguments for the instruction, both the and instructions should be specified in the exec form.\n\nThe instruction adds metadata to an image. A is a key-value pair. To include spaces within a value, use quotes and backslashes as you would in command-line parsing. A few usage examples:\n\nAn image can have more than one label. You can specify multiple labels on a single line. Prior to Docker 1.10, this decreased the size of the final image, but this is no longer the case. You may still choose to specify multiple labels in a single instruction, in one of the following two ways:\n\nLabels included in base images (images in the line) are inherited by your image. If a label already exists but with a different value, the most-recently-applied value overrides any previously-set value.\n\nTo view an image's labels, use the command. You can use the option to show just the labels;\n\nThe instruction sets the Author field of the generated images. The instruction is a much more flexible version of this and you should use it instead, as it enables setting any metadata you require, and can be viewed easily, for example with . To set a label corresponding to the field you could use:\n\nThis will then be visible from with the other labels.\n\nThe instruction informs Docker that the container listens on the specified network ports at runtime. You can specify whether the port listens on TCP or UDP, and the default is TCP if you don't specify a protocol.\n\nThe instruction doesn't actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container, about which ports are intended to be published. To publish the port when running the container, use the flag on to publish and map one or more ports, or the flag to publish all exposed ports and map them to high-order ports.\n\nBy default, assumes TCP. You can also specify UDP:\n\nTo expose on both TCP and UDP, include two lines:\n\nIn this case, if you use with , the port will be exposed once for TCP and once for UDP. Remember that uses an ephemeral high-ordered host port on the host, so TCP and UDP doesn't use the same port.\n\nRegardless of the settings, you can override them at runtime by using the flag. For example\n\nTo set up port redirection on the host system, see using the -P flag . The command supports creating networks for communication among containers without the need to expose or publish specific ports, because the containers connected to the network can communicate with each other over any port. For detailed information, see the overview of this feature .\n\nThe instruction sets the environment variable to the value . This value will be in the environment for all subsequent instructions in the build stage and can be replaced inline in many as well. The value will be interpreted for other environment variables, so quote characters will be removed if they are not escaped. Like command line parsing, quotes and backslashes can be used to include spaces within values.\n\nThe instruction allows for multiple variables to be set at one time, and the example below will yield the same net results in the final image:\n\nThe environment variables set using will persist when a container is run from the resulting image. You can view the values using , and change them using .\n\nA stage inherits any environment variables that were set using by its parent stage or any ancestor. Refer to the multi-stage builds section in the manual for more information.\n\nEnvironment variable persistence can cause unexpected side effects. For example, setting changes the behavior of , and may confuse users of your image.\n\nIf an environment variable is only needed during build, and not in the final image, consider setting a value for a single command instead:\n\nOr using , which is not persisted in the final image:\n\nADD has two forms. The latter form is required for paths containing whitespace.\n\nThe available are:\n\nThe instruction copies new files or directories from and adds them to the filesystem of the image at the path . Files and directories can be copied from the build context, a remote URL, or a Git repository.\n\nThe and instructions are functionally similar, but serve slightly different purposes. Learn more about the differences between and .\n\nYou can specify multiple source files or directories with . The last argument must always be the destination. For example, to add two files, and , from the build context to in the build container:\n\nIf you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash ).\n\nTo add files from a remote location, you can specify a URL or the address of a Git repository as the source. For example:\n\nBuildKit detects the type of and processes it accordingly.\n• If is a local file or directory, the contents of the directory are copied to the specified destination. See Adding files from the build context.\n• If is a local tar archive, it is decompressed and extracted to the specified destination. See Adding local tar archives.\n• If is a URL, the contents of the URL are downloaded and placed at the specified destination. See Adding files from a URL.\n• If is a Git repository, the repository is cloned to the specified destination. See Adding files from a Git repository.\n\nAny relative or local path that doesn't begin with a , , or protocol prefix is considered a local file path. The local file path is relative to the build context. For example, if the build context is the current directory, adds the file at to the root of the filesystem in the build container.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as , automatically removes any parent directory navigation ( ). Trailing slashes in the source path are also disregarded, making equivalent to .\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build ( ), there is no build context. In this case, you can only use the instruction to copy remote files. You can also pass a tar archive through stdin: ( ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nFor local files, each may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with :\n\nIn the following example, is a single-character wildcard, matching e.g. and .\n\nWhen adding files or directories that contain special characters (such as and ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named , use the following;\n\nWhen using a local tar archive as the source for , and the archive is in a recognized compression format ( , or , or uncompressed), the archive is decompressed and extracted into the specified destination. Only local tar archives are extracted. If the tar archive is a remote URL, the archive is not extracted, but downloaded and placed at the destination.\n\nWhen a directory is extracted, it has the same behavior as . The result is the union of:\n• Whatever existed at the destination path, and\n• The contents of the source tree, with conflicts resolved in favor of the content being added, on a file-by-file basis.\n\nIn the case where source is a remote file URL, the destination will have permissions of 600. If the HTTP response contains a header, the timestamp from that header will be used to set the on the destination file. However, like any other file processed during an , isn't included in the determination of whether or not the file has changed and the cache should be updated.\n\nIf the destination ends with a trailing slash, then the filename is inferred from the URL path. For example, would create the file . The URL must have a nontrivial path so that an appropriate filename can be discovered ( doesn't work).\n\nIf the destination doesn't end with a trailing slash, the destination path becomes the filename of the file downloaded from the URL. For example, creates the file .\n\nIf your URL files are protected using authentication, you need to use , or use another tool from within the container as the instruction doesn't support authentication.\n\nTo use a Git repository as the source for , you can reference the repository's HTTP or SSH address as the source. The repository is cloned to the specified destination in the image.\n\nYou can use URL fragments to specify a specific branch, tag, commit, or subdirectory. For example, to add the directory of the tag of the repository:\n\nFor more information about Git URL fragments, see URL fragments .\n\nWhen adding from a Git repository, the permissions bits for files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nWhen using a Git repository as the source, the repository must be accessible from the build context. To add a repository via SSH, whether public or private, you must pass an SSH key for authentication. For example, given the following Dockerfile:\n\nTo build this Dockerfile, pass the flag to the to mount the SSH agent socket to the build. For example:\n\nFor more information about building with secrets, see Build secrets .\n\nIf the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage.\n\nTrailing slashes are significant. For example, creates a file at , whereas creates .\n\nIf the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container.\n\nIf destination doesn't exist, it's created, along with all missing directories in its path.\n\nIf the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file.\n\nWhen is the HTTP or SSH address of a remote Git repository, BuildKit adds the contents of the Git repository to the image excluding the directory by default.\n\nThe flag lets you preserve the directory.\n\nThe flag lets you verify the checksum of a remote resource. The checksum is formatted as . SHA-256 is the only supported hash algorithm.\n\nCOPY has two forms. The latter form is required for paths containing whitespace.\n\nThe available are:\n\nThe instruction copies new files or directories from and adds them to the filesystem of the image at the path . Files and directories can be copied from the build context, build stage, named context, or an image.\n\nThe and instructions are functionally similar, but serve slightly different purposes. Learn more about the differences between and .\n\nYou can specify multiple source files or directories with . The last argument must always be the destination. For example, to copy two files, and , from the build context to in the build container:\n\nIf you specify multiple source files, either directly or using a wildcard, then the destination must be a directory (must end with a slash ).\n\naccepts a flag that lets you specify the source location to be a build stage, context, or image. The following example copies files from a stage named :\n\nFor more information about copying from named sources, see the flag.\n\nWhen copying source files from the build context, paths are interpreted as relative to the root of the context.\n\nSpecifying a source path with a leading slash or one that navigates outside the build context, such as , automatically removes any parent directory navigation ( ). Trailing slashes in the source path are also disregarded, making equivalent to .\n\nIf the source is a directory, the contents of the directory are copied, including filesystem metadata. The directory itself isn't copied, only its contents. If it contains subdirectories, these are also copied, and merged with any existing directories at the destination. Any conflicts are resolved in favor of the content being added, on a file-by-file basis, except if you're trying to copy a directory onto an existing file, in which case an error is raised.\n\nIf the source is a file, the file and its metadata are copied to the destination. File permissions are preserved. If the source is a file and a directory with the same name exists at the destination, an error is raised.\n\nIf you pass a Dockerfile through stdin to the build ( ), there is no build context. In this case, you can only use the instruction to copy files from other stages, named contexts, or images, using the flag. You can also pass a tar archive through stdin: ( ), the Dockerfile at the root of the archive and the rest of the archive will be used as the context of the build.\n\nWhen using a Git repository as the build context, the permissions bits for copied files are 644. If a file in the repository has the executable bit set, it will have permissions set to 755. Directories have permissions set to 755.\n\nFor local files, each may contain wildcards and matching will be done using Go's filepath.Match rules.\n\nFor example, to add all files and directories in the root of the build context ending with :\n\nIn the following example, is a single-character wildcard, matching e.g. and .\n\nWhen adding files or directories that contain special characters (such as and ), you need to escape those paths following the Golang rules to prevent them from being treated as a matching pattern. For example, to add a file named , use the following;\n\nIf the destination path begins with a forward slash, it's interpreted as an absolute path, and the source files are copied into the specified destination relative to the root of the current build stage.\n\nTrailing slashes are significant. For example, creates a file at , whereas creates .\n\nIf the destination path doesn't begin with a leading slash, it's interpreted as relative to the working directory of the build container.\n\nIf destination doesn't exist, it's created, along with all missing directories in its path.\n\nIf the source is a file, and the destination doesn't end with a trailing slash, the source file will be written to the destination path as a file.\n\nBy default, the instruction copies files from the build context. The flag lets you copy files from an image, a build stage, or a named context instead.\n\nTo copy from a build stage in a multi-stage build , specify the name of the stage you want to copy from. You specify stage names using the keyword with the instruction.\n\nYou can also copy files directly from named contexts (specified with ) or images. The following example copies an file from the official Nginx image.\n\nThe source path of is always resolved from filesystem root of the image or stage that you specify.\n\nThe and features are only supported on Dockerfiles used to build Linux containers, and doesn't work on Windows containers. Since user and group ownership concepts do not translate between Linux and Windows, the use of and for translating user and group names to IDs restricts this feature to only be viable for Linux OS-based containers.\n\nAll files and directories copied from the build context are created with a UID and GID of unless the optional flag specifies a given username, groupname, or UID/GID combination to request specific ownership of the copied content. The format of the flag allows for either username and groupname strings or direct integer UID and GID in any combination. Providing a username without groupname or a UID without GID will use the same numeric UID as the GID. If a username or groupname is provided, the container's root filesystem and files will be used to perform the translation from name to integer UID or GID respectively. The following examples show valid definitions for the flag:\n\nIf the container root filesystem doesn't contain either or files and either user or group names are used in the flag, the build will fail on the operation. Using numeric IDs requires no lookup and does not depend on container root filesystem content.\n\nWith the Dockerfile syntax version 1.10.0 and later, the flag supports variable interpolation, which lets you define the permission bits using build arguments:\n\nEnabling this flag in or commands allows you to copy files with enhanced semantics where your files remain independent on their own layer and don't get invalidated when commands on previous layers are changed.\n\nWhen is used your source files are copied into an empty destination directory. That directory is turned into a layer that is linked on top of your previous state.\n\nIs equivalent of doing two builds:\n\nand merging all the layers of both images together.\n\nUse to reuse already built layers in subsequent builds with even if the previous layers have changed. This is especially important for multi-stage builds where a statement would previously get invalidated if any previous commands in the same stage changed, causing the need to rebuild the intermediate stages again. With the layer the previous build generated is reused and merged on top of the new layers. This also means you can easily rebase your images when the base images receive updates, without having to execute the whole build again. In backends that support it, BuildKit can do this rebase action without the need to push or pull any layers between the client and the registry. BuildKit will detect this case and only create new image manifest that contains the new layers and old layers in correct order.\n\nThe same behavior where BuildKit can avoid pulling down the base image can also happen when using and no other commands that would require access to the files in the base image. In that case BuildKit will only build the layers for the commands and push them to the registry directly on top of the layers of the base image.\n\nWhen using the commands are not allowed to read any files from the previous state. This means that if in previous state the destination directory was a path that contained a symlink, can not follow it. In the final image the destination path created with will always be a path containing only directories.\n\nIf you don't rely on the behavior of following symlinks in the destination path, using is always recommended. The performance of is equivalent or better than the default behavior and, it creates much better conditions for cache reuse.\n\nThe flag preserves parent directories for entries. This flag defaults to .\n\nThis behavior is similar to the Linux utility's or flag.\n\nAs with Rsync, it is possible to limit which parent directories are preserved by inserting a dot and a slash ( ) into the source path. If such point exists, only parent directories after it will be preserved. This may be especially useful copies between stages with where the source paths need to be absolute.\n\nNote that, without the flag specified, any filename collision will fail the Linux operation with an explicit error message ( cp: will not overwrite just-created './x/a.txt' with './y/a.txt' ), where the Buildkit will silently overwrite the target file at the destination.\n\nWhile it is possible to preserve the directory structure for instructions consisting of only one entry, usually it is more beneficial to keep the layer count in the resulting image as low as possible. Therefore, with the flag, the Buildkit is capable of packing multiple instructions together, keeping the directory structure intact.\n\nThe flag lets you specify a path expression for files to be excluded.\n\nThe path expression follows the same format as , supporting wildcards and matching using Go's filepath.Match rules. For example, to add all files starting with \"hom\", excluding files with a extension:\n\nYou can specify the option multiple times for a instruction. Multiple are files matching its patterns not to be copied, even if the files paths match the pattern specified in . To add all files starting with \"hom\", excluding files with either or extensions:\n\nAn allows you to configure a container that will run as an executable.\n\nhas two possible forms:\n• None The exec form, which is the preferred form:\n\nFor more information about the different forms, see Shell and exec form.\n\nThe following command starts a container from the with its default content, listening on port 80:\n\nCommand line arguments to will be appended after all elements in an exec form , and will override all elements specified using .\n\nThis allows arguments to be passed to the entry point, i.e., will pass the argument to the entry point. You can override the instruction using the flag.\n\nThe shell form of prevents any command line arguments from being used. It also starts your as a subcommand of , which does not pass signals. This means that the executable will not be the container's , and will not receive Unix signals. In this case, your executable doesn't receive a from .\n\nOnly the last instruction in the Dockerfile will have an effect.\n\nYou can use the exec form of to set fairly stable default commands and arguments and then use either form of to set additional defaults that are more likely to be changed.\n\nWhen you run the container, you can see that is the only process:\n\nTo examine the result further, you can use :\n\nAnd you can gracefully request to shut down using .\n\nThe following Dockerfile shows using the to run Apache in the foreground (i.e., as ):\n\nIf you need to write a starter script for a single executable, you can ensure that the final executable receives the Unix signals by using and commands:\n\nLastly, if you need to do some extra cleanup (or communicate with other containers) on shutdown, or are co-ordinating more than one executable, you may need to ensure that the script receives the Unix signals, passes them on, and then does some more work:\n\nIf you run this image with , you can then examine the container's processes with , or , and then ask the script to stop Apache:\n\nYou can specify a plain string for the and it will execute in . This form will use shell processing to substitute shell environment variables, and will ignore any or command line arguments. To ensure that will signal any long running executable correctly, you need to remember to start it with :\n\nWhen you run this image, you'll see the single process:\n\nIf you forget to add to the beginning of your :\n\nYou can then run it (giving it a name for the next step):\n\nYou can see from the output of that the specified is not .\n\nIf you then run , the container will not exit cleanly - the command will be forced to send a after the timeout:\n\nBoth and instructions define what command gets executed when running a container. There are few rules that describe their co-operation.\n• None Dockerfile should specify at least one of or commands.\n• None should be defined when using the container as an executable.\n• None should be used as a way of defining default arguments for an command or for executing an ad-hoc command in a container.\n• None will be overridden when running the container with alternative arguments.\n\nThe table below shows what command is executed for different / combinations:\n\nThe instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. The value can be a JSON array, , or a plain string with multiple arguments, such as or . For more information/examples and mounting instructions via the Docker client, refer to Share Directories via Volumes documentation.\n\nThe command initializes the newly created volume with any data that exists at the specified location within the base image. For example, consider the following Dockerfile snippet:\n\nThis Dockerfile results in an image that causes to create a new mount point at and copy the file into the newly created volume.\n\nKeep the following things in mind about volumes in the Dockerfile.\n• None Volumes on Windows-based containers: When using Windows-based containers, the destination of a volume inside the container must be one of:\n• None Changing the volume from within the Dockerfile: If any build steps change the data within the volume after it has been declared, those changes will be discarded when using the legacy builder. When using Buildkit, the changes will instead be kept.\n• None JSON formatting: The list is parsed as a JSON array. You must enclose words with double quotes ( ) rather than single quotes ( ).\n• None The host directory is declared at container run-time: The host directory (the mountpoint) is, by its nature, host-dependent. This is to preserve image portability, since a given host directory can't be guaranteed to be available on all hosts. For this reason, you can't mount a host directory from within the Dockerfile. The instruction does not support specifying a parameter. You must specify the mountpoint when you create or run the container.\n\nThe instruction sets the user name (or UID) and optionally the user group (or GID) to use as the default user and group for the remainder of the current stage. The specified user is used for instructions and at runtime, runs the relevant and commands.\n\nThe instruction sets the working directory for any , , , and instructions that follow it in the Dockerfile. If the doesn't exist, it will be created even if it's not used in any subsequent Dockerfile instruction.\n\nThe instruction can be used multiple times in a Dockerfile. If a relative path is provided, it will be relative to the path of the previous instruction. For example:\n\nThe output of the final command in this Dockerfile would be .\n\nThe instruction can resolve environment variables previously set using . You can only use environment variables explicitly set in the Dockerfile. For example:\n\nThe output of the final command in this Dockerfile would be\n\nIf not specified, the default working directory is . In practice, if you aren't building a Dockerfile from scratch ( ), the may likely be set by the base image you're using.\n\nTherefore, to avoid unintended operations in unknown directories, it's best practice to set your explicitly.\n\nThe instruction defines a variable that users can pass at build-time to the builder with the command using the flag.\n\nA Dockerfile may include one or more instructions. For example, the following is a valid Dockerfile:\n\nAn instruction can optionally include a default value:\n\nIf an instruction has a default value and if there is no value passed at build-time, the builder uses the default.\n\nAn variable comes into effect from the line on which it is declared in the Dockerfile. For example, consider this Dockerfile:\n• The instruction on line 2 evaluates to the fallback, because the variable is not yet declared.\n• The variable is declared on line 3, and available for reference in Dockerfile instruction from that point onwards.\n• The instruction on line 4 evaluates to , since at that point the argument has a value of which was passed on the command line. Prior to its definition by an instruction, any use of a variable results in an empty string.\n\nAn variable declared within a build stage is automatically inherited by other stages based on that stage. Unrelated build stages do not have access to the variable. To use an argument in multiple distinct stages, each stage must include the instruction, or they must both be based on a shared base stage in the same Dockerfile where the variable is declared.\n\nFor more information, refer to variable scoping .\n\nYou can use an or an instruction to specify variables that are available to the instruction. Environment variables defined using the instruction always override an instruction of the same name. Consider this Dockerfile with an and instruction.\n\nThen, assume this image is built with this command:\n\nIn this case, the instruction uses instead of the setting passed by the user: This behavior is similar to a shell script where a locally scoped variable overrides the variables passed as arguments or inherited from environment, from its point of definition.\n\nUsing the example above but a different specification you can create more useful interactions between and instructions:\n\nUnlike an instruction, values are always persisted in the built image. Consider a docker build without the flag:\n\nUsing this Dockerfile example, is still persisted in the image but its value would be as it is the default set in line 3 by the instruction.\n\nThe variable expansion technique in this example allows you to pass arguments from the command line and persist them in the final image by leveraging the instruction. Variable expansion is only supported for a limited set of Dockerfile instructions.\n\nDocker has a set of predefined variables that you can use without a corresponding instruction in the Dockerfile.\n\nTo use these, pass them on the command line using the flag, for example:\n\nBy default, these pre-defined variables are excluded from the output of . Excluding them reduces the risk of accidentally leaking sensitive authentication information in an variable.\n\nFor example, consider building the following Dockerfile using\n\nIn this case, the value of the variable is not available in the and is not cached. If you were to change location, and your proxy server changed to , a subsequent build does not result in a cache miss.\n\nIf you need to override this behaviour then you may do so by adding an statement in the Dockerfile as follows:\n\nWhen building this Dockerfile, the is preserved in the , and changing its value invalidates the build cache.\n\nThis feature is only available when using the BuildKit backend.\n\nBuildKit supports a predefined set of variables with information on the platform of the node performing the build (build platform) and on the platform of the resulting image (target platform). The target platform can be specified with the flag on .\n\nThe following variables are set automatically:\n• - platform of the node performing the build.\n\nThese arguments are defined in the global scope so are not automatically available inside build stages or for your commands. To expose one of these arguments inside the build stage redefine it without value.\n\nWhen using a Git context, dir is not kept on checkouts. It can be useful to keep it around if you want to retrieve git information during your build:\n\nvariables are not persisted into the built image as variables are. However, variables do impact the build cache in similar ways. If a Dockerfile defines an variable whose value is different from a previous build, then a \"cache miss\" occurs upon its first usage, not its definition. In particular, all instructions following an instruction use the variable implicitly (as an environment variable), thus can cause a cache miss. All predefined variables are exempt from caching unless there is a matching statement in the Dockerfile.\n\nFor example, consider these two Dockerfile:\n\nIf you specify on the command line, in both cases, the specification on line 2 doesn't cause a cache miss; line 3 does cause a cache miss. causes the line to be identified as the same as running , so if the changes, you get a cache miss.\n\nConsider another example under the same command line:\n\nIn this example, the cache miss occurs on line 3. The miss happens because the variable's value in the references the variable and that variable is changed through the command line. In this example, the command causes the image to include the value.\n\nIf an instruction overrides an instruction of the same name, like this Dockerfile:\n\nLine 3 doesn't cause a cache miss because the value of is a constant ( ). As a result, the environment variables and values used on the (line 4) doesn't change between builds.\n\nThe instruction adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the instruction in the downstream Dockerfile.\n\nThis is useful if you are building an image which will be used as a base to build other images, for example an application build environment or a daemon which may be customized with user-specific configuration.\n\nFor example, if your image is a reusable Python application builder, it will require application source code to be added in a particular directory, and it might require a build script to be called after that. You can't just call and now, because you don't yet have access to the application source code, and it will be different for each application build. You could simply provide application developers with a boilerplate Dockerfile to copy-paste into their application, but that's inefficient, error-prone and difficult to update because it mixes with application-specific code.\n\nThe solution is to use to register advance instructions to run later, during the next build stage.\n• When it encounters an instruction, the builder adds a trigger to the metadata of the image being built. The instruction doesn't otherwise affect the current build.\n• At the end of the build, a list of all triggers is stored in the image manifest, under the key . They can be inspected with the command.\n• Later the image may be used as a base for a new build, using the instruction. As part of processing the instruction, the downstream builder looks for triggers, and executes them in the same order they were registered. If any of the triggers fail, the instruction is aborted which in turn causes the build to fail. If all triggers succeed, the instruction completes and the build continues as usual.\n• Triggers are cleared from the final image after being executed. In other words they aren't inherited by \"grand-children\" builds.\n\nFor example you might add something like this:\n\nCopy or mount from stage, image, or context\n\nAs of Dockerfile syntax 1.11, you can use with instructions that copy or mount files from other stages, images, or build contexts. For example:\n\nIf the source of is a build stage, the stage must be defined in the Dockerfile where gets triggered. If it's a named context, that context must be passed to the downstream build.\n• The instruction may not trigger or instructions.\n\nThe instruction sets the system call signal that will be sent to the container to exit. This signal can be a signal name in the format , for instance , or an unsigned number that matches a position in the kernel's syscall table, for instance . The default is if not defined.\n\nThe image's default stopsignal can be overridden per container, using the flag on and .\n\nThe instruction has two forms:\n\nThe instruction tells Docker how to test a container to check that it's still working. This can detect cases such as a web server stuck in an infinite loop and unable to handle new connections, even though the server process is still running.\n\nWhen a container has a healthcheck specified, it has a health status in addition to its normal status. This status is initially . Whenever a health check passes, it becomes (whatever state it was previously in). After a certain number of consecutive failures, it becomes .\n\nThe options that can appear before are:\n\nThe health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes.\n\nIf a single run of the check takes longer than timeout seconds then the check is considered to have failed.\n\nIt takes retries consecutive failures of the health check for the container to be considered .\n\nstart period provides initialization time for containers that need time to bootstrap. Probe failure during that period will not be counted towards the maximum number of retries. However, if a health check succeeds during the start period, the container is considered started and all consecutive failures will be counted towards the maximum number of retries.\n\nstart interval is the time between health checks during the start period. This option requires Docker Engine version 25.0 or later.\n\nThere can only be one instruction in a Dockerfile. If you list more than one then only the last will take effect.\n\nThe command after the keyword can be either a shell command (e.g. ) or an exec array (as with other Dockerfile commands; see e.g. for details).\n\nThe command's exit status indicates the health status of the container. The possible values are:\n• 0: success - the container is healthy and ready for use\n\nFor example, to check every five minutes or so that a web-server is able to serve the site's main page within three seconds:\n\nTo help debug failing probes, any output text (UTF-8 encoded) that the command writes on stdout or stderr will be stored in the health status and can be queried with . Such output should be kept short (only the first 4096 bytes are stored currently).\n\nWhen the health status of a container changes, a event is generated with the new status.\n\nThe instruction allows the default shell used for the shell form of commands to be overridden. The default shell on Linux is , and on Windows is . The instruction must be written in JSON form in a Dockerfile.\n\nThe instruction is particularly useful on Windows where there are two commonly used and quite different native shells: and , as well as alternate shells available including .\n\nThe instruction can appear multiple times. Each instruction overrides all previous instructions, and affects all subsequent instructions. For example:\n\nThe following instructions can be affected by the instruction when the shell form of them is used in a Dockerfile: , and .\n\nThe following example is a common pattern found on Windows which can be streamlined by using the instruction:\n\nThe command invoked by the builder will be:\n\nThis is inefficient for two reasons. First, there is an unnecessary command processor (aka shell) being invoked. Second, each instruction in the shell form requires an extra prefixing the command.\n\nTo make this more efficient, one of two mechanisms can be employed. One is to use the JSON form of the command such as:\n\nWhile the JSON form is unambiguous and does not use the unnecessary , it does require more verbosity through double-quoting and escaping. The alternate mechanism is to use the instruction and the shell form, making a more natural syntax for Windows users, especially when combined with the parser directive:\n\nThe instruction could also be used to modify the way in which a shell operates. For example, using on Windows, delayed environment variable expansion semantics could be modified.\n\nThe instruction can also be used on Linux should an alternate shell be required such as , , and others.\n\nHere-documents allow redirection of subsequent Dockerfile lines to the input of or commands. If such command contains a here-document the Dockerfile considers the next lines until the line only containing a here-doc delimiter as part of the same command.\n\nIf the command only contains a here-document, its contents is evaluated with the default shell.\n\nAlternatively, shebang header can be used to define an interpreter.\n\nMore complex examples may use multiple here-documents.\n\nWith instructions, you can replace the source parameter with a here-doc indicator to write the contents of the here-document directly to a file. The following example creates a file containing using a instruction.\n\nRegular here-doc variable expansion and tab stripping rules apply. The following example shows a small Dockerfile that creates a script file using a instruction with a here-document.\n\nIn this case, file script prints \"hello bar\", because the variable is expanded when the instruction gets executed.\n\nIf instead you were to quote any part of the here-document word , the variable would not be expanded at build-time.\n\nNote that is excessive here, and can be removed. The variable gets interpreted at runtime, when the script is invoked:\n\nFor examples of Dockerfiles, refer to:"
    },
    {
        "link": "https://geeksforgeeks.org/docker-copy-instruction",
        "document": "In Docker, there are two ways to copy a file, namely, ADD and COPY. Though there is a slight difference between them in regard to the scope of the functions, they more or less perform the same task. In this article, we will primarily focus on the COPY instruction of Docker. If you want to copy files and directories inside a Docker Container from your Local machine, you can use the COPY instruction inside your Dockerfile. The general form of a COPY instruction is:\n\nIn this article, we will discuss how to use the COPY Instruction to copy files and directories inside a Docker Container. To do so follow the below steps:\n\nIn this example, we will create a directory and a file which we will copy using the COPY command. Create a folder and inside it create a file called “dockerfile” which we will edit in the next step. Create another folder in the same directory where you have created the Dockerfile and a file inside it. We will copy this folder to our Docker Container. The final directory structure will be –\n\nAfter you have created the directory structure, edit the Dockerfile that we created in the previous step.\n\nIn the above Dockerfile, we have tried to pull the Ubuntu base image OS with the latest tag and run an update inside the Container. We have then included the COPY instruction to copy the directory created previously.\n\nAfter creating the Dockerfile, we can now build the Docker Image using the Docker Build command.\n\nAfter you have built the Docker Image, you can verify it by using the Docker Images command to list all the images in your system.\n\nAfter you have built the Docker Image with the COPY Instruction, you can now run the Docker container using the Docker RUN command.\n\nStep 6: Verify the Copying of the Directory\n\nYou can now verify whether the directory has been copied or not by listing the directories inside the Container."
    },
    {
        "link": "https://docs.docker.com/get-started/docker-concepts/building-images/writing-a-dockerfile",
        "document": "A Dockerfile is a text-based document that's used to create a container image. It provides instructions to the image builder on the commands to run, files to copy, startup command, and more.\n\nAs an example, the following Dockerfile would produce a ready-to-run Python application:\n\nSome of the most common instructions in a include:\n• - this specifies the base image that the build will extend.\n• - this instruction specifies the \"working directory\" or the path in the image where files will be copied and commands will be executed.\n• - this instruction tells the builder to copy files from the host and put them into the container image.\n• - this instruction tells the builder to run the specified command.\n• - this instruction sets an environment variable that a running container will use.\n• - this instruction sets configuration on the image that indicates a port the image would like to expose.\n• - this instruction sets the default user for all subsequent instructions.\n• - this instruction sets the default command a container using this image will run.\n\nTo read through all of the instructions or go into greater detail, check out the Dockerfile reference .\n\nTry it out\n\nJust as you saw with the previous example, a Dockerfile typically follows these steps:\n\nIn this quick hands-on guide, you'll write a Dockerfile that builds a simple Node.js application. If you're not familiar with JavaScript-based applications, don't worry. It isn't necessary for following along with this guide.\n\nDownload this ZIP file and extract the contents into a directory on your machine.\n\nNow that you have the project, you’re ready to create the .\n• None Create a file named in the same folder as the file . It's important to note that the has no file extension. Some editors will automatically add an extension to the file (or complain it doesn't have one).\n• None In the , define your base image by adding the following line:\n• None Now, define the working directory by using the instruction. This will specify where future commands will run and the directory files will be copied inside the container image.\n• None Copy all of the files from your project on your machine into the container image by using the instruction:\n• None Install the app's dependencies by using the CLI and package manager. To do so, run a command using the instruction:\n• None Finally, specify the default command to run by using the instruction: And with that, you should have the following Dockerfile:\n\nIt's important to note that this Dockerfile is not following all of the best practices yet (by design). It will build the app, but the builds won't be as fast, or the images as secure, as they could be. Keep reading to learn more about how to make the image maximize the build cache, run as a non-root user, and multi-stage builds.\n\nTo learn more about writing a Dockerfile, visit the following resources:\n\nNow that you have created a Dockerfile and learned the basics, it's time to learn about building, tagging, and pushing the images."
    },
    {
        "link": "https://refine.dev/blog/docker-copy",
        "document": "We will discuss how to copy files in Docker using the docker copy command. The command is a powerful command through which you can move files between your host file system and your Docker containers. It works with both files and directories.\n• How to copy files in Docker\n• When should you use Docker copy\n\nCopying important files and directories is an essential part of container building. There are three ways to copy files in Docker:\n\nToday we will discuss the command in detail. We will also touch upon the command in Dockerfile. Let’s start with the command first. is a powerful command through which you can move files between your host file system and your Docker containers. It works with both files and directories. The command is the quickest way to copy files to and from docker container. The command is very similar to the unix “cp” command. The basic syntax of the command is as follows:\n\nIf you are running from the windows prompt, then you need to mention the container name/ID as part of the source. Let’s try one example. The command will display the running containers.\n\nNote the ID of the running container you want the files to copied to. In above example the container ID is “613a1a58b70d”. If you have opened the terminal/CLI for a specific container, then no need to mention the container ID.\n\nNow let’s find the files in the container through basic command.\n\nThe below command will copy the file from container to host file system in the folder . Note the container ID is mentioned in the source.\n\nInstead of ID, you can mention container name as well. See the example below:\n\nJust reverse the command and you can copy the file from host file system to the container. The below example will copy a text file named “tocopy.txt” from host file system to the container.\n\nTo verify the successfully copy, run the command in the container terminal. You can see the file copied to container.\n\nYou can use the same command to copy not just single file but whole folder as well.\n\nNote that the docker command assumes that container path is relative to the container's / (root) directory. Also, the command will work regardless of container is running or stopped.\n\nThere are only two optional arguments for this command: -a: Archive mode. It preserves all uid/gid information of the files to be copied\n\nYou can also use the command to copy files between two containers. Just use the container name or ID in the source and destination paths.\n\n## Copy through Dockerfile Let’s discuss how to copy files through dockerfile copy command. The basic purpose of Dockerfile is to build Docker image, which is then converted into Docker containers. Dockerfiles can contain different commands, one of which is . The command allows us to copy a file or folder from the host system into the docker image. The copied files become a part of every container that is created from that docker image.\n\nThe syntax is same as the cp command discussed above: The below example will copy index.html to the container to replace the default index.html file. If you build the docker image and run the container, you will see the file will be copied. Note that the command cannot be used copy files between your host and a running container. The sole purpose of command is to add files in the docker image during the construction process.\n\nThe command is used to copy files to/from your docker container to the host file system. This command works even if the container is not running. As Docker uses layers for storing images, when you run the container, Docker creates another layer on top of the existing layer. The new layer contains all the changes done inside the container. Note that it is not a data volume like a file system or a directory that we can mount directly to a docker container. The data inside the container is managed by the storage driver. Data is present inside the block level of the parent image of the container, and this is where the command is executed. Container is actually a snapshot of the image so it does not have the block, but it has a pointer to the block on the nearest parent image where it is actually residing. It reads the respective block from there and copies it upon execution of command.\n\nThe command has been there even before the command. Although the command essentially performs the same function i.e. copying files and folders to a docker container. However, there are a few additional features in the command.\n• None You can mention a URL as source. The command will download the files from the URL and will copy them to the destination. The above command will download all files from the source folder and will copy them to the destination folder of the container.\n• None You can mention a compressed file in the source, and it will decompress the contents and then copy them to the destination. The above example will extract the contents of a local zip file to the destination folder. Note that you cannot mention a URL of a compressed file as a source; the compressed file must be from the local file system. This technique will stop creating an additional image layer and will save space too.\n\nHowever, in spite of all the extra features provided by command, Docker discourages its use because of safety reasons. Docker suggests using only the command if copying a local file. If you want to download and copy files from the internet, then it suggests using the command with a command. The only recommended use of is to extract the local tar file into the image, as shown in below example:\n\nDespite of all he usefulness of Docker command, it has certain limitations, as below:\n• You cannot use it to copy files between two containers. You will need to multi-stage it by copying files from container 1 to a host file system and then copying from the host file system to container 2\n• You cannot use it to download files from a URL and then copy them. Files must be present on the local file system\n• You cannot copy certain system files such as resources under , , , , etc.\n\nFollowing are some of the common use cases for the docker copy command.\n• None You can copy any important file from inside a docker container to the host file system, which will allow you to create a custom docker image the way you want it. E.g. you can run the official Nginx docker image and then use the command to copy the Nginx configuration file to the host file system\n• None is very useful when debugging containers or when working in a non-production environment. If you have to manually inject a temporary configuration file or extract a buried log, it can be extremely helpful. Using is more convenient and faster than rebuilding the entire image every time you update the code."
    },
    {
        "link": "https://stackoverflow.com/questions/24958140/what-is-the-difference-between-the-copy-and-add-commands-in-a-dockerfile",
        "document": "When creating a Dockerfile, there are two commands that you can use to copy files/directories into it – and . Although there are slight differences in the scope of their function, they essentially perform the same task.\n\nSo, why do we have two commands, and how do we know when to use one or the other?\n\nLet’s start by noting that the command is older than . Since the launch of the Docker platform, the instruction has been part of its list of commands.\n\nThe command copies files/directories to a file system of the specified container.\n\nThe basic syntax for the command is:\n\nIt includes the source you want to copy ( ) followed by the destination where you want to store it ( ). If the source is a directory, copies everything inside of it (including file system metadata).\n\nFor instance, if the file is locally available and you want to add it to the directory of an image, you type:\n\ncan also copy files from a URL. It can download an external file and copy it to the wanted destination. For example:\n\nAn additional feature is that it copies compressed files, automatically extracting the content to the given destination. This feature only applies to locally stored compressed files/directories.\n\nBear in mind that you cannot download and extract a compressed file/directory from a URL. The command does not unpack external packages when copying them to the local filesystem.\n\nDue to some functionality issues, Docker had to introduce an additional command for duplicating content – .\n\nUnlike its closely related command, only has only one assigned function. Its role is to duplicate files/directories in a specified location in their existing format. This means that it doesn’t deal with extracting a compressed file, but rather copies it as-is.\n\nThe instruction can be used only for locally stored files. Therefore, you cannot use it with URLs to copy external files to your container.\n\nTo use the instruction, follow the basic command format:\n\nType in the source and where you want the command to extract the content as follows:\n\nWhich command to use? (Best Practice)\n\nConsidering the circumstances in which the command was introduced, it is evident that keeping was a matter of necessity. Docker released an official document outlining best practices for writing Dockerfiles, which explicitly advises against using the command.\n\nDocker’s official documentation notes that should always be the go-to instruction as it is more transparent than .\n\nIf you need to copy from the local build context into a container, stick to using .\n\nThe Docker team also strongly discourages using to download and copy a package from a URL. Instead, it’s safer and more efficient to use wget or curl within a command. By doing so, you avoid creating an additional image layer and save space."
    },
    {
        "link": "https://docker.com/blog/docker-best-practices-understanding-the-differences-between-add-and-copy-instructions-in-dockerfiles",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/57286183/best-practices-when-copying-source-files-with-docker",
        "document": "Lastly, I have been trying to improve my Docker skills, but there is something I'm still stuck when I have to deal with it. Let's say I have an application divided into modules, each of them being its own Docker service.\n\nAll modules use the config stored under config/config.ini, but this config needs some preprocessing so what they use is the functions of config/config.py.\n\nI have implemented that as:\n• Each Dockerfile of each module COPY the contents of the config/ folder into their own ( )\n• Each module imports the config functions with:\n\nThis method does work, but it breaks the IDE as the config folder is not on the pythonpath so I lost some important functionality, and what is worse I don't think this is the cleanest way of achieving what I want.\n\nFurthermore, this also happens with source files. Normally in my python development, I would have just created a package called \"APP\" (see the folder structure below) and then I could have imported any source file in any folder of the package, but because each subfolder is a different Docker service I cannot make this project a python package. For example, the tests are also dockerized so each test also copies the needed source files from other folders:\n\nthe file needs to test all the source files in so it copies the files with the Dockerfile."
    },
    {
        "link": "https://docs.docker.com/build/building/best-practices",
        "document": "Multi-stage builds let you reduce the size of your final image, by creating a cleaner separation between the building of your image and the final output. Split your Dockerfile instructions into distinct stages to make sure that the resulting output only contains the files that are needed to run the application.\n\nUsing multiple stages can also let you build more efficiently by executing build steps in parallel.\n\nSee Multi-stage builds for more information.\n\nIf you have multiple images with a lot in common, consider creating a reusable stage that includes the shared components, and basing your unique stages on that. Docker only needs to build the common stage once. This means that your derivative images use memory on the Docker host more efficiently and load more quickly.\n\nIt's also easier to maintain a common base stage (\"Don't repeat yourself\"), than it is to have multiple different stages doing similar things.\n\nThe first step towards achieving a secure image is to choose the right base image. When choosing an image, ensure it's built from a trusted source and keep it small.\n• None Docker Official Images are some of the most secure and dependable images on Docker Hub. Typically, Docker Official images have few or no packages containing CVEs, and are thoroughly reviewed by Docker and project maintainers.\n• None Verified Publisher images are high-quality images published and maintained by the organizations partnering with Docker, with Docker verifying the authenticity of the content in their repositories.\n• None Docker-Sponsored Open Source are published and maintained by open source projects sponsored by Docker through an open source program.\n\nWhen you pick your base image, look out for the badges indicating that the image is part of these programs.\n\nWhen building your own image from a Dockerfile, ensure you choose a minimal base image that matches your requirements. A smaller base image not only offers portability and fast downloads, but also shrinks the size of your image and minimizes the number of vulnerabilities introduced through the dependencies.\n\nYou should also consider using two types of base image: one for building and unit testing, and another (typically slimmer) image for production. In the later stages of development, your image may not require build tools such as compilers, build systems, and debugging tools. A small image with minimal dependencies can considerably lower the attack surface.\n\nDocker images are immutable. Building an image is taking a snapshot of that image at that moment. That includes any base images, libraries, or other software you use in your build. To keep your images up-to-date and secure, make sure to rebuild your image often, with updated dependencies.\n\nTo ensure that you're getting the latest versions of dependencies in your build, you can use the option to avoid cache hits.\n\nThe following Dockerfile uses the tag of the image. Over time, that tag may resolve to a different underlying version of the image, as the publisher rebuilds the image with new security patches and updated libraries. Using the , you can avoid cache hits and ensure a fresh download of base images and dependencies.\n\nTo exclude files not relevant to the build, without restructuring your source repository, use a file. This file supports exclusion patterns similar to files.\n\nFor example, to exclude all files with the extension:\n\nFor information on creating one, see Dockerignore file.\n\nThe image defined by your Dockerfile should generate containers that are as ephemeral as possible. Ephemeral means that the container can be stopped and destroyed, then rebuilt and replaced with an absolute minimum set up and configuration.\n\nRefer to Processes under The Twelve-factor App methodology to get a feel for the motivations of running containers in such a stateless fashion.\n\nAvoid installing extra or unnecessary packages just because they might be nice to have. For example, you don’t need to include a text editor in a database image.\n\nWhen you avoid installing extra or unnecessary packages, your images have reduced complexity, reduced dependencies, reduced file sizes, and reduced build times.\n\nEach container should have only one concern. Decoupling applications into multiple containers makes it easier to scale horizontally and reuse containers. For instance, a web application stack might consist of three separate containers, each with its own unique image, to manage the web application, database, and an in-memory cache in a decoupled manner.\n\nLimiting each container to one process is a good rule of thumb, but it's not a hard and fast rule. For example, not only can containers be spawned with an init process, some programs might spawn additional processes of their own accord. For instance, Celery can spawn multiple worker processes, and Apache can create one process per request.\n\nUse your best judgment to keep containers as clean and modular as possible. If containers depend on each other, you can use Docker container networks to ensure that these containers can communicate.\n\nWhenever possible, sort multi-line arguments alphanumerically to make maintenance easier. This helps to avoid duplication of packages and make the list much easier to update. This also makes PRs a lot easier to read and review. Adding a space before a backslash ( ) helps as well.\n\nHere’s an example from the buildpack-deps image :\n\nWhen building an image, Docker steps through the instructions in your Dockerfile, executing each in the order specified. For each instruction, Docker checks whether it can reuse the instruction from the build cache.\n\nUnderstanding how the build cache works, and how cache invalidation occurs, is critical for ensuring faster builds. For more information about the Docker build cache and how to optimize your builds, see Docker build cache.\n\nImage tags are mutable, meaning a publisher can update a tag to point to a new image. This is useful because it lets publishers update tags to point to newer versions of an image. And as an image consumer, it means you automatically get the new version when you re-build your image.\n\nFor example, if you specify in your Dockerfile, resolves to the latest patch version for .\n\nAt one point in time, the tag might point to version 3.19.1 of the image. If you rebuild the image 3 months later, the same tag might point to a different version, such as 3.19.4. This publishing workflow is best practice, and most publishers use this tagging strategy, but it isn't enforced.\n\nThe downside with this is that you're not guaranteed to get the same for every build. This could result in breaking changes, and it means you also don't have an audit trail of the exact image versions that you're using.\n\nTo fully secure your supply chain integrity, you can pin the image version to a specific digest. By pinning your images to a digest, you're guaranteed to always use the same image version, even if a publisher replaces the tag with a new image. For example, the following Dockerfile pins the Alpine image to the same tag as earlier, , but this time with a digest reference as well.\n\nWith this Dockerfile, even if the publisher updates the tag, your builds would still use the pinned image version: .\n\nWhile this helps you avoid unexpected changes, it's also more tedious to have to look up and include the image digest for base image versions manually each time you want to update it. And you're opting out of automated security fixes, which is likely something you want to get.\n\nDocker Scout's default Up-to-Date Base Images policy checks whether the base image version you're using is in fact the latest version. This policy also checks if pinned digests in your Dockerfile correspond to the correct version. If a publisher updates an image that you've pinned, the policy evaluation returns a non-compliant status, indicating that you should update your image.\n\nDocker Scout also supports an automated remediation workflow for keeping your base images up-to-date. When a new image digest is available, Docker Scout can automatically raise a pull request on your repository to update your Dockerfiles to use the latest version. This is better than using a tag that changes the version automatically, because you're in control and you have an audit trail of when and how the change occurred.\n\nFor more information about automatically updating your base images with Docker Scout, see Remediation.\n\nBuild and test your images in CI\n\nWhen you check in a change to source control or create a pull request, use GitHub Actions or another CI/CD pipeline to automatically build and tag a Docker image and test it.\n\nFollow these recommendations on how to properly use the Dockerfile instructions to create an efficient and maintainable Dockerfile.\n\nWhenever possible, use current official images as the basis for your images. Docker recommends the Alpine image as it is tightly controlled and small in size (currently under 6 MB), while still being a full Linux distribution.\n\nFor more information about the instruction, see Dockerfile reference for the FROM instruction.\n\nYou can add labels to your image to help organize images by project, record licensing information, to aid in automation, or for other reasons. For each label, add a line beginning with with one or more key-value pairs. The following examples show the different acceptable formats. Explanatory comments are included inline.\n\nStrings with spaces must be quoted or the spaces must be escaped. Inner quote characters ( ), must also be escaped. For example:\n\nAn image can have more than one label. Prior to Docker 1.10, it was recommended to combine all labels into a single instruction, to prevent extra layers from being created. This is no longer necessary, but combining labels is still supported. For example:\n\nThe above example can also be written as:\n\nSee Understanding object labels for guidelines about acceptable label keys and values. For information about querying labels, refer to the items related to filtering in Managing labels on objects. See also LABEL in the Dockerfile reference.\n\nSplit long or complex statements on multiple lines separated with backslashes to make your Dockerfile more readable, understandable, and maintainable.\n\nFor example, you can chain commands with the operator, and use escape characters to break long commands into multiple lines.\n\nBy default, backslash escapes a newline character, but you can change it with the directive.\n\nYou can also use here documents to run multiple commands without chaining them with a pipeline operator:\n\nFor more information about , see Dockerfile reference for the RUN instruction.\n\nOne common use case for instructions in Debian-based images is to install software using . Because installs packages, the command has several counter-intuitive behaviors to look out for.\n\nAlways combine with in the same statement. For example:\n\nUsing alone in a statement causes caching issues and subsequent instructions to fail. For example, this issue will occur in the following Dockerfile:\n\nAfter building the image, all layers are in the Docker cache. Suppose you later modify by adding an extra package as shown in the following Dockerfile:\n\nDocker sees the initial and modified instructions as identical and reuses the cache from previous steps. As a result the isn't executed because the build uses the cached version. Because the isn't run, your build can potentially get an outdated version of the and packages.\n\nUsing ensures your Dockerfile installs the latest package versions with no further coding or manual intervention. This technique is known as cache busting. You can also achieve cache busting by specifying a package version. This is known as version pinning. For example:\n\nVersion pinning forces the build to retrieve a particular version regardless of what’s in the cache. This technique can also reduce failures due to unanticipated changes in required packages.\n\nBelow is a well-formed instruction that demonstrates all the recommendations.\n\nThe argument specifies a version . If the image previously used an older version, specifying the new one causes a cache bust of and ensures the installation of the new version. Listing packages on each line can also prevent mistakes in package duplication.\n\nIn addition, when you clean up the apt cache by removing it reduces the image size, since the apt cache isn't stored in a layer. Since the statement starts with , the package cache is always refreshed prior to .\n\nOfficial Debian and Ubuntu images automatically run , so explicit invocation is not required.\n\nSome commands depend on the ability to pipe the output of one command into another, using the pipe character ( ), as in the following example:\n\nDocker executes these commands using the interpreter, which only evaluates the exit code of the last operation in the pipe to determine success. In the example above, this build step succeeds and produces a new image so long as the command succeeds, even if the command fails.\n\nIf you want the command to fail due to an error at any stage in the pipe, prepend to ensure that an unexpected error prevents the build from inadvertently succeeding. For example:\n\nThe instruction should be used to run the software contained in your image, along with any arguments. should almost always be used in the form of . Thus, if the image is for a service, such as Apache and Rails, you would run something like . Indeed, this form of the instruction is recommended for any service-based image.\n\nIn most other cases, should be given an interactive shell, such as bash, python and perl. For example, , , or . Using this form means that when you execute something like , you’ll get dropped into a usable shell, ready to go. should rarely be used in the manner of in conjunction with , unless you and your expected users are already quite familiar with how works.\n\nFor more information about , see Dockerfile reference for the CMD instruction.\n\nThe instruction indicates the ports on which a container listens for connections. Consequently, you should use the common, traditional port for your application. For example, an image containing the Apache web server would use , while an image containing MongoDB would use and so on.\n\nFor external access, your users can execute with a flag indicating how to map the specified port to the port of their choice. For container linking, Docker provides environment variables for the path from the recipient container back to the source (for example, ).\n\nFor more information about , see Dockerfile reference for the EXPOSE instruction.\n\nTo make new software easier to run, you can use to update the environment variable for the software your container installs. For example, ensures that just works.\n\nThe instruction is also useful for providing the required environment variables specific to services you want to containerize, such as Postgres’s .\n\nLastly, can also be used to set commonly used version numbers so that version bumps are easier to maintain, as seen in the following example:\n\nSimilar to having constant variables in a program, as opposed to hard-coding values, this approach lets you change a single instruction to automatically bump the version of the software in your container.\n\nEach line creates a new intermediate layer, just like commands. This means that even if you unset the environment variable in a future layer, it still persists in this layer and its value can be dumped. You can test this by creating a Dockerfile like the following, and then building it.\n\nTo prevent this, and really unset the environment variable, use a command with shell commands, to set, use, and unset the variable all in a single layer. You can separate your commands with or . If you use the second method, and one of the commands fails, the also fails. This is usually a good idea. Using as a line continuation character for Linux Dockerfiles improves readability. You could also put all of the commands into a shell script and have the command just run that shell script.\n\nFor more information about , see Dockerfile reference for the ENV instruction.\n\nand are functionally similar. supports basic copying of files into the container, from the build context or from a stage in a multi-stage build. supports features for fetching files from remote HTTPS and Git URLs, and extracting tar files automatically when adding files from the build context.\n\nYou'll mostly want to use for copying files from one stage to another in a multi-stage build. If you need to add files from the build context to the container temporarily to execute a instruction, you can often substitute the instruction with a bind mount instead. For example, to temporarily add a file for a instruction:\n\nBind mounts are more efficient than for including files from the build context in the container. Note that bind-mounted files are only added temporarily for a single instruction, and don't persist in the final image. If you need to include files from the build context in the final image, use .\n\nThe instruction is best for when you need to download a remote artifact as part of your build. is better than manually adding files using something like and , because it ensures a more precise build cache. also has built-in support for checksum validation of the remote resources, and a protocol for parsing branches, tags, and subdirectories from Git URLs.\n\nThe following example uses to download a .NET installer. Combined with multi-stage builds, only the .NET runtime remains in the final stage, no intermediate files.\n\nFor more information about or , see the following:\n\nThe best use for is to set the image's main command, allowing that image to be run as though it was that command, and then use as the default flags.\n\nThe following is an example of an image for the command line tool :\n\nYou can use the following command to run the image and show the command's help:\n\nOr, you can use the right parameters to execute a command, like in the following example:\n\nThis is useful because the image name can double as a reference to the binary as shown in the command above.\n\nThe instruction can also be used in combination with a helper script, allowing it to function in a similar way to the command above, even when starting the tool may require more than one step.\n\nFor example, the Postgres Official Image uses the following script as its :\n\nThis script uses the Bash command so that the final running application becomes the container's PID 1. This allows the application to receive any Unix signals sent to the container. For more information, see the reference.\n\nIn the following example, a helper script is copied into the container and run via on container start:\n\nThis script lets you interact with Postgres in several ways.\n\nOr, you can use it to run Postgres and pass parameters to the server:\n\nLastly, you can use it to start a totally different tool, such as Bash:\n\nFor more information about , see Dockerfile reference for the ENTRYPOINT instruction.\n\nYou should use the instruction to expose any database storage area, configuration storage, or files and folders created by your Docker container. You are strongly encouraged to use for any combination of mutable or user-serviceable parts of your image.\n\nFor more information about , see Dockerfile reference for the VOLUME instruction.\n\nIf a service can run without privileges, use to change to a non-root user. Start by creating the user and group in the Dockerfile with something like the following example:\n\nAvoid installing or using as it has unpredictable TTY and signal-forwarding behavior that can cause problems. If you absolutely need functionality similar to , such as initializing the daemon as but running it as non- , consider using “gosu” .\n\nLastly, to reduce layers and complexity, avoid switching back and forth frequently.\n\nFor more information about , see Dockerfile reference for the USER instruction.\n\nFor clarity and reliability, you should always use absolute paths for your . Also, you should use instead of proliferating instructions like , which are hard to read, troubleshoot, and maintain.\n\nFor more information about , see Dockerfile reference for the WORKDIR instruction.\n\nAn command executes after the current Dockerfile build completes. executes in any child image derived the current image. Think of the command as an instruction that the parent Dockerfile gives to the child Dockerfile.\n\nA Docker build executes commands before any command in a child Dockerfile.\n\nis useful for images that are going to be built a given image. For example, you would use for a language stack image that builds arbitrary user software written in that language within the Dockerfile, as you can see in Ruby’s variants .\n\nImages built with should get a separate tag. For example, or .\n\nBe careful when putting or in . The onbuild image fails catastrophically if the new build's context is missing the resource being added. Adding a separate tag, as recommended above, helps mitigate this by allowing the Dockerfile author to make a choice.\n\nFor more information about , see Dockerfile reference for the ONBUILD instruction."
    },
    {
        "link": "https://stackoverflow.com/questions/37789984/how-to-copy-folders-to-docker-image-from-dockerfile",
        "document": "I tried the following command in my Dockerfile: and got mighty surprised at the result. Seems the naive docker code traverses the directories from the glob and then dumps the each file in the target directory while respectfully ignoring my directory structure.\n\nAt least that is how I understand this ticket and it certainly corresponds to the result I got.\n\nI guess the only reason this behavior can still exist must be that there is some other way this should be done. But it is not so easy for a bear of very little brain to understand how, does anyone know?"
    },
    {
        "link": "https://reddit.com/r/docker/comments/16p6m51/best_practice_for_copying_files_and_keeping",
        "document": "If it was normal docker build, I can use COPY command to move local config files to docker.\n\nI am using docker-compose to start prometheus. How can I manage it prometheus.yml file locally in source code and copy at required place in container when it starts?\n\nMy config is below.\n\n2) What is the best practice to manage volumes so that they can also be changed by code? Like copy files to volumes from local windows folder etc?"
    }
]