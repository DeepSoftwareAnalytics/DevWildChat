[
    {
        "link": "https://curl.se/libcurl/c/curl_easy_setopt.html",
        "document": ""
    },
    {
        "link": "https://curl.se/libcurl/c/libcurl.html",
        "document": "This is a short overview on how to use libcurl in your C programs. There are specific man pages for each function mentioned in here. See libcurl-easy, libcurl-multi, libcurl-share, libcurl-url, libcurl-ws and libcurl-tutorial for in-depth understanding on how to program with libcurl.\n\nThere are many bindings available that bring libcurl access to your favorite language. Look elsewhere for documentation on those.\n\nTo transfer files, you create an \"easy handle\" using curl_easy_init for a single individual transfer (in either direction). You then set your desired set of options in that handle with curl_easy_setopt. Options you set with curl_easy_setopt stick. They are then used for every repeated use of this handle until you either change the option, or you reset them all with curl_easy_reset.\n\nTo actually transfer data you have the option of using the \"easy\" interface, or the \"multi\" interface.\n\nThe easy interface is a synchronous interface with which you call curl_easy_perform and let it perform the transfer. When it is completed, the function returns and you can continue. More details are found in the libcurl-easy man page.\n\nThe multi interface on the other hand is an asynchronous interface, that you call and that performs only a little piece of the transfer on each invoke. It is perfect if you want to do things while the transfer is in progress, or similar. The multi interface allows you to select() on libcurl action, and even to easily download multiple files simultaneously using a single thread. See further details in the libcurl-multi man page.\n\nThere is also a series of other helpful functions and interface families to use, including these:\n\ngets detailed libcurl (and other used libraries) version info. See curl_version_info\n\nget information about a performed transfer. See curl_easy_getinfo\n\nYou can have multiple easy handles share certain data, even if they are used in different threads. This magic is setup using the share interface, as described in the libcurl-share man page.\n\nOn Unix-like machines, there is a tool named curl-config that gets installed with the rest of the curl stuff when 'make install' is performed.\n\ncurl-config is added to make it easier for applications to link with libcurl and developers to learn about libcurl and how to use it.\n\nRun 'curl-config --libs' to get the (additional) linker options you need to link with the particular version of libcurl you have installed. See the curl-config man page for further details.\n\nUnix-like operating system that ship libcurl as part of their distributions often do not provide the curl-config tool, but simply install the library and headers in the common path for this purpose.\n\nMany Linux and similar systems use pkg-config to provide build and link options about libraries and libcurl supports that as well.\n\nAll public functions in the libcurl interface are prefixed with 'curl_' (with a lowercase c). You can find other functions in the library source code, but other prefixes indicate that the functions are private and may change without further notice in the next release.\n\nOnly use documented functions and functionality.\n\nlibcurl works exactly the same, on any of the platforms it compiles and builds on.\n\nlibcurl is thread safe but there are a few exceptions. Refer to libcurl-thread for more information.\n\nPersistent connections means that libcurl can reuse the same connection for several transfers, if the conditions are right.\n\nlibcurl always attempts to use persistent connections. Whenever you use curl_easy_perform or curl_multi_perform etc, libcurl attempts to use an existing connection to do the transfer, and if none exists it opens a new one that is subject for reuse on a possible following call to curl_easy_perform or curl_multi_perform.\n\nTo allow libcurl to take full advantage of persistent connections, you should do as many of your file transfers as possible using the same handle.\n\nIf you use the easy interface, and you call curl_easy_cleanup, all the possibly open connections held by libcurl are closed and forgotten.\n\nWhen you have created a multi handle and are using the multi interface, the connection pool is instead kept in the multi handle so closing and creating new easy handles to do transfers do not affect them. Instead all added easy handles can take advantage of the single shared pool.\n\nThere are a variety of constants that libcurl uses, mainly through its internal use of other libraries, which are too complicated for the library loader to set up. Therefore, a program must call a library function after the program is loaded and running to finish setting up the library code. For example, when libcurl is built for SSL capability via the GNU TLS library, there is an elaborate tree inside that library that describes the SSL protocol.\n\ncurl_global_init is the function that you must call. This may allocate resources (e.g. the memory for the GNU TLS tree mentioned above), so the companion function curl_global_cleanup releases them.\n\nIf libcurl was compiled with support for multiple SSL backends, the function curl_global_sslset can be called before curl_global_init to select the active SSL backend.\n\nThe global constant functions are thread-safe since libcurl 7.84.0 if curl_version_info has the CURL_VERSION_THREADSAFE feature bit set (most platforms). Read libcurl-thread for thread safety guidelines.\n\nIf the global constant functions are not thread safe, then you must not call them when any other thread in the program is running. It is not good enough that no other thread is using libcurl at the time, because these functions internally call similar functions of other libraries, and those functions are similarly thread-unsafe. You cannot generally know what these libraries are, or whether other threads are using them.\n\nIf the global constant functions are not thread safe, then the basic rule for constructing a program that uses libcurl is this: Call curl_global_init, with a CURL_GLOBAL_ALL argument, immediately after the program starts, while it is still only one thread and before it uses libcurl at all. Call curl_global_cleanup immediately before the program exits, when the program is again only one thread and after its last use of libcurl.\n\nIt is not actually required that the functions be called at the beginning and end of the program -- that is just usually the easiest way to do it.\n\nYou can call both of these multiple times, as long as all calls meet these requirements and the number of calls to each is the same.\n\nThe global constant situation merits special consideration when the code you are writing to use libcurl is not the main program, but rather a modular piece of a program, e.g. another library. As a module, your code does not know about other parts of the program -- it does not know whether they use libcurl or not. Its code does not necessarily run at the start and end of the whole program.\n\nA module like this must have global constant functions of its own, just like curl_global_init and curl_global_cleanup. The module thus has control at the beginning and end of the program and has a place to call the libcurl functions. If multiple modules in the program use libcurl, they all separately call the libcurl functions, and that is OK because only the first curl_global_init and the last curl_global_cleanup in a program change anything. (libcurl uses a reference count in static memory).\n\nIn a C++ module, it is common to deal with the global constant situation by defining a special class that represents the global constant environment of the module. A program always has exactly one object of the class, in static storage. That way, the program automatically calls the constructor of the object as the program starts up and the destructor as it terminates. As the author of this libcurl-using module, you can make the constructor call curl_global_init and the destructor call curl_global_cleanup and satisfy libcurl's requirements without your user having to think about it. (Caveat: If you are initializing libcurl from a Windows DLL you should not initialize it from DllMain or a static initializer because Windows holds the loader lock during that time and it could cause a deadlock.)\n\ncurl_global_init has an argument that tells what particular parts of the global constant environment to set up. In order to successfully use any value except CURL_GLOBAL_ALL (which says to set up the whole thing), you must have specific knowledge of internal workings of libcurl and all other parts of the program of which it is part.\n\nA special part of the global constant environment is the identity of the memory allocator. curl_global_init selects the system default memory allocator, but you can use curl_global_init_mem to supply one of your own. However, there is no way to use curl_global_init_mem in a modular program -- all modules in the program that might use libcurl would have to agree on one allocator.\n\nThere is a failsafe in libcurl that makes it usable in simple situations without you having to worry about the global constant environment at all: curl_easy_init sets up the environment itself if it has not been done yet. The resources it acquires to do so get released by the operating system automatically when the program exits.\n\nThis failsafe feature exists mainly for backward compatibility because there was a time when the global functions did not exist. Because it is sufficient only in the simplest of programs, it is not recommended for any program to rely on it.\n\nThis HTML page was made with roffit."
    },
    {
        "link": "https://curl.se/libcurl/c/example.html",
        "document": "A collection of smaller stand-alone applications using the libcurl API in different ways to show how to use it for different Internet transfer scenarios.\n\nYou can also see a list of all libcurl easy options and which example source codes that use them.\n\nAll examples are written in C, unless specifically mentioned.\n\nYou also find these examples in the distribution archive, in docs/examples."
    },
    {
        "link": "https://curl.se/libcurl/c/libcurl-tutorial.html",
        "document": "There are many different ways to build C programs. This chapter assumes a Unix style build process. If you use a different build system, you can still read this to get general information that may apply to your environment as well.\n\nYou program libcurl the same way on all platforms that libcurl runs on. There are only a few minor details that differ. If you just make sure to write your code portable enough, you can create a portable program. libcurl should not stop you from that.\n\nand it takes one parameter which is a bit pattern that tells libcurl what to initialize. Using CURL_GLOBAL_ALL makes it initialize all known internal sub modules, and might be a good default option. The current two bits that are specified are:\n\nlibcurl first introduced the so called easy interface. All operations in the easy interface are prefixed with 'curl_easy'. The easy interface lets you do single transfers with a synchronous and blocking function call.\n\nlibcurl also offers another interface that allows multiple simultaneous transfers in a single thread, the so called multi interface. More about that interface is detailed in a separate chapter further down. You still need to understand the easy interface first, so please continue reading for better understanding.\n\nIf you at any point would like to blank all previously set options for a single easy handle, you can call curl_easy_reset and you can also make a clone of an easy handle (with all its set options) using curl_easy_duphandle.\n\nMany of the options you set in libcurl are \"strings\", pointers to data terminated with a zero byte. When you set strings with curl_easy_setopt, libcurl makes its own copy so that they do not need to be kept around in your application after being set[4].\n\nOne of the most basic properties to set in the handle is the URL. You set your preferred URL to transfer with CURLOPT_URL in a manner similar to:\n\nLet's assume for a while that you want to receive data as the URL identifies a remote resource you want to get here. Since you write a sort of application that needs this transfer, I assume that you would like to get the data passed to you directly instead of simply getting it passed to stdout. So, you write your own function that matches this prototype:\n\nlibcurl offers its own default internal callback that takes care of the data if you do not set the callback with CURLOPT_WRITEFUNCTION. It simply outputs the received data to stdout. You can have the default callback write the data to a different file handle by passing a 'FILE *' to a file opened for writing with the CURLOPT_WRITEDATA option.\n\nNow, we need to take a step back and take a deep breath. Here is one of those rare platform-dependent nitpicks. Did you spot it? On some platforms[2], libcurl is not able to operate on file handles opened by the program. Therefore, if you use the default callback and pass in an open file handle with CURLOPT_WRITEDATA, libcurl crashes. You should avoid this to make your program run fine virtually everywhere.\n\nWhen the transfer is complete, the function returns a return code that informs you if it succeeded in its mission or not. If a return code is not enough for you, you can use the CURLOPT_ERRORBUFFER to point libcurl to a buffer of yours where it stores a human readable error message as well.\n\nFor some protocols, downloading a file can involve a complicated process of logging in, setting the transfer mode, changing the current directory and finally transferring the file data. libcurl takes care of all that complication for you. Given simply the URL to a file, libcurl takes care of all the details needed to get the file moved from one machine to another.\n\nThere is one golden rule when these things occur: set the CURLOPT_VERBOSE option to 1. it causes the library to spew out the entire protocol details it sends, some internal info and some received protocol data as well (especially when using FTP). If you are using HTTP, adding the headers in the received output to study is also a clever way to get a better understanding why the server behaves the way it does. Include headers in the normal body output with CURLOPT_HEADER set 1.\n\nlibcurl tries to keep a protocol independent approach to most transfers, thus uploading to a remote FTP site is similar to uploading data to an HTTP server with a PUT request.\n\nWhere bufptr is the pointer to a buffer we fill in with data to upload and sizenitems* is the size of the buffer and therefore also the maximum amount of data we can return to libcurl in this call. The userp pointer is the custom pointer we set to point to a struct of ours to pass private data between the application and the callback.\n\nWhen you call curl_easy_perform this time, it performs all the necessary operations and when it has invoked the upload it calls your supplied callback to get the data to upload. The program should return as much data as possible in every invoke, as that is likely to make the upload perform as fast as possible. The callback should return the number of bytes it wrote in the buffer. Returning 0 signals the end of the upload.\n\nlibcurl also provides options to set various passwords. The username and password as shown embedded in the URL can instead get set with the CURLOPT_USERPWD option. The argument passed to libcurl should be a char * to a string in the format \"user:password\". In a manner like this:\n\nThere is a long time Unix \"standard\" way of storing FTP usernames and passwords, namely in the $HOME/.netrc file (on Windows, libcurl also checks the %USERPROFILE% environment variable if %HOME% is unset, and tries \"_netrc\" as name). The file should be made private so that only the user may read it (see also the \"Security Considerations\" chapter), as it might contain the password in plain text. libcurl has the ability to use this file to figure out what set of username and password to use for a particular host. As an extension to the normal functionality, libcurl also supports this file for non-FTP protocols such as HTTP. To make curl use this file, use the CURLOPT_NETRC option:\n\nThe previous chapter showed how to set username and password for getting URLs that require authentication. When using the HTTP protocol, there are many different ways a client can provide those credentials to the server and you can control which way libcurl uses them. The default HTTP authentication method is called 'Basic', which is sending the name and password in clear-text in the HTTP request, base64-encoded. This is insecure.\n\nWhat if you want to post binary data that also requires you to set the Content-Type: header of the post? Well, binary posts prevent libcurl from being able to do strlen() on the data to figure out the size, so therefore we must tell libcurl the size of the post data. Setting headers in libcurl requests are done in a generic way, by building a list of our own headers and then passing that list to libcurl.\n\nWhile the simple examples above cover the majority of all cases where HTTP POST operations are required, they do not do multi-part formposts. Multi-part formposts were introduced as a better way to post (possibly large) binary data and were first documented in the RFC 1867 (updated in RFC 2388). They are called multi-part because they are built by a chain of parts, each part being a single unit of data. Each part has its own name and contents. You can in fact create and post a multi-part formpost with the regular libcurl POST support described above, but that would require that you build a formpost yourself and provide to libcurl.\n\nTo make that easier, libcurl provides a MIME API consisting in several functions: using those, you can create and fill a multi-part form. Function curl_mime_init creates a multi-part body; you can then append new parts to a multi-part body using curl_mime_addpart.\n\nThere are three possible data sources for a part: memory using curl_mime_data, file using curl_mime_filedata and user-defined data read callback using curl_mime_data_cb. curl_mime_name sets a part's (i.e.: form field) name, while curl_mime_filename fills in the remote filename. With curl_mime_type, you can tell the MIME type of a part, curl_mime_headers allows defining the part's headers. When a multi-part body is no longer needed, you can destroy it using curl_mime_free.\n\nTo post multiple files for a single form field, you must supply each file in a separate part, all with the same field name. Although function curl_mime_subparts implements nested multi-parts, this way of multiple files posting is deprecated by RFC 7578, chapter 4.3.\n\nFor historical and traditional reasons, libcurl has a built-in progress meter that can be switched on and then makes it present a progress meter in your terminal.\n\nThere is basically only one thing to keep in mind when using C++ instead of C when interfacing libcurl:\n\nWhat \"proxy\" means according to Merriam-Webster: \"a person authorized to act for another\" but also \"the agency, function, or office of a deputy who acts as a substitute for another\".\n\nFor HTTP proxies: the fact that the proxy is an HTTP proxy puts certain restrictions on what can actually happen. A requested URL that might not be a HTTP URL is passed to the HTTP proxy to deliver back to libcurl. This happens transparently, and an application may not need to know. I say \"may\", because at times it is important to understand that all operations over an HTTP proxy use the HTTP protocol. For example, you cannot invoke your own custom FTP commands or even proper FTP directory listings.\n\nlibcurl automatically checks and uses a set of environment variables to know what proxies to use for certain protocols. The names of the variables are following an old tradition and are built up as \"[protocol]_proxy\" (note the lower casing). Which makes the variable 'http_proxy' checked for a name of a proxy to use when the input URL is HTTP. Following the same rule, the variable named 'ftp_proxy' is checked for FTP URLs. Again, the proxies are always HTTP proxies, the different names of the variables simply allows different HTTP proxies to be used.\n\nThere are two special environment variables. 'all_proxy' is what sets proxy for any URL in case the protocol specific variable was not set, and 'no_proxy' defines a list of hosts that should not use a proxy even though a variable may say so. If 'no_proxy' is a plain asterisk (\"*\") it matches all hosts.\n\nSSL is for secure point-to-point connections. This involves strong encryption and similar things, which effectively makes it impossible for a proxy to operate as a \"man in between\" which the proxy's task is, as previously discussed. Instead, the only way to have SSL work over an HTTP proxy is to ask the proxy to tunnel everything through without being able to check or fiddle with the traffic.\n\nOpening an SSL connection over an HTTP proxy is therefore a matter of asking the proxy for a straight connection to the target host on a specified port. This is made with the HTTP request CONNECT. (\"please dear proxy, connect me to that remote host\").\n\nBecause of the nature of this operation, where the proxy has no idea what kind of data that is passed in and out through this tunnel, this breaks some of the few advantages that come from using a proxy, such as caching. Many organizations prevent this kind of tunneling to other destination port numbers than 443 (which is the default HTTPS port number).\n\nIn fact, there might even be times when you want to do plain HTTP operations using a tunnel like this, as it then enables you to operate on the remote server instead of asking the proxy to do so. libcurl does not stand in the way for such innovative actions either!\n\nNetscape first came up with this. It is basically a webpage (usually using a .pac extension) with a JavaScript that when executed by the browser with the requested URL as input, returns information to the browser on how to connect to the URL. The returned information might be \"DIRECT\" (which means no proxy should be used), \"PROXY host:port\" (to tell the browser where the proxy for this particular URL is) or \"SOCKS host:port\" (to direct the browser to a SOCKS proxy).\n\nlibcurl has no means to interpret or evaluate JavaScript and thus it does not support this. If you get yourself in a position where you face this nasty invention, the following advice have been mentioned and used in the past:\n\nEach easy handle attempts to keep the last few connections alive for a while in case they are to be used again. You can set the size of this \"cache\" with the CURLOPT_MAXCONNECTS option. Default is 5. There is rarely any point in changing this value, and if you think of changing this it is often just a matter of thinking again.\n\nSending custom commands to an FTP server means that you need to send the commands exactly as the FTP server expects them (RFC 959 is a good guide here), and you can only use commands that work on the control-connection alone. All kinds of commands that require data interchange and thus need a data-connection must be left to libcurl's own judgment. Also be aware that libcurl does its best to change directory to the target directory before doing any transfer, so if you change directory (with CWD or similar) you might confuse libcurl and then it might not attempt to transfer the file in the correct remote directory.\n\nThe custom FTP commands are issued to the server in the same order they are added to the list, and if a command gets an error code returned back from the server, no more commands are issued and libcurl bails out with an error code (CURLE_QUOTE_ERROR). Note that if you use CURLOPT_QUOTE to send commands before a transfer, no transfer actually takes place when a quote command has failed.\n\nIn the HTTP sense, a cookie is a name with an associated value. A server sends the name and value to the client, and expects it to get sent back on every subsequent request to the server that matches the particular conditions set. The conditions include that the domain name and path match and that the cookie has not become too old.\n\nOne way to do this, is to save all headers you receive in a plain file and when you make a request, you tell libcurl to read the previous headers to figure out which cookies to use. Set the header file to read cookies from with CURLOPT_COOKIEFILE.\n\nThe CURLOPT_COOKIEFILE option also automatically enables the cookie parser in libcurl. Until the cookie parser is enabled, libcurl does not parse or understand incoming cookies and they are just be ignored. However, when the parser is enabled the cookies are understood and the cookies are kept in memory and used properly in subsequent requests when the same handle is used. Many times this is enough, and you may not have to save the cookies to disk at all. Note that the file you specify to CURLOPT_COOKIEFILE does not have to exist to enable the parser, so a common way to just enable the parser and not read any cookies is to use the name of a file you know does not exist.\n\nPerhaps the most advanced cookie operation libcurl offers, is saving the entire internal cookie state back into a Netscape/Mozilla formatted cookie file. We call that the cookie-jar. When you set a filename with CURLOPT_COOKIEJAR, that filename is created and all received cookies get stored in it when curl_easy_cleanup is called. This enables cookies to get passed on properly between multiple handles without any information getting lost.\n\nlibcurl can either connect to the server a second time or tell the server to connect back to it. The first option is the default and it is also what works best for all the people behind firewalls, NATs or IP-masquerading setups. libcurl then tells the server to open up a new port and wait for a second connection. This is by default attempted with EPSV first, and if that does not work it tries PASV instead. (EPSV is an extension to the original FTP spec and does not exist nor work on all FTP servers.)\n\nIn some cases, you want to have the server connect back to you for the second connection. This might be when the server is perhaps behind a firewall or something and only allows connections on a single port. libcurl then informs the remote server which IP address and port number to connect to. This is made with the CURLOPT_FTPPORT option. If you set it to \"-\", libcurl uses your system's \"default IP address\". If you want to use a particular IP, you can set the full IP address, a hostname to resolve to an IP address or even a local network interface name that libcurl gets the IP address from.\n\nTo build such a message, you prepare the nth-level multi-part and then include it as a source to the parent multi-part using function curl_mime_subparts. Once it has been bound to its parent multi-part, a nth-level multi-part belongs to it and should not be freed explicitly.\n\nEmail messages data is not supposed to be non-ASCII and line length is limited: fortunately, some transfer encodings are defined by the standards to support the transmission of such incompatible data. Function curl_mime_encoder tells a part that its source data must be encoded before being sent. It also generates the corresponding header for that part. If the part data you want to send is already encoded in such a scheme, do not use this function (this would over-encode it), but explicitly set the corresponding part header.\n\nThe easy interface as described in detail in this document is a synchronous interface that transfers one file at a time and does not return until it is done.\n\nYou create the easy handles you want, one for each concurrent transfer, and you set all the options just like you learned above, and then you create a multi handle with curl_multi_init and add all those easy handles to that multi handle with curl_multi_add_handle.\n\nThe best usage of this interface is when you do a select() on all possible file descriptors or sockets to know when to call libcurl again. This also makes it easy for you to wait and respond to actions on your own application's sockets/handles. You figure out what to select() for by using curl_multi_fdset, that fills in a set of fd_set variables for you with the particular file descriptors libcurl uses for the moment.\n\nWhen a transfer within the multi stack has finished, the counter of running transfers (as filled in by curl_multi_perform) decreases. When the number reaches zero, all transfers are done.\n\nWhen you add easy handles to a multi handle, these easy handles automatically share a lot of the data that otherwise would be kept on a per-easy handle basis when the easy interface is used."
    },
    {
        "link": "https://stackoverflow.com/questions/54572834/i-am-writing-a-rest-api-using-libcurl-library-in-c-language-to-login-to-my-web-p",
        "document": "You're doing a mix of do-it-yourself and let libcurl do it, both only half way.\n\nYou can add the HTTP authorization header yourself, but this is not a valid such header:\n\nYou're missing the base64(\"user:password\") part that follows \"Basic\". Similar to this:\n\nlet libcurl do it\n\nThen you don't pass on the header manually and instead ask libcurl to deal with the authentication. You do that by setting the option like you already do - but in your case you ruin it by your own custom (broken) header that overrides that internally generated header."
    },
    {
        "link": "https://stackoverflow.com/questions/6951161/downloading-multiple-files-with-libcurl-in-c",
        "document": "I am currently trying to make an updater for my software project. I need it to be able to download multiple files, I don't mind if they download in sync or one after each other, whatever is easier (file size is not an issue). I followed the example from the libcurl webpage and a few other resources and came up with this:\n\nThe first issue is if i remove the new file assignments ( ) and just try to loop the download code twice, the program simply stops responding. This occurs in any combination of the download being called more than once in the program. The other issue is that I am unable to change the value of the character array . I feel like this is just some silly error I am making but no solution comes to mind. Thank you!"
    },
    {
        "link": "https://stackoverflow.com/questions/2162022/downloading-all-files-in-directory-using-libcurl",
        "document": "You need the list of files on the FTP server. Which isn't straightforward as each FTP server might return a different format of file listing...\n\nAnyway, the ftpgetresp.c example shows a way to do it, I think. FTP Custom CUSTOMREQUEST suggests another way."
    },
    {
        "link": "https://curl.se/libcurl/c/multi-app.html",
        "document": ""
    },
    {
        "link": "https://superuser.com/questions/1783197/trying-to-download-multiple-files-with-curl-files-does-not-exist-error",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://curl.se/libcurl/c/libcurl-tutorial.html",
        "document": "There are many different ways to build C programs. This chapter assumes a Unix style build process. If you use a different build system, you can still read this to get general information that may apply to your environment as well.\n\nYou program libcurl the same way on all platforms that libcurl runs on. There are only a few minor details that differ. If you just make sure to write your code portable enough, you can create a portable program. libcurl should not stop you from that.\n\nand it takes one parameter which is a bit pattern that tells libcurl what to initialize. Using CURL_GLOBAL_ALL makes it initialize all known internal sub modules, and might be a good default option. The current two bits that are specified are:\n\nlibcurl first introduced the so called easy interface. All operations in the easy interface are prefixed with 'curl_easy'. The easy interface lets you do single transfers with a synchronous and blocking function call.\n\nlibcurl also offers another interface that allows multiple simultaneous transfers in a single thread, the so called multi interface. More about that interface is detailed in a separate chapter further down. You still need to understand the easy interface first, so please continue reading for better understanding.\n\nIf you at any point would like to blank all previously set options for a single easy handle, you can call curl_easy_reset and you can also make a clone of an easy handle (with all its set options) using curl_easy_duphandle.\n\nMany of the options you set in libcurl are \"strings\", pointers to data terminated with a zero byte. When you set strings with curl_easy_setopt, libcurl makes its own copy so that they do not need to be kept around in your application after being set[4].\n\nOne of the most basic properties to set in the handle is the URL. You set your preferred URL to transfer with CURLOPT_URL in a manner similar to:\n\nLet's assume for a while that you want to receive data as the URL identifies a remote resource you want to get here. Since you write a sort of application that needs this transfer, I assume that you would like to get the data passed to you directly instead of simply getting it passed to stdout. So, you write your own function that matches this prototype:\n\nlibcurl offers its own default internal callback that takes care of the data if you do not set the callback with CURLOPT_WRITEFUNCTION. It simply outputs the received data to stdout. You can have the default callback write the data to a different file handle by passing a 'FILE *' to a file opened for writing with the CURLOPT_WRITEDATA option.\n\nNow, we need to take a step back and take a deep breath. Here is one of those rare platform-dependent nitpicks. Did you spot it? On some platforms[2], libcurl is not able to operate on file handles opened by the program. Therefore, if you use the default callback and pass in an open file handle with CURLOPT_WRITEDATA, libcurl crashes. You should avoid this to make your program run fine virtually everywhere.\n\nWhen the transfer is complete, the function returns a return code that informs you if it succeeded in its mission or not. If a return code is not enough for you, you can use the CURLOPT_ERRORBUFFER to point libcurl to a buffer of yours where it stores a human readable error message as well.\n\nFor some protocols, downloading a file can involve a complicated process of logging in, setting the transfer mode, changing the current directory and finally transferring the file data. libcurl takes care of all that complication for you. Given simply the URL to a file, libcurl takes care of all the details needed to get the file moved from one machine to another.\n\nThere is one golden rule when these things occur: set the CURLOPT_VERBOSE option to 1. it causes the library to spew out the entire protocol details it sends, some internal info and some received protocol data as well (especially when using FTP). If you are using HTTP, adding the headers in the received output to study is also a clever way to get a better understanding why the server behaves the way it does. Include headers in the normal body output with CURLOPT_HEADER set 1.\n\nlibcurl tries to keep a protocol independent approach to most transfers, thus uploading to a remote FTP site is similar to uploading data to an HTTP server with a PUT request.\n\nWhere bufptr is the pointer to a buffer we fill in with data to upload and sizenitems* is the size of the buffer and therefore also the maximum amount of data we can return to libcurl in this call. The userp pointer is the custom pointer we set to point to a struct of ours to pass private data between the application and the callback.\n\nWhen you call curl_easy_perform this time, it performs all the necessary operations and when it has invoked the upload it calls your supplied callback to get the data to upload. The program should return as much data as possible in every invoke, as that is likely to make the upload perform as fast as possible. The callback should return the number of bytes it wrote in the buffer. Returning 0 signals the end of the upload.\n\nlibcurl also provides options to set various passwords. The username and password as shown embedded in the URL can instead get set with the CURLOPT_USERPWD option. The argument passed to libcurl should be a char * to a string in the format \"user:password\". In a manner like this:\n\nThere is a long time Unix \"standard\" way of storing FTP usernames and passwords, namely in the $HOME/.netrc file (on Windows, libcurl also checks the %USERPROFILE% environment variable if %HOME% is unset, and tries \"_netrc\" as name). The file should be made private so that only the user may read it (see also the \"Security Considerations\" chapter), as it might contain the password in plain text. libcurl has the ability to use this file to figure out what set of username and password to use for a particular host. As an extension to the normal functionality, libcurl also supports this file for non-FTP protocols such as HTTP. To make curl use this file, use the CURLOPT_NETRC option:\n\nThe previous chapter showed how to set username and password for getting URLs that require authentication. When using the HTTP protocol, there are many different ways a client can provide those credentials to the server and you can control which way libcurl uses them. The default HTTP authentication method is called 'Basic', which is sending the name and password in clear-text in the HTTP request, base64-encoded. This is insecure.\n\nWhat if you want to post binary data that also requires you to set the Content-Type: header of the post? Well, binary posts prevent libcurl from being able to do strlen() on the data to figure out the size, so therefore we must tell libcurl the size of the post data. Setting headers in libcurl requests are done in a generic way, by building a list of our own headers and then passing that list to libcurl.\n\nWhile the simple examples above cover the majority of all cases where HTTP POST operations are required, they do not do multi-part formposts. Multi-part formposts were introduced as a better way to post (possibly large) binary data and were first documented in the RFC 1867 (updated in RFC 2388). They are called multi-part because they are built by a chain of parts, each part being a single unit of data. Each part has its own name and contents. You can in fact create and post a multi-part formpost with the regular libcurl POST support described above, but that would require that you build a formpost yourself and provide to libcurl.\n\nTo make that easier, libcurl provides a MIME API consisting in several functions: using those, you can create and fill a multi-part form. Function curl_mime_init creates a multi-part body; you can then append new parts to a multi-part body using curl_mime_addpart.\n\nThere are three possible data sources for a part: memory using curl_mime_data, file using curl_mime_filedata and user-defined data read callback using curl_mime_data_cb. curl_mime_name sets a part's (i.e.: form field) name, while curl_mime_filename fills in the remote filename. With curl_mime_type, you can tell the MIME type of a part, curl_mime_headers allows defining the part's headers. When a multi-part body is no longer needed, you can destroy it using curl_mime_free.\n\nTo post multiple files for a single form field, you must supply each file in a separate part, all with the same field name. Although function curl_mime_subparts implements nested multi-parts, this way of multiple files posting is deprecated by RFC 7578, chapter 4.3.\n\nFor historical and traditional reasons, libcurl has a built-in progress meter that can be switched on and then makes it present a progress meter in your terminal.\n\nThere is basically only one thing to keep in mind when using C++ instead of C when interfacing libcurl:\n\nWhat \"proxy\" means according to Merriam-Webster: \"a person authorized to act for another\" but also \"the agency, function, or office of a deputy who acts as a substitute for another\".\n\nFor HTTP proxies: the fact that the proxy is an HTTP proxy puts certain restrictions on what can actually happen. A requested URL that might not be a HTTP URL is passed to the HTTP proxy to deliver back to libcurl. This happens transparently, and an application may not need to know. I say \"may\", because at times it is important to understand that all operations over an HTTP proxy use the HTTP protocol. For example, you cannot invoke your own custom FTP commands or even proper FTP directory listings.\n\nlibcurl automatically checks and uses a set of environment variables to know what proxies to use for certain protocols. The names of the variables are following an old tradition and are built up as \"[protocol]_proxy\" (note the lower casing). Which makes the variable 'http_proxy' checked for a name of a proxy to use when the input URL is HTTP. Following the same rule, the variable named 'ftp_proxy' is checked for FTP URLs. Again, the proxies are always HTTP proxies, the different names of the variables simply allows different HTTP proxies to be used.\n\nThere are two special environment variables. 'all_proxy' is what sets proxy for any URL in case the protocol specific variable was not set, and 'no_proxy' defines a list of hosts that should not use a proxy even though a variable may say so. If 'no_proxy' is a plain asterisk (\"*\") it matches all hosts.\n\nSSL is for secure point-to-point connections. This involves strong encryption and similar things, which effectively makes it impossible for a proxy to operate as a \"man in between\" which the proxy's task is, as previously discussed. Instead, the only way to have SSL work over an HTTP proxy is to ask the proxy to tunnel everything through without being able to check or fiddle with the traffic.\n\nOpening an SSL connection over an HTTP proxy is therefore a matter of asking the proxy for a straight connection to the target host on a specified port. This is made with the HTTP request CONNECT. (\"please dear proxy, connect me to that remote host\").\n\nBecause of the nature of this operation, where the proxy has no idea what kind of data that is passed in and out through this tunnel, this breaks some of the few advantages that come from using a proxy, such as caching. Many organizations prevent this kind of tunneling to other destination port numbers than 443 (which is the default HTTPS port number).\n\nIn fact, there might even be times when you want to do plain HTTP operations using a tunnel like this, as it then enables you to operate on the remote server instead of asking the proxy to do so. libcurl does not stand in the way for such innovative actions either!\n\nNetscape first came up with this. It is basically a webpage (usually using a .pac extension) with a JavaScript that when executed by the browser with the requested URL as input, returns information to the browser on how to connect to the URL. The returned information might be \"DIRECT\" (which means no proxy should be used), \"PROXY host:port\" (to tell the browser where the proxy for this particular URL is) or \"SOCKS host:port\" (to direct the browser to a SOCKS proxy).\n\nlibcurl has no means to interpret or evaluate JavaScript and thus it does not support this. If you get yourself in a position where you face this nasty invention, the following advice have been mentioned and used in the past:\n\nEach easy handle attempts to keep the last few connections alive for a while in case they are to be used again. You can set the size of this \"cache\" with the CURLOPT_MAXCONNECTS option. Default is 5. There is rarely any point in changing this value, and if you think of changing this it is often just a matter of thinking again.\n\nSending custom commands to an FTP server means that you need to send the commands exactly as the FTP server expects them (RFC 959 is a good guide here), and you can only use commands that work on the control-connection alone. All kinds of commands that require data interchange and thus need a data-connection must be left to libcurl's own judgment. Also be aware that libcurl does its best to change directory to the target directory before doing any transfer, so if you change directory (with CWD or similar) you might confuse libcurl and then it might not attempt to transfer the file in the correct remote directory.\n\nThe custom FTP commands are issued to the server in the same order they are added to the list, and if a command gets an error code returned back from the server, no more commands are issued and libcurl bails out with an error code (CURLE_QUOTE_ERROR). Note that if you use CURLOPT_QUOTE to send commands before a transfer, no transfer actually takes place when a quote command has failed.\n\nIn the HTTP sense, a cookie is a name with an associated value. A server sends the name and value to the client, and expects it to get sent back on every subsequent request to the server that matches the particular conditions set. The conditions include that the domain name and path match and that the cookie has not become too old.\n\nOne way to do this, is to save all headers you receive in a plain file and when you make a request, you tell libcurl to read the previous headers to figure out which cookies to use. Set the header file to read cookies from with CURLOPT_COOKIEFILE.\n\nThe CURLOPT_COOKIEFILE option also automatically enables the cookie parser in libcurl. Until the cookie parser is enabled, libcurl does not parse or understand incoming cookies and they are just be ignored. However, when the parser is enabled the cookies are understood and the cookies are kept in memory and used properly in subsequent requests when the same handle is used. Many times this is enough, and you may not have to save the cookies to disk at all. Note that the file you specify to CURLOPT_COOKIEFILE does not have to exist to enable the parser, so a common way to just enable the parser and not read any cookies is to use the name of a file you know does not exist.\n\nPerhaps the most advanced cookie operation libcurl offers, is saving the entire internal cookie state back into a Netscape/Mozilla formatted cookie file. We call that the cookie-jar. When you set a filename with CURLOPT_COOKIEJAR, that filename is created and all received cookies get stored in it when curl_easy_cleanup is called. This enables cookies to get passed on properly between multiple handles without any information getting lost.\n\nlibcurl can either connect to the server a second time or tell the server to connect back to it. The first option is the default and it is also what works best for all the people behind firewalls, NATs or IP-masquerading setups. libcurl then tells the server to open up a new port and wait for a second connection. This is by default attempted with EPSV first, and if that does not work it tries PASV instead. (EPSV is an extension to the original FTP spec and does not exist nor work on all FTP servers.)\n\nIn some cases, you want to have the server connect back to you for the second connection. This might be when the server is perhaps behind a firewall or something and only allows connections on a single port. libcurl then informs the remote server which IP address and port number to connect to. This is made with the CURLOPT_FTPPORT option. If you set it to \"-\", libcurl uses your system's \"default IP address\". If you want to use a particular IP, you can set the full IP address, a hostname to resolve to an IP address or even a local network interface name that libcurl gets the IP address from.\n\nTo build such a message, you prepare the nth-level multi-part and then include it as a source to the parent multi-part using function curl_mime_subparts. Once it has been bound to its parent multi-part, a nth-level multi-part belongs to it and should not be freed explicitly.\n\nEmail messages data is not supposed to be non-ASCII and line length is limited: fortunately, some transfer encodings are defined by the standards to support the transmission of such incompatible data. Function curl_mime_encoder tells a part that its source data must be encoded before being sent. It also generates the corresponding header for that part. If the part data you want to send is already encoded in such a scheme, do not use this function (this would over-encode it), but explicitly set the corresponding part header.\n\nThe easy interface as described in detail in this document is a synchronous interface that transfers one file at a time and does not return until it is done.\n\nYou create the easy handles you want, one for each concurrent transfer, and you set all the options just like you learned above, and then you create a multi handle with curl_multi_init and add all those easy handles to that multi handle with curl_multi_add_handle.\n\nThe best usage of this interface is when you do a select() on all possible file descriptors or sockets to know when to call libcurl again. This also makes it easy for you to wait and respond to actions on your own application's sockets/handles. You figure out what to select() for by using curl_multi_fdset, that fills in a set of fd_set variables for you with the particular file descriptors libcurl uses for the moment.\n\nWhen a transfer within the multi stack has finished, the counter of running transfers (as filled in by curl_multi_perform) decreases. When the number reaches zero, all transfers are done.\n\nWhen you add easy handles to a multi handle, these easy handles automatically share a lot of the data that otherwise would be kept on a per-easy handle basis when the easy interface is used."
    }
]