[
    {
        "link": "https://people.csail.mit.edu/hubert/pyaudio/docs",
        "document": "PyAudio provides Python bindings for PortAudio, the cross-platform audio I/O library. With PyAudio, you can easily use Python to play and record audio on a variety of platforms.\n\nTo use PyAudio, first instantiate PyAudio using (1), which acquires system resources for PortAudio. To record or play audio, open a stream on the desired device with the desired audio parameters using (2). This sets up a to play or record audio. Play audio by writing audio data to the stream using , or read audio data from the stream using . (3) Note that in “blocking mode”, each or blocks until all frames have been played/recorded. An alternative approach is “callback mode”, described below, in which PyAudio invokes a user-defined function to process recorded audio or generate output audio. Use to close the stream. (4) Finally, terminate the PortAudio session and release system resources using . (5)\n\n# If len(data) is less than requested frame_count, PyAudio automatically # assumes the stream is finished, and the stream stops. In callback mode, PyAudio will call a user-defined callback function (1) whenever it needs new audio data to play and/or when new recorded audio data becomes available. PyAudio calls the callback function in a separate thread. The callback function must have the following signature . It must return a tuple containing frames of audio data to output (for output streams) and a flag signifying whether there are more expected frames to play or record. (For input-only streams, the audio data portion of the return value is ignored.) The audio stream starts processing once the stream is opened (3), which will call the callback function repeatedly until that function returns or , or until either or is called. Note that if the callback returns fewer frames than the argument (2), the stream automatically closes after those frames are played. To keep the stream active, the main thread must remain alive, e.g., by sleeping (4). In the example above, once the entire wavefile is read, will eventually return fewer than the requested frames. The stream will stop, and the while loop (4) will end."
    },
    {
        "link": "https://people.csail.mit.edu/hubert/pyaudio",
        "document": "PyAudio is distributed under the MIT License.\n\nThis library was originally inspired by:\n\nPyAudio 0.2.14 pre-compiled wheels for Microsoft Windows (32-bit and 64-bit) are now available foron PyPI . Install using pip, as described below PyAudio 0.2.14 is a new release that fixes a build issue. Thanks to Bhanu Victor DiCara for the help.\n\nAlso, PyAudio 0.2.14 pre-compiled wheels for Microsoft Windows (32-bit and 64-bit) are now available for Python 3.12 on PyPI. Install using pip, as described below.\n• Fixed installation on Apple silicon. Thanks to Michael Parque for help with updating and testing!\n• is now , which should not affect existing code, as directly accessing the module-level class has always been unsupported. Use instead.\n• and methods are deprecated. Use the and properties, respectively, instead.\n• Sean Zimmermann for help with modernizing the Microsoft Windows build process\n• Jason Hihn for suggesting the use paFramesPerBufferUnspecified as the default frames per buffer\n• Eiichi Takamori for the patch that uses the system's preferred encoding\n• Matěj Cepl for the patches that skip unit tests that require audio hardware\n• Vasily Zakharov for fixes and suggestions to the documentation\n• Artur Janowiec for fixes to the website documentation\n\nPyAudio 0.2.11 is a new release with a bug fix related to memory management. PyAudio 0.2.11 is a new release with a bug fix related to memory management. Many thanks to both Blaise Potard and Matthias Schaff for discovering the issue and for their patches! Thanks as well to Timothy Port for helping to correct a docstring. PyAudio 0.2.10 is a new release with bug fixes related to the Python GIL. It also introduces a few automated unit tests. PyAudio 0.2.10 is a new release with bug fixes related to the Python GIL. It also introduces a few automated unit tests. Great thanks to Michael Graczyk for discovering the GIL-related issues and for submitting a patch! PyAudio PyAudio 0.2.9 is a new release with bug fixes related to overflow error handling and IOError exception arguments. PyAudio installation is better streamlined. Install PyAudio on most platforms using pip.PyAudio 0.2.9 is a new release with bug fixes related to overflow error handling and IOError exception arguments. Many thanks to Tony Jacobson for discovering and helping with the overflow error. Thanks also to Sami Liedes for reporting the IOError exception issue! PyAudio 0.2.8 is a new release with bug fixes related to the Python GIL and device name encoding. PyAudio 0.2.8 is a new release with bug fixes related to the Python GIL and device name encoding. Many thanks to Jason Roehm for discovering and patching the threading-related issue! Many thanks to Sebastian Audet, who wrote PyAudio 0.2.7 is a new release with: Many thanks to Sebastian Audet, who wrote instructions for building PyAudio using Microsoft Visual Studio!PyAudio 0.2.7 is a new release with: Thanks again to Bastian Bechtold for his help converting the documentation for use with Sphinx! In addition, thanks to John K. Luebs for the callback fix. PyAudio 0.2.6 is a new release with: PyAudio 0.2.6 is a new release with: Many thanks to Bastian Bechtold and Bob Jamison for their contributions! Without their patches and Bastian's careful review, this release would still be far away. Also, great thanks to Danilo J. S. Bellini for reporting bugs. Note: As of this update, PyAudio is compatible with Python 2.6, Python 2.7, and Python 3.2. For Python installations older than 2.6, use PyAudio 0.2.4. This web page has been This web page has been This web page has been This web page has been PyAudio 0.2.4 has been uploaded to Debian (sid). A huge thanks to Felipe Sateler for sponsoring the package! PyAudio 0.2.4 is a maintenance release—there are no new features or bug fixes. The binary packages now include PortAudio-v19 (r1535). The source for PyAudio is now in PyAudio 0.2.3 fixes several outstanding bugs ( PyAudio 0.2.0 now works with both Python 2.4 and Python 2.5. Additionally, PyAudio features support for PortAudio's Mac OS X Host API Specific Stream Info extensions (e.g., for channel mapping)—see examples below. The new binary installers include an updated version of PortAudio-v19 (r1368). PyAudio 0.1.0 is released. This web page has been translated to French—thanks to the efforts of Cyril Danilevski!This web page has been translated to Russian—thanks to the efforts of Oleg Meister!This web page has been translated to German—thanks to the efforts of Olga Babenko!This web page has been translated to Belorussian—thanks to the efforts of Jason Fragoso!PyAudio 0.2.4 has been uploaded to Debian (sid). A huge thanks to Felipe Sateler for sponsoring the package!PyAudio 0.2.4 is a maintenance release—there are no new features or bug fixes. The binary packages now include PortAudio-v19 (r1535). The source for PyAudio is now in git (previously subversion).PyAudio 0.2.3 fixes several outstanding bugs ( thanks to all who have sent patches); see the CHANGELOG for details. PyAudio 0.2.3 provides bindings for PortAudio-v19 (r1395) and now includes binary distributions for Python 2.4, 2.5, and 2.6.PyAudio 0.2.0 now works with both Python 2.4 and Python 2.5. Additionally, PyAudio features support for PortAudio's Mac OS X Host API Specific Stream Info extensions (e.g., for)—see examples below. The new binary installers include an updated version of PortAudio-v19 (r1368).PyAudio 0.1.0 is released.\n\nPyAudio 0.2.13 is a new release focused on refactoring, maintenance, and cleanup. See the CHANGELOG in the source distribution for details. Notable changes include:PyAudio 0.2.12 pre-compiled wheels for Microsoft Windows (32-bit and 64-bit) are now available foron PyPI . Install using pip, as described below PyAudio 0.2.12 is a new release with many updates. See the CHANGELOG in the source distribution for details. Notably, the PyAudio build process for Microsoft Windows is streamlined, using the native toolchain. Special thanks to:\n\nThe current version is PyAudio v0.2.14 and supports Python version 3.8+. Install PyAudio using pip on most platforms. For PyAudio versions prior to v0.2.9, PyAudio distributed installation binaries, which are archived here.\n\nThe PyAudio source distribution contains a set of demos. Here's a selection from that set:\n\n\"\"\"PyAudio Example: Record a few seconds of audio and save to a wave file.\"\"\" \"\"\"PyAudio Example: full-duplex wire between input and output.\"\"\" # If len(data) is less than requested frame_count, PyAudio automatically # assumes the stream is finished, and the stream stops. \"\"\"PyAudio Example: Audio wire between input and output. Callback version.\"\"\" # right channel audio --> right speaker; no left channel 'Could not find PaMacCoreStreamInfo. Ensure you are running on macOS.'\n\nPyAudio is distributed under the MIT License: PyAudio is distributed under the Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
    },
    {
        "link": "https://pypi.org/project/PyAudio",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://github.com/jleb/pyaudio/blob/master/src/pyaudio.py",
        "document": ""
    },
    {
        "link": "http://lira.epac.to:8080/doc/python3-pyaudio/docs",
        "document": "PyAudio provides Python bindings for PortAudio, the cross-platform audio I/O library. With PyAudio, you can easily use Python to play and record audio on a variety of platforms.\n\nTo use PyAudio, first instantiate PyAudio using (1), which acquires system resources for PortAudio. To record or play audio, open a stream on the desired device with the desired audio parameters using (2). This sets up a to play or record audio. Play audio by writing audio data to the stream using , or read audio data from the stream using . (3) Note that in “blocking mode”, each or blocks until all frames have been played/recorded. An alternative approach is “callback mode”, described below, in which PyAudio invokes a user-defined function to process recorded audio or generate output audio. Use to close the stream. (4) Finally, terminate the PortAudio session and release system resources using . (5)\n\n# If len(data) is less than requested frame_count, PyAudio automatically # assumes the stream is finished, and the stream stops. In callback mode, PyAudio will call a user-defined callback function (1) whenever it needs new audio data to play and/or when new recorded audio data becomes available. PyAudio calls the callback function in a separate thread. The callback function must have the following signature . It must return a tuple containing frames of audio data to output (for output streams) and a flag signifying whether there are more expected frames to play or record. (For input-only streams, the audio data portion of the return value is ignored.) The audio stream starts processing once the stream is opened (3), which will call the callback function repeatedly until that function returns or , or until either or is called. Note that if the callback returns fewer frames than the argument (2), the stream automatically closes after those frames are played. To keep the stream active, the main thread must remain alive, e.g., by sleeping (4). In the example above, once the entire wavefile is read, will eventually return fewer than the requested frames. The stream will stop, and the while loop (4) will end."
    },
    {
        "link": "https://github.com/rany2/edge-tts/issues/187",
        "document": "I've been trying to implement fast audio streaming for 6 days now, but I just can't do it. Indeed, without the stream method, the save function of edge-tts takes about 1-2s to generate depending the text, which is too long.\n\nIn my code bellow, the audio is indeed played instantly, regardless of text size, but there are artefacts between chunks, like tiny spaces.\n\nDo you know how to do audio streaming correctly please ? Thank you!\n\nif name == \"main\":\n\n # Run the asyncio event loop\n\n asyncio.run(stream_tts(TEXT, VOICE))"
    },
    {
        "link": "https://github.com/KoljaB/RealtimeTTS",
        "document": "Easy to use, low-latency text-to-speech library for realtime applications\n\nRealtimeTTS is a state-of-the-art text-to-speech (TTS) library designed for real-time applications. It stands out in its ability to convert text streams fast into high-quality auditory output with minimal latency.\n• Robust and Reliable:\n• switches to alternative engines in case of disruptions guaranteeing consistent performance and reliability, which is vital for critical and professional use cases\n\nCheck the FAQ page for answers to a lot of questions around the usage of RealtimeTTS.\n\nThe documentation for RealtimeTTS is available in the following languages:\n\nLet me know if you need any adjustments or additional languages!\n\nSupport for more kokoro languages. Full installation for also japanese and chinese languages (see updated test file):\n\nIf you run into problems with japanese (Error \"module 'jieba' has no attribute 'lcut'\") try:\n\nAdded ParlerEngine. Needs flash attention, then barely runs fast enough for realtime inference on a 4090.\n• Text-to-Speech Engines\n• PiperEngine 🏠: Very fast TTS system, also runs on Raspberry Pi\n• Sentence Boundary Detection\n• NLTK Sentence Tokenizer: Natural Language Toolkit's sentence tokenizer for straightforward text-to-speech tasks in English or when simplicity is preferred.\n• Stanza Sentence Tokenizer: Stanza sentence tokenizer for working with multilingual text or when higher accuracy and performance are required.\n\nBy using \"industry standard\" components RealtimeTTS offers a reliable, high-end technological foundation for developing advanced voice solutions.\n\nThe RealtimeTTS library provides installation options for various dependencies for your use case. Here are the different ways you can install RealtimeTTS depending on your needs:\n\nTo install RealtimeTTS with support for all TTS engines:\n\nInstall only required dependencies using these options:\n• all: Complete package with all engines\n\nFor those who want to perform a full installation within a virtual environment, follow these steps:\n\nDifferent engines supported by RealtimeTTS have unique requirements. Ensure you fulfill these requirements based on the engine you choose.\n\nThe works out of the box with your system's built-in TTS capabilities. No additional setup is needed.\n\nThe works out of the box using Google Translate's text-to-speech API. No additional setup is needed.\n\nTo use the :\n\nTo use the , you will need:\n• Microsoft Azure Text-to-Speech API key (provided via AzureEngine constructor parameter \"speech_key\" or in the environment variable AZURE_SPEECH_KEY)\n\nMake sure you have these credentials available and correctly configured when initializing the .\n\nFor the , you need:\n• Elevenlabs API key (provided via ElevenlabsEngine constructor parameter \"api_key\" or in the environment variable ELEVENLABS_API_KEY)\n• installed on your system (essential for streaming mpeg audio, Elevenlabs only delivers mpeg).\n• \n• Piper must be installed independently from RealtimeTTS. Follow the Piper installation tutorial for Windows.\n• \n• Provide the correct paths to the Piper executable and voice model files when initializing .\n• Ensure that the is correctly set up with the model and configuration files.\n\nDownloads a neural TTS model first. In most cases it be fast enough for Realtime using GPU synthesis. Needs around 4-5 GB VRAM.\n• to clone a voice submit the filename of a wave file containing the source voice as \"voice\" parameter to the CoquiEngine constructor\n• voice cloning works best with a 22050 Hz mono 16bit WAV file containing a short (~5-30 sec) sample\n\nOn most systems GPU support will be needed to run fast enough for realtime, otherwise you will experience stuttering.\n\nOr you can feed generators and character iterators for real-time streaming:\n\nThe test subdirectory contains a set of scripts to help you evaluate and understand the capabilities of the RealtimeTTS library.\n\nNote that most of the tests still rely on the \"old\" OpenAI API (<1.0.0). Usage of the new OpenAI API is demonstrated in openai_1.0_test.py.\n• \n• Description: A \"hello world\" styled demonstration of the library's simplest usage.\n• \n• Description: A comprehensive demonstration showcasing most of the features provided by the library.\n• \n• Description: Real-time translations into six different languages.\n• \n• Description: Wake word activated and voice based user interface to the OpenAI API.\n• \n• Description: Simple demonstration of how to integrate the library with large language models (LLMs).\n• \n• Description: Showcases the callbacks and lets you check the latency times in a real-world application environment.\n• \n• Reason: The library depends on the GitHub library \"TTS\" from coqui, which requires Python versions in this range.\n• stream2sentence: to split the incoming text stream into sentences\n• Shoutout to Idiap Research Institute for maintaining a fork of coqui tts.\n\nWhen you initialize the class, you have various options to customize its behavior. Here are the available parameters:\n• Description: The core engine(s) used for text-to-audio synthesis.\n• If a single engine instance is provided, it will be used for all synthesis tasks.\n• If a list of engine instances is provided, the system uses them for fallback mechanisms.\n• Description: A callback function triggered when the text streaming process ends.\n• Use Case: Cleaning up resources or signaling that the text-to-speech pipeline has completed processing.\n• Description: A callback function triggered when the audio playback starts.\n• Use Case: Logging playback events or updating UI elements to reflect active audio playback.\n• Description: A callback function triggered when the audio playback ends.\n• Use Case: Resetting UI elements or initiating follow-up actions after playback.\n• Description: A callback function triggered for every character processed during synthesis.\n• Use Case: Real-time visualization of character-level processing, useful for debugging or monitoring.\n• Description: A callback function triggered when a word starts playing. The callback receives an object (an instance of ) that includes:\n• word: the text of the word,\n• start_time: the time offset (in seconds) when the word starts,\n• end_time: the time offset (in seconds) when the word ends.\n• Use Case: Useful for tracking word-level progress or highlighting spoken words in a display.\n• Notes: Currently supported only by AzureEngine and KokoroEngine (for English voices, both American and British). Other engines don't provide word-level timings.\n• Description: The index of the audio output device to use for playback.\n• How It Works: The system will use the device corresponding to this index for audio playback. If , the system's default audio output device is used.\n• Obtaining Device Indices: Use PyAudio's device query methods to retrieve available indices.\n• Description: Specifies the tokenizer used for splitting text into sentences or fragments.\n• Custom Tokenization: You can provide a custom tokenizer by setting the parameter instead.\n• Description: Language code for sentence splitting.\n• Examples: for English, for German, for French.\n• Ensure that the tokenizer supports the specified language.\n• Description: Controls whether audio playback is muted.\n• If , audio playback is disabled and no audio stream will be opened, allowing the synthesis to generate audio data without playing it.\n• Use Case: Useful for scenarios where you want to save audio to a file or process audio chunks without hearing the output.\n• Description: Defines the number of audio frames processed per buffer by PyAudio.\n• If set to , PyAudio selects a default value based on the platform and hardware.\n• Description: Specifies the size of audio chunks (in bytes) to play out to the stream.\n• Behavior:\n• If , the chunk size is determined dynamically based on or a default internal value.\n• Smaller chunk sizes can reduce latency but may increase overhead.\n• Description: Sets the logging level for the internal logger.\n\nThese methods are responsible for executing the text-to-audio synthesis and playing the audio stream. The difference is that is a blocking function, while runs in a separate thread, allowing other operations to proceed.\n• Description: When set to , the method will prioritize speed, generating and playing sentence fragments faster. This is useful for applications where latency matters.\n• Description: When set to , applies the fast sentence fragment processing to all sentences, not just the first one.\n• Description: When set to , allows yielding multiple sentence fragments instead of just a single one.\n• Description: Specifies the time in seconds for the buffering threshold, which impacts the smoothness and continuity of audio playback.\n• How it Works: Before synthesizing a new sentence, the system checks if there is more audio material left in the buffer than the time specified by . If so, it retrieves another sentence from the text generator, assuming that it can fetch and synthesize this new sentence within the time window provided by the remaining audio in the buffer. This process allows the text-to-speech engine to have more context for better synthesis, enhancing the user experience. A higher value ensures that there's more pre-buffered audio, reducing the likelihood of silence or gaps during playback. If you experience breaks or pauses, consider increasing this value.\n• Description: Sets the minimum character length to consider a string as a sentence to be synthesized. This affects how text chunks are processed and played.\n• Description: The minimum number of characters required for the first sentence fragment before yielding.\n• Description: When enabled, logs the text chunks as they are synthesized into audio. Helpful for auditing and debugging.\n• Description: If True, reset the generated text before processing.\n• Description: If set, save the audio to the specified WAV file.\n• Description: A callback function that gets called after a single sentence fragment was synthesized.\n• Description: A callback function that gets called before a single sentence fragment gets synthesized.\n• Description: Callback function that gets called when a single audio chunk is ready.\n• Description: Tokenizer to use for sentence splitting. Currently supports \"nltk\" and \"stanza\".\n• Description: A custom function that tokenizes sentences from the input text. You can provide your own lightweight tokenizer if you are unhappy with nltk and stanza. It should take text as a string and return split sentences as a list of strings.\n• Description: Language to use for sentence splitting.\n• Description: The number of characters used to establish context for sentence boundary detection. A larger context improves the accuracy of detecting sentence boundaries.\n• Description: Additional context size for looking ahead when detecting sentence boundaries.\n• Description: If True, disables audio playback via local speakers. Useful when you want to synthesize to a file or process audio chunks without playing them.\n• Description: A string of characters that are considered sentence delimiters.\n• Description: The number of words after which the first sentence fragment is forced to be yielded.\n\nThese steps are recommended for those who require better performance and have a compatible NVIDIA GPU.\n\nTo use a torch with support via CUDA please follow these steps:\n• Install NVIDIA CUDA Toolkit: For example, to install Toolkit 12.X, please or to install Toolkit 11.8, please\n• For example, to install cuDNN 8.7.0 for CUDA 11.x please\n• You can download an installer for your OS from the ffmpeg Website.\n• To upgrade your PyTorch installation to enable GPU support with CUDA, follow these instructions based on your specific CUDA version. This is useful if you wish to enhance the performance of RealtimeSTT with CUDA capabilities.\n• To update PyTorch and Torchaudio to support CUDA 11.8, use the following commands:\n• To update PyTorch and Torchaudio to support CUDA 12.X, execute the following: Replace with the version of PyTorch that matches your system and requirements.\n• Fix for to resolve compatibility issues: If you run into library compatibility issues, try setting these libraries to fixed versions:\n\nHuge shoutout to the team behind Coqui AI - especially the brilliant Eren Gölge - for being the first to give us local high-quality synthesis with real-time speed and even a clonable voice!\n\nThank you Pierre Nicolas Durette for giving us a free tts to use without GPU using Google Translate with his gtts python library.\n\nContributions are always welcome (e.g. PR to add a new engine).\n\nWhile the source of this library is open-source, the usage of many of the engines it depends on is not: External engine providers often restrict commercial use in their free plans. This means the engines can be used for noncommercial projects, but commercial usage requires a paid plan.\n• Commercial Use: Available with every paid plan.\n• Commercial Use: Available from the standard tier upwards.\n• Commercial Use: Allowed under this license.\n• Commercial Use: It's under the MIT license, so it should be theoretically possible. Some caution might be necessary since it utilizes undocumented Google Translate speech functionality.\n• License: please read OpenAI Terms of Use\n\nDisclaimer: This is a summarization of the licenses as understood at the time of writing. It is not legal advice. Please read and respect the licenses of the different engine providers if you plan to use them in a project."
    },
    {
        "link": "https://swiftpackageindex.com/brewusinc/Edge-TTS",
        "document": "If nothing happens, you may not have the app installed. Download the Swift Package Index Playgrounds app and try again."
    },
    {
        "link": "https://medium.com/@pshanarat/tts-text-to-speech-streaming-with-chunk-encoding-a-technique-used-by-openai-4852458f7270",
        "document": "In voice-based applications, streaming speech from the text in real-time is a common use case. This technique is used in various applications, including voice-based customer service, medical history narration, chatbots, and more.\n\nImplementing this in real-time applications poses significant challenges. In this brief paper, we will discuss a method developed by OpenAI known as chunk-encoding. This method processes chunks of text and plays them using an in-memory temporary file, allowing speech to begin before the entire text is processed by the TTS model. This approach is particularly effective for handling large texts. For more details, please refer to the OpenAI documentation linked here: link.\n\nI have used the same technique for an open-source TTS model gTTS . GTTS is (Google Text-to-Speech), a Python library and CLI tool to interface with Google Translate’s text-to-speech API. Hopefully, it gave better results for large texts.\n\nPlease note that this might not be the most optimized solution and the idea can be valuable for any TTS-related implementation. Also, please note that to run the above code you need to have gTTS and pydub python packages (pip install <package>)installed and ffmpeg must be available in your environment."
    },
    {
        "link": "https://pypi.org/project/realtimetts",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://stackoverflow.com/questions/32373996/pydub-raw-audio-data",
        "document": "I'm using Pydub in Python 3.4 to try to detect the pitch of some audio files.\n\nI have a working pitch detection algorithm (McLeod Pitch Method), which is robust for real-time applications (I even made an Android pitch detection app with it: https://github.com/sevagh/Pitcha).\n\nMy issue is that I'm not getting any meaningful output from the algorithm when I apply it to AudioSegment._data.\n\nIf I play the same wav file from my speakers, record it from my microphone and apply the algorithm on the raw microphone capture (signed 16-bit little endian PCM, 44100Hz, mono), I get the correct pitch.\n\nDoes AudioSegment._data not return what I'm expecting?"
    },
    {
        "link": "https://geeksforgeeks.org/working-with-wav-files-in-python-using-pydub",
        "document": "Audio files are a widespread means of transferring information. So let’s see how to work with audio files using Python. Python provides a module called pydub to work with audio files. pydub is a Python library to work with only .wav files. By using this library we can play, split, merge, edit our .wav audio files.\n\nThis module does not come built-in with Python. To install it type the below command in the terminal.\n\nFollowing are some functionalities that can be performed by pydub:\n• We can get certain information of file like length channels.\n• Merging two or more audio files.\n\nLet’s see the code for some functionalities of pydub library:\n\n1) Playing Audio File: This is done using play() method.\n\n2) Knowing about .wav file: for this we will use attributes of audio file object.\n\n3) Increasing/Decreasing volume of the file: By using ‘+’ and ‘-‘ operator.\n\n4) Merging files: This is done using ‘+’ operator.\n\n5) Exporting files: This is done using export() method."
    },
    {
        "link": "https://github.com/jiaaro/pydub/blob/master/API.markdown",
        "document": "This document is a work in progress.\n\nIf you're looking for some functionality in particular, it's a good idea to take a look at the source code. Core functionality is mostly in – a number of methods are in the module, and added to via the effect registration process (the decorator function)\n\nobjects are immutable, and support a number of operators.\n\nAny operations that combine multiple objects in any way will first ensure that they have the same number of channels, frame rate, sample rate, bit depth, etc. When these things do not match, the lower quality sound is modified to match the quality of the higher quality sound so that quality is not lost: mono is converted to stereo, bit depth and frame rate/sample rate are increased as needed. If you do not want this behavior, you may explicitly reduce the number of channels, bits, etc using the appropriate methods.\n\nOpen an audio file as an instance and return it. there are also a number of wrappers provided for convenience, but you should probably just use this directly.\n\nThe first argument is the path (as a string) of the file to read, or a file handle to read from.\n• | example: | default: autodetected Format of the output file. Supports and natively, requires ffmpeg for all other formats. files require 3 additional keyword arguments, , , and , denoted below with: only. This extra info is required because raw audio files do not have headers to include this info in the file itself like wav files do.\n• | example: only — Use for 8-bit audio for 16-bit (CD quality) and for 32-bit. It’s the number of bytes per sample.\n• | example: only — for mono, for stereo.\n• | example: only — Also known as sample rate, common values are (44.1kHz - CD audio), and (48kHz - DVD audio)\n• | example: | default: Offset (in seconds) to start loading the audio file. If , the audio will start loading from the beginning.\n• | example: | default: Number of seconds to be loaded. If , full audio will be loaded.\n\nWrite the object to a file – returns a file handle of the output file (you don't have to do anything with it, though).\n\nThe first argument is the location (as a string) to write the output, or a file handle to write to. If you do not pass an output file or path, a temporary file is generated.\n• | example: | default: Format of the output file. Supports and natively, requires ffmpeg for all other formats.\n• | example: For formats that may contain content encoded with different codecs, you can specify the codec you'd like the encoder to use. For example, the \"ogg\" format is often used with the \"libvorbis\" codec. (requires ffmpeg)\n• | example: For compressed formats, you can pass the bitrate you'd like the encoder to use (requires ffmpeg). Each codec accepts different bitrate arguments so take a look at the ffmpeg documentation for details (bitrate usually shown as , or ).\n• | example: Allows you to supply media info tags for the encoder (requires ffmpeg). Not all formats can receive tags (mp3 can).\n• | example: Pass additional command line parameters to the ffmpeg call. These are added to the end of the call (in the output file section).\n• | example: | default: Set the ID3v2 version used by ffmpeg to add tags to the output file. If you want Windows Exlorer to display tags, use here (source).\n• | example: Allows you to supply a cover image (path to the image file). Currently, only MP3 files allow this keyword argument. Cover image must be a jpeg, png, bmp, or tiff file.\n\nThis is useful for aggregation loops:\n\nCreates a silent audiosegment, which can be used as a placeholder, spacer, or as a canvas to overlay other sounds on top of.\n• | example: | default: (1 second) Length of the silent , in milliseconds\n• | example | default: (11.025 kHz) Frame rate (i.e., sample rate) of the silent in Hz\n\nCreates a multi-channel audiosegment out of multiple mono audiosegments (two or more). Each mono audiosegment passed in should be exactly the same length, down to the frame count.\n\nReturns the loudness of the in dBFS (db relative to the maximum possible loudness). A Square wave at maximum amplitude will be roughly 0 dBFS (maximum loudness), whereas a Sine Wave at maximum amplitude will be roughly -3 dBFS.\n\nNumber of channels in this audio segment (1 means mono, 2 means stereo)\n\nNumber of bytes in each sample (1 means 8 bit, 2 means 16 bit, etc). CD Audio is 16 bit, (sample width of 2 bytes).\n\nCD Audio has a 44.1kHz sample rate, which means will be (same as sample rate, see ). Common values are (CD), (DVD), , , and .\n\nNumber of bytes for each \"frame\". A frame contains a sample for each channel (so for stereo you have 2 samples per frame, which are played simultaneously). is equal to . For CD Audio it'll be (2 channels times 2 bytes per sample).\n\nA measure of loudness. Used to compute dBFS, which is what you should use in most cases. Loudness is logarithmic (rms is not), which makes dB a much more natural scale.\n\nThe highest amplitude of any sample in the . Useful for things like normalization (which is provided in ).\n\nThe highest amplitude of any sample in the , in dBFS (relative to the highest possible amplitude value). Useful for things like normalization (which is provided in ).\n\nReturns the duration of the in seconds ( returns milliseconds). This is provided for convenience; it calls internally.\n\nThe raw audio data of the AudioSegment. Useful for interacting with other audio libraries or weird APIs that want audio data in the form of a bytestring. Also comes in handy if you’re implementing effects or other direct signal processing.\n\nYou probably don’t need this, but if you do… you’ll know.\n\nReturns the number of frames in the . Optionally you may pass in a keywork argument to retrieve the number of frames in that number of milliseconds of audio in the (useful for slicing, etc).\n• | example: | default: (entire duration of ) When specified, method returns number of frames in X milliseconds of the\n\nReturns a new , created by appending another to this one (i.e., adding it to the end), Optionally using a crossfade. is used internally when adding objects together with the operator.\n\nBy default a 100ms (0.1 second) crossfade is used to eliminate pops and crackles.\n• | example: | default: (entire duration of ) When specified, method returns number of frames in X milliseconds of the\n\nOverlays an onto this one. In the resulting they will play simultaneously. If the overlaid is longer than this one, the result will be truncated (so the end of the overlaid sound will be cut off). The result is always the same length as this even when using the , and keyword arguments.\n\nSince objects are immutable, you can get around this by overlaying the shorter sound on the longer one, or by creating a silent with the appropriate duration, and overlaying both sounds on to that one.\n• | example: | default: (beginning of this ) The overlaid will not begin until X milliseconds have passed\n• | example: | default: (entire duration of ) The overlaid will repeat (starting at ) until the end of this\n• | example: | default: (entire duration of ) The overlaid will repeat X times (starting at ) but will still be truncated to the length of this\n• | example: | default: (no change in volume during overlay) Change the original audio by this many dB while overlaying audio. This can be used to make the original audio quieter while the overlaid audio plays.\n\nChange the amplitude (generally, loudness) of the . Gain is specified in dB. This method is used internally by the operator.\n\nA more general (more flexible) fade method. You may specify and , or one of the two along with duration (e.g., and ).\n• | example: | default: (0dB, no change) Resulting change at the end of the fade. means fade will be be from 0dB (no change) to -6dB, and everything after the fade will be -6dB.\n• | example: | default: (0dB, no change) Change at the beginning of the fade. means fade (and all audio before it) will be be at -6dB will fade up to 0dB – the rest of the audio after the fade will be at 0dB (i.e., unchanged).\n• | example: | NO DEFAULT Position to begin fading (in milliseconds). means fade will begin after 5.5 seconds.\n• | example: | NO DEFAULT The overlaid will repeat X times (starting at ) but will still be truncated to the length of this\n• | example: | NO DEFAULT You can use or with duration, instead of specifying both - provided as a convenience.\n\nFade out (to silent) the end of this . Uses internally.\n• | example: | NO DEFAULT How long (in milliseconds) the fade should last. Passed directly to internally\n\nFade in (from silent) the beginning of this . Uses internally.\n• | example: | NO DEFAULT How long (in milliseconds) the fade should last. Passed directly to internally\n\nMake a copy of this that plays backwards. Useful for Pink Floyd, screwing around, and some audio processing algorithms.\n\nCreates an equivalent version of this with the specified sample width (in bytes). Increasing this value does not generally cause a reduction in quality. Reducing it definitely does cause a loss in quality. Higher Sample width means more dynamic range.\n\nCreates an equivalent version of this with the specified frame rate (in Hz). Increasing this value does not generally cause a reduction in quality. Reducing it definitely does cause a loss in quality. Higher frame rate means larger frequency response (higher frequencies can be represented).\n\nCreates an equivalent version of this with the specified number of channels (1 is Mono, 2 is Stereo). Converting from mono to stereo does not cause any audible change. Converting from stereo to mono may result in loss of quality (but only if the left and right chanels differ).\n\nSplits a stereo into two, one for each channel (Left/Right). Returns a list with the new objects with the left channel at index 0 and the right channel at index 1.\n\nApply gain to the left and right channel of a stereo . If the is mono, it will be converted to stereo before applying the gain.\n\nBoth gain arguments are specified in dB.\n\nTakes one positional argument, pan amount, which should be between -1.0 (100% left) and +1.0 (100% right)\n\nWhen pan_amount == 0.0 the left/right balance is not changed.\n\nPanning does not alter the perceived loundness, but since loudness is decreasing on one side, the other side needs to get louder to compensate. When panned hard left, the left channel will be 3dB louder and the right channel will be silent (and vice versa).\n\nReturns the raw audio data as an array of (numeric) samples. Note: if the audio has multiple channels, the samples for each channel will be serialized – for example, stereo audio would look like .\n\nThis method is mainly for use in implementing effects, and other processing.\n\nnote that when using numpy or scipy you will need to convert back to an array before you spawn:\n\nHere's how to convert to a numpy float32 array:\n\nAnd how to convert it back to an AudioSegment:\n\nReturns a value between -1.0 and 1.0 representing the DC offset of a channel. This is calculated using and normalizing the result by samples max value.\n• | example: | default: Selects left (1) or right (2) channel to calculate DC offset. If segment is mono, this value is ignored.\n\nRemoves DC offset from channel(s). This is done by using , so watch out for overflows.\n• | example: | default: None Selects left (1) or right (2) channel remove DC offset. If value if None, removes from all available channels. If segment is mono, this value is ignored.\n• | example: | default: None Offset to be removed from channel(s). Calculates offset if it's None. Offset values must be between -1.0 and 1.0.\n\nCollection of DSP effects that are implemented by objects.\n\nMake a copy of this and inverts the phase of the signal. Can generate anti-phase waves for noise suppression or cancellation.\n\nVarious functions for finding/manipulating silence in AudioSegments. For creating silent AudioSegments, see AudioSegment.silent().\n\nReturns a list of all silent sections [start, end] in milliseconds of audio_segment. Inverse of detect_nonsilent(). Can be very slow since it has to iterate over the whole segment.\n• | example: | default: 1000 The minimum length for silent sections in milliseconds. If it is greater than the length of the audio segment an empty list will be returned.\n• | example: | default: -16 The upper bound for how quiet is silent in dBFS.\n• | example: | default: 1 Size of the step for checking for silence in milliseconds. Smaller is more precise. Must be a positive whole number.\n\nReturns a list of all silent sections [start, end] in milliseconds of audio_segment. Inverse of detect_silence() and has all the same arguments. Can be very slow since it has to iterate over the whole segment.\n• | example: | default: 1000 The minimum length for silent sections in milliseconds. If it is greater than the length of the audio segment an empty list will be returned.\n• | example: | default: -16 The upper bound for how quiet is silent in dBFS.\n• | example: | default: 1 Size of the step for checking for silence in milliseconds. Smaller is more precise. Must be a positive whole number.\n\nReturns list of audio segments from splitting audio_segment on silent sections.\n• | example: | default: 1000 The minimum length for silent sections in milliseconds. If it is greater than the length of the audio segment an empty list will be returned.\n• | example: | default: -16 The upper bound for how quiet is silent in dBFS.\n• | example: | default: 1 Size of the step for checking for silence in milliseconds. Smaller is more precise. Must be a positive whole number.\n• ~ example: True | default: 100 How much silence to keep in ms or a bool. leave some silence at the beginning and end of the chunks. Keeps the sound from sounding like it is abruptly cut off. When the length of the silence is less than the keep_silence duration it is split evenly between the preceding and following non-silent segments. If True is specified, all the silence is kept, if False none is kept.\n\nReturns the millisecond/index that the leading silence ends. If there is no end it will return the length of the audio_segment.\n• | example: | default: -50 The upper bound for how quiet is silent in dBFS.\n• | example: | default: 10 Size of the step for checking for silence in milliseconds. Smaller is more precise. Must be a positive whole number."
    },
    {
        "link": "https://discuss.streamlit.io/t/normalize-audio-using-pydub-and-load-it-on-streamlit-audio-player/13574",
        "document": "These cookies are necessary for the website to function and cannot be switched off. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms.\n\nThese cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us understand how visitors move around the site and which pages are most frequently visited.\n\nThese cookies are used to record your choices and settings, maintain your preferences over time and recognize you when you return to our website. These cookies help us to personalize our content for you and remember your preferences.\n\nThese cookies may be deployed to our site by our advertising partners to build a profile of your interest and provide you with content that is relevant to you, including showing you relevant ads on other websites."
    },
    {
        "link": "https://stackoverflow.com/questions/45526996/split-audio-files-using-silence-detection",
        "document": "I found pydub to be easiest tool to do this kind of audio manipulation in simple ways and with compact code.\n\nYou can install pydub with\n\nYou may need to install ffmpeg/avlib if needed. See this link for more details.\n\nHere is a snippet that does what you asked. Some of the parameters such as and may need some tuning to match your requirements. Overall, I was able to split files, although I had to try different values for .\n\nIf your original audio is stereo (2-channel), your chunks will also be stereo. You can check the original audio like this:"
    }
]