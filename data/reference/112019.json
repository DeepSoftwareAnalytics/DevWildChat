[
    {
        "link": "https://blog.keras.io/building-autoencoders-in-keras.html",
        "document": "This post was written in early 2016. It is therefore badly outdated.\n\nIn this tutorial, we will answer some common questions about autoencoders, and we will cover code examples of the following models:\n\nNote: all code examples have been updated to the Keras 2.0 API on March 14, 2017. You will need Keras version 2.0.0 or higher to run them.\n\n\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n\n1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n\n2) Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n\n3) Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n\nTo build an autoencoder, you need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a \"loss\" function). The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent. It's simple! And you don't even need to understand any of these words to start using autoencoders in practice.\n\nAre they good at data compression?\n\nUsually, not really. In picture compression for instance, it is pretty difficult to train an autoencoder that does a better job than a basic algorithm like JPEG, and typically the only way it can be achieved is by restricting yourself to a very specific type of picture (e.g. one for which JPEG does not do a good job). The fact that autoencoders are data-specific makes them generally impractical for real-world data compression problems: you can only use them on data that is similar to what they were trained on, and making them more general thus requires lots of training data. But future advances might change this, who knows.\n\nWhat are autoencoders good for?\n\nThey are rarely used in practical applications. In 2012 they briefly found an application in greedy layer-wise pretraining for deep convolutional neural networks [1], but this quickly fell out of fashion as we started realizing that better random weight initialization schemes were sufficient for training deep networks from scratch. In 2014, batch normalization [2] started allowing for even deeper networks, and from late 2015 we could train arbitrarily deep networks from scratch using residual learning [3].\n\nToday two interesting practical applications of autoencoders are data denoising (which we feature later in this post), and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n\nFor 2D visualization specifically, t-SNE (pronounced \"tee-snee\") is probably the best algorithm around, but it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32-dimensional), then use t-SNE for mapping the compressed data to a 2D plane. Note that a nice parametric implementation of t-SNE in Keras was developed by Kyle McDonald and is available on Github. Otherwise scikit-learn also has a simple and practical implementation.\n\nSo what's the big deal with autoencoders?\n\nTheir main claim to fame comes from being featured in many introductory machine learning classes available online. As a result, a lot of newcomers to the field absolutely love autoencoders and can't get enough of them. This is the reason why this tutorial exists!\n\nOtherwise, one reason why they have attracted so much research and attention is because they have long been thought to be a potential avenue for solving the problem of unsupervised learning, i.e. the learning of useful representations without the need for labels. Then again, autoencoders are not a true unsupervised learning technique (which would imply a different learning process altogether), they are a self-supervised technique, a specific instance of supervised learning where the targets are generated from the input data. In order to get self-supervised models to learn interesting features, you have to come up with an interesting synthetic target and loss function, and that's where problems arise: merely learning to reconstruct your input in minute detail might not be the right choice here. At this point there is significant evidence that focusing on the reconstruction of a picture at the pixel level, for instance, is not conductive to learning interesting, abstract features of the kind that label-supervized learning induces (where targets are fairly abstract concepts \"invented\" by humans such as \"dog\", \"car\"...). In fact, one may argue that the best features in this regard are those that are the worst at exact input reconstruction while achieving high performance on the main task that you are interested in (classification, localization, etc).\n\nIn self-supervized learning applied to vision, a potentially fruitful alternative to autoencoder-style input reconstruction is the use of toy tasks such as jigsaw puzzle solving, or detail-context matching (being able to match high-resolution but small patches of pictures with low-resolution versions of the pictures they are extracted from). The following paper investigates jigsaw puzzle solving and makes for a very interesting read: Noroozi and Favaro (2016) Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. Such tasks are providing the model with built-in assumptions about the input data which are missing in traditional autoencoders, such as \"visual macro-structure matters more than pixel-level details\".\n\nWe'll start simple, with a single fully-connected neural layer as encoder and as decoder:\n\nAs well as the decoder model:\n\nNow let's train our autoencoder to reconstruct MNIST digits.\n\nFirst, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adam optimizer:\n\nLet's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images).\n\nWe will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n\nNow let's train our autoencoder for 50 epochs:\n\nAfter 50 epochs, the autoencoder seems to reach a stable train/validation loss value of about . We can try to visualize the reconstructed inputs and the encoded representations. We will use Matplotlib.\n\nHere's what we get. The top row is the original digits, and the bottom row is the reconstructed digits. We are losing quite a bit of detail with this basic approach.\n\nIn the previous example, the representations were only constrained by the size of the hidden layer (32). In such a situation, what typically happens is that the hidden layer is learning an approximation of PCA (principal component analysis). But another way to constrain the representations to be compact is to add a sparsity contraint on the activity of the hidden representations, so fewer units would \"fire\" at a given time. In Keras, this can be done by adding an to our layer:\n\nLet's train this model for 100 epochs (with the added regularization the model is less likely to overfit and can be trained longer). The models ends with a train loss of and test loss of . The difference between the two is mostly due to the regularization term being added to the loss during training (worth about 0.01).\n\nHere's a visualization of our new results:\n\nThey look pretty similar to the previous model, the only significant difference being the sparsity of the encoded representations. yields a value (over our 10,000 test images), whereas with the previous model the same quantity was . So our new model yields encoded representations that are twice sparser.\n\nWe do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, such as:\n\nAfter 100 epochs, it reaches a train and validation loss of ~0.08, a bit better than our previous models. Our reconstructed digits look a bit better too:\n\nSince our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n\nLet's implement one. The encoder will consist in a stack of and layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of and layers.\n\nTo train it, we will use the original MNIST digits with shape , and we will just normalize pixel values between 0 and 1.\n\nLet's train this model for 50 epochs. For the sake of demonstrating how to visualize the results of a model during training, we will be using the TensorFlow backend and the TensorBoard callback.\n\nFirst, let's open up a terminal and start a TensorBoard server that will read logs stored at .\n\nThen let's train our model. In the list we pass an instance of the callback. After every epoch, this callback will write logs to , which can be read by our TensorBoard server.\n\nThis allows us to monitor training in the TensorBoard web interface (by navighating to ):\n\nThe model converges to a loss of 0.094, significantly better than our previous models (this is in large part due to the higher entropic capacity of the encoded representation, 128 dimensions vs. 32 previously). Let's take a look at the reconstructed digits:\n\nWe can also have a look at the 128-dimensional encoded representations. These representations are 8x4x4, so we reshape them to 4x32 in order to be able to display them as grayscale images.\n\nLet's put our convolutional autoencoder to work on an image denoising problem. It's simple: we will train the autoencoder to map noisy digits images to clean digits images.\n\nHere's how we will generate synthetic noisy digits: we just apply a gaussian noise matrix and clip the images between 0 and 1.\n\nHere's what the noisy digits look like:\n\nIf you squint you can still recognize them, but barely. Can our autoencoder learn to recover the original digits? Let's find out.\n\nCompared to the previous convolutional autoencoder, in order to improve the quality of the reconstructed, we'll use a slightly different model with more filters per layer:\n\nNow let's take a look at the results. Top, the noisy digits fed to the network, and bottom, the digits are reconstructed by the network.\n\nIt seems to work pretty well. If you scale this process to a bigger convnet, you can start building document denoising or audio denoising models. Kaggle has an interesting dataset to get you started.\n\nIf you inputs are sequences, rather than vectors or 2D images, then you may want to use as encoder and decoder a type of model that can capture temporal structure, such as a LSTM. To build a LSTM-based autoencoder, first use a LSTM encoder to turn your input sequences into a single vector that contains information about the entire sequence, then repeat this vector times (where is the number of timesteps in the output sequence), and run a LSTM decoder to turn this constant sequence into the target sequence.\n\nWe won't be demonstrating that one on any specific dataset. We will just put a code example here for future reference for the reader!\n\nVariational autoencoders are a slightly more modern and interesting take on autoencoding.\n\nWhat is a variational autoencoder, you ask? It's a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \"generative model\".\n\nFirst, an encoder network turns the input samples into two parameters in a latent space, which we will note and . Then, we randomly sample similar points from the latent normal distribution that is assumed to generate the data, via , where is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.\n\nThe parameters of the model are trained via two loss functions: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data.\n\nBecause a VAE is a more complex example, we have made the code available on Github as a standalone script. Here we will review step by step how the model is created.\n\nFirst, here's our encoder network, mapping inputs to our latent distribution parameters:\n\nWe can use these parameters to sample new similar points from the latent space:\n\nFinally, we can map these sampled latent points back to reconstructed inputs:\n\nWhat we've done so far allows us to instantiate 3 models:\n• an encoder mapping inputs to the latent space\n• a generator that can take points on the latent space and will output the corresponding reconstructed samples.\n\nWe train the model using the end-to-end model, with a custom loss function: the sum of a reconstruction term, and the KL divergence regularization term.\n\nWe train our VAE on MNIST digits:\n\nBecause our latent space is two-dimensional, there are a few cool visualizations that can be done at this point. One is to look at the neighborhoods of different classes on the latent 2D plane:\n\nEach of these colored clusters is a type of digit. Close clusters are digits that are structurally similar (i.e. digits that share information in the latent space).\n\nBecause the VAE is a generative model, we can also use it to generate new digits! Here we will scan the latent plane, sampling latent points at regular intervals, and generating the corresponding digit for each of these points. This gives us a visualization of the latent manifold that \"generates\" the MNIST digits.\n\nThat's it! If you have suggestions for more topics to be covered in this post (or in future posts), you can contact me on Twitter at @fchollet.\n\n[1] Why does unsupervised pre-training help deep learning?"
    },
    {
        "link": "https://machinelearningmastery.com/autoencoder-for-classification",
        "document": "Autoencoder is a type of neural network that can be used to learn a compressed representation of raw data.\n\nAn autoencoder is composed of an encoder and a decoder sub-models. The encoder compresses the input and the decoder attempts to recreate the input from the compressed version provided by the encoder. After training, the encoder model is saved and the decoder is discarded.\n\nThe encoder can then be used as a data preparation technique to perform feature extraction on raw data that can be used to train a different machine learning model.\n\nIn this tutorial, you will discover how to develop and evaluate an autoencoder for classification predictive modeling.\n\nAfter completing this tutorial, you will know:\n• An autoencoder is a neural network model that can be used to learn a compressed representation of raw data.\n• How to train an autoencoder model on a training dataset and save just the encoder part of the model.\n• How to use the encoder as a data preparation step when training a machine learning model.\n\nThis tutorial is divided into three parts; they are:\n\nAn autoencoder is a neural network model that seeks to learn a compressed representation of an input.\n\nThey are an unsupervised learning method, although technically, they are trained using supervised learning methods, referred to as self-supervised.\n\nAutoencoders are typically trained as part of a broader model that attempts to recreate the input.\n\nThe design of the autoencoder model purposefully makes this challenging by restricting the architecture to a bottleneck at the midpoint of the model, from which the reconstruction of the input data is performed.\n\nThere are many types of autoencoders, and their use varies, but perhaps the more common use is as a learned or automatic feature extraction model.\n\nIn this case, once the model is fit, the reconstruction aspect of the model can be discarded and the model up to the point of the bottleneck can be used. The output of the model at the bottleneck is a fixed-length vector that provides a compressed representation of the input data.\n\nInput data from the domain can then be provided to the model and the output of the model at the bottleneck can be used as a feature vector in a supervised learning model, for visualization, or more generally for dimensionality reduction.\n\nNext, let’s explore how we might develop an autoencoder for feature extraction on a classification predictive modeling problem.\n\nIn this section, we will develop an autoencoder to learn a compressed representation of the input features for a classification predictive modeling problem.\n\nWe will use the make_classification() scikit-learn function to define a synthetic binary (2-class) classification task with 100 input features (columns) and 1,000 examples (rows). Importantly, we will define the problem in such a way that most of the input variables are redundant (90 of the 100 or 90 percent), allowing the autoencoder later to learn a useful compressed representation.\n\nThe example below defines the dataset and summarizes its shape.\n\nRunning the example defines the dataset and prints the shape of the arrays, confirming the number of rows and columns.\n\nNext, we will develop a Multilayer Perceptron (MLP) autoencoder model.\n\nThe model will take all of the input columns, then output the same values. It will learn to recreate the input pattern exactly.\n\nThe autoencoder consists of two parts: the encoder and the decoder. The encoder learns how to interpret the input and compress it to an internal representation defined by the bottleneck layer. The decoder takes the output of the encoder (the bottleneck layer) and attempts to recreate the input.\n\nOnce the autoencoder is trained, the decoder is discarded and we only keep the encoder and use it to compress examples of input to vectors output by the bottleneck layer.\n\nIn this first autoencoder, we won’t compress the input at all and will use a bottleneck layer the same size as the input. This should be an easy problem that the model will learn nearly perfectly and is intended to confirm our model is implemented correctly.\n\nWe will define the model using the functional API; if this is new to you, I recommend this tutorial:\n• How to Use the Keras Functional API for Deep Learning\n\nPrior to defining and fitting the model, we will split the data into train and test sets and scale the input data by normalizing the values to the range 0-1, a good practice with MLPs.\n\nWe will define the encoder to have two hidden layers, the first with two times the number of inputs (e.g. 200) and the second with the same number of inputs (100), followed by the bottleneck layer with the same number of inputs as the dataset (100).\n\nTo ensure the model learns well, we will use batch normalization and leaky ReLU activation.\n\nThe decoder will be defined with a similar structure, although in reverse.\n\nIt will have two hidden layers, the first with the number of inputs in the dataset (e.g. 100) and the second with double the number of inputs (e.g. 200). The output layer will have the same number of nodes as there are columns in the input data and will use a linear activation function to output numeric values.\n\nThe model will be fit using the efficient Adam version of stochastic gradient descent and minimizes the mean squared error, given that reconstruction is a type of multi-output regression problem.\n\nWe can plot the layers in the autoencoder model to get a feeling for how the data flows through the model.\n\nThe image below shows a plot of the autoencoder.\n\nNext, we can train the model to reproduce the input and keep track of the performance of the model on the hold-out test set.\n\nAfter training, we can plot the learning curves for the train and test sets to confirm the model learned the reconstruction problem well.\n\nFinally, we can save the encoder model for use later, if desired.\n\nAs part of saving the encoder, we will also plot the encoder model to get a feeling for the shape of the output of the bottleneck layer, e.g. a 100 element vector.\n\nAn example of this plot is provided below.\n\nTying this all together, the complete example of an autoencoder for reconstructing the input data for a classification dataset without any compression in the bottleneck layer is listed below.\n\nRunning the example fits the model and reports loss on the train and test sets along the way.\n\nNote: if you have problems creating the plots of the model, you can comment out the import and call the plot_model() function.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we see that loss gets low, but does not go to zero (as we might have expected) with no compression in the bottleneck layer. Perhaps further tuning the model architecture or learning hyperparameters is required.\n\nA plot of the learning curves is created showing that the model achieves a good fit in reconstructing the input, which holds steady throughout training, not overfitting.\n\nSo far, so good. We know how to develop an autoencoder without compression.\n\nNext, let’s change the configuration of the model so that the bottleneck layer has half the number of nodes (e.g. 50).\n\nTying this together, the complete example is listed below.\n\nRunning the example fits the model and reports loss on the train and test sets along the way.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we see that loss gets similarly low as the above example without compression, suggesting that perhaps the model performs just as well with a bottleneck half the size.\n\nA plot of the learning curves is created, again showing that the model achieves a good fit in reconstructing the input, which holds steady throughout training, not overfitting.\n\nThe trained encoder is saved to the file “encoder.h5” that we can load and use later.\n\nNext, let’s explore how we might use the trained encoder model.\n\nIn this section, we will use the trained encoder from the autoencoder to compress input data and train a different predictive model.\n\nFirst, let’s establish a baseline in performance on this problem. This is important as if the performance of a model is not improved by the compressed encoding, then the compressed encoding does not add value to the project and should not be used.\n\nWe can train a logistic regression model on the training dataset directly and evaluate the performance of the model on the holdout test set.\n\nThe complete example is listed below.\n\nRunning the example fits a logistic regression model on the training dataset and evaluates it on the test set.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we can see that the model achieves a classification accuracy of about 89.3 percent.\n\nWe would hope and expect that a logistic regression model fit on an encoded version of the input to achieve better accuracy for the encoding to be considered useful.\n\nWe can update the example to first encode the data using the encoder model trained in the previous section.\n\nFirst, we can load the trained encoder model from the file.\n\nWe can then use the encoder to transform the raw input data (e.g. 100 columns) into bottleneck vectors (e.g. 50 element vectors).\n\nThis process can be applied to the train and test datasets.\n\nWe can then use this encoded data to train and evaluate the logistic regression model, as before.\n\nTying this together, the complete example is listed below.\n\nRunning the example first encodes the dataset using the encoder, then fits a logistic regression model on the training dataset and evaluates it on the test set.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we can see that the model achieves a classification accuracy of about 93.9 percent.\n\nThis is a better classification accuracy than the same model evaluated on the raw dataset, suggesting that the encoding is helpful for our chosen model and test harness.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• How to Use the Keras Functional API for Deep Learning\n• TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras\n\nIn this tutorial, you discovered how to develop and evaluate an autoencoder for classification predictive modeling.\n• An autoencoder is a neural network model that can be used to learn a compressed representation of raw data.\n• How to train an autoencoder model on a training dataset and save just the encoder part of the model.\n• How to use the encoder as a data preparation step when training a machine learning model.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://hex.tech/blog/autoencoders-for-feature-selection",
        "document": "Machine learning models work most effectively when they are trained on high-quality data. The quality of this data is determined by the quality of its features.\n\nSo features need to be relevant, informative, and non-redundant for the model to learn effectively. Feature selection is the process of identifying the most essential features in the dataset that lead to the optimal model performance in machine learning. It is critical for improving model performance, avoiding any confusion for models to identify relationships between features, reducing computational complexity, and improving interpretability.\n\nAlthough there are different techniques for feature selection (Filter methods, wrapper methods, and Embedded Methods), one remarkable approach that has gained appreciation in recent years is the use of autoencoders. Autoencoders are a class of artificial neural networks used in tasks like data compression and reconstruction. In this article, we will delve into the world of autoencoders, explore their architecture, and discuss autoencoders as a powerful tool for feature selection.\n\nAutoencoders are neural network models that are trained to reconstruct the input data. They do this by learning a compressed representation of data, called the latent space, and then reconstructing the data from the latent space. For example, if you have an image of a cat, the autoencoder learns to compress the picture into a smaller, more abstract representation, such as a set of numbers, and then reconstruct the picture from this compressed representation.\n\nThe architecture of the autoencoder is the critical aspect of its functionality. It consists of several components that work to compress and reconstruct the input data.\n\nThe input layer is where the original data is fed to the autoencoder. The input layer should have the same number of neurons as the number of features in the data.\n\nThe encoder contains a series of encoding layers (hidden layers) where each layer consists of a set of neurons responsible for compressing the input data into latent space by reducing the dimensionality of data. Neurons in the first encoding layer capture simple, low-level features, while deeper layers capture more abstract and complex features. Each layer of the encoder uses a non-linear activation function to transform its input. The encoder’s final layer, known as the bottleneck layer, has a significantly small number of neurons compared to the input layer. This layer represents the compressed form of input data.\n\nThe bottleneck layer serves as the latent space. It encodes the most salient information from input data in a reduced-dimensional format. The compressed representation is a vital part of the autoencoder that captures the important features while discarding the non-relevant ones.\n\nThe decoder contains a symmetric set of decoding layers that reconstruct the original data from the latent space (encoded form). Similar to encoding layers each decoding layer consists of neurons that gradually expand the dimensionality of data. The last decoding layer produces the output that should closely resemble the original input data.\n\nThe output layer is where the reconstructed data is concluded by the autoencoder. The output layer should have the same number of neurons as the number of features in the input data.\n\nOne of the best applications of the autoencoder is dimensionality reduction, which is the process of reducing the number of features in a dataset while still retaining as much useful information as possible. Training the autoencoder network to compress and decompress the data, significantly reduces the dimensionality of the dataset by dropping the non-relevant features. Autoencoders can also learn non-linear relationships between input features, so can perform non-linear dimensionality reduction. Hence it not only simplifies the data but also serves as a powerful feature selection mechanism.\n\nAutoencoders have a unique approach to feature selection, addressing several challenges, and limitations associated with conventional methods.\n\nTraditional feature selection methods like filter methods, wrapper methods, and embedded methods simplify complex datasets for machine learning models and are simple to understand and implement. However, they come with some set of drawbacks.\n• None Lack of Data Understanding: Filter methods that use some statistical measure for feature selection lack in deeper understanding of data which may lead to the removal of potentially relevant features.\n• None Computational Intensity: Wrapper methods like forward selection or backward elimination require training of the network model multiple times, making them computationally intensive and impractical to use for large datasets.\n• None Overfitting Risks: These methods may lead to overfitting when trying to optimize the model performance, as they may select the features that are beneficial but are not generalizable.\n• None Curse of Dimensionality: Embedded methods, while integrating feature selection into model training, can struggle with high-dimensional data, leading to increased computational complexity.\n• None Manual Feature Engineering: Many traditional methods rely on domain knowledge and require manual feature engineering which makes them less adaptable to diverse datasets.\n\nThe adoption of autoencoders for feature selection arises from several compelling motivations\n• None Automated Feature Extraction: Autoencoders use deep neural network to automatically learn and extract meaningful features from data which eliminate the need for manual feature engineering.\n• None Non-linearity Handling: Autoencoders can capture complex, non-linear relationships from data that traditional methods fail to detect.\n• None Dimensionality Reduction: Autoencoder compresses data into lower dimensional latent space with the help of encoding layers to address the curse of dimensionality.\n• None Data Reconstruction: Autoencoders are designed to reconstruct data accurately. It determines the important features that are necessary to perform the predictive task and retain those features, performing feature selection.\n• None Adaptability: Autoencoders are versatile and can be applied to different types of datasets including images, audio, text, and numerical data which enable them to work on multiple applications.\n• None Transfer Learning: There are various pre-trained autoencoders present that can be fine-tuned for specific tasks which provides a transfer learning advantage in feature selection.\n\nAutoencoders can be used as unsupervised feature selection tools by identifying the latent space representations that are most important for reconstructing the input data. These latent space representations can be used to select a subset of features for training an AI model. This can improve the performance of the AI model by reducing the noise in the data and by focusing on the most essential features.\n\nSteps to Use Autoencoders For Feature Selection\n\nLet’s go through an example of using autoencoders for feature selection. We will be using the publicly available Iris dataset to demonstrate how autoencoders can efficiently select important features for predictive modeling. This dataset consists of the measurement of four features (sepal length, sepal width, petal length, and petal width) of three different species of iris flowers. Each species is labelled properly making it suitable for supervised learning tasks.\n\nYou can see the entire Hex notebook here:\n\nFor this section, you will need some of the core computational libraries for Python: Numpy, Pandas, SKLearn, and Keras. These libraries can be installed through a terminal with Python Package Manager (PIP) as follows:\n\nOnce the libraries are installed, you need to import them into Hex:\n\nBefore diving into feature selection with autoencoders, the next step is to load and preprocess the data to make it ready to feed to the model.\n\nIn this step, you will load the iris dataset and standardize the dataset to ensure all features have a mean of 0 and a standard deviation of 1. Standardization helps autoencoders converge faster during training and ensures that features are on a similar scale.\n\nIn the above code, we first load the data using the method from sklearn. Then we apply the method to bring all the features to the same scale. Finally, for the experiment, we divide the data into a training and testing set.\n\nThe next step is to construct an autoencoder, which is a neural network that will learn to compress and reconstruct the input data. The autoencoder architecture layers are defined below.\n• None Input Layer: Matches the number of features in the dataset.\n• None Decoding layer: Reconstructs the input from the encoded representation.\n\nKeras library of Python provides the implementation of all these layers of autoencoder.\n\nIn the above code, we first define the input dimension and the dimension of the encoder. Then we created three different layers using the module that correspond to the input layer, endcoding layer, and decoding layer. We are using the adam optimization algorithm and mean squared error loss for training the autoencoder. Finally, if you want to check the whole architecture, you can use the method from keras.\n\nAfter constructing the complete neural network model of the autoencoder it’s time to feed the input data to the model and train the model on the iris dataset. Using this process autoencoder network can compress and learn important features from the dataset.\n\nWe call the method from Keras to train the model. This method requires the Independent and dependent variables (training data), number of epochs, batch size, and other optional parameters. Once you run this code, the model will start training for the given number of epochs.\n\nNote: Batch size and number of epochs depend on the dataset that you are using for training.\n\nAfter training the Autoencoder, you can extract the important features that the encoder has found useful. These features represent the compressed version of input data and can be used as selected features.\n\nFinally, you can integrate the selected features with the predictive model (any relevant machine learning algorithm) like logistic regression to evaluate the effectiveness of feature selection using autoencoders.\n\nBy following the above steps you can successfully implement the autoencoder for feature selection on any dataset for the specific problem statement you are dealing with.\n\nThis method allows us to select the best features that highly dominate to target variable and then integrate these features with a machine learning algorithm to build the well generalized model with good performance while reducing dimensionality and computational complexity.\n\nChallenges and Limitations of Using Autoencoders as Feature Selectors\n\nAutoencoders are a powerful tool for feature selection and data compression, but they are not without challenges and limitations. It is important to be aware of these factors to make informed decisions when using autoencoders in solving any machine learning problem statement.\n\nAutoencoders are prone to overfitting, a common problem in deep learning. Overfitting occurs when the autoencoder model learns the training data too well capturing noise and irregularities in data and then is unable to generalize to new data or does not perform on any new data with expected performance. Underfitting occurs when the autoencoder does not learn the training data well enough and is unable to reconstruct the input data accurately.\n\nTo avoid overfitting and underfitting, it is important to carefully choose the above architecture of the autoencoder and to address it using techniques like dropout, early stopping, and regularization techniques.\n\nAutoencoders are also known as black-box models, making it difficult to interpret the significance of selected features. Unlike traditional feature selection methods that provide explicit feature rankings, autoencoders learn a compressed representation of data which can make it difficult to understand which features are the most important.\n\nTo interpret the feature importance from autoencoders use a technique called latent space analysis. It involves looking at the relationships between features in the latent space. Features that are highly correlated in latent space are likely to be important.\n\nAutoencoders can be used to handle high-dimensional data but they can be computationally expensive to train and prone to overfitting. This is because autoencoders need to learn a mapping from high-dimensional input space to low-dimensional latent space.\n\nTo reduce the computational cost of autoencoders use techniques such as batch normalization or convolutional autoencoder for image datasets.\n\nIt is important to note that autoencoders trained on one dataset cannot generalize well to other datasets because autoencoders learn to represent the data in the latent space and the latent space representation may not be transferable to other datasets.\n\nTo improve the generalization performance of autoencoders, try to use the techniques such as data augmentation, and semi-supervised learning.\n\nAutoencoders offer several advantages over traditional feature selection methods, such as their ability to learn non-linear relationships between input features and learn compressed representation of data.\n\nHowever, autoencoder also possesses some limitations like their susceptibility to overfitting and difficulty in determining the feature importance. Despite these autoencoders are a valuable tool for machine learning practitioners. By implementing a generalized architecture and using regularization techniques you can avoid all the limitations, and achieve good results for feature selection and dimensionality reduction.\n\nIf you are keen on autoencoders then we encourage you to research more on feature selection and dimensionality reduction and explore Variational Autoencoder(VAEs) which can be used in a variety of machine learning applications such as medical diagnosis, image classification, and natural language processing."
    },
    {
        "link": "https://machinelearningmastery.com/autoencoder-for-regression",
        "document": "Autoencoder is a type of neural network that can be used to learn a compressed representation of raw data.\n\nAn autoencoder is composed of encoder and a decoder sub-models. The encoder compresses the input and the decoder attempts to recreate the input from the compressed version provided by the encoder. After training, the encoder model is saved and the decoder is discarded.\n\nThe encoder can then be used as a data preparation technique to perform feature extraction on raw data that can be used to train a different machine learning model.\n\nIn this tutorial, you will discover how to develop and evaluate an autoencoder for regression predictive\n\nAfter completing this tutorial, you will know:\n• An autoencoder is a neural network model that can be used to learn a compressed representation of raw data.\n• How to train an autoencoder model on a training dataset and save just the encoder part of the model.\n• How to use the encoder as a data preparation step when training a machine learning model.\n\nThis tutorial is divided into three parts; they are:\n\nAn autoencoder is a neural network model that seeks to learn a compressed representation of an input.\n\nThey are an unsupervised learning method, although technically, they are trained using supervised learning methods, referred to as self-supervised. They are typically trained as part of a broader model that attempts to recreate the input.\n\nThe design of the autoencoder model purposefully makes this challenging by restricting the architecture to a bottleneck at the midpoint of the model, from which the reconstruction of the input data is performed.\n\nThere are many types of autoencoders, and their use varies, but perhaps the more common use is as a learned or automatic feature extraction model.\n\nIn this case, once the model is fit, the reconstruction aspect of the model can be discarded and the model up to the point of the bottleneck can be used. The output of the model at the bottleneck is a fixed length vector that provides a compressed representation of the input data.\n\nInput data from the domain can then be provided to the model and the output of the model at the bottleneck can be used as a feature vector in a supervised learning model, for visualization, or more generally for dimensionality reduction.\n\nNext, let’s explore how we might develop an autoencoder for feature extraction on a regression predictive modeling problem.\n\nIn this section, we will develop an autoencoder to learn a compressed representation of the input features for a regression predictive modeling problem.\n\nWe will use the make_regression() scikit-learn function to define a synthetic regression task with 100 input features (columns) and 1,000 examples (rows). Importantly, we will define the problem in such a way that most of the input variables are redundant (90 of the 100 or 90 percent), allowing the autoencoder later to learn a useful compressed representation.\n\nThe example below defines the dataset and summarizes its shape.\n\nRunning the example defines the dataset and prints the shape of the arrays, confirming the number of rows and columns.\n\nNext, we will develop a Multilayer Perceptron (MLP) autoencoder model.\n\nThe model will take all of the input columns, then output the same values. It will learn to recreate the input pattern exactly.\n\nThe autoencoder consists of two parts: the encoder and the decoder. The encoder learns how to interpret the input and compress it to an internal representation defined by the bottleneck layer. The decoder takes the output of the encoder (the bottleneck layer) and attempts to recreate the input.\n\nOnce the autoencoder is trained, the decode is discarded and we only keep the encoder and use it to compress examples of input to vectors output by the bottleneck layer.\n\nIn this first autoencoder, we won’t compress the input at all and will use a bottleneck layer the same size as the input. This should be an easy problem that the model will learn nearly perfectly and is intended to confirm our model is implemented correctly.\n\nWe will define the model using the functional API. If this is new to you, I recommend this tutorial:\n• How to Use the Keras Functional API for Deep Learning\n\nPrior to defining and fitting the model, we will split the data into train and test sets and scale the input data by normalizing the values to the range 0-1, a good practice with MLPs.\n\nWe will define the encoder to have one hidden layer with the same number of nodes as there are in the input data with batch normalization and ReLU activation.\n\nThis is followed by a bottleneck layer with the same number of nodes as columns in the input data, e.g. no compression.\n\nThe decoder will be defined with the same structure.\n\nIt will have one hidden layer with batch normalization and ReLU activation. The output layer will have the same number of nodes as there are columns in the input data and will use a linear activation function to output numeric values.\n\nThe model will be fit using the efficient Adam version of stochastic gradient descent and minimizes the mean squared error, given that reconstruction is a type of multi-output regression problem.\n\nWe can plot the layers in the autoencoder model to get a feeling for how the data flows through the model.\n\nThe image below shows a plot of the autoencoder.\n\nNext, we can train the model to reproduce the input and keep track of the performance of the model on the holdout test set. The model is trained for 400 epochs and a batch size of 16 examples.\n\nAfter training, we can plot the learning curves for the train and test sets to confirm the model learned the reconstruction problem well.\n\nFinally, we can save the encoder model for use later, if desired.\n\nAs part of saving the encoder, we will also plot the model to get a feeling for the shape of the output of the bottleneck layer, e.g. a 100-element vector.\n\nAn example of this plot is provided below.\n\nTying this all together, the complete example of an autoencoder for reconstructing the input data for a regression dataset without any compression in the bottleneck layer is listed below.\n\nRunning the example fits the model and reports loss on the train and test sets along the way.\n\nNote: if you have problems creating the plots of the model, you can comment out the import and call the plot_model() function.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we see that loss gets low but does not go to zero (as we might have expected) with no compression in the bottleneck layer. Perhaps further tuning the model architecture or learning hyperparameters is required.\n\nA plot of the learning curves is created showing that the model achieves a good fit in reconstructing the input, which holds steady throughout training, not overfitting.\n\nSo far, so good. We know how to develop an autoencoder without compression.\n\nThe trained encoder is saved to the file “encoder.h5” that we can load and use later.\n\nNext, let’s explore how we might use the trained encoder model.\n\nIn this section, we will use the trained encoder model from the autoencoder model to compress input data and train a different predictive model.\n\nFirst, let’s establish a baseline in performance on this problem. This is important as if the performance of a model is not improved by the compressed encoding, then the compressed encoding does not add value to the project and should not be used.\n\nWe can train a support vector regression (SVR) model on the training dataset directly and evaluate the performance of the model on the holdout test set.\n\nAs is good practice, we will scale both the input variables and target variable prior to fitting and evaluating the model.\n\nThe complete example is listed below.\n\nRunning the example fits an SVR model on the training dataset and evaluates it on the test set.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we can see that the model achieves a mean absolute error (MAE) of about 89.\n\nWe would hope and expect that a SVR model fit on an encoded version of the input to achieve lower error for the encoding to be considered useful.\n\nWe can update the example to first encode the data using the encoder model trained in the previous section.\n\nFirst, we can load the trained encoder model from the file.\n\nWe can then use the encoder to transform the raw input data (e.g. 100 columns) into bottleneck vectors (e.g. 100 element vectors).\n\nThis process can be applied to the train and test datasets.\n\nWe can then use this encoded data to train and evaluate the SVR model, as before.\n\nTying this together, the complete example is listed below.\n\nRunning the example first encodes the dataset using the encoder, then fits an SVR model on the training dataset and evaluates it on the test set.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we can see that the model achieves a MAE of about 69.\n\nThis is a better MAE than the same model evaluated on the raw dataset, suggesting that the encoding is helpful for our chosen model and test harness.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• How to Use the Keras Functional API for Deep Learning\n• TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras\n\nIn this tutorial, you discovered how to develop and evaluate an autoencoder for regression predictive modeling.\n• An autoencoder is a neural network model that can be used to learn a compressed representation of raw data.\n• How to train an autoencoder model on a training dataset and save just the encoder part of the model.\n• How to use the encoder as a data preparation step when training a machine learning model.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://datacamp.com/tutorial/autoencoder-keras-tutorial",
        "document": "Learn the fundamentals of neural networks and how to build deep learning models using Keras 2.0 in Python."
    },
    {
        "link": "https://machinelearningmastery.com/autoencoder-for-classification",
        "document": "Autoencoder is a type of neural network that can be used to learn a compressed representation of raw data.\n\nAn autoencoder is composed of an encoder and a decoder sub-models. The encoder compresses the input and the decoder attempts to recreate the input from the compressed version provided by the encoder. After training, the encoder model is saved and the decoder is discarded.\n\nThe encoder can then be used as a data preparation technique to perform feature extraction on raw data that can be used to train a different machine learning model.\n\nIn this tutorial, you will discover how to develop and evaluate an autoencoder for classification predictive modeling.\n\nAfter completing this tutorial, you will know:\n• An autoencoder is a neural network model that can be used to learn a compressed representation of raw data.\n• How to train an autoencoder model on a training dataset and save just the encoder part of the model.\n• How to use the encoder as a data preparation step when training a machine learning model.\n\nThis tutorial is divided into three parts; they are:\n\nAn autoencoder is a neural network model that seeks to learn a compressed representation of an input.\n\nThey are an unsupervised learning method, although technically, they are trained using supervised learning methods, referred to as self-supervised.\n\nAutoencoders are typically trained as part of a broader model that attempts to recreate the input.\n\nThe design of the autoencoder model purposefully makes this challenging by restricting the architecture to a bottleneck at the midpoint of the model, from which the reconstruction of the input data is performed.\n\nThere are many types of autoencoders, and their use varies, but perhaps the more common use is as a learned or automatic feature extraction model.\n\nIn this case, once the model is fit, the reconstruction aspect of the model can be discarded and the model up to the point of the bottleneck can be used. The output of the model at the bottleneck is a fixed-length vector that provides a compressed representation of the input data.\n\nInput data from the domain can then be provided to the model and the output of the model at the bottleneck can be used as a feature vector in a supervised learning model, for visualization, or more generally for dimensionality reduction.\n\nNext, let’s explore how we might develop an autoencoder for feature extraction on a classification predictive modeling problem.\n\nIn this section, we will develop an autoencoder to learn a compressed representation of the input features for a classification predictive modeling problem.\n\nWe will use the make_classification() scikit-learn function to define a synthetic binary (2-class) classification task with 100 input features (columns) and 1,000 examples (rows). Importantly, we will define the problem in such a way that most of the input variables are redundant (90 of the 100 or 90 percent), allowing the autoencoder later to learn a useful compressed representation.\n\nThe example below defines the dataset and summarizes its shape.\n\nRunning the example defines the dataset and prints the shape of the arrays, confirming the number of rows and columns.\n\nNext, we will develop a Multilayer Perceptron (MLP) autoencoder model.\n\nThe model will take all of the input columns, then output the same values. It will learn to recreate the input pattern exactly.\n\nThe autoencoder consists of two parts: the encoder and the decoder. The encoder learns how to interpret the input and compress it to an internal representation defined by the bottleneck layer. The decoder takes the output of the encoder (the bottleneck layer) and attempts to recreate the input.\n\nOnce the autoencoder is trained, the decoder is discarded and we only keep the encoder and use it to compress examples of input to vectors output by the bottleneck layer.\n\nIn this first autoencoder, we won’t compress the input at all and will use a bottleneck layer the same size as the input. This should be an easy problem that the model will learn nearly perfectly and is intended to confirm our model is implemented correctly.\n\nWe will define the model using the functional API; if this is new to you, I recommend this tutorial:\n• How to Use the Keras Functional API for Deep Learning\n\nPrior to defining and fitting the model, we will split the data into train and test sets and scale the input data by normalizing the values to the range 0-1, a good practice with MLPs.\n\nWe will define the encoder to have two hidden layers, the first with two times the number of inputs (e.g. 200) and the second with the same number of inputs (100), followed by the bottleneck layer with the same number of inputs as the dataset (100).\n\nTo ensure the model learns well, we will use batch normalization and leaky ReLU activation.\n\nThe decoder will be defined with a similar structure, although in reverse.\n\nIt will have two hidden layers, the first with the number of inputs in the dataset (e.g. 100) and the second with double the number of inputs (e.g. 200). The output layer will have the same number of nodes as there are columns in the input data and will use a linear activation function to output numeric values.\n\nThe model will be fit using the efficient Adam version of stochastic gradient descent and minimizes the mean squared error, given that reconstruction is a type of multi-output regression problem.\n\nWe can plot the layers in the autoencoder model to get a feeling for how the data flows through the model.\n\nThe image below shows a plot of the autoencoder.\n\nNext, we can train the model to reproduce the input and keep track of the performance of the model on the hold-out test set.\n\nAfter training, we can plot the learning curves for the train and test sets to confirm the model learned the reconstruction problem well.\n\nFinally, we can save the encoder model for use later, if desired.\n\nAs part of saving the encoder, we will also plot the encoder model to get a feeling for the shape of the output of the bottleneck layer, e.g. a 100 element vector.\n\nAn example of this plot is provided below.\n\nTying this all together, the complete example of an autoencoder for reconstructing the input data for a classification dataset without any compression in the bottleneck layer is listed below.\n\nRunning the example fits the model and reports loss on the train and test sets along the way.\n\nNote: if you have problems creating the plots of the model, you can comment out the import and call the plot_model() function.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we see that loss gets low, but does not go to zero (as we might have expected) with no compression in the bottleneck layer. Perhaps further tuning the model architecture or learning hyperparameters is required.\n\nA plot of the learning curves is created showing that the model achieves a good fit in reconstructing the input, which holds steady throughout training, not overfitting.\n\nSo far, so good. We know how to develop an autoencoder without compression.\n\nNext, let’s change the configuration of the model so that the bottleneck layer has half the number of nodes (e.g. 50).\n\nTying this together, the complete example is listed below.\n\nRunning the example fits the model and reports loss on the train and test sets along the way.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we see that loss gets similarly low as the above example without compression, suggesting that perhaps the model performs just as well with a bottleneck half the size.\n\nA plot of the learning curves is created, again showing that the model achieves a good fit in reconstructing the input, which holds steady throughout training, not overfitting.\n\nThe trained encoder is saved to the file “encoder.h5” that we can load and use later.\n\nNext, let’s explore how we might use the trained encoder model.\n\nIn this section, we will use the trained encoder from the autoencoder to compress input data and train a different predictive model.\n\nFirst, let’s establish a baseline in performance on this problem. This is important as if the performance of a model is not improved by the compressed encoding, then the compressed encoding does not add value to the project and should not be used.\n\nWe can train a logistic regression model on the training dataset directly and evaluate the performance of the model on the holdout test set.\n\nThe complete example is listed below.\n\nRunning the example fits a logistic regression model on the training dataset and evaluates it on the test set.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we can see that the model achieves a classification accuracy of about 89.3 percent.\n\nWe would hope and expect that a logistic regression model fit on an encoded version of the input to achieve better accuracy for the encoding to be considered useful.\n\nWe can update the example to first encode the data using the encoder model trained in the previous section.\n\nFirst, we can load the trained encoder model from the file.\n\nWe can then use the encoder to transform the raw input data (e.g. 100 columns) into bottleneck vectors (e.g. 50 element vectors).\n\nThis process can be applied to the train and test datasets.\n\nWe can then use this encoded data to train and evaluate the logistic regression model, as before.\n\nTying this together, the complete example is listed below.\n\nRunning the example first encodes the dataset using the encoder, then fits a logistic regression model on the training dataset and evaluates it on the test set.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nIn this case, we can see that the model achieves a classification accuracy of about 93.9 percent.\n\nThis is a better classification accuracy than the same model evaluated on the raw dataset, suggesting that the encoding is helpful for our chosen model and test harness.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• How to Use the Keras Functional API for Deep Learning\n• TensorFlow 2 Tutorial: Get Started in Deep Learning With tf.keras\n\nIn this tutorial, you discovered how to develop and evaluate an autoencoder for classification predictive modeling.\n• An autoencoder is a neural network model that can be used to learn a compressed representation of raw data.\n• How to train an autoencoder model on a training dataset and save just the encoder part of the model.\n• How to use the encoder as a data preparation step when training a machine learning model.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://stackoverflow.com/questions/53843573/extracting-encoding-decoding-models-from-keras-autoencoder-using-sequential-api",
        "document": "I am training an autoencoder constructed using the Sequential API in Keras. I'd like to create separate models that implement the encoding and decoding functions. I know from examples how to do this with the functional API, but I can't find an example of how it's done with the Sequential API. The following sample code is my starting point:\n\nI realize I can select individual layers using , but I don't know how to associate a new model with a range of such layers. I naively tried the following:\n\nwhich seemingly worked for the encoder part (a valid summary was shown), but the decoder part generated an error:"
    },
    {
        "link": "https://datascience.stackexchange.com/questions/64412/how-to-extract-features-from-the-encoded-layer-of-an-autoencoder",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://blog.keras.io/building-autoencoders-in-keras.html",
        "document": "This post was written in early 2016. It is therefore badly outdated.\n\nIn this tutorial, we will answer some common questions about autoencoders, and we will cover code examples of the following models:\n\nNote: all code examples have been updated to the Keras 2.0 API on March 14, 2017. You will need Keras version 2.0.0 or higher to run them.\n\n\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n\n1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n\n2) Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n\n3) Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n\nTo build an autoencoder, you need three things: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a \"loss\" function). The encoder and decoder will be chosen to be parametric functions (typically neural networks), and to be differentiable with respect to the distance function, so the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent. It's simple! And you don't even need to understand any of these words to start using autoencoders in practice.\n\nAre they good at data compression?\n\nUsually, not really. In picture compression for instance, it is pretty difficult to train an autoencoder that does a better job than a basic algorithm like JPEG, and typically the only way it can be achieved is by restricting yourself to a very specific type of picture (e.g. one for which JPEG does not do a good job). The fact that autoencoders are data-specific makes them generally impractical for real-world data compression problems: you can only use them on data that is similar to what they were trained on, and making them more general thus requires lots of training data. But future advances might change this, who knows.\n\nWhat are autoencoders good for?\n\nThey are rarely used in practical applications. In 2012 they briefly found an application in greedy layer-wise pretraining for deep convolutional neural networks [1], but this quickly fell out of fashion as we started realizing that better random weight initialization schemes were sufficient for training deep networks from scratch. In 2014, batch normalization [2] started allowing for even deeper networks, and from late 2015 we could train arbitrarily deep networks from scratch using residual learning [3].\n\nToday two interesting practical applications of autoencoders are data denoising (which we feature later in this post), and dimensionality reduction for data visualization. With appropriate dimensionality and sparsity constraints, autoencoders can learn data projections that are more interesting than PCA or other basic techniques.\n\nFor 2D visualization specifically, t-SNE (pronounced \"tee-snee\") is probably the best algorithm around, but it typically requires relatively low-dimensional data. So a good strategy for visualizing similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32-dimensional), then use t-SNE for mapping the compressed data to a 2D plane. Note that a nice parametric implementation of t-SNE in Keras was developed by Kyle McDonald and is available on Github. Otherwise scikit-learn also has a simple and practical implementation.\n\nSo what's the big deal with autoencoders?\n\nTheir main claim to fame comes from being featured in many introductory machine learning classes available online. As a result, a lot of newcomers to the field absolutely love autoencoders and can't get enough of them. This is the reason why this tutorial exists!\n\nOtherwise, one reason why they have attracted so much research and attention is because they have long been thought to be a potential avenue for solving the problem of unsupervised learning, i.e. the learning of useful representations without the need for labels. Then again, autoencoders are not a true unsupervised learning technique (which would imply a different learning process altogether), they are a self-supervised technique, a specific instance of supervised learning where the targets are generated from the input data. In order to get self-supervised models to learn interesting features, you have to come up with an interesting synthetic target and loss function, and that's where problems arise: merely learning to reconstruct your input in minute detail might not be the right choice here. At this point there is significant evidence that focusing on the reconstruction of a picture at the pixel level, for instance, is not conductive to learning interesting, abstract features of the kind that label-supervized learning induces (where targets are fairly abstract concepts \"invented\" by humans such as \"dog\", \"car\"...). In fact, one may argue that the best features in this regard are those that are the worst at exact input reconstruction while achieving high performance on the main task that you are interested in (classification, localization, etc).\n\nIn self-supervized learning applied to vision, a potentially fruitful alternative to autoencoder-style input reconstruction is the use of toy tasks such as jigsaw puzzle solving, or detail-context matching (being able to match high-resolution but small patches of pictures with low-resolution versions of the pictures they are extracted from). The following paper investigates jigsaw puzzle solving and makes for a very interesting read: Noroozi and Favaro (2016) Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles. Such tasks are providing the model with built-in assumptions about the input data which are missing in traditional autoencoders, such as \"visual macro-structure matters more than pixel-level details\".\n\nWe'll start simple, with a single fully-connected neural layer as encoder and as decoder:\n\nAs well as the decoder model:\n\nNow let's train our autoencoder to reconstruct MNIST digits.\n\nFirst, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adam optimizer:\n\nLet's prepare our input data. We're using MNIST digits, and we're discarding the labels (since we're only interested in encoding/decoding the input images).\n\nWe will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n\nNow let's train our autoencoder for 50 epochs:\n\nAfter 50 epochs, the autoencoder seems to reach a stable train/validation loss value of about . We can try to visualize the reconstructed inputs and the encoded representations. We will use Matplotlib.\n\nHere's what we get. The top row is the original digits, and the bottom row is the reconstructed digits. We are losing quite a bit of detail with this basic approach.\n\nIn the previous example, the representations were only constrained by the size of the hidden layer (32). In such a situation, what typically happens is that the hidden layer is learning an approximation of PCA (principal component analysis). But another way to constrain the representations to be compact is to add a sparsity contraint on the activity of the hidden representations, so fewer units would \"fire\" at a given time. In Keras, this can be done by adding an to our layer:\n\nLet's train this model for 100 epochs (with the added regularization the model is less likely to overfit and can be trained longer). The models ends with a train loss of and test loss of . The difference between the two is mostly due to the regularization term being added to the loss during training (worth about 0.01).\n\nHere's a visualization of our new results:\n\nThey look pretty similar to the previous model, the only significant difference being the sparsity of the encoded representations. yields a value (over our 10,000 test images), whereas with the previous model the same quantity was . So our new model yields encoded representations that are twice sparser.\n\nWe do not have to limit ourselves to a single layer as encoder or decoder, we could instead use a stack of layers, such as:\n\nAfter 100 epochs, it reaches a train and validation loss of ~0.08, a bit better than our previous models. Our reconstructed digits look a bit better too:\n\nSince our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n\nLet's implement one. The encoder will consist in a stack of and layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of and layers.\n\nTo train it, we will use the original MNIST digits with shape , and we will just normalize pixel values between 0 and 1.\n\nLet's train this model for 50 epochs. For the sake of demonstrating how to visualize the results of a model during training, we will be using the TensorFlow backend and the TensorBoard callback.\n\nFirst, let's open up a terminal and start a TensorBoard server that will read logs stored at .\n\nThen let's train our model. In the list we pass an instance of the callback. After every epoch, this callback will write logs to , which can be read by our TensorBoard server.\n\nThis allows us to monitor training in the TensorBoard web interface (by navighating to ):\n\nThe model converges to a loss of 0.094, significantly better than our previous models (this is in large part due to the higher entropic capacity of the encoded representation, 128 dimensions vs. 32 previously). Let's take a look at the reconstructed digits:\n\nWe can also have a look at the 128-dimensional encoded representations. These representations are 8x4x4, so we reshape them to 4x32 in order to be able to display them as grayscale images.\n\nLet's put our convolutional autoencoder to work on an image denoising problem. It's simple: we will train the autoencoder to map noisy digits images to clean digits images.\n\nHere's how we will generate synthetic noisy digits: we just apply a gaussian noise matrix and clip the images between 0 and 1.\n\nHere's what the noisy digits look like:\n\nIf you squint you can still recognize them, but barely. Can our autoencoder learn to recover the original digits? Let's find out.\n\nCompared to the previous convolutional autoencoder, in order to improve the quality of the reconstructed, we'll use a slightly different model with more filters per layer:\n\nNow let's take a look at the results. Top, the noisy digits fed to the network, and bottom, the digits are reconstructed by the network.\n\nIt seems to work pretty well. If you scale this process to a bigger convnet, you can start building document denoising or audio denoising models. Kaggle has an interesting dataset to get you started.\n\nIf you inputs are sequences, rather than vectors or 2D images, then you may want to use as encoder and decoder a type of model that can capture temporal structure, such as a LSTM. To build a LSTM-based autoencoder, first use a LSTM encoder to turn your input sequences into a single vector that contains information about the entire sequence, then repeat this vector times (where is the number of timesteps in the output sequence), and run a LSTM decoder to turn this constant sequence into the target sequence.\n\nWe won't be demonstrating that one on any specific dataset. We will just put a code example here for future reference for the reader!\n\nVariational autoencoders are a slightly more modern and interesting take on autoencoding.\n\nWhat is a variational autoencoder, you ask? It's a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \"generative model\".\n\nFirst, an encoder network turns the input samples into two parameters in a latent space, which we will note and . Then, we randomly sample similar points from the latent normal distribution that is assumed to generate the data, via , where is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.\n\nThe parameters of the model are trained via two loss functions: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term. You could actually get rid of this latter term entirely, although it does help in learning well-formed latent spaces and reducing overfitting to the training data.\n\nBecause a VAE is a more complex example, we have made the code available on Github as a standalone script. Here we will review step by step how the model is created.\n\nFirst, here's our encoder network, mapping inputs to our latent distribution parameters:\n\nWe can use these parameters to sample new similar points from the latent space:\n\nFinally, we can map these sampled latent points back to reconstructed inputs:\n\nWhat we've done so far allows us to instantiate 3 models:\n• an encoder mapping inputs to the latent space\n• a generator that can take points on the latent space and will output the corresponding reconstructed samples.\n\nWe train the model using the end-to-end model, with a custom loss function: the sum of a reconstruction term, and the KL divergence regularization term.\n\nWe train our VAE on MNIST digits:\n\nBecause our latent space is two-dimensional, there are a few cool visualizations that can be done at this point. One is to look at the neighborhoods of different classes on the latent 2D plane:\n\nEach of these colored clusters is a type of digit. Close clusters are digits that are structurally similar (i.e. digits that share information in the latent space).\n\nBecause the VAE is a generative model, we can also use it to generate new digits! Here we will scan the latent plane, sampling latent points at regular intervals, and generating the corresponding digit for each of these points. This gives us a visualization of the latent manifold that \"generates\" the MNIST digits.\n\nThat's it! If you have suggestions for more topics to be covered in this post (or in future posts), you can contact me on Twitter at @fchollet.\n\n[1] Why does unsupervised pre-training help deep learning?"
    },
    {
        "link": "https://stackoverflow.com/questions/52271644/extract-encoder-and-decoder-from-trained-autoencoder",
        "document": "I want to divide the autoencoder learning and applying into two parts following https://blog.keras.io/building-autoencoders-in-keras.html and using the fashion-mnist data for testing purposes:\n• Load the images, do the fitting that may take some hours or days and use a callback to save the best autoencoder model. That process can be some weeks before the following part.\n• Use this best model (manually selected by filename) and plot original image, the encoded representation made by the encoder of the autoencoder and the prediction using the decoder of the autoencoder. I have problems (see second step) to extract the encoder and decoder layers from the trained and saved autoencoder.\n\nFor step one I have the very simple network as follows:\n\nSo I train the model and save it by . In my real example I save it with a callback so a workaround by saving the encoder and decoder does not seem a real solution. Later, I load the images (not shown) and do the predictions like\n\nand get a shape of .\n\nSo lets go to step 2 where I have my problems. I load the model using\n\nand the encoder looks the same as the original in step one what makes me think the extraction has worked well:\n\nBut I also get the warning\n\nthat I understand in a kind of way but do not know how important it is. Then I load images again (not shown) and use the encoder\n\nbut shape is not right with .\n\nSo, my extraction for the encoder did not work since the dimensions are not correct. I even have less success extracting the decoder (from the saved autoencoder) since I cannot use and tried stuff like but it did not work.\n\nSo, my general question is how to extract parts of loaded models."
    }
]