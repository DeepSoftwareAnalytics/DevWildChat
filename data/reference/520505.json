[
    {
        "link": "https://docs.python.org/3/library/pickle.html",
        "document": "The module implements binary protocols for serializing and de-serializing a Python object structure. “Pickling” is the process whereby a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse operation, whereby a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy. Pickling (and unpickling) is alternatively known as “serialization”, “marshalling,” or “flattening”; however, to avoid confusion, the terms used here are “pickling” and “unpickling”.\n\nThe module is not secure. Only unpickle data you trust. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Never unpickle data that could have come from an untrusted source, or that could have been tampered with. Consider signing data with if you need to ensure that it has not been tampered with. Safer serialization formats such as may be more appropriate if you are processing untrusted data. See Comparison with json.\n\nThe data format used by is Python-specific. This has the advantage that there are no restrictions imposed by external standards such as JSON (which can’t represent pointer sharing); however it means that non-Python programs may not be able to reconstruct pickled Python objects. By default, the data format uses a relatively compact binary representation. If you need optimal size characteristics, you can efficiently compress pickled data. The module contains tools for analyzing data streams generated by . source code has extensive comments about opcodes used by pickle protocols. There are currently 6 different protocols which can be used for pickling. The higher the protocol used, the more recent the version of Python needed to read the pickle produced.\n• None Protocol version 0 is the original “human-readable” protocol and is backwards compatible with earlier versions of Python.\n• None Protocol version 1 is an old binary format which is also compatible with earlier versions of Python.\n• None Protocol version 2 was introduced in Python 2.3. It provides much more efficient pickling of new-style classes. Refer to PEP 307 for information about improvements brought by protocol 2.\n• None Protocol version 3 was added in Python 3.0. It has explicit support for objects and cannot be unpickled by Python 2.x. This was the default protocol in Python 3.0–3.7.\n• None Protocol version 4 was added in Python 3.4. It adds support for very large objects, pickling more kinds of objects, and some data format optimizations. It is the default protocol starting with Python 3.8. Refer to PEP 3154 for information about improvements brought by protocol 4.\n• None Protocol version 5 was added in Python 3.8. It adds support for out-of-band data and speedup for in-band data. Refer to PEP 574 for information about improvements brought by protocol 5. Serialization is a more primitive notion than persistence; although reads and writes file objects, it does not handle the issue of naming persistent objects, nor the (even more complicated) issue of concurrent access to persistent objects. The module can transform a complex object into a byte stream and it can transform the byte stream into an object with the same internal structure. Perhaps the most obvious thing to do with these byte streams is to write them onto a file, but it is also conceivable to send them across a network or store them in a database. The module provides a simple interface to pickle and unpickle objects on DBM-style database files.\n\nWhat can be pickled and unpickled?¶ The following types can be pickled:\n• None tuples, lists, sets, and dictionaries containing only picklable objects;\n• None functions (built-in and user-defined) accessible from the top level of a module (using , not );\n• None classes accessible from the top level of a module;\n• None instances of such classes whose the result of calling is picklable (see section Pickling Class Instances for details). Attempts to pickle unpicklable objects will raise the exception; when this happens, an unspecified number of bytes may have already been written to the underlying file. Trying to pickle a highly recursive data structure may exceed the maximum recursion depth, a will be raised in this case. You can carefully raise this limit with . Note that functions (built-in and user-defined) are pickled by fully qualified name, not by value. This means that only the function name is pickled, along with the name of the containing module and classes. Neither the function’s code, nor any of its function attributes are pickled. Thus the defining module must be importable in the unpickling environment, and the module must contain the named object, otherwise an exception will be raised. Similarly, classes are pickled by fully qualified name, so the same restrictions in the unpickling environment apply. Note that none of the class’s code or data is pickled, so in the following example the class attribute is not restored in the unpickling environment: These restrictions are why picklable functions and classes must be defined at the top level of a module. Similarly, when class instances are pickled, their class’s code and data are not pickled along with them. Only the instance data are pickled. This is done on purpose, so you can fix bugs in a class or add methods to the class and still load objects that were created with an earlier version of the class. If you plan to have long-lived objects that will see many versions of a class, it may be worthwhile to put a version number in the objects so that suitable conversions can be made by the class’s method.\n\nIn this section, we describe the general mechanisms available to you to define, customize, and control how class instances are pickled and unpickled. In most cases, no additional code is needed to make instances picklable. By default, pickle will retrieve the class and the attributes of an instance via introspection. When a class instance is unpickled, its method is usually not invoked. The default behaviour first creates an uninitialized instance and then restores the saved attributes. The following code shows an implementation of this behaviour: Classes can alter the default behaviour by providing one or several special methods: In protocols 2 and newer, classes that implements the method can dictate the values passed to the method upon unpickling. The method must return a pair where args is a tuple of positional arguments and kwargs a dictionary of named arguments for constructing the object. Those will be passed to the method upon unpickling. You should implement this method if the method of your class requires keyword-only arguments. Otherwise, it is recommended for compatibility to implement . Changed in version 3.6: is now used in protocols 2 and 3. This method serves a similar purpose as , but supports only positional arguments. It must return a tuple of arguments which will be passed to the method upon unpickling. will not be called if is defined. Changed in version 3.6: Before Python 3.6, was called instead of in protocols 2 and 3. Classes can further influence how their instances are pickled by overriding the method . It is called and the returned object is pickled as the contents for the instance, instead of a default state. There are several cases:\n• None For a class that has no instance and no , the default state is .\n• None For a class that has an instance and no , the default state is .\n• None For a class that has an instance and , the default state is a tuple consisting of two dictionaries: , and a dictionary mapping slot names to slot values. Only slots that have a value are included in the latter.\n• None For a class that has and no instance , the default state is a tuple whose first item is and whose second item is a dictionary mapping slot names to slot values described in the previous bullet. Changed in version 3.11: Added the default implementation of the method in the class. Upon unpickling, if the class defines , it is called with the unpickled state. In that case, there is no requirement for the state object to be a dictionary. Otherwise, the pickled state must be a dictionary and its items are assigned to the new instance’s dictionary. If returns a state with value at pickling, the method will not be called upon unpickling. Refer to the section Handling Stateful Objects for more information about how to use the methods and . At unpickling time, some methods like , , or may be called upon the instance. In case those methods rely on some internal invariant being true, the type should implement to establish such an invariant, as is not called when unpickling an instance. As we shall see, pickle does not use directly the methods described above. In fact, these methods are part of the copy protocol which implements the special method. The copy protocol provides a unified interface for retrieving the data necessary for pickling and copying objects. Although powerful, implementing directly in your classes is error prone. For this reason, class designers should use the high-level interface (i.e., , and ) whenever possible. We will show, however, cases where using is the only option or leads to more efficient pickling or both. The interface is currently defined as follows. The method takes no argument and shall return either a string or preferably a tuple (the returned object is often referred to as the “reduce value”). If a string is returned, the string should be interpreted as the name of a global variable. It should be the object’s local name relative to its module; the pickle module searches the module namespace to determine the object’s module. This behaviour is typically useful for singletons. When a tuple is returned, it must be between two and six items long. Optional items can either be omitted, or can be provided as their value. The semantics of each item are in order:\n• None A callable object that will be called to create the initial version of the object.\n• None A tuple of arguments for the callable object. An empty tuple must be given if the callable does not accept any argument.\n• None Optionally, the object’s state, which will be passed to the object’s method as previously described. If the object has no such method then, the value must be a dictionary and it will be added to the object’s attribute.\n• None Optionally, an iterator (and not a sequence) yielding successive items. These items will be appended to the object either using or, in batch, using . This is primarily used for list subclasses, but may be used by other classes as long as they have append and extend methods with the appropriate signature. (Whether or is used depends on which pickle protocol version is used as well as the number of items to append, so both must be supported.)\n• None Optionally, an iterator (not a sequence) yielding successive key-value pairs. These items will be stored to the object using . This is primarily used for dictionary subclasses, but may be used by other classes as long as they implement .\n• None Optionally, a callable with a signature. This callable allows the user to programmatically control the state-updating behavior of a specific object, instead of using ’s static method. If not , this callable will have priority over ’s . Added in version 3.8: The optional sixth tuple item, , was added. Alternatively, a method may be defined. The only difference is this method should take a single integer argument, the protocol version. When defined, pickle will prefer it over the method. In addition, automatically becomes a synonym for the extended version. The main use for this method is to provide backwards-compatible reduce values for older Python releases. For the benefit of object persistence, the module supports the notion of a reference to an object outside the pickled data stream. Such objects are referenced by a persistent ID, which should be either a string of alphanumeric characters (for protocol 0) or just an arbitrary object (for any newer protocol). The resolution of such persistent IDs is not defined by the module; it will delegate this resolution to the user-defined methods on the pickler and unpickler, and respectively. To pickle objects that have an external persistent ID, the pickler must have a custom method that takes an object as an argument and returns either or the persistent ID for that object. When is returned, the pickler simply pickles the object as normal. When a persistent ID string is returned, the pickler will pickle that object, along with a marker so that the unpickler will recognize it as a persistent ID. To unpickle external objects, the unpickler must have a custom method that takes a persistent ID object and returns the referenced object. Here is a comprehensive example presenting how persistent ID can be used to pickle external objects by reference. # Simple example presenting how persistent ID can be used to pickle # Instead of pickling MemoRecord as a regular class instance, we emit a # Here, our persistent ID is simply a tuple, containing a tag and a # key, which refers to a specific record in the database. # If obj does not have a persistent ID, return None. This means obj # needs to be pickled as usual. # This method is invoked whenever a persistent ID is encountered. # Here, pid is the tuple returned by DBPickler. # Fetch the referenced record from the database and return it. # Always raises an error if you cannot return the correct object. # Otherwise, the unpickler will think None is the object referenced # Fetch the records to be pickled. # Save the records using our custom DBPickler. # Load the records from the pickle data stream. If one wants to customize pickling of some classes without disturbing any other code which depends on pickling, then one can create a pickler with a private dispatch table. The global dispatch table managed by the module is available as . Therefore, one may choose to use a modified copy of as a private dispatch table. creates an instance of with a private dispatch table which handles the class specially. Alternatively, the code does the same but all instances of will by default share the private dispatch table. On the other hand, the code modifies the global dispatch table shared by all users of the module. Here’s an example that shows how to modify pickling behavior for a class. The class below opens a text file, and returns the line number and line contents each time its method is called. If a instance is pickled, all attributes except the file object member are saved. When the instance is unpickled, the file is reopened, and reading resumes from the last location. The and methods are used to implement this behavior. # Copy the object's state from self.__dict__ which contains # all our instance attributes. Always use the dict.copy() # Restore the previously opened file's state. To do so, we need to # reopen it and read from it until the line count is restored. A sample usage might be something like this:\n\nIn some contexts, the module is used to transfer massive amounts of data. Therefore, it can be important to minimize the number of memory copies, to preserve performance and resource consumption. However, normal operation of the module, as it transforms a graph-like structure of objects into a sequential stream of bytes, intrinsically involves copying data to and from the pickle stream. This constraint can be eschewed if both the provider (the implementation of the object types to be transferred) and the consumer (the implementation of the communications system) support the out-of-band transfer facilities provided by pickle protocol 5 and higher. The large data objects to be pickled must implement a method specialized for protocol 5 and higher, which returns a instance (instead of e.g. a object) for any large data. A object signals that the underlying buffer is eligible for out-of-band data transfer. Those objects remain compatible with normal usage of the module. However, consumers can also opt-in to tell that they will handle those buffers by themselves. A communications system can enable custom handling of the objects generated when serializing an object graph. On the sending side, it needs to pass a buffer_callback argument to (or to the or function), which will be called with each generated while pickling the object graph. Buffers accumulated by the buffer_callback will not see their data copied into the pickle stream, only a cheap marker will be inserted. On the receiving side, it needs to pass a buffers argument to (or to the or function), which is an iterable of the buffers which were passed to buffer_callback. That iterable should produce buffers in the same order as they were passed to buffer_callback. Those buffers will provide the data expected by the reconstructors of the objects whose pickling produced the original objects. Between the sending side and the receiving side, the communications system is free to implement its own transfer mechanism for out-of-band buffers. Potential optimizations include the use of shared memory or datatype-dependent compression. Here is a trivial example where we implement a subclass able to participate in out-of-band buffer pickling: # Get a handle over the original buffer object The reconstructor (the class method) returns the buffer’s providing object if it has the right type. This is an easy way to simulate zero-copy behaviour on this toy example. On the consumer side, we can pickle those objects the usual way, which when unserialized will give us a copy of the original object: But if we pass a buffer_callback and then give back the accumulated buffers when unserializing, we are able to get back the original object: This example is limited by the fact that allocates its own memory: you cannot create a instance that is backed by another object’s memory. However, third-party datatypes such as NumPy arrays do not have this limitation, and allow use of zero-copy pickling (or making as few copies as possible) when transferring between distinct processes or systems.\n\nBy default, unpickling will import any class or function that it finds in the pickle data. For many applications, this behaviour is unacceptable as it permits the unpickler to import and invoke arbitrary code. Just consider what this hand-crafted pickle data stream does when loaded: In this example, the unpickler imports the function and then apply the string argument “echo hello world”. Although this example is inoffensive, it is not difficult to imagine one that could damage your system. For this reason, you may want to control what gets unpickled by customizing . Unlike its name suggests, is called whenever a global (i.e., a class or a function) is requested. Thus it is possible to either completely forbid globals or restrict them to a safe subset. Here is an example of an unpickler allowing only few safe classes from the module to be loaded: # Only allow safe classes from builtins. A sample usage of our unpickler working as intended: As our examples shows, you have to be careful with what you allow to be unpickled. Therefore if security is a concern, you may want to consider alternatives such as the marshalling API in or third-party solutions."
    },
    {
        "link": "https://stackoverflow.com/questions/4530611/saving-and-loading-objects-and-using-pickle",
        "document": "I´m trying to save and load objects using module. \n\n First I declare my objects:\n\nAfter that I open a file called 'Fruits.obj'(previously I created a new .txt file and I renamed 'Fruits.obj'):\n\nAfter do this I close my session and I began a new one and I put the next (trying to access to the object that it supposed to be saved):\n\nBut I have this message:\n\nI don´t know what to do because I don´t understand this message. Does anyone know How I can load my object 'banana'? Thank you!\n\nEDIT: As some of you have sugested I put:\n\nThere were no problem, but the next I put was:"
    },
    {
        "link": "https://datacamp.com/tutorial/pickle-python-tutorial",
        "document": "Grow your machine learning skills with scikit-learn in Python. Use real-world datasets in this interactive course and learn how to make powerful predictions!"
    },
    {
        "link": "https://realpython.com/python-pickle-module",
        "document": "As a developer, you may sometimes need to send complex object hierarchies over a network or save the internal state of your objects to a disk or database for later use. To accomplish this, you can use a process called serialization, which is fully supported by the standard library thanks to the Python module.\n• What it means to serialize and deserialize an object\n• Which modules you can use to serialize objects in Python\n• Which kinds of objects can be serialized with the Python module\n• How to use the Python module to serialize object hierarchies\n• What the risks are when deserializing an object from an untrusted source\n\nThe serialization process is a way to convert a data structure into a linear form that can be stored or transmitted over a network. In Python, serialization allows you to take a complex object structure and transform it into a stream of bytes that can be saved to a disk or sent over a network. You may also see this process referred to as marshalling. The reverse process, which takes a stream of bytes and converts it back into a data structure, is called deserialization or unmarshalling. Serialization can be used in a lot of different situations. One of the most common uses is saving the state of a neural network after the training phase so that you can use it later without having to redo the training. Python offers three different modules in the standard library that allow you to serialize and deserialize objects: In addition, Python supports XML, which you can also use to serialize objects. The module is the oldest of the three listed above. It exists mainly to read and write the compiled bytecode of Python modules, or the files you get when the interpreter imports a Python module. So, even though you can use to serialize some of your objects, it’s not recommended. The module is the newest of the three. It allows you to work with standard JSON files. JSON is a very convenient and widely used format for data exchange. There are several reasons to choose the JSON format: It’s human readable and language independent, and it’s lighter than XML. With the module, you can serialize and deserialize several standard Python types: The Python module is another way to serialize and deserialize objects in Python. It differs from the module in that it serializes objects in a binary format, which means the result is not human readable. However, it’s also faster and it works with many more Python types right out of the box, including your custom-defined objects. Note: From now on, you’ll see the terms pickling and unpickling used to refer to serializing and deserializing with the Python module. So, you have several different ways to serialize and deserialize objects in Python. But which one should you use? The short answer is that there’s no one-size-fits-all solution. It all depends on your use case. Here are three general guidelines for deciding which approach to use:\n• Don’t use the module. It’s used mainly by the interpreter, and the official documentation warns that the Python maintainers may modify the format in backward-incompatible ways.\n• The module and XML are good choices if you need interoperability with different languages or a human-readable format.\n• The Python module is a better choice for all the remaining use cases. If you don’t need a human-readable format or a standard interoperable format, or if you need to serialize custom objects, then go with .\n\nThe Python module basically consists of four methods: The first two methods are used during the pickling process, and the other two are used during unpickling. The only difference between and is that the first creates a file containing the serialization result, whereas the second returns a string. To differentiate from , it’s helpful to remember that the at the end of the function name stands for . The same concept also applies to and : The first one reads a file to start the unpickling process, and the second one operates on a string. Consider the following example. Say you have a custom-defined class named with several different attributes, each of a different type: The example below shows how you can instantiate the class and pickle the instance to get a plain string. After pickling the class, you can change the value of its attributes without affecting the pickled string. You can then unpickle the pickled string in another variable, restoring an exact copy of the previously pickled class: \"This is my pickled object: \"This is a_dict of the unpickled object: In the example above, you create several different objects and serialize them with . This produces a single string with the serialized result: python pickling.py This is my pickled object: This is a_dict of the unpickled object: {'first': 'a', 'second': 2, 'third': [1, 2, 3]} The pickling process ends correctly, storing your entire instance in this string: After the pickling process ends, you modify your original object by setting the attribute to . Finally, you unpickle the string to a completely new instance. What you get is a deep copy of your original object structure from the time that the pickling process began.\n\nAs mentioned above, the module is Python-specific, and the result of a pickling process can be read only by another Python program. But even if you’re working with Python, it’s important to know that the module has evolved over time. This means that if you’ve pickled an object with a specific version of Python, then you may not be able to unpickle it with an older version. The compatibility depends on the protocol version that you used for the pickling process. There are currently six different protocols that the Python module can use. The higher the protocol version, the more recent the Python interpreter needs to be for unpickling.\n• Protocol version 0 was the first version. Unlike later protocols, it’s human readable.\n• Protocol version 1 was the first binary format.\n• Protocol version 3 was added in Python 3.0. It can’t be unpickled by Python 2.x.\n• Protocol version 4 was added in Python 3.4. It features support for a wider range of object sizes and types and is the default protocol starting with Python 3.8.\n• Protocol version 5 was added in Python 3.8. It features support for out-of-band data and improved speeds for in-band data. Note: Newer versions of the protocol offer more features and improvements but are limited to higher versions of the interpreter. Be sure to consider this when choosing which protocol to use. To identify the highest protocol that your interpreter supports, you can check the value of the attribute. To choose a specific protocol, you need to specify the protocol version when you invoke , , or . If you don’t specify a protocol, then your interpreter will use the default version specified in the attribute.\n\nYou’ve already learned that the Python module can serialize many more types than the module. However, not everything is picklable. The list of unpicklable objects includes database connections, opened network sockets, running threads, and others. If you find yourself faced with an unpicklable object, then there are a couple of things that you can do. The first option is to use a third-party library such as . The module extends the capabilities of . According to the official documentation, it lets you serialize less common types like functions with yields, nested functions, lambdas, and many others. To test this module, you can try to pickle a function: If you try to run this program, then you will get an exception because the Python module can’t serialize a function: python pickling_error.py _pickle.PicklingError: Can't pickle <function <lambda> at 0x10cd52cb0>: attribute lookup <lambda> on __main__ failed Now try replacing the Python module with to see if there’s any difference: If you run this code, then you’ll see that the module serializes the without returning an error: Another interesting feature of is that it can even serialize an entire interpreter session. Here’s an example: In this example, you start the interpreter, import a module, and define a function along with a couple of other variables. You then import the module and invoke to serialize the entire session. If everything goes okay, then you should get a file in your current directory: Now you can start a new instance of the interpreter and load the file to restore your last session: dict_items([('__name__', '__main__'), ('__doc__', None), ('__package__', None), ('__loader__', <class '_frozen_importlib.BuiltinImporter'>), ('__spec__', None), ('__annotations__', {}), ('__builtins__', <module 'builtins' (built-in)>)]) dict_items([('__name__', '__main__'), ('__doc__', None), ('__package__', None), ('__loader__', <class '_frozen_importlib.BuiltinImporter'>), ('__spec__', None), ('__annotations__', {}), ('__builtins__', <module 'builtins' (built-in)>), ('dill', <module 'dill' from '/usr/local/lib/python3.7/site-packages/dill/__init__.py'>), ('square', <function <lambda> at 0x10a013a70>), ('a', 1225), ('math', <module 'math' from '/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7/lib/python3.7/lib-dynload/math.cpython-37m-darwin.so'>), ('b', 22.0)]) The first statement demonstrates that the interpreter is in the initial state. This means that you need to import the module and call to restore your serialized interpreter session. Note: Before you use instead of , keep in mind that is not included in the standard library of the Python interpreter and is typically slower than . Even though lets you serialize a wider range of objects than , it can’t solve every serialization problem that you may have. If you need to serialize an object that contains a database connection, for example, then you’re in for a tough time because it’s an unserializable object even for . So, how can you solve this problem? The solution in this case is to exclude the object from the serialization process and to reinitialize the connection after the object is deserialized. You can use to define what should be included in the pickling process. This method allows you to specify what you want to pickle. If you don’t override , then the default instance’s will be used. In the following example, you’ll see how you can define a class with several attributes and exclude one attribute from serialization with : In this example, you create an object with three attributes. Since one attribute is a , the object is unpicklable with the standard module. To address this issue, you specify what to pickle with . You first clone the entire of the instance to have all the attributes defined in the class, and then you manually remove the unpicklable attribute. If you run this example and then deserialize the object, then you’ll see that the new instance doesn’t contain the attribute: But what if you wanted to do some additional initializations while unpickling, say by adding the excluded object back to the deserialized instance? You can accomplish this with : By passing the excluded object to , you ensure that it appears in the of the unpickled string.\n\nYou now know how to use the module to serialize and deserialize objects in Python. The serialization process is very convenient when you need to save your object’s state to disk or to transmit it over a network. However, there’s one more thing you need to know about the Python module: It’s not secure. Do you remember the discussion of ? Well, that method is great for doing more initialization while unpickling, but it can also be used to execute arbitrary code during the unpickling process! So, what can you do to reduce this risk? Sadly, not much. The rule of thumb is to never unpickle data that comes from an untrusted source or is transmitted over an insecure network. In order to prevent man-in-the-middle attacks, it’s a good idea to use libraries such as to sign the data and ensure it hasn’t been tampered with. The following example illustrates how unpickling a tampered pickle could expose your system to attackers, even giving them a working remote shell: # The attack is from 192.168.1.10 # The attacker is listening on port 8080 In this example, the unpickling process executes , which executes a Bash command to open a remote shell to the machine on port . Here’s how you can safely test this script on your Mac or your Linux box. First, open the terminal and use the command to listen for a connection to port 8080: This will be the attacker terminal. If everything works, then the command will seem to hang. Next, open another terminal on the same computer (or on any other computer on the network) and execute the Python code above for unpickling the malicious code. Be sure to change the IP address in the code to your attacking terminal’s IP address. In my example, the attacker’s IP address is . By executing this code, the victim will expose a shell to the attacker: If everything works, a Bash shell will appear on the attacking console. This console can now operate directly on the attacked system: nc -l bash: no job control in this shell The default interactive shell is now zsh. To update your account to use zsh, please run `chsh -s /bin/zsh`. For more details, please visit https://support.apple.com/kb/HT208050. So, let me repeat this critical point once again: Do not use the module to deserialize objects from untrusted sources!"
    },
    {
        "link": "https://geeksforgeeks.org/understanding-python-pickling-example",
        "document": "In Python, we sometimes need to save the object on the disk for later use. This can be done by using Python pickle. In this article, we will learn about pickles in Python along with a few examples.\n\nPython pickle module is used for serializing and de-serializing a Python object structure. Any object in Python can be pickled so that it can be saved on disk. What Pickle does is it “serializes” the object first before writing it to a file. Pickling is a way to convert a Python object (list, dictionary, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another Python script. It provides a facility to convert any Python object to a byte stream. This Byte stream contains all essential information about the object so that it can be reconstructed, or “unpickled” and get back into its original form in any Python.\n\nIn this example, we will serialize the dictionary data and store it in a byte stream. Then this data is deserialized using pickle.loads() function back into the original Python object.\n\nIn this example, we will use a pickle file to first write the data in it using the pickle.dump() function. Then using the pickle.load() function, we will load the pickle fine in Python script and print its data in the form of a Python dictionary.\n\nAdvantages of Using Pickle in Python\n• Recursive objects (objects containing references to themselves): Pickle keeps track of the objects it has already serialized, so later references to the same object won’t be serialized again. (The marshal module breaks for this.)\n• Object sharing (references to the same object in different places): This is similar to self-referencing objects. Pickle stores the object once, and ensures that all other references point to the master copy. Shared objects remain shared, which can be very important for mutable objects.\n• User-defined classes and their instances: Marshal does not support these at all, but Pickle can save and restore class instances transparently. The class definition must be importable and live in the same module as when the object was stored.\n\nDisadvatages of Using Pickle in Python\n• Python Version Dependency: Data of picle is so sensitive to the version of Python that produced. Pickled object created with one version of Python that might not be unpickled with a various versions.\n• Non-Readble: The format of pickle is binary and not easily readable or editable by humans. The contracts that are in JSON or XML format can be easily modified.\n• Large data inefficiency: Large datasets can slow down the pickling and unpickling. Serialization might be more appropriate for such use-cases.\n\nWhile Python Pickle offers capabilities for object serialization, developers that maintain limitations , especially while working across various Python versions or dealing with the large datasets. It’s important to remember always consider the specific needs of your application to determine if ickle or an alternative like JSON, XML is suited for serialization.\n\nWhat is Pickling in Python with an Example?\n\nPickling in Python refers to the process of converting a Python object into a byte stream, which can be stored on disk or sent over a network. This byte stream can later be converted back into the original object, a process known as unpickling. The module in Python provides this functionality. Example of Pickling and Unpickling:\n\nWhat is the Pickle Load Function?\n\nWhat Objects Cannot Be Pickled in Python?\n\nHow to Convert a Model to Pickle in Python?\n\nIs Joblib Better than Pickle?"
    },
    {
        "link": "https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html",
        "document": "Join a sequence of arrays along an existing axis.\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default). The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0. If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified. If provided, the destination array will have this dtype. Cannot be provided together with out. Controls what kind of data casting may occur. Defaults to ‘same_kind’. For a description of the options, please see casting.\n\nWhen one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead.\n\nThis function will not preserve masking of MaskedArray inputs."
    },
    {
        "link": "https://numpy.org/doc/2.0/reference/generated/numpy.concatenate.html",
        "document": "Join a sequence of arrays along an existing axis.\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default). The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0. If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified. If provided, the destination array will have this dtype. Cannot be provided together with out. Controls what kind of data casting may occur. Defaults to ‘same_kind’. For a description of the options, please see casting.\n\nWhen one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead.\n\nThis function will not preserve masking of MaskedArray inputs."
    },
    {
        "link": "https://numpy.org/devdocs/reference/generated/numpy.concatenate.html",
        "document": "Join a sequence of arrays along an existing axis.\n\nThe arrays must have the same shape, except in the dimension corresponding to axis (the first, by default). The axis along which the arrays will be joined. If axis is None, arrays are flattened before use. Default is 0. If provided, the destination to place the result. The shape must be correct, matching that of what concatenate would have returned if no out argument were specified. If provided, the destination array will have this dtype. Cannot be provided together with out. Controls what kind of data casting may occur. Defaults to ‘same_kind’. For a description of the options, please see casting.\n\nSplit an array into multiple sub-arrays of equal or near-equal size. Split array into a list of multiple sub-arrays of equal size. Split array into multiple sub-arrays along the 3rd axis (depth). Stack a sequence of arrays along a new axis. Stack arrays in sequence depth wise (along third dimension).\n\nWhen one or more of the arrays to be concatenated is a MaskedArray, this function will return a MaskedArray object instead of an ndarray, but the input masks are not preserved. In cases where a MaskedArray is expected as input, use the ma.concatenate function from the masked array module instead.\n\nThis function will not preserve masking of MaskedArray inputs."
    },
    {
        "link": "https://geeksforgeeks.org/numpy-concatenate-function-python",
        "document": ""
    },
    {
        "link": "https://stratascratch.com/blog/numpy-concatenate-efficient-array-manipulation-in-python",
        "document": "Mastering advanced techniques in data manipulation with NumPy through comprehensive understanding and application of array concatenation methods.\n\nNumPy is a cornerstone of the Python data science ecosystem, renowned for its powerful array manipulation capabilities. One of the most potent functions in NumPy is the ability to combine different datasets.\n\nIf we compare a data scientist to a chef, using concatenate is akin to blending various ingredients to create a delicious meal. A skilled chef knows there is more than one way to prepare the perfect dish.\n\nIn this article, we will explore various techniques, beginning with basic array operations and progressing to more advanced analyses. We'll achieve this by using concatenate in diverse ways. Let's start with the fundamentals!\n\nWhat is a NumPy array in Python?\n\nA NumPy array in Python is like a different version of a list. It's a collection of elements, that you can do math on very easily and quickly. Unlike regular lists in Python, elements in a NumPy array are all of the same type, which makes calculations faster and more efficient.\n\nHere is the output.\n\nBasic operations with arrays are straightforward. Let's say you're analyzing daily temperatures. Adding, subtracting, multiplying, and dividing numbers in an array is as easy as it sounds.\n\nHere is the output.\n\nIn our example, we added 2 to each temperature and then doubled them. This shows how NumPy makes data manipulation efficient and intuitive.\n\nNow, we have a basic knowledge of what is NumPy array and how it works, now we can use NumPy concatenate function.\n\nWhat is the NumPy concatenate() Function?\n\nNumPy concatenate function is used to combine two arrays together. Let’s see an example.\n\nHere is the output.\n\nIn this example, we've merged two arrays into one.\n\nThe NumPy concatenate() function primarily takes two parameters: the sequence of arrays to join and the axis along which to join them.\n\nHere's a brief overview of all its parameters:\n• a1, a2, ...: These are the arrays you want to join together.\n• axis: If you don't specify anything, it defaults to 0, which means it will stack the arrays vertically.\n• out: If you provide an array here, NumPy will place the result into this array instead of creating a new one.\n• dtype: This optional parameter lets you specify the data type of the resulting array.\n• casting: This optional parameter controls how NumPy should handle the casting of types during the concatenation.\n\nIf you want to know more about parameters of concatenate(), here is the official documentation link.\n\nIn real-world data science applications, it's common to see the situations where we need to concatenate arrays that have different dimensions. Let's explore how we can efficiently combine such arrays.\n\nConsider a scenario in a financial or health dataset where you have a main set of data and an additional set of features that you want to integrate. These arrays might have different dimensions, but you need to combine them for comprehensive analysis.\n\nFor starters, let’s use simple exercise. Here is the code, which creates two different array’s and at the end, we will vertically stack one array on top of other.\n\nHere is the output.\n\nIf you feel that you need to improve your python skills too, here you can start from these python interview questions.\n\nNow let’s do more complicated example, which concatenating array along a specific axis. To do that, we will use world happiness report from Kaggle, you can find the dataset here.\n\nThe World Happiness Report from Kaggle shows how happy people are in different countries. It uses scores based on things like money, health, and freedom. The data helps us see which countries are happier.\n\nLet's start by identifying European countries, adding the \"Year\" column. We'll focus on a few columns like 'Country or region', 'Score', 'GDP per capita', and 'Healthy life expectancy' to observe trends over the years.\n\nNext, we will concatenate the filtered datasets to combine them into a single DataFrame. To do that, first we will turn our datasets for each years into values, to be able to concatenate them by using numpy afterwards.\n\nAnd at the end, we will turn the arrays to the pandas datafame, and add column names to see at the end our data, in dataframe format. Here is the code.\n\nNow, let’s see our dataframes first rows.\n\nHere is the output.\n\nGreat, but don’t forget, we did 3 different years.\n\nLet’s visualize the output to evaluate the result better. Here is the code:\n\nHere is the output.\n\nLet’s use happiness dataset that we mentioned before.\n\nLet’s see 2015’s dataset for you to see. Here is the code:\n\nHere is the output.\n\nWith the World Happiness Report data for different years, let's say we want to compare the happiness scores of countries across two consecutive years this time.\n\nWe can do this by concatenating the datasets horizontally. This is like putting two yearly reports, 2015 and 2016, side by side to see changes over time.\n\nIn the code below, first we select relevant data from two different datasets and turn these dataframes into NumPy arrays, by using values() method. At the end we will horizontally concatenate them into single big arrays and convert back to dataframe to see the result.\n\nNow, let’s see first few rows of our dataframe.\n\nHere is the output.\n\nIn this code, we merge the 2015 and 2016 happiness data side by side, horizontally. It helps to visually compare how each country's happiness score changed from one year to the next.\n\nYou can find Tesla stock data on Kaggle, for example, this one shows between 2010 to 2020.\n\nTo demonstrate concatenating vertically, we are now using two distinct datasets of Tesla stock data, representing different time periods: 2012-2013 and 2014-2015.\n\nHere is our first dataset. Let’s see the code.\n\nHere is the output.\n\nHere is our second dataset. Let’s see the code.\n\nHere is the output.\n\nNow let’s concatenate two datasets of Tesla stock data from different times into one big set using NumPy.\n\nThis way of putting data together is good for looking at trends over a long time. It helps us see how Tesla's stock did over many years, which is important for understanding its financial performance.\n\nOf course to do that first, we will convert dataframe to arrays, concatenate, then convert back to the dataframe, here is the code:\n\nHere is the output.\n\nBefore finishing the article, if you want to test your NumPy knowledge, check out these NumPy interview questions.\n\nBonus: Using NumPy Concatenate() instead of Pandas concat()\n\nSometimes, working with arrays provides more flexibility and speed. So, if your project includes using Pandas' concat(), you might consider using NumPy's concatenate() first.\n\nTo do that, first thing you must do is changing format of your data, like in this data project from our platform, Laptop Price Prediction.\n\nAs you can see, your task will be to define and train a machine learning model to predict laptop price. To do that, there are several times you must use pa ndas concat function, like combining dataframes at first.\n\nThen pick your position, and concatenate your arrays, according to your wishes. But before everything, don’t forget to convert it to dataframe too.\n\nJust like in the above example you can repeat same actions, for this data project too.\n\nFor this data project, we need a model to predict how many interactions a property gets over time.\n\nFor any data project you choose, converting pandas to arrays will provide the flexibility and speed you need. It's a rare but powerful technique.\n\nIn this article, we went into the features of NumPy concatenate() function, exploring how it can blend different datasets.\n\nRemember, just like in cooking, practice is key in data science; using tools like NumPy's concatenate() in various scenarios will sharpen your skills. We invite you to join StrataScratch platform, where you can take your data science journey to the next level, tackling real-world projects and interview questions that will prepare you for an exciting career in this field.\n\nDon't miss this chance to grow – explore the platform today!"
    },
    {
        "link": "https://discuss.pytorch.org/t/how-to-feed-string-data-to-a-gpu-to-encode-data-sentencetransformers/85572",
        "document": "I am using Bert sentence transformers to encode strings into sentence embeddings in a scalable way and I’ve run into a blocking problem: I can’t get the code to run on a GPU.\n\nWithout using a GPU I can pass individual strings to SentenceTransformer.encode() (a Model) and get vector representations. When I use a GPU I get a GPU error that arguments are located on different machines. How do I get string data to the GPU? The docs I find indicate this can’t be done and I need to find a way to batch a bunch of strings to get encoded at once on the GPU. When I pass a numpy array of strings to Model.encode(), I get an exception: arguments are located on different GPUs . How do you get string data on a GPU?\n\nI looked up how to send a numpy array of strings to a GPU using PyTorch and there seems to be no way to do that. I looked at and it doesn’t seem to encode data, and I can’t get it to work.\n\nI start out creating a SentenceTransformer using a pooling model and I can run it on one string at a time. The method takes a string or list of strings as an argument. Without sending it to the GPU it works fine. I can’t figure out how to get a list or numpy array of strings to the GPU! It is maddening, there is nothing out there on this. It must be possible because sentence transformers operate on strings. But how?\n\nWhen I look at the series, it is a numpy array of .\n\nWhen I try to create a tensor to get the data on a GPU, I run into this problem:\n\nSo I am very much stuck. How do you encode multiple strings on a GPU?"
    },
    {
        "link": "https://pytorch.org/tutorials",
        "document": ""
    },
    {
        "link": "https://medium.com/data-scientists-diary/implementation-of-transformer-encoder-in-pytorch-daeb33a93f9c",
        "document": "“Code is like humor. When you have to explain it, it’s bad.” — Cory House We’re here to get our hands dirty with code, specifically implementing a Transformer Encoder from scratch using PyTorch. So, if you’re here for a high-level theory recap, this might not be the right place. We’ll keep the explanations lean and code-heavy, focusing solely on implementation. The goal is simple but ambitious: to walk you through the essential steps of building a fully functional Transformer Encoder layer by layer. From multi-head self-attention to the feedforward networks, each component will be coded from the ground up. By the end, you’ll not only have a clear grasp of implementing a Transformer Encoder, but you’ll also understand how these layers interact in practical applications. Let’s be real: Transformers have taken the deep learning world by storm, powering everything from NLP to computer vision models. At the heart of this success is the Encoder, a marvel in capturing sequential data’s dependencies. Here’s the deal: instead of discussing how Transformers “revolutionized” the field, we’re jumping straight into building the nuts and bolts of this encoder in PyTorch. If you’re familiar with the basics, that’s great. We’ll leverage that knowledge to go deeper into implementation rather than revisit why Transformers work. Let’s get started.\n\nGetting everything running smoothly is key. You don’t want to get bogged down in version mismatches or missing libraries while diving into code. So, let’s make sure we’re on the same page when it comes to our coding environment. To ensure compatibility, it’s best to work with a recent version of PyTorch and Python. Here’s a quick rundown of the environment setup: # Install compatible versions of PyTorch and supporting libraries\n\npip install torch==1.9.0 # Example version, adjust as needed\n\npip install numpy Make sure to install a compatible version of PyTorch based on your GPU (if you have one). I typically use for its reliability in handling the architecture we’ll be implementing. While the version isn’t overly restrictive, some functions may vary in behavior or performance, so consistency is key here. We’ll use a few essential libraries to build our encoder. Let me break down each one, so there’s no ambiguity: import torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nimport numpy as np\n• : This is the backbone of our implementation. PyTorch will be handling all tensor operations, so you’ll be calling it frequently.\n• : For layers like Linear, Dropout, and LayerNorm, we’ll rely heavily on , which simplifies neural network building.\n• : If you’re planning to train this encoder later, provides easy access to optimizers (e.g., Adam, SGD).\n• : Optional but helpful for initializing certain components, like positional encodings, if you want to experiment with numpy’s utility functions. Here’s how it might look in code to keep everything tidy from the get-go: import torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nimport numpy as np\n\n\n\n# Set device to GPU if available\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(f\"Using device: {device}\") This code snippet sets the scene, ensuring we’re prepared to harness GPU power if it’s available. Nothing breaks momentum faster than needing to debug device compatibility issues halfway through an implementation. With these configurations in place, you’re ready to tackle the next stage: building each component of the Transformer Encoder. Starting clean and well-prepared gives you a strong foundation to work from, without having to revisit compatibility issues down the line. Ready? Let’s dive into the nitty-gritty of the encoder layers next.\n\nIn Transformers, every layer serves a specific purpose, and the synergy between them is what brings this architecture to life. We’ll tackle each layer with clear code and practical reasoning, so you know why each part is essential. Without a positional encoding layer, a Transformer would treat every word in a sequence as if they appeared in a vacuum, disregarding order. The positional encoding provides a way to inject information about the sequence order directly into the embeddings, which is essential because Transformers, unlike RNNs, don’t process tokens in order. Here’s how we can implement positional encoding: import torch\n\nimport math\n\n\n\nclass PositionalEncoding(nn.Module):\n\n def __init__(self, d_model, max_len=5000):\n\n super(PositionalEncoding, self).__init__()\n\n \n\n # Create a long tensor of max_len positions\n\n position = torch.arange(0, max_len).unsqueeze(1)\n\n \n\n # Compute the positional encodings using sin and cos\n\n div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n\n pe = torch.zeros(max_len, d_model)\n\n pe[:, 0::2] = torch.sin(position * div_term)\n\n pe[:, 1::2] = torch.cos(position * div_term)\n\n \n\n # Register buffer so it’s saved in the model state_dict but not optimized\n\n self.register_buffer('pe', pe.unsqueeze(0))\n\n \n\n def forward(self, x):\n\n x = x + self.pe[:, :x.size(1)]\n\n return x So, what’s happening here? By using and on the positions, we create distinct patterns that give each position in the sequence a unique encoding. Every dimension in the embedding gets a different frequency through , resulting in a position-specific yet learnable addition to the word embeddings. Imagine the attention layer as an engine for capturing dependencies. It’s not just about looking back and forth in the sequence — it’s about finding out which words (or tokens) influence each other most. The multi-head self-attention layer does this by creating multiple attention “heads,” each one responsible for looking at different parts of the sequence. This isn’t just for robustness but to enrich the embedding with various perspectives. class MultiHeadAttention(nn.Module):\n\n def __init__(self, d_model, num_heads):\n\n super(MultiHeadAttention, self).__init__()\n\n assert d_model % num_heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n\n \n\n self.num_heads = num_heads\n\n self.d_k = d_model // num_heads\n\n \n\n # Linear layers for Q, K, V transformations\n\n self.q_linear = nn.Linear(d_model, d_model)\n\n self.k_linear = nn.Linear(d_model, d_model)\n\n self.v_linear = nn.Linear(d_model, d_model)\n\n self.fc_out = nn.Linear(d_model, d_model)\n\n \n\n def forward(self, query, key, value):\n\n batch_size = query.size(0)\n\n \n\n # Perform linear transformations and split into heads\n\n Q = self.q_linear(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n K = self.k_linear(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n V = self.v_linear(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n\n \n\n # Compute attention\n\n attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n attn_weights = torch.softmax(attn_weights, dim=-1)\n\n attn_output = torch.matmul(attn_weights, V)\n\n \n\n # Concatenate heads and put through final linear layer\n\n attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n\n return self.fc_out(attn_output) By dividing by , we create smaller, “independent” attention heads that each compute their own weights. These heads then concatenate to form the final output. Using here enables efficient dot-product computation between queries and keys, which gives us the attention score. After the self-attention step, the FFN injects a non-linear transformation into each position of the sequence, enriching the learned representation. In this FFN layer, we first use a layer to expand the dimensions (e.g., from to ) before passing it through ReLU. Then, another layer brings it back down. The dropout here ensures that we avoid overfitting, especially when training deep Transformer models. Layer normalization is essential for stabilizing the network and speeding up training. Instead of normalizing over the batch, we normalize over the features, keeping our model’s layers balanced. Residual connections help preserve the original input signal, which stabilizes the gradients during training. By adding the input to the output after each sublayer, we’re essentially saying, “Keep some of the original information.” makes this easy, ensuring that both signals remain in the output.\n\nNow that you have a single encoder layer, it’s time to stack it into a deeper, more powerful architecture. Each layer in the stack captures additional hierarchical relationships, improving the model’s ability to understand complex dependencies. In PyTorch, the most efficient way to handle multiple layers is with . This lets you manage the encoder layers flexibly, allowing for easy adjustments to the depth of your Transformer Encoder. class TransformerEncoder(nn.Module):\n\n def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):\n\n super(TransformerEncoder, self).__init__()\n\n self.layers = nn.ModuleList(\n\n [TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n\n )\n\n self.num_layers = num_layers\n\n \n\n def forward(self, x):\n\n for layer in self.layers:\n\n x = layer(x)\n\n return x\n• We use a to contain multiple instances. Each layer processes the output of the previous one, making the stack deeper and more sophisticated.\n• determines the depth of the encoder stack. Feel free to adjust it depending on your task’s complexity—NLP tasks often use 6–12 layers, while simpler applications may need fewer. Here’s the deal: with , you have the flexibility to control each layer independently if you ever want to modify individual layers or track their outputs separately. This modular approach makes debugging and fine-tuning the architecture a breeze. To ensure that our encoder can handle various downstream tasks, we need to align the input and output dimensions across the layers.\n• Input Dimension ( ): This dimension remains consistent throughout all encoder layers to keep each layer’s output compatible with the next.\n• Output Dimension: The output shape will match the input shape in terms of batch size and sequence length, but the content within each vector will be transformed by the encoder layers. In practice, you’ll define based on your application, ensuring that any data flowing into the encoder (like token embeddings) matches the expected input size.\n\nBuilding is one thing; testing ensures everything works as expected. Here’s where we validate the setup by feeding sample data through the encoder stack. You might be wondering how to generate a test input for the encoder. Here’s a straightforward way to do it with random tensors that match the dimensions of typical sequence data: Here, we simulate a tensor that represents a batch of sequences. For NLP tasks, each sequence typically represents a sentence or phrase, with each vector in the sequence representing an embedding of a word or token. With our sample input ready, let’s run it through the full encoder stack to check if everything flows smoothly: # Initialize Transformer Encoder\n\nnum_layers = 6\n\nnum_heads = 8\n\nd_ff = 2048\n\nencoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff)\n\n\n\n# Run the encoder on the sample input\n\noutput = encoder(sample_input)\n\nprint(\"Output shape:\", output.shape) If everything’s wired correctly, you should see an output shape that matches your input: . This tells you the encoder is preserving the sequence structure, transforming the content within each vector without disrupting the overall shape. The final shape verification is essential, especially when integrating this encoder into larger architectures. Here’s a quick check to make sure you’re getting what you expect: If the assertion passes, you know the encoder is handling the data correctly. This step might seem trivial, but it’s invaluable in complex architectures, saving you from potential dimension mismatches down the line. By now, you have a fully functional Transformer Encoder stack, capable of handling complex data. You’ve stacked multiple encoder layers to deepen the architecture and verified its functionality with sample inputs. This process ensures that the encoder can seamlessly integrate into larger models for tasks in NLP, vision, and beyond. With this foundation, you’re well-equipped to start experimenting with real-world datasets, finetuning hyperparameters, or even exploring the decoder side of Transformers if your project demands it. This concludes the implementation of the Transformer Encoder in PyTorch. Let’s move on to applying this in a practical context if you’re ready!\n\nImplementing a Transformer Encoder is one thing, but applying it in custom architectures is where it truly shines. Let’s go over how you can use this encoder in real-world scenarios and optimize it for efficiency. Using the Transformer Encoder in Custom Architectures You might be wondering: how exactly does this encoder plug into larger models? In practice, Transformer Encoders are highly adaptable and can connect to different downstream layers depending on your end task. Here’s the deal:\n• For NLP Tasks: In language models or text classifiers, your Transformer Encoder typically takes token embeddings as input, which it processes to capture context and relationships between tokens. This processed sequence can then flow into fully connected (FC) layers for classification or regression tasks. Here, I used to average the output embeddings across tokens, which is a common approach in classification tasks for NLP. You can replace this with if using a CLS token or with attention pooling for different tasks.\n• For Vision Tasks: In computer vision, Transformers often receive flattened patches of an image as input. You can take the output of the Transformer Encoder and pass it to a series of convolutional layers or even directly to fully connected layers if you’re working with smaller inputs. class VisionTransformer(nn.Module):\n\n def __init__(self, patch_size, d_model, num_layers, num_heads, d_ff, num_classes):\n\n super(VisionTransformer, self).__init__()\n\n self.encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff)\n\n self.fc = nn.Linear(d_model, num_classes)\n\n \n\n def forward(self, x):\n\n # Assuming x is already in flattened patch form\n\n x = self.encoder(x)\n\n x = x.mean(dim=1) # Aggregate features across patches\n\n return self.fc(x)\n• This approach lets you use the encoder as a feature extractor for image classification tasks, where each patch of the image is represented as a “token.” These examples provide flexibility to connect the Transformer Encoder with other layers, letting you experiment with different downstream structures. Working with Transformers can be memory-intensive, especially for large datasets and deep architectures. Here are some practical tips to help:\n• Manage Memory with : When evaluating your model (e.g., during validation), use to prevent PyTorch from tracking gradients, saving memory and speeding up inference. 2. Adjust Batch Size: Larger batch sizes increase GPU memory usage significantly. If you’re hitting memory limits, consider reducing the batch size or using gradient accumulation. 3. Use Mixed Precision Training: Mixed precision (16-bit) training saves memory and speeds up training. PyTorch’s provides an easy-to-use API to leverage this optimization on compatible GPUs. 4. Maximize GPU Utilization: Ensure the model and data are on the same device to avoid bottlenecks. This might sound obvious, but mismatches between CPU and GPU during data transfer can slow down training. 5. Adjust Sequence Length: If possible, limit the sequence length during experimentation, then scale up only when necessary. Reducing sequence length can significantly cut memory usage. These optimizations help ensure that your Transformer Encoder can handle real-world data effectively and scale as needed."
    },
    {
        "link": "https://stackoverflow.com/questions/54706146/moving-member-tensors-with-module-to-in-pytorch",
        "document": "I am building a Variational Autoencoder (VAE) in PyTorch and have a problem writing device agnostic code. The Autoencoder is a child of with an encoder and decoder network, which are too. All weights of the network can be moved from one device to another by calling .\n\nThe problem I have is with the reparametrization trick:\n\nThe noise is a tensor of the same size as and and saved as a member variable of the autoencoder module. It is initialized in the constructor and resampled in-place each training step. I do it that way to avoid constructing a new noise tensor each step and pushing it to the desired device. Additionally, I want to fix the noise in the evaluation. Here is the code:\n\nWhen I now move the autoencoder to the GPU with I get an error in forwarding because the noise tensor is not moved.\n\nI don't want to add a device parameter to the constructor, because then it is still not possible to move it to another device later. I also tried to wrap the noise into so that it is affected by , but that gives an error from the optimizer, as the noise is flagged as .\n\nAnyone has a solution to move all of the modules with ?"
    },
    {
        "link": "https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1",
        "document": "I have recently been getting more involved in the world of machine learning. When I’ve had a problem understanding a complex issue or coding a neural network, the internet has seemed to have all the answers: from a simple linear regression to complex convolutional networks. At least that is what I thought…\n\nOnce I began getting better at this Deep Learning thing, I stumbled upon the all-glorious transformer. The original paper: \"Attention is all you need\", proposed an innovative way to construct neural networks. No more convolutions! The paper proposes an encoder-decoder neural network made up of repeated encoder and decoder blocks. The structure is the following:\n\nThe left block is the encoder, and the right block is the decoder. If you don’t understand the parts of this model yet, I highly recommend going over Harvard’s \"The Annotated Transformer\" guide where they code the transformer model in PyTorch from scratch. I will not be covering important concepts like \"multi-head attention\" or \"feed-forward layers\" in this tutorial, so you should know them before you continue reading. If you have already taken a look at the code from scratch, you are probably wondering if you are going to have to copy-paste that code all over the place for every project you make. Thankfully, no. Modern python libraries like PyTorch and Tensorflow already include easily accessible transformer models through an import. However, there is more to it than just importing the model and plugging it in. Today I will explain how to use and tune PyTorch nn.Transformer() module. I personally struggled trying to find information about how to implement, train, and infer from it, so I decided to create my own guide for all of you.\n\nTo start, we need to import PyTorch and some other libraries we are going to be using:\n\nNow, let’s take a closer look at the transformer module. I recommend starting by reading over PyTorch’s documentation about it. As they explain, there are no mandatory parameters. The module comes with the \"Attention is all you need\" model hyperparameters. To use it, let’s begin by creating a simple PyTorch model. I will only change some of the default parameters so our model doesn’t take unnecessarily long to train. I made those parameters part of our class:\n\nThe transformer blocks don’t care about the order of the input sequence. This, of course, is a problem. Saying \"I ate a pizza with pineapple\" is not the same as saying \"a pineapple ate I with pizza\". Thankfully, we have a solution: positional encoding. This is a way to \"give importance\" to elements depending on their position. A detailed explanation of how it works can be found here, but a quick explanation is that we create a vector for each element representing its position with regard to every other element in the sequence. Positional encoding follows this very complicated-looking formula which, in practice, we won’t really need to understand:\n\nFor the sake of organization and reusability, let’s create a separate class for the positional encoding layer (it looks hard but it is really just the formula, dropout, and a residual connection):\n\nNow that we have the only layer not included in PyTorch, we are ready to finish our model. Before adding the positional encoding, we need an embedding layer so that each element in our sequences is converted into a vector we can manipulate (instead of a fixed integer). We will also need a final linear layer so that we can convert the model’s output into the dimensions of our desired output. The final model should look something like this:\n\nI know… It looks very intimidating, but if you understand what each part does, it is actually a pretty simple model to implement.\n\nYou may recall there was a special block in the model structure called \"masked multi-head attention\":\n\nSo… what is masking? Before I can explain it to you, let’s quickly recapitulate what is going on with our tensors when we feed them into our model. First, we embed and encode (positional encoding) our source tensor. Then, our source tensor is encoded into an unintelligible encoded tensor that we feed into our decoder with our embedded and encoded (positionally) target vector. For our model to learn, we can’t just show it the whole target tensor! This would just give him the answer straight up.\n\nThe solution to this is a masking tensor. This tensor is made up of size (sequence length x sequence length) since for every element in the sequence, we show the model one more element. This matrix will be added to our target vector, so the matrix will be made up of zeros in the positions where the transformer can have access to the elements, and minus infinity where it can’t. An illustrated explanation might help you a bit more:\n\nIn case you didn’t know, tensors are matrices that can be stored in a GPU, and since they are matrices, all dimensions must have elements of the same size. Of course, this won’t happen when treating with tasks like NLP or different-sized images. Therefore, we use the so-called \"special tokens\". These tokens allow our model to know where the start of the sentence is (), where the end of the sentence is () and what elements are just there to fill up the remaining space so that our matrices have the sam sequence size (). These tokens must also be converted into their corresponding integer id (In our example they will be 2, 3, and 4 respectively). Padding a sequence looks something like this:\n\nTo tell our model that these tokens should be irrelevant, we use a binary matrix where there is a True value on the positions where the padding token is and False where it isn’t:\n\nTo create the two masking matrices we talked about, we need to extend our transformer model. If you know a bit of NumPy, you will have no problem understanding what these methods do. If you can’t understand it, I recommend opening a Jupyter notebook and going step by step to understand what they do.\n\nThe full extended model looks like this (note the change in the forward method as well):\n\nFor the sake of this project, I am going to create a set of fake data we can use to train our model. This data will be made up of sequences like:\n\nFeel free to skip to the next section if you aren’t interested in the data creation part.\n\nI won’t bother explaining what these functions do since they are pretty easy to understand with basic NumPy knowledge. I’ll create all the sentences of size 8 so I don’t need padding, and I’ll organize them randomly into batches of size 16:\n\nNow that we have data to work with, we can get to training our model. Let’s begin by creating an instance of our model, loss function, and optimizer. We will use the Stochastic Gradient Descent optimizer, the Cross-Entropy Loss function, and a learning rate of 0.01. I will also use my graphics card for this training since it will take less time, but it is not necessary.\n\nAn important concept we need to understand before continuing is that the target tensor we give as an input to the transformer must be shifted by one to the right (compared to the target output tensor). In other words, the tensor we want to give the model for training must have one extra element at the beginning and one less element at the end, and the tensor we compute the loss function with must be shifted in the other direction. This is so that if we give the model an element during inference, it gives us the next one.\n\nNow that we have grasped this concept, let’s get to coding! The training loop is a standard training loop except:\n• The target tensor is passed to the model during the prediction\n• A target mask is generated to hide the next words\n• A padding mask might be generated and passed to the model as well\n\nThe validation loop is exactly the same as our training loop except we don’t read or update gradients:\n\nIn this example, I am training the model for 10 epochs. To simplify the training I created a fit function that calls the train and validation loop every epoch and prints the loss:\n\nThis produces the following out\n\nAfter training we obtain the following losses per epoch:\n\nAs we can see, our model seems to have learned something. It is time to check if it has, but… how do we check it? We don’t have target tensors for data we have never seen. Here is where shifting our input target and output target tensor has an effect. As we saw before, our model learned to predict the next token when given an element. Therefore, we should be able to give our model the input tensor and the start token, and it should give us back the next element. If when the model predicts a token, we concatenate it with our previous input, we should slowly be able to add words to our output until our model predicts the token.\n\nHere is the code for that process:\n\nThe output of running this code is:\n\nSo the model has indeed gotten the gist of our sequences, but it still makes some mistakes when trying to predict the continuation. For example, in \"Example 4\", the model should predict a 1 as the first token, since the ending of the input is a 0. We can also see how during inference our sentences don’t need to have the same length, and the outputs will also not have the same length (see \"Example 5\").\n\nI believe this article can help a lot of beginner/intermediate Machine Learning developers learn how to work with transformer models in PyTorch, and, since the structure is the same in other languages, this tutorial is probably also useful for other frameworks like Tensorflow (hopefully).\n\nIf you have any suggestions or find any bugs feel free to leave a comment and I will fix it ASAP."
    }
]