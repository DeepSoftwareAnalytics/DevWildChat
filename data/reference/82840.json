[
    {
        "link": "https://pandas.pydata.org/docs/user_guide/missing_data.html",
        "document": "pandas uses different sentinel values to represent a missing (also referred to as NA) depending on the data type. for NumPy data types. The disadvantage of using NumPy data types is that the original data type will be coerced to or . for NumPy , , and . For typing applications, use . for , (and other bit widths), and . These types will maintain the original data type of the data. For typing applications, use . To detect these missing value, use the or methods. or will also consider a missing value. Equality compaisons between , , and do not act like Therefore, an equality comparison between a or with one of these missing values does not provide the same information as or .\n\nExperimental: the behaviour of can still change without warning. Starting from pandas 1.0, an experimental value (singleton) is available to represent scalar missing values. The goal of is provide a “missing” indicator that can be used consistently across data types (instead of , or depending on the data type). For example, when having missing values in a with the nullable integer dtype, it will use : Currently, pandas does not yet use those data types using by default a or , so you need to specify the dtype explicitly. An easy way to convert to those dtypes is explained in the conversion section. In general, missing values propagate in operations involving . When one of the operands is unknown, the outcome of the operation is also unknown. For example, propagates in arithmetic operations, similarly to : There are a few special cases when the result is known, even when one of the operands is . In equality and comparison operations, also propagates. This deviates from the behaviour of , where comparisons with always return . To check if a value is equal to , use An exception on this basic propagation rule are reductions (such as the mean or the minimum), where pandas defaults to skipping missing values. See the calculation section for more. For logical operations, follows the rules of the three-valued logic (or Kleene logic, similarly to R, SQL and Julia). This logic means to only propagate missing values when it is logically required. For example, for the logical “or” operation ( ), if one of the operands is , we already know the result will be , regardless of the other value (so regardless the missing value would be or ). In this case, does not propagate: On the other hand, if one of the operands is , the result depends on the value of the other operand. Therefore, in this case propagates: The behaviour of the logical “and” operation ( ) can be derived using similar logic (where now will not propagate if one of the operands is already ): Since the actual value of an NA is unknown, it is ambiguous to convert NA to a boolean value. Traceback (most recent call last) in : boolean value of NA is ambiguous This also means that cannot be used in a context where it is evaluated to a boolean, such as where can potentially be . In such cases, can be used to check for or being can be avoided, for example by filling missing values beforehand. A similar situation occurs when using or objects in statements, see Using if/truth statements with pandas. implements NumPy’s protocol. Most ufuncs work with , and generally return : Currently, ufuncs involving an ndarray and will return an object-dtype filled with NA values. The return type here may change to return a different array type in the future. See DataFrame interoperability with NumPy functions for more on ufuncs. If you have a or using , and in that can convert data to use the data types that use such as or . This is especially helpful after reading in data sets from IO methods where data types were inferred. In this example, while the dtypes of all columns are changed, we show the results for the first 10 columns.\n\nNA values can be replaced with corresponding value from a or where the index and column aligns between the original object and the filled object. can also be used to fill NA values.Same result as above. and fills NA values using various interpolation methods. Interpolation relative to a in the is available by setting If you have scipy installed, you can pass the name of a 1-d interpolation routine to . as specified in the scipy interpolation documentation and reference guide. The appropriate interpolation method will depend on the data type. If you are dealing with a time series that is growing at an increasing rate, use . If you have values approximating a cumulative distribution function, use . To fill missing values with goal of smooth plotting use . When interpolating via a polynomial or spline approximation, you must also specify the degree or order of the approximation: Interpolating new observations from expanding data with . accepts a keyword argument to limit the number of consecutive values filled since the last valid observation By default, values are filled in a direction. Use parameter to fill or from directions. By default, values are filled whether they are surrounded by existing valid values or outside existing valid values. The parameter restricts filling to either inside or outside values. # fill one consecutive inside value in both directions # fill all consecutive outside values in both directions and can be used similar to and to replace or insert missing values. Replacing more than one value is possible by passing a list. Python strings prefixed with the character such as are “raw” strings. They have different semantics regarding backslashes than strings without this prefix. Backslashes in raw strings will be interpreted as an escaped backslash, e.g., . Replace the ‘.’ with with regular expression that removes surrounding whitespace Pass nested dictionaries of regular expressions that use the keyword. Pass a list of regular expressions that will replace matches with a scalar. All of the regular expression examples can also be passed with the argument as the argument. In this case the argument must be passed explicitly by name or must be a nested dictionary. A regular expression object from is a valid input as well."
    },
    {
        "link": "https://pandas.pydata.org",
        "document": ""
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/index.html",
        "document": "The User Guide covers all of pandas by topic area. Each of the subsections introduces a topic (such as “working with missing data”), and discusses how pandas approaches the problem, with many examples throughout.\n\nUsers brand-new to pandas should start with 10 minutes to pandas.\n\nFor a high level summary of the pandas fundamentals, see Intro to data structures and Essential basic functionality.\n\nFurther information on any specific method can be obtained in the API reference.\n\nHow to read these guides# In these guides you will see input code inside code blocks such as: The first block is a standard python input, while in the second the indicates the input is inside a notebook. In Jupyter Notebooks the last line is printed and plots are shown inline."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/10min.html",
        "document": "This is a short introduction to pandas, geared mainly for new users. You can see more complex recipes in the Cookbook.\n\nCustomarily, we import as follows:\n\nSee the Intro to data structures section. Creating a by passing a NumPy array with a datetime index using and labeled columns: Creating a by passing a dictionary of objects where the keys are the column labels and the values are the column values. The columns of the resulting have different dtypes: If you’re using IPython, tab completion for column names (as well as public attributes) is automatically enabled. Here’s a subset of the attributes that will be completed: As you can see, the columns , , , and are automatically tab completed. and are there as well; the rest of the attributes have been truncated for brevity.\n\nUse and to view the top and bottom rows of the frame respectively: Return a NumPy representation of the underlying data with without the index or column labels: NumPy arrays have one dtype for the entire array while pandas DataFrames have one dtype per column. When you call , pandas will find the NumPy dtype that can hold all of the dtypes in the DataFrame. If the common data type is , will require copying data.\n\nWhile standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods, , , and . See the indexing documentation Indexing and Selecting Data and MultiIndex / Advanced Indexing. For a , passing a single label selects a columns and yields a equivalent to : See more in Selection by Label using or . For label slicing, both endpoints are included: For getting fast access to a scalar (equivalent to the prior method): See more in Selection by Position using or . Select via the position of the passed integers: For getting a value explicitly: For getting fast access to a scalar (equivalent to the prior method): Select rows where is greater than . Selecting values from a where a boolean condition is met: Setting a new column automatically aligns the data by the indexes: The result of the prior setting operations:\n\nFor NumPy data types, represents missing data. It is by default not included in computations. See the Missing Data section. Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data: drops any rows that have missing data: gets the boolean mask where values are :\n\nSee the Basic section on Binary Ops. Calculate the mean value for each column: Calculate the mean value for each row: Operating with another or with a different index or column will align the result with the union of the index or column labels. In addition, pandas automatically broadcasts along the specified dimension and will fill unaligned labels with . and applies a user defined function that reduces or broadcasts its result respectively. See more at Histogramming and Discretization. is equipped with a set of string processing methods in the attribute that make it easy to operate on each element of the array, as in the code snippet below. See more at Vectorized String Methods.\n\nBy “group by” we are referring to a process involving one or more of the following steps:\n• None Splitting the data into groups based on some criteria Grouping by a column label, selecting column labels, and then applying the function to the resulting groups:\n\nSee the sections on Hierarchical Indexing and Reshaping. The method “compresses” a level in the DataFrame’s columns: With a “stacked” DataFrame or Series (having a as the ), the inverse operation of is , which by default unstacks the last level: See the section on Pivot Tables. pivots a specifying the , and\n\npandas can include categorical data in a . For full docs, see the categorical introduction and the API documentation. Rename the categories to more meaningful names: Reorder the categories and simultaneously add the missing categories (methods under return a new by default): Sorting is per order in the categories, not lexical order: Grouping by a categorical column with also shows empty categories:\n\nIf you are attempting to perform a boolean operation on a or you might see an exception like: Traceback (most recent call last) in in : The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). See Comparisons and Gotchas for an explanation and what to do."
    },
    {
        "link": "https://pypi.org/project/pandas",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html",
        "document": "A random forest is a meta estimator that fits a number of decision tree regressors on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. Trees in the forest use the best split strategy, i.e. equivalent to passing to the underlying . The sub-sample size is controlled with the parameter if (default), otherwise the whole dataset is used to build each tree.\n\nFor a comparison between tree-based ensemble models see the example Comparing Random Forests and Histogram Gradient Boosting models.\n\nRead more in the User Guide.\n\nThe number of trees in the forest. Changed in version 0.22: The default value of changed from 10 to 100 in 0.22. The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error, which is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node, “friedman_mse”, which uses mean squared error with Friedman’s improvement score for potential splits, “absolute_error” for the mean absolute error, which minimizes the L1 loss using the median of each terminal node, and “poisson” which uses reduction in Poisson deviance to find splits. Training using “absolute_error” is significantly slower than when using “squared_error”. The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. The minimum number of samples required to split an internal node:\n• None If int, then consider as the minimum number.\n• None If float, then is a fraction and are the minimum number of samples for each split. The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n• None If int, then consider as the minimum number.\n• None If float, then is a fraction and are the minimum number of samples for each node. The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. The number of features to consider when looking for the best split:\n• None If int, then consider features at each split.\n• None If float, then is a fraction and features are considered at each split.\n• None If None or 1.0, then . The default of 1.0 is equivalent to bagged trees and more randomness can be achieved by setting smaller values, e.g. 0.3. Changed in version 1.1: The default of changed from to 1.0. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than features. Grow trees with in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following: where is the total number of samples, is the number of samples at the current node, is the number of samples in the left child, and is the number of samples in the right child. , , and all refer to the weighted sum, if is passed. Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. Whether to use out-of-bag samples to estimate the generalization score. By default, is used. Provide a callable with signature to use a custom metric. Only available if . The number of jobs to run in parallel. , , and are all parallelized over the trees. means 1 unless in a context. means using all processors. See Glossary for more details. Controls both the randomness of the bootstrapping of the samples used when building trees (if ) and the sampling of the features to consider when looking for the best split at each node (if ). See Glossary for details. Controls the verbosity when fitting and predicting. When set to , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. See Glossary and Fitting additional trees for details. Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than will be chosen. By default, no pruning is performed. See Minimal Cost-Complexity Pruning for details. See Post pruning decision trees with cost complexity pruning for an example of such pruning. If bootstrap is True, the number of samples to draw from X to train each base estimator.\n• None If None (default), then draw samples.\n• None If float, then draw samples. Thus, should be in the interval . Indicates the monotonicity constraint to enforce on each feature. If monotonic_cst is None, no constraints are applied. Monotonicity constraints are not supported for: Read more in the User Guide. The child estimator template used to create the collection of fitted sub-estimators. Added in version 1.2: was renamed to . Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings. The number of outputs when is performed. Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when is True. Prediction computed with out-of-bag estimate on the training set. This attribute exists only when is True. The subset of drawn samples for each base estimator.\n\nThe default values for the parameters controlling the size of the trees (e.g. , , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.\n\nThe features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, and , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, has to be fixed.\n\nThe default value uses rather than . The latter was originally suggested in [1], whereas the former was more recently justified empirically in [2].\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nThis example shows the use of a forest of trees to evaluate the importance of features on an artificial classification task. The blue bars are the feature importances of the forest, along with their inter-trees variability represented by the error bars.\n\nAs expected, the plot suggests that 3 features are informative, while the remaining are not.\n\nWe generate a synthetic dataset with only 3 informative features. We will explicitly not shuffle the dataset to ensure that the informative features will correspond to the three first columns of X. In addition, we will split our dataset into training and testing subsets. A random forest classifier will be fitted to compute the feature importances. In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. \n\nOn GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. \n\n \n\n\n\nFeature importance based on mean decrease in impurity# Feature importances are provided by the fitted attribute and they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree. Impurity-based feature importances can be misleading for high cardinality features (many unique values). See Permutation feature importance as an alternative below. We observe that, as expected, the three first features are found important."
    },
    {
        "link": "https://stackoverflow.com/questions/56725486/plot-feature-importance-in-randomforestregressor-sklearn",
        "document": "I Am new in Data Science. I am trying to find out the feature importance ranking for my dataset. I already applied Random forest and got the output.\n\nHere is my code:\n\nIn the importance part i almost copied the example shown in : https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n\nHere is the code:\n\nI am expecting the output shown in the documentation. Can Anyone Help me please ? Thanks in Advance.\n\nMy dataset is here:"
    },
    {
        "link": "https://scikit-learn.org/stable/api/index.html",
        "document": ""
    },
    {
        "link": "https://mljar.com/blog/feature-importance-in-random-forest",
        "document": "The feature importance (variable importance) describes which features are relevant. It can help with better understanding of the solved problem and sometimes lead to model improvements by employing the feature selection. In this post, I will present 3 ways (with code examples) how to compute feature importance for the Random Forest algorithm from package (in Python).\n\nYou will learn how to compute and plot:\n\nThe Random Forest algorithm has built-in feature importance which can be computed in two ways:\n• Gini importance (or mean decrease impurity), which is computed from the Random Forest structure. Let's look how the Random Forest is constructed. It is a set of Decision Trees. Each Decision Tree is a set of internal nodes and leaves. In the internal node, the selected feature is used to make decision how to divide the data set into two separate sets with similars responses within. The features for internal nodes are selected with some criterion, which for classification tasks can be gini impurity or infomation gain, and for regression is variance reduction. We can measure how each feature decrease the impurity of the split (the feature with highest decrease is selected for internal node). For each feature we can collect how on average it decreases the impurity. The average over all trees in the forest is the measure of the feature importance. This method is available in implementation of the Random Forest (for both classifier and regressor). It is worth to mention, that in this method we should look at relative values of the computed importances. This biggest advantage of this method is a speed of computation - all needed values are computed during the Radom Forest training. The drawbacks of the method is to tendency to prefer (select as important) numerical features and categorical features with high cardinality. What is more, in the case of correlated features it can select one of the feature and neglect the importance of the second one (which can lead to wrong conclusions).\n• Mean Decrease Accuracy - is a method of computing the feature importance on permuted out-of-bag (OOB) samples based on mean decrease in the accuracy. This method is not implemented in the package. The very similar to this method is permutation based importance described below in this post.\n\nI will show how to compute feature importance for the Random Forest with package and Boston dataset (house price regression task).\n\nLoad the data set and split for training and testing.\n\nTo get the feature importances from the Random Forest model use the attribute:\n\nLet's plot the importances (chart will be easier to interpret than values).\n\nTo have even better chart, let's sort the features, and plot again:\n\nThe permutation based importance can be used to overcome drawbacks of default feature importance computed with mean impurity decrease. It is implemented in as method. As arguments it requires trained model (can be any model compatible with API) and validation (test data). This method will randomly shuffle each feature and compute the change in the model's performance. The features which impact the performance the most are the most important one.\n\nThe permutation importance can be easily computed:\n\nThe permutation based importance is computationally expensive. The permutation based method can have problem with highly-correlated features, it can report them as unimportant.\n\nThe SHAP interpretation can be used (it is model-agnostic) to compute the feature importances from the Random Forest. It is using the Shapley values from game theory to estimate the how does each feature contribute to the prediction. It can be easily installed ( ) and used with Random Forest:\n\nTo plot feature importance as the horizontal bar plot we need to use method:\n\nThe feature importance can be plotted with more details, showing the feature value:\n\nThe computing feature importances with SHAP can be computationally expensive. However, it can provide more information like decision plots or dependence plots.\n\nThe 3 ways to compute the feature importance for the Random Forest were presented:\n\nIn my opinion, it is always good to check all methods, and compare the results. I'm using permutation and SHAP based methods in MLJAR's AutoML open-source package . I'm using them becasue they are model-agnostic and works well with algorithms not from : Xgboost, Neural Networks (keras+tensorflow), LigthGBM, CatBoost.\n\nMaybe you will find interesting article about the Random Forest Regressor and when does it fail and why?\n• The more accurate model is, the more trustworthy computed importances are.\n• The computed importances describe how important features are for the machine learning model. It is an approximation of how important features are in the data."
    },
    {
        "link": "https://stackoverflow.com/questions/12444716/how-do-i-set-the-figure-title-and-axes-labels-font-size",
        "document": "Globally setting font sizes via should be done with\n\nThe defaults can be restored using\n\nYou can also do this by creating a style sheet in the directory under the matplotlib configuration directory (you can get your configuration directory from ). The style sheet format is\n\nIf you have a style sheet at then you can use it via\n\nYou can also create (or modify) a matplotlibrc file which shares the format\n\nDepending on which matplotlibrc file you modify these changes will be used for only the current working directory, for all working directories which do not have a matplotlibrc file, or for all working directories which do not have a matplotlibrc file and where no other matplotlibrc file has been specified. See this section of the customizing matplotlib page for more details.\n\nA complete list of the keys can be retrieved via , but for adjusting font sizes you have (italics quoted from here)\n• - Fontsize of the x and y labels\n• - Fontsize for legend titles, sets to the same as the default axes. See this answer for usage example.\n\nall of which accept string sizes or a in . The string sizes are defined relative to the default font size which is specified by\n• - the default font size for text, given in pts. 10 pt is the standard value\n\nAdditionally, the weight can be specified (though only for the default it appears) by\n• - The default weight of the font used by . Accepts or (400), (700), , and (relative with respect to current weight)."
    },
    {
        "link": "https://matplotlib.org/stable/users/explain/customizing.html",
        "document": "## NOTE FOR END USERS: DO NOT EDIT THIS FILE! ## This is a sample Matplotlib configuration file - you can find a copy ## of it on your system in site-packages/matplotlib/mpl-data/matplotlibrc ## If you wish to change your default style, copy this file to one of the ## for more details on the paths which are checked for the configuration file. ## Blank lines, or lines starting with a comment symbol, are ignored, as are ## trailing comments. Other lines must have the format: ## Formatting: Use PEP8-like style (as enforced in the rest of the codebase). ## All lines start with an additional '#', so that removing all leading '#'s ## Colors: for the color values below, you can either use ## - a Matplotlib color string, such as r, k, or b ## - an RGB tuple, such as (1.0, 0.5, 0.0) ## The unquoted string ff00ff is also supported for backward ## String values may optionally be enclosed in double quotes, which allows ## using the comment character # in the string. ## This file (and other style files) must be encoded as utf-8. ## Matplotlib configuration are currently divided into following parts: ## The default backend. If you omit this parameter, the first working ## backend from the following list is used: ## You can also deploy your own backend outside of Matplotlib by referring to ## the module name (which must be in the PYTHONPATH) as 'module://my_backend'. ## The port to use for the web server in the WebAgg backend. ## The address on which the WebAgg web server should be reachable ## If webagg.port is unavailable, a number of other random ports will ## be tried until one that is available is found. ## When True, open the web browser to the plot that is shown ## If you are running pyplot inside a GUI and your backend choice ## conflicts, we will automatically try to find a compatible one for ## you if backend_fallback is True ## for more information on line properties. #lines.color: C0 # has no affect on plot(); see axes.prop_cycle #lines.markeredgewidth: 1.0 # the line width around the marker symbol ## The three standard dash patterns. These are scaled by the linewidth. #pcolormesh.snap: True # Whether to snap the mesh to pixel boundaries. This is # provided solely to allow old test images to remain # unchanged. Set to False to obtain the previous behavior. ## Patches are graphical objects that fill 2D space, like polygons or circles. ## for more information on patch properties. #patch.edgecolor: black # By default, Patches and Collections do not draw edges. # This value is only used if facecolor is \"none\" # (an Artist without facecolor and edgecolor would be # invisible) or if patch.force_edgecolor is True. #patch.force_edgecolor: False # By default, Patches and Collections do not draw edges. # Set this to True to draw edges with patch.edgedcolor # This is mainly relevant for styles. ## The font properties used by `text.Text`. ## See https://matplotlib.org/stable/api/font_manager_api.html for more information ## on font properties. The 6 font properties used for font matching are ## given below with their default values. ## The font.family property can take either a single or multiple entries of any ## combination of concrete font names (not supported when rendering text with ## usetex) or the following five generic values: ## Each of these values has a corresponding default list of font names ## (font.serif, etc.); the first available font in the list is used. Note that ## for font.serif, font.sans-serif, and font.monospace, the first element of ## the list (a DejaVu font) will always be used because DejaVu is shipped with ## Matplotlib and is thus guaranteed to be available; the other entries are ## left as examples of other possible values. ## The font.style property has three values: normal (or roman), italic ## or oblique. The oblique style will be used for italic, if it is not ## The font.variant property has two values: normal or small-caps. For ## TrueType fonts, which are scalable fonts, small-caps is equivalent ## to using a font size of 'smaller', or about 83 % of the current font ## bolder, lighter, 100, 200, 300, ..., 900. Normal is the same as ## 400, and bold is 700. bolder and lighter are relative values with ## property is not currently implemented. ## The font.size property is the default font size for text, given in points. ## 10 pt is the standard value. ## special text sizes tick labels, axes, labels, title, etc., see the rc ## settings for axes and ticks. Special text sizes can be defined ## relative to font.size, using the following values: xx-small, x-small, ## The text properties used by `text.Text`. ## for more information on text properties ## FreeType hinting flag (\"foo\" corresponds to FT_LOAD_FOO); may be one of the ## following (Proprietary Matplotlib-specific synonyms are given in parentheses, ## but their use is discouraged): ## - default: Use the font's native hinter if possible, else FreeType's auto-hinter. ## - no_autohint: Use the font's native hinter if possible, else don't hint. #text.hinting_factor: 8 # Specifies the amount of softness for hinting in the # horizontal direction. A value of 1 will hint to full # pixels. A value of 2 will hint to half pixels etc. #text.kerning_factor: 0 # Specifies the scaling factor for kerning values. This # is provided solely to allow old test images to remain # Values other than 0 or 6 have no defined meaning. #text.antialiased: True # If True (default), the text will be antialiased. #text.parse_math: True # Use mathtext if there is an even number of unescaped ## For more information on LaTeX properties, see #text.usetex: False # use latex for all text handling. The following fonts # are supported through the usual rc parameter settings: #text.latex.preamble: # IMPROPER USE OF THIS FEATURE WILL LEAD TO LATEX FAILURES # AND IS THEREFORE UNSUPPORTED. PLEASE DO NOT ASK FOR HELP # IF THIS FEATURE DOES NOT DO WHAT YOU EXPECT IT TO. # text.latex.preamble is a single line of LaTeX code that # will be passed on to the LaTeX system. It may contain # any code that is valid for the LaTeX \"preamble\", i.e. # between the \"\\documentclass\" and \"\\begin{document}\" # Note that it has to be put on a single line, which may # The following packages are always loaded with usetex, # PostScript (PSNFSS) font packages may also be ## The following settings allow you to select the fonts in math mode. ## \"mathtext.fontset: custom\" is defined by the mathtext.bf, .cal, .it, ... ## settings which map a TeX font name to a fontconfig font pattern. (These ## settings are not used for other font sets.) # 'stixsans'] when a symbol cannot be found in one of the # custom math fonts. Select 'None' to not perform fallback # and replace the missing character by a dummy symbol. #mathtext.default: it # The default font to use for math. # Can be any of the LaTeX font names, including # the special name \"regular\" for the same font ## Following are default face and edge colors, default tick sizes, ## default font sizes for tick labels, and so on. See #axes.grid.axis: both # which axis the grid should apply to #axes.titlelocation: center # alignment of the title: {left, right, center} #axes.titlecolor: auto # color of the axes title, auto falls back to #axes.titlepad: 6.0 # pad between axes and title in points #axes.labelsize: medium # font size of the x and y labels #axes.labelweight: normal # weight of the x and y labels # - above patches but below lines ('line') # of the axis range is smaller than the # first or larger than the second # according to the user's locale. # For example, use ',' as a decimal #axes.formatter.use_mathtext: False # When True, use mathtext for scientific # to an offset when the data range is #axes.formatter.offset_threshold: 4 # When useoffset is True, the offset # will be used when it can remove # at least this number of significant #axes.unicode_minus: True # use Unicode for the minus symbol rather than hyphen. See # color cycle for plot lines as list of string color specs: # As opposed to all other parameters in this file, the color # values must be enclosed in quotes for this parameter, # for more details on prop_cycle usage. #axes.autolimit_mode: data # If \"data\", use axes.xmargin and axes.ymargin as is. # If \"round_numbers\", after application of margins, axis # limits are further expanded to the nearest \"round\" number. #axes3d.trackballsize: 0.667 # trackball diameter, in units of the Axes bbox #axes3d.trackballborder: 0.2 # trackball border width, in units of the Axes bbox (only for 'sphere' and 'arcball' style) #xaxis.labellocation: center # alignment of the xaxis label: {left, right, center} ## These control the default format strings used in AutoDateFormatter. ## Any valid format datetime format string can be used (see the python ## `datetime` for details). For example, by using: ## - '%x' will use the locale date representation ## - '%X' will use the locale time representation ## - '%c' will use the full locale datetime representation ## These values map to the scales: ## For auto converter whether to use interval_multiples: #xtick.minor.pad: 3.4 # distance to the minor tick label in points #xtick.labelcolor: inherit # color of the tick labels or inherit from xtick.color #xtick.direction: out # direction: {in, out, inout} #xtick.minor.ndivs: auto # number of minor ticks between the major ticks on x-axis #ytick.right: False # draw ticks on the right side #ytick.labelright: False # draw tick labels on the right side #ytick.minor.pad: 3.4 # distance to the minor tick label in points #ytick.labelcolor: inherit # color of the tick labels or inherit from ytick.color #ytick.direction: out # direction: {in, out, inout} #ytick.minor.ndivs: auto # number of minor ticks between the major ticks on y-axis #legend.frameon: True # if True, draw the legend on a background patch #legend.fancybox: True # if True, use a rounded box for the #legend.numpoints: 1 # the number of marker points in the legend line #legend.markerscale: 1.0 # the relative size of legend markers vs. original #legend.title_fontsize: None # None sets to the same as the default axes. #legend.labelspacing: 0.5 # the vertical space between the legend entries #legend.handlelength: 2.0 # the length of the legend lines #legend.handleheight: 0.7 # the height of the legend handle #legend.handletextpad: 0.8 # the space between the legend line and legend text #legend.borderaxespad: 0.5 # the border between the axes and legend edge #figure.max_open_warning: 20 # The maximum number of figures to open through # If less than one this feature is disabled. #figure.raise_window : True # Raise the GUI window to front when show() is called. ## The figure subplot parameters. All dimensions are a fraction of the figure width and height. #figure.subplot.left: 0.125 # the left side of the subplots of the figure #figure.subplot.right: 0.9 # the right side of the subplots of the figure #figure.subplot.bottom: 0.11 # the bottom of the subplots of the figure #figure.subplot.top: 0.88 # the top of the subplots of the figure #figure.subplot.wspace: 0.2 # the amount of width reserved for space between subplots, # expressed as a fraction of the average axis width #figure.subplot.hspace: 0.2 # the amount of height reserved for space between subplots, # expressed as a fraction of the average axis height # parameters to make the plot fit the figure # elements fit on the figure. (Not ## Padding (in inches) around axes; defaults to 3/72 inches, i.e. 3 points. ## Spacing between subplots, relative to the subplot sizes. Much smaller than for #image.lut: 256 # the size of the colormap lookup table #image.composite_image: True # When True, all the images on a set of axes are #contour.linewidth: None # {float, None} Size of the contour line # widths. If set to None, it falls back to #errorbar.capsize: 0 # length of end cap on error bars in pixels #hist.bins: 10 # The default number of histogram bins or 'auto'. #agg.path.chunksize: 0 # 0 to disable; values in the range # especially if they are very gappy. # It may cause minor artifacts, though. # A value of 20000 is probably a good #path.simplify_threshold: 0.111111111111 # The threshold of similarity below # which vertices will be removed in #path.snap: True # When True, rectilinear axis-aligned paths will be snapped # to the nearest pixel when certain criteria are met. # When False, paths will never be snapped. #path.sketch: None # May be None, or a tuple of the form: # - *scale* is the amplitude of the wiggle # perpendicular to the line (in pixels). # - *length* is the length of the wiggle along the # - *randomness* is the factor by which the length is ## The default savefig parameters can be different from the display parameters ## e.g., you may want a higher resolution, or to make the figure #savefig.pad_inches: 0.1 # padding to be used, when bbox is set to 'tight' #savefig.directory: ~ # default directory in savefig dialog, gets updated after # interactive saves, unless set to the empty string (i.e. # the current directory); use '.' to start at the current #savefig.transparent: False # whether figures are saved with a transparent #savefig.orientation: portrait # orientation of saved figure, for PostScript output only #macosx.window_mode : system # How to open new figures (system, tab, window) # None: Assume fonts are installed on the # machine where the SVG will be viewed. #svg.hashsalt: None # If not None, use this string as hash salt instead of uuid4 #svg.id: None # If not None, use this string as the value for the `id` ## See https://matplotlib.org/stable/tutorials/text/pgf.html for more information. #docstring.hardcopy: False # set this when you want to generate hardcopy docstring ## Event keys to interact with figures/plots via keyboard. ## See https://matplotlib.org/stable/users/explain/interactive.html for more ## details on interactive navigation. Customize these settings according to ## your needs. Leave the field(s) empty if you don't need a key-map. (i.e., #animation.html: none # How to display the animation as HTML in #animation.codec: h264 # Codec to use for writing movie ## Path to ffmpeg binary. Unqualified paths are resolved by subprocess.Popen. ## Path to ImageMagick's convert binary. Unqualified paths are resolved by ## subprocess.Popen, except that on Windows, we look up an install of ## ImageMagick in the registry (as convert is also the name of a system tool). #animation.embed_limit: 20.0 # Limit, in MB, of size of base64 encoded"
    },
    {
        "link": "https://stackoverflow.com/questions/3899980/how-to-change-the-font-size-on-a-matplotlib-plot",
        "document": "This answer is for anyone trying to change all the fonts, including for the legend, and for anyone trying to use different fonts and sizes for each thing. It does not use rc (which doesn't seem to work for me). It is perhaps a bit cumbersome but I could not get to grips with any other method, personally.\n\nI have worked out a slightly different, less cluttered approach than my original answer below. It allows any font on your system, even fonts. To have separate fonts for each thing, just write more and like variables.\n\nThis basically combines ryggyr's answer here with other answers on SO.\n\nBy having several font dictionaries, you can choose different fonts/sizes/weights/colours for the various titles, choose the font for the tick labels, and choose the font for the legend, all independently."
    },
    {
        "link": "https://matplotlib.org/stable/users/explain/text/text_intro.html",
        "document": "Go to the end to download the full example code.\n\nMatplotlib has extensive text support, including support for mathematical expressions, truetype support for raster and vector outputs, newline separated text with arbitrary rotations, and Unicode support.\n\nBecause it embeds fonts directly in output documents, e.g., for postscript or PDF, what you see on the screen is what you get in the hardcopy. FreeType support produces very nice, antialiased fonts, that look good even at small raster sizes. Matplotlib includes its own (thanks to Paul Barrett), which implements a cross platform, W3C compliant font finding algorithm.\n\nThe user has a great deal of control over text properties (font size, font weight, text location and color, etc.) with sensible defaults set in the rc file. And significantly, for those interested in mathematical or scientific figures, Matplotlib implements a large number of TeX math symbols and commands, supporting mathematical expressions anywhere in your figure."
    },
    {
        "link": "https://earthdatascience.org/courses/scientists-guide-to-plotting-data-in-python/plot-with-matplotlib/introduction-to-matplotlib-plots/customize-plot-colors-labels-matplotlib",
        "document": "Previously in this chapter, you learned how to create your and objects using the function from pyplot (which you imported using the alias ):\n\nNow you know how to create basic plots using matplotlib, you can begin adding data to the plots in your figure.\n\nBegin by importing the module with the alias and creating a few lists to plot the monthly average precipitation (inches) for Boulder, Colorado provided by the U.S. National Oceanic and Atmospheric Administration (NOAA).\n\nYou can add data to your plot by calling the desired object, which is the axis element that you previously defined with:\n\nYou can call the method of the object and specify the arguments for the x axis (horizontal axis) and the y axis (vertical axis) of the plot as follows:\n\nIn this example, you are adding data from lists that you previously defined, with months along the x axis and boulder_monthly_precip along the y axis.\n\nData Tip: Note that the data plotted along the x and y axes can also come from numpy arrays as well as rows or columns in a pandas dataframes.\n\nNote that the output displays the object type as well as the unique identifier (or the memory location) for the figure.\n\nYou can hide this information from the output by adding as the last line you call in your plot code.\n\nNote that the object that you created above can actually be called anything that you want; for example, you could decide you wanted to call it !\n\nHowever, it is not good practice to use random names for objects such as .\n\nThe convention in the Python community is to use to name the object, but it is good to know that objects in Python do not have to be named something specific.\n\nYou simply need to use the same name to call the object that you want, each time that you call it.\n\nFor example, if you did name the object when you created it, then you would use the same name to call the object when you want to add data to it.\n\nCreate Different Types of Matplotlib Plots: Scatter and Bar Plots\n\nYou may have noticed that by default, creates the plot as a line plot (meaning that all of the values are connected by a continuous line across the plot).\n\nYou can also use the object to create:\n• scatter plots (using ): values are displayed as individual points that are not connected with a continuous line.\n• bar plots (using ): values are displayed as bars with height indicating the value at a specific point.\n\nYou can customize and add more information to your plot by adding a plot title and labels for the axes using the , , arguments within the method:\n\nYou can also create titles and axes labels with have multiple lines of text using the new line character between two words to identity the start of the new line.\n\nYou can use to set properties in your plot, such as customizing labels including the tick labels.\n\nIn the example below, grabs the tick labels from the x axis, and then the argument specifies an angle of rotation (e.g. 45), so that the tick labels along the x axis are rotated 45 degrees.\n\nYou can change the point marker type in your line or scatter plot using the argument and setting it equal to the symbol that you want to use to identify the points in the plot.\n\nFor example, will display the point markers as a pixel or box, and “o” will display point markers as a circle.\n\nVisit the Matplotlib documentation for a list of marker types.\n\nYou can customize the color of your plot using the argument and setting it equal to the color that you want to use for the plot.\n\nA list of some of the base color options available in matplotlib is below:\n\nFor these base colors, you can set the argument equal to the full name (e.g. ) or simply just the key letter as shown in the table above (e.g. ).\n\nData Tip: For more colors, visit the matplotlib documentation on color.\n\nYou can also adjust the transparency of color using the argument, with values closer to 0.0 indicating a higher transparency.\n\nYou can customize your bar plot further by changing the outline color for each bar to be blue using the argument and specifying a color from the matplotlib color options previously discussed.\n\nWhen using scatter plots, you can also assign each point a color based upon its data value using the and arguments.\n\nThe argument allows you to specify the sequence of values that will be color-mapped (e.g. ), while allows you to specify the color map to use for the sequence.\n\nThe example below uses the colormap, in which lower values are filled in with yellow to green shades, while higher values are filled in with increasingly darker shades of blue.\n\nData Tip: To see a list of color map options, visit the matplotlib documentation on colormaps.\n\nRecall that matplotlib’s object oriented approach makes it easy to include more than one plot in a figure by creating additional objects:\n\nOnce you have your and two objects defined, you can add data to each and define the plot with unique characteristics.\n\nIn the example below, creates a customized bar plot in the first plot, and creates a customized scatter in the second plot.\n\nYou can continue to add to and such as adding the title and axes labels for each individual plot, just like you did before when the figure only had one plot.\n\nYou can use to define these elements for the first plot (the bar plot), and to define them for the second plot (the scatter plot).\n\nNow that you have more than one plot (each with their own labels), you can also add an overall title (with a specified font size) for the entire figure using:\n\nYou can easily save a figure to an image file such as .png using:\n\nwhich will save the latest figure rendered.\n\nIf you do not specify a path for the file, the file will be created in your current working directory.\n\nReview the Matplotlib documentation to see a list of the additional file formats that be used to save figures."
    }
]