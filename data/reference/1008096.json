[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html",
        "document": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by:\n\nThis transformation is often used as an alternative to zero mean, unit variance scaling.\n\ndoesn’t reduce the effect of outliers, but it linearly scales them down into a fixed range, where the largest occurring data point corresponds to the maximum value and the smallest one corresponds to the minimum value. For an example visualization, refer to Compare MinMaxScaler with other scalers.\n\nRead more in the User Guide.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform."
    },
    {
        "link": "https://scikit-learn.org/1.0/modules/generated/sklearn.preprocessing.MinMaxScaler.html",
        "document": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by:\n\nThis transformation is often used as an alternative to zero mean, unit variance scaling.\n\nRead more in the User Guide.\n\nSet to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array). Set to True to clip transformed values of held-out data to provided . Per feature adjustment for minimum. Equivalent to Per feature relative scaling of the data. Equivalent to Per feature minimum seen in the data Per feature maximum seen in the data Per feature range seen in the data Number of features seen during fit. The number of samples processed by the estimator. It will be reset on new calls to fit, but increments across calls. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform.\n\nFor a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py.\n\nCompute the minimum and maximum to be used for later scaling. Fit to data, then transform it. Get parameters for this estimator. Undo the scaling of X according to feature_range. Online computation of min and max on X for later scaling. Set the parameters of this estimator. Scale features of X according to feature_range."
    },
    {
        "link": "https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python",
        "document": "Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n\nThis includes algorithms that use a weighted sum of the input, like linear regression, and algorithms that use distance measures, like k-nearest neighbors.\n\nThe two most popular techniques for scaling numerical data prior to modeling are normalization and standardization. Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision. Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.\n\nIn this tutorial, you will discover how to use scaler transforms to standardize and normalize numerical input variables for classification and regression.\n\nAfter completing this tutorial, you will know:\n• Data scaling is a recommended pre-processing step when working with many machine learning algorithms.\n• Data scaling can be achieved by normalizing or standardizing real-valued input and output variables.\n• How to apply standardization and normalization to improve the performance of predictive modeling algorithms.\n\nKick-start your project with my new book Data Preparation for Machine Learning, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into six parts; they are:\n• The Scale of Your Data Matters\n\nThe Scale of Your Data Matters\n\nMachine learning models learn a mapping from input variables to an output variable.\n\nAs such, the scale and distribution of the data drawn from the domain may be different for each variable.\n\nInput variables may have different units (e.g. feet, kilometers, and hours) that, in turn, may mean the variables have different scales.\n\nDifferences in the scales across input variables may increase the difficulty of the problem being modeled. An example of this is that large input values (e.g. a spread of hundreds or thousands of units) can result in a model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher generalization error.\n\nThis difference in scale for input variables does not affect all machine learning algorithms.\n\nFor example, algorithms that fit a model that use a weighted sum of input variables are affected, such as linear regression, logistic regression, and artificial neural networks (deep learning).\n\nAlso, algorithms that use distance measures between examples or exemplars are affected, such as k-nearest neighbors and support vector machines. There are also algorithms that are unaffected by the scale of numerical input variables, most notably decision trees and ensembles of trees, like random forest.\n\nIt can also be a good idea to scale the target variable for regression predictive modeling problems to make the problem easier to learn, most notably in the case of neural network models. A target variable with a large spread of values, in turn, may result in large error gradient values causing weight values to change dramatically, making the learning process unstable.\n\nScaling input and output variables is a critical step in using neural network models.\n\nBoth normalization and standardization can be achieved using the scikit-learn library.\n\nLet’s take a closer look at each in turn.\n\nNormalization is a rescaling of the data from the original range so that all values are within the new range of 0 and 1.\n\nNormalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data.\n\nA value is normalized as follows:\n\nWhere the minimum and maximum values pertain to the value x being normalized.\n\nFor example, for a dataset, we could guesstimate the min and max observable values as 30 and -10. We can then normalize any value, like 18.8, as follows:\n\nYou can see that if an x value is provided that is outside the bounds of the minimum and maximum values, the resulting value will not be in the range of 0 and 1. You could check for these observations prior to making predictions and either remove them from the dataset or limit them to the pre-defined maximum or minimum values.\n\nYou can normalize your dataset using the scikit-learn object MinMaxScaler.\n\nGood practice usage with the MinMaxScaler and other scaling techniques is as follows:\n• Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the fit() function.\n• Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling the transform() function.\n• Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions.\n\nThe default scale for the MinMaxScaler is to rescale variables into the range [0,1], although a preferred scale can be specified via the “feature_range” argument and specify a tuple, including the min and the max for all variables.\n\nWe can demonstrate the usage of this class by converting two variables to a range 0-to-1, the default range for normalization. The first variable has values between about 4 and 100, the second has values between about 0.1 and 0.001.\n\nThe complete example is listed below.\n\nRunning the example first reports the raw dataset, showing 2 columns with 4 rows. The values are in scientific notation which can be hard to read if you’re not used to it.\n\nNext, the scaler is defined, fit on the whole dataset and then used to create a transformed version of the dataset with each column normalized independently. We can see that the largest raw value for each column now has the value 1.0 and the smallest value for each column now has the value 0.0.\n\nNow that we are familiar with normalization, let’s take a closer look at standardization.\n\nStandardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.\n\nThis can be thought of as subtracting the mean value or centering the data.\n\nLike normalization, standardization can be useful, and even required in some machine learning algorithms when your data has input values with differing scales.\n\nStandardization assumes that your observations fit a Gaussian distribution (bell curve) with a well-behaved mean and standard deviation. You can still standardize your data if this expectation is not met, but you may not get reliable results.\n\nStandardization requires that you know or are able to accurately estimate the mean and standard deviation of observable values. You may be able to estimate these values from your training data, not the entire dataset.\n\nSubtracting the mean from the data is called centering, whereas dividing by the standard deviation is called scaling. As such, the method is sometime called “center scaling“.\n\nA value is standardized as follows:\n\nWhere the mean is calculated as:\n\nAnd the standard_deviation is calculated as:\n\nWe can guesstimate a mean of 10.0 and a standard deviation of about 5.0. Using these values, we can standardize the first value of 20.7 as follows:\n\nThe mean and standard deviation estimates of a dataset can be more robust to new data than the minimum and maximum.\n\nYou can standardize your dataset using the scikit-learn object StandardScaler.\n\nWe can demonstrate the usage of this class by converting two variables to a range 0-to-1 defined in the previous section. We will use the default configuration that will both center and scale the values in each column, e.g. full standardization.\n\nThe complete example is listed below.\n\nRunning the example first reports the raw dataset, showing 2 columns with 4 rows as before.\n\nNext, the scaler is defined, fit on the whole dataset and then used to create a transformed version of the dataset with each column standardized independently. We can see that the mean value in each column is assigned a value of 0.0 if present and the values are centered around 0.0 with values both positive and negative.\n\nNext, we can introduce a real dataset that provides the basis for applying normalization and standardization transforms as a part of modeling.\n\nThe sonar dataset is a standard machine learning dataset for binary classification.\n\nIt involves 60 real-valued inputs and a two-class target variable. There are 208 examples in the dataset and the classes are reasonably balanced.\n\nA baseline classification algorithm can achieve a classification accuracy of about 53.4 percent using repeated stratified 10-fold cross-validation. Top performance on this dataset is about 88 percent using repeated stratified 10-fold cross-validation.\n\nThe dataset describes radar returns of rocks or simulated mines.\n\nYou can learn more about the dataset from here:\n\nNo need to download the dataset; we will download it automatically from our worked examples.\n\nFirst, let’s load and summarize the dataset. The complete example is listed below.\n\nRunning the example first summarizes the shape of the loaded dataset.\n\nThis confirms the 60 input variables, one output variable, and 208 rows of data.\n\nA statistical summary of the input variables is provided showing that values are numeric and range approximately from 0 to 1.\n\nFinally, a histogram is created for each input variable.\n\nIf we ignore the clutter of the plots and focus on the histograms themselves, we can see that many variables have a skewed distribution.\n\nThe dataset provides a good candidate for using scaler transforms as the variables have differing minimum and maximum values, as well as different data distributions.\n\nNext, let’s fit and evaluate a machine learning model on the raw dataset.\n\nWe will use a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated stratified k-fold cross-validation. The complete example is listed below.\n\nRunning the example evaluates a KNN model on the raw sonar dataset.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWe can see that the model achieved a mean classification accuracy of about 79.7 percent, showing that it has skill (better than 53.4 percent) and is in the ball-park of good performance (88 percent).\n\nNext, let’s explore a scaling transform of the dataset.\n\nWe can apply the MinMaxScaler to the Sonar dataset directly to normalize the input variables.\n\nWe will use the default configuration and scale values to the range 0 and 1. First, a MinMaxScaler instance is defined with default hyperparameters. Once defined, we can call the fit_transform() function and pass it to our dataset to create a transformed version of our dataset.\n\nLet’s try it on our sonar dataset.\n\nThe complete example of creating a MinMaxScaler transform of the sonar dataset and plotting histograms of the result is listed below.\n\nRunning the example first reports a summary of each input variable.\n\nWe can see that the distributions have been adjusted and that the minimum and maximum values for each variable are now a crisp 0.0 and 1.0 respectively.\n\nHistogram plots of the variables are created, although the distributions don’t look much different from their original distributions seen in the previous section.\n\nNext, let’s evaluate the same KNN model as the previous section, but in this case, on a MinMaxScaler transform of the dataset.\n\nThe complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example, we can see that the MinMaxScaler transform results in a lift in performance from 79.7 percent accuracy without the transform to about 81.3 percent with the transform.\n\nNext, let’s explore the effect of standardizing the input variables.\n\nWe can apply the StandardScaler to the Sonar dataset directly to standardize the input variables.\n\nWe will use the default configuration and scale values to subtract the mean to center them on 0.0 and divide by the standard deviation to give the standard deviation of 1.0. First, a StandardScaler instance is defined with default hyperparameters.\n\nOnce defined, we can call the fit_transform() function and pass it to our dataset to create a transformed version of our dataset.\n\nLet’s try it on our sonar dataset.\n\nThe complete example of creating a StandardScaler transform of the sonar dataset and plotting histograms of the results is listed below.\n\nRunning the example first reports a summary of each input variable.\n\nWe can see that the distributions have been adjusted and that the mean is a very small number close to zero and the standard deviation is very close to 1.0 for each variable.\n\nHistogram plots of the variables are created, although the distributions don’t look much different from their original distributions seen in the previous section other than their scale on the x-axis.\n\nNext, let’s evaluate the same KNN model as the previous section, but in this case, on a StandardScaler transform of the dataset.\n\nThe complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example, we can see that the StandardScaler transform results in a lift in performance from 79.7 percent accuracy without the transform to about 81.0 percent with the transform, although slightly lower than the result using the MinMaxScaler.\n\nThis section lists some common questions and answers when scaling numerical data.\n\nWhether input variables require scaling depends on the specifics of your problem and of each variable.\n\nYou may have a sequence of quantities as inputs, such as prices or temperatures.\n\nIf the distribution of the quantity is normal, then it should be standardized, otherwise, the data should be normalized. This applies if the range of quantity values is large (10s, 100s, etc.) or small (0.01, 0.0001).\n\nIf the quantity values are small (near 0-1) and the distribution is limited (e.g. standard deviation near 1), then perhaps you can get away with no scaling of the data.\n\nPredictive modeling problems can be complex, and it may not be clear how to best scale input data.\n\nIf in doubt, normalize the input sequence. If you have the resources, explore modeling with the raw data, standardized data, and normalized data and see if there is a beneficial difference in the performance of the resulting model.\n\nStandardization can give values that are both positive and negative centered around zero.\n\nIt may be desirable to normalize data after it has been standardized.\n\nThis might be a good idea of you have a mixture of standardized and normalized variables and wish all input variables to have the same minimum and maximum values as input for a given algorithm, such as an algorithm that calculates distance measures.\n\nQ. But Which is Best?\n\nEvaluate models on data prepared with each transform and use the transform or combination of transforms that result in the best performance for your data set on your model.\n\nYou may normalize your data by calculating the minimum and maximum on the training data.\n\nLater, you may have new data with values smaller or larger than the minimum or maximum respectively.\n\nOne simple approach to handling this may be to check for such out-of-bound values and change their values to the known minimum or maximum prior to scaling. Alternately, you may want to estimate the minimum and maximum values used in the normalization manually based on domain knowledge.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• How to use Data Scaling Improve Deep Learning Model Stability and Performance\n• Rescaling Data for Machine Learning in Python with Scikit-Learn\n• How to Scale Data for Long Short-Term Memory Networks in Python\n• How to Normalize and Standardize Time Series Data in Python\n\nIn this tutorial, you discovered how to use scaler transforms to standardize and normalize numerical input variables for classification and regression.\n• Data scaling is a recommended pre-processing step when working with many machine learning algorithms.\n• Data scaling can be achieved by normalizing or standardizing real-valued input and output variables.\n• How to apply standardization and normalization to improve the performance of predictive modeling algorithms.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://geeksforgeeks.org/data-pre-processing-wit-sklearn-using-standard-and-minmax-scaler",
        "document": "Data Scaling is a data preprocessing step for numerical features. Many machine learning algorithms like Gradient descent methods, KNN algorithm, linear and logistic regression, etc. require data scaling to produce good results. Various scalers are defined for this purpose. This article concentrates on Standard Scaler and Min-Max scaler. The task here is to discuss what they mean and how they are implemented using in-built functions that come with this package.\n\nApart from supporting library functions other functions that will be used to achieve the functionality are:\n• The fit(data) method is used to compute the mean and std dev for a given feature so that it can be used further for scaling.\n• The transform(data) method is used to perform scaling using mean and std dev calculated using the .fit() method.\n• The fit_transform() method does both fit and transform.\n\nStandard Scaler helps to get standardized distribution, with a zero mean and standard deviation of one (unit variance). It standardizes features by subtracting the mean value from the feature and then dividing the result by feature standard deviation.\n\nThe standard scaling is calculated as:\n• x is to be scaled data.\n• u is the mean of the training samples\n• s is the standard deviation of the training samples.\n\nSklearn preprocessing supports StandardScaler() method to achieve this directly in merely 2-3 steps.\n\nThere is another way of data scaling, where the minimum of feature is made equal to zero and the maximum of feature equal to one. MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution.\n\nThe MinMax scaling is done using:"
    },
    {
        "link": "https://stackoverflow.com/questions/68866946/how-does-minmaxscaler-work-scaled-per-row-or-scaled-for-the-entire-data-set",
        "document": "I wonder how the MinMaxScaler from sklearn works on a numpy array.\n\nDoes it scale based on the min max values per row, or does it scale based on the min max values of the entire data set?\n\nSince this a dumped X data array and there is no 1.0 present? How could that be explained if the array is scaled per row?"
    },
    {
        "link": "https://stackoverflow.com/questions/47732108/how-to-scale-dataframes-consistently-minmaxscaler-sklearn",
        "document": "I have three data frames that are each scaled individually with MinMaxScaler().\n\nThe problem I am having is that each dataframe gets scaled according to its own individual set of column min and max values. I need all of my dataframes to scale to the same values as if they all shared the same set of column min and max values for the data overall. Is there a way to accomplish this with MinMaxScaler()? One option would be to make one large dataframe, then scale the dataframe before partitioning, but this would not be ideal."
    },
    {
        "link": "https://stackoverflow.com/questions/52642940/feature-scaling-for-a-big-dataset",
        "document": "I am trying to use a deep learning model for time series prediction, and before passing the data to the model I want to scale the different variables as they have widely different ranges.\n\nI have normally done this \"on the fly\": load the training subset of the data set, obtain the scaler from the whole subset, store it and then load it when I want to use it for testing.\n\nNow the data is pretty big and I will not load all the training data at once for training.\n\nHow could I go to obtain the scaler? A priori I thought of doing a one-time operation of loading all the data just to calculate the scaler (normally I use the sklearn scalers, like StandardScaler), and then load it when I do my training process.\n\nIs this a common practice? If it is, how would you do if you add data to the training dataset? can scalers be combined to avoid that one-time operation and just \"update\" the scaler?"
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html",
        "document": "Transform features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n\nThe transformation is given by:\n\nThis transformation is often used as an alternative to zero mean, unit variance scaling.\n\ndoesn’t reduce the effect of outliers, but it linearly scales them down into a fixed range, where the largest occurring data point corresponds to the maximum value and the smallest one corresponds to the minimum value. For an example visualization, refer to Compare MinMaxScaler with other scalers.\n\nRead more in the User Guide.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform."
    },
    {
        "link": "https://medium.com/falabellatechnology/build-models-on-massively-big-data-using-continuous-online-learning-c1707d4bb02c",
        "document": "The world seems to be in some mad race in accumulating more data by the day. The rate of data growth is being measured in zetta bytes. With this mammothic data accumulation, comes mammothic task of managing them and use them for model building. A lot of times, it gets difficult to preview such data, let alone do any operations on them. Below, I’ve listed a few steps on how not to get overwhelmed by this scale. I would be using the data from Kaggle — Click Through Rate Prediction Competition for most of the examples and illustrations.\n\nMost of us get comfortable only when we preview the data, before we start analyzing and building models . Like Andrej Karpathy says:\n\nBut when you are presented with a few gigabytes of data in a single file, it gets harder to open/preview the data using traditional tools like notepad++ or vim, which consumes the entire system RAM to load the files. One alternative is to use tools like less in UNIX, which helps us just get the preview of the data instead of loading the entire data. In Windows, there are options like EditPad Lite and Large Text File Viewer, although, I still prefer the using the more command from the powershell or even better, using all the benefits of Unix from WSL. Below if the data preview by running a less command on the train.csv from the CTR prediction dataset:\n\nOnce we preview the data visually, we at least know the basic traits of the file like the number of columns, column names (if present as a part of the header), delimiter, basic types of the fields (numeric, floating, string, etc.), presence of missing values (if you can capture it in the first few records). You could also compute the total records by wc -l command (word count)\n\nThe next logical step would be to read this data into your system. Again, given the massive data size, it would be close to impossible to read the dataset into the system RAM.\n\nOne option would be to read it line by line as per the example given below. Note: I have not added any preprocessing, but one may choose to perform any processing after the read step (at the comment). Also note that, the above code will read every variable as string. One may want to do necessary type conversions in the processing step.\n\nThis works well without running into any RAM constraints, and it takes about 70 seconds to read 5 .8 GB of data. This method will work in any data size, in any given system RAM.\n\nWhile the above method works perfectly well on any small machine, we can see that it is pretty inefficient, as it processes line after line. This has two problems:\n• We are not utilizing the complete RAM of the system and we can definitely do better on time\n• Some of the data processing may require us to see more than 1 line of data (for example finding out the distribution of a variable)\n\nThis can be addressed by mini-batch read. One easiest way to read the data in minibatch would be to use chunksize in pandas.read_csv module as shown below.\n\nYou could also choose the columns that you wish to read, by specifying usecols option in read_csv, thus reducing the memory consumption even further.\n\nAnother alternative is to use dask, which utilizes clever distributed computing and lazy loading. This means that it uses multiple cores to read the data in parallel, and at times when when you run short of memory, it only loads the structure of the data and returns the actual data only when required. In the below example, you can see that it hardly takes a few micro-seconds to perform read_csv.\n\nThe below code summaries all the three approaches:\n\nSpeaking about lazy loading, if you happen to have only numeric data, then we can alternatively make use of numpy memmap, which only maps the addresses of the data on the disk. It actually fetches the data only when it is referenced with the indices when required. (https://stackoverflow.com/questions/43393821/how-to-concat-many-numpy-arrays)\n\nOther ways to read such massive data would be to create a mini database like sqlite in your own system and use tools like sqlalchemy to read them with filters, etc. I’ll probably reserve that discussion for another post.\n\nOnce the data is loaded, the next task would be to perform some variable reduction or some modelling. A lot of the readily available machine learning packages mostly expect the entire data to be loaded into the RAM for performing their ‘fit’ functions. Below I’ll illustrate some of the ways to ensure that the ‘fit’ happens irrespective of the data size.\n\nA lot of algorithms are sensitive to the weight initialisers. A purely random weight initialisation would take a longer duration to converge as it will have to re-adjust its weights a lot more to the optimal points. Also poor initialisation may lead to exploding/vanishing gradients. There are smarter hacks to intelligently initialise your weights before beginning to learn. For example, if we are trying to perform a regression whose mean value is say — 250, then initialise your bias to be equal to 250. Also scale your input variables before fitting, so as to reduce the range of gradients while learning. For classification problems, initialise the logit bias such that your model predicts probability equal to the 1:0 ratios at initialisation. While clustering, determine the distribution of points by various features (Eg: get the 5th, 10th, 25th, 50th, 75th, 90th and 95th percentiles of each features) and initialise the centroids around these ranges.\n\nAnother alternative could be to fit a model on a very small sample and use these weights (in case of supervised models) OR centroids (in case of unsupervised models) as initialisers for the large dataset modelling.\n\nA lot of the algorithms allow stochastic learning. That means, instead of learning from the entire data, they can learn if they were provided row-by-row data. This could even be extended to a mini-batch data (a small set of rows, instead of single row). That means, in a typical supervised learning setup, we start with some randomly assigned — learnable weights, and keep adjusting those weights as and when we encounter more data and labels. Below, I shall discuss some of the methods that uses these partial fit/incremental fit —\n\nThe data can be across different units of measurements (like percentages, large float numbers, small ranged integers, binary, etc.). In order to bring all these variables to a comparable scale, we perform scaling. The most popular API for scaling is StandardScaler or MinMaxScaler. StandardScaler computes the Mean/Standard Deviation of the variables and subtracts the mean from each row of the variable and divides it with the Standard Deviation, thus ensuring that distribution of the variable has a mean of 0 and standard deviation of 1. i.e. —\n\nwhere μ is the mean of the variable and σ is the standard deviation of the variable.\n\nSimilarly, the MinMax Scaling is performed by taking the ratio of the difference of each row of the variable with that of the min value of the variable to the difference max value of the variable with that of each row of the variable. i.e. —\n\nHowever, both the StandardScaler and the MinMax Scaler in the above equations will require the entire distribution to be made available at once (in order to compute the mean, standard deviation, max and min). But we still are stuck with the big data problem of not being able to load the entire data into the memory. In order to circumvent this, we employ incremental aggregation computation methods, which will compute the mean, standard deviation, min and max incrementally. Though this may not result in accurate values, these approximations usually hold well for the large data.\n• We know that a simple mean of a variable is expressed as the sum of the variable values divided by the number of observations\n• By re ordering some of the terms as given below, we can express the mean of the variable upto the n-th observation as a function of the the mean upto the n-1 th observation\n\nThus, you can see that the mean for the n-th observation can be derived from the n-1 th observation. In other words, the mean of a variable can be calculated incrementally. Similarly, we can also show that the standard deviation too, can be calculated incrementally. And so is the min and max of a variable (keep a pseudo min/ max variable and keep reassigning that variable as and when you encounter the new min and new max in the variable observations).\n\nUsing the above incremental aggregation concept, scikit learn has enabled a partial fit function for both the scaler, which keeps learning the scale function everytime you pass a chunk of data to it. So the learning can be done chunkwise. Once the entire data is fit, you can use the scaler object for further transformations. Below code shows how it can be done —\n• Principal Component Analysis is typically used to reduce the large dimension to smaller components, where each of these components are expressed as a linear function of all the underlying features, while ensuring that each of these linear components are orthogonal to each other. I would not delve more into the working of PCA as such, but would point out that there exists an IncrementalPCA API in the sklearn library. This takes in the data batch-by-batch and incrementally fits the Components, which turns out to be pretty useful while training a large data. Below is an example usage of IncrementalPCA on the same large data example. Note that we also scale the data before we fit the PCA.\n• AutoEncoders: Any neural network based method will naturally allow stochastic OR minibatch learning. It requires us to use the fit method (earlier known as the fit_generator, now overloaded into fit), by passing in a data generator instead of the actual data. In the next section, I’ve shown a sample — generic data generator, that could be used for any data, and also built a sample autoencoder that can compress the dimensions, similar to a PCA. More discussions on how an AutoEncoder compares to a PCA is outside the scope of this post — so probably another post on this..\n• Incremental Matrix Factorization: Matrix Factorization is a popular method that is used not only in dimension reduction, but also in recommender systems, generating user/item embeddings, etc. The latent dimension that is generated out of the matrix factorization an be used as the reduced set of dimensions that represents the matrix as shown in the figure below. The typical set up requires us to load the entire response matrix to the RAM and perform the matrix decomposition which gets very expensive as the data and the sparsity increases. Alternatively, one can use a stochastic learning framework to learn the responses using any common algorithm like Alternating Least Squares, etc. to Factorize the Matrix, thus, being able to decompose any large matrix, with smaller RAM size. One such application is explained well in Incremental SGD (ISGD) — J. Vinagre, et al., 2014. Below is a small snippet that can perform such incremental sgd in a simple response matrix setup\n\nA lot of partial fit based algorithms are made available that can incrementally learn the homogeneous groups of observations and add them to the clusters. Some of the examples include MiniBatchKMeans and Birch.\n\nMiniBatchKMeans randomly samples mini batches of observations during each iterations during the training. At the initial iterations, the centroids are created local to the sampled space, thus leading to major updates in the centroids as and when we sample different spaces over subsequent iterations. After enough number of iterations, this would converge to the true universal centroids. In order to decrease the major updates to the centroid at each iterations, owing to the random samples, one hack could be to update the centroids by taking an incremental average (there we go again!!) of the current centroid position with respect to all the previous mini batches.\n\nBirch on the other hand, creates trees called as Clustering Feature Tree, which can be treated as a lossy compression. The leaf nodes can then be treated as centroids. For more details, I recommend this blog by Cory Maklin which is quite intuitive to understand.\n\nThe codes for partial_fit for MiniBatchKMeans and Birch are well documented in the scikit-learn site.\n\nSupervised learning can be performed either using Stochastic Gradient Descent’s partial fit or by using the incremental least squares (Code given below). The idea is similar, where we read mini batch observations and try adjusting the weights stochastically. Scikit-learn has a bunch of algorithms like SGDRegressor, SGDClassification, MultiNomialNB and BernoulliNB (Naive Bayes), PassiveAgressiveClassifier, Perceptron.\n\nOne can also tweak the LeastSquare algorithm to incrementally update the co-variance matrix, that can enable us to partially fit the data. Below is a code that I found on stackexchange, which does this —\n\nTensorflow too, supports fitgenerator (now overloaded with fit function itself), where once can write a generator method to push the data in a streaming fashion for mini-batch stochastic learning. Tensorflow has a ready implemented image generator. Below is an implementation of a sample custom fitgenerator.\n\nIn summary, do not get bogged down by the size of the data. Bring on all the data in the world and with the right set of tools (both mathematical and computational) and the Attitude to solve them, we should be able to take on any monster to tackle.\n\nLet me know your comments and thoughts."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/scale.html",
        "document": "pandas provides data structures for in-memory analytics, which makes using pandas to analyze datasets that are larger than memory datasets somewhat tricky. Even datasets that are a sizable fraction of memory become unwieldy, as some pandas operations need to make intermediate copies.\n\nThis document provides a few recommendations for scaling your analysis to larger datasets. It’s a complement to Enhancing performance, which focuses on speeding up analysis for datasets that fit in memory.\n\nSuppose our raw dataset on disk has many columns. To load the columns we want, we have two options. Option 1 loads in all the data and then filters to what we need. Option 2 only loads the columns we request. If we were to measure the memory usage of the two calls, we’d see that specifying uses about 1/10th the memory in this case. With , you can specify to limit the columns read into memory. Not all file formats that can be read by pandas provide an option to read a subset of columns.\n\nThe default pandas data types are not the most memory efficient. This is especially true for text data columns with relatively few unique values (commonly referred to as “low-cardinality” data). By using more efficient data types, you can store larger datasets in memory. Now, let’s inspect the data types and memory usage to see where we should focus our attention. The column is taking up much more memory than any other. It has just a few unique values, so it’s a good candidate for converting to a . With a , we store each unique name once and use space-efficient integers to know which specific name is used in each row. We can go a bit further and downcast the numeric columns to their smallest types using . In all, we’ve reduced the in-memory footprint of this dataset to 1/5 of its original size. See Categorical data for more on and dtypes for an overview of all of pandas’ dtypes.\n\nSome workloads can be achieved with chunking by splitting a large problem into a bunch of small problems. For example, converting an individual CSV file into a Parquet file and repeating that for each file in a directory. As long as each chunk fits in memory, you can work with datasets that are much larger than memory. Chunking works well when the operation you’re performing requires zero or minimal coordination between chunks. For more complicated workflows, you’re better off using other libraries. Suppose we have an even larger “logical dataset” on disk that’s a directory of parquet files. Each file in the directory represents a different year of the entire dataset. Now we’ll implement an out-of-core . The peak memory usage of this workflow is the single largest chunk, plus a small series storing the unique value counts up to this point. As long as each individual file fits in memory, this will work for arbitrary-sized datasets. Some readers, like , offer parameters to control the when reading a single file. Manually chunking is an OK option for workflows that don’t require too sophisticated of operations. Some operations, like , are much harder to do chunkwise. In these cases, you may be better switching to a different library that implements these out-of-core algorithms for you."
    }
]