[
    {
        "link": "https://hbase.apache.org/book.html",
        "document": ""
    },
    {
        "link": "https://docs.cloudera.com/runtime/7.3.1/accessing-hbase/topics/hbase-script-with-shell.html",
        "document": "You can use HBase shell in your scripts. You can also write Ruby scripts for use with HBase Shell. Example Ruby scripts are included in the directory.\n\nThe non-interactive mode allows you to use HBase Shell in scripts, and allow the script to access the exit status of the HBase Shell commands. To invoke non-interactive mode, use the or switch. This small example script shows how to use HBase Shell in a Bash script.\n\nSuccessful HBase Shell commands return an exit status of 0. However, an exit status other than 0 does not necessarily indicate a failure, but should be interpreted as unknown. For example, a command may succeed, but while waiting for the response, the client may lose connectivity. In that case, the client has no way to know the outcome of the command. In the case of a non-zero exit status, your script should check to be sure the command actually failed before taking further action."
    },
    {
        "link": "https://hbase.apache.org/2.1/book.html",
        "document": ""
    },
    {
        "link": "https://github.com/apache/hbase/blob/master/bin/hbase-daemon.sh",
        "document": "Only one will get called. Either the trap or the flow will go through.\n\nOnly one will get called. Either the trap or the flow will go through.\n\nif the file does not exist it means that it was not stopped properly by the stop command\n\nIf ZooKeeper cannot be found, then do not restart\n\nkill autostart if the retry limit is exceeded within the given window size (window size other then 0)"
    },
    {
        "link": "https://old-docs.janusgraph.org/0.2.1/hbase.html",
        "document": "The following sections outline the various ways in which JanusGraph can be used in concert with Apache HBase. HBase can be run as a standalone database on the same local host as JanusGraph and the end-user application. In this model, JanusGraph and HBase communicate with one another via a socket. Running JanusGraph over HBase requires the following setup steps:\n• Start HBase by invoking the script in the directory inside the extracted HBase directory. To stop HBase, use . Now, you can create an HBase JanusGraph as follows: Note, that you do not need to specify a hostname since a localhost connection is attempted by default. Also, in the Gremlin Console, you can not define the type of the variables and . Therefore, simply leave off the type declaration. When the graph needs to scale beyond the confines of a single machine, then HBase and JanusGraph are logically separated into different machines. In this model, the HBase cluster maintains the graph representation and any number of JanusGraph instances maintain socket-based read/write access to the HBase cluster. The end-user application can directly interact with JanusGraph within the same JVM as JanusGraph. For example, suppose we have a running HBase cluster with a ZooKeeper quorum composed of three machines at IP address 77.77.77.77, 77.77.77.78, and 77.77.77.79, then connecting JanusGraph with the cluster is accomplished as follows: accepts a comma separated list of IP addresses and hostname for any subset of machines in the HBase cluster JanusGraph should connect to. Also, in the Gremlin Console, you can not define the type of the variables and . Therefore, simply leave off the type declaration. Finally, Gremlin Server can be wrapped around each JanusGraph instance defined in the previous subsection. In this way, the end-user application need not be a Java-based application as it can communicate with Gremlin Server as a client. This type of deployment is great for polyglot architectures where various components written in different languages need to reference and compute on the graph. In this case, each Gremlin Server would be configured to connect to the HBase cluster. The following shows the graph specific fragment of the Gremlin Server configuration. Refer to Chapter 7, JanusGraph Server for a complete example and more information on how to configure the server.\n\nRefer to Chapter 13, Configuration Reference for a complete listing of all HBase specific configuration options in addition to the general JanusGraph configuration options. When configuring HBase it is recommended to consider the following HBase specific configuration options:\n• : Name of the HBase table in which to store the JanusGraph graph. Allows multiple JanusGraph graphs to co-exist in the same HBase cluster. Please refer to the HBase configuration documentation for more HBase configuration options and their description. By prefixing the respective HBase configuration option with in the JanusGraph configuration it will be passed on to HBase at initialization time. For example, to use the znode /hbase-secure for HBase, set the property: . The prefix allows arbitrary HBase configuration options to be configured through JanusGraph. HBase backend uses millisecond for timestamps. In JanusGraph 0.2.0 and earlier, if the property is not explicitly set, the default is . In this case, the property must be explicitly set to . Do not set the property to another value in any cases.\n\nAmazon EC2 is a web service that provides resizable compute capacity in the cloud. It is designed to make web-scale computing easier for developers. Follow these steps to setup an HBase cluster on EC2 and deploy JanusGraph over HBase. To follow these instructions, you need an Amazon AWS account with established authentication credentials and some basic knowledge of AWS and EC2. The following commands first launch a four-node HBase cluster on EC2 via Whirr, then run a basic JanusGraph test case using the cluster. The configuration described below puts one HBase master server in charge of three HBase regionservers. The master will be the sole member of the Zookeeper quorum by which JanusGraph connects to HBase. Whirr 0.7.1 sometimes fails when run on a machine behind a NAT WHIRR-459. For this reason, it’s recommended to use at least Whirr 0.7.2. Whirr 0.8.0 was used to test the following commands on a t1.micro instance running Amazon Linux 2012.03. These commands might need tweaking to produce the intended results on environments besides a t1.micro instance running Amazon Linux 2012.03. # These commands were executed on a t1.micro instance running Amazon Linux 2012.03 x86_64. # The AMI identifier for Amazon Linux 2012.03 x86_64 is ami-aecd60c7. # https://console.aws.amazon.com/ec2/home?region=us-east-1#launchAmi=ami-aecd60c7 export AWS_ACCESS_KEY_ID=... # Set your Access Key here export AWS_SECRET_ACCESS_KEY=... # Set your Secret Key here curl -O http://www.apache.org/dist/whirr/whirr- . /whirr- . tar.gz tar -xzf whirr- . tar.gz && cd whirr- . # Generate an SSH keypair with which Whirr will deploy and manage instances ssh-keygen -t rsa -P -f ~/.ssh/id_rsa_whirr # Download a Whirr recipe for deploying HBase 0.94.1 with hadoop-core 1.0.3 pushd recipes && wget ; popd bin/whirr launch-cluster --config recipes/whirr-hbase.properties --private-key-file ~/.ssh/id_rsa_whirr # Run a superficial health check on the hbase-master node (this should print \"imok\") echo | nc $(awk ~/.whirr/hbase-testing/instances | head - ) ; echo # Login to the HBase master node to run the remaining commands ssh -i ~/.ssh/id_rsa_whirr -o \\ -o StrictHostKeyChecking=no \\ `grep hbase-master ~/.whirr/hbase-testing/instances \\ | awk ` # Maven 2 is available through the package manager, but an incompatibility # with surefire 2.12 makes it a pain to use; here we download Maven 3 without # the OS package manager wget tar -xzf apache-maven- . -bin.tar.gz # Install git sudo apt-get install -y git-core # Clone JanusGraph git clone && cd janusgraph # Run a HBase-backed test of JanusGraph # # This test should produce pages of output ending in something like this: # # ------------------------------------------------------- # T E S T S # ------------------------------------------------------- # Running org.janusgraph.graphdb.hbase.ExternalHBaseGraphPerformanceTest # Starting trial 1/1 # 10000 # 20000 # 30000 # 40000 # 50000 # Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 303.659 sec # # Results : # # Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 # # [INFO] ------------------------------------------------------------------------ # [INFO] BUILD SUCCESS # [INFO] ------------------------------------------------------------------------ ~/apache-maven- . /bin/mvn test -Dtest=ExternalHBaseGraphPerformanceTest#unlabeledEdgeInsertion # Check on hadoop hadoop version # Should print 1.0.3 # List the hadoop root; should print something like: # # Found 4 items # drwxr-xr-x - hadoop supergroup 0 2012-09-20 00:20 /hadoop # drwxr-xr-x - hadoop supergroup 0 2012-09-20 00:42 /hbase # drwxrwxrwx - hadoop supergroup 0 2012-09-20 00:20 /tmp # drwxrwxrwx - hadoop supergroup 0 2012-09-20 00:20 /user hadoop fs -ls /\n\n17.5. Tips and Tricks for Managing an HBase Cluster The HBase shell on the master server can be used to get an overall status check of the cluster. From the shell, the following commands are generally useful for understanding the status of the cluster. The above commands can identify if a region server has gone down. If so, it is possible to into the failed region server machines and do the following: The use of pssh can make this process easy as there is no need to log into each machine individually to run the commands. Put the IP addresses of the regionservers into a file and then execute the following. Next, sometimes you need to restart the master server (e.g. connection refused exceptions). To do so, on the master execute the following: Finally, if an HBase cluster has already been deployed and more memory is required of the master or region servers, simply edit the files on the respective machines with requisite parameters. Once edited, stop/start the master and/or region servers as described previous."
    },
    {
        "link": "https://hbase.apache.org/book.html",
        "document": ""
    },
    {
        "link": "https://docs.cloudera.com/runtime/7.3.1/configuring-hbase/hbase-configuring.pdf",
        "document": ""
    },
    {
        "link": "https://hbase.apache.org/2.4/apache_hbase_reference_guide.pdf",
        "document": ""
    },
    {
        "link": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hbase-configure.html",
        "document": "Although the default HBase settings should work for most applications, you can modify your HBase configuration settings. To do this, use properties of HBase configuration classifications. For more information, see Configure applications.\n\nThe following example creates a cluster with an alternate HBase root directory based on a configuration file, , stored in Amazon S3.\n\nThe file specifies the property for the configuration classification as shown in the following example. Replace with the internal DNS hostname of the cluster's primary node.\n\nChanges to memory allocation in YARN\n\nHBase is not running as a YARN application, thus it is necessary to recalculate the memory allocated to YARN and its applications, which results in a reduction in overall memory available to YARN if HBase is installed. You should take this into account when planning to co-locate YARN applications and HBase on the same clusters. The instance types with less than 64 GB of memory have half the memory available to NodeManager, which is then allocated to the HBase RegionServer. For instance types with memory greater than 64 GB, HBase RegionServer memory is capped at 32 GB. As a general rule, YARN setting memory is some multiple of MapReduce reducer task memory.\n\nThe tables in Default values for task configuration settings show changes to YARN settings based on the memory needed for HBase.\n\nSome port numbers chosen for HBase are different from the default. The following are interfaces and ports for HBase on Amazon EMR.\n\nYou can set any or all of the HBase site settings to optimize the HBase cluster for your application's workload. We recommend the following settings as a starting point in your investigation.\n\nThe default timeout is 40 seconds (40000 ms). If a region server crashes, this is how long it takes the master server to notice the absence of the region server and start recovery. To help the master server recover faster, you can reduce this value to a shorter time period. The following example uses 30 seconds, or 30000 ms:\n\nThis defines the number of threads the region server keeps open to serve requests to tables. The default of 10 is low, in order to prevent users from killing their region servers when using large write buffers with a high number of concurrent clients. The rule of thumb is to keep this number low when the payload per request approaches the MB range (big puts, scans using a large cache) and high when the payload is small (gets, small puts, ICVs, deletes). The following example raises the number of open threads to 30:\n\nThis parameter governs the size, in bytes, of the individual regions. By default, it is set to . If you are writing a lot of data into your HBase cluster, and it's causing frequent splitting, you can increase this size to make individual regions bigger. It reduces splitting but takes more time to load-balance regions from one server to another.\n\nThis parameter governs the maximum size of memstore, in bytes, before it is flushed to disk. By default, it is . If your workload consists of short bursts of write operations, you might want to increase this limit so that all writes stay in memory during the burst and get flushed to disk later. This can boost performance during bursts."
    },
    {
        "link": "https://stackoverflow.com/questions/7105730/how-to-establish-the-regionserver-of-hbase-to-master",
        "document": "Please tell me how to establish the RegionServer of Hbase to master.\n\n I configured 5 region servers, however, only 2 server is worked properly.\n\nThe hostname of this two servers are sm3-10 and sm3-12 from http://hbase-master:60010. But the other servers like sm3-8 not work.\n\n \n\n I'd like to know the trouble shooting step and resolutions.\n\n\n\nsm3-8:slave, not work properly, however, the status looks good"
    }
]