[
    {
        "link": "https://realpython.com/python-json",
        "document": "Python’s module provides you with the tools you need to effectively handle JSON data. You can convert Python data types to a JSON-formatted string with or write them to files using . Similarly, you can read JSON data from files with and parse JSON strings with .\n\nJSON, or JavaScript Object Notation, is a widely-used text-based format for data interchange. Its syntax resembles Python dictionaries but with some differences, such as using only double quotes for strings and lowercase for Boolean values. With built-in tools for validating syntax and manipulating JSON files, Python makes it straightforward to work with JSON data.\n\nBy the end of this tutorial, you’ll understand that:\n• JSON in Python is handled using the standard-library module, which allows for data interchange between JSON and Python data types.\n• JSON is a good data format to use with Python as it’s human-readable and straightforward to serialize and deserialize, which makes it ideal for use in APIs and data storage.\n• You write JSON with Python using to serialize data to a file.\n• You can minify and prettify JSON using Python’s module.\n\nSince its introduction, JSON has rapidly emerged as the predominant standard for the exchange of information. Whether you want to transfer data with an API or store information in a document database, it’s likely you’ll encounter JSON. Fortunately, Python provides robust tools to facilitate this process and help you manage JSON data efficiently.\n\nWhile JSON is the most common format for data distribution, it’s not the only option for such tasks. Both XML and YAML serve similar purposes. If you’re interested in how the formats differ, then you can check out the tutorial on how to serialize your data with Python.\n\nThe acronym JSON stands for JavaScript Object Notation. As the name suggests, JSON originated from JavaScript. However, JSON has transcended its origins to become language-agnostic and is now recognized as the standard for data interchange. The popularity of JSON can be attributed to native support by the JavaScript language, resulting in excellent parsing performance in web browsers. On top of that, JSON’s straightforward syntax allows both humans and computers to read and write JSON data effortlessly. To get a first impression of JSON, have a look at this example code: You’ll learn more about the JSON syntax later in this tutorial. For now, recognize that the JSON format is text-based. In other words, you can create JSON files using the code editor of your choice. Once you set the file extension to , most code editors display your JSON data with syntax highlighting out of the box: The screenshot above shows how VS Code displays JSON data using the Bearded color theme. You’ll have a closer look at the syntax of the JSON format next! In the previous section, you got a first impression of how JSON data looks. And as a Python developer, the JSON structure probably reminds you of common Python data structures, like a dictionary that contains a string as a key and a value. If you understand the syntax of a dictionary in Python, you already know the general syntax of a JSON object. Note: Later in this tutorial, you’ll learn that you’re free to use lists and other data types at the top level of a JSON document. The similarity between Python dictionaries and JSON objects is no surprise. One idea behind establishing JSON as the go-to data interchange format was to make working with JSON as convenient as possible, independently of which programming language you use: [A collection of key-value pairs and arrays] are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages is also based on these structures. (Source) To explore the JSON syntax further, create a new file named and add a more complex JSON structure as the content of the file: In the code above, you see data about a dog named Frieda, which is formatted as JSON. The top-level value is a JSON object. Just like Python dictionaries, you wrap JSON objects inside curly braces ( ). In line 1, you start the JSON object with an opening curly brace ( ), and then you close the object at the end of line 20 with a closing curly brace ( ). Note: Although whitespace doesn’t matter in JSON, it’s customary for JSON documents to be formatted with two or four spaces to indicate indentation. If the file size of the JSON document is important, then you may consider minifying the JSON file by removing the whitespace. You’ll learn more about minifying JSON data later in the tutorial. Inside the JSON object, you can define zero, one, or more key-value pairs. If you add multiple key-value pairs, then you must separate them with a comma ( ). A key-value pair in a JSON object is separated by a colon ( ). On the left side of the colon, you define a key. A key is a string you must wrap in double quotes ( ). Unlike Python, JSON strings don’t support single quotes ( ). The values in a JSON document are limited to the following data types: Either or without quotes Just like in dictionaries and lists, you’re able to nest data in JSON objects and arrays. For example, you can include an object as the value of an object. Also, you’re free to use any other allowed value as an item in a JSON array. As a Python developer, you may need to pay extra attention to the Boolean values. Instead of using or in title case, you must use the lowercase JavaScript-style Booleans or . Unfortunately, there are some other details in the JSON syntax that you may stumble over as a developer. You’ll have a look at them next. The JSON standard doesn’t allow any comments, trailing commas, or single quotes for strings. This can be confusing to developers who are used to Python dictionaries or JavaScript objects. Here’s a smaller version of the JSON file from before with invalid syntax:\n• Line 5 has a trailing comma after the final key-value pair.\n• Line 10 contains a trailing comma in the array. Using double quotes is something you can get used to as a Python developer. Comments can be helpful in explaining your code, and trailing commas can make moving lines around in your code less fragile. This is why some developers like to use Human JSON (Hjson) or JSON with comments (JSONC). Hjson gives you the freedom to use comments, ditch commas between properties, or create quoteless strings. Apart from the curly braces ( ), the Hjson syntax look like a mix of YAML and JSON. JSONC is a bit stricter than Hjson. Compared to regular JSON, JSONC allows you to use comments and trailing commas. You may have encountered JSONC when editing the file of VS Code. Inside its configuration files, VS Code works in a JSONC mode. For common JSON files, VS Code is more strict and points out JSON syntax errors. If you want to make sure you write valid JSON, then your coding editor can be of great help. The invalid JSON document above contains marks for each occurrence of incorrect JSON syntax: When you don’t want to rely on your code editor, you can also use online tools to verify that the JSON syntax you write is correct. Popular online tools for validating JSON are JSON Lint and JSON Formatter. Later in the tutorial, you’ll learn how to validate JSON documents from the comfort of your terminal. But before that, it’s time to find out how you can work with JSON data in Python.\n\nPython supports the JSON format through the built-in module named . The module is specifically designed for reading and writing strings formatted as JSON. That means you can conveniently convert Python data types into JSON data and the other way around. The act of converting data into the JSON format is referred to as serialization. This process involves transforming data into a series of bytes for storage or transmission over a network. The opposite process, deserialization, involves decoding data from the JSON format back into a usable form within Python. You’ll start with the serialization of Python code into JSON data with the help of the module. One of the most common actions when working with JSON in Python is to convert a Python dictionary into a JSON object. To get an impression of how this works, hop over to your Python REPL and follow along with the code below: After importing the module, you can use to convert a Python dictionary to a JSON-formatted string, which represents a JSON object. It’s important to understand that when you use , you get a Python string in return. In other words, you don’t create any kind of JSON data type. The result is similar to what you’d get if you used Python’s built-in function: Using gets more interesting when your Python dictionary doesn’t contain strings as keys or when values don’t directly translate to a JSON format: In the dictionary, the keys , , and are numbers. Once you use , the dictionary keys become strings in the JSON-formatted string. Note: When you convert a dictionary to JSON, the dictionary keys will always be strings in JSON. The Boolean Python values of your dictionary become JSON Booleans. As mentioned before, the tiny but significant difference between JSON Booleans and Python Booleans is that JSON Booleans are lowercase. The cool thing about Python’s module is that it takes care of the conversion for you. This can come in handy when you’re using variables as dictionary keys: When converting Python data types into JSON, the module receives the evaluated values. While doing so, sticks tightly to the JSON standard. For example, when converting integer keys like to the string . The module allows you to convert common Python data types to JSON. Here’s an overview of all Python data types and values that you can convert to JSON values: Note that different Python data types like lists and tuples serialize to the same JSON data type. This can cause problems when you convert JSON data back to Python, as the data type may not be the same as before. You’ll explore this pitfall later in this tutorial when you learn how to read JSON. Dictionaries are probably the most common Python data type that you’ll use as a top-level value in JSON. But you can convert the data types listed above just as smoothly as dictionaries using . Take a Boolean or a list, for example: A JSON document may contain a single scalar value, like a number, at the top level. That’s still valid JSON. But more often than not, you want to work with a collection of key-value pairs. Similar to how not every data type can be used as a dictionary key in Python, not all keys can be converted into JSON key strings: You can’t use dictionaries, lists, or tuples as JSON keys. For dictionaries and lists, this rule makes sense as they’re not hashable. But even when a tuple is hashable and allowed as a key in a dictionary, you’ll get a when you try to use a tuple as a JSON key: : keys must be str, int, float, bool or None, not tuple By providing the argument, you can prevent getting a when creating JSON data with unsupported Python keys: When you set in to , then Python skips the keys that are not supported and would otherwise raise a . The result is a JSON-formatted string that only contains a subset of the input dictionary. In practice, you usually want your JSON data to resemble the input object as close as possible. So, you must use with caution to not lose information when calling . Note: If you’re ever in a situation where you need to convert an unsupported object into JSON, then you can consider creating a subclass of the and implementing a method. When you use , you can use additional arguments to control the look of the resulting JSON-formatted string. For example, you can sort the dictionary keys by setting the parameter to : When you set to , then Python sorts the keys alphabetically for you when serializing a dictionary. Sorting the keys of a JSON object can come in handy when your dictionary keys formerly represented the column names of a database, and you want to display them in an organized fashion to the user. Another notable parameter of is , which you’ll probably use the most when serializing JSON data. You’ll explore later in this tutorial in the prettify JSON section. When you convert Python data types into the JSON format, you usually have a goal in mind. Most commonly, you’ll use JSON to persist and exchange data. To do so, you need to save your JSON data outside of your running Python program. Conveniently, you’ll explore saving JSON data to a file next. The JSON format can come in handy when you want to save data outside of your Python program. Instead of spinning up a database, you may decide to use a JSON file to store data for your workflows. Again, Python has got you covered. To write Python data into an external JSON file, you use . This is a similar function to the one you saw earlier, but without the s at the end of its name: In lines 3 to 22, you define a dictionary that you write to a JSON file in line 25 using a context manager. To properly indicate that the file contains JSON data, you set the file extension to . When you use , then it’s good practice to define the encoding. For JSON, you commonly want to use as the encoding when reading and writing files: The RFC requires that JSON be represented using either UTF-8, UTF-16, or UTF-32, with UTF-8 being the recommended default for maximum interoperability. (Source) The function has two required arguments:\n• The object you want to write\n• The file you want to write into Other than that, there are a bunch of optional parameters for . The optional parameters of are the same as for . You’ll investigate some of them later in this tutorial when you prettify and minify JSON files.\n\nIn the former sections, you learned how to serialize Python data into JSON-formatted strings and JSON files. Now, you’ll see what happens when you load JSON data back into your Python program. In parallel to and , the library provides two functions to deserialize JSON data into a Python object: As a rule of thumb, you work with when your data is already present in your Python program. You use with external files that are saved on your disk. The conversion from JSON data types and values to Python follows a similar mapping as before when you converted Python objects into the JSON format: When you compare this table to the one in the previous section, you may recognize that Python offers a matching data type for all JSON types. That’s very convenient because this way, you can be sure you won’t lose any information when deserializing JSON data to Python. Note: Deserialization is not the exact reverse of the serialization process. The reason for this is that JSON keys are always strings, and not all Python data types can be converted to JSON data types. This discrepancy means that certain Python objects may not retain their original type when serialized and then deserialized. To get a better feeling for the conversion of data types, you’ll start with serializing a Python object to JSON and then convert the JSON data back to Python. That way, you can spot differences between the Python object you serialize and the Python object you end up with after deserializing the JSON data. To investigate how to load a Python dictionary from a JSON object, revisit the example from before. Start by creating a dictionary and then serialize the Python dictionary to a JSON string using : By passing into , you’re creating a string with a JSON object that you save in . If you want to convert back to a Python dictionary, then you can use : By using , you can convert JSON data back into Python objects. With the knowledge about JSON that you’ve gained so far, you may already suspect that the content of the dictionary is not identical to the content of : The difference between and is subtle but can be impactful in your Python programs. In JSON, the keys must always be strings. When you converted to using , the integer key became the string . When you used , there was no way for Python to know that the string key should be an integer again. That’s why your dictionary key remained a string after deserialization. You’ll investigate a similar behavior by doing another conversion roundtrip with other Python data types! To explore how different data types behave in a roundtrip from Python to JSON and back, take a portion of the dictionary from a former section. Note how the dictionary contains different data types as values: The dictionary contains a bunch of common Python data types as values. For example, a string in line 2, a Boolean in line 3, a in line 7, and a tuple in line 8, just to name a few. Next, convert to a JSON-formatted string and back to Python again. Afterward, have a look at the newly created dictionary: You can convert every JSON data type perfectly into a matching Python data type. The JSON Boolean deserializes into , converts back into , and objects and arrays become dictionaries and lists. Still, there’s one exception that you may encounter in roundtrips: When you serialize a Python tuple, it becomes a JSON array. When you load JSON, a JSON array correctly deserializes into a list because Python has no way of knowing that you want the array to be a tuple. Problems like the one described above can always be an issue when you’re doing data roundtrips. When the roundtrip happens in the same program, you may be more aware of the expected data types. Data type conversions may be even more obfuscated when you’re dealing with external JSON files that originated in another program. You’ll investigate a situation like this next! In a previous section, you created a file that saved a file. If you need to refresh your memory, you can expand the collapsible section below that shows the code again: Take a look at the data types of the dictionary. Is there a data type in a value that the JSON format doesn’t support? When you want to write content to a JSON file, you use . The counterpart to is . As the name suggests, you can use to load a JSON file into your Python program. Jump back into the Python REPL and load the JSON file from before: Just like when writing files, it’s a good idea to use a context manager when reading a file in Python. That way, you don’t need to bother with closing the file again. When you want to read a JSON file, then you use inside the statement’s block. The argument for the function must be either a text file or a binary file. The Python object that you get from depends on the top-level data type of your JSON file. In this case, the JSON file contains an object at the top level, which deserializes into a dictionary. When you deserialize a JSON file as a Python object, then you can interact with it natively—for example, by accessing the value of the key with square bracket notation ( ). Still, there’s a word of caution here. Import the original dictionary from before and compare it to : When you load a JSON file as a Python object, then any JSON data type happily deserializes into Python. That’s because Python knows about all data types that the JSON format supports. Unfortunately, it’s not the same the other way around. As you learned before, there are Python data types like that you can convert into JSON, but you’ll end up with an data type in the JSON file. Once you convert the JSON data back to Python, then an array deserializes into the Python data type. Generally, being cautious about data type conversions should be the concern of the Python program that writes the JSON. With the knowledge you have about JSON files, you can always anticipate which Python data types you’ll end up with as long as the JSON file is valid. If you use , then the content of the file you load must contain valid JSON syntax. Otherwise, you’ll receive a . Luckily, Python caters to you with more tools you can use to interact with JSON. For example, it allows you to check a JSON file’s validity from the convenience of the terminal.\n\nSo far, you’ve explored the JSON syntax and have already spotted some common JSON pitfalls like trailing commas and single quotes for strings. When writing JSON, you may have also spotted some annoying details. For example, neatly indented Python dictionaries end up being a blob of JSON data. In the last section of this tutorial, you’ll try out some techniques to make your life easier as you work with JSON data in Python. To start, you’ll give your JSON object a well-deserved glow-up. One huge advantage of the JSON format is that JSON data is human-readable. Even more so, JSON data is human-writable. This means you can open a JSON file in your favorite text editor and change the content to your liking. Well, that’s the idea, at least! Editing JSON data by hand is not particularly easy when your JSON data looks like this in the text editor: Even with word wrapping and syntax highlighting turned on, JSON data is hard to read when it’s a single line of code. And as a Python developer, you probably miss some whitespace. But worry not, Python has got you covered! When you call or to serialize a Python object, then you can provide the argument. Start by trying out with different indentation levels: The default value for is . When you call without or with as a value, you’ll end up with one line of a compact JSON-formatted string. If you want linebreaks in your JSON string, then you can set to or provide an empty string. Although probably less useful, you can even provide a negative number as the indentation or any other string. More commonly, you’ll provide values like or for : When you use positive integers as the value for when calling , then you’ll indent every level of the JSON object with the given count as spaces. Also, you’ll have newlines for each key-value pair. Note: To actually see the whitespace in the REPL, you can wrap the calls in function calls. The parameter works exactly the same for as it does for . Go ahead and write the dictionary into a JSON file with an indentation of spaces: When you set the indentation level when serializing JSON data, then you end up with prettified JSON data. Have a look at how the file looks in your editor: Python can work with JSON files no matter how they’re indented. As a human, you probably prefer a JSON file that contains newlines and is neatly indented. A JSON file that looks like this is way more convenient to edit. The convenience of being able to edit JSON data in the editor comes with a risk. When you move key-value pairs around or add strings with one quote instead of two, you end up with an invalid JSON. To swiftly check if a JSON file is valid, you can leverage Python’s . You can run the module as an executable in the terminal using the switch. To see in action, also provide as the positional argument: When you run only with an option, then Python validates the JSON file and outputs the JSON file’s content in the terminal if the JSON is valid. Running in the example above means that contains valid JSON syntax. Note: The prints the JSON data with an indentation of 4 by default. You’ll explore this behavior in the next section. To make complain, you need to invalidate your JSON document. You can make the JSON data of invalid by removing the comma ( ) between the key-value pairs: After saving , run again to validate the file: The module successfully stumbles over the missing comma in . Python notices that there’s a delimiter missing once the property name enclosed in double quotes starts in line 3 at position 5. Go ahead and try fixing the JSON file again. You can also be creative with invalidating and check how reports your error. But keep in mind that only reports the first error. So you may need to go back and forth between fixing a JSON file and running . Once is valid, you may notice that the output always looks the same. Of course, like any well-made command-line interface, offers you some options to control the program. In the previous section, you used to validate a JSON file. When the JSON syntax was valid, showed the content with newlines and an indentation of four spaces. To control how prints the JSON, you can set the option. If you followed along with the tutorial, then you’ve got a file that doesn’t contain newlines or indentation. Alternatively, you can download in the materials by clicking the link below: Free Bonus: Click here to download the free sample code that shows you how to work with JSON data in Python. When you pass in to , then you can pretty print the content of the JSON file in your terminal. When you set , then you can control which indentation level uses to display the code: Seeing the prettified JSON data in the terminal is nifty. But you can step up your game even more by providing another option to the run! By default, writes the output to , just like you commonly do when calling the function. But you can also redirect the output of into a file by providing a positional argument: With as the value of the option, you write the output into the JSON file instead of showing the content in the terminal. If the file doesn’t exist yet, then Python creates the file on the way. If the target file already exists, then you overwrite the file with the new content. Note: You can prettify a JSON file in place by using the same file as and arguments. You can verify that the file exists by running the terminal command: The whitespace you added to comes with a price. Compared to the original, unindented file, the file size of is now around double that. Here, the 308-byte increase may not be significant. But when you’re dealing with big JSON data, then a good-looking JSON file will take up quite a bit of space. Having a small data footprint is especially useful when serving data over the web. Since the JSON format is the de facto standard for exchanging data over the web, it’s worth keeping the file size as small as possible. And again, Python’s has got your back! As you know by now, Python is a great helper when working with JSON. You can minify JSON data with Python in two ways:\n• Use the module in your Python code Before, you used with the option to add whitespace. Instead of using here, you can use provide to do the opposite and remove any whitespace between the key-value pairs of your JSON: After calling the module, you provide a JSON file as the and another JSON file as the . If the target JSON file exists, then you overwrite its contents. Otherwise, you create a new file with the filename you provide. Just like with , you provide the same file as a source and target file to minify the file in-place. In the example above, you minify into . Run the command to see how many bytes you squeezed out of the original JSON file: Compared to , the file size of is 337 bytes smaller. That’s even 29 bytes less than the original file that didn’t contain any indentation. To investigate where Python managed to remove even more whitespace from the original JSON, open the Python REPL again and minify the content of the original file with Python’s module: In the code above, you use Python’s to get the content of as text. Then, you use to deserialize to , which is a Python dictionary. You could use to get a Python dictionary right away, but you need the JSON data as a string first to compare it properly. That’s also why you use to create and then use instead of leveraging directly to save the minified JSON data in . As you learned before, needs JSON data as the first argument and then accepts a value for the indentation. The default value for is , so you could skip setting the argument explicitly like you do above. But with , you’re making your intention clear that you don’t want any indentation, which will be a good thing for others who read your code later. The parameter for allows you to define a tuple with two values:\n• The separator between the key-value pairs or list items. By default, this separator is a comma followed by a space ( ).\n• The separator between the key and the value. By default, this separator is a colon followed by a space ( ). By setting to , you continue to use valid JSON separators. But you tell Python not to add any spaces after the comma ( ) and the colon ( ). That means that the only whitespace left in your JSON data can be whitespace appearing in key names and values. That’s pretty tight! With both and containing your JSON strings, it’s time to compare them: You can already spot the difference between and when you look at the output. You then use the function to verify that the size of is indeed smaller. If you’re curious about why the length of the JSON strings almost exactly matches the file size of the written files, then looking into Unicode & character encodings in Python is a great idea. Both and are excellent helpers when you want to make JSON data look prettier, or if you want to minify JSON data to save some bytes. With the module, you can conveniently interact with JSON data in your Python programs. That’s great when you need to have more control over the way you interact with JSON. The module comes in handy when you want to work with JSON data directly in your terminal."
    },
    {
        "link": "https://reddit.com/r/dataengineering/comments/l0m6j9/project_structure_and_best_practices_for",
        "document": "Looking for feedback. First time working on a project like this and no one to bounce ideas from. Been lurking around this subreddit for some time, first time poster.\n\nI'm working on a project where I have to pull json data from different API providers and store the results in a database for reporting and analysis. I have experience working with straight forward elt - moving data from source and transforming it, but json is new to me, as such I'm unsure about the best practices, project structure, naming conventions, etc.\n\nIn this project I'm building Python CLI tool for all integrations and transformations and SQL Server for storage.\n\nThe project structure looks something like this:\n\nEvery API, get's a module in /readers (getting and storing raw data as is from API) and /parsers (parsing the json into flat csvs) packages and share a common folder structure, that looks something like this:\n\nEach API .py creates a dedicated folder in raw/parsed/aggregated folders, the folder name is used as a schema identifier in the database. Raw contains the raw JSON received from an API, parsed contains CSVs generated from JSON and aggregated contains (you guessed it) aggregated cvs based on name in parsed (speeds up upload). Every run, creates a unique folder name, which propagates across raw/parsed/aggregated and is used as a batch identifier in the database. Some APIs can generate hundreds/thousands of files in raw, as I need to pass different parameters, each creating their own json response. The parameter values passed with an associated batch/request are stored in a database at run time.\n\nBoth the aggregator.py and uploder.py modules know how to interact with the folder structure, so they are generalizable, uploader.py uses the api folder name and the csv file name to identify into which schema and table it needs to be inserted. queries.py contains all the table definitions, insert statements, etc..\n\nMy goal of course is that the project would be maintainable and scalable.\n\nIs this a reasonable approach? Do you envision any problems? What would you have done differently?\n• where and how do you define your target tables? In the project I store the SQL statements for all target tables in a dedicated queries.py file, would using SqlAlchemy be a better approach?\n• Is there a standard naming convention for such projects? for example, should /readers be called adapters/connectors/plugins/datagetters?\n• are there any github repos I could take a look for inspiration?\n\nI'd appreciate any input and feedback I can get."
    },
    {
        "link": "https://medium.com/@aarav.gupta9/a-comprehensive-guide-to-json-with-python-api-172be340e7c0",
        "document": "In my previous blog post, I shared about APIs. Today I will be talking about json through this blog post. JSON (JavaScript Object Notation) has become a universal data interchange format due to its simplicity and versatility. In the realm of Python programming, integrating JSON with APIs (Application Programming Interfaces) has become an essential skill. This blog aims to provide a comprehensive guide to working with JSON in Python, focusing on API integration.\n\nJSON is a lightweight data interchange format that is easy to read and write, and easy for machines to parse and generate. It consists of key-value pairs and supports various data types, including strings, numbers, objects, arrays, and more.\n\nIn Python, you can work with JSON using the built-in module, which provides methods for encoding Python objects into JSON format ( ) and decoding JSON data into Python objects ( ).\n\nAPIs allow different software systems to communicate with each other. Many APIs use JSON as their data format, making it a popular choice for data exchange. Python’s library simplifies the process of making HTTP requests to APIs.\n\nHere’s a basic example of how to make a GET request to an API and parse the JSON response:\n\nThe function is used to convert a Python object into a JSON-formatted string. For example:\n\nConversely, is used to parse a JSON string and convert it back into a Python object:\n\nJSON often contains nested structures. To navigate through nested data, access the elements using their keys. For example:\n\nWhen working with APIs that require sending data in the request body (e.g., for creating or updating resources), use the parameter in the method:\n\nIn this guide, we’ve explored the fundamentals of working with JSON in Python and integrating it with APIs. This knowledge is essential for any developer involved in web development, data science, or any field that requires interaction with external services. By mastering these techniques, you’ll be well-equipped to handle a wide range of scenarios involving JSON and API integration in your Python projects."
    },
    {
        "link": "https://stackoverflow.blog/2020/03/02/best-practices-for-rest-api-design",
        "document": "REST APIs are one of the most common kinds of web interfaces available today. They allow various clients including browser apps to communicate with services via the REST API. Therefore, it's very important to design REST APIs properly so that we won't run into problems down the road. We have to take into account security, performance, and ease of use for API consumers.\n\nOtherwise, we create problems for clients that use our APIs, which isn’t pleasant and detracts people from using our API. If we don’t follow commonly accepted conventions, then we confuse the maintainers of the API and the clients that use them since it’s different from what everyone expects.\n\nIn this article, we'll look at how to design REST APIs to be easy to understand for anyone consuming them, future-proof, and secure and fast since they serve data to clients that may be confidential.\n• Use nouns instead of verbs in endpoint paths\n\nA REST API is an application programming interface architecture style that conforms to specific architectural constraints, like stateless communication and cacheable data. It is not a protocol or standard. While REST APIs can be accessed through a number of communication protocols, most commonly, they are called over HTTPS, so the guidelines below apply to REST API endpoints that will be called over the internet.\n\nNote: For REST APIs called over the internet, you'll like want to follow the best practices for REST API authentication.\n\nEven though some people think REST should only return hypertext (including Roy Fielding who created the term) REST APIs should accept JSON for request payload and also send responses to JSON. JSON is the standard for transferring data. Almost every networked technology can use it: JavaScript has built-in methods to encode and decode JSON either through the Fetch API or another HTTP client. Server-side technologies have libraries that can decode JSON without doing much work.\n\nThere are other ways to transfer data. XML isn’t widely supported by frameworks without transforming the data ourselves to something that can be used, and that’s usually JSON. We can’t manipulate this data as easily on the client-side, especially in browsers. It ends up being a lot of extra work just to do normal data transfer.\n\nForm data is good for sending data, especially if we want to send files. But for text and numbers, we don’t need form data to transfer those since—with most frameworks—we can transfer JSON by just getting the data from it directly on the client side. It’s by far the most straightforward to do so.\n\nTo make sure that when our REST API app responds with JSON that clients interpret it as such, we should set Content-Type in the response header to application/json after the request is made. Many server-side app frameworks set the response header automatically. Some HTTP clients look at the Content-Type response header and parse the data according to that format.\n\nThe only exception is if we’re trying to send and receive files between client and server. Then we need to handle file responses and send form data from client to server. But that is a topic for another time.\n\nWe should also make sure that our endpoints return JSON as a response. Many server-side frameworks have this as a built-in feature.\n\nLet’s take a look at an example API that accepts JSON payloads. This example will use the Express back end framework for Node.js. We can use the body-parser middleware to parse the JSON request body, and then we can call the res.json method with the object that we want to return as the JSON response as follows:\n\nbodyParser.json() parses the JSON request body string into a JavaScript object and then assigns it to the req.body object.\n\nSet the Content-Type header in the response to application/json; charset=utf-8 without any changes. The method above applies to most other back end frameworks.\n\nUse nouns instead of verbs in endpoint paths\n\nWe shouldn't use verbs in our endpoint paths. Instead, we should use the nouns which represent the entity that the endpoint that we're retrieving or manipulating as the pathname.\n\nThis is because our HTTP request method already has the verb. Having verbs in our API endpoint paths isn’t useful and it makes it unnecessarily long since it doesn’t convey any new information. The chosen verbs could vary by the developer’s whim. For instance, some like ‘get’ and some like ‘retrieve’, so it’s just better to let the HTTP GET verb tell us what and endpoint does.\n\nThe action should be indicated by the HTTP request method that we're making. The most common methods include GET, POST, PUT, and DELETE.\n• POST submits new data to the server.\n\nWith the two principles we discussed above in mind, we should create routes like GET /articles/ for getting news articles. Likewise, POST /articles/ is for adding a new article , PUT /articles/:id is for updating the article with the given id. DELETE /articles/:id is for deleting an existing article with the given ID.\n\n/articles represents a REST API resource. For instance, we can use Express to add the following endpoints for manipulate articles as follows:\n\nIn the code above, we defined the endpoints to manipulate articles. As we can see, the path names do not have any verbs in them. All we have are nouns. The verbs are in the HTTP verbs.\n\nThe POST, PUT, and DELETE endpoints all take JSON as the request body, and they all return JSON as the response, including the GET endpoint.\n\nWhen designing endpoints, it makes sense to group those that contain associated information. That is, if one object can contain another object, you should design the endpoint to reflect that. This is good practice regardless of whether your data is structured like this in your database. In fact, it may be advisable to avoid mirroring your database structure in your endpoints to avoid giving attackers unnecessary information.\n\nFor example, if we want an endpoint to get the comments for a news article, we should append the /comments path to the end of the /articles path. We can do that with the following code in Express:\n\nIn the code above, we can use the GET method on the path '/articles/:articleId/comments'. We get comments on the article identified by articleId and then return it in the response. We add 'comments' after the '/articles/:articleId' path segment to indicate that it's a child resource of /articles.\n\nThis makes sense since comments are the children objects of the articles, assuming each article has its own comments. Otherwise, it’s confusing to the user since this structure is generally accepted to be for accessing child objects. The same principle also applies to the POST, PUT, and DELETE endpoints. They can all use the same kind of nesting structure for the path names.\n\nHowever, nesting can go too far. After about the second or third level, nested endpoints can get unwieldy. Consider, instead, returning the URL to those resources instead, especially if that data is not necessarily contained within the top level object.\n\nFor example, suppose you wanted to return the author of particular comments. You could use /articles/:articleId/comments/:commentId/author. But that's getting out of hand. Instead, return the URI for that particular user within the JSON response instead:\n\nTo eliminate confusion for API users when an error occurs, we should handle errors gracefully and return HTTP response codes that indicate what kind of error occurred. This gives maintainers of the API enough information to understand the problem that’s occurred. We don’t want errors to bring down our system, so we can leave them unhandled, which means that the API consumer has to handle them.\n• 401 Unauthorized - This means the user isn't not authorized to access a resource. It usually returns when the user isn't authenticated.\n• 403 Forbidden - This means the user is authenticated, but it's not allowed to access a resource.\n• 404 Not Found - This indicates that a resource is not found.\n• 500 Internal server error - This is a generic server error. It probably shouldn't be thrown explicitly.\n• 502 Bad Gateway - This indicates an invalid response from an upstream server.\n• 503 Service Unavailable - This indicates that something unexpected happened on server side (It can be anything like server overload, some parts of the system failed, etc.).\n\nWe should be throwing errors that correspond to the problem that our app has encountered. For example, if we want to reject the data from the request payload, then we should return a 400 response as follows in an Express API:\n\nIn the code above, we have a list of existing users in the users array with the given email.\n\nThen if we try to submit the payload with the email value that already exists in users, we'll get a 400 response status code with a 'User already exists' message to let users know that the user already exists. With that information, the user can correct the action by changing the email to something that doesn't exist.\n\nError codes need to have messages accompanied with them so that the maintainers have enough information to troubleshoot the issue, but attackers can’t use the error content to carry our attacks like stealing information or bringing down the system.\n\nWhenever our API does not successfully complete, we should fail gracefully by sending an error with information to help users make corrective action.\n\nThe databases behind a REST API can get very large. Sometimes, there's so much data that it shouldn’t be returned all at once because it’s way too slow or will bring down our systems. Therefore, we need ways to filter items.\n\nWe also need ways to paginate data so that we only return a few results at a time. We don't want to tie up resources for too long by trying to get all the requested data at once.\n\nFiltering and pagination both increase performance by reducing the usage of server resources. As more data accumulates in the database, the more important these features become.\n\nHere’s a small example where an API can accept a query string with various query parameters to let us filter out items by their fields:\n\nIn the code above, we have the req.query variable to get the query parameters. We then extract the property values by destructuring the individual query parameters into variables using the JavaScript destructuring syntax. Finally, we run filter on with each query parameter value to locate the items that we want to return.\n\nOnce we have done that, we return the results as the response. Therefore, when we make a GET request to the following path with the query string:\n\nas the returned response since we filtered by lastName and age.\n\nLikewise, we can accept the page query parameter and return a group of entries in the position from (page - 1) * 20 to page * 20.\n\nWe can also specify the fields to sort by in the query string. For instance, we can get the parameter from a query string with the fields we want to sort the data for. Then we can sort them by those individual fields.\n\nFor instance, we may want to extract the query string from a URL like:\n\nWhere + means ascending and - means descending. So we sort by author’s name in alphabetical order and datepublished from most recent to least recent.\n\nMost communication between client and server should be private since we often send and receive private information. Therefore, using SSL/TLS for security is a must.\n\nA SSL certificate isn't too difficult to load onto a server and the cost is free or very low. There's no reason not to make our REST APIs communicate over secure channels instead of in the open.\n\nPeople shouldn't be able to access more information that they requested. For example, a normal user shouldn't be able to access information of another user. They also shouldn't be able to access data of admins.\n\nTo enforce the principle of least privilege, we need to add role checks either for a single role, or have more granular roles for each user.\n\nIf we choose to group users into a few roles, then the roles should have the permissions that cover all they need and no more. If we have more granular permissions for each feature that users have access to, then we have to make sure that admins can add and remove those features from each user accordingly. Also, we need to add some preset roles that can be applied to a group users so that we don’t have to do that for every user manually.\n\nWe can add caching to return data from the local memory cache instead of querying the database to get the data every time we want to retrieve some data that users request. The good thing about caching is that users can get data faster. However, the data that users get may be outdated. This may also lead to issues when debugging in production environments when something goes wrong as we keep seeing old data.\n\nThere are many kinds of caching solutions like Redis, in-memory caching, and more. We can change the way data is cached as our needs change.\n\nFor instance, Express has the apicache middleware to add caching to our app without much configuration. We can add a simple in-memory cache into our server like so:\n\nThe code above just references the apicache middleware with apicache.middleware and then we have:\n\nto apply the caching to the whole app. We cache the results for five minutes, for example. We can adjust this for our needs.\n\nIf you are using caching, you should also include Cache-Control information in your headers. This will help users effectively use your caching system.\n\nWe should have different versions of API if we're making any changes to them that may break clients. The versioning can be done according to semantic version (for example, 2.0.6 to indicate major version 2 and the sixth patch) like most apps do nowadays.\n\nThis way, we can gradually phase out old endpoints instead of forcing everyone to move to the new API at the same time. The v1 endpoint can stay active for people who don’t want to change, while the v2, with its shiny new features, can serve those who are ready to upgrade. This is especially important if our API is public. We should version them so that we won't break third party apps that use our APIs.\n\nVersioning is usually done with /v1/, /v2/, etc. added at the start of the API path.\n\nFor example, we can do that with Express as follows:\n\nWe just add the version number to the start of the endpoint URL path to version them.\n\nThe most important takeaways for designing high-quality REST APIs is to have consistency by following web standards and conventions. JSON, SSL/TLS, and HTTP status codes are all standard building blocks of the modern web.\n\nPerformance is also an important consideration. We can increase it by not returning too much data at once. Also, we can use caching so that we don't have to query for data all the time.\n\nPaths of endpoints should be consistent, we use nouns only since the HTTP methods indicate the action we want to take. Paths of nested resources should come after the path of the parent resource. They should tell us what we’re getting or manipulating without the need to read extra documentation to understand what it’s doing."
    },
    {
        "link": "https://stackoverflow.com/questions/61821190/best-practice-design-typed-json-objects",
        "document": "Currently we are designing a System that will send JSON Objects to registered consumers. The consumers will be able to register for different Object Types that they want to receive. Currently we are designing the JSON Objects that will be consumed by the registered consumers.\n\nAs we see there are two possible approaches to define typed JSON Objects.\n• A generic JSON Object that has some properties set or it has them not set:\n• A Type field on each JSON Object that specifies which Type this object is from:\n\nFrom our point of view a more generic approach would be easier to handle within our system, because we would not need to handle multiple types, we just can append a property or leave it away.\n\nFrom point of view of a developer I could imagine that a type field could help the developer when handling such objects (f.e. mapping them to objects) or switch on the type field to execute some specific logic.\n\nFinally, to my question - which style is the one to be preferred (best-practice) when we want to make the life for consumers as easy as possible and why? Is there a best-practice approach for typed json objects?"
    },
    {
        "link": "https://stackoverflow.com/questions/76086717/parsing-an-sql-like-micro-language-which-is-a-superset-of-javascript",
        "document": "First, I'm legally required to warn you about the security dangers of using . With that out of the way:\n\nOption 1: forbid JS expressions from containing your keyword\n\nIf you can force users to never use your language keywords in their expressions, your parsing becomes very straightforward. If I read the grammar correctly, it can even be parsed by a regular expression. Here's an example of parsing a SET MODULE TO WHEN rule:\n\nThis is more restrictive than your intended language (e.g. will parse incorrectly), but your uppercase keywords help against simple mistakes, and you can catch more mistakes by ensuring at most one of each keyword exists.\n\nWhat syntax can you ignore? Operators, declarations, control flow, etc. What syntax can't you ignore? Anything that might accidentally contain your keyword (names, literal strings, comments, regexps).\n\nThis is harder than it sounds once you take into account all the different ways to declare strings, escape sequences, optional whitespace, etc. But it's doable, and doesn't require parsing the full JS syntax.\n\nTo reduce false positives, and to give users an escape hatch, consider ignoring keywords that appear inside balanced parenthesis/braces/brackets, as in:\n\nThis allows the user to have a JS variable \"MODULE\" without conflicting with your keyword.\n\nAcorn looks like a good option, and has a handy function:\n\nBy enabling the option, you can then alternate between looking for your keywords and handing over the rest of the string to Acorn to find an expression. Then look for a keyword after the last location, and repeat."
    },
    {
        "link": "https://alibabacloud.com/blog/how-to-efficiently-implement-sql-like-syntax-in-java_600079",
        "document": "This article mainly introduces how mainstream parsers implement the LIKE syntax logic.\n\nThis article mainly introduces how mainstream parsers implement the LIKE syntax logic. The author analyzes the advantages and disadvantages of several implementation methods and adopts the state machine method to optimize the performance of the scene step by step.\n\nRecently, we have been optimizing the LIKE syntax of the project. Since we have talked about SQL, we might as well look at how some mainstream parsers implement the LIKE syntax logic. The two mainstream SQL parsers are ANTLR and Calcite.\n\nANTLR is a powerful syntax parser and generator that can be used to read, process, execute, and transform structured text or binary files. It is widely used in some SQL frameworks of big data. For example, Hive's lexical file is written by ANTLR3, and Presto's lexical file is implemented by ANTLR4. However, ANTLR does not directly implement the specific syntax, so there is no way to find the implementation statement.\n\nCalcite simplifies the process of ANTLR code generation. It provides standard SQL language, multiple query optimization, and the ability to connect various data sources. At the same time, Calcite has a good pluggable architecture design, which allows users to easily put a SQL shell on their systems and provides efficient query performance optimization. Therefore, it won the favor of many developers. The Calcite implementation for LIKE logical matching is attached below:\n\nThere are also some other compilers or middleware attached (such as TDDL). Let's take a brief look at the implementation. The whole is similar, while the logic of buildPattern is not the same.\n\nAt this point, to sum up, many projects are completed based on regular expressions. Next, I sorted out several ways I have implemented recently.\n\nJava's regular expressions have a different syntax from SQL's LIKE. The most important thing is that you must escape the Java special characters. A simple handling of the regexParse function is the traversal replacement operation for special characters .\n\nThis method is simple and clear at the code level, but the performance is poor. The repeated use of replace method has already performed multiple traversals. Here is a point that can be optimized. can be used for single character replacement.\n\nThe regular expression execution engine used by the Java language is non-deterministic finite automaton (NFA). This engine is characterized by powerful functions, but the backtracking mechanism leads to slow execution efficiency (when backtracking is serious, it can lead to 100% CPU utilization of the machine and directly jam the machine). Optimization related to Pattern processing can be done in the regular expression, and the compiled Pattern object can be cached to avoid repeatedly compiling patterns (one pattern needs to be cached for each pattern-expr str). Try choosing the lazy mode and exclusive mode and avoid using the greedy mode (default).\n\nThe three modes here are greedy mode, lazy mode, and exclusive mode.\n\nThe number indicator defaults to the greedy mode. The expression of the greedy mode will continue to match until it cannot be matched. If we find that the result of the expression does not match the expected result, it is probably because we thought the expression would only match the first few characters, but instead, it will keep matching.\n\nIn contrast to the greedy mode, the lazy mode will try to match less content, as described above for the percent sign.\n\nThe exclusive mode should be regarded as a variant of the greedy mode. It will try to match more content. The difference is that the backtracking mechanism will not be triggered in case of matching failure but will continue to judge backward, so this mode is the most efficient.\n\nWhat is better than directly writing a customized version of LIKE? It is simple and easy to use. The memory and performance are almost the best. The most complicated situation is O(n*m), which is similar to doing an algorithm problem. It is a pure match process and does not need pre-cache processing. The following double pointer method can be implemented by dynamic planning. (Part of the code is hidden).\n\nThere is a scene we have to consider, which is backtracking. For example, it is not the same for pattern = \"a%bcd\" to match abcde and abcdbcdbcd at the same time. As such, it is necessary to record the backtrace mark. (I will talk about a method that does not need backtrace later.)\n\nIf this is the most memory-saving method, only a few internal variables are used to complete the entire work. If you insist on listing the shortcomings, we can say the maintainability is too poor. The logic processing is kind of connected, and if you make some extensions to the syntax in the future, it will be tricky.\n\nA state machine has three components: states, events, and actions.\n\nStates: All possible statuses, including the current state and the state to be migrated after the condition is met\n\nEvent: When a condition is met, an action is triggered, or a state transition is performed.\n\nAction: The action that is executed after the condition is met. After the action is executed, it can migrate to the new state or remain in the original state. The action is not required. When the condition is met, you can directly migrate to the new state without performing any action.\n\nGeneral use involves the exhaustive method, look-up table method, and state mode.\n\nThe simplest implementation of the state machine uses if-else or switch-case to translate each state transition into code regarding the state transition diagram. For a simple state machine, the implementation of the branch logic method is acceptable. For a complex state machine, the disadvantage is that it is easy to miss writing and mistakenly write some state transitions. In addition, the code is filled with if-else, and the readability and maintainability are poor.\n\nThe look-up table method is suitable for state machines with many types of implementation states, events, and complex state transitions. The use of a two-dimensional array to represent the state transition table can significantly improve the readability and maintainability of the code. State transitions and action execution are represented by array transitionTable and actionTable in the representation of a two-dimensional array, respectively. In both arrays, the x-axis represents the current state, the y-axis represents the event that occurred, and the values represent the new state after the transition and the action to be performed, respectively.\n\nThe look-up table method cites an introductory example here.\n\nFor example, there is a string with the pattern SCSAS, and we try to sort out the state transition table corresponding to this string. ! indicates characters unrelated to S, C, and A. The following is the finite-state machine state transition table.\n\nThe following figure shows the finite-state machine state transition diagram with the pattern SCSAS.\n\nNext is the match process. Get the dest string and match the characters according to the state data in the table until the value of State = 5 is found.\n\nThe state mode is usually a state machine that expresses few implementation states and simple state transitions but with complex business logic contained in event-triggered actions. The state transition and action execution triggered by events in different states are split into different state classes to avoid branch judgment logic.\n\nCompared with the look-up table method, when the state mode introduces more state classes, we recommend using the look-up table method for more states. The state mode is more suitable for fewer states with complex actions.\n\nThen, the implementation of LIKE based on the state mode will be more convenient. We need to do a simple analysis before we start. If the test statement is CBEED like %B%E%D%, this result must be true. How do we implement such a state machine?\n\nThe specific disassembly can be divided into two parts: the construction of the state machine and the matching of the operation.\n\nThe process of building is the process of parsing and loading the pattern. I used a linked list to construct it. Implementation is the process of traversing the construction. Compile time complexity O(n).\n\nNext is the match string CBEED process. The implementation of the code is to traverse the matching process. Match time complexity O(n*m).\n\nThen, the final matching result can be like Figure 1 or Figure 2, depending on whether the matching logic is reversed or positive priority.\n\nThe difficulty here is that the matching possibilities are not unique and not every percent sign corresponds to the same character. For example:\n\nSo, when I did the matcher state design, I defined five state types, which ultimately helped me implement the overall logic. The approach above also needs to be implemented with backtracking in mind, and JUMP is specifically designed for backtracking.\n\n\n\nTherefore, five classes related to state machines need to be defined, which is sufficient to complete our functions and extensibility.\n\nI am thinking about how to optimize this reversible scenario. Since the existence of backtracking makes the whole logic complicated, my goal is to make the expression of complex logic simpler. I try to handle more things in the compilation stage to obtain better performance when matching. Compared with the previous case, I carried out a split-LIKE operation on the pattern data, expressing the non-percent sign part in the form of a string, and the effect was unexpectedly good. For example, %BF%EA%D. The following is its optimization process:\n\nI have made scene type definitions for several scenes:\n\nAny pattern with % can be parsed into these three types, and the number of nodes in our state machine has been optimized a lot. Based on this implementation, the backtracking logic no longer exists, and it can be identified through the joint check of the current node and the next node. Finally, O(n) can be achieved. At the same time, for some scenarios, it can be identified through type + length judgment without further traversal. Let's look at the previous backtracking case, pattern = \"a%bcd\" matches abcde and abcdbcdbcd at the same time. Do special processing for this RIGHT + LEFT scenario.\n\nCan we do further optimization so far? It can be optimized further based on common scenarios. I will try to list these scenarios.\n\nMore than 80% of usage scenarios are the conventional applications above. At this time, I think I can identify such scenarios at compile time and make further optimization.\n\nTry to use the ability of JDK at the bottom to do such a thing, and there may be no need to traverse the judgment such as length test.\n\nThe code given above is to see the operation inside. It can be optimized and streamlined further according to the code style (functional Function, interface, etc.) that you are good at.\n\nThe chart above is a test case designed according to a typical scenario. The general conclusion of the data is that the longer the string is, the slower it will be for the general algorithm. The state machine will perform better than the general algorithm in most cases, and the performance is similar in quite a few cases.\n\nAccording to the comprehensive evaluation of multiple advantages and disadvantages, I think state machine implementation > direct implementation > regular expression implementation. The compilation process of the state machine and regular expression can be put into the code compilation stage or initialization, which can avoid frequent performance loss. In terms of extensibility and maintainability, I prefer to implement LIKE syntax based on state machine."
    },
    {
        "link": "https://quora.com/Is-there-a-general-purpose-programming-language-with-SQL-like-syntax",
        "document": "Something went wrong. Wait a moment and try again."
    },
    {
        "link": "https://bcpublication.org/index.php/FSE/article/view/6208",
        "document": "John M. Zelle and Raymond J. Mooney: Learning to parse database queries using inductive logic programming. Proceedings of the thirteenth national conference on Artificial intelligence (Portland, Oregon, 1996). Vol.2, p1050–1055.\n\nCatherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev: Improving Text-to-SQL Evaluation Methodology. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Melbourne, Australia, July ,2018). Vol.1, p351–360.\n\nVictor Zhong, Caiming Xiong, and Richard Socher: Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning.arXiv:1709.00103, 2017.\n\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev: Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (Brussels, Belgium, October-November ,2018), p3911–3921.\n\nBinyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Bowen Li, Jian Sun, and Yongbin Li: \"S\" ^\"2\" SQL: Injecting syntax to question-schema interaction graph encoder for text-to-sql parsers.2022. arXiv:2203.06958.\n\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and Minjoon Seo: A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization. arXiv:1902.01069, 2019.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Minneapolis, Minnesota, June,2019).\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin: Attention Is All You Need. arXiv:1706.03762, 2017.\n\nXiaojun Xu, Chang Liu, and Dawn Song: SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning.arXiv:1711.04436, 2017.\n\nDongHyun Choi, Myeong Cheol Shin, EungGyun Kim, and Dong Ryeol Shin: RYANSQL: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-SQL in Cross-Domain Databases.ArXiv: 2004.03125,2020.\n\nPengcheng He, Yi Mao, Kaushik Chakrabarti, and Weizhu Chen: X-SQL: reinforce schema representation with context. ArXiv:1908.08113, 2019.\n\nJiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao,Jian-Guang Lou, Ting Liu, and Dongmei Zhang: Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation. ArXiv:1905. 08205, 2019.\n\nXi Victoria Lin, Richard Socher, and Caiming Xiong: Bridging textual and tabular data for crossdomain text-to-SQL semantic parsing. Findings of the Association for Computational Linguistics: EMNLP 2020(Online, November,2020), p4870–4888.\n\nBen Bogin, Matt Gardner, and Jonathan Berant: Global reasoning over database structures for text-to-SQL parsing. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (Hong Kong, China, November,2019), p3657–3662.\n\nZhi Chen, Lu Chen, Yanbin Zhao, Ruisheng Cao, Zihan Xu, Su Zhu, and Kai Yu: Shadowgnn: Graph projection neural network for text-to-sql parser. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies(Online, June,2021), p 5567–5577.\n\nRuichu Cai, Jinjie Yuan, Boyan Xu, and Zhifeng Hao: SADGA: Structure-aware dual graph aggregation network for text-to-sql. Advances in Neural Information Processing Systems (2021). Vol.34, p7664–7676.\n\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova: Compositional generalization and natural language variation: Can a semantic parsing approach handle both? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Online, August,2021). Vol.1, p922–938, Online.\n\nTorsten Scholak, Nathan Schucher, and Dzmitry Bahdanau: PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (Online and Punta Cana, Dominican Republic.November,2021), p9895–9901.\n\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson: RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Online, July,2020), p7567-7578.\n\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani: Self-Attention with Relative Position Representations. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (New Orleans, Louisiana, June,2018). Vol.2, p464–468.\n\nYujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, and Pengsheng Huang: Towards robustness of text-to-SQL models against synonym substitution. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Online, August,2021). Vol.1,p2505-2515.\n\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer: Learning a neural semantic parser from user feedback. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vancouver, Canada, July,2017). Vol .1, p963–973.\n\nJohn M. Zelle and Raymond J. Mooney: Learning to parse database queries using inductive logic programming. Proceedings of the Thirteenth National Conference on Artificial Intelligence (1996). Vol. 2, p1050–1055.\n\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning: Stanza: A Python Natural Language Processing Toolkit for Many Human Languages. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (Online, July,2020), p101–108.\n\nBinyuan Hui, Ruiying Geng, Lihan Wang, Bowen Qin, Bowen Li, Jian Sun, and Yongbin Li: \"S\" ^\"2\" SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers.arXiv: 2203.06958,2022.\n\nRuisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao, Su Zhu, and Kai Yu.: LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations. ArXiv:2106.01093,2021.\n\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, and Zhouhan Lin: RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL. arXiv:2205.06983,2022.\n\nJinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, Luo Si, and Yongbin Li: Graphix-t5: Mixing pretrained transformers with graph-aware layers for text-to-sql parsing. arXiv:2301.07507,2023."
    },
    {
        "link": "https://stackoverflow.com/questions/10379956/parsing-sql-like-syntax-design-pattern",
        "document": "I am trying mock sql syntax to build a simple sql like interface to a key-value storage. The values are essentially POJOs\n\nAn example would be\n\nOBJ_POOL is just a list of POJOs of the same class. In this example A would be the base class.\n\nI am using Antlr to parse the above statement to get an AST, and then to use Apache BeanUtils to reflectively get/set the field names.\n\nI wrote the grammar thats builds an AST Now I am facing two problems\n• How should the visitor be implemented for the where clause ? A.B.X = 45 implies all objects having field X as 45, how should the filtering happen is there any nice way to do this ?\n• Is there any way to traverse the generated AST without cluttering the visitor code with custom logic (storage access,property getters/setters etc..)\n\nThe second problem is more worrying since there might be many things that the statement might do.\n\nIn a nutshell any suggestions/links/design-patterns to nicely parse a small subset of the sql select statment would be greatly appreciated"
    }
]