[
    {
        "link": "https://docs.python.org/3/library/math.html",
        "document": "This module provides access to the mathematical functions defined by the C standard.\n\nThese functions cannot be used with complex numbers; use the functions of the same name from the module if you require support for complex numbers. The distinction between functions which support complex numbers and those which don’t is made since most users do not want to learn quite as much mathematics as required to understand complex numbers. Receiving an exception instead of a complex result allows earlier detection of the unexpected complex number used as a parameter, so that the programmer can determine how and why it was generated in the first place.\n\nThe following functions are provided by this module. Except when explicitly noted otherwise, all return values are floats.\n\nReturn the number of ways to choose k items from n items without repetition and without order. Evaluates to when and evaluates to zero when . Also called the binomial coefficient because it is equivalent to the coefficient of k-th term in polynomial expansion of . Raises if either of the arguments are not integers. Raises if either of the arguments are negative. Return n factorial as an integer. Raises if n is not integral or is negative. Changed in version 3.10: Floats with integral values (like ) are no longer accepted. Return the greatest common divisor of the specified integer arguments. If any of the arguments is nonzero, then the returned value is the largest positive integer that is a divisor of all arguments. If all arguments are zero, then the returned value is . without arguments returns . Changed in version 3.9: Added support for an arbitrary number of arguments. Formerly, only two arguments were supported. Return the integer square root of the nonnegative integer n. This is the floor of the exact square root of n, or equivalently the greatest integer a such that a² ≤ n. For some applications, it may be more convenient to have the least integer a such that n ≤ a², or in other words the ceiling of the exact square root of n. For positive n, this can be computed using . Return the least common multiple of the specified integer arguments. If all arguments are nonzero, then the returned value is the smallest positive integer that is a multiple of all arguments. If any of the arguments is zero, then the returned value is . without arguments returns . Return the number of ways to choose k items from n items without repetition and with order. Evaluates to when and evaluates to zero when . If k is not specified or is , then k defaults to n and the function returns . Raises if either of the arguments are not integers. Raises if either of the arguments are negative.\n\nReturn the ceiling of x, the smallest integer greater than or equal to x. If x is not a float, delegates to , which should return an value. Return the absolute value of x. Return the floor of x, the largest integer less than or equal to x. If x is not a float, delegates to , which should return an value. Fused multiply-add operation. Return , computed as though with infinite precision and range followed by a single round to the format. This operation often provides better accuracy than the direct expression . This function follows the specification of the fusedMultiplyAdd operation described in the IEEE 754 standard. The standard leaves one case implementation-defined, namely the result of and . In these cases, returns a NaN, and does not raise any exception. Return the floating-point remainder of , as defined by the platform C library function . Note that the Python expression may not return the same result. The intent of the C standard is that be exactly (mathematically; to infinite precision) equal to for some integer n such that the result has the same sign as x and magnitude less than . Python’s returns a result with the sign of y instead, and may not be exactly computable for float arguments. For example, is , but the result of Python’s is , which cannot be represented exactly as a float, and rounds to the surprising . For this reason, function is generally preferred when working with floats, while Python’s is preferred when working with integers. Return the fractional and integer parts of x. Both results carry the sign of x and are floats. Note that has a different call/return pattern than its C equivalents: it takes a single argument and return a pair of values, rather than returning its second return value through an ‘output parameter’ (there is no such thing in Python). Return the IEEE 754-style remainder of x with respect to y. For finite x and finite nonzero y, this is the difference , where is the closest integer to the exact value of the quotient . If is exactly halfway between two consecutive integers, the nearest even integer is used for . The remainder thus always satisfies . Special cases follow IEEE 754: in particular, is x for any finite x, and and raise for any non-NaN x. If the result of the remainder operation is zero, that zero will have the same sign as x. On platforms using IEEE 754 binary floating point, the result of this operation is always exactly representable: no rounding error is introduced. Return x with the fractional part removed, leaving the integer part. This rounds toward 0: is equivalent to for positive x, and equivalent to for negative x. If x is not a float, delegates to , which should return an value. For the , , and functions, note that all floating-point numbers of sufficiently large magnitude are exact integers. Python floats typically carry no more than 53 bits of precision (the same as the platform C double type), in which case any float x with necessarily has no fractional bits.\n\nReturn a float with the magnitude (absolute value) of x but the sign of y. On platforms that support signed zeros, returns -1.0. Return the mantissa and exponent of x as the pair . m is a float and e is an integer such that exactly. If x is zero, returns , otherwise . This is used to “pick apart” the internal representation of a float in a portable way. Note that has a different call/return pattern than its C equivalents: it takes a single argument and return a pair of values, rather than returning its second return value through an ‘output parameter’ (there is no such thing in Python). Return if the values a and b are close to each other and otherwise. Whether or not two values are considered close is determined according to given absolute and relative tolerances. If no errors occur, the result will be: . rel_tol is the relative tolerance – it is the maximum allowed difference between a and b, relative to the larger absolute value of a or b. For example, to set a tolerance of 5%, pass . The default tolerance is , which assures that the two values are the same within about 9 decimal digits. rel_tol must be nonnegative and less than . abs_tol is the absolute tolerance; it defaults to and it must be nonnegative. When comparing to , is computed as , which is for any nonzero and rel_tol less than . So add an appropriate positive abs_tol argument to the call. The IEEE 754 special values of , , and will be handled according to IEEE rules. Specifically, is not considered close to any other value, including . and are only considered close to themselves. Return if x is neither an infinity nor a NaN, and otherwise. (Note that is considered finite.) Return if x is a positive or negative infinity, and otherwise. Return if x is a NaN (not a number), and otherwise. Return . This is essentially the inverse of function . Return the floating-point value steps steps after x towards y. If x is equal to y, return y, unless steps is zero.\n• None goes up: towards positive infinity.\n• None goes down: towards minus infinity.\n• None goes towards zero.\n• None goes away from zero. Return the value of the least significant bit of the float x:\n• None If x is a NaN (not a number), return x.\n• None If x is equal to zero, return the smallest positive denormalized representable float (smaller than the minimum positive normalized float, ).\n• None If x is equal to the largest positive representable float, return the value of the least significant bit of x, such that the first float smaller than x is .\n• None Otherwise (x is a positive finite number), return the value of the least significant bit of x, such that the first float bigger than x is . ULP stands for “Unit in the Last Place”. See also and .\n\nReturn e raised to the power x, where e = 2.718281… is the base of natural logarithms. This is usually more accurate than or . Return e raised to the power x, minus 1. Here e is the base of natural logarithms. For small floats x, the subtraction in can result in a significant loss of precision; the function provides a way to compute this quantity to full precision: With one argument, return the natural logarithm of x (to base e). With two arguments, return the logarithm of x to the given base, calculated as . Return the natural logarithm of 1+x (base e). The result is calculated in a way which is accurate for x near zero. Return the base-2 logarithm of x. This is usually more accurate than . returns the number of bits necessary to represent an integer in binary, excluding the sign and leading zeros. Return the base-10 logarithm of x. This is usually more accurate than . Return x raised to the power y. Exceptional cases follow the IEEE 754 standard as far as possible. In particular, and always return , even when x is a zero or a NaN. If both x and y are finite, x is negative, and y is not an integer then is undefined, and raises . Unlike the built-in operator, converts both its arguments to type . Use or the built-in function for computing exact integer powers. Changed in version 3.11: The special cases and were changed to return instead of raising , for consistency with IEEE 754.\n\nReturn the Euclidean distance between two points p and q, each given as a sequence (or iterable) of coordinates. The two points must have the same dimension. Return an accurate floating-point sum of values in the iterable. Avoids loss of precision by tracking multiple intermediate partial sums. The algorithm’s accuracy depends on IEEE-754 arithmetic guarantees and the typical case where the rounding mode is half-even. On some non-Windows builds, the underlying C library uses extended precision addition and may occasionally double-round an intermediate sum causing it to be off in its least significant bit. For further discussion and two alternative approaches, see the ASPN cookbook recipes for accurate floating-point summation. Return the Euclidean norm, . This is the length of the vector from the origin to the point given by the coordinates. For a two dimensional point , this is equivalent to computing the hypotenuse of a right triangle using the Pythagorean theorem, . Changed in version 3.8: Added support for n-dimensional points. Formerly, only the two dimensional case was supported. Changed in version 3.10: Improved the algorithm’s accuracy so that the maximum error is under 1 ulp (unit in the last place). More typically, the result is almost always correctly rounded to within 1/2 ulp. Calculate the product of all the elements in the input iterable. The default start value for the product is . When the iterable is empty, return the start value. This function is intended specifically for use with numeric values and may reject non-numeric types. Return the sum of products of values from two iterables p and q. Raises if the inputs do not have the same length. For float and mixed int/float inputs, the intermediate products and sums are computed with extended precision.\n\nThe mathematical constant π = 3.141592…, to available precision. The mathematical constant e = 2.718281…, to available precision. The mathematical constant τ = 6.283185…, to available precision. Tau is a circle constant equal to 2π, the ratio of a circle’s circumference to its radius. To learn more about Tau, check out Vi Hart’s video Pi is (still) Wrong, and start celebrating Tau day by eating twice as much pie! A floating-point positive infinity. (For negative infinity, use .) Equivalent to the output of . A floating-point “not a number” (NaN) value. Equivalent to the output of . Due to the requirements of the IEEE-754 standard, and are not considered to equal to any other numeric value, including themselves. To check whether a number is a NaN, use the function to test for NaNs instead of or . Example: Changed in version 3.11: It is now always available. CPython implementation detail: The module consists mostly of thin wrappers around the platform C math library functions. Behavior in exceptional cases follows Annex F of the C99 standard where appropriate. The current implementation will raise for invalid operations like or (where C99 Annex F recommends signaling invalid operation or divide-by-zero), and for results that overflow (for example, ). A NaN will not be returned from any of the functions above unless one or more of the input arguments was a NaN; in that case, most functions will return a NaN, but (again following C99 Annex F) there are some exceptions to this rule, for example or . Note that Python makes no effort to distinguish signaling NaNs from quiet NaNs, and behavior for signaling NaNs remains unspecified. Typical behavior is to treat all NaNs as though they were quiet. Complex number versions of many of these functions."
    },
    {
        "link": "https://docs.python.org/3.6/library/math.html",
        "document": "This module is always available. It provides access to the mathematical functions defined by the C standard.\n\nThese functions cannot be used with complex numbers; use the functions of the same name from the module if you require support for complex numbers. The distinction between functions which support complex numbers and those which don’t is made since most users do not want to learn quite as much mathematics as required to understand complex numbers. Receiving an exception instead of a complex result allows earlier detection of the unexpected complex number used as a parameter, so that the programmer can determine how and why it was generated in the first place.\n\nThe following functions are provided by this module. Except when explicitly noted otherwise, all return values are floats.\n\nReturn the ceiling of x, the smallest integer greater than or equal to x. If x is not a float, delegates to , which should return an value. Return a float with the magnitude (absolute value) of x but the sign of y. On platforms that support signed zeros, returns -1.0. Return the absolute value of x. Return x factorial. Raises if x is not integral or is negative. Return the floor of x, the largest integer less than or equal to x. If x is not a float, delegates to , which should return an value. Return , as defined by the platform C library. Note that the Python expression may not return the same result. The intent of the C standard is that be exactly (mathematically; to infinite precision) equal to for some integer n such that the result has the same sign as x and magnitude less than . Python’s returns a result with the sign of y instead, and may not be exactly computable for float arguments. For example, is , but the result of Python’s is , which cannot be represented exactly as a float, and rounds to the surprising . For this reason, function is generally preferred when working with floats, while Python’s is preferred when working with integers. Return the mantissa and exponent of x as the pair . m is a float and e is an integer such that exactly. If x is zero, returns , otherwise . This is used to “pick apart” the internal representation of a float in a portable way. Return an accurate floating point sum of values in the iterable. Avoids loss of precision by tracking multiple intermediate partial sums: The algorithm’s accuracy depends on IEEE-754 arithmetic guarantees and the typical case where the rounding mode is half-even. On some non-Windows builds, the underlying C library uses extended precision addition and may occasionally double-round an intermediate sum causing it to be off in its least significant bit. For further discussion and two alternative approaches, see the ASPN cookbook recipes for accurate floating point summation. Return the greatest common divisor of the integers a and b. If either a or b is nonzero, then the value of is the largest positive integer that divides both a and b. returns . Return if the values a and b are close to each other and otherwise. Whether or not two values are considered close is determined according to given absolute and relative tolerances. rel_tol is the relative tolerance – it is the maximum allowed difference between a and b, relative to the larger absolute value of a or b. For example, to set a tolerance of 5%, pass . The default tolerance is , which assures that the two values are the same within about 9 decimal digits. rel_tol must be greater than zero. abs_tol is the minimum absolute tolerance – useful for comparisons near zero. abs_tol must be at least zero. If no errors occur, the result will be: . The IEEE 754 special values of , , and will be handled according to IEEE rules. Specifically, is not considered close to any other value, including . and are only considered close to themselves. Return if x is neither an infinity nor a NaN, and otherwise. (Note that is considered finite.) Return if x is a positive or negative infinity, and otherwise. Return if x is a NaN (not a number), and otherwise. Return . This is essentially the inverse of function . Return the fractional and integer parts of x. Both results carry the sign of x and are floats. Return the value x truncated to an (usually an integer). Delegates to . Note that and have a different call/return pattern than their C equivalents: they take a single argument and return a pair of values, rather than returning their second return value through an ‘output parameter’ (there is no such thing in Python). For the , , and functions, note that all floating-point numbers of sufficiently large magnitude are exact integers. Python floats typically carry no more than 53 bits of precision (the same as the platform C double type), in which case any float x with necessarily has no fractional bits.\n\nReturn . For small floats x, the subtraction in can result in a significant loss of precision; the function provides a way to compute this quantity to full precision: With one argument, return the natural logarithm of x (to base e). With two arguments, return the logarithm of x to the given base, calculated as . Return the natural logarithm of 1+x (base e). The result is calculated in a way which is accurate for x near zero. Return the base-2 logarithm of x. This is usually more accurate than . returns the number of bits necessary to represent an integer in binary, excluding the sign and leading zeros. Return the base-10 logarithm of x. This is usually more accurate than . Return raised to the power . Exceptional cases follow Annex ‘F’ of the C99 standard as far as possible. In particular, and always return , even when is a zero or a NaN. If both and are finite, is negative, and is not an integer then is undefined, and raises . Unlike the built-in operator, converts both its arguments to type . Use or the built-in function for computing exact integer powers.\n\nThe mathematical constant π = 3.141592…, to available precision. The mathematical constant e = 2.718281…, to available precision. The mathematical constant τ = 6.283185…, to available precision. Tau is a circle constant equal to 2π, the ratio of a circle’s circumference to its radius. To learn more about Tau, check out Vi Hart’s video Pi is (still) Wrong, and start celebrating Tau day by eating twice as much pie! A floating-point positive infinity. (For negative infinity, use .) Equivalent to the output of . A floating-point “not a number” (NaN) value. Equivalent to the output of . CPython implementation detail: The module consists mostly of thin wrappers around the platform C math library functions. Behavior in exceptional cases follows Annex F of the C99 standard where appropriate. The current implementation will raise for invalid operations like or (where C99 Annex F recommends signaling invalid operation or divide-by-zero), and for results that overflow (for example, ). A NaN will not be returned from any of the functions above unless one or more of the input arguments was a NaN; in that case, most functions will return a NaN, but (again following C99 Annex F) there are some exceptions to this rule, for example or . Note that Python makes no effort to distinguish signaling NaNs from quiet NaNs, and behavior for signaling NaNs remains unspecified. Typical behavior is to treat all NaNs as though they were quiet. Complex number versions of many of these functions."
    },
    {
        "link": "https://w3schools.com/python/module_math.asp",
        "document": "Python has a built-in module that you can use for mathematical tasks.\n\nThe module has a set of methods and constants."
    },
    {
        "link": "https://geeksforgeeks.org/python-math-function-sqrt",
        "document": "math.sqrt() returns the square root of a number. It is an inbuilt function in the Python programming language, provided by the math module. In this article, we will learn about how to find the square root using this function.\n\nWe need to import before using this function.\n• None x: A number greater than or equal to 0.\n• None The square root of the number x.\n\nLet’s look at some different uses of math.sqrt() .\n\nExample 1: Check if number is prime\n\nmath.sqrt() can be used to optimize prime number checking. We only need to check divisibility up to the square root of the number.\n• n > 1, we check for factors from 2 to sqrt(n).\n• None If any divisor is found, it prints “Not prime”.\n• None If no divisors are found, it prints “Prime”.\n\nWe can use math.sqrt() to find the hypotenuse of a right-angled triangle using the Pythagorean theorem.\n• None The formula used is c = sqrt(a^2 + b^2)\n• None It calculates the hypotenuse c using the values of a b\n\nmath.sqrt() does not work for negative numbers. It raises a ValueError if we pass a number less than 0.\n• None Square roots of negative numbers are not real numbers."
    },
    {
        "link": "https://uvm.edu/~cbcafier/cs1210/book/05_functions/intro_to_math_module.html",
        "document": ""
    },
    {
        "link": "https://geeksforgeeks.org/precision-handling-python",
        "document": "Python in its definition allows handling the precision of floating-point numbers in several ways using different functions. Most of them are defined under the “math” module. In this article, we will use high-precision calculations in Python with Decimal in Python.\n\nwe will explore the various functions that Python provides for managing precise data and decimal points.\n\nPython provides for managing precise data and decimal points, the round function is used to round a number to a specified number of decimal places. Alternatively, string formatting options, such as the f-string syntax or the method, allow you to format numbers with precise decimal places.\n\nIn Python, we can handle precision values using getcontext(). The function is used to access the current decimal context, which provides control over precision settings. The precision is used to set the desired number of decimal places for mathematical operations using the decimal class.\n\nIn Python, we can handle precision values using Math Module. In this example, we will cover some Python math methods. such as trunc() method, ceil() method, and floor() method.\n• trunc() – This function is used to eliminate all decimal parts of the floating-point number and return the integer without the decimal part.\n• ceil() – This function is used to print the least integer greater than the given number.\n• floor() – This function is used to print the greatest integer smaller than the given integer.\n\nIn Python, we can handle precision values using Decimal Module. In this example, we will see How to Limit Float to Two Decimal Points in Python. In Python float precision to 2 floats in Python, and in Python float precision to 3. There are many ways to set the precision of the floating-point values. Some of them are discussed below.\n• Using % – % operator is used to format as well as set precision in Python. This is similar to “printf” statement in\n• Using format() – This is yet another way to format the string for setting precision.\n• Using round(x,n) – This function takes 2 arguments, number, and the number till which we want the decimal part rounded.\n• Using f-string – PEP 498 introduced a new string formatting mechanism known as Literal String Interpolation or more commonly as F-strings\n\nHow to Get 2 Decimal Places in Python?\n\nHow Do I Use .2f in Python?\n\nWhat Are the Three Types of Precision?\n\nWhat is the Maximum Value of Precision in Python?"
    },
    {
        "link": "https://stackoverflow.com/questions/19687812/numerical-precision-in-python",
        "document": "I've been doing simple numerical experiments with python, like computing factorials. For instance, compute the factorial of 32:\n\nI want to point out that my routine calculates the logarithm of the factorial, it calculates the sumation of logarithms starting from 1 to 32 (in this case) and then I just take the exp function (I do it this way because of stuff learned from Fortran 90).\n\nIt is a surprise that the correct answer is\n\nI would be very happy if someone can point me out to references where I can look for correct numerical answers in Python. The documentation it's ok but only if one want \"short\" numbers."
    },
    {
        "link": "https://medium.com/@goldengrisha/understanding-floating-point-precision-issues-in-python-a-practical-guide-5e17b2f14057",
        "document": "If you’ve ever worked with numbers in Python, or any programming language for that matter, you may have encountered a puzzling scenario where calculations that should yield the same result produce slightly different values. Often, this is due to floating-point precision issues, which are subtle yet fundamental limitations in how computers represent numbers. In this article, we’ll explore what floating-point numbers are, why these precision errors happen, and how you can handle them effectively.\n\nAt the core of most numerical computations are floating-point numbers. These are used to represent real numbers (numbers with decimal points) in a way that balances accuracy and performance. Floating-point numbers are stored in a format that includes:\n\n1. A sign (positive or negative),\n\n2. A mantissa (or fraction),\n\n3. An exponent (which shifts the decimal point).\n\nIn Python, floating-point numbers are represented using the IEEE 754 standard, which is the industry norm for binary floating-point arithmetic. While this system allows us to work with a wide range of values, it comes with a limitation: finite precision.\n\nThe problem with floating-point arithmetic stems from the fact that many decimal numbers cannot be represented exactly in binary. For example, the decimal number `0.1` cannot be precisely stored as a binary floating-point number. Instead, it is approximated by a fraction that is as close as possible within the limits of the system’s precision.\n\nTo see this in action, consider the following Python code:\n\nHere, adding `0.1` and `0.2` should yield `0.3`, but instead, we get `0.30000000000000004`. The reason is that neither `0.1` nor `0.2` has an exact binary representation, and the small rounding errors accumulate when the values are added.\n\nLet’s consider a more complex scenario, one involving a physics simulation where we calculate forces. Suppose we have the following Python function to compute a force value `v(k, n)` and sum it over multiple iterations:\n\nThis code calculates a summation of forces using two methods: one (`total`) accumulates results in batches, and another (`old_total`) accumulates continuously within the loop. After running this, you might find a small difference in the results:\n\nBoth `total` and `old_total` are supposed to hold the same value, but due to floating-point precision issues, a tiny discrepancy arises. This is a classic floating-point issue: repeated additions of very small values can accumulate small errors over time.\n\nWhy Does This Happen?\n\nFloating-point numbers have a **limited number of bits** to store values. As computations grow, especially when dealing with very small or very large numbers, the system has to round off values to fit within this finite space. When many small floating-point operations (like additions) are performed, these rounding errors can accumulate, leading to small but noticeable differences in the results.\n\nThe difference in our example (`0.6850971813742038` vs `0.6850971813742046`) is insignificant in most cases, but in high-precision applications, even this small error might matter.\n\nWhen comparing floating-point numbers, it’s a bad idea to check for exact equality. Instead, use a small tolerance to account for potential floating-point errors. Python provides a built-in method called `math.isclose()` for this purpose:\n\nThe `rel_tol` argument specifies the relative tolerance, or how close the two numbers must be to be considered equal. This method is especially useful when dealing with cumulative floating-point operations like in our summing example.\n\n2. Using the `decimal` Module for Higher Precision\n\nIn cases where precision is critical, you can use Python’s `decimal` module, which provides support for arbitrary precision arithmetic. This ensures that numbers are stored with as much precision as you need, though at the cost of computational speed.\n\nHere’s how you can rewrite the `v()` function using `decimal` for higher precision:\n\nWith `decimal`, you can store numbers with many more digits than the default floating-point representation, thus reducing the impact of rounding errors. However, be aware that this can slow down your program because high-precision arithmetic is more computationally expensive.\n\nAnother way to minimize floating-point errors is to reduce the number of operations involving small numbers. For example, accumulating sums over large loops can be split into smaller, more manageable parts, or grouped in a way that reduces the impact of rounding.\n\nIn many numerical computing scenarios, specialized libraries like NumPy and SciPy handle these precision issues for you by implementing optimized algorithms for floating-point arithmetic. These libraries are highly recommended when working with large-scale numerical computations, as they are both fast and reliable.\n\nFloating-point precision issues are an unavoidable reality of working with real numbers in computing. While they can be frustrating, understanding why they occur and how to mitigate them will help you avoid errors in your code. In most cases, comparing floating-point numbers with a tolerance or using high-precision tools like Python’s `decimal` module can provide satisfactory solutions.\n\nAs a rule of thumb:\n\n- Use `math.isclose()` for comparisons.\n\n- Avoid unnecessary floating-point operations where possible.\n\n- Leverage libraries like NumPy for advanced numerical tasks.\n\n- If precision is critical, switch to the `decimal` module, but be mindful of performance trade-offs.\n\nFloating-point arithmetic is powerful, but it comes with limitations. By recognizing and addressing these limitations, you can write more robust and reliable numerical programs.\n\nFurther Reading:\n\n- [The IEEE 754 Standard](https://en.wikipedia.org/wiki/IEEE_754)\n\n- [Python’s `decimal` Module](https://docs.python.org/3/library/decimal.html)\n\n- [Floating Point Arithmetic: Issues and Limitations](https://docs.python.org/3/tutorial/floatingpoint.html)\n\nThis should give you a deeper understanding of floating-point precision in Python and provide you with practical tools to handle it in your code!"
    },
    {
        "link": "https://reddit.com/r/learnpython/comments/u6asvb/scientific_precision_in_python",
        "document": "When the matter is precision - like, really precise - floats are crap. Even double precision can be a gamble when working with scientific data - very small and very large numbers.\n\nWhat are the best options when I need to work with numbers on the -15th and +20th orders of magnitude? (at the same time, mind you)\n\nIs the decimal.py module precise enough for those sorts of calculations? If not, is it possible to get precise results with python or will I have to write some matlab modules to crunch my numbers?"
    },
    {
        "link": "https://docs.python.org/3/tutorial/floatingpoint.html",
        "document": "Floating-point numbers are represented in computer hardware as base 2 (binary) fractions. For example, the decimal fraction has value 6/10 + 2/100 + 5/1000, and in the same way the binary fraction has value 1/2 + 0/4 + 1/8. These two fractions have identical values, the only real difference being that the first is written in base 10 fractional notation, and the second in base 2.\n\nUnfortunately, most decimal fractions cannot be represented exactly as binary fractions. A consequence is that, in general, the decimal floating-point numbers you enter are only approximated by the binary floating-point numbers actually stored in the machine.\n\nThe problem is easier to understand at first in base 10. Consider the fraction 1/3. You can approximate that as a base 10 fraction:\n\nand so on. No matter how many digits you’re willing to write down, the result will never be exactly 1/3, but will be an increasingly better approximation of 1/3.\n\nIn the same way, no matter how many base 2 digits you’re willing to use, the decimal value 0.1 cannot be represented exactly as a base 2 fraction. In base 2, 1/10 is the infinitely repeating fraction\n\nStop at any finite number of bits, and you get an approximation. On most machines today, floats are approximated using a binary fraction with the numerator using the first 53 bits starting with the most significant bit and with the denominator as a power of two. In the case of 1/10, the binary fraction is which is close to but not exactly equal to the true value of 1/10.\n\nMany users are not aware of the approximation because of the way values are displayed. Python only prints a decimal approximation to the true decimal value of the binary approximation stored by the machine. On most machines, if Python were to print the true decimal value of the binary approximation stored for 0.1, it would have to display:\n\nThat is more digits than most people find useful, so Python keeps the number of digits manageable by displaying a rounded value instead:\n\nJust remember, even though the printed result looks like the exact value of 1/10, the actual stored value is the nearest representable binary fraction.\n\nInterestingly, there are many different decimal numbers that share the same nearest approximate binary fraction. For example, the numbers and and are all approximated by . Since all of these decimal values share the same approximation, any one of them could be displayed while still preserving the invariant .\n\nHistorically, the Python prompt and built-in function would choose the one with 17 significant digits, . Starting with Python 3.1, Python (on most systems) is now able to choose the shortest of these and simply display .\n\nNote that this is in the very nature of binary floating point: this is not a bug in Python, and it is not a bug in your code either. You’ll see the same kind of thing in all languages that support your hardware’s floating-point arithmetic (although some languages may not display the difference by default, or in all output modes).\n\nFor more pleasant output, you may wish to use string formatting to produce a limited number of significant digits:\n\nIt’s important to realize that this is, in a real sense, an illusion: you’re simply rounding the display of the true machine value.\n\nOne illusion may beget another. For example, since 0.1 is not exactly 1/10, summing three values of 0.1 may not yield exactly 0.3, either:\n\nAlso, since the 0.1 cannot get any closer to the exact value of 1/10 and 0.3 cannot get any closer to the exact value of 3/10, then pre-rounding with function cannot help:\n\nThough the numbers cannot be made closer to their intended exact values, the function can be useful for comparing inexact values:\n\nAlternatively, the function can be used to compare rough approximations:\n\nBinary floating-point arithmetic holds many surprises like this. The problem with “0.1” is explained in precise detail below, in the “Representation Error” section. See Examples of Floating Point Problems for a pleasant summary of how binary floating point works and the kinds of problems commonly encountered in practice. Also see The Perils of Floating Point for a more complete account of other common surprises.\n\nAs that says near the end, “there are no easy answers.” Still, don’t be unduly wary of floating point! The errors in Python float operations are inherited from the floating-point hardware, and on most machines are on the order of no more than 1 part in 2**53 per operation. That’s more than adequate for most tasks, but you do need to keep in mind that it’s not decimal arithmetic and that every float operation can suffer a new rounding error.\n\nWhile pathological cases do exist, for most casual use of floating-point arithmetic you’ll see the result you expect in the end if you simply round the display of your final results to the number of decimal digits you expect. usually suffices, and for finer control see the method’s format specifiers in Format String Syntax.\n\nFor use cases which require exact decimal representation, try using the module which implements decimal arithmetic suitable for accounting applications and high-precision applications.\n\nAnother form of exact arithmetic is supported by the module which implements arithmetic based on rational numbers (so the numbers like 1/3 can be represented exactly).\n\nIf you are a heavy user of floating-point operations you should take a look at the NumPy package and many other packages for mathematical and statistical operations supplied by the SciPy project. See <https://scipy.org>.\n\nPython provides tools that may help on those rare occasions when you really do want to know the exact value of a float. The method expresses the value of a float as a fraction:\n\nSince the ratio is exact, it can be used to losslessly recreate the original value:\n\nThe method expresses a float in hexadecimal (base 16), again giving the exact value stored by your computer:\n\nThis precise hexadecimal representation can be used to reconstruct the float value exactly:\n\nSince the representation is exact, it is useful for reliably porting values across different versions of Python (platform independence) and exchanging data with other languages that support the same format (such as Java and C99).\n\nAnother helpful tool is the function which helps mitigate loss-of-precision during summation. It uses extended precision for intermediate rounding steps as values are added onto a running total. That can make a difference in overall accuracy so that the errors do not accumulate to the point where they affect the final total:\n\nThe goes further and tracks all of the “lost digits” as values are added onto a running total so that the result has only a single rounding. This is slower than but will be more accurate in uncommon cases where large magnitude inputs mostly cancel each other out leaving a final sum near zero:\n\nThis section explains the “0.1” example in detail, and shows how you can perform an exact analysis of cases like this yourself. Basic familiarity with binary floating-point representation is assumed. Representation error refers to the fact that some (most, actually) decimal fractions cannot be represented exactly as binary (base 2) fractions. This is the chief reason why Python (or Perl, C, C++, Java, Fortran, and many others) often won’t display the exact decimal number you expect. Why is that? 1/10 is not exactly representable as a binary fraction. Since at least 2000, almost all machines use IEEE 754 binary floating-point arithmetic, and almost all platforms map Python floats to IEEE 754 binary64 “double precision” values. IEEE 754 binary64 values contain 53 bits of precision, so on input the computer strives to convert 0.1 to the closest fraction it can of the form J/2**N where J is an integer containing exactly 53 bits. Rewriting and recalling that J has exactly 53 bits (is but ), the best value for N is 56: That is, 56 is the only value for N that leaves J with exactly 53 bits. The best possible value for J is then that quotient rounded: Since the remainder is more than half of 10, the best approximation is obtained by rounding up: Therefore the best possible approximation to 1/10 in IEEE 754 double precision is: Dividing both the numerator and denominator by two reduces the fraction to: Note that since we rounded up, this is actually a little bit larger than 1/10; if we had not rounded up, the quotient would have been a little bit smaller than 1/10. But in no case can it be exactly 1/10! So the computer never “sees” 1/10: what it sees is the exact fraction given above, the best IEEE 754 double approximation it can get: If we multiply that fraction by 10**55, we can see the value out to 55 decimal digits: meaning that the exact number stored in the computer is equal to the decimal value 0.1000000000000000055511151231257827021181583404541015625. Instead of displaying the full decimal value, many languages (including older versions of Python), round the result to 17 significant digits: The and modules make these calculations easy:"
    }
]