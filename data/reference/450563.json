[
    {
        "link": "https://docs.python.org/3/howto/sorting.html",
        "document": "Python lists have a built-in method that modifies the list in-place. There is also a built-in function that builds a new sorted list from an iterable.\n\nIn this document, we explore the various techniques for sorting data using Python.\n\nA simple ascending sort is very easy: just call the function. It returns a new sorted list: You can also use the method. It modifies the list in-place (and returns to avoid confusion). Usually it’s less convenient than - but if you don’t need the original list, it’s slightly more efficient. Another difference is that the method is only defined for lists. In contrast, the function accepts any iterable.\n\nBoth and have a key parameter to specify a function (or other callable) to be called on each list element prior to making comparisons. \"This is a test string from Andrew\" ['a', 'Andrew', 'from', 'is', 'string', 'test', 'This'] The value of the key parameter should be a function (or other callable) that takes a single argument and returns a key to use for sorting purposes. This technique is fast because the key function is called exactly once for each input record. A common pattern is to sort complex objects using some of the object’s indices as keys. For example: The same technique works for objects with named attributes. For example: Objects with named attributes can be made by a regular class as shown above, or they can be instances of or a named tuple.\n\nThe key function patterns shown above are very common, so Python provides convenience functions to make accessor functions easier and faster. The module has , , and a function. Using those functions, the above examples become simpler and faster: The operator module functions allow multiple levels of sorting. For example, to sort by grade then by age: The module provides another helpful tool for making key-functions. The function can reduce the arity of a multi-argument function making it suitable for use as a key-function.\n\nSorts are guaranteed to be stable. That means that when multiple records have the same key, their original order is preserved. Notice how the two records for blue retain their original order so that is guaranteed to precede . This wonderful property lets you build complex sorts in a series of sorting steps. For example, to sort the student data by descending grade and then ascending age, do the age sort first and then sort again using grade: This can be abstracted out into a wrapper function that can take a list and tuples of field and order to sort them on multiple passes. The Timsort algorithm used in Python does multiple sorts efficiently because it can take advantage of any ordering already present in a dataset.\n\nThis idiom is called Decorate-Sort-Undecorate after its three steps:\n• None First, the initial list is decorated with new values that control the sort order.\n• None Second, the decorated list is sorted.\n• None Finally, the decorations are removed, creating a list that contains only the initial values in the new order. For example, to sort the student data by grade using the DSU approach: This idiom works because tuples are compared lexicographically; the first items are compared; if they are the same then the second items are compared, and so on. It is not strictly necessary in all cases to include the index i in the decorated list, but including it gives two benefits:\n• None The sort is stable – if two items have the same key, their order will be preserved in the sorted list.\n• None The original items do not have to be comparable because the ordering of the decorated tuples will be determined by at most the first two items. So for example the original list could contain complex numbers which cannot be sorted directly. Another name for this idiom is Schwartzian transform, after Randal L. Schwartz, who popularized it among Perl programmers. Now that Python sorting provides key-functions, this technique is not often needed.\n\nUnlike key functions that return an absolute value for sorting, a comparison function computes the relative ordering for two inputs. For example, a balance scale compares two samples giving a relative ordering: lighter, equal, or heavier. Likewise, a comparison function such as will return a negative value for less-than, zero if the inputs are equal, or a positive value for greater-than. It is common to encounter comparison functions when translating algorithms from other languages. Also, some libraries provide comparison functions as part of their API. For example, is a comparison function. To accommodate those situations, Python provides to wrap the comparison function to make it usable as a key function:\n• None For locale aware sorting, use for a key function or for a comparison function. This is necessary because “alphabetical” sort orderings can vary across cultures even if the underlying alphabet is the same.\n• None The reverse parameter still maintains sort stability (so that records with equal keys retain the original order). Interestingly, that effect can be simulated without the parameter by using the builtin function twice:\n• None The sort routines use when making comparisons between two objects. So, it is easy to add a standard sort order to a class by defining an method: However, note that can fall back to using if is not implemented (see for details on the mechanics). To avoid surprises, PEP 8 recommends that all six comparison methods be implemented. The decorator is provided to make that task easier.\n• None Key functions need not depend directly on the objects being sorted. A key function can also access external resources. For instance, if the student grades are stored in a dictionary, they can be used to sort a separate list of student names:"
    },
    {
        "link": "https://docs.python.org/3.10/howto/sorting.html",
        "document": "Python lists have a built-in method that modifies the list in-place. There is also a built-in function that builds a new sorted list from an iterable.\n\nIn this document, we explore the various techniques for sorting data using Python.\n\nA simple ascending sort is very easy: just call the function. It returns a new sorted list: You can also use the method. It modifies the list in-place (and returns to avoid confusion). Usually it’s less convenient than - but if you don’t need the original list, it’s slightly more efficient. Another difference is that the method is only defined for lists. In contrast, the function accepts any iterable.\n\nBoth and have a key parameter to specify a function (or other callable) to be called on each list element prior to making comparisons. \"This is a test string from Andrew\" ['a', 'Andrew', 'from', 'is', 'string', 'test', 'This'] The value of the key parameter should be a function (or other callable) that takes a single argument and returns a key to use for sorting purposes. This technique is fast because the key function is called exactly once for each input record. A common pattern is to sort complex objects using some of the object’s indices as keys. For example: The same technique works for objects with named attributes. For example:\n\nThe key-function patterns shown above are very common, so Python provides convenience functions to make accessor functions easier and faster. The module has , , and a function. Using those functions, the above examples become simpler and faster: The operator module functions allow multiple levels of sorting. For example, to sort by grade then by age:\n\nSorts are guaranteed to be stable. That means that when multiple records have the same key, their original order is preserved. Notice how the two records for blue retain their original order so that is guaranteed to precede . This wonderful property lets you build complex sorts in a series of sorting steps. For example, to sort the student data by descending grade and then ascending age, do the age sort first and then sort again using grade: This can be abstracted out into a wrapper function that can take a list and tuples of field and order to sort them on multiple passes. The Timsort algorithm used in Python does multiple sorts efficiently because it can take advantage of any ordering already present in a dataset.\n\nThe Old Way Using Decorate-Sort-Undecorate¶ This idiom is called Decorate-Sort-Undecorate after its three steps:\n• None First, the initial list is decorated with new values that control the sort order.\n• None Second, the decorated list is sorted.\n• None Finally, the decorations are removed, creating a list that contains only the initial values in the new order. For example, to sort the student data by grade using the DSU approach: This idiom works because tuples are compared lexicographically; the first items are compared; if they are the same then the second items are compared, and so on. It is not strictly necessary in all cases to include the index i in the decorated list, but including it gives two benefits:\n• None The sort is stable – if two items have the same key, their order will be preserved in the sorted list.\n• None The original items do not have to be comparable because the ordering of the decorated tuples will be determined by at most the first two items. So for example the original list could contain complex numbers which cannot be sorted directly. Another name for this idiom is Schwartzian transform, after Randal L. Schwartz, who popularized it among Perl programmers. Now that Python sorting provides key-functions, this technique is not often needed.\n\nThe Old Way Using the cmp Parameter¶ Many constructs given in this HOWTO assume Python 2.4 or later. Before that, there was no builtin and took no keyword arguments. Instead, all of the Py2.x versions supported a cmp parameter to handle user specified comparison functions. In Py3.0, the cmp parameter was removed entirely (as part of a larger effort to simplify and unify the language, eliminating the conflict between rich comparisons and the magic method). In Py2.x, sort allowed an optional function which can be called for doing the comparisons. That function should take two arguments to be compared and then return a negative value for less-than, return zero if they are equal, or return a positive value for greater-than. For example, we can do: Or you can reverse the order of comparison with: When porting code from Python 2.x to 3.x, the situation can arise when you have the user supplying a comparison function and you need to convert that to a key function. The following wrapper makes that easy to do: To convert to a key function, just wrap the old comparison function: In Python 3.2, the function was added to the module in the standard library."
    },
    {
        "link": "https://docs.python.org/3/tutorial/datastructures.html",
        "document": "This chapter describes some things you’ve learned about already in more detail, and adds some new things as well.\n\nThe list data type has some more methods. Here are all of the methods of list objects: Add an item to the end of the list. Similar to . Extend the list by appending all the items from the iterable. Similar to . Insert an item at a given position. The first argument is the index of the element before which to insert, so inserts at the front of the list, and is equivalent to . Remove the first item from the list whose value is equal to x. It raises a if there is no such item. Remove the item at the given position in the list, and return it. If no index is specified, removes and returns the last item in the list. It raises an if the list is empty or the index is outside the list range. Remove all items from the list. Similar to . Return zero-based index in the list of the first item whose value is equal to x. Raises a if there is no such item. The optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument. Return the number of times x appears in the list. Sort the items of the list in place (the arguments can be used for sort customization, see for their explanation). Reverse the elements of the list in place. Return a shallow copy of the list. Similar to . An example that uses most of the list methods: You might have noticed that methods like , or that only modify the list have no return value printed – they return the default . This is a design principle for all mutable data structures in Python. Another thing you might notice is that not all data can be sorted or compared. For instance, doesn’t sort because integers can’t be compared to strings and can’t be compared to other types. Also, there are some types that don’t have a defined ordering relation. For example, isn’t a valid comparison. The list methods make it very easy to use a list as a stack, where the last element added is the first element retrieved (“last-in, first-out”). To add an item to the top of the stack, use . To retrieve an item from the top of the stack, use without an explicit index. For example: It is also possible to use a list as a queue, where the first element added is the first element retrieved (“first-in, first-out”); however, lists are not efficient for this purpose. While appends and pops from the end of list are fast, doing inserts or pops from the beginning of a list is slow (because all of the other elements have to be shifted by one). To implement a queue, use which was designed to have fast appends and pops from both ends. For example: # The first to arrive now leaves # The second to arrive now leaves List comprehensions provide a concise way to create lists. Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition. For example, assume we want to create a list of squares, like: Note that this creates (or overwrites) a variable named that still exists after the loop completes. We can calculate the list of squares without any side effects using: which is more concise and readable. A list comprehension consists of brackets containing an expression followed by a clause, then zero or more or clauses. The result will be a new list resulting from evaluating the expression in the context of the and clauses which follow it. For example, this listcomp combines the elements of two lists if they are not equal: Note how the order of the and statements is the same in both these snippets. If the expression is a tuple (e.g. the in the previous example), it must be parenthesized. # create a new list with the values doubled # apply a function to all the elements # the tuple must be parenthesized, otherwise an error is raised File , line : did you forget parentheses around the comprehension target? # flatten a list using a listcomp with two 'for' List comprehensions can contain complex expressions and nested functions: The initial expression in a list comprehension can be any arbitrary expression, including another list comprehension. Consider the following example of a 3x4 matrix implemented as a list of 3 lists of length 4: The following list comprehension will transpose rows and columns: As we saw in the previous section, the inner list comprehension is evaluated in the context of the that follows it, so this example is equivalent to: which, in turn, is the same as: # the following 3 lines implement the nested listcomp In the real world, you should prefer built-in functions to complex flow statements. The function would do a great job for this use case: See Unpacking Argument Lists for details on the asterisk in this line.\n\nWe saw that lists and strings have many common properties, such as indexing and slicing operations. They are two examples of sequence data types (see Sequence Types — list, tuple, range). Since Python is an evolving language, other sequence data types may be added. There is also another standard sequence data type: the tuple. A tuple consists of a number of values separated by commas, for instance: File , line , in : # but they can contain mutable objects: As you see, on output tuples are always enclosed in parentheses, so that nested tuples are interpreted correctly; they may be input with or without surrounding parentheses, although often parentheses are necessary anyway (if the tuple is part of a larger expression). It is not possible to assign to the individual items of a tuple, however it is possible to create tuples which contain mutable objects, such as lists. Though tuples may seem similar to lists, they are often used in different situations and for different purposes. Tuples are immutable, and usually contain a heterogeneous sequence of elements that are accessed via unpacking (see later in this section) or indexing (or even by attribute in the case of ). Lists are mutable, and their elements are usually homogeneous and are accessed by iterating over the list. A special problem is the construction of tuples containing 0 or 1 items: the syntax has some extra quirks to accommodate these. Empty tuples are constructed by an empty pair of parentheses; a tuple with one item is constructed by following a value with a comma (it is not sufficient to enclose a single value in parentheses). Ugly, but effective. For example: The statement is an example of tuple packing: the values , and are packed together in a tuple. The reverse operation is also possible: This is called, appropriately enough, sequence unpacking and works for any sequence on the right-hand side. Sequence unpacking requires that there are as many variables on the left side of the equals sign as there are elements in the sequence. Note that multiple assignment is really just a combination of tuple packing and sequence unpacking.\n\nPython also includes a data type for sets. A set is an unordered collection with no duplicate elements. Basic uses include membership testing and eliminating duplicate entries. Set objects also support mathematical operations like union, intersection, difference, and symmetric difference. Curly braces or the function can be used to create sets. Note: to create an empty set you have to use , not ; the latter creates an empty dictionary, a data structure that we discuss in the next section. Here is a brief demonstration: # show that duplicates have been removed # Demonstrate set operations on unique letters from two words # letters in a but not in b # letters in a or b or both # letters in both a and b # letters in a or b but not both Similarly to list comprehensions, set comprehensions are also supported:\n\nAnother useful data type built into Python is the dictionary (see Mapping Types — dict). Dictionaries are sometimes found in other languages as “associative memories” or “associative arrays”. Unlike sequences, which are indexed by a range of numbers, dictionaries are indexed by keys, which can be any immutable type; strings and numbers can always be keys. Tuples can be used as keys if they contain only strings, numbers, or tuples; if a tuple contains any mutable object either directly or indirectly, it cannot be used as a key. You can’t use lists as keys, since lists can be modified in place using index assignments, slice assignments, or methods like and . It is best to think of a dictionary as a set of key: value pairs, with the requirement that the keys are unique (within one dictionary). A pair of braces creates an empty dictionary: . Placing a comma-separated list of key:value pairs within the braces adds initial key:value pairs to the dictionary; this is also the way dictionaries are written on output. The main operations on a dictionary are storing a value with some key and extracting the value given the key. It is also possible to delete a key:value pair with . If you store using a key that is already in use, the old value associated with that key is forgotten. It is an error to extract a value using a non-existent key. Performing on a dictionary returns a list of all the keys used in the dictionary, in insertion order (if you want it sorted, just use instead). To check whether a single key is in the dictionary, use the keyword. Here is a small example using a dictionary: The constructor builds dictionaries directly from sequences of key-value pairs: In addition, dict comprehensions can be used to create dictionaries from arbitrary key and value expressions: When the keys are simple strings, it is sometimes easier to specify pairs using keyword arguments:\n\nWhen looping through dictionaries, the key and corresponding value can be retrieved at the same time using the method. When looping through a sequence, the position index and corresponding value can be retrieved at the same time using the function. To loop over two or more sequences at the same time, the entries can be paired with the function. What is your name? It is lancelot. What is your quest? It is the holy grail. What is your favorite color? It is blue. To loop over a sequence in reverse, first specify the sequence in a forward direction and then call the function. To loop over a sequence in sorted order, use the function which returns a new sorted list while leaving the source unaltered. Using on a sequence eliminates duplicate elements. The use of in combination with over a sequence is an idiomatic way to loop over unique elements of the sequence in sorted order. It is sometimes tempting to change a list while you are looping over it; however, it is often simpler and safer to create a new list instead.\n\nThe conditions used in and statements can contain any operators, not just comparisons. The comparison operators and are membership tests that determine whether a value is in (or not in) a container. The operators and compare whether two objects are really the same object. All comparison operators have the same priority, which is lower than that of all numerical operators. Comparisons can be chained. For example, tests whether is less than and moreover equals . Comparisons may be combined using the Boolean operators and , and the outcome of a comparison (or of any other Boolean expression) may be negated with . These have lower priorities than comparison operators; between them, has the highest priority and the lowest, so that A and not B or C is equivalent to (A and (not B)) or C . As always, parentheses can be used to express the desired composition. The Boolean operators and are so-called short-circuit operators: their arguments are evaluated from left to right, and evaluation stops as soon as the outcome is determined. For example, if and are true but is false, A and B and C does not evaluate the expression . When used as a general value and not as a Boolean, the return value of a short-circuit operator is the last evaluated argument. It is possible to assign the result of a comparison or other Boolean expression to a variable. For example, Note that in Python, unlike C, assignment inside expressions must be done explicitly with the walrus operator . This avoids a common class of problems encountered in C programs: typing in an expression when was intended.\n\nSequence objects typically may be compared to other objects with the same sequence type. The comparison uses lexicographical ordering: first the first two items are compared, and if they differ this determines the outcome of the comparison; if they are equal, the next two items are compared, and so on, until either sequence is exhausted. If two items to be compared are themselves sequences of the same type, the lexicographical comparison is carried out recursively. If all items of two sequences compare equal, the sequences are considered equal. If one sequence is an initial sub-sequence of the other, the shorter sequence is the smaller (lesser) one. Lexicographical ordering for strings uses the Unicode code point number to order individual characters. Some examples of comparisons between sequences of the same type: Note that comparing objects of different types with or is legal provided that the objects have appropriate comparison methods. For example, mixed numeric types are compared according to their numeric value, so 0 equals 0.0, etc. Otherwise, rather than providing an arbitrary ordering, the interpreter will raise a exception."
    },
    {
        "link": "https://freecodecamp.org/news/lambda-sort-list-in-python",
        "document": "The method and the function let you sort iterable data like lists and tuples in ascending or descending order.\n\nThey take parameters with which you can modify how they perform the sorting. And one of those parameters could be a function or even a lambda function.\n\nIn this article, you’ll learn how to sort a list with the lambda function.\n• How to Sort a List in Python\n• How to Sort a List with the Lambda Function\n• How to Lambdasort with the Method\n• How to Lambdasort with the Function\n\nHow to Sort a List in Python\n\nYou can sort a list with the method and function.\n\nThe method takes two parameters – and . You can use a function as the key, but the reverse parameter can only take a Boolean.\n\nIf you specify the value of the reverse parameter as , the method will perform the sorting in descending order. And if you specify as the value of the , the sorting will be in ascending order. You don’t even need to specify false as the value because it’s the default.\n\nBut both parameters are optional, so the method still works fine without them:\n\nThe function, on the other hand, also works like . It takes the optional and parameters too, but it takes a compulsory parameter of the iterable you want to sort – making it ideal for sorting other iterables apart from a list.\n\nAs I pointed out earlier, you can also sort other iterables with the function. This is how I sorted a tuple with the function:\n\nRemember I also pointed out you can pass a function as the value for the key parameter of the method and function. This function can help you be more decisive with the way you want to sort the iterable list or tuple.\n\nFor example, the function and method would only sort by the first part of the string or number. But you can also sort by the second part by passing in a function as the key parameter. It is with this function that you will decide how you want to sort the list or other iterables.\n\nA lambda function would be ideal to do this because it takes one expression. But before we dive into sorting with a lambda function, let me remind you what the lambda function is.\n\nA lambda function is an anonymous function – a function you don’t write with the keyword. A lambda function can take many arguments, but it can only have one expression.\n\nSince you don’t define a lambda function with the keyword, how do you call it? You can assign a lambda function to a variable, then call it by the name of that variable.\n\nIn the example below, the lambda function has 3 arguments and adds them together:\n\nHow to Sort a List with the Lambda Function\n\nYou can “lambda sort” a list with both the method and the function. Let’s look at how to lambda sort with the method first.\n\nHow to Lambdasort with the Method\n\nLet’s take the names we sorted before and sort them by the second name. This lambda function would be ideal in sorting by the second name:\n\nThe lambda function splits a name and takes the second part of the name – the second name. The first part is the first name and it would be .\n\nYou can pass in this lambda function as the key parameter – sorting the names by the second names:\n\nYou can see the names got sorted by the alphabetical order of the second names. came first, and came last. Ann starts with A and Jack starts with J.\n\nIf you want, you can even pass in a function directly. What you need to do is to not call the function. You have to pass it in as an object.\n\nNow, let’s sort the numbers based on their last digits. In case you don’t know, if you use the remainder ( ) operator on two numbers, it divides the two numbers and returns the remainder:\n\nBut if you “mod” a number with multiple digits with 10, it returns the last digit of the number:\n\nThat’s how you can get the second number and sort the numbers based on it.\n\nHere’s how I sorted the numbers from the previous examples with a lambda function:\n\nAs you can see, this lambda function, is responsible for sorting the numbers based on each of the second digits. I passed in the number to the lambda function and got the last digit with . This lambda function runs through each of the numbers and gets their last digits.\n\nIf you want, you can even pass in a function directly as the key:\n\nHow to Lambdasort with the Function\n\nIn this example, we are going to sort a list of tuples using the jersey numbers of some footballers.\n\nThe only difference between the and method is that takes a compulsory iterable and does not.\n\nSo, to lambda sort with the function, all you need to do is pass in the list as the iterable and your lambda function as the key:\n\nThe lambda function that performed the sorting is this . The lambda went through all the tuples in the list, took the second index ( ), and used those to do the sorting.\n\nThis article showed you how to sort a list with the method and function using a lambda function.\n\nBut that was not all. We looked at how both the method and function work on their own without a lambda function. I also reminded you of what the lambda function is, so you could understand how I did the sorting with a lambda function.\n\nThank you for reading!"
    },
    {
        "link": "https://realpython.com/python-lambda",
        "document": "Python and other languages like Java, C#, and even C++ have had lambda functions added to their syntax, whereas languages like LISP or the ML family of languages, Haskell, OCaml, and F#, use lambdas as a core concept.\n\nPython lambdas are little, anonymous functions, subject to a more restrictive but more concise syntax than regular Python functions.\n\nBy the end of this article, you’ll know:\n• How Python lambdas came to be\n• Which functions in the Python standard library leverage lambdas\n• When to use or avoid Python lambda functions\n\nThis tutorial is mainly for intermediate to experienced Python programmers, but it is accessible to any curious minds with interest in programming and lambda calculus.\n\nAll the examples included in this tutorial have been tested with Python 3.7.\n\nLambda expressions in Python and other programming languages have their roots in lambda calculus, a model of computation invented by Alonzo Church. You’ll uncover when lambda calculus was introduced and why it’s a fundamental concept that ended up in the Python ecosystem. Alonzo Church formalized lambda calculus, a language based on pure abstraction, in the 1930s. Lambda functions are also referred to as lambda abstractions, a direct reference to the abstraction model of Alonzo Church’s original creation. Lambda calculus can encode any computation. It is Turing complete, but contrary to the concept of a Turing machine, it is pure and does not keep any state. Functional languages get their origin in mathematical logic and lambda calculus, while imperative programming languages embrace the state-based model of computation invented by Alan Turing. The two models of computation, lambda calculus and Turing machines, can be translated into each another. This equivalence is known as the Church-Turing hypothesis. Functional languages directly inherit the lambda calculus philosophy, adopting a declarative approach of programming that emphasizes abstraction, data transformation, composition, and purity (no state and no side effects). Examples of functional languages include Haskell, Lisp, or Erlang. By contrast, the Turing Machine led to imperative programming found in languages like Fortran, C, or Python. The imperative style consists of programming with statements, driving the flow of the program step by step with detailed instructions. This approach promotes mutation and requires managing state. The separation in both families presents some nuances, as some functional languages incorporate imperative features, like OCaml, while functional features have been permeating the imperative family of languages in particular with the introduction of lambda functions in Java, or Python. Python is not inherently a functional language, but it adopted some functional concepts early on. In January 1994, , , , and the operator were added to the language. Here are a few examples to give you an appetite for some Python code, functional style. The identity function, a function that returns its argument, is expressed with a standard Python function definition using the keyword as follows: takes an argument and returns it upon invocation. In contrast, if you use a Python lambda construction, you get the following: In the example above, the expression is composed of: Note: In the context of this article, a bound variable is an argument to a lambda function. In contrast, a free variable is not bound and may be referenced in the body of the expression. A free variable can be a constant or a variable defined in the enclosing scope of the function. You can write a slightly more elaborated example, a function that adds to an argument, as follows: You can apply the function above to an argument by surrounding the function and its argument with parentheses: Reduction is a lambda calculus strategy to compute the value of the expression. In the current example, it consists of replacing the bound variable with the argument : Because a lambda function is an expression, it can be named. Therefore you could write the previous code as follows: The above lambda function is equivalent to writing this: These functions all take a single argument. You may have noticed that, in the definition of the lambdas, the arguments don’t have parentheses around them. Multi-argument functions (functions that take more than one argument) are expressed in Python lambdas by listing arguments and separating them with a comma ( ) but without surrounding them with parentheses: The lambda function assigned to takes two arguments and returns a string interpolating the two parameters and . As expected, the definition of the lambda lists the arguments with no parentheses, whereas calling the function is done exactly like a normal Python function, with parentheses surrounding the arguments.\n\nThe following terms may be used interchangeably depending on the programming language type and culture: For the rest of this article after this section, you’ll mostly see the term lambda function. Taken literally, an anonymous function is a function without a name. In Python, an anonymous function is created with the keyword. More loosely, it may or not be assigned a name. Consider a two-argument anonymous function defined with but not bound to a variable. The lambda is not given a name: The function above defines a lambda expression that takes two arguments and returns their sum. Other than providing you with the feedback that Python is perfectly fine with this form, it doesn’t lead to any practical use. You could invoke the function in the Python interpreter: The example above is taking advantage of the interactive interpreter-only feature provided via the underscore ( ). See the note below for more details. You could not write similar code in a Python module. Consider the in the interpreter as a side effect that you took advantage of. In a Python module, you would assign a name to the lambda, or you would pass the lambda to a function. You’ll use those two approaches later in this article. Note: In the interactive interpreter, the single underscore ( ) is bound to the last expression evaluated. In the example above, the points to the lambda function. For more details about the usage of this special character in Python, check out The Meaning of Underscores in Python. Another pattern used in other languages like JavaScript is to immediately execute a Python lambda function. This is known as an Immediately Invoked Function Expression (IIFE, pronounce “iffy”). Here’s an example: The lambda function above is defined and then immediately called with two arguments ( and ). It returns the value , which is the sum of the arguments. Several examples in this tutorial use this format to highlight the anonymous aspect of a lambda function and avoid focusing on in Python as a shorter way of defining a function. Python does not encourage using immediately invoked lambda expressions. It simply results from a lambda expression being callable, unlike the body of a normal function. Lambda functions are frequently used with higher-order functions, which take one or more functions as arguments or return one or more functions. A lambda function can be a higher-order function by taking a function (normal or lambda) as an argument like in the following contrived example: Python exposes higher-order functions as built-in functions or in the standard library. Examples include , , , as well as key functions like , , , and . You’ll use lambda functions together with Python higher-order functions in Appropriate Uses of Lambda Expressions.\n\nThis quote from the Python Design and History FAQ seems to set the tone about the overall expectation regarding the usage of lambda functions in Python: Unlike lambda forms in other languages, where they add functionality, Python lambdas are only a shorthand notation if you’re too lazy to define a function. (Source) Nevertheless, don’t let this statement deter you from using Python’s . At first glance, you may accept that a lambda function is a function with some syntactic sugar shortening the code to define or invoke a function. The following sections highlight the commonalities and subtle differences between normal Python functions and lambda functions. At this point, you may wonder what fundamentally distinguishes a lambda function bound to a variable from a regular function with a single line: under the surface, almost nothing. Let’s verify how Python sees a function built with a single return statement versus a function constructed as an expression ( ). The module exposes functions to analyze Python bytecode generated by the Python compiler: You can see that expose a readable version of the Python bytecode allowing the inspection of the low-level instructions that the Python interpreter will use while executing the program. Now see it with a regular function object: The bytecode interpreted by Python is the same for both functions. But you may notice that the naming is different: the function name is for a function defined with , whereas the Python lambda function is seen as . You saw in the previous section that, in the context of the lambda function, Python did not provide the name of the function, but only . This can be a limitation to consider when an exception occurs, and a traceback shows only : The traceback of an exception raised while a lambda function is executed only identifies the function causing the exception as . Here’s the same exception raised by a normal function: The normal function causes a similar error but results in a more precise traceback because it gives the function name, . As you saw in the previous sections, a lambda form presents syntactic distinctions from a normal function. In particular, a lambda function has the following characteristics:\n• It can only contain expressions and can’t include statements in its body.\n• It is written as a single line of execution.\n• It does not support type annotations.\n• It can be immediately invoked (IIFE). A lambda function can’t contain any statements. In a lambda function, statements like , , , or will raise a exception. Here’s an example of adding to the body of a lambda: This contrived example intended to that parameter had a value of . But, the interpreter identifies a while parsing the code that involves the statement in the body of the . In contrast to a normal function, a Python lambda function is a single expression. Although, in the body of a , you can spread the expression over several lines using parentheses or a multiline string, it remains a single expression: The example above returns the string when the lambda argument is odd, and when the argument is even. It spreads across two lines because it is contained in a set of parentheses, but it remains a single expression. If you’ve started adopting type hinting, which is now available in Python, then you have another good reason to prefer normal functions over Python lambda functions. Check out Python Type Checking (Guide) to get learn more about Python type hints and type checking. In a lambda function, there is no equivalent for the following: Any type error with can be caught by tools like or , whereas a with the equivalent lambda function is raised at runtime: Like trying to include a statement in a lambda, adding type annotation immediately results in a at runtime. You’ve already seen several examples of immediately invoked function execution: Outside of the Python interpreter, this feature is probably not used in practice. It’s a direct consequence of a lambda function being callable as it is defined. For example, this allows you to pass the definition of a Python lambda expression to a higher-order function like , , or , or to a key function. Like a normal function object defined with , Python lambda expressions support all the different ways of passing arguments. This includes:\n• Variable list of arguments (often referred to as varargs) The following examples illustrate options open to you in order to pass arguments to lambda expressions: In Python, a decorator is the implementation of a pattern that allows adding a behavior to a function or a class. It is usually expressed with the syntax prefixing a function. Here’s a contrived example: In the example above, is a function that adds a behavior to , so that invoking results in the following output: only prints , but the decorator adds an extra behavior that also prints . A decorator can be applied to a lambda. Although it’s not possible to decorate a lambda with the syntax, a decorator is just a function, so it can call the lambda function: , decorated with on line 11, is invoked with argument on line 15. By contrast, on line 18, a lambda function is immediately involved and embedded in a call to , the decorator. When you execute the code above you obtain the following: See how, as you’ve already seen, the name of the lambda function appears as , whereas is clearly identified for the normal function. Decorating the lambda function this way could be useful for debugging purposes, possibly to debug the behavior of a lambda function used in the context of a higher-order function or a key function. Let’s see an example with : The first argument of is a lambda that multiplies its argument by . This lambda is decorated with . When executed, the example above outputs the following: The result is a list obtained from multiplying each element of . For now, consider equivalent to the list . You will be exposed to in more details in Map. A lambda can also be a decorator, but it’s not recommended. If you find yourself needing to do this, consult PEP 8, Programming Recommendations. For more on Python decorators, check out Primer on Python Decorators. A closure is a function where every free variable, everything except parameters, used in that function is bound to a specific value defined in the enclosing scope of that function. In effect, closures define the environment in which they run, and so can be called from anywhere. The concepts of lambdas and closures are not necessarily related, although lambda functions can be closures in the same way that normal functions can also be closures. Some languages have special constructs for closure or lambda (for example, Groovy with an anonymous block of code as Closure object), or a lambda expression (for example, Java Lambda expression with a limited option for closure). returns , a nested function that computes the sum of three arguments:\n• is passed as an argument to .\n• is an argument passed to . To test the behavior of and , is invoked three times in a loop that prints the following: On line 9 of the code, returned by the invocation of is bound to the name . On line 5, captures and because it has access to its embedding environment, such that upon invocation of the closure, it is able to operate on the two free variables and . Similarly, a can also be a closure. Here’s the same example with a Python lambda function: When you execute the code above, you obtain the following output: On line 6, returns a lambda and assigns it to to the variable . On line 3, the body of the lambda function references and . The variable is available at definition time, whereas is defined at runtime when is invoked. In this situation, both the normal function and the lambda behave similarly. In the next section, you’ll see a situation where the behavior of a lambda can be deceptive due to its evaluation time (definition time vs runtime). In some situations involving loops, the behavior of a Python lambda function as a closure may be counterintuitive. It requires understanding when free variables are bound in the context of a lambda. The following examples demonstrate the difference when using a regular function vs using a Python lambda. Test the scenario first using a regular function: In a normal function, is evaluated at definition time, on line 9, when the function is added to the list: . Now, with the implementation of the same logic with a lambda function, observe the unexpected behavior: The unexpected result occurs because the free variable , as implemented, is bound at the execution time of the lambda expression. The Python lambda function on line 4 is a closure that captures , a free variable bound at runtime. At runtime, while invoking the function on line 7, the value of is . To overcome this issue, you can assign the free variable at definition time as follows: A Python lambda function behaves like a normal function in regard to arguments. Therefore, a lambda parameter can be initialized with a default value: the parameter takes the outer as a default value. The Python lambda function could have been written as and have the same result. The Python lambda function is invoked without any argument on line 7, and it uses the default value set at definition time. Python lambdas can be tested similarly to regular functions. It’s possible to use both and . defines a test case with three test methods, each of them exercising a test scenario for implemented as a lambda function. The execution of the Python file that contains produces the following: As expected, we have two successful test cases and one failure for : the result is , but the expected result was . This failure is due to an intentional mistake in the test case. Changing the expected result from to will satisfy all the tests for . The module extracts interactive Python code from to execute tests. Although the syntax of Python lambda functions does not support a typical , it is possible to assign a string to the element of a named lambda: The in the doc comment of lambda describes the same test cases as in the previous section. When you execute the tests via , you get the following: The failed test results from the same failure explained in the execution of the unit tests in the previous section. You can add a to a Python lambda via an assignment to to document a lambda function. Although possible, the Python syntax better accommodates for normal functions than lambda functions. For a comprehensive overview of unit testing in Python, you may want to refer to Getting Started With Testing in Python.\n\nSeveral examples in this article, if written in the context of professional Python code, would qualify as abuses. If you find yourself trying to overcome something that a lambda expression does not support, this is probably a sign that a normal function would be better suited. The for a lambda expression in the previous section is a good example. Attempting to overcome the fact that a Python lambda function does not support statements is another red flag. The next sections illustrate a few examples of lambda usages that should be avoided. Those examples might be situations where, in the context of Python lambda, the code exhibits the following pattern:\n• It’s unnecessarily clever at the cost of difficult readability. Trying to raise an exception in a Python lambda should make you think twice. There are some clever ways to do so, but even something like the following is better to avoid: Because a statement is not syntactically correct in a Python lambda body, the workaround in the example above consists of abstracting the statement call with a dedicated function . Using this type of workaround should be avoided. If you encounter this type of code, you should consider refactoring the code to use a regular function. As in any programming languages, you will find Python code that can be difficult to read because of the style used. Lambda functions, due to their conciseness, can be conducive to writing code that is difficult to read. The following lambda example contains several bad style choices: The underscore ( ) refers to a variable that you don’t need to refer to explicitly. But in this example, three refer to different variables. An initial upgrade to this lambda code could be to name the variables: Admittedly, it’s still difficult to read. By still taking advantage of a , a regular function would go a long way to render this code more readable, spreading the logic over a few lines and function calls: This is still not optimal but shows you a possible path to make code, and Python lambda functions in particular, more readable. In Alternatives to Lambdas, you’ll learn to replace and with list comprehensions or generator expressions. This will drastically improve the readability of the code. You can but should not write class methods as Python lambda functions. The following example is perfectly legal Python code but exhibits unconventional Python code relying on . For example, instead of implementing as a regular function, it uses a . Similarly, and are properties also implemented with lambda functions, instead of regular functions or decorators: Running a tool like , a style guide enforcement tool, will display the following errors for and : E731 do not assign a lambda expression, use a def Although doesn’t point out an issue for the usage of the Python lambda functions in the properties, they are difficult to read and prone to error because of the usage of multiple strings like and . Proper implementation of would be expected to be as follows: would be written as follows: As a general rule, in the context of code written in Python, prefer regular functions over lambda expressions. Nonetheless, there are cases that benefit from lambda syntax, as you will see in the next section.\n\nAppropriate Uses of Lambda Expressions Lambdas in Python tend to be the subject of controversies. Some of the arguments against lambdas in Python are:\n• The imposition of a functional way of thinking Despite the heated debates questioning the mere existence of this feature in Python, lambda functions have properties that sometimes provide value to the Python language and to developers. The following examples illustrate scenarios where the use of lambda functions is not only suitable but encouraged in Python code. Lambda functions are regularly used with the built-in functions and , as well as , exposed in the module . The following three examples are respective illustrations of using those functions with lambda expressions as companions: You may have to read code resembling the examples above, albeit with more relevant data. For that reason, it’s important to recognize those constructs. Nevertheless, those constructs have equivalent alternatives that are considered more Pythonic. In Alternatives to Lambdas, you’ll learn how to convert higher-order functions and their accompanying lambdas into other more idiomatic forms. Key functions in Python are higher-order functions that take a parameter as a named argument. receives a function that can be a . This function directly influences the algorithm driven by the key function itself. Here are some key functions:\n• and : in the Heap queue algorithm module Imagine that you want to sort a list of IDs represented as strings. Each ID is the concatenation of the string and a number. Sorting this list with the built-in function , by default, uses a lexicographic order as the elements in the list are strings. To influence the sorting execution, you can assign a lambda to the named argument , such that the sorting will use the number associated with the ID: UI frameworks like Tkinter, wxPython, or .NET Windows Forms with IronPython take advantage of lambda functions for mapping actions in response to UI events. The naive Tkinter program below demonstrates the usage of a assigned to the command of the Reverse button: Clicking the button Reverse fires an event that triggers the lambda function, changing the label from Lambda Calculus to suluclaC adbmaL*: Both wxPython and IronPython on the .NET platform share a similar approach for handling events. Note that is one way to handle firing events, but a function may be used for the same purpose. It ends up being self-contained and less verbose to use a when the amount of code needed is very short. To explore wxPython, check out How to Build a Python GUI Application With wxPython. When you’re playing with Python code in the interactive interpreter, Python lambda functions are often a blessing. It’s easy to craft a quick one-liner function to explore some snippets of code that will never see the light of day outside of the interpreter. The lambdas written in the interpreter, for the sake of speedy discovery, are like scrap paper that you can throw away after use. In the same spirit as the experimentation in the Python interpreter, the module provides functions to time small code fragments. in particular can be called directly, passing some Python code in a string. Here’s an example: When the statement is passed as a string, needs the full context. In the example above, this is provided by the second argument that sets up the environment needed by the main function to be timed. Not doing so would raise a exception. Another approach is to use a : This solution is cleaner, more readable, and quicker to type in the interpreter. Although the execution time was slightly less for the version, executing the functions again may show a slight advantage for the version. The execution time of the is excluded from the overall execution time and shouldn’t have any impact on the result. For testing, it’s sometimes necessary to rely on repeatable results, even if during the normal execution of a given software, the corresponding results are expected to differ, or even be totally random. Let’s say you want to test a function that, at runtime, handles random values. But, during the testing execution, you need to assert against predictable values in a repeatable manner. The following example shows how, with a function, monkey patching can help you: A context manager helps with insulating the operation of monkey patching a function from the standard library ( , in this example). The lambda function assigned to substitutes the default behavior by returning a static value. This allows testing any function depending on in a predictable fashion. Prior to exiting from the context manager, the default behavior of is reestablished to eliminate any unexpected side effects that would affect other areas of the testing that may depend on the default behavior of . Unit test frameworks like and take this concept to a higher level of sophistication. With , still using a function, the same example becomes more elegant and concise : With the pytest fixture, is overwritten with a lambda that will return a deterministic value, , allowing to validate the test. The pytest fixture allows you to control the scope of the override. In the example above, invoking in subsequent tests, without using monkey patching, would execute the normal implementation of this function. Executing the test gives the following result: The test passes as we validated that the was exercised, and the results were the expected ones in the context of the test.\n\nWhile there are great reasons to use , there are instances where its use is frowned upon. So what are the alternatives? Higher-order functions like , , and can be converted to more elegant forms with slight twists of creativity, in particular with list comprehensions or generator expressions. To learn more about list comprehensions, check out When to Use a List Comprehension in Python. To learn more about generator expressions, check out How to Use Generators and yield in Python. The built-in function takes a function as a first argument and applies it to each of the elements of its second argument, an iterable. Examples of iterables are strings, lists, and tuples. For more information on iterables and iterators, check out Iterables and Iterators. returns an iterator corresponding to the transformed collection. As an example, if you wanted to transform a list of strings to a new list with each string capitalized, you could use , as follows: You need to invoke to convert the iterator returned by into an expanded list that can be displayed in the Python shell interpreter. Using a list comprehension eliminates the need for defining and invoking the lambda function: The built-in function , another classic functional construct, can be converted into a list comprehension. It takes a predicate as a first argument and an iterable as a second argument. It builds an iterator containing all the elements of the initial collection that satisfies the predicate function. Here’s an example that filters all the even numbers in a given list of integers: Note that returns an iterator, hence the need to invoke the built-in type that constructs a list given an iterator. The implementation leveraging the list comprehension construct gives the following: Since Python 3, has gone from a built-in function to a module function. As and , its first two arguments are respectively a function and an iterable. It may also take an initializer as a third argument that is used as the initial value of the resulting accumulator. For each element of the iterable, applies the function and accumulates the result that is returned when the iterable is exhausted. To apply to a list of pairs and calculate the sum of the first item of each pair, you could write this: A more idiomatic approach using a generator expression, as an argument to in the example, is the following: A slightly different and possibly cleaner solution removes the need to explicitly access the first element of the pair and instead use unpacking: The use of underscore ( ) is a Python convention indicating that you can ignore the second value of the pair. takes a unique argument, so the generator expression does not need to be in parentheses."
    },
    {
        "link": "https://stratascratch.com/blog/ranking-in-python-and-sql",
        "document": "In this article, we will explore the ranking parameters that can help you handle the ranking tasks in a more efficient and accurate manner.\n\nWe discussed the basics of ranking in Python in our previous tutorial “Methods for Ranking in Pandas” where we looked into the most commonly used parameters of the Pandas ranking function. Specifically, we understood the various methods of ranking, which is crucial in dealing with ties. Aside from ranking methods, there are other parameters in the Pandas ranking function that allow for further customization and refinement of how ranks are determined. Understanding these parameters is critical in working with datasets that require more complex ranking techniques, such as percentile ranking or dealing with missing data.\n\nIn this article, we will explore these parameters so you will be better equipped to handle ranking tasks in a more efficient and accurate manner. We will also demonstrate how to implement some of the ranking methods in SQL, which is a vital skill for data analysts and data scientists, especially in organizations that use SQL as their primary data storage and retrieval language.\n\nBy the end of this article, you will have a comprehensive understanding of ranking methods in both Python and SQL, which will make you feel more confident in tackling ranking questions and analyzing datasets.\n\nThe Pandas rank() function has several parameters that allow you to customize your ranking based on your dataset. These parameters include:\n• na_option: to control how nulls are dealt with during ranking\n\nDid you know that in pandas, you can obtain percentage rankings instead of integer rankings? By default, the .rank() method sets ‘pct’ to False and returns integer rankings. However, when set to True, .rank() computes percentage rankings that are normalized from 0 to 1.\n\nPercentage rankings are useful when you want to compare rankings across different columns, regardless of their actual values. They provide a standardized way of measuring an individual's performance or status relative to the rest of the group. This is why percentage rankings are commonly used in fields like education and healthcare.\n\nSo, what exactly is a percentage ranking and how is it calculated?\n• None percentage of values in the distribution that are less than or equal to the given value or have a lower rank\n\nEssentially, the percentile ranking tells you the percentage of values in the distribution that are less than or equal to the given value or have a lower rank. For example, if a student scored 80 out of 100 on a test and the distribution of scores in the class is such that 60% of the students scored below 80, then the percentile ranking of the student's score is 60%.\n\nLet’s look at a question example from General Assembly.\n\nThe question asks us to find the 80th percentile of hours studied. In other words, “80% of the students studied less than x hours?”\n\nBut let’s expand this requirement. To show how ranking works, we’ll list the hours studied, rank them, and show the percentile rank.\n\nTo solve this, we will have to use the ranking function, setting pct to True.\n\nThe above output shows the first 28 rows, which is enough to demonstrate everything.\n\nAs you can see, the top value (i.e. 200 hours) gets a percentile rank of 1 because 100% of the values are ranking below this (while the lowest will get 0 because it is already the lowest rank).\n\nIn case of ties here, rankings are determined by the average percentile rank of the tied group.\n\nRegarding the 80th percentile, it’s at 163 hours. The next amount of hours studied (162) is below the 80th percentile.\n\nThe ‘na_option’, which allows you to control how nulls are dealt with during ranking. By default, na_option is set to 'keep', which means that there is no ranking returned for these records. However, you can set the na_option to 'bottom' or 'top' which assigns either the lowest or highest rank to these rows.\n\nAs you can see here, the default option leaves the rank null but we can change this easily to give it the top or bottom rank.\n\nThe axis tells you whether to rank rows or columns. Most of the time we are ranking rows and the default option caters to this. However, if you ever find yourself with another dataset type, like a time series of KPIs, you might want to rank columns instead to identify the trends over time for each KPI (represented by a column). To do this, set the axis to 1 instead of 0.\n\nAgain, with the ‘numeric_only’ option, the default option is set to the usual use case which is ranking based on numeric values. However, if you want to rank both numeric and non-numeric values like arranging records by name as well, you can do so by setting this to False.\n\nData analysts and scientists typically must be proficient in SQL and Python so now that you know the variations of ranking in Python, let’s discover their equivalent functions in SQL!\n\nIn SQL, you can achieve this by using the percent_rank() window function. Why don’t you try it on the same question example?\n\nBy default, nulls are left at the bottom and therefore, get the largest ranking (i.e. close to 1). This is unlike the Python rank function where nulls were not given any ranking by default unless we change the na_option parameter.\n\nTo rank nulls, you can specify this in the order by clause. For example:\n\n\n\nWhy don’t you try this?\n\nUsing this guest dataset from Airbnb, we will rank guests according to their age with the eldest receiving the top rank.\n\nRecall that in Python, we used the .rank() function and set ascending to False.\n\nIn SQL, we’ll use the rank window function and specify that age should be ordered in descending order.\n\nLook at the ties and notice that the RANK() window function in SQL is using the minimum method to rank tied groups. In case you haven’t watched our previous ranking video, the minimum method gives the lowest possible rank to the tied values. For example, guests with ages 27 occupy positions 6 and 7. Because of the tie, 6 is now used as the rank of both guests.\n\nIn Python, we use the rank() function with method='dense' to achieve a ranking with no gaps between ranks. In SQL, we use the dense_rank() window function to achieve the same result.\n\nTo start with, let’s get the number of beds listed by each host.\n\nThen, let’s add in the ranking:\n\nAnd this achieves the ranking method we want, which is to not have gaps in the ranking.\n\nIn Python, we use the rank() function with method='first' to assign a unique rank to every record. In SQL, we use the row_number() window function to achieve the same result.\n\nFirst, let’s get the total number of emails sent out by each user.\n\nThen, let’s use this as a basis for our ranking. We can use row_number() in SQL to create the unique rankings.\n\nAnd there you have it - ranking methods in Python and SQL.\n\nIn this post, we've covered other parameters of the Pandas rank function, such as pct, na_option, axis, and numeric_only. We also looked at several ranking methods in Python and discovered their equivalents in SQL. Here’s a cheat sheet for you:\n\nIf you need to change the way nulls are being ranked, specify this in the ORDER BY() clause. For example, RANK() OVER(ORDER BY col NULLS FIRST)\n\nRanking helps identify patterns and trends in datasets so it’s an important tool for data analysis. To master ranking in SQL, check out these two articles:\n• An Introduction to the SQL Rank Functions"
    },
    {
        "link": "https://reddit.com/r/Python/comments/1j4wdwo/best_practices_re_passing_parameters_as_keyword",
        "document": "I've been a professional programmer for 20 years but I have seen a peculiar trend in the last few years. A lot of newer or more junior developers specify arguments as keyword arguments if there are 2 or more. So for something like the below where there are no optional or keyword-only args (i.e. the function is defined ):\n\nThey will insist on writing it as:\n\nTo me this kind of thing is really peculiar and quite redundant. Is this something that is getting taught during, say, \"Intro to Data Engineering\" courses or introductions Python in general? It's kinda grating to me and now I'm seeing some of them requesting changes to Pull Requests they're assigned to review, asking that method/function calls be rewritten this way.\n\nAm I right in considering this to be weird, or is this considered to be current best practice in Python?\n\nupdate: a few people have taken issue with the example I gave. Honestly I just threw it together to be illustrative of the principle itself, it wasn't intended to be held up as a paragon of Good Code :-) Instead I've picked out some code from a real codebase most of us will have used at some point - the \"requests\" library. If we take this snippet ...\n\nand apply the \"always use keywords, always\" dogma to this we get something like the below. What I'm trying to avoid is a codebase that looks like this - because it's visually quite noisy and hard to follow."
    },
    {
        "link": "https://stackoverflow.com/questions/3071415/efficient-method-to-calculate-the-rank-vector-of-a-list-in-python-handle-ties",
        "document": "I'm looking for an efficient way to calculate the rank vector of a list in Python, similar to R's function. In a simple list with no ties between the elements, element i of the rank vector of a list should be x if and only if is the x-th element in the sorted list. This is simple so far, the following code snippet does the trick:\n\nThings get complicated, however, if the original list has ties (i.e. multiple elements with the same value). In that case, all the elements having the same value should have the same rank, which is the average of their ranks obtained using the naive method above. So, for instance, if I have , the naive ranking gives me , but what I would like to have is . Which one would be the most efficient way to do this in Python?\n\nFootnote: let me know if there's a NumPy method to achieve this; but I am interested in a pure Python solution anyway as I'm developing a tool which should work without NumPy as well."
    },
    {
        "link": "https://thepythoncodingbook.com/2023/01/18/best-practices-in-python-functions",
        "document": "You’ve covered a lot of material in this Intermediate Python Functions Series. In this final article, you’ll read about some best practices in Python functions.\n\nThis topic is different from the previous ones discussed in this series. You won’t get a , or any other error if you don’t follow best practices. Your code may still work exactly the same if you use best practices or if you don’t, although bugs are more likely to creep into your code if you don’t.\n\nLet’s see why it still matters to know and use best practices. There were two options when writing this article: either write a very long article covering every possible best practice or write a much more concise article highlighting two key best practices. I’ve chosen to go down the route of writing a short article and focus on the two I think are most relevant.\n\nHere’s an overview of the seven articles in this series:\n• Introduction to the series: Do you know all your functions terminology well?\n• Choosing whether to use positional or keyword arguments when calling a function\n• Using optional arguments by including default values when defining a function\n• Using any number of optional positional and keyword arguments: and\n• Using positional-only arguments and keyword-only arguments: the “rogue” forward slash / or asterisk * in function signatures\n• [This article] Best practices when defining and using functions\n\nStart The Function Name With A Verb\n\nLet’s start with a function you’ve seen often in this series of articles:\n\nIt is clear from the name of the function what the function does. It greets a person! If you choose to call the function , it will be unclear what the function is doing with that greeting.\n\nAnd you should always avoid calling a function or or other names with no descriptive value.\n\nLet’s look at another example. Let’s assume you’re reading code that displays shapes and patterns. You see a function called . Is this function drawing a square or working out the square of a number to use in some calculation? You may be able to figure this out quickly by reading further or running the code. However, if the function name were , you wouldn’t even need to ask yourself the question about the function’s purpose.\n\nDid you notice what’s common in the two best practice examples you read about above? Here are a few more examples of best practice in Python function names compared to versions that you should avoid:\n\nA function performs an action. In the languages we use for human communication, such as English, verbs perform a similar role of denoting an action. Therefore, the best practice in Python functions is to use a verb as the first word in a function name to describe what the function does.\n\nIn the examples you read in the table above, the parameter name is also different in some cases. This series is about functions, so I’ll focus on function names. But when choosing variable names, including parameter names, you often want to use a noun which tells you what the value stored in the variable represents.\n\nWrite A Function That Only Does One Thing\n\nLet’s assume you’re writing a program that deals with historical temperatures which you want to analyse. The data is in Fahrenheit, but you need to work in Celsius and then find the range of temperature in a subset of the data by subtracting the minimum value from the maximum value.\n\nYou decide to write the function . You’ve followed the ‘start with a verb’ best practice and feel you’ve written a descriptive name. All good, then, right?\n\nThere’s a word in your function that’s a giveaway for the next best practice I’ll write about. This is the word “and” in the function name. If you feel the need to add an “and” in your function name, you probably want to write two functions instead.\n\nA function should only perform one action. In this example, you can write a function and another function called .\n\nThis rule sounds simple enough initially. However, some nuance is involved in defining a “single action”.\n\nFor example, consider the function , which takes a string with a person’s full name and returns a string with the initials. This function may need to split the string to separate the components in the full name, such as first name, last name, and perhaps middle names. Then, it will need to extract the first letter from each subcomponent and concatenate them with periods after each letter.\n\nThese are several actions. Does this mean this function goes against the “perform one action” best practice? Every programmer will need to make these decisions in the context of the program and application they’re writing. In this example, it’s likely that grouping the steps listed above into a single function is fine. The single action you want to perform is to get the initials from a name.\n\nAs a rule, if the steps you want to include in a function will always be performed together, then you probably want them in the same function. You’ll get better at making these decisions.\n\nBest practices in Python functions matter. They often make sure code is more readable for others and your future self. They also make bugs less likely and the code more maintainable.\n\nProgrammers will have different views on what constitutes a best practice. You’ll have to make up your mind on which ones to follow. And you may find that as you progress through your Python learning journey, you’ll also change your views on which best practices to adopt!\n\nThis is the final article in the Intermediate Python Functions Series\n• Chapter 3: Power-up Your Coding: Create Your Own Functions for an in-depth introduction to Python functions\n• Chapter 6: Functions Revisited. This chapter covers topics that will be dealt with later on in the series\n• The White Room: Understanding Programming. In this article, I briefly referred to parameters as boxes which store data. This is part of a broader analogy I like to use. You can read more in this chapter\n• Using Python Optional Arguments When Defining Functions is an article I wrote for Real Python if you want to read ahead."
    },
    {
        "link": "https://reddit.com/r/learnpython/comments/1dz5wre/how_do_you_know_when_to_use_arguments_while",
        "document": "The instructor in the course I'm taking just accomplished in 7 lines of code what it took me 24 lines to do, because she included a couple of arguments in the definition of her function. I've never claimed to be the most creative person in the world, but when I was defining my function, why wasn't it immediately obvious that those arguments would be helpful?"
    },
    {
        "link": "https://geeksforgeeks.org/elo-rating-algorithm",
        "document": "The Elo Rating Algorithm is a widely used rating algorithm used to rank players in many competitive games.\n• None Players with higher ELO ratings have a higher probability of winning a game than players with lower ELO ratings.\n• None After each game, the ELO rating of players is updated.\n• None If a player with a higher ELO rating wins, only a few points are transferred from the lower-rated player.\n• None However if the lower-rated player wins, then the transferred points from a higher-rated player are far greater.\n\nTo Solve the problem follow the below idea:\n\nP1: Probability of winning of the player with rating2, P2: Probability of winning of the player with rating1. \n\nP1 = (1.0 / (1.0 + pow(10, ((rating1 – rating2) / 400)))); \n\nP2 = (1.0 / (1.0 + pow(10, ((rating2 – rating1) / 400)))); Obviously, P1 + P2 = 1. The rating of the player is updated using the formula given below:- \n\nrating1 = rating1 + K*(Actual Score – Expected score); In most of the games, “Actual Score” is either 0 or 1 means the player either wins or loose. K is a constant. If K is of a lower value, then the rating is changed by a small fraction but if K is of a higher value, then the changes in the rating are significant. Different organizations set a different value of K.\n\nFollow the below steps to solve the problem:\n• None Calculate the probability of winning of players A and B using the formula given above\n• None If player A wins or player B wins then the ratings are updated accordingly using the formulas:\n• None Where the Actual score is 0 or 1\n\nBelow is the implementation of the above approach:\n\n// outcome determines the outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw. // Outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw // outcome determines the outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw. // Outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw # outcome determines the outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw. # Outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw // outcome determines the outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw. // Outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw // outcome determines the outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw. // Outcome: 1 for Player A win, 0 for Player B win, 0.5 for draw\n\nTime Complexity: The time complexity of the algorithm depends mostly on the complexity of the pow function whose complexity is dependent on Computer Architecture. On x86, this is constant time operation:-O(1)\n\nAuxiliary Space: O(1)"
    },
    {
        "link": "https://medium.com/@adelbasli/unlocking-the-elo-system-a-deep-dive-into-the-mathematics-of-skill-measurement-0e824ef3f1b6",
        "document": "The thrill of competition, the desire to prove oneself against others, lies at the heart of countless games and sports. But how do we measure and compare skill levels fairly, especially in games of strategy and intellect? Enter the Elo rating system, a mathematical framework that has become synonymous with competitive ranking.\n\nDeveloped by Hungarian-American physicist Arpad Elo in the 1950s, the Elo system revolutionized the way we approach skill assessment in zero-sum games, where one player’s gain is another’s loss. Initially designed for chess, its elegant simplicity and robust statistical foundation have led to its widespread adoption in diverse fields, from esports and football to online matchmaking and even beyond the realm of games.\n\nThe Essence of Elo: Predicting Outcomes and Tracking Progress\n\nAt its core, the Elo system assigns a numerical rating to each player, reflecting their relative skill level within a specific pool of competitors. This rating, far from being a static measure, dynamically adjusts based on the outcome of games played.\n\nThe beauty of Elo lies in its ability to predict the outcome of a match between two players. A higher-rated player is statistically more likely to win against a lower-rated opponent. The larger the rating difference, the more skewed the odds become. For instance, a player rated 200 points higher than their opponent has a statistically significant advantage. We can express this mathematically using the Elo formula:\n• Ra is the rating of player A.\n• Rb is the rating of player B.\n\nThis formula outputs a probability between 0 and 1. For a 200-point rating difference, the expected win probability for the higher-rated player is approximately 0.75 or 75%.\n\nBut Elo is more than just a predictor; it’s a dynamic tracker of progress. After every game, points are exchanged between the players based on the outcome and their rating difference. A win against a higher-rated opponent yields a greater rating gain, while a loss against a lower-rated player results in a more significant rating loss. This self-correcting mechanism ensures that ratings constantly evolve to reflect a player’s true skill level. The rating update rule is:\n• Rn is the new rating.\n• Ro is the old rating.\n• K is the K-factor, which controls the magnitude of rating adjustments.\n• S is the actual score (1 for a win, 0.5 for a draw, 0 for a loss).\n• E is the expected score calculated using the Elo formula.\n\nDelving Deeper: The Mathematics Behind the Magic\n\nWhile the Elo system’s application is remarkably straightforward, its mathematical underpinnings are rooted in sound statistical principles. Elo’s central assumption posits that a player’s performance in any given game can be represented as a normally distributed random variable. Though performance may fluctuate from game to game, the average performance over time is a more stable indicator of their true skill.\n\nElo utilizes the logistic function, as seen in the Elo formula, to calculate the expected score for each player in a matchup. This function takes into account the rating difference between the players, translating it into a probability of winning. The actual outcome of the game is then compared to this expected score, and the ratings are adjusted accordingly using the rating update rule.\n\nBeyond the Basics: K-factor, Inflation, and Adaptations\n\nThe Elo system, in its various implementations, utilizes a parameter known as the K-factor, which determines the magnitude of rating adjustments after each game. A higher K-factor leads to more volatile ratings, making the system more sensitive to recent results. Conversely, a lower K-factor results in more stable ratings, reflecting long-term performance.\n\nOne of the challenges faced by rating systems is the phenomenon of inflation or deflation. Inflation occurs when the average rating of the player pool increases over time, while deflation represents a decrease in the average rating. These fluctuations can make it difficult to compare players across different eras. To mitigate these issues, Elo systems often incorporate mechanisms like rating floors and K-factor adjustments based on experience and rating level.\n\nThis code defines functions for calculating the expected score and updating ratings based on game outcomes. You can modify this code and integrate it into larger projects to implement Elo-based ranking systems.\n\nFrom Chessboards to Esports Arenas: The Enduring Legacy of Elo\n\nThe Elo system, despite its initial focus on chess, has transcended its origins to become a ubiquitous tool for measuring and ranking skill in a vast array of competitive domains. From online gaming platforms and esports leagues to traditional sports and even non-gaming contexts like matchmaking services, Elo’s influence is undeniable.\n\nWhile alternative rating systems like Glicko-2 have emerged, offering refinements and addressing some of Elo’s limitations, the core principles and intuitive appeal of Elo continue to resonate. Its ability to quantify skill, predict outcomes, and track progress has cemented its place as a cornerstone of competitive gaming and beyond.\n\nIn an increasingly competitive world, where the desire to measure and compare ourselves is ever-present, the Elo system stands as a testament to the power of mathematics to bring order and objectivity to the often subjective realm of skill assessment."
    },
    {
        "link": "https://grant592.github.io/elo-ratings-code",
        "document": "So my previous post looked at the maths behind Elo ratings and this post will go over how I’ve coded the rating system up in python. When I originally coded this up it was mainly to see how Elo ratings looked in the context of Prem rugby however now I plan to update them each week in light of the prem results and post predictions prior to each round. In terms of the code, in the words of Prussian field marshall Helmuth Karl Bernhard von Moltke: “No battle plan ever survives contact with the enemy.” What do I mean by this? I mean the code is probably going to change once the season starts as it was initially written to do a single piece of analysis but will need modified to make it easier to update on a weekly basis.\n\nI’ll go over the initial implementation and how I calculated $k$ and home advantage constants.\n\n2 - Create a class to parse and store a single seasons worth of results and fixture data\n\nThe above code creates a class which will scrape the season data from a certain website and create a pandas DataFrame with all the season’s data in the following format:\n\nThis dataframe will give us all the info we need for calculating the Elo ratings.\n\n3 - Create a simple Team class containing the team name and their current Elo rating.\n\nNote that any new team will start with an Elo rating of 1500:\n\nThe function is just implementing the equations that I went over in my previous post. It takes as inputs a single row from the table and a dictionary of instances with the key being the team’s name and the value being an instance of PremTeam.\n• is a returning the home team from the row of the row that was passed to the function.\n• is then using this ‘home_team’ value to access the dictionary entry of which will return an instance of from which we can read the elo_rating for a given team.\n\n5 - We now need to write some code to bring all these classes together. I’ll go through it step by step.\n\n# Create list of season dates for parsing season data. # We'll be parsing all seasons from 2008 to 2020 # Parse each season data and append to season_list # Create an empty list to store all the seasons. # For each season, scrape the season data and # append the instance of SeasonData class to the list # Create list of all team names over parsed seasons. # For every team that has a fixture in our data, add # this team to the set of all team names # Create a dictionary of all teams. # The key is the team name and the value an # instance of PremTeam class # Ensure that all the seasons are sorted in chronological # order in ou list of SeasonData instances # Ensure the matches are in chronological order # At the start of a season, revert each teams Elo ratings 1/3 towards the average # to account for off-season changes. Bad teams ofetn get slightly better and # good teams often sligthly worse so we regress everyone toward the mean to \\ # Count the number of matches in the season # As we have sorted our SeasonData dataframe, we can access each fixture # in order and update the each teams elo rating. The teams are stored in the # dict called teams which we pass into this function # Print the up to date elo_ratings\n\nAt the end of the 2019-2020 season, the Elo ratings ended up as follows (the teams are ordered by league position - note that Elo rating may not perfectly reflect league rating as it may depend on which teams a team beat/lost to):\n\n6 - we also need to look at the best values for $k$ and $\\mbox{HA}$.\n\nThe way we will do this is by calculating the mean squared error between our predictions and our outcomes. More formally:\n\n# Order values to iterate through seaason 1 round at a time # The teams Elo rating are based on fixtures prior to 2020-21 season, therefore # prior to start of season, revert all elo ratings 1/3 toward the mean. \\ # Work through each match in the season. Calculate prediction for each match and updat elo # Store a baseline of the teams dict which we can revert to # everytime we use a new set of parameters # Create an empty dataframe to store the erros for # each pair of parameters\n\nWhen we plot the output of the above we can see that the best values for $k$ and $\\mbox{home advantage}$ are 40 and 50 respectively. It should probably be noted that home advantage may have had less of an influence in this season due to the lack of crowds but we’ll roll with it and see how it pans out over the current season.\n\n7 - Finally, let’s plug the values for $k$ and $\\mbox{HA}$ back into the whole thing.\n\nI’ll run it from the start and see where each team will be starting from this coming season. The first value represents where they ended up at the end of 2020-2021 season and the second value is their starting value for this coming season after reverting each team towards the mean.\n\nThis wraps up my post. I’ll start making predictions prior to the start of the season and we can see how accurate the Elo ratings are. Once we’re into the season and internationals start, I’ll consider revising the ratings depending on the number of international players missing amongst other things."
    },
    {
        "link": "https://towardsdatascience.com/rating-sports-teams-elo-vs-win-loss-d46ee57c1314",
        "document": "There are many ways to determine who is the best team or player in any sport. You can look at the last 5 games. The last 10 games. You can use score differential. You can look at the roster and see which team has Tom Brady or LeBron James. Whether it be power rankings, divisional rankings, or anything else – there are scientific ways to rate teams and there are unscientific ways. The following is an attempt to scientifically decide on a good ratings system.\n\nWhat is an Elo Rating?\n\nI want to focus on comparing rating systems, and so defining the Elo rating isn’t the purpose of this article. There are very good resources such as 538’s thorough description of Elo. The Wikipedia page is also packed full of good information. I will however give a quick synopsis.\n\nThe Elo rating system is a very simple yet extremely effective ratings system. It was originally developed for Chess, but nothing about it is Chess-specific. Elo, in its original form, can be used for any head to head game. Arm wrestling, Scrabble, Pong, you name it. Another advantage that it can be used for games between two players, or games between two teams.\n\nEach player or team has their own Elo rating. It is convention that new players start with a rating of 1500, but 1500 is somewhat arbitrary. When two players play, they automatically each \"wager\" some of their Elo points before the game. The size of this wager is known as the K-value. The winner is awarded points after the match and the loser loses points after the match. The amount of points awarded depends on the relative difference in initial ratings. For example, if a 1700 skill player beats a 1300 skill player, not many points are awarded – this outcome was expected. On the other hand, if the 1300 level player beats the 1700 level player, almost the entire pool of wagered points is awarded to the 1300 level player and deducted from the 1700 level player. At its most fundamental level, the Elo system simply relies on the sigmoid function to relate ratings difference to win probability:\n\nWhile it’s not necessarily important to remember the exact equation, it is beneficial in many applications to relate a continuous, potentially infinite variable to a probability. There are of course ways to shape this function and add parameters to make it even more reflective of reality. I will get to that in more advanced blog posts.\n\nThe K-value, or amount of points wagered, depends on the game. Larger K-values will react quicker to unexpected results. In a sport such as baseball, the K-value would be small. Even the best baseball teams struggle to win more than 70% of their games. Also, they play 162 games in the MLB. So if the best team in the MLB loses three straight games it’s probably not a huge deal, and their solid rating should remain mostly intact. In a case like baseball, a large K-value would be detrimental, because the system would overreact to the more random results. However, in the NFL or college basketball, the best teams are expected to win more than 80% of their games. In the NFL, there’s a small sample size as well. It’s desirable in that case to allow more reactive Elo ratings – a team with a 3 game losing streak is probably not the best team in the league. Also, over a short season, you want the ratings to adjust quickly.\n\nIf we’re interested in predicting the outcome of games, Elo ratings are powerful predictive tools. As I plan on showing, they are usually more predictive than win-loss records and the exotic variations can come close to prediction markets. However, most sports markets are simply too advanced now for a vanilla Elo to have any sort of chance at being profitable.\n\nNote: This post was updated April 2023 to use a Python oddsmaker package that I’ve written to make the following experiment require only a few lines of code.\n\nI claim that Elo scores are better than win-loss records in describing the skill of a player or team. Let’s not take that for granted. First, to demonstrate the point, I will design an abstract game in which some players are better than others, have them play each other. Players will have a win-loss record and an Elo rating. We will then be able to compare the Ratings in a friendly environment to compare them. In a future post, I will use easy-to-obtain sports data and see if the process works in the real world, and not just a friendly experiment.\n\nIn the real-world, there are messy complications that the classic version of Elo is not designed to handle. For example, a team will draft players in the offseason and become better or worse. If a star player is traded or injured, Elo has no idea. There is the whole concept of home field advantage, too. So let’s design a game that doesn’t have messy complications.\n\nThere’s a new game called Sportball, and a major league with all the best Sportball players just formed. Fans from around the globe travel to see their favorite Sportball teams. Major League Sportball has 32 players, and players always play at the midpoint between the cities that they are from (to neutralize home field advantage). While ticket offices don’t like this move, people who calculate Elo ratings love it.\n\nTo simulate true ability, there will be a \"true rating\" for each player. When two Sportball players play, a random variation will be added to the true rating so that lesser teams can upset better teams. Otherwise, it would be a boring experiment. The random variation is basically a way to simulate luck and how players are feeling that day.\n\nLastly, to simulate change over time, each player’s true ratings will randomly be nudged higher or lower after each game. Season win-loss records will not be able to adjust to these nudges yet Elo will be able to do so. This gives Elo an advantage that it would also have in the real world. At the end of the 1000 game season, we will compare true rating, win-loss record, and Elo scores.\n\nAs mentioned before, this post has been updated to use my oddsmaker package. The package Github page has the full code below from start to finish, I will just show the highlights. Ultimately, we just want to create 24 (or any number) of fake players:\n\nWhile I have made this into a single function call, you could easily try writing this code for yourself. I’m simply generating fake names using the faker package, and then randomly drawing from a normal distribution with mean 1500 to create a \"true\" sportsball rating. Then, we will try recovering this true rating with an Elo model that only sees game results.\n\nNext, I create a random walk that nudges each player up and down so their true rating isn’t constant over time. You can imagine this is them changing workout routines or diet, or maybe going through relationship issues, who knows.\n\nAbove I plotted the ratings evolution for the first two players, Angela and Ashley. You can see how this might mimic real-world talent evolution. While a team like Manchester United might fluctuate in form, they won’t suddenly go to the bottom of the league, or at least that is extremely rare over a 100+ year history. This is of course different depending on the sport. It is very easy to play with the parameters to create different types of simulations.\n\nNow, let’s see how good our model is at recovering results. I simply run a few lines of code from the oddsmaker Elo class:\n\nNow we have pregame ratings and postgame adjustments for 6000 games in under a second. It’s worth taking a second to appreciate that Arpad Elo used to do these calculations by hand. I have no doubt he’d be awed and perhaps frustrated at how quickly modern tools work. Below, I’ve added some dashed lines, which represent our attempt to recover the true ratings from results over time:\n\nBoth players are some of the better players in the sample. You can see our ratings system takes a while to catch up to their ratings from the initial guess of average. This is especially true for Angela, who we needed almost 300 games of data to believe she was actually as good as she is. Once we catch up, there is still a decent amount of noise. Ashley must’ve run hot and gotten lucky, as we think she’s better than she actually is at sportsball based on results. If I were to increase the k-value, we’d adjust quicker initially, but would be more noisy at the end. The opposite would happen for a decrease in k.\n\nHow do we know how good our ratings system is? Our live ratings clearly approach the true ratings, but seems like it can be improved by either being quicker to adjust initially, or less noisy toward the end, or somehow both. Those adjustments to the ratings system will be the topic of future posts. But first, we need a way to rate the ratings system, so that we can empirically judge if we are improving or not.\n\nBrier scores are a straightforward way of scoring classification predictions. I’m choosing to use Brier, but there are very similar methods like log-loss. Like Elo, I’m not the one who can explain it best. They are a central component of one of my favorite books, Superforcasting. I will do my best to give a quick synopsis.\n\nLet’s say I’m forecasting the weather. I say there’s a 30% chance of rain. Then it rains. It’s tempting, and very human, to say \"well that’s an awful prediction\". Humans (and I’m guilty of this too) often assume if someone is on the wrong side of 50%, they are wrong. Whenever someone says there is a 65% chance of something, and it doesn’t happen, it feels like that person is wrong. But that person could’ve been exactly right. In fact, someone might suggest there is a 5% chance of rain, and then it rains, and that person is actually correct because if you take 1,000 of their predictions that say 5%, exactly 5% of them occur. I promise I’m not a weatherman or weatherman apologist. We simply live in an unpredictable world, and unlikely events occur. Luckily we are Sportsball analysts, and we get thousands of games to figure out our error!\n\nBrier scores basically sum up prediction error. A good forecaster will have a minimal Brier score. I’ll take the Wikipedia examples on a rain forecast to demonstrate:\n• If the forecast is 100% and it rains, then the Brier Score is 0, the best score achievable.\n• If the forecast is 100% and it does not rain, then the Brier Score is 1, the worst score achievable.\n• If the forecast is 70% and it rains, then the Brier Score is (0.70−1)^2 = 0.09.\n• If the forecast is 30% and it rains, then the Brier Score is (0.30−1)^2 = 0.49.\n• If the forecast is 50% then the Brier score is (0.50−1)^2 = (0.50−0)^2 = 0.25, regardless of whether it rains.\n\nSo, there is a tradeoff here. I can choose to be very conservative with my rain forecasts. I can say there’s a 50% chance of rain every day. In that case, no matter what happens, my Brier error is 0.25 per prediction. That’s completely useless. Any error above that is therefore bad, and in some cases, it might be better to take the opposite of the prediction.\n\nThe strength of Brier scores is that it penalizes overconfident forecasters. You’re rewarded for predicting \"I’m sure of something\" when the outcome is near certain, and also rewarded for predicting \"It might happen, I’m not sure\" when the outcome isn’t certain.\n\nBrier scores aren’t always perfect, and sometimes they need additional context. Let’s say I’m in the desert, where it barely ever rains. Then I can have a reasonably good Brier score by predicting 0% every day. On one hand, it feels like cheating. The one or two times it does rain I will have a huge error (1 each day). This is a severe penalty. On the other hand, there is some degree of knowledge going into the prediction (that I’m in a desert) and so a smaller Brier makes sense. This is a case where it is good to establish a contextual baseline before giving out weatherman awards.\n\nLet’s go back to the title – comparing how the Elo system does to just using win-loss record alone. If you’ve made it this far, you’re obviously pretty interested in working with analytics. The lesson here isn’t which is better, or to show off the oddsmaker package I made, it’s that in a lot of situations you can empirically back-test to find answers and confidence intervals to those answers. That is potentially useful in a lot of contexts.\n\nIn the following sim, I run a 2,000 game season with 24 contestants 300 times. I track the error for win-loss and Elo for each of the 1,000 sims. I vary how much the initial players are different in skill, and how much their skill changes over time. In my simulations, Elo took a few games to catch up to the win-loss record method in performance. Once it does, it becomes clearly the better system.\n\nThere are a lot of potential paths from here. In the comments, some suggested using a high k value at first, and then dropping to a lower k value once ratings had stabilized. That would almost surely improve performance. That’s the idea behind using an uncertainty parameter too, which I am currently writing about. Glicko and TrueSkill ratings systems use a variation of that idea. With all the accessible data and libraries, it would also be worthwhile to try on real data. I encourage you to play with these things yourself, it is fairly easy.\n\nI hope you learned something throughout the article. I wrote a companion Jupyter notebook if you want to see the code used to create the figures."
    },
    {
        "link": "https://medium.com/@ML_Soccer_Betting/implementing-the-elo-ratings-for-soccer-teams-in-python-e3a067b024be",
        "document": "In this article I will present pythonic implementation of several variations of ELO ratings and examine their performance in a simple logistic regression model.\n\nThe basis for my code to implement the ELO ratings can be found here:\n\nhttps://medium.com/mlearning-ai/how-to-calculate-elo-score-for-international-teams-using-python-66c136f01048\n\nI have made changes to the code to speed it up.\n\nFirst let’s take a look at our data. Our soccer data covers the top leagues in the four Scandinavian countries.\n\nWe have the team names, the country and league, the kick off time and the scores. The odds data is an average of bookmakers odds. Before obtaining our ratings we check our data for NaN values, and any inconsistencies.\n\nWe have only 2 NaN values, and they are in the odds columns. We can fix this easily. Now, the implied probability of our bookies prices should lie in the range between 1.04 to 1.10. The excess over 100% is called the overround and it is there to ensure the bookmakers always have an advantage. A total implied probability below 1.0 would imply an error in our data. The implied probability is calculated by dividing 1 by the decimal odds.\n\nThe average is 106.9% with a small standard deviation. All values lie above 100%, indicated above by the red dotted line. We see only one outlier in the data which occurs at around index 2200.\n\nAfter fixing this outlier and the NaN values we can proceed with the calculation of ratings. Here is the code:\n\nThe code follows that of the article linked at the beginning, with some changes. You will find a detailed explanation of the code and the ratings there. The changes I’ve made are:\n• Introducing a timestamp variable to ensure our data is ordered chronologically.\n• Converting the dataframe to a list of lists to speed up computation.\n\nThe parameter k = 22 was found following optimisation. In later articles I will demonstrate how to find the optimal values for these parameters.\n\nThe next piece of code implements the goals-based ELO ratings, as outlined in this paper.\n\nThe ELO-goals rating is a modification of the basic ELO that takes margin of victory into account. To prevent extreme score lines from having a significant impact on the outcome, we cap the number of home goals and away goals at 6 before performing the calculation.\n\nThe code is mostly similar to the first implementation, with the adjustment\n\nSo now k is not a constant, but depends on delta, the goal difference. The magnitude of the change in ratings now depends on margin of victory in addition to match outcome.\n\nFinally, I introduce my own variation on the ELO ratings. This is very similar to the goals-based system. We amend it by replacing goal difference with the difference in their implied probabilities. This will mean that consistent favourites, who win when they are expected to will be given additional reward and inconsistent favourites additional penalty.\n\nBefore comparing their performance in a predictive model, let’s examine our features more closely.\n\nSurprisingly enough, the basic ELO ratings shows the strongest correlation with our targets.\n\nThis chart shows the 250 period moving average of the correlations. The most stable is the ELO Goals based rating.\n\nThis chart shows the evolution of the ELO ratings for the Norwegian team Molde. Molde are strong performers in the Norwegian league. What is noticeable is the stability of the Odds based rating. The relative volatility of the basic ELO ratings can be attributed to the optimal k used for that ratings system.\n\nWe now compare the predictive power of each of our features, here is the code:\n\nI compare the features using accuracy, precision, recall, brier loss, and roc auc. The inclusion of “home_ip” allows us to see how our features measure up against the bookies predictions. Then we create a Lasso classifier that uses an ensemble of all features to see if this improves our results.\n\nThe ELO rankings are clearly inferior in their predictive power compared to the bookmakers predictions. The basic ELO rating difference is the best performing of our features. Combining all features together improves the performance but we cannot match the performance of the bookies with these features. Most notable is the superior performance in the brier score loss and roc auc. These metrics measure how far off our probabilities are from the correct label. We will have to find better ratings!\n\nThat is it for this article, hope you enjoyed. I will be looking further into new adaptations of the ELO ratings using the bookmakers odds, and I will be publishing implementations of the ratings systems found here and here. Follow for more!"
    }
]