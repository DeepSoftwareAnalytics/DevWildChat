[
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/Model",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nA model grouping layers into an object with training/inference features.\n\nUsed in the notebooks\n\nThere are three ways to instantiate a :\n\nYou start from , you chain layer calls to specify the model's forward pass, and finally you create your model from inputs and outputs:\n\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\n\nNote that the and models are not created with objects, but with the tensors that originate from objects. Under the hood, the layers and weights will be shared across these models, so that user can train the , and use or to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.\n\nIn that case, you should define your layers in and you should implement the model's forward pass in .\n\nIf you subclass , you can optionally have a argument (boolean) in , which you can use to specify a different behavior in training and inference:\n\nOnce the model is created, you can config the model with losses and metrics with , train the model with , or use the model to do prediction with .\n\nIn addition, is a special case of model where the model is purely a stack of single-input, single-output layers.\n\nCompiles the model with the information given in config.\n\nThis method uses the information in the config (optimizer, loss, metrics, etc.) to compile the model.\n\nCompute the total loss, validate it, and return it.\n\nSubclasses can optionally override this method to provide custom loss computation logic.\n\nUpdate metric states and collect all metrics to be returned.\n\nSubclasses can optionally override this method to provide custom metric updating and collection logic.\n\nReturns the loss value & metrics values for the model in test mode.\n\nComputation is done in batches (see the arg.)\n\nThis method lets you export a model to a lightweight SavedModel artifact that contains the model's forward pass only (its method) and can be served via e.g. TF-Serving. The forward pass is registered under the name (see example below).\n\nThe original code of the model (including any custom layers you may have used) is no longer necessary to reload the artifact -- it is entirely standalone.\n\nIf you would like to customize your serving endpoints, you can use the lower-level class. The method relies on internally.\n\nTrains the model for a fixed number of epochs (dataset iterations).\n\nUnpacking behavior for iterator-like inputs: A common pattern is to pass an iterator like object such as a or a to , which will in fact yield not only features ( ) but optionally targets ( ) and sample weights ( ). Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for and respectively. Any other type provided will be wrapped in a length-one tuple, effectively treating everything as . When yielding dicts, they should still adhere to the top-level tuple structure, e.g. . Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the . The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: where it is unclear if the tuple was intended to be unpacked into , , and or passed through as a single element to .\n\nThis method is the reverse of , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by ).\n\nReturns a serialized config with information for compiling the model.\n\nThis method returns a config dictionary containing all the information (optimizer, loss, metrics, etc.) with which the model was compiled.\n\nRetrieves a layer based on either its name (unique) or index.\n\nIf and are both provided, will take precedence. Indices are based on order of horizontal graph traversal (bottom-up).\n\nIf any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method.\n\nWeights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights.\n\nIf you have modified your model, for instance by adding a new layer (with weights) or by changing the shape of the weights of a layer, you can choose to ignore errors and continue loading by setting . In this case any layer with mismatching weights will be skipped. A warning will be displayed for each skipped layer.\n\nComputation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time.\n\nFor small numbers of inputs that fit in one batch, directly use for faster execution, e.g., , or if you have layers such as that behave differently during inference.\n\nNote that is an alias for .\n• The model's optimizer's state (if any)\n\nThus models can be reinstantiated in the exact same state.\n\nTest the model on a single batch of samples.\n\nTo load a network from a JSON save file, use ."
    },
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/models/load_model",
        "document": "Used in the notebooks\n\nNote that the model variables may have different name values ( property, e.g. ) after being reloaded. It is recommended that you use layer attributes to access specific variables, e.g. ."
    },
    {
        "link": "https://tensorflow.org/tutorials/keras/save_and_load",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nModel progress can be saved during and after training. This means a model can resume where it left off and avoid long training times. Saving also means you can share your model and others can recreate your work. When publishing research models and techniques, most machine learning practitioners share:\n• code to create the model, and\n• the trained weights, or parameters, for the model\n\nSharing this data helps others understand how the model works and try it themselves with new data.\n\nThere are different ways to save TensorFlow models depending on the API you're using. This guide uses tf.keras—a high-level API to build and train models in TensorFlow. The new, high-level format used in this tutorial is recommended for saving Keras objects, as it provides robust, efficient name-based saving that is often easier to debug than low-level or legacy formats. For more advanced saving or serialization workflows, especially those involving custom objects, please refer to the Save and load Keras models guide. For other approaches, refer to the Using the SavedModel format guide.\n\nGet an example dataset\n\nTo demonstrate how to save and load weights, you'll use the MNIST dataset. To speed up these runs, use the first 1000 examples:\n\nYou can use a trained model without having to retrain it, or pick-up training where you left off in case the training process was interrupted. The callback allows you to continually save the model both during and at the end of training.\n\nCreate a callback that saves weights only during training:\n\nThis creates a single collection of TensorFlow checkpoint files that are updated at the end of each epoch:\n\nAs long as two models share the same architecture you can share weights between them. So, when restoring a model from weights-only, create a model with the same architecture as the original model and then set its weights.\n\nNow rebuild a fresh, untrained model and evaluate it on the test set. An untrained model will perform at chance levels (~10% accuracy):\n\nThen load the weights from the checkpoint and re-evaluate:\n\nThe callback provides several options to provide unique names for checkpoints and adjust the checkpointing frequency.\n\nTrain a new model, and save uniquely named checkpoints once every five epochs:\n\nNow, review the resulting checkpoints and choose the latest one:\n\nTo test, reset the model, and load the latest checkpoint:\n\nWhat are these files?\n\nThe above code stores the weights to a collection of checkpoint-formatted files that contain only the trained weights in a binary format. Checkpoints contain:\n• One or more shards that contain your model's weights.\n• An index file that indicates which weights are stored in which shard.\n\nIf you are training a model on a single machine, you'll have one shard with the suffix:\n\nTo save weights manually, use . By default, —and the method in particular—uses the TensorFlow Checkpoint format with a extension. To save in the HDF5 format with a extension, refer to the Save and load models guide.\n\nCall to save a model's architecture, weights, and training configuration in a single zip archive.\n\nAn entire model can be saved in three different file formats (the new format and two legacy formats: , and ). Saving a model as automatically saves in the latest format.\n\nYou can switch to the SavedModel format by:\n\nYou can switch to the H5 format by:\n\nSaving a fully-functional model is very useful—you can load them in TensorFlow.js (Saved Model, HDF5) and then train and run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (Saved Model, HDF5)\n\n*Custom objects (for example, subclassed models or layers) require special attention when saving and loading. Refer to the Saving custom objects section below.\n\nThe new Keras v3 saving format, marked by the extension, is a more simple, efficient format that implements name-based saving, ensuring what you load is exactly what you saved, from Python's perspective. This makes debugging much easier, and it is the recommended format for Keras.\n\nThe section below illustrates how to save and restore the model in the format.\n\nTry running evaluate and predict with the loaded model:\n\nThe SavedModel format is another way to serialize models. Models saved in this format can be restored using and are compatible with TensorFlow Serving. The SavedModel guide goes into detail about how to the SavedModel. The section below illustrates the steps to save and restore the model.\n\nThe SavedModel format is a directory containing a protobuf binary and a TensorFlow checkpoint. Inspect the saved model directory:\n\nThe restored model is compiled with the same arguments as the original model. Try running evaluate and predict with the loaded model:\n\nKeras provides a basic legacy high-level save format using the HDF5 standard.\n\nNow, recreate the model from that file:\n\nKeras saves models by inspecting their architectures. This technique saves everything:\n• The model's training configuration (what you pass to the method)\n• The optimizer and its state, if any (this enables you to restart training where you left off)\n\nKeras is not able to save the optimizers (from ) since they aren't compatible with checkpoints. For v1.x optimizers, you need to re-compile the model after loading—losing the state of the optimizer.\n\nIf you are using the SavedModel format, you can skip this section. The key difference between high-level /HDF5 formats and the low-level SavedModel format is that the /HDF5 formats uses object configs to save the model architecture, while SavedModel saves the execution graph. Thus, SavedModels are able to save custom objects like subclassed models and custom layers without requiring the original code. However, debugging low-level SavedModels can be more difficult as a result, and we recommend using the high-level format instead due to its name-based, Keras-native nature.\n\nTo save custom objects to and HDF5, you must do the following:\n• Define a method in your object, and optionally a classmethod.\n• returns a JSON-serializable dictionary of parameters needed to recreate the object.\n• uses the returned config from to create a new object. By default, this function will use the config as initialization kwargs ( ).\n• Pass the custom objects to the model in one of three ways:\n• Register the custom object with the decorator. (recommended)\n• Directly pass the object to the argument when loading the model. The argument must be a dictionary mapping the string class name to the Python class. E.g.,\n• Use a with the object included in the dictionary argument, and place a call within the scope.\n\nRefer to the Writing layers and models from scratch tutorial for examples of custom objects and .\n\n# Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the \"Software\"), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL # THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER"
    },
    {
        "link": "https://keras.io/2.18/api/models/model_saving_apis/model_saving_and_loading",
        "document": "See the Serialization and Saving guide for details.\n• filepath: or object. Path where to save the model.\n• overwrite: Whether we should overwrite any existing model at the target location, or instead ask the user via an interactive prompt.\n• save_format: Either , , , indicating whether to save the model in the native TF-Keras format ( ), in the TensorFlow SavedModel format (referred to as \"SavedModel\" below), or in the legacy HDF5 format ( ). Defaults to in TF 2.X, and in TF 1.X.\n\nSavedModel format arguments: include_optimizer: Only applied to SavedModel and legacy HDF5 formats. If False, do not save the optimizer state. Defaults to . signatures: Only applies to SavedModel format. Signatures to save with the SavedModel. See the argument in for details. options: Only applies to SavedModel format. object that specifies SavedModel saving options. save_traces: Only applies to SavedModel format. When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a method.\n\nNote that is an alias for .\n\nSee the Serialization and Saving guide for details.\n• filepath: or object. Path where to save the model.\n• overwrite: Whether we should overwrite any existing model at the target location, or instead ask the user via an interactive prompt.\n• save_format: Either , , , indicating whether to save the model in the native TF-Keras format ( ), in the TensorFlow SavedModel format (referred to as \"SavedModel\" below), or in the legacy HDF5 format ( ). Defaults to in TF 2.X, and in TF 1.X.\n\nSavedModel format arguments: include_optimizer: Only applied to SavedModel and legacy HDF5 formats. If False, do not save the optimizer state. Defaults to True. signatures: Only applies to SavedModel format. Signatures to save with the SavedModel. See the argument in for details. options: Only applies to SavedModel format. object that specifies SavedModel saving options. save_traces: Only applies to SavedModel format. When enabled, the SavedModel will store the function traces for each layer. This can be disabled, so that only the configs of each layer are stored. Defaults to . Disabling this will decrease serialization time and reduce file size, but it requires that all custom layers/models implement a method.\n\nNote that is an alias for .\n\nThe SavedModel or HDF5 file contains:\n• The model's optimizer's state (if any)\n\nThus models can be reinstantiated in the exact same state, without any of the code used for model definition or training.\n\nNote that the model weights may have different scoped names after being loaded. Scoped names include the model/layer names, such as . It is recommended that you use the layer properties to access specific variables, e.g. .\n\nWith , the model and all trackable objects attached to the it (e.g. layers and variables) are saved as a TensorFlow SavedModel. The model config, weights, and optimizer are included in the SavedModel. Additionally, for every TF-Keras layer attached to the model, the SavedModel stores:\n• The config and metadata – e.g. name, dtype, trainable status\n• Traced call and loss functions, which are stored as TensorFlow subgraphs.\n\nThe traced functions allow the SavedModel format to save and load custom layers without the original class definition.\n\nYou can choose to not save the traced functions by disabling the option. This will decrease the time it takes to save the model and the amount of disk space occupied by the output SavedModel. If you enable this option, then you must provide all custom class definitions when loading the model. See the argument in .\n• filepath: or object, path to the saved model file.\n• custom_objects: Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization.\n• compile: Boolean, whether to compile the model after loading.\n• safe_mode: Boolean, whether to disallow unsafe deserialization. When , loading an object has the potential to trigger arbitrary code execution. This argument is only applicable to the TF-Keras v3 model format. Defaults to True.\n\nSavedModel format arguments: options: Only applies to SavedModel format. Optional object that specifies SavedModel loading options.\n\nA TF-Keras model instance. If the original model was compiled, and the argument is set, then the returned model will be compiled. Otherwise, the model will be left uncompiled.\n\nNote that the model variables may have different name values ( property, e.g. ) after being reloaded. It is recommended that you use layer attributes to access specific variables, e.g. ."
    },
    {
        "link": "https://stackoverflow.com/questions/58720431/how-to-read-tensorflow-2-0-keras-api-documentation",
        "document": "I am looking at the documentation for Keras applications in TF 2.0 and I feel like I must be missing something.\n\nThe TF 2.0 documentation for ResNet101V2, for example, is supremely unhelpful when compared to the Keras documentation of the same application. Following the TF documentation's 'View source code on GitHub' link just sends you down a rabbit hole as the documentation references a decorator. Am I using the TF documentation incorrectly?"
    },
    {
        "link": "https://medium.com/@avinashmachinelearninginfo/sequences-and-padding-methods-in-nlp-in-tensorflow-ii-e5fabc06869b",
        "document": "This is in continuation of my previous blogs. Please read to get started with NLP in TensorFlow.\n\nIn this blog, I will mostly focus on generating sequences and padding along with tokenizer.\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\n\n\n\n# Define your input texts\n\nsentences = [\n\n 'I love my dog',\n\n 'I love my cat',\n\n 'You love my dog!',\n\n 'Do you think my dog is amazing?'\n\n]\n\n# Initialize the Tokenizer class\n\ntokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n\n\n\n# Tokenize the input sentences\n\ntokenizer.fit_on_texts(sentences)\n\n\n\n# Get the word index dictionary\n\nword_index = tokenizer.word_index\n\n\n\n# Generate list of token sequences\n\nsequences = tokenizer.texts_to_sequences(sentences)\n\n\n\n# Print the result\n\nprint(\"\n\nWord Index = \" , word_index)\n\nprint(\"\n\nSequences = \" , sequences)\n\n\n\n\n\n\n\nWord Index = {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n\n\n\nSequences = [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n\nTo read more about padding , please refer\n\nIn short, padding can be pre or post depends if we need padding sequence in the beginning or end .\n\nWe can also use max_len along with truncating option to restrict sequence value less than max_len\n\nAlso here one interesting point to note here is we have used OOV token.\n\nThis will be used when you have input words that are not found in the dictionary\n\nFor example, you may decide to collect more text after your initial training and decide to not re-generate the\n\nHere we can see word really was not part of dictionary , so it marked as OOV token."
    },
    {
        "link": "https://thetalkingmachines.com/article/tokenization-and-text-data-preparation-tensorflow-keras",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/72326025/cannot-import-name-pad-sequences-from-keras-preprocessing-sequence",
        "document": "most likely you are using tf version 2.9 - go back to 2.8 and the same path works\n\nTF is not so stable with paths - the best way is check their git source corresponding to the version you succeeded to install !! in the case of TF2.9 you can see how it is imported here"
    },
    {
        "link": "https://saadsohail5104.medium.com/understanding-padding-in-nlp-types-and-when-to-use-them-bacae6cae401",
        "document": "Understanding the Importance of Padding for Consistent Input Lengths in Natural Language Processing\n\nThe Real-World Problem: Why Does Padding Matter in NLP?\n\nImagine you’re tasked with building an email spam filter. The sentences in emails differ vastly in length: some are just a few words, while others are paragraphs long. This variance presents a challenge for machine learning models like LSTMs, which require inputs of the same shape. This is where padding becomes crucial — it ensures that all input sequences have the same length, making the data compatible with the model.\n\nIn NLP (Natural Language Processing), padding is essential for managing sequence lengths, especially when dealing with text data. Without it, machine learning models would struggle to handle varying input sizes, resulting in inefficient learning or even errors during training.\n\nWhat Is Padding in NLP?\n\nPadding in NLP involves adding special tokens (usually zeros) to input sequences to make them uniform in length. This step ensures that every sentence, regardless of how long or short it is, fits the model’s expected input size. Padding comes into play after tokenization, where sentences are broken into sequences of word tokens or subword tokens.\n\nThe simplest way to understand padding is to think of it as leveling the playing field. If one sentence is much shorter than another, padding makes sure that both are the same length by adding extra tokens to the shorter one. This keeps things consistent for the model without altering the meaning of the data.\n\nExample of Padding in Python:\n\nIn this example, ensures both sentences are the same length by adding zeros to the shorter sequence.\n\nNow that we understand the basic concept of padding, let’s dive into the two main types: pre-padding and post-padding.\n• Pre-Padding (Default): Adds zeros to the beginning of a sequence.\n• Post-Padding: Adds zeros to the end of a sequence.\n\nIn this example, you can see how padding at the beginning or end of the sequence affects the data. The choice between pre-padding and post-padding can depend on the specific requirements of your model or task.\n\nWhen padding, it’s also necessary to decide on a maximum sequence length. Should you choose the length of the longest sentence, or should you trim excessively long sentences? This decision can impact both model performance and training time.\n\nHere, we define a of 5, meaning sequences longer than five tokens will be truncated, while shorter sequences will be padded with zeros. Truncation can either happen at the beginning ( ) or the end ( ) of the sequence.\n• Inconsistent Padding Across Datasets: Ensure that padding is applied consistently to both training and testing datasets. Inconsistent padding can result in mismatched input shapes during inference.\n• Padding Too Early: Padding should typically be applied after tokenization and word vectorization. Padding too early in the process might affect how your model interprets and processes the data.\n\nIf you’re working with word embeddings (like Word2Vec or GloVe), padding plays a critical role. The padding token should have its own embedding vector, typically initialized as zeros, so it doesn’t interfere with the learning process.\n\nThe specifies the padded length of the input sequences, ensuring that every input is uniform.\n\nPadding is especially important in recurrent neural networks (RNNs) like LSTMs and GRUs. These models are designed to process sequences of data, and uniform input lengths ensure consistent matrix dimensions during training.\n\nExample: Using Padding in an LSTM Model\n\nIn this example, the embedding layer expects input sequences of length 10. Any sequence shorter than 10 will be padded, and any sequence longer than 10 will be truncated.\n\nWhen to Use Pre-Padding vs. Post-Padding\n• Pre-Padding: Often used when working with LSTMs, as it allows the model to focus on the actual sequence data first.\n• Post-Padding: Sometimes preferred in transformer-based models, where attention mechanisms might perform better with the original order preserved.\n\nPadding may seem like a simple preprocessing step, but it’s fundamental to building robust and efficient NLP models. Whether you’re working on sentiment analysis, machine translation, or text generation, understanding how and when to pad your input sequences can dramatically improve your model’s performance. By choosing the right padding strategy and avoiding common pitfalls, you can ensure that your NLP models are well-equipped to handle the diverse and variable nature of text data."
    },
    {
        "link": "https://codesignal.com/learn/courses/advanced-modeling-for-text-classification/lessons/text-preprocessing-for-deep-learning-with-tensorflow",
        "document": "Welcome, data enthusiasts! In this lesson, we will continue our journey into the world of Natural Language Processing (NLP), with an introduction to deep learning for text classification. To harness the power of deep learning, it's important to start with proper data preparation. That's why we will focus today on text preprocessing, shifting from , which we used previously in this course, to the powerful library. The goal of this lesson is to leverage for textual data preparation and understand how it differs from the methods we used earlier. We will implement tokenization, convert tokens into sequences, learn how to pad these sequences to a consistent length, and transform categorical labels into integer labels to input into our deep learning model. Let's dive in!\n\nUnderstanding TensorFlow and its Role in Text Preprocessing is an open-source library developed by Google, encompassing a comprehensive ecosystem of tools, libraries, and resources that facilitate machine learning and deep learning tasks, including NLP. As with any machine learning task, preprocessing of your data is a key step in NLP as well. A significant difference between text preprocessing with and using libraries like Scikit-learn, lies in the approach to tokenization and sequence generation. incorporates a highly efficient tokenization process, handling both tokenization and sequence generation within the same library. Let's understand how this process works.\n\nTokenization is a foundational step in NLP, where sentences or texts are segmented into individual words or tokens. This process facilitates the comprehension of the language structure and produces meaningful units of text that serve as input for numerous machine learning algorithms. In , we utilize the class for tokenization. A unique feature of TensorFlow's tokenizer is its robust handling of 'out-of-vocabulary' (OOV) words, or words not present in the tokenizer's word index. By specifying the parameter, we can assign a special token, , to represent these OOV words. Let's look at a practical example of tokenization: In this example, examines the text it receives and constructs a vocabulary from the unique words found within. Specifically, for the sentence provided, it generates a word index, where each unique word is assigned a distinct integer value. Importantly, this vocabulary is built exclusively from the text data passed to , ensuring that tokenization aligns precisely with the text's lexical composition. For instance, future texts processed by this tokenizer will be tokenized according to this vocabulary, with any unknown words being represented by the token. Through this mechanism, TensorFlow's Tokenizer effectively prepares text data for subsequent machine learning tasks by mapping words to consistent integer values while gracefully handling words not encountered during the initial vocabulary construction.\n\nAfter tokenization, the next step is to represent text as sequences of integers. Sequences are lists of integers where each integer corresponds to a token in the dictionary created during tokenization. This conversion process translates natural language text into structured data that can be input into a machine learning model. Let's see how we can convert the sentences into sequences to demonstrate the handling of words that are and are not in the vocabulary. Our original sentence \"Love is a powerful entity.\" has been converted into a sequence , and each number directly corresponds to a word in our word index. Looking at the second sequence, , it effectively demonstrates how the handles words that are not part of the initial vocabulary (OOV words) using the specified . In the sequence for the input \"very powerful\", the word “very” is not found in the tokenizer's word index, thus it is labeled as token , which we designated as the token. The word “powerful”, being recognized in the vocabulary, retains its assigned index . This illustrates TensorFlow's capability to manage unknown words gracefully, using the OOV token to ensure continuous processing of text data even when faced with unfamiliar tokens.\n\nDeep learning models require input data of a consistent shape. In the context of NLP, it means all text must be represented by the same number of tokens. Padding is a process to ensure this by adding zeros to shorter sequences to match the length of the longest sequence. Here's how we pad sequences in : In this updated example, after adding the \"very powerful\" sentence to our , we apply padding. The result shows our original sentence \"Love is a powerful entity.\" as , where each number directly corresponds to a word in our word index. For the second sequence, , it illustrates the addition of s at the end to ensure both sequences have the same length. The word “very” in the newer sequence is not found in the tokenizer's word index, thus labeled with the token , and “powerful” retains its assigned index . The padding ensures all sequences are unified in length, catering to the requirements of deep learning models for consistent input shape. These integers represent the word indexes for each token, and sequences are made uniform through padding at the end, demonstrating how TensorFlow's padding function can accommodate data with variable sequence lengths to maintain a consistent input shape across all data inputs."
    },
    {
        "link": "https://tensorflow.org/text/guide/tokenizers",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nTokenization is the process of breaking up a string into tokens. Commonly, these tokens are words, numbers, and/or punctuation. The package provides a number of tokenizers available for preprocessing text required by your text-based models. By performing the tokenization in the TensorFlow graph, you will not need to worry about differences between the training and inference workflows and managing preprocessing scripts.\n\nThis guide discusses the many tokenization options provided by TensorFlow Text, when you might want to use one option over another, and how these tokenizers are called from within your model.\n\nThe main interfaces are and which have single methods and . The variant (which extends ) includes an option for getting byte offsets. This allows the caller to know which bytes in the original string the created token was created from.\n\nThe and are specialized versions of the that provide the convenience methods and respectively.\n\nGenerally, for any N-dimensional input, the returned tokens are in a N+1-dimensional RaggedTensor with the inner-most dimension of tokens mapping to the original individual strings.\n\nThere is also a interface. Any tokenizer implementing this interface can accept a N-dimensional ragged tensor of tokens, and normally returns a N-1-dimensional tensor or ragged tensor that has the given tokens assembled together.\n\nBelow is the suite of tokenizers provided by TensorFlow Text. String inputs are assumed to be UTF-8. Please review the Unicode guide for converting strings to UTF-8.\n\nThese tokenizers attempt to split a string by words, and is the most intuitive way to split text.\n\nThe is the most basic tokenizer which splits strings on ICU defined whitespace characters (eg. space, tab, new line). This is often good for quickly building out prototype models.\n\nYou may notice a shortcome of this tokenizer is that punctuation is included with the word to make up a token. To split the words and punctuation into separate tokens, the should be used.\n\nThe splits strings based on Unicode script boundaries. The script codes used correspond to International Components for Unicode (ICU) UScriptCode values. See: http://icu-project.org/apiref/icu4c/uscript_8h.html\n\nIn practice, this is similar to the with the most apparent difference being that it will split punctuation (USCRIPT_COMMON) from language texts (eg. USCRIPT_LATIN, USCRIPT_CYRILLIC, etc) while also separating language texts from each other. Note that this will also split contraction words into separate tokens.\n\nSubword tokenizers can be used with a smaller vocabulary, and allow the model to have some information about novel words from the subwords that make create it.\n\nWe briefly discuss the Subword tokenization options below, but the Subword Tokenization tutorial goes more in depth and also explains how to generate the vocab files.\n\nWordPiece tokenization is a data-driven tokenization scheme which generates a set of sub-tokens. These sub tokens may correspond to linguistic morphemes, but this is often not the case.\n\nThe WordpieceTokenizer expects the input to already be split into tokens. Because of this prerequisite, you will often want to split using the or beforehand.\n\nAfter the string is split into tokens, the can be used to split into subtokens.\n\nThe BertTokenizer mirrors the original implementation of tokenization from the BERT paper. This is backed by the WordpieceTokenizer, but also performs additional tasks such as normalization and tokenizing to words first.\n\nThe SentencepieceTokenizer is a sub-token tokenizer that is highly configurable. This is backed by the Sentencepiece library. Like the BertTokenizer, it can include normalization and token splitting before splitting into sub-tokens.\n\nThis splits a string into UTF-8 characters. It is useful for CJK languages that do not have spaces between words.\n\nThe output is Unicode codepoints. This can be also useful for creating character ngrams, such as bigrams. To convert back into UTF-8 characters.\n\nThis is a wrapper around models deployed to TF Hub to make the calls easier since TF Hub currently does not support ragged tensors. Having a model perform tokenization is particularly useful for CJK languages when you want to split into words, but do not have spaces to provide a heuristic guide. At this time, we have a single segmentation model for Chinese.\n\nIt may be difficult to view the results of the UTF-8 encoded byte strings. Decode the list values to make viewing easier.\n\nThe & have a targeted purpose of splitting a string based on provided values that indicate where the string should be split. This is useful when building your own segmentation models like the previous Segmentation example.\n\nFor the , a value of 0 is used to indicate the start of a new string, and the value of 1 indicates the character is part of the current string.\n\nThe is similar, but it instead accepts logit value pairs from a neural network that predict if each character should be split into a new string or merged into the current one.\n\nThe is able to segment strings at arbitrary breakpoints defined by a provided regular expression.\n\nWhen tokenizing strings, it is often desired to know where in the original string the token originated from. For this reason, each tokenizer which implements has a tokenize_with_offsets method that will return the byte offsets along with the tokens. The start_offsets lists the bytes in the original string each token starts at, and the end_offsets lists the bytes immediately after the point where each token ends. To refrase, the start offsets are inclusive and the end offsets are exclusive.\n\nTokenizers which implement the provide a method which attempts to combine the strings. This has the chance of being lossy, so the detokenized string may not always match exactly the original, pre-tokenized string.\n\nTF Data is a powerful API for creating an input pipeline for training models. Tokenizers work as expected with the API."
    },
    {
        "link": "https://machinelearningmastery.com/prepare-text-data-deep-learning-keras",
        "document": "You cannot feed raw text directly into deep learning models.\n\nText data must be encoded as numbers to be used as input or output for machine learning and deep learning models.\n\nThe Keras deep learning library provides some basic tools to help you prepare your text data.\n\nIn this tutorial, you will discover how you can use Keras to prepare your text data.\n\nAfter completing this tutorial, you will know:\n• About the convenience methods that you can use to quickly prepare text data.\n• The Tokenizer API that can be fit on training data and used to encode training, validation, and test documents.\n• The range of 4 different document encoding schemes offered by the Tokenizer API.\n\nKick-start your project with my new book Deep Learning for Natural Language Processing, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into 4 parts; they are:\n\nA good first step when working with text is to split it into words.\n\nWords are called tokens and the process of splitting text into tokens is called tokenization.\n\nKeras provides the text_to_word_sequence() function that you can use to split text into a list of words.\n\nBy default, this function automatically does 3 things:\n\nYou can change any of these defaults by passing arguments to the function.\n\nBelow is an example of using the text_to_word_sequence() function to split a document (in this case a simple string) into a list of words.\n\nRunning the example creates an array containing all of the words in the document. The list of words is printed for review.\n\nThis is a good first step, but further pre-processing is required before you can work with the text.\n\nIt is popular to represent a document as a sequence of integer values, where each word in the document is represented as a unique integer.\n\nKeras provides the one_hot() function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one-hot encoding of the document, which is not the case.\n\nInstead, the function is a wrapper for the hashing_trick() function described in the next section. The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values.\n\nAs with the text_to_word_sequence() function in the previous section, the one_hot() function will make the text lower case, filter out punctuation, and split words based on white space.\n\nIn addition to the text, the vocabulary size (total words) must be specified. This could be the total number of words in the document or more if you intend to encode additional documents that contains additional words. The size of the vocabulary defines the hashing space from which words are hashed. Ideally, this should be larger than the vocabulary by some percentage (perhaps 25%) to minimize the number of collisions. By default, the ‘hash’ function is used, although as we will see in the next section, alternate hash functions can be specified when calling the hashing_trick() function directly.\n\nWe can use the text_to_word_sequence() function from the previous section to split the document into words and then use a set to represent only the unique words in the document. The size of this set can be used to estimate the size of the vocabulary for one document.\n\nWe can put this together with the one_hot() function and one hot encode the words in the document. The complete example is listed below.\n\nThe vocabulary size is increased by one-third to minimize collisions when hashing words.\n\nRunning the example first prints the size of the vocabulary as 8. The encoded document is then printed as an array of integer encoded words.\n\nA limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers.\n\nAn alternative to this approach is to use a one-way hash function to convert words to integers. This avoids the need to keep track of a vocabulary, which is faster and requires less memory.\n\nKeras provides the hashing_trick() function that tokenizes and then integer encodes the document, just like the one_hot() function. It provides more flexibility, allowing you to specify the hash function as either ‘hash’ (the default) or other hash functions such as the built in md5 function or your own function.\n\nBelow is an example of integer encoding a document using the md5 hash function.\n\nRunning the example prints the size of the vocabulary and the integer encoded document.\n\nWe can see that the use of a different hash function results in consistent, but different integers for words as the one_hot() function in the previous section.\n\nSo far we have looked at one-off convenience methods for preparing text with Keras.\n\nKeras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents. This may be the preferred approach for large projects.\n\nKeras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents.\n\nOnce fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n• word_counts: A dictionary of words and their counts.\n• word_docs: A dictionary of words and how many documents each appeared in.\n• word_index: A dictionary of words and their uniquely assigned integers.\n• document_count:An integer count of the total number of documents that were used to fit the Tokenizer.\n\nOnce the Tokenizer has been fit on training data, it can be used to encode documents in the train or test datasets.\n\nThe texts_to_matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary.\n\nThis function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function.\n• ‘binary‘: Whether or not each word is present in the document. This is the default.\n• ‘count‘: The count of each word in the document.\n• ‘tfidf‘: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n• ‘freq‘: The frequency of each word as a ratio of words within each document.\n\nWe can put all of this together with a worked example.\n\nRunning the example fits the Tokenizer with 5 small documents. The details of the fit Tokenizer are printed. Then the 5 documents are encoded using a word count.\n\nEach document is encoded as a 9-element vector with one position for each word and the chosen encoding scheme value for each word position. In this case, a simple word count mode is used.\n\nThis section provides more resources on the topic if you are looking go deeper.\n\nIn this tutorial, you discovered how you can use the Keras API to prepare your text data for deep learning.\n• About the convenience methods that you can use to quickly prepare text data.\n• The Tokenizer API that can be fit on training data and used to encode training, validation, and test documents.\n• The range of 4 different document encoding schemes offered by the Tokenizer API.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://stackoverflow.com/questions/62232477/how-to-encode-text-for-an-nlp-in-tensorflow",
        "document": "You can use the gensim library effective for text embedding without losing its meaning and sequence. An example code below. Feel free to make changes according to your needs. Choose algorithms, optimizers etc. as per your requirement."
    },
    {
        "link": "https://thetalkingmachines.com/article/tokenization-and-text-data-preparation-tensorflow-keras",
        "document": ""
    },
    {
        "link": "https://kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html",
        "document": "This article will look at tokenizing and further preparing text data for feeding into a neural network using TensorFlow and Keras preprocessing tools.\n\nIn the past we have had a look at a general approach to preprocessing text data, which focused on tokenization, normalization, and noise removal. We then followed that up with an overview of text data preprocessing using Python for NLP projects, which is essentially a practical implementation of the framework outlined in the former article, and which encompasses a mainly manual approach to text data preprocessing. We have also had a look at what goes into building an elementary text data vocabulary using Python.\n\nThere are numerous tools available for automating much of this preprocessing and text data preparation, however. These tools existed prior to the publication of those articles for certain, but there has been an explosion in their proliferation since. Since much NLP work is now accomplished using neural networks, it makes sense that neural network implementation libraries such as TensorFlow — and also, yet simultaneously, Keras — would include methods for achieving these preparation tasks.\n\nThis article will look at tokenizing and further preparing text data for feeding into a neural network using TensorFlow and Keras preprocessing tools. While the additional concept of creating and padding sequences of encoded data for neural network consumption were not treated in these previous articles, it will be added herein. Conversely, while noise removal was covered in the previous articles, it will not be here. What constitutes noise in text data can be a task-specific undertaking, and the previous treatment of this topic is still relevant as it is.\n\nFor what we will accomplish today, we will make use of 2 Keras preprocessing tools: the class, and the module.\n\nInstead of using a real dataset, either a TensorFlow inclusion or something from the real world, we use a few toy sentences as stand-ins while we get the coding down. Next time we can extend our code to both use a real dataset and perform some interesting tasks, such as classification or something similar. Once this process is understood, extending it to larger datasets is trivial.\n\nLet's start with the necessary imports and some \"data\" for demonstration.\n\nNext, some hyperparameters for performing tokenization and preparing the standardized data representation, with explanations below.\n• This will be the maximum number of words from our resulting tokenized data vocabulary which are to be used, truncated after the 1000 most common words in our case. This will not be an issue in our small dataset, but is being shown for demonstration purposes.\n• This is the token which will be used for out of vocabulary tokens encountered during the tokenizing and encoding of test data sequences, created using the word index built during tokenization of our training data.\n• When we are encoding our numeric sequence representations of the text data, our sentences (or arbitrary text chunk) lengths will not be uniform, and so we will need to select a maximum length for sentences and pad unused sentence positions in shorter sentences with a padding character. In our case, our maximum sentence length will be determined by searching our sentences for the one of maximum length, and padding characters will be '0'.\n• As in the above, when we are encoding our numeric sequence representations of the text data, our sentences (or arbitrary text chunk) lengths will not be uniform, and so we will need to select a maximum length for sentences and pad unused sentence positions in shorter sentences with a padding character. Whether we pre-pad or post-pad sentences is our decision to make, and we have selected 'post', meaning that our sentence sequence numeric representations corresponding to word index entries will appear at the left-most positions of our resulting sentence vectors, while the padding characters ('0') will appear after our actual data at the right-most positions of our resulting sentence vectors.\n\nNow let's perform the tokenization, sequence encoding, and sequence padding. We will walk through this code chunk by chunk below.\n• This is straightforward; we are using the TensorFlow (Keras) class to automate the tokenization of our training data. First we create the object, providing the maximum number of words to keep in our vocabulary after tokenization, as well as an out of vocabulary token to use for encoding test data words we have not come across in our training, without which these previously-unseen words would simply be dropped from our vocabulary and mysteriously unaccounted for. To learn more about other arguments for the TensorFlow tokenizer, check out the documentation. After the has been created, we then fit it on the training data (we will use it later to fit the testing data as well).\n• A byproduct of the tokenization process is the creation of a word index, which maps words in our vocabulary to their numeric representation, a mapping which will be essential for encoding our sequences. Since we will reference this later to print out, we assign it a variable here to simplify a bit.\n• Now that we have tokenized our data and have a word to numeric representation mapping of our vocabulary, let's use it to encode our sequences. Here, we are converting our text sentences from something like \"My name is Matthew,\" to something like \"6 8 2 19,\" where each of those numbers match up in the index to the corresponding words. Since neural networks work by performing computation on numbers, passing in a bunch of words won't work. Hence, sequences. And remember that this is only the training data we are working on right now; testing data is necessarily tokenized and encoded afterwards, below.\n• Remember when we said we needed to have a maximum sequence length for padding our encoded sentences? We could set this limit ourselves, but in our case we will simply find the longest encoded sequence and use that as our maximum sequence length. There would certainly be reasons you would not want to do this in practice, but there would also be times it would be appropriate. The variable is then used below in the actual training sequence padding.\n• As mentioned above, we need our encoded sequences to be of the same length. We just found out the length of the longest sequence, and will use that to pad all other sequences with extra '0's at the end ('post') and will also truncate any sequences longer than maximum length from the end ('post') as well. Here we use the TensorFlow (Keras) module to accomplish this. You can look at the documentation for additional padding options.\n• None # Output the results of our work \n\n Now let's see what we've done. We would expect to note the longest sequence and the padding of those which are shorter. Also note that when padded, our sequences are converted from Python lists to Numpy arrays, which is helpful since that is what we will ultimately feed into our neural network. The shape of our training sequences matrix is the number of sentences (sequences) in our training set (4) by the length of our longest sequence ( , or 12).\n\nNow let's use our tokenizer to tokenize the test data, and then similarly encode our sequences. This is all quite similar to the above. Note that we are using the same tokenizer we created for training in order to facilitate simpatico between the 2 datasets, using the same vocabulary. We also pad to the same length and specifications as the training sequences.\n\nCan you see, for instance, how having different lengths of padded sequences between training and testing sets would cause a problem?\n\nNote that, since we are encoding some words in the test data which were not seen in the training data, we now have some out of vocabulary tokens which we encoded as <UNK> (specifically 'want', for example).\n\nNow that we have padded sequences, and more importantly know how to get them again with different data, we are ready to do something with them. Next time, we will replace the toy data we were using this time with actual data, and with very little change to our code (save the possible necessity of classification labels for our train and test data), we will move forward with an NLP task of some sort, most likely classification.\n• How to Create a Vocabulary for NLP Tasks in Python"
    }
]