[
    {
        "link": "https://aws.amazon.com/blogs/security/security-at-multiple-layers-for-web-administered-apps",
        "document": "In this post, I will show you how to apply security at multiple layers of a web application hosted on AWS.\n\nApply security at all layers is a design principle of the Security pillar of the AWS Well-Architected Framework. It encourages you to apply security at the network edge, virtual private cloud (VPC), load balancer, compute instance (or service), operating system, application, and code.\n\nMany popular web apps are designed with a single layer of security: the login page. Behind that login page is an in-built administration interface that is directly exposed to the internet. Admin interfaces for these apps typically have simple login mechanisms and often lack multi-factor authentication (MFA) support, which can make them an attractive target for threat actors.\n\nThe in-built admin interface can also be problematic if you want to horizontally scale across multiple servers. The admin interface is available on every server that runs the app, so it creates a large attack surface. Because the admin interface updates the software on its own server, you must synchronize updates across a fleet of instances.\n\nMulti-layered security is about identifying (or creating) isolation boundaries around the parts of your architecture and minimizing what is permitted to cross each boundary. Adding more layers to your architecture gives you the opportunity to introduce additional controls at each layer, creating more boundaries where security controls can be enforced.\n\nIn the example app scenario in this post, you have the opportunity to add many additional layers of security.\n\nThis post demonstrates how you can use the Run Web-Administered Apps on AWS sample project to help address these challenges, by implementing a horizontally-scalable architecture with multi-layered security. The project builds and configures many different AWS services, each designed to help provide security at different layers.\n\nBy running this solution, you can produce a segmented architecture that separates the two functions of these apps into an unprivileged public-facing view and an admin view. This design limits access to the web app’s admin functions while creating a fleet of unprivileged instances to serve the app at scale.\n\nFigure 1 summarizes how the different services in this solution work to help provide security at the following layers:\n\nThe following diagram shows the solution architecture deployed by Run Web-Administered Apps on AWS. The figure shows how the services deployed in this solution are deployed in different AWS Regions, and how requests flow from the application user through the different service layers.\n\nThis post will dive deeper into each of the architecture’s layers to see how security is added at each layer. But before we talk about the technology, let’s consider how infrastructure is built and managed — by people.\n\nSecurity starts with the people in your team and your organization’s operational practices. How your “people layer” builds and manages your infrastructure contributes significantly to your security posture.\n\nA design principle of the Security pillar of the Well-Architected Framework is to automate security best practices. This helps in two ways: it reduces the effort required by people over time, and it helps prevent resources from being in inconsistent or misconfigured states. When people use manual processes to complete tasks, misconfigurations and missed steps are common.\n\nThe simplest way to automate security while reducing human effort is to adopt services that AWS manages for you, such as Amazon Relational Database Service (Amazon RDS). With Amazon RDS, AWS is responsible for the operating system and database software patching, and provides tools to make it simple for you to back up and restore your data.\n\nYou can automate and integrate key security functions by using managed AWS security services, such as Amazon GuardDuty, AWS Config, Amazon Inspector, and AWS Security Hub. These services provide network monitoring, configuration management, and detection of software vulnerabilities and unintended network exposure. As your cloud environments grow in scale and complexity, automated security monitoring is critical.\n\nInfrastructure as code (IaC) is a best practice that you can follow to automate the creation of infrastructure. By using IaC to define, configure, and deploy the AWS resources that you use, you reduce the likelihood of human error when building AWS infrastructure.\n\nAdopting IaC can help you improve your security posture because it applies the rigor of application code development to infrastructure provisioning. Storing your infrastructure definition in a source control system (such as AWS CodeCommit) creates an auditable artifact. With version control, you can track changes made to it over time as your architecture evolves.\n\nYou can add automated testing to your IaC project to help ensure that your infrastructure is aligned with your organization’s security policies. If you ever need to recover from a disaster, you can redeploy the entire architecture from your IaC project.\n\nAnother people-layer discipline is to apply the principle of least privilege. AWS Identity and Access Management (IAM) is a flexible and fine-grained permissions system that you can use to grant the smallest set of actions that your solution needs. You can use IAM to control access for both humans and machines, and we use it in this project to grant the compute instances the least privileges required.\n\nYou can also adopt other IAM best practices such as using temporary credentials instead of long-lived ones (such as access keys), and regularly reviewing and removing unused users, roles, permissions, policies, and credentials.\n\nThe internet is public and therefore untrusted, so you must proactively address the risks from threat actors and network-level attacks.\n\nTo reduce the risk of distributed denial of service (DDoS) attacks, this solution uses AWS Shield for managed protection at the network edge. AWS Shield Standard is automatically enabled for all AWS customers at no additional cost and is designed to provide protection from common network and transport layer DDoS attacks. For higher levels of protection against attacks that target your applications, subscribe to AWS Shield Advanced.\n\nAmazon Route 53 resolves the hostnames that the solution uses and maps the hostnames as aliases to an Amazon CloudFront distribution. Route 53 is a robust and highly available globally distributed DNS service that inspects requests to protect against DNS-specific attack types, such as DNS amplification attacks.\n\nCloudFront also operates at the AWS network edge and caches, transforms, and forwards inbound requests to the relevant origin services across the low-latency AWS global network. The risk of DDoS attempts overwhelming your application servers is further reduced by caching web requests in CloudFront.\n\nThe solution configures CloudFront to add a shared secret to the origin request within a custom header. A CloudFront function copies the originating user’s IP to another custom header. These headers get checked when the request arrives at the load balancer.\n\nAWS WAF, a web application firewall, blocks known bad traffic, including cross-site scripting (XSS) and SQL injection events that come into CloudFront. This project uses AWS Managed Rules, but you can add your own rules, as well. To restrict frontend access to permitted IP CIDR blocks, this project configures an IP restriction rule on the web application firewall.\n\nAfter CloudFront and AWS WAF check the request, CloudFront forwards it to the compute services inside an Amazon Virtual Private Cloud (Amazon VPC). VPCs are logically isolated networks within your AWS account that you can use to control the network traffic that is allowed in and out. This project configures its VPC to use a private IPv4 CIDR block that cannot be directly routed to or from the internet, creating a network perimeter around your resources on AWS.\n\nThe Amazon Elastic Compute Cloud (Amazon EC2) instances are hosted in private subnets within the VPC that have no inbound route from the internet. Using a NAT gateway, instances can make necessary outbound requests. This design hosts the database instances in isolated subnets that don’t have inbound or outbound internet access. Amazon RDS is a managed service, so AWS manages patching of the server and database software.\n\nThe solution accesses AWS Secrets Manager by using an interface VPC endpoint. VPC endpoints use AWS PrivateLink to connect your VPC to AWS services as if they were in your VPC. In this way, resources in the VPC can communicate with Secrets Manager without traversing the internet.\n\nThe project configures VPC Flow Logs as part of the VPC setup. VPC flow logs capture information about the IP traffic going to and from network interfaces in your VPC. GuardDuty analyzes these logs and uses threat intelligence data to identify unexpected, potentially unauthorized, and malicious activity within your AWS environment.\n\nAlthough using VPCs and subnets to segment parts of your application is a common strategy, there are other ways that you can achieve partitioning for application components:\n• You can use separate VPCs to restrict access to a database, and use VPC peering to route traffic between them.\n• You can use a multi-account strategy so that different security and compliance controls are applied in different accounts to create strong logical boundaries between parts of a system. You can route network requests between accounts by using services such as AWS Transit Gateway, and control them using AWS Network Firewall.\n\nThere are always trade-offs between complexity, convenience, and security, so the right level of isolation between components depends on your requirements.\n\nAfter the request is sent to the VPC, an Application Load Balancer (ALB) processes it. The ALB distributes requests to the underlying EC2 instances. The ALB uses TLS version 1.2 to encrypt incoming connections with an AWS Certificate Manager (ACM) certificate.\n\nPublic access to the load balancer isn’t allowed. A security group applied to the ALB only allows inbound traffic on port 443 from the CloudFront IP range. This is achieved by specifying the Region-specific AWS-managed CloudFront prefix list as the source in the security group rule.\n\nThe ALB uses rules to decide whether to forward the request to the target instances or reject the traffic. As an additional layer of security, it uses the custom headers that the CloudFront distribution added to make sure that the request is from CloudFront. In another rule, the ALB uses the originating user’s IP to decide which target group of Amazon EC2 instances should handle the request. In this way, you can direct admin users to instances that are configured to allow admin tasks.\n\nIf a request doesn’t match a valid rule, the ALB returns a 404 response to the user.\n\nA security group creates an isolation boundary around the EC2 instances. The only traffic that reaches the instance is the traffic that the security group rules allow. In this solution, only the ALB is allowed to make inbound connections to the EC2 instances.\n\nA common practice is for customers to also open ports, or to set up and manage bastion hosts to provide remote access to their compute instances. The risk in this approach is that the ports could be left open to the whole internet, exposing the instances to vulnerabilities in the remote access protocol. With remote work on the rise, there is an increased risk for the creation of these overly permissive inbound rules.\n\nUsing AWS Systems Manager Session Manager, you can remove the need for bastion hosts or open ports by creating secure temporary connections to your EC2 instances using the installed SSM agent. As with every software package that you install, you should check that the SSM agent aligns with your security and compliance requirements. To review the source code to the SSM agent, see amazon-ssm-agent GitHub repo.\n\nThe compute layer of this solution consists of two separate Amazon EC2 Auto Scaling groups of EC2 instances. One group handles requests from administrators, while the other handles requests from unprivileged users. This creates another isolation boundary by keeping the functions separate while also helping to protect the system from a failure in one component causing the whole system to fail. Each Amazon EC2 Auto Scaling group spans multiple Availability Zones (AZs), providing resilience in the event of an outage in an AZ.\n\nBy using managed database services, you can reduce the risk that database server instances haven’t been proactively patched for security updates. Managed infrastructure helps reduce the risk of security issues that result from the underlying operating system not receiving security patches in a timely manner and the risk of downtime from hardware failures.\n\nWhen instances are first launched, the operating system must be secure, and the instances must be updated as required when new security patches are released. We recommend that you create immutable servers that you build and harden by using a tool such as EC2 Image Builder. Instead of patching running instances in place, replace them when an updated Amazon Machine Image (AMI) is created. This approach works in our example scenario because the application code (which changes over time) is stored on Amazon Elastic File System (Amazon EFS), so when you replace the instances with a new AMI, you don’t need to update them with data that has changed after the initial deployment.\n\nAnother way that the solution helps improve security on your instances at the operating system is to use EC2 instance profiles to allow them to assume IAM roles. IAM roles grant temporary credentials to applications running on EC2, instead of using hard-coded credentials stored on the instance. Access to other AWS resources is provided using these temporary credentials.\n\nThe IAM roles have least privilege policies attached that grant permission to mount the EFS file system and access AWS Systems Manager. If a database secret exists in Secrets Manager, the IAM role is granted permission to access it.\n\nBoth Amazon EC2 Auto Scaling groups of EC2 instances share access to Amazon EFS, which hosts the files that the application uses. IAM authorization applies IAM file system policies to control the instance’s access to the file system. This creates another isolation boundary that helps prevent the non-admin instances from modifying the application’s files.\n\nThe admin group’s instances have the file system mounted in read-write mode. This is necessary so that the application can update itself, install add-ons, upload content, or make configuration changes. On the unprivileged instances, the file system is mounted in read-only mode. This means that these instances can’t make changes to the application code or configuration files.\n\nThe unprivileged instances have local file caching enabled. This caches files from the EFS file system on the local Amazon Elastic Block Store (Amazon EBS) volume to help improve scalability and performance.\n\nThis solution applies different web server configurations to the instances running in each Amazon EC2 Auto Scaling group. This creates a further isolation boundary at the web server layer.\n\nThe admin instances use the default configuration for the application that permits access to the admin interface. Non-admin, public-facing instances block admin routes, such as wp-login.php, and will return a 403 Forbidden response. This creates an additional layer of protection for those routes.\n\nThe database layer is within two additional isolation boundaries. The solution uses Amazon RDS, with database instances deployed in isolated subnets. Isolated subnets have no inbound or outbound internet access and can only be reached through other network interfaces within the VPC. The RDS security group further isolates the database instances by only allowing inbound traffic from the EC2 instances on the database server port.\n\nBy using IAM authentication for the database access, you can add an additional layer of security by configuring the non-admin instances with less privileged database user credentials.\n\nTo apply security at the application code level, you should establish good practices around installing updates as they become available. Most applications have email lists that you can subscribe to that will notify you when updates become available.\n\nYou should evaluate the quality of an application before you adopt it. The following are some metrics to consider:\n• Number of developers who are actively working on it\n• Frequency of updates to it\n• How quickly the developers respond with patches when bugs are reported\n\nOther steps that you can take\n\nUse AWS Verified Access to help secure application access for human users. With Verified Access, you can add another user authentication stage, to help ensure that only verified users can access an application’s administrative functions.\n\nAmazon GuardDuty is a threat detection service that continuously monitors your AWS accounts and workloads for malicious activity and delivers detailed security findings for visibility and remediation. It can detect communication with known malicious domains and IP addresses and identify anomalous behavior. GuardDuty Malware Protection helps you detect the potential presence of malware by scanning the EBS volumes that are attached to your EC2 instances.\n\nAmazon Inspector is an automated vulnerability management service that automatically discovers the Amazon EC2 instances that are running and scans them for software vulnerabilities and unintended network exposure. To help ensure that your web server instances are updated when security patches are available, use AWS Systems Manager Patch Manager.\n\nWe wrote the Run Web-Administered Apps on AWS project by using the AWS Cloud Development Kit (AWS CDK). With the AWS CDK, you can use the expressive power of familiar programming languages to define your application resources and accelerate development. The AWS CDK has support for multiple languages, including TypeScript, Python, .NET, Java, and Go.\n\nThis project uses Python. To deploy it, you need to have a working version of Python 3 on your computer. For instructions on how to install the AWS CDK, see Get Started with AWS CDK.\n\nTo enable this project to deploy multiple different web projects, you must do the configuration in the parameters.properties file. Two variables identify the configuration blocks: app (which identifies the web application to deploy) and env (which identifies whether the deployment is to a dev or test environment, or to production).\n\nWhen you deploy the stacks, you specify the app and env variables as CDK context variables so that you can select between different configurations at deploy time. If you don’t specify a context, a [default] stanza in the parameters.properties file specifies the default app name and environment that will be deployed.\n\nTo name other stanzas, combine valid app and env values by using the format <app>-<env>. For each stanza, you can specify its own Regions, accounts, instance types, instance counts, hostnames, and more. For example, if you want to support three different WordPress deployments, you might specify the app name as wp, and for env, you might want dev, test, and prod, giving you three stanzas: wp-dev, wp-test, and wp-prod.\n\nThe project includes sample configuration items that are annotated with comments that explain their function.\n\nBefore you can use the AWS CDK to deploy stacks into your account, you need to use CDK bootstrapping to provision resources in each AWS environment (account and Region combination) that you plan to use. For this project, you need to bootstrap both the US East (N. Virginia) Region (us-east-1) and the home Region in which you plan to host your application.\n\nYou need to have a hosted zone in Route 53 to allow the creation of DNS records and certificates. You must manually create the hosted zone by using the AWS Management Console. You can delegate a domain that you control to Route 53 and use it with this project. You can also register a domain through Route 53 if you don’t currently have one.\n\nClone the project to your local machine and navigate to the project root. To create the Python virtual environment (venv) and install the dependencies, follow the steps in the Generic CDK instructions.\n\nTo create and configure the parameters.properties file\n\nCopy the parameters-template.properties file (in the root folder of the project) to a file called parameters.properties and save it in the root folder. Open it with a text editor and then do the following:\n• Replace with the name of the hosted zone that you created in the previous step.\n• Replace with your admin computer’s IP address (usually the public IP of the computer that you’re using now).\n\nIf you want to restrict public access to your site, change 192.0.2.0/24 to the IP range that you want to allow. By providing a comma-separated list of allowedIps, you can add multiple allowed CIDR blocks.\n\nIf you don’t want to restrict public access, set allowedIps=* instead.\n\nIf you have forked this project into your own private repository, you can commit the parameters.properties file to your repo. To do that, comment out the parameters.properties line in the .gitignore file.\n\nThe solution uses an AWS CloudFormation custom resource for cross-Region configuration management. To install the needed Python package, run the following command in the custom_resource directory:\n\nTo learn more about CloudFormation custom resource creation, see AWS CloudFormation custom resource creation with Python, AWS Lambda, and crhelper.\n\nBefore you deploy the stacks, decide whether you want to include a data layer as part of the deployment. The dbConfig parameter determines what will happen, as follows:\n• If is left empty — no database will be created and no database credentials will be available in your compute stacks\n• If is set to — you will get a new Amazon RDS instance\n• If is set to — you will get an Amazon Aurora cluster\n• If is set to — if you previously created a database in this stack, the database will be deleted\n\nIf you specify either instance or cluster, you should also configure the following database parameters to match your requirements:\n• — set the database engine to either or\n• — specify the named snapshot for your database\n• — if you are using an existing database, specify the Amazon Resource Name (ARN) of the secret where the database credentials and DNS endpoint are located\n• — set the major version of the engine that you have chosen; leave blank to get the default version\n• — set the minor version of the engine that you have chosen; leave blank to get the default version\n• — set the instance type that you want (note that these vary by service); don’t prefix with db. because the CDK will automatically prepend it\n• — if you request a cluster, set this parameter to determine how many Amazon Aurora replicas are created\n\nYou can choose between mysql or postgres for the database engine. Other settings that you can choose are determined by that choice.\n\nYou will need to use an Amazon Machine Image (AMI) that has the CLI preinstalled, such as Amazon Linux 2, or install the AWS Command Line Interface (AWS CLI) yourself with a user data command. If instead of creating a new, empty database, you want to create one from a snapshot, supply the snapshot name by using the dbSnapshot parameter.\n\nAWS automatically creates and stores the RDS instance or Aurora cluster credentials in a Secrets Manager secret when you create a new instance or cluster. You make these credentials available to the compute stack through the db_secret_command variable, which contains a single-line bash command that returns the JSON from the AWS CLI command aws secretsmanager get-secret-value. You can interpolate this variable into your user data commands as follows:\n\nIf you create a database from a snapshot, make sure that your Secrets Manager secret and Amazon RDS snapshot are in the target Region. If you supply the secret for an existing database, make sure that the secret contains at least the following four key-value pairs (replace the <placeholder values> with your values):\n\nThe name for the secret must match the app value followed by the env value (both in title case), followed by DatabaseSecret, so for app=wp and env=dev, your secret name should be WpDevDatabaseSecret.\n\nThe following commands deploy the stacks defined in the CDK app. To deploy them individually, use the specific stack names (these will vary according to the info that you supplied previously), as shown in the following.\n\nTo create a database stack, deploy the network and database stacks first.\n\nYou can then initiate the deployment of the compute stack.\n\nAfter the compute stack deploys, you can deploy the stack that creates the CloudFront distribution.\n\nThis deploys the CloudFront infrastructure to the US East (N. Virginia) Region (us-east-1). CloudFront is a global AWS service, which means that you must create it in this Region. The other stacks are deployed to the Region that you specified in your configuration stanza.\n\nIf your stacks deploy successfully, your site appears at one of the following URLs:\n• (if you specified a value for the subdomain) — for example,\n• (if you didn’t specify a value for the subdomain) — for example, .\n\nIf you connect through the IP address that you configured in the adminIps configuration, you should be connected to the admin instance for your site. Because the admin instance can modify the file system, you should use it to do your administrative tasks.\n\nUsers who connect to your site from an IP that isn’t in your allowedIps list will be connected to your fleet instances and won’t be able to alter the file system (for example, they won’t be able to install plugins or upload media).\n\nIf you need to redeploy the same app-env combination, manually remove the parameter store items and the replicated secret that you created in us-east-1. You should also delete the cdk.context.json file because it caches values that you will be replacing.\n\nYou can modify the configuration file in this project to deploy different applications to different environments using the same project. Each app can have different configurations for dev, test, or production environments.\n\nUsing this mechanism, you can deploy sites for test and production into different accounts or even different Regions. The solution uses CDK context variables as command-line switches to select different configuration stanzas from the configuration file.\n\nCDK projects allow for multiple deployments to coexist in one account by using unique names for the deployed stacks, based on their configuration.\n\nCheck the configuration file into your source control repo so that you track changes made to it over time.\n\nGot a different web app that you want to deploy? Create a new configuration by copying and pasting one of the examples and then modify the build commands as needed for your use case.\n\nIn this post, you learned how to build an architecture on AWS that implements multi-layered security. You can use different AWS services to provide protections to your application at different stages of the request lifecycle.\n\nYou can learn more about the services used in this sample project by building it in your own account. It’s a great way to explore how the different services work and the full features that are available. By understanding how these AWS services work, you will be ready to use them to add security, at multiple layers, in your own architectures.\n\n\n\nIf you have feedback about this post, submit comments in the Comments section below. If you have questions about this post, contact AWS Support.\n\nWant more AWS Security news? Follow us on Twitter."
    },
    {
        "link": "https://medium.com/java-vault/layered-architecture-b2f4ebe8d587",
        "document": "When you develop a web application do you ever wonder about the architecture of the solution? Which architecture do you usually use? Why that one especially? What are its pros and cons and is it a good architecture for the problem your application is trying to solve?\n\nSince my first web application projects on college until this very day I almost always worked on a 3-Layered Architecture. Sometimes I had my vote on it (I didn’t know about any other architecture), sometimes it was already decided for me by a software architect. But I never stopped to wonder why use 3-layered for this specific case? is there a better architecture for this problem? how did the architect decide to use 3-layered architecture specifically?\n\nWe can now take a closer look at the layered architecture and answer those questions for ourselves.\n\nWhat exactly are layers?\n\nLayers in a Layered architecture are a collection of modules (classes in Java world, if you will) with common functionality.\n\nIn a layered architecture, layers are stacked on top of one of another:\n\nComponentes from one layer are only allowed to communicate with the components from the layer one level below:\n\nThe most common type of Layered Architecture is a 3-Layered Architecture.\n\nPresentation Layer is our connection to the outside world. This is where we handle all the incoming requests to our application and return a response.\n\nThis layer is also the first line of defense in our application because this is where we do authorization checks.\n\nOne layer below Presentation Layer is Application Layer.\n\nPresentation Layer relies upon Application Layer to do all the functions the system should provide. Thus Presentation Layer has dependency only on the Application Layer.\n\nApplication Layer is where we develop all the functions our application should provide. So in terms of a Web Shop, this is where we implement functions for ...\n\nThis is also where we do all our validations. For example, before adding an article to the customer's basket we can check that the customer has enough money to pay for the article.\n\nApplication Layer relies upon Data Layer to save all the data for later use or fetch some previously-saved data. Thus Application Layer has dependency only on Data Layer.\n\nApplication Layer returns the result of its calculations back to the Presentation Layer.\n\nData Layer handles the persisting of our data. It communicates with the Database and has no further dependencies.\n\nData Layer returns its data back to the Application Layer.\n\nIf you are familiar with Spring Framework, chances are, you’ve used 3-Layered architecture.\n\nIn Spring applications our Controller classes are in a Presentation Layer. Our Service classes are in an Application Layer. Our Repository classes are in a Data Layer.\n\nAnd between those layers, we have various objects used for communication:\n• Data Transfer Object (DTO) — is sent by a client from outside of our application. DTO is received in the Presentation Layer (Spring Controller class). Presentation Layer sends DTO objects to the Application Layer (Spring Service class). Application Layer returns DTO objects as a result back to Presentation Layer\n• Domain Object — the object that the Application Layer works with. It can be created somehow out of DTO or Entity Object, or both, there are actually no rules here\n• Entity Object — the object that Presentation Layer works with. Application Layer sends Entity Objects to Data Layer and also Data Layer sends Entity Objects to the Application Layer. Data fetched from the database will be mapped to an Entity Object. The object we want to persist in the database has to be mapped to an Entity Object first.\n\nHere, in the diagram, we can see how the Layers communicate using DTO, Domain and Entity objects:\n\nWe can see that in 3-Layered Architecture, the database is the most important component. All other components depend on the database directly or indirectly.\n\nThis is in strong contrast with Domain-Centric Design which I will cover later in another blog post.\n\nNow we can write up a code example to demonstrate all the things we covered so far.\n\nFirst, let's start with our POJOs: Data Transfer Objects, Domain Objects, Entity Objects.\n\nNext, we can add our Data Layer that will be our connection to the database. I will use from the :\n\nIn the Application Layer, I created a simple mapper class. I use the mapper class to transform Entity model to DTO. It's a good practice to never return Entity objects to the Presentation Layer, and especially not to the client.\n\nIn the Application Layer I also created CustomerService class. This is where we add all our functionalities concerning the Customers of our application.\n\nIn the Presentation Layer we got our Controller classes. This is where we define our REST routes.\n\nWhat are the pros and cons of this type of architecture. Why did it become so popular and so widely used?\n• Layers are isolated — changes to one layer don’t affect the other layers\n• Separation of concerns — each layer handles one aspect of our application and that makes our code more manageable\n• Defacto standard — it's well known by developers so everybody can easily find its way through the codebase\n• A quick way to get the application running without much complexity\n• might be too much for simple CRUD — going through 3 layers only to create one database record\n• Layers all make a single application. Changes to one layer require redeployment of the whole app\n• Every Layer has a direct dependency on a layer below. This makes a tight coupling. It is not easy to swap out a layer with another one.\n\nAnother con I notice while working with a layered architecture: I would like to have code reuse within the same layer. But it's not allowed for a component to call other components in the same layer. So how do we handle a situation like this? I personally allow a component to call other components from the same layer, but this could lead to dependency hell.\n\nThis sums up my tutorial about the Layered Software Architecture. Please feel free to share your thoughts in the comment section. Until next time … 🍻"
    },
    {
        "link": "https://stackoverflow.com/questions/23445298/design-of-layered-architecture-for-a-java-application",
        "document": "I have code with the following architecture:\n• DAO (reads /write BO to DB, converts BO to DBModels and vice versa)\n• I plan to have a manager layer on top of DAO.Manager will call DAOS.Manager handles business logic.And transactions\n\nConsider that I have 3 tables: A,B,C\n\nAll write operations of BO are handles by respective Manager (e.g. To write into A_BO, manager A is always called).\n\nFor some operations, I need to access multiple tables/BO.\n\nFor example to insert a record into A, I need to check something into table B. Write of A is handled by A managed.\n\nCan manager A call B_DAO? or should it only call B_Manager? and not access B_DAO?\n\nIf manager calls some other manager I cannot put an @Transaction Annotation on manager and I will need one more layers on top of the Manager then."
    },
    {
        "link": "https://oreilly.com/library/view/software-architecture-patterns/9781491971437/ch01.html",
        "document": "Notice in Figure 1-2 that each of the layers in the architecture is marked as being closed. This is a very important concept in the layered architecture pattern. A closed layer means that as a request moves from layer to layer, it must go through the layer right below it to get to the next layer below that one. For example, a request originating from the presentation layer must first go through the business layer and then to the persistence layer before finally hitting the database layer.\n\nSo why not allow the presentation layer direct access to either the persistence layer or database layer? After all, direct database access from the presentation layer is much faster than going through a bunch of unnecessary layers just to retrieve or save database information. The answer to this question lies in a key concept known as layers of isolation.\n\nThe layers of isolation concept means that changes made in one layer of the architecture generally don’t impact or affect components in other layers: the change is isolated to the components within that layer, and possibly another associated layer (such as a persistence layer containing SQL). If you allow the presentation layer direct access to the persistence layer, then changes made to SQL within the persistence layer would impact both the business layer and the presentation layer, thereby producing a very tightly coupled application with lots of interdependencies between components. This type of architecture then becomes very hard and expensive to change.\n\nThe layers of isolation concept also means that each layer is independent of the other layers, thereby having little or no knowledge of the inner workings of other layers in the architecture. To understand the power and importance of this concept, consider a large refactoring effort to convert the presentation framework from JSP (Java Server Pages) to JSF (Java Server Faces). Assuming that the contracts (e.g., model) used between the presentation layer and the business layer remain the same, the business layer is not affected by the refactoring and remains completely independent of the type of user-interface framework used by the presentation layer.\n\nWhile closed layers facilitate layers of isolation and therefore help isolate change within the architecture, there are times when it makes sense for certain layers to be open. For example, suppose you want to add a shared-services layer to an architecture containing common service components accessed by components within the business layer (e.g., data and string utility classes or auditing and logging classes). Creating a services layer is usually a good idea in this case because architecturally it restricts access to the shared services to the business layer (and not the presentation layer). Without a separate layer, there is nothing architecturally that restricts the presentation layer from accessing these common services, making it difficult to govern this access restriction.\n\nIn this example, the new services layer would likely reside below the business layer to indicate that components in this services layer are not accessible from the presentation layer. However, this presents a problem in that the business layer is now required to go through the services layer to get to the persistence layer, which makes no sense at all. This is an age-old problem with the layered architecture, and is solved by creating open layers within the architecture.\n\nAs illustrated in Figure 1-3, the services layer in this case is marked as open, meaning requests are allowed to bypass this open layer and go directly to the layer below it. In the following example, since the services layer is open, the business layer is now allowed to bypass it and go directly to the persistence layer, which makes perfect sense.\n\nLeveraging the concept of open and closed layers helps define the relationship between architecture layers and request flows and also provides designers and developers with the necessary information to understand the various layer access restrictions within the architecture. Failure to document or properly communicate which layers in the architecture are open and closed (and why) usually results in tightly coupled and brittle architectures that are very difficult to test, maintain, and deploy."
    },
    {
        "link": "https://medium.com/@sagar.hudge/layers-in-software-architecture-c8cc16329ff6",
        "document": "Layered architecture is a design pattern used in software development to organize code and structure applications into distinct layers, each responsible for a specific aspect of the application. This approach promotes modularity, separation of concerns, and maintainability, making it easier to develop, test, and manage applications. Here’s an overview of layered architecture and its benefits:\n\nLayered architecture structures an application into several layers, each serving a different purpose. These layers interact in a top-down manner, where higher layers depend on lower layers but not vice-versa.\n• The presentation layer, also called the UI layer, handles the interactions that users have with the software.\n• Focuses on the user interface and user experience. Developers in this layer work on making applications visually appealing and easy to use.\n• This layer acts as a bridge between the presentation layer and business logic.\n• The core layer where business rules and application logic are implemented.\n• This layer handles data storage and retrieval from databases or other persistent storage systems.\n• It manages CRUD (Create, Read, Update, Delete) operations and interacts with data sources like SQL or NoSQL databases.\n• The database itself, where data is stored in a structured or semi-structured format.\n\nHow Does Layered Architecture Help Create Applications?\n• By dividing the application into layers, each component has a clear responsibility. This separation makes the codebase easier to understand, maintain, and modify.\n• Layers are modular, allowing developers to build reusable components or services that can be used across different parts of the application or even other projects.\n• Since each layer is independent, changes in one layer (e.g., database layer) do not necessarily affect others. This allows for scaling individual parts of the application as needed.\n• With distinct layers, unit testing, integration testing, and end-to-end testing become easier because each layer can be tested in isolation.\n• The modularity and separation make it simpler to maintain, update, and refactor parts of the application without affecting the entire codebase.\n• Layered architecture allows for security measures to be applied at different levels (e.g., input validation in the presentation layer, business rule enforcement in the domain layer, and data protection in the data access layer).\n• Application Layer: Manages the flow of actions (e.g., adding products to the cart and processing orders)\n• Business Logic Layer: Contains rules like calculating discounts, verifying payment, and managing inventory\n• Data Access Layer: Interacts with the database to retrieve products and store user information\n• Ensure that each layer has a specific and well-defined responsibility:\n• The Presentation Layer should only handle user interface and interaction.\n• The Application Layer manages the flow of data and communication between layers.\n• The Business Logic Layer should encapsulate all business rules and processes.\n• Avoid mixing responsibilities between layers to maintain separation of concerns.\n• Allow communication only between adjacent layers to prevent tight coupling and dependencies:\n• The Presentation Layer communicates with the Application Layer, not directly with the Business Logic Layer or Data Access Layer.\n• Use service interfaces in the Application Layer to interact with the Business Logic Layer.\n• Avoid cross-layer dependencies, as they make the application harder to maintain and test.\n• Implement interfaces and abstract classes, especially in the Business Logic and Data Access Layers, to decouple components.\n• Interfaces allow different implementations (e.g., switching databases) without affecting other layers.\n• Abstractions make it easier to replace or modify components, supporting flexibility and extensibility.\n• Use Data Transfer Objects (DTOs) to pass data between layers, especially from the Business Logic Layer to the Application Layer and vice-versa.\n• Keep DTOs simple, containing only the necessary information needed by the receiving layer.\n• Avoid passing domain models directly to the Presentation Layer to maintain encapsulation and protect business logic.\n• Test each layer independently using appropriate testing methods:\n• Business Logic Layer: Focus on unit tests for business rules and logic.\n• Separate testing for each layer helps isolate issues and ensures a robust, maintainable application.\n• Implement DI to inject dependencies and services into each layer, promoting loose coupling.\n• This practice allows different implementations or mock versions of components during testing.\n• Use caching mechanisms in the Application and Data Access Layers to reduce load and enhance performance.\n• Apply performance monitoring tools to identify bottlenecks at different layers.\n• Presentation Layer: Implement input validation and sanitize user inputs to prevent attacks like XSS (Cross-Site Scripting).\n• Business Logic Layer: Apply business rule validation and ensure consistent application of logic to prevent security breaches.\n• Data Access Layer: Use encryption for data in transit and at rest, and manage database access securely.\n• Layered security ensures that even if one layer is compromised, the others can provide defense.\n\n9. Modularize and Reuse Components Where Possible\n• Build components in a modular way, especially for the Application and Business Logic Layers, so they can be reused across different parts of the application.\n• Reusable modules reduce duplication and allow for consistency in application behavior.\n• Maintain clear documentation of how each layer interacts and what dependencies exist.\n• Documenting interactions helps team members understand the architecture quickly and ensures consistency during development and maintenance.\n• Avoid over-layering or making each layer too complex; only include layers that add value.\n• Periodically review the architecture to identify areas where separation may have become blurred or where components are too tightly coupled.\n• Refactoring helps keep the architecture clean, maintainable, and ready to adapt to future changes.\n\nCommon Pitfalls and How to Avoid Them\n\nPitfall: Layers become tightly coupled when components from one layer directly depend on the implementation details of another layer. This leads to difficulty in maintenance, testing, and making changes without affecting multiple layers.\n• Use interfaces and abstractions to decouple layers.\n• Ensure that each layer only interacts with its adjacent layer through well-defined APIs or service interfaces.\n\nPitfall: Adding unnecessary layers increases complexity without any real benefit, leading to slower development and degraded performance.\n• Define the essential layers based on your application needs. Typically, 4–5 layers are sufficient (e.g., Presentation, Application, Business Logic, Data Access, Database).\n• Avoid splitting responsibilities into too many small layers unless there is a clear and justified need.\n\nPitfall: When layers bypass their adjacent layers and directly communicate with non-adjacent layers, it violates the separation of concerns and creates dependencies that are hard to manage.\n• Enforce strict layer communication rules where each layer only interacts with its adjacent layers.\n• Refactor code to eliminate cross-layer dependencies and ensure communication flows through the proper layers.\n\nPitfall: Implementing business rules directly in the Presentation Layer results in duplicated or inconsistent logic and makes it harder to update or scale the application.\n• Keep business logic confined to the Business Logic Layer and only send data or status updates to the Presentation Layer.\n• Use the Application Layer as an intermediary to pass information between the Presentation and Business Logic Layers.\n\nPitfall: If layers are not abstracted properly (e.g., exposing internal implementation details), it becomes difficult to replace or update components without affecting other layers.\n• Ensure each layer has a clear API that hides internal details from other layers.\n• Apply design patterns like the Facade pattern to create simplified interfaces for interacting with a layer’s functionality.\n\nPitfall: Layers like the Data Access Layer can become performance bottlenecks if queries are inefficient, or if data retrieval logic is poorly implemented.\n• Implement caching mechanisms in the Application Layer to reduce repetitive database calls.\n• Monitor performance regularly and adjust database or caching configurations as needed.\n\nPitfall: Each layer might be vulnerable to attacks if security is not implemented consistently (e.g., lack of input validation in the Presentation Layer or unprotected APIs in the Application Layer).\n• Apply security measures specific to each layer (e.g., input validation in the Presentation Layer, API authentication/authorization in the Application Layer, and data encryption in the Data Access Layer).\n• Regularly audit the architecture for security gaps and implement measures like logging and monitoring to detect threats.\n\nPitfall: Overusing or incorrectly implementing Data Transfer Objects (DTOs) can lead to passing unnecessary or invalid data between layers, which can cause bugs and increase the application’s attack surface.\n• Ensure DTOs are minimal, containing only the required data for each operation.\n• Implement validation mechanisms to verify DTOs before they are passed between layers, ensuring data integrity and security.\n\nPitfall: Failing to test each layer independently or in isolation may lead to undetected bugs and integration issues.\n• Establish a testing strategy that includes unit tests for business logic, integration tests for the application flow, and end-to-end tests for the full stack.\n• Use mocking frameworks to simulate the behavior of dependent layers during testing, ensuring isolated and focused tests.\n\n10. Failure to Refactor and Maintain the Architecture\n\nPitfall: Over time, the architecture may become outdated or cluttered with technical debt, making it hard to scale or add new features.\n• Regularly review and refactor the codebase to keep layers clean and maintainable.\n• Address technical debt promptly and update components, libraries, or technologies used in each layer as needed.\n\nPitfall: Implementing similar logic in multiple layers (e.g., validation logic in both the Presentation and Business Logic Layers) can lead to inconsistencies and makes maintenance more challenging.\n• Centralize shared logic (e.g., validation rules) in the appropriate layer (usually the Business Logic Layer) and reuse it.\n• Avoid duplicating functionality across layers and ensure clear boundaries for responsibility.\n\nBy avoiding these common pitfalls and applying the suggested solutions, you can build a well-structured, scalable, and maintainable application using layered architecture.\n\nLayered architecture helps developers create scalable, maintainable, and modular applications by organizing code into distinct layers with defined responsibilities. It promotes best practices such as separation of concerns, modularity, and maintainability, enabling efficient development and easy modification as applications grow."
    },
    {
        "link": "https://geeksforgeeks.org/introduction-to-jsp",
        "document": "JavaServer Pages (JSP) is a server-side technology that creates dynamic web applications. It allows developers to embed Java code directly into HTML or XML pages and it makes web development more efficient.\n\nJSP is an advanced version of Servlets. It provides enhanced capabilities for building scalable and platform-independent web pages.\n\nHow is JSP More Advantageous than Servlets?\n\nJSP simplifies web development by combining the strengths of Java with the flexibility of HTML. Some of the advantages of JSP over Servlets are listed below:\n• None JSP code is easier to manage than Servlets as it separates UI and business logic.\n• None JSP minimizes the amount of code required for web applications.\n• None It provides access to the complete range of Java APIs for robust application development.\n• None JSP is suitable for applications with growing user bases.\n• None It contains predefined objects like request, response, session, and application reduce development time.\n• None It has built-in mechanisms for exception and error management.\n• Client Layer: The browser sends a request to the server.\n• Web Server Layer: The server processes the request using a JSP engine.\n• Database/Backend Layer : Interacts with the database and returns the response to the client.\n\nJSP allows us to embed Java code within HTML pages and making it easy to create dynamic content. Let us start with a simple exercise to convert an existing HTML file into a JSP file.\n• None Take any HTML file you have previously created.\n• None Change the file extension from .html to .jsp.\n• None Load the new .jsp file in browser.\n\nWhen we load a JSP file for the first time:\n• None The JSP is converted into a Java file.\n• None The Java file is compiled into a servlet.\n• None The compiled servlet is loaded and executed.\n\nHere is an example to demonstrate how JSP can generate dynamic content:\n• None The new java.util.Date() expression retrieves the current date and time.\n• None When the JSP page is loaded in the browser, the Java expression is evaluated at runtime, and the output is embedded into the HTML.\n• None Each time you reload the page, it displays the current time, demonstrating how JSP dynamically generates HTML content based on Java logic.\n\nWe will learn about the several elements available in JSP with suitable examples. In JSP elements can be divided into 4 different types.\n\nThis tag is used to output any data on the generated page. These data are automatically converted to a string and printed on the output stream.\n\nNote: JSP Expressions start with Syntax of JSP Scriptles are with <%=and ends with %>. Between these, you can put anything that will convert to the String and that will be displayed.\n\nThis allows inserting any amount of valid Java code. These codes are placed in the _jspService() method by the JSP engine.\n\nNote: JSP Scriptlets begins with <% and ends %> . We can embed any amount of Java code in the JSP Scriptlets. JSP Engine places these codes in the _jspService() method.\n\nVariables available to the JSP Scriptlets are:\n\nA JSP directive starts with <%@ characters. In the directives, we can import packages, define error-handling pages, or configure session information for the JSP page.\n\nThis is used for defining functions and variables to be used in the JSP.\n\nNote: JSP Declaratives begins with <%! and ends %> with We can embed any amount of java code in the JSP Declaratives. Variables and functions defined in the declaratives are class-level and can be used anywhere on the JSP page.\n• None Save the JSP file using the .jsp extension (e.g., hello.jsp).\n• None Place your application inside the appropriate folder (e.g., webapps for Tomcat).\n• None Open the browser and enter the JSP page URL:\n\nThe JSP file is compiled and executed.\n\nJSP is powerful because it allows us to:\n• None To create dynamic pages that respond to user actions.\n• None To customize content for each user or session."
    },
    {
        "link": "https://oracle.com/java/technologies/jspt.html",
        "document": ""
    },
    {
        "link": "https://docs.oracle.com/javaee/5/tutorial/doc/bnagx.html",
        "document": "JavaServer Pages (JSP) technology allows you to easily create web content that has both static and dynamic components. JSP technology makes available all the dynamic capabilities of Java Servlet technology but provides a more natural approach to creating static content.\n\nThe main features of JSP technology are as follows:\n• None A language for developing JSP pages, which are text-based documents that describe how to process a request and construct a response\n• None Mechanisms for defining extensions to the JSP language\n\nJSP technology also contains an API that is used by developers of web containers, but this API is not covered in this tutorial."
    },
    {
        "link": "https://moldstud.com/articles/p-the-role-of-jsp-in-java-web-development",
        "document": "Integrating JavaServer Pages (JSP) into your web development toolkit can greatly enhance your ability to create dynamic web applications. JSP serves as a powerful technology that allows developers to embed Java code directly into HTML, streamlining the process of generating dynamic content. This approach not only simplifies the integration of user input but also promotes a robust separation between the presentation layer and business logic.\n\nEfficiency is a key advantage of JSP. By enabling the reuse of code through tags and custom tag libraries, developers can reduce redundancy and accelerate the development lifecycle. This results in a cleaner, more maintainable codebase, which is especially beneficial in collaborative environments. Utilizing libraries such as JavaServer Pages Standard Tag Library (JSTL) further simplifies common tasks, such as iteration and conditionals, allowing developers to focus on crafting high-quality applications.\n\nPerformance also stands out as a significant benefit of JSP. The server compiles JSP files into servlets, which are executed on the server side. This leads to faster response times and efficient resource usage compared to traditional methods of serving dynamic content. Additionally, JSP supports caching, which can dramatically decrease load times and enhance the user experience, making it an excellent choice for high-traffic websites.\n\nSecurity is another area where JSP shines. The framework includes built-in features to help safeguard web applications from common vulnerabilities, such as cross-site scripting (XSS) and SQL injection. By incorporating security best practices within your JSP development, you can provide users with a safer browsing experience.\n\nJSP provides dynamic content generation capabilities, allowing developers to create responsive and interactive web applications effortlessly. Through embedded Java code within HTML, it improves readability and streamlines the development process.\n\nUsing JSP, you can integrate JavaBeans for reusable components, enhancing modularity. This approach simplifies the management of complex applications, as you can encapsulate the business logic in separate Java classes and call them within your JSP pages.\n\nJSP supports tag libraries, like JavaServer Pages Standard Tag Library (JSTL), which promotes clean coding practices and reduces dependency on scriptlets. This keeps the HTML structure intact and separates the presentation layer from the business logic, facilitating maintenance.\n\nAdditionally, JSP benefits from seamless integration with various Java technologies, such as JDBC for database access and Java Servlets for handling requests and responses. This creates a cohesive framework for building powerful web applications.\n\nJSP's automatic error handling mechanism is another strong point. It simplifies debugging by providing simple error pages and custom responses when exceptions occur, enhancing user experience during failures.\n\nFor developers interested in expanding their skills, exploring other technologies can be beneficial. For example, if you consider your next project in a different programming domain, you might want to hire lua developers for innovative solutions.\n\nIn conclusion, JSP's features facilitate the development of scalable, maintainable web applications with dynamic content, robust component management, and effective error handling.\n\nUtilize JSP to streamline server-side rendering by directly embedding Java code into HTML. This integration allows developers to dynamically generate content on the server based on user interactions, database queries, or application logic without excessive configuration.\n\nOne of the primary strengths of JSP lies in its ability to separate presentation from business logic. By employing JSP tags and expressions, developers can mix static content with dynamic elements effortlessly. This leads to cleaner code and easier maintainability. Utilize the JavaBeans component model to encapsulate data, facilitating smooth interaction within JSP pages.\n\nLeveraging JSP custom tags enhances code reusability and readability. Create custom tag libraries to encapsulate complex functionality, reducing redundancy and allowing for a cohesive development approach. This modularity simplifies both initial development and future updates, enabling teams to focus on features rather than repetition.\n\nJSP supports expression language (EL), which allows for straightforward access to application data without extensive boilerplate code. EL provides a clean syntax for accessing JavaBeans properties, making it easier to bind data directly to UI components. This reduces the potential for errors and accelerates the rendering process.\n\nThe ability to integrate with established frameworks like Spring and Struts further enhances JSP's capability for server-side rendering. These frameworks provide additional features such as data binding and view management, ensuring smoother data flow between the server and client.\n\nEfficiency in server resource utilization is another advantage. JSP pages are compiled into servlets, which improves performance by allowing rapid processing of requests. Once compiled, subsequent requests for the same page are handled more swiftly, leading to a better user experience.\n\nIn summary, JSP excels at simplifying server-side rendering through its seamless integration of Java logic within HTML, support for reusable components, and strong framework compatibility, all while ensuring efficient processing and enhanced maintainability.\n\nThe Integration of Java Code with HTML using JSP Scriptlets\n\nUse JSP scriptlets to seamlessly integrate Java code within HTML pages. This approach allows for dynamic content generation directly in your web application. By embedding Java code inside `<% %>` tags, you can execute Java statements alongside HTML markup. For instance:\n\nIn this example, the Java variable `userName` is declared and then interpolated into the HTML to produce a welcoming message. This straightforward communication between Java and HTML enhances flexibility and responsiveness, reflecting user-specific data in real time.\n\nTo manage complex logic without cluttering the HTML, consider keeping business logic in Java classes and using JSP only for presentation. This keeps your code organized and maintainable. Alternatively, for more extensive data manipulation or database interactions, implement JavaBeans, making your JSP pages cleaner and more efficient.\n\nUtilizing scriptlets is especially beneficial when displaying data from databases. For example, you can iterate over a list of results and generate HTML dynamically:\n\nThis shows how effortlessly JSP can create lists based on dynamic content. However, remember to minimize Java logic in the view layer to adhere to best practices in design.\n\nTo enhance your development process, consider utilizing tools and services specifically designed for Java web applications. For instance, you can 'hire phonegap developers' for mobile compatibility, ensuring that your JSP pages function well across various devices. Additionally, for more industry-specific applications, explore 'automotive product development services' to align your project with sector standards.\n\nBy harnessing the power of JSP scriptlets, you can create dynamic, interactive web applications that are responsive to user needs while keeping your code organized and efficient.\n\nCustom Tags and Tag Libraries as Tools for Reusable Components\n\nImplement custom tags and tag libraries in JSP to streamline your web applications. These features enable developers to create reusable components that encapsulate specific functionalities, promoting cleaner and more maintainable code.\n\nCustom tags allow encapsulating complex logic into simple, reusable components. This leads to improved readability, making your JSP pages cleaner and easier to manage. Instead of repeating the same code, you can define a custom tag that represents a frequently used piece of functionality, which you can easily integrate into any JSP page.\n\nThe creation of a tag library is straightforward. Start by defining tag handler classes that implement the necessary business logic, and then create a Tag Library Descriptor (TLD) file to declare your custom tags. This process integrates seamlessly with JSP, allowing you to leverage these tags just like HTML elements.\n\nBy utilizing custom tags, developers can simplify their JSP pages while enhancing functionality. The improved organization and modularity make the content more manageable and promote collaboration among team members. For example, a custom tag can handle user authentication, while another can fetch and display data from a database. This allows your development team to focus on building features rather than getting bogged down in repetitive coding.\n\nExploring existing tag libraries can also provide inspiration and accelerate your development process. The JSTL (JavaServer Pages Standard Tag Library) is a great starting point, offering prebuilt tags for common tasks, which can be easily extended as your application grows.\n\nTake advantage of these tools to optimize your Java web development process, ensuring that your applications are robust and maintainable while keeping code duplication to a minimum. For those interested in expanding their skills further, consider diving into areas like game developer roles, where similar principles of organization and reusability apply in larger contexts.\n\nAutomatic Compilation of JSP to Servlets: How it Works\n\nJSP pages undergo a transformation into servlets automatically upon request. This process begins with the server detecting a JSP request, triggering the compilation of the JSP page into a Java servlet. Here's how it unfolds:\n• JSP Compilation Trigger: When a JSP file is requested for the first time or if it has been modified, the server initiates the compilation process.\n• Parsing the JSP: The JSP engine parses the JSP file, looking for HTML markup and JSP tags, and converts them into Java code, ensuring clear separation between presentation and business logic.\n• Java Servlet Creation: The parsed JSP elements are mapped to a Java servlet, where each JSP element transforms into corresponding Java methods and code blocks.\n• Compilation to Bytecode: Once the servlet is generated, the Java compiler compiles it into bytecode, producing a .class file that the Java Virtual Machine can execute.\n• Servlet Lifecycle Management: The server manages the lifecycle of the servlet, including instantiation, initialization, request handling, and destruction, optimizing performance as needed.\n\nThis automatic compilation not only streamlines the workflow but also simplifies error identification and debugging. Developers can directly modify JSP files and rely on the server to handle updates efficiently. For robust web applications, consider incorporating best practices in JSP management to enhance performance and maintainability.\n\nFor companies looking to pair their JSP implementations with efficient database interactions, a well-structured approach can significantly improve the development process. To further this, consider exploring 'hire sql clr developers' for tailored solutions.\n\nAdditionally, to create high-quality web platforms using JSP, investing in professional assistance for your web development needs is wise. Look no further than 'corporate web design&development services', which can elevate your project to the next level.\n\nExpression Language (EL) significantly streamlines data access in JSP applications. It allows developers to access JavaBeans properties, collections, and other data without cumbersome syntax. Instead of relying on complex scriptlets, EL offers a cleaner and more readable format for fetching data from Java objects.\n\nUsing EL enhances maintainability. You can easily reference attributes stored in the request, session, and application scopes with just a few characters. For instance, accessing a user's name from a session attribute can be done with a simple syntax like ${sessionScope.userName}. This simplicity not only reduces the amount of code but also minimizes the risk of errors.\n\nMoreover, EL supports built-in functions that add functionality to your JSP pages. For example, you can perform basic operations, manipulate strings, or check conditions directly within the JSP. This eliminates the need for additional Java methods, allowing for more straightforward and efficient JSP pages.\n\nFor developers looking to enhance their web application's data handling without overcomplicating their code, integrating EL is a smart choice. If you want to improve your project by hiring skilled professionals, consider reaching out to hire offshore developers who specialize in JSP and Java technologies.\n\nBy embracing EL in your JSP development workflow, you create a more streamlined process, enabling faster iterations and more robust applications. This adjustment dramatically improves the user experience while also simplifying the coding process for your development team.\n\nFor those looking to craft immersive web environments, hiring experienced designers can significantly add value. Explore the opportunity to hire immersive designers who can elevate your project's aesthetics and functionality.\n\nBenefits of Using JSP in Modern Web Applications\n\nJSP offers specific advantages that enhance web application development. Here are the key benefits:\n• Separation of Concerns: JSP promotes a clear division between the presentation layer and the business logic. This separation simplifies maintenance, making it easier for developers to manage code changes independently.\n• Integration with Java EE: As part of the Java EE framework, JSP seamlessly integrates with other Java technologies, facilitating the development of robust enterprise-level applications.\n• Built-in Support for Template Engines: JSP allows for the integration of various template engines, enabling the creation of dynamic and reusable user interfaces without excessive complexity in code.\n• Large Ecosystem: The extensive Java ecosystem offers numerous libraries and tools that can be easily integrated with JSP. This enhances functionality and speeds up development processes.\n• Reusability: JSP supports custom tag libraries and reusable components, which promote code reuse across different projects. This significantly reduces redundancy and improves productivity.\n• Improved Performance: JSP pages are compiled to servlets, resulting in optimized performance. This reduces latency in serving dynamic content, which is crucial for high-traffic applications.\n• Ease of Use with EL: JSP’s Expression Language simplifies data access, allowing developers to interact with JavaBeans more intuitively. This improves code readability and reduces potential errors.\n• Wide Community Support: A large community of developers continuously contributes to the improvement of JSP, providing a wealth of resources, documentation, and forums for troubleshooting and best practices.\n\nLeveraging these benefits enables developers to create robust and maintainable web applications that can easily adapt to evolving user needs.\n\nMaintaining Separation of Concerns with JSP and MVC Architecture\n\nJSP can be effectively utilized within the Model-View-Controller (MVC) architecture to maintain a clear separation of concerns, which enhances both maintainability and scalability of web applications. Assign the role of a view to JSP, which focuses solely on rendering the presentation layer of the application. This allows developers to keep business logic and presentation logic distinct.\n\nThe Controller component manages user input and interactions. It processes requests, invoking the appropriate model objects and determining which view should be displayed. By isolating these responsibilities, the system becomes more modular and easier to test.\n\nIn this architecture, the Model represents the data layer, managing the business logic and data storage. This separation allows developers to update the data handling mechanics without altering the presentation layer. As a result, modifications to the database structure or business rules occur independently from how information is displayed to the user.\n\nLeverage JSP's capabilities, such as using JSTL (JavaServer Pages Standard Tag Library) and EL (Expression Language) to simplify interaction with model data. This fosters a clean separation by keeping Java code minimal in JSP pages, further making them more intuitive and easier to modify.\n\nAdopting the MVC pattern with JSP mitigates complexity in larger applications. When changes are needed, only the relevant components require updates without necessitating a complete overhaul of the system. This approach not only improves code organization but also leads to more efficient development cycles and easier debugging processes.\n\nUtilizing JSP built-in methods significantly enhances developer efficiency. The ` ` method allows seamless integration of separate JSP files, simplifying component reuse. Instead of duplicating code, developers can reference existing content, reducing maintenance overhead.\n\nThe ` ` tag facilitates the creation and management of JavaBeans, promoting a clean and organized approach to accessing data. This method streamlines the retrieval of complex data structures, allowing developers to focus on functionality rather than boilerplate code.\n\nBy leveraging the built-in ` ` method, developers can redirect requests to other resources effortlessly. This enhances navigation within an application and helps maintain clean and readable JSP files, as it decouples business logic from navigation control.\n\nUsing JSP's built-in error handling features can also save time and resources. Implementing the ` ` directive allows developers to create custom error handling pages, significantly improving user experience without requiring extensive code changes in multiple places.\n\nIntegrating JSP with the Java standard tag library (JSTL) further adds to productivity. JSTL's built-in functions provide a rich set of operations for iterating, condition checking, and formatting data, minimizing the need for repetitive Java code in the JSP pages.\n\nTaking advantage of these built-in methods not only speeds up development but also leads to cleaner, more maintainable code. This results in reduced development time and quicker iterations, allowing teams to focus on delivering valuable features. Emphasizing best practices like these contributes to a smooth and efficient development process.\n\nUtilize JSP's ability to generate dynamic content, essential for handling large-scale applications. By combining Java with HTML, JSP allows developers to create pages that can quickly adapt to varying user requests without extensive overhead. This capability is crucial for applications with fluctuating user traffic.\n\nImplement a model where your JSP pages interact seamlessly with back-end services. Use Servlets to handle business logic and JSP for rendering views, ensuring that dynamic data is efficiently processed and displayed. This approach minimizes bottlenecks, especially under high load conditions.\n\nIncorporate caching mechanisms in your JSPs to enhance performance. Utilize page output caching and data caching strategies to store rendered pages or frequently accessed data, reducing database calls and speeding up content delivery. Configure cache expiration to maintain data freshness according to your application's needs.\n\nMaintain scalability with modular JSP components. Use custom tags and tag libraries to encapsulate reusable functionalities, allowing for cleaner code organization and easier maintenance. This practice enhances response times while simplifying updates and modifications across multiple pages.\n\nAdopt a content delivery network (CDN) to distribute static assets effectively. Pairing JSP with a CDN ensures that static resources such as images, CSS, and JavaScript files are delivered from geographically dispersed servers, significantly reducing load times and enhancing user experience during peak traffic periods.\n\nUtilize XML or JSON data formats for communication between JSP and client-side frameworks. This technique allows for dynamic page updates without full reloads, using AJAX calls to fetch updated content. This method improves user engagement and performance in scalable applications.\n\nRegularly monitor application performance and user interaction metrics. Analyze where dynamic content generation may slow down due to heavy processing or inefficient queries, and optimize accordingly. Use profiling tools to identify bottlenecks and implement solutions proactively to maintain a smooth user experience.\n\nJSP offers immediate advantages for rapid prototyping of user interfaces, allowing developers to create interactive web applications without extensive setup. Leveraging features like custom tags and scriptlets, teams can efficiently iterate on designs while maintaining functionality.\n\nUtilize the following methods to streamline your prototyping process:\n• Template Creation: Build reusable templates using JSP that can be adapted quickly for various UI components, reducing the need for redundant code.\n• Data Binding with Expression Language (EL): Simplify the process of linking UI elements to data models, allowing for a faster response to changes in data.\n• Incorporating Java Code Directly: Embed Java code within JSP pages using scriptlets, enabling rapid changes to logic without requiring separate files. This allows for quick evaluation of various functionalities.\n\nThese practices not only speed up the development cycle but also enhance collaboration among team members. Teams can leverage JSP's capability to integrate design elements naturally with server-side logic, improving workflows. Developers can create a UI prototype swiftly, collect user feedback, and iterate rapidly based on that input.\n\nConsider using JSP in tandem with tools that evaluate costs, such as the cost to hire a web developer, to balance both budget and development projects effectively.\n\nFor teams tackling permissions and access management, exploring resources like the iam policy simulator can assist in creating secure applications quickly, making user interface development more secure.\n\nAdopting JSP for UI prototyping not only expedites development but also cultivates a more agile and responsive project environment, facilitating adjustments based on user interaction and stakeholder feedback.\n\nJSP seamlessly integrates with various Java EE technologies, enhancing its functionality within the enterprise environment. It works harmoniously with components such as Servlets, EJB, and JSF, providing a comprehensive framework for web applications.\n\nWhen combined with Servlets, JSP can handle the presentation layer while Servlets manage control logic. This division allows developers to modularize their code, making it easier to maintain and scale. For instance, a Servlet can process user input and delegate the response rendering to a JSP page, ensuring clarity in responsibilities.\n\nWith Enterprise JavaBeans (EJB), JSP can access business logic easily. EJB components encapsulate the business tier, allowing JSP pages to retrieve data or invoke business methods with minimal overhead. This separation promotes a more organized structure and encourages reuse of business logic across different web components.\n\nJSP also pairs well with JavaServer Faces (JSF). While JSF provides sophisticated user interface component libraries, JSP can serve as a rendering layer. Developers can embed JSF components within JSP pages, blending the strengths of both technologies to create user-friendly interfaces without sacrificing performance.\n\nUsing JSP in conjunction with frameworks like Spring further extends its capabilities. Spring MVC, for example, integrates seamlessly with JSP to provide a powerful development model while maintaining the MVC architecture. This allows developers to leverage Spring's dependency injection and transaction management features in their JSP applications.\n\nOverall, JSP's ability to interoperate with other Java EE technologies not only improves the architecture of web applications but also streamlines development processes, making it a valuable asset in Java web development.\n\nUtilize implicit JSP objects to streamline your coding process and decrease repetitive tasks. JSP provides several built-in objects, such as , , , , and others, which are automatically available in your JSP pages without the need for explicit declaration. This feature enhances code readability and productivity.\n\nFor instance, by accessing the object, you can effortlessly retrieve user input from forms using methods like . Instead of instantiating and managing this object manually, JSP handles it automatically, allowing you to focus on more critical business logic.\n\nLeveraging the object, you can maintain user-specific data across multiple pages seamlessly. There's no need to recreate session management code, which minimizes potential errors and simplifies debugging efforts. Use to store user data and to retrieve it effortlessly.\n\nThe object simplifies the sharing of global data among all users. Utilize it for configuration settings or resource references, eliminating the need for complex global variable declarations. This reduces manual setup and enhances collaboration within multi-threaded environments.\n\nBy taking advantage of implicit objects, developers experience a reduction in boilerplate code, leading to more maintainable pages. This approach not only saves time but also encourages best practices by enforcing a clear structure in your JSP development process."
    },
    {
        "link": "https://jakarta.ee/specifications/pages/3.0/jakarta-server-pages-spec-3.0",
        "document": "This chapter describes the standard actions of Jakarta Server Pages 3.0 (JSP 3.0). Standard actions are represented using XML elements with a prefix of (though that prefix can be redefined in the XML syntax). A translation error will result if the JSP prefix is used for an element that is not a standard action. A action associates an instance of a Java programming language object defined within a given scope and available with a given with a newly declared scripting variable of the same . When a action is used in an scriptless page, or in an scriptless context (as in the body of an action so indicated), there are no Java scripting variables created but instead an EL variable is created. The action is quite flexible; its exact semantics depends on the attributes given. The basic semantic tries to find an existing object using and . If the object is not found it will attempt to create the object using the other attributes. It is also possible to use this action to give a local name to an object defined elsewhere, as in another JSP page or in a servlet. This can be done by using the attribute and not providing or attributes. At least one of and must be present, and it is not valid to provide both and . If and are present, must be assignable to (in the Java platform sense). For it not to be assignable is a translation-time error. The attribute specifies the name of a Bean, as specified in the JavaBeans specification. It is used as an argument to the method in the class. It must be of the form , which may be either a class, or the name of a resource of the form that will be resolved in the current . If this is not true, a request-time exception, as indicated in the semantics of the method will be raised. The value of this attribute can be a request-time attribute expression. The attribute/value tuple in a action has special meaning to a JSP container, at page translation time and at client request processing time. In particular:\n• the must be unique within the translation unit, and identifies the particular element in which it appears to the JSP container and page. Duplicate ’s found in the same translation unit shall result in a fatal translation error.\n• The JSP container will associate an object (a JavaBean component) with the named value and accessed via that name in various contexts through the object described later in this specification. The is also used to expose a variable ( ) in the page’s scripting language environment. The scope of the scripting language variable is dependent upon the scoping rules and capabilities of the scripting language used in the page. Note that this implies the value syntax must comply with the variable naming syntax rules of the scripting language used in the page. Chapter 9, Scripting provides details for the case where the language attribute is . An example of the scope rules just mentioned is shown next: <% { // introduce a new block %> ... <jsp:useBean id=\"customer\" class=\"com.myco.Customer\" /> <% /* * the tag above creates or obtains the Customer Bean * reference, associates it with the name “customer” in the * PageContext, and declares a Java programming language * variable of the same name initialized to the object reference * in this block’s scope. */ %> ... <%= customer.getName(); %> ... <% } // close the block %> <% // the variable customer is out of scope now but // the object is still valid (and accessible via pageContext) %> The attribute/value tuple is associated with, and modifies the behavior of, the attribute described above (it has both translation time and client request processing time semantics). In particular it describes the namespace, the implicit lifecycle of the object reference associated with the , and the APIs used to access this association. For all scopes, it is illegal to change the instance object so associated, such that its new runtime type is a subset of the type(s) of the object previously so associated. See Section 1.8.2, “Objects and Scopes” for details on the available scopes. The actions performed in a action are:\n• An attempt to locate an object based on the attribute values . For application and session scope, the inspection is done synchronized per scope namespace to avoid non-deterministic behavior.\n• A scripting language variable of the specified type (if given) or (if is not given) is defined with the given in the current lexical scope of the scripting language. The attribute should be used to specify a Java type that cannot be instantiated as a JavaBean (i.e. a Java type that is an abstract class, interface, or a class with no public no-args constructor). If the attribute is used for a Java type that cannot be instantiated as a JavaBean, the container may consider the page invalid, and is recommended to (but not required to) produce a fatal translation error at translation time, or a at request time. Similarly, if either or specify a type that can not be found, the container may consider the page invalid, and is recommended to (but not required to) produce a fatal translation error at translation time, or a at request time.\n• If the object is found, the variable’s value is initialized with a reference to the located object, cast to the specified . If the cast fails, a shall occur. This completes the processing of this action.\n• If the action had a non-empty body it is ignored. This completes the processing of this action.\n• If the object is not found in the specified scope and neither class nor beanName are given, a shall occur. This completes the processing of this action.\n• If the object is not found in the specified , and the specified names a non-abstract class that defines a public no-args constructor, then the class is instantiated. The new object reference is associated with the scripting variable and with the specified name in the specified scope using the appropriate scope dependent association mechanism (see ). After this, step 8 is performed.\n\n If the object is not found, and the is either abstract, an , or no public no-args constructor is defined therein, then a shall occur. This completes the processing of this action.\n• If the object is not found in the specified ; and is given, then the method of will be invoked with the of the servlet object and the as arguments. If the method succeeds, the new object reference is associated the with the scripting variable and with the specified name in the specified scope using the appropriate scope dependent association mechanism (see ). After this, step 8 is performed.\n• If the action has a non-empty body, the body is processed. The variable is initialized and available within the scope of the body. The text of the body is treated as elsewhere. Any template text will be passed through to the out stream. Scriptlets and action tags will be evaluated. A common use of a non-empty body is to complete initializing the created instance. In that case the body will likely contain actions and scriptlets that are evaluated. This completes the processing of this action. In the following example, a Bean with name of type is available after actions on this element, either because it was already created and found, or because it is newly created. In the next example, the property is set to 33 if the Bean was instantiated. In the final example, the object should have been present in the session. If so, it is given the local name with . A may be raised if the object is of the wrong class, and an may be raised if the object is not defined. This action may or not have a body. If the action has no body, it is of the form: If the action has a body, it is of the form: In this case, the body will be invoked if the Bean denoted by the action is created. Typically, the body will contain either scriptlets or tags that will be used to modify the newly created object, but the contents of the body are not restricted. The tag has the following attributes: The name used to identify the object instance in the specified scope’s namespace, and also the scripting variable name declared and initialized with that object reference. The name specified is case sensitive and shall conform to the current scripting language variable-naming conventions. The scope within which the reference is available. The default value is . See the description of the attribute defined earlier herein. A translation error must occur if scope is not one of “ ”, “ ”, “ ” or “ ”. The fully qualified name of the class that defines the implementation of the object. The class name is case sensitive.\n\n If the and attributes are not specified the object must be present in the given scope. The name of a bean, as expected by the method of the class.\n\n This attribute can accept a request-time attribute expression as a value. If specified, it defines the type of the scripting variable defined.\n\n This allows the type of the scripting variable to be distinct from, but related to, the type of the implementation class specified.\n\n The type is required to be either the class itself, a superclass of the class, or an interface implemented by the class specified.\n\n The object referenced is required to be of this type, otherwise a shall occur at request time when the assignment of the object referenced to the scripting variable is attempted.\n\n If unspecified, the value is the same as the value of the attribute. The action sets the values of properties in a bean. The attribute that denotes the bean must be defined before this action appears. There are two variants of the action. Both variants set the values of one or more properties in the bean based on the type of the properties. The usual bean introspection is done to discover what properties are present, and, for each, its name, whether it is simple or indexed, its type, and the and methods. Introspection also indicates if a given property type has a class. Properties in a Bean can be set from one or more parameters in the request object, from a constant, or from a computed request-time expression. Simple and indexed properties can be set using . When assigning from a parameter in the request object, the conversions described in Section 1.14.2.1, “Conversions from String values” are applied, using the target property to determine the target type. When assigning from a value given as a String constant, the conversions described in Section 1.14.2.1, “Conversions from String values” are applied, using the target property to determine the target type. When assigning from a value given as a request-time attribute, no type conversions are applied if a scripting expression is used, as indicated in Section 1.14.2.2, “Conversions from request-time expressions”. If an EL expression is used, the type conversions described in Section 1.23 “Type Conversion” of the EL specification document are performed. When assigning values to indexed properties the value must be an array; the rules described in the previous paragraph apply to the actions. A conversion failure leads to an error, whether at translation time or request-time. The following two actions set a value from the request parameter values. The following two elements set a property from a value The value can also be a request-time attribute value, as described in Section 1.14.1, “Request Time Attribute Values”. The action has the following attributes: The name of a bean instance defined by a action or some other action. The bean instance must contain the property to be set. The must appear before the action in the same file. The name of the property whose value will be set. If is set to then the tag will iterate over the current parameters, matching parameter names and value type(s) to property names and setter method type(s), setting each matched property to the value of the matching parameter. If a parameter has a value of , the corresponding property is not modified. The name of the request parameter whose value is given to a bean property. The name of the request parameter usually comes from a web form.\n\n If is omitted, the request parameter name is assumed to be the same as the bean property name.\n\n If the is not set in the Request object, or if it has the value of , the action has no effect (a noop).\n\n An action may not have both and attributes. The value to assign to the given property.\n\n This attribute can accept a request-time attribute expression as a value.\n\n An action may not have both and attributes. The action places the value of a bean instance property, converted to a , into the implicit object, from which the value can be displayed as output. The bean instance must be defined as indicated in the attribute before this point in the page (usually via a action). The conversion to String is done as in the methods, i.e. the method of the object is used for Object instances, and the primitive types are converted directly. If the object is not found, a request-time exception is raised. The value of the name attribute in and will refer to an object that is obtained from the object through its method. The object named by the name must have been “introduced” to the JSP processor using either the action or a custom action with an associated entry for this name. If the object was not introduced in this manner, the container implementation is recommended (but not required) to raise a translation error, since the page implementation is in violation of the specification. A consequence of the previous paragraph is that objects that are stored in, say, the session by a front component are not automatically visible to jsp:setProperty and jsp:getProperty actions in that page unless a jsp:useBean action, or some other action, makes them visible. If the JSP processor can ascertain that there is an alternate way guaranteed to access the same object, it can use that information. For example it may use a scripting variable, but it must guarantee that no intervening code has invalidated the copy held by the scripting variable. The truth is always the value held by the object. The name of the object instance from which the property is obtained. Names the property to get. A action provides for the inclusion of static and dynamic resources in the same context as the current page. See Table JSP.1-10 , “Summary of Include Mechanisms in JSP 3.0” for a summary of include facilities. Inclusion is into the current value of . The resource is specified using a that is interpreted in the context of the web application (i.e. it is mapped). The attribute of both the and the actions are interpreted relative to the current JSP page, while the attribute in an include directive is interpreted relative to the current JSP file. See below for some examples of combinations of this. An included page cannot change the response status code or set headers. This precludes invoking methods like . Attempts to invoke these methods will be ignored. The constraint is equivalent to the one imposed on the method of the class. A action may have subelements that can provide values for some parameters in the request to be used for the inclusion. Request processing resumes in the calling JSP page, once the inclusion is completed. The attribute controls flushing. If true, then, if the page output is buffered and the flush attribute is given a true value, then the buffer is flushed prior to the inclusion, otherwise the buffer is not flushed. The default value for the flush attribute is . The above example is a simple inclusion of an object. The path is interpreted in the context of the Web Application. It is likely a static object, but it could be mapped into, for instance, a servlet via . For an example of a more complex set of inclusions, consider the following four situations built using four JSP files: , , and :\n• says and says . In this case the relative specification resolves to .\n• says and says . In this case the relative specification resolves to .\n• says and says . In this case the relative specification resolves to .\n• says and says . In this case the relative specification resolves to . The first syntax just does a request-time inclusion. In the second case, the values in the subelements are used to augment the request for the purposes of the inclusion. The URL is a relative as in Section 1.2.1, “Relative URL Specifications”. Relative paths are interpreted relative to the current JSP page.\n\n Accepts a request-time attribute value (which must evaluate to a String that is a relative URL specification). Optional boolean attribute. If the value is , the buffer is flushed now. The default value is . A action allows the runtime dispatch of the current request to a static resource, a JSP page or a servlet in the same context as the current page. A effectively terminates the execution of the current page. The relative is as in Section 1.2.1, “Relative URL Specifications”. The request object will be adjusted according to the value of the page attribute. A action may have subelements that can provide values for some parameters in the request to be used for the forwarding. If the page output is buffered, the buffer is cleared prior to forwarding. If the page output is buffered and the buffer was flushed, an attempt to forward the request will result in an If the page output was unbuffered and anything has been written to it, an attempt to forward the request will result in an . The following action might be used to forward to a static page based on some dynamic condition. This tag allows the page author to cause the current request processing to be affected by the specified attributes as follows: The URL is a relative as in Section 1.2.1, “Relative URL Specifications”. Relative paths are interpreted relative to the current JSP page.\n\n Accepts a request-time attribute value (which must evaluate to a String that is a relative URL specification). The element is used to provide key/value information. This element is used in the , , and elements. A translation error shall occur if the element is used elsewhere. When doing or , the included page or forwarded page will see the original request object, with the original parameters augmented with the new parameters, in the order of appearance, with new values taking precedence over existing values when applicable. The scope of the new parameters is the or call; i.e. in the case of an the new parameters (and values) will not apply after the include. This is the same behavior as in the and methods (see Section 9.1.1 in the Servlet 5.0 specification). For example, if the request has a parameter and a parameter is specified for forward, the forwarded request shall have . Note that the new has precedence. The parameter names and values specified should be left unencoded by the page author. The JSP container must encode the parameter names and values using the character encoding from the request object when necessary. For example, if the container chooses to append the parameters to the URL in the dispatched request, both the names and values must be encoded as per the content type in the HTML specification. This action has two mandatory attributes: and . indicates the name of the parameter, and , which may be a request-time expression, indicates its value. The plugin action enables a JSP page author to generate HTML that contains the appropriate client browser dependent constructs ( or ) that will result in the download of the Java Plugin software (if required) and subsequent execution of the Applet or JavaBeans component specified therein. The tag is replaced by either an or tag, as appropriate for the requesting user agent, and emitted into the output stream of the response. The attributes of the tag provide configuration data for the presentation of the element, as indicated in the table below. The action containing one or more actions provides parameters to the Applet or JavaBeans component. The element indicates the content to be used by the client browser if the plugin cannot be started (either because or is not supported by the client browser or due to some other problem). If the plugin can start but the Applet or JavaBeans component cannot be found or started, a plugin specific message will be presented to the user, most likely a popup window reporting a . The actual plugin code need not be bundled with the JSP container and a reference to Sun’s plugin location can be used instead, although some vendors will choose to include the plugin for the benefit of their customers. Identifies the type of the component; a bean, or an Applet. As defined by HTML spec.\n\n Accepts a run-time expression value. Identifies the spec version number of the JRE the component requires in order to operate; the default is: . As defined by the HTML spec. As defined by HTML spec.\n\n Accepts a run-time expression value. URL where JRE plugin can be downloaded for Netscape Navigator, default is implementation defined. URL where JRE plugin can be downloaded for IE, default is implementation defined. The action is part of the action and can only occur as a direct child of a action. Using the element in any other context shall result in a translation-time error. The semantics and syntax of are described in Section 5.7, “<jsp:plugin>”. The action is part of the action and can only occur as a direct child of a element. Using the element in any other context shall result in a translation-time error. The semantics and syntax of are described in Section 5.7, “<jsp:plugin>”. The standard action has two uses. It allows the page author to define the value of an action attribute in the body of an XML element instead of in the value of an XML attribute. It also allows the page author to specify the attributes of the element being output, when used inside a action. The action must only appear as a subelement of a standard or custom action. An attempt to use it otherwise must result in a translation error. For example, it cannot be used to specify the value of an attribute for XML elements that are template text. For custom action invocations, JSP containers must support the use of for both Classic and Simple Tag Handlers. The behavior of the standard action varies depending on the type of attribute being specified, as follows:\n• A translation error must occur if is used to define the value of an attribute of .\n• If the enclosing action is , the value of the name attribute and the body of the action will be used as attribute name/value pairs in the dynamically constructed element. See Section 5.14, “<jsp:element>” for more details on . Note that in this context, the attribute does not apply to the action itself, but rather to the output of the element. That is, cannot be used to specify the attribute of the action.\n• For custom action attributes of type , the container must create a out of the body of the action and pass it to the tag handler. This applies for both Classic Tag Handlers and Simple Tag Handlers. A translation error must result if the body of the action is not scriptless in this case.\n• If the custom action accepts dynamic attributes (Section 7.1.8, “Attributes With Dynamic Names”), and the name of the attribute is not one explicitly indicated for the tag, then the container will evaluate the body of and assign the computed value to the attribute using the dynamic attribute machinery. Since the type of the attribute is unknown and the body of evaluates to a , the container must pass in an instance of .\n• For standard or custom action attributes that accept a request-time expression value, the Container must evaluate the body of the action and use the result of this evaluation as the value of the attribute. The body of the attribute action can be any JSP content in this case. If the type of the attribute is not , the standard type conversion rules are applied, as per Section 1.14.2.1, “Conversions from String values”.\n• For standard or custom action attributes that do not accept a request-time expression value, the Container must use the body of the action as the value of the attribute. A translation error must result if the body of the action contains anything but template text. If the body of the action is empty, it is the equivalent of specifying as the value of the attribute. Note that after being trimmed, non-empty bodies can result in a value of as well. The action accepts a attribute, a attribute, and a omit attribute. The attribute associates the action with one of the attributes the tag handler is declared to accept, or in the case of it associates the action with one of the attributes in the element being output. The optional attribute determines whether the whitespace appearning at the beginning and at the end of the element body should be discarded or not. By default, the leading and trailing whitespace is discarded. The Container must trim at translation time only. The Container must not trim at runtime. For example, if a body contains a custom action that produces leading or trailing whitespace, that whitespace is preserved regardless of the value of the attribute. The optional omit attribute, when used with <jsp:element>, determines whether the attribute in the element being output should be omitted. The following is an example of using the standard action to define an attribute that is evaluated by the container prior to the custom action invocation. This example assumes the name attribute is declared with type in the TLD. The following is an example of using the standard action within , to define which attributes are to be output with that element: This would produce the following output: See Section 1.3.10, “JSP Syntax Grammar” for the formal syntax definition of the standard action. (required) If not being used with , then if the action does not accept dynamic attributes, the name must match the name of an attribute for the action being invoked, as declared in the Tag Library Descriptor for a custom action, or as specified for a standard action, or a translation error will result. Except for when used with , a translation error will result if both an XML element attribute and a element are used to specify the value for the same attribute.\n\n The value of name can be a QName. If so, a translation error must occur if the prefix does not match that of the action it applies to, unless the action supports dynamic attributes, or unless the action is .\n\n When used with , this attribute specifies the name of the attribute to be included in the generated element. (optional) Valid values are and . If , the whitespace, including spaces, carriage returns, line feeds, and tabs, that appears at the beginning and at the end of the body of the action will be ignored by the JSP compiler. If the whitespace is not ignored. Defaults to . (optional) Valid values are and . If , and when used with <jsp:element>, the attribute in the element being ouput is omitted. Ignored when used with a standard or custom action. Defaults to . Normally, the body of a standard or custom action invocation is defined implicitly as the body of the XML element used to represent the invocation. The body of a standard or custom action can also be defined explicitly using the standard action. This is required if one or more elements appear in the body of the tag. If one or more elements appear in the body of a tag invocation but no element appears or an empty element appears, it is the equivalent of the tag having an empty body. It is also legal to use the standard action to supply bodies to standard actions, for any standard action that accepts a body (except for , , , and ). The standard action can only be used in tag files (see Chapter 8, Tag Files), and must result in a translation error if used in a JSP. It takes the name of an attribute that is a fragment, and invokes the fragment, sending the output of the result to the , or to a scoped attribute that can be examined and manipulated. If the fragment identified by the given name is , will behave as though a fragment was passed in that produces no output. The most basic usage of this standard action will invoke a fragment with the given name with no parameters. The fragment will be invoked using the method, passing in null for the parameter so that the results will be sent to the of the associated with the . The following is an example of such a basic fragment invocation: It is also possible to invoke the fragment and send the results to a scoped attribute for further examination and manipulation. This can be accomplished by specifying the or attribute in the action. In this usage, the fragment is invoked using the method, but a custom is passed in instead of . If is specified, the container must ensure that a object is made available in a scoped attribute with the name specified by . The must contain the content sent by the fragment to the provided in the call. If is specified, the container must ensure that a object is constructed and is made available in a scoped attribute with the name specified by . The object can then be passed to a custom action for further processing. The object must produce the content sent by the fragment to the provided . The must also be resettable. That is, if its method is called, the result of the invoked fragment must be able to be read again without re-executing the fragment. An optional attribute indicates the scope of the resulting scoped variable. The following is an example of using or and the attribute: JSP fragments have access to the same page scope variables as the page or tag file in which they were defined (in addition to variables in the request, session, and application scopes). Tag files have access to a local page scope, separate from the page scope of the calling page. When a tag file invokes a fragment that appears in the calling page, the JSP container provides a way to synchronize variables between the local page scope in the tag file and the page scope of the calling page. For each variable that is to be synchronized, the tag file author must declare the variable with a scope of either or . The container must then generate code to synchronize the page scope values for the variable in the tag file with the page scope equivalent in the calling page or tag file. The details of how variables are synchronized can be found in Section 8.9, “Variable Synchronization”. The following is an example of a tag file providing a fragment access to a variable: A translation error shall result if the action contains a non-empty body. See Section 1.3.10, “JSP Syntax Grammar” for the formal syntax definition of the standard action. (required) The name used to identify this fragment during this tag invocation. (optional) The name of a scoped attribute to store the result of the fragment invocation in, as a object. A translation error must occur if both and are specified. If neither nor are specified, the result of the fragment goes directly to the , as described above. (optional) The name of a scoped attribute to store the result of the fragment invocation in, as a object. A translation error must occur if both and are specified. If neither nor is specified, the result of the fragment invocation goes directly to the , as described above. (optional) The scope in which to store the resulting variable. A translation error must result if the value is not one of , , , or . A translation error will result if this attribute appears without specifying either the or attribute as well. Note that a value of should be used with caution since not all calling pages may be participating in a session. A container must throw an at runtime if is and the calling page does not participate in a session. Defaults to . The standard action can only be used in tag files (see Chapter 8, Tag Files), and must result in a translation error if used in a JSP. It invokes the body of the tag, sending the output of the result to the , or to a scoped attribute that can be examined and manipulated. The standard action behaves exactly like , except that it operates on the body of the tag instead of on a specific fragment passed as an attribute. Because it always operates on the body of the tag, there is no attribute for this standard action. The , , and attributes are all supported with the same semantics as for . Fragments are provided access to variables the same way for as they are for . If no body was passed to the tag, will behave as though a body was passed in that produces no output. The body of a tag is passed to the simple tag handler as a object. A translation error shall result if the action contains a non-empty body. See Section 1.3.10, “JSP Syntax Grammar” for the formal syntax definition of the standard action. (optional) The name of a scoped attribute to store the result of the body invocation in, as a object. A translation error must occur if both and are specified. If neither nor are specified, the result of the body goes directly to the , as described above. (optional) The name of a scoped attribute to store the result of the body invocation in, as a object. A translation error must occur if both and are specified. If neither nor is specified, the result of the body invocation goes directly to the , as described above. (optional) The scope in which to store the resulting variable. A translation error must result if the value is not one of , , , or . A translation error will result if this attribute appears without specifying either the or attribute as well. Note that a value of should be used with caution since not all calling pages may be participating in a session. A container must throw an at runtime if is and the calling page does not participate in a session. Defaults to . The action is used to dynamically define the value of the tag of an XML element. This action can be used in JSP pages, tag files and JSP documents. This action has an optional body; the body can use the A action has one mandatory attribute, name, of type . The value of the attribute is used as that of the tag of the element generated. The following example generates an XML element whose name depends on the result of an EL expression, content.headerName. The element has an attribute, lang, and the value of the attribute is that of the expression content.lang. The body of the element is the value of the expression content.body. The next example fragment shows that needs no children. The example generates an empty element with name that of the value of the expression myName. The action may have a body. Two forms are valid, depending on whether the element is to have attributes or not. In the first form, no attributes are present: In the second form, zero or more attributes are requested, using and , as appropriate. The one valid, mandatory, attribute of is its name. Unlike other standard actions, the value of the attribute must be given as an XML-style attribute and cannot be specified using This is because has a special meaning when used in the body of . See Section 5.10, “<jsp:attribute>” for more details.. (required) The value of name is that of the element generated. The name can be a QName; JSP 3.0 places no constraints on this value: it is accepted as is. A request-time attribute value may be used for this attribute. A action can be used to enclose template data in a JSP page, a JSP document, or a tag file. A action has no attributes and can appear anywhere that template data can. Its syntax is: The interpretation of a element is to pass its content through to the current value of . This is very similar to the XSLT element. The following example is a fragment that could be in both a JSP page or a JSP document. <jsp:text> This is some content </jsp:text> Expressions may appear within , as in the next example, where the expression foo.content is evaluated and the result is inserted. <jsp:text> This is some content: ${foo.content} </jsp:text> No subelements may appear within ; for example the following fragment is invalid and must generate a translation error. <jsp:text> This is some content: <jsp:text>foo</jsp:text> </jsp:text> When within a JSP document, of course, the body content needs to additionally conform to the constraints of being a well-formed XML document, so the following example, although valid in a JSP page is invalid in a JSP document: <jsp:text> This is some content: ${foo.content > 3} </jsp:text> The same example can be made legal, with no semantic changes, by using instead of in the expression; i.e. . In an JSP document, CDATA sections can also be used to quote, uninterpreted, content, as in the following example: The action has no attributes. The action may have a body. The body may not have nested actions nor scripting elements. The body may have EL expressions. The syntax is of the form: The action can only be used in JSP documents and in tag files in XML syntax, and a translation error must result if used in a standard syntax JSP or tag file. This action is used to modify some properties of the output of a JSP document or a tag file. In JSP 3.0 there are four properties that can be specified, all of which affect the output of the XML prolog. The property allows the page author to adjust whether an XML declaration is to be inserted at the beginning of the output. Since XML declarations only make sense for when the generated content is XML, the default value of this property is defined so that it is unnecessary in most cases. The property is of type and the valid values are \"yes\", \"no\", \"true\" and \"false\". The name, values and semantics mimic that of the element in the XSLT specification: if a value of \"yes\" or \"true\" is given, the container will not add an XML declaration; if a value of \"no\" or \"false\" is given, the container will add an XML declaration. The default value for a JSP document that has a element is \"yes\". The default value for JSP documents without a element is \"no\". The default value for a tag file in XML syntax is always \"yes\". If the value is \"false\" or \"no\" the tag file will emit an XML declaration as its first content. The generated XML declaration is of the form: Where is the response character encoding, as determined in Section 4.2, “Response Character Encoding”. The , and properties allow the page author to specify that a DOCTYPE be automatically generated in the XML prolog of the output. Without these properties, the DOCTYPE would need to be output manually via a element before the root element of the JSP document, which is inconvenient. A DOCTYPE must be automatically output if and only if the element appears in the translation unit as part of a action. The must appear and must only appear if the property appears, or a translation error must occur. The property is optional, but must not appear unless the property appears, or a translation error must occur. The DOCTYPE to be automatically output, if any, is statically determined at translation time. Multiple occurrences of the , or properties will cause a translation error if the values for the properties differ from the previous occurrence. The DOCTYPE that is automatically output, if any, must appear immediately before the first element of the output document. The name following <!DOCTYPE must be the value of the property. If a property appears, then the format of the generated DOCTYPE is: If a property does not appear, then the format of the generated DOCTYPE is: Where is the value of the property, is the value of the attribute, and is the value of the property. The values for and must be enclosed in either single or double quotes, depending on the value provided by the page author. It is the responsibility of the page author to provide a syntactically-valid URI as per the XML specification (see ). The following JSP document (with an extension of or with set to in the JSP configuration): generates an XML document as follows: The following JSP document is like the previous one, except that the XML declaration is omited. A typical use would be where the XML fragment is to be included within another document. The following JSP document is equivalent but uses instead of . The following JSP document specifies both a and a : and generates and XML document as follows: The following JSP document omits the and explicitly omits the XML declaration: and generates an XML document as follows: The action cannot have a body. The <jsp:output> action has the following syntax: The following are the valid attributes of jsp:output: (optional) Indicates whether to omit the generation of an XML declaration. Acceptable values are \"true\", \"yes\", \"false\" and \"no\". (optional) Must be specified if and only if is specified or a translation error must occur. Indicates the name that is to be output in the generated DOCTYPE declaration. (optional) Specifies that a DOCTYPE declaration is to be generated and gives the value for the System Literal. (optional) Must not be specified unless is specified. Gives the value for the Public ID for the generated DOCTYPE. Chapter 6, JSP Documents defines several other standard actions that are either convenient or needed to describe JSP pages with an XML document, some of which are available in all JSP pages. They are:\n\nThis chapter describes the tag library facility for introducing new actions into a JSP page. The tag library facility includes portable run-time support, a validation mechanism, and authoring tool support. Both the classic JSP 1.2 style tag extension mechanism and the newer JSP 2.0 onwards simple tag extension mechanism are described. In Chapter 8, Tag Files, a mechanism for authoring tag extensions using only JSP syntax is described. This brings the power of tag extensions to page authors that may not know the Java programming language. This chapter also provides an overview of the tag library concept. It describes the Tag Library Descriptor, and the directive. A detailed description of the APIs involved may be found in the Javadoc. A Tag Library abstracts functionality used by a JSP page by defining a specialized (sub)language that enables a more natural use of that functionality within JSP pages. The actions introduced by the Tag Library can be used by the JSP page author in JSP pages explicitly, when authoring the page manually, or implicitly, when using an authoring tool. Tag Libraries are particularly useful to authoring tools because they make intent explicit and the parameters expressed in the action instance provide information to the tool. Actions that are delivered as tag libraries are imported into a JSP page using the directive. They are available for use in the page using the prefix given by the directive. An action can create new objects that can be passed to other actions, or can be manipulated programmatically through a scripting element in the JSP page. The semantics of a specific custom action in a tag library is described via a tag handler class which is usually instantiated at runtime by the JSP page implementation class. When the tag library is well known to the JSP container (Section 7.3.9, “Well-Known URIs”), the Container can use alternative implementations as long as the semantics are preserved. Tag libraries are portable: they can be used in any legal JSP page regardless of the scripting language used in that page.\n• Execute a JSP page that uses the tag library.\n• Present the JSP page to the end user. A Tag Library is described via the Tag Library Descriptor ( TLD), an XML document that is described below. The tag extension mechanism described in this chapter addresses the following goals. It is designed to be:\n• Portable - An action described in a tag library must be usable in any JSP container.\n• Simple - Unsophisticated users must be able to understand and use this mechanism. Vendors of JSP functionality must find it easy to make it available to users as actions.\n• Expressive - The mechanism must support a wide range of actions, including nested actions, scripting elements inside action bodies, and creation, use, and updating of scripting variables.\n• Usable from different scripting languages - Although the JSP specification currently only defines the semantics for scripts in the Java programming language, we want to leave open the possibility of other scripting languages.\n• Built upon existing concepts and machinery - We do not want to reinvent what exists elsewhere. Also, we want to avoid future conflicts whenever we can predict them. The processing of a JSP page conceptually follows these steps: JSP pages can be authored using two different syntaxes: a JSP syntax and an XML syntax. The semantics and validation of a JSP syntax page is described with reference to the semantics and validation of an equivalent document in the XML syntax. The first step is to parse the JSP page. The page that is parsed is as expanded by the processing of include directives. Information in the TLD is used in this step, including the identification of custom tags, so there is some processing of the taglib directives in the JSP page. The tag libraries in the XML document are processed in the order in which they appear in the page. Each library is checked for a validator class. If one is present, the whole document is made available to its method as a object. As of JSP 2.0, the Container must provide a attribute. This information can be used to provide location information on errors. Each custom tag in the library is checked for a class. If one is present, its method is invoked. The default implementation of is to call . See the APIs for more details. Finally, the XML document is processed to create a JSP page implementation class. This process may involve creating scripting variables. Each custom action will provide information about variables, either statically in the TLD, or more flexibly by using the method of a class. Once a JSP page implementation class has been associated with a JSP page, the class will be treated as any other servlet class: requests will be directed to instances of the class. At run-time, tag handler instances will be created and methods will be invoked in them. A classic tag handler is a Java class that implements the , , or interface, and is the run-time representation of a custom action. The JSP page implementation class instantiates a tag handler object, or reuses an existing tag handler object, for each action in the JSP page. The handler object is a Java object that implements the interface. The handler object is responsible for the interaction between the JSP page and additional server-side objects. There are three main interfaces: , , and .\n• The interface defines the basic methods needed in all tag handlers. These methods include setter methods to initialize a tag handler with context data and attribute values of the action, and the and methods.\n• The is an extension to that provides the additional method, , invoked for the reevaluation of the body of the tag.\n• The interface is an extension of with two new methods for when the tag handler wants to manipulate the tag body: passes a buffer, the object, and provides an opportunity to process the buffer before the first evaluation of the body into the buffer. The use of interfaces simplifies making an existing Java object a tag handler. There are also two support classes that can be used as base classes: and . JSP 1.2 introduced a new interface designed to help maintain data integrity and resource management in the presence of exceptions. The interface is a “mix-in” interface that can be added to a class implementing any of , , or . As examples, we describe prototypical uses of tag extensions, briefly sketching how they take advantage of these mechanisms. The simplest type of action just does something, perhaps with parameters to modify what the “something” is, and improve reusability. This type of action can be implemented with a tag handler that implements the interface. The tag handler needs to use only the method which is invoked when the start tag is encountered. It can access the attributes of the tag and information about the state of the JSP page. The information is passed to the object through setter method calls, prior to the call to . Since simple actions with empty tag bodies are common, the Tag Library Descriptor can be used to indicate that the tag is always intended to be empty. This indication leads to better error checking at translation time, and to better code quality in the JSP page implementation class. Another set of simple actions require something to happen when the start tag is found, and when the end tag is found. The interface can also be used for these actions. The is similar to the method except that it is invoked when the end tag of the action is encountered. The result of the invocation indicates whether the remainder of the page is to be evaluated or not. In some cases, a body needs to be invoked only when some (possibly complex) condition happens. Again, this type of action is supported by the basic interface through the use of return values in the method. For iteration the interface is needed. The method is invoked to determine whether to reevaluate the body or not. Consider an action that evaluates its body many times, creating a stream of response data. The protocol is used for this. If the result of the reinterpretation is to be further manipulated for whatever reason, including just discarding it, we need a way to divert the output of reevaluations. This is done through the creation of a object and use of the method, which is part of the interface. also provides the method which is invoked after and before the first body evaluation provides an opportunity to interact with the body. Cooperating actions may offer the best way to describe a desired functionality. For example, one action may be used to describe information leading to the creation of a server-side object, while another action may use that object elsewhere in the page. These actions may cooperate explicitly, via scoped variables: one action creates an object and gives it a name; the other refers to the object through the name. Two actions can also cooperate implicitly. A flexible and convenient mechanism for action cooperation uses the nested structure of the actions to describe scoping. This is supported in the specification by providing each tag handler with its parent tag handler (if any) through the method. The static method in can then be used to locate a tag handler, and, once located, to perform valid operations on the tag handler. A custom action may create server-side objects and make them available to scripting elements by creating or updating the scripting variables. The variables thus affected are part of the semantics of the custom action and are the responsibility of the tag library author. This information is used at JSP page translation time and can be described in one of two ways: directly in the TLD for simple cases, or through subclasses of . Either mechanism will indicate the names and types of the scripting variables. At request time the tag handler will associate objects with the scripting variables through the object. It is the responsibility of the JSP page translator to automatically supply the code required to do the “synchronization” between the values and the scripting variables. There are some sections of JSP where scripting is not allowed. For example, this is the case in a tag body where the is declared as ‘ ’, or in a page where is true. In these sections, it is not possible to access scripting variables directly via scriptlets or expressions, and therefore the container need not synchronize them. Instead, the page author can use the EL to access the values. The API and invocation protocol for classic tag handlers is necessarily somewhat complex because scriptlets and scriptlet expressions in tag bodies can rely on surrounding context defined using scriptlets in the enclosing page. With the advent of the Expression Language (EL) and JSP Standard Tag Library (JSTL), it is now feasible to develop JSP pages that do not need scriptlets or scriptlet expressions. This allows us to define a tag invocation protocol that is easier to use for many use cases. In that interest, JSP 2.0 introduced a new type of tag extension called a Simple Tag Extension. Simple Tag Extensions can be written in one of two ways:\n• In Java, by defining a class that implements the interface. This class is intended for use by advanced page authors and tag library developers who need the flexibility of the Java language in order to write their tag handlers. The class provides a default implementation for all methods in .\n• In JSP, using tag files. This method can be used by page authors who do not know Java. It can also be used by advanced page authors or tag library developers who know Java but are producing tag libraries that are presentation-centric or can take advantage of existing tag libraries. See Chapter 8, Tag Files for more details. The lifecycle of a Simple Tag Handler is straightforward and is not complicated by caching semantics. Once a Simple Tag Handler is instantiated by the Container, it is executed and then discarded. The same instance must not be cached and reused. Initial performance metrics show that caching a tag handler instance does not necessarily lead to greater performance, and to accommodate such caching makes writing portable tag handlers difficult and makes the tag handler prone to error. In addition to being simpler to work with, Simple Tag Extensions do not directly rely on any servlet APIs, which allows for potential future integration with other technologies. This is facilitated by the class, which now extends. provides generic services such as storing the and keeping track of scoped attributes, whereas has functionality specific to serving JSPs in the context of servlets. Whereas the interface relies on , only relies on . The body of a Simple Tag, if present, is translated into a JSP Fragment and passed to the method. The tag can then execute the fragment as many times as needed. See Section 7.1.6, “JSP Fragments” for more details on JSP Fragments. Because JSP Fragments do not support scriptlets, the <body-content> of a SimpleTag cannot be \"JSP\". A JSP page is invalid if it references a custom tag whose tag handler implements the SimpleTag interface and whose <body-content> is equal to \"JSP\" as per the supporting TLD. During the translation phase, various pieces of the page are translated into implementations of the abstract class, before being passed to a tag handler. This is done automatically for any JSP code in the body of a named attribute (one that is defined by ) that is declared to be a fragment, or of type , in the TLD. This is also automatically done for the body of any tag handled by a Simple Tag handler. Once passed in, the tag handler can then evaluate and re-evaluate the fragment as many times as needed, or even pass it along to other tag handlers, in the case of Tag Files. A JSP fragment can be parameterized by a tag handler by setting page-scoped attributes in the associated with the fragment. These attributes can then be accessed via the EL. A translation error must occur if a piece of JSP code that is to be translated into a JSP Fragment contains scriptlets or scriptlet expressions. See the Javadoc for more details on the abstract class. In this section, we revisit the prototypical uses of classic tag extensions, as was presented in Section 7.1.4, “Simple Examples of Classic Tag Handlers”, and briefly describe how they are implemented using simple tag handlers. To implement plain actions, the tag library developer creates a class that extends and implements the method. The details on accessing attributes and enforcing an empty body are the same as with classic tag handlers. By default, the rest of the page will be evaluated after invoking . To signal that the page is to be skipped, throws . To implement actions with a body, the tag library developer implements and invokes the body at any point by calling on the object passed in via the method. The tag handler can provide the fragment access to variables through the object. All conditional logic is handled in the method. If the body is not to be invoked, the tag library developer simply does not call on the object passed in via . All iteration logic is handled in the method. The tag library developer simply calls on the object passed in via as many times as needed. To divert the result of the body invocation, the tag library developer passes a object to the method on the body . Unlike the standard tag handler’s solution, the result of the invocation does not need to be buffered. Cooperating actions work the same way as with classic tag handlers. A method is available in the interface and is called by the container before calling if one tag invocation is nested within another. A method is available on . This should be used, instead of in all cases where the desired return value may implement . Note that does not extend . Because of this, the common base is used in these new APIs instead of . Furthermore, because only accepts an object of type , tag collaboration becomes more difficult when classic tag handlers are nested inside custom actions. To make things easier, the class can wrap any and expose it as if it were a instace. The original can be retrieved through its method. Whenever calling the method on a classic in a case where the outer tag does not implement , the JSP Container must construct a new and call on the classic passing in the adapter. See the Javadoc for more details on these APIs. Prior to JSP 2.0, the name of every attribute that a tag handler accepted was predetermined at the time the tag handler was developed. It is sometimes useful, however, to be able to define a tag handler that accepts attributes with dynamic names that are not known until the page author uses the tag. For example, it is time consuming and error-prone to anticipate what attributes a user may wish to pass to a tag that mimics an HTML element. Available since JSP 2.0 is the ability to declare that a tag handler accepts additional attributes with dynamic names. This is done by having the tag handler implement the interface. See the Javadoc for more details on this interface. A tag library may include classes that are event listeners (see the Servlet 5.0 specification). The listeners classes are listed in the tag library descriptor and the JSP container automatically instantiates them and registers them. A Container is required to locate all TLD files (see Section 7.3.1, “Identifying Tag Library Descriptors” for details on how they are identified), read their elements, and treat the event listeners as extensions of those listed in . The order in which the listeners are registered is undefined, but they are registered before application start. Sometimes it may be useful to provide unique identifications for tag handlers. A tag handler can implement the interface for such functionality. See Javadoc for more details. The Java Metadata specification (JSR-175), which is part of Java SE 5.0 and greater, provides a means of specifying configuration data in Java code. Metadata in Java code is also referred to as annotations. In Jakarta EE, annotations are used to declare dependencies on external resources and configuration data in Java code without the need to define that data in a configuration file. Section SRV.15.5 of the Servlet Specification describes the behavior of annotations and resource injection in Jakarta EE technology compliant web containers. In the JSP specification, tag handlers which implement interfaces and may be annotated for injection. In both cases, injection occurs immediately after an instance of the tag handler is constructed, and before any of the tag properties are initialized. Event Listeners (See Section 7.1.9, “Event Listeners”) can also be annotated for resource injection. Injection occurs immediately after an instance of the event handler is constructed, and before it is registered. Please see Section SRV.15.5 of the servlet specification for more details on these annotations. A JSP container that is not part of a Jakarta EE technology-compliant implementation is encouraged, but not required, to support resource injection. Resource injection is not supported for JSP pages or tag files. A tag library is a collection of actions that encapsulate some functionality to be used from within a JSP page. A tag library is made available to a JSP page through a directive that identifies the tag library via a URI (Universal Resource Identifier). The URI identifying a tag library may be any valid URI as long as it can be used to uniquely identify the semantics of the tag library. The URI identifying the tag library is associated with a Tag Library Description (TLD) file and with tag handler classes as indicated in Section 7.3, “The Tag Library Descriptor” below. JSP page authoring tools and JSP containers are required to accept a tag library that is packaged as a JAR file. When deployed in a JSP container, the standard JAR conventions described in the Servlet 5.0 specification apply, including the conventions for dependencies on extensions. Packaged tag libraries must have at least one tag library descriptor file. The JSP 1.1 specification allowed only a single TLD, in , but as of JSP 1.2 multiple tag libraries are allowed. See Section 7.3.1, “Identifying Tag Library Descriptors” for how TLDs are identified. Both Classic and Simple Tag Handlers (implemented either in Java or as tag files) can be packaged together. A tag library contains classes for instantiation at translation time and classes for instantiation at request time. The former includes classes such as and . The latter includes tag handler classes. The usual conventions for Java classes apply: as part of a web application, they must reside either in a JAR file in the directory, or in a directory in the directory. A JAR containing packaged tag libraries must be dropped into the directory to make its classes available at request time (and also at translation time, see Section 7.3.7, “Translation-Time Class Loader”). The mapping between the URI and the TLD is explained further below. The directive in a JSP page declares that the page uses a tag library, uniquely identifies the tag library using a URI, and associates a tag prefix with usage of the actions in the library. A JSP container maps the URI used in the directive into a Tag Library Descriptor in two steps: it resolves the URI into a TLD resource path, and then derives the TLD object from the TLD resource path. If the JSP container cannot locate a TLD resource path for a given URI, a fatal translation error shall result. Similarly, it is a fatal translation error for a URI attribute value to resolve to two different TLD resource paths. It is a fatal translation error for the directive to appear after actions using the prefix introduced by it. The Tag Library Descriptor (TLD) is an XML document that describes a tag library. The TLD for a tag library is used by a JSP container to interpret pages that include directives referring to that tag library. The TLD is also used by JSP page authoring tools that will generate JSP pages that use a library, and by authors who do the same manually. The TLD includes documentation on the library as a whole and on its individual tags, version information on the JSP container and on the tag library, and information on each of the actions defined in the tag library. The TLD may name a class that can validate that a JSP page conforms to a set of constraints expected by the tag library. Each action in the library is described by giving its name, the class of its tag handler, information on any scripting variables created by the action, and information on attributes of the action. Scripting variable information can be given directly in the TLD or through a class. For each valid attribute there is an indication about whether it is mandatory, whether it can accept request-time expressions, and additional information. A TLD file is useful for providing information on a tag library. It can be read by tools without instantiating objects or loader classes. Our approach conforms to the conventions used in other Jakarta EE technologies. As of JSP 2.0, the format for the Tag Library Descriptor is represented in XML Schema. This allows for a more extensible TLD that can be used as a true single-source document. Tag library descriptor files have names that use the extension , and the extension indicates a tag library descriptor file. When deployed inside a JAR file, the tag library descriptor files must be in the directory, or a subdirectory of it. When deployed directly into a web application, the tag library descriptor files must always be in the directory, or some subdirectory of it. TLD files should not be placed in or , and must not be placed inside or a subdirectory of it, unless named and intended to configure an implicit tag library with its JSP version and . The XML Schema for a TLD document is . Note that tag files, which collectively form tag libraries, may or may not have an explicitly defined TLD. In the case that they do not, the container generates an implicit TLD that can be referenced using the attribute of the directive. More details about identifying this implicit Tag Library Descriptor can be found in Chapter 8, Tag Files. A URI in a directive is mapped into a context relative path (as discussed in Section 1.2.1, “Relative URL Specifications”). The context relative path is a URL without a protocol and host components that starts with and is called the TLD resource path. The TLD resource path is interpreted relative to the root of the web application and should resolve to a TLD file directly, or to a JAR file that has a TLD file at location . If the TLD resource path is not one of these two cases, a fatal translation error will occur. The URI describing a tag library is mapped to a TLD resource path though a taglib map, and a fallback interpretation that is to be used if the map does not contain the URI. The taglib map is built from an explicit taglib map in (described in Section 7.3.3, “Taglib Map in web.xml”) that is extended with implicit entries deduced from packaged tag libraries in the web application (described in Section 7.3.4, “Implicit Map Entries from TLDs”), and implicit entries known to the JSP container. The fallback interpretation is targetted to a casual use of the mechanism, as in the development cycle of the Web Application; in that case the URI is interpreted as a direct path to the TLD (see Section 7.3.6.2, “Computing the TLD Resource Path”). The following order of precedence applies (from highest to lowest) when building the taglib map (see the following sections for details):\n• If the container is Jakarta EE platform compliant, the Map Entries for the tag libraries that are part of the Jakarta EE platform. This currently includes the Jakarta Server Pages Standard Tag Library libraries and the Jakarta Server Faces libraries. The file can include an explicit taglib map between URIs and TLD resource paths described using the elements of the Web Application Deployment descriptor in . See Section 3.2, “Taglib Map” for more details. The taglib map described in is extended with new entries extracted from TLD files in the Web Application. The new entries are computed as follows:\n• The container searches for all files with a extension under or a subdirectory, and inside JAR files that are in . When examining a JAR file, only resources under or a subdirectory are considered. The order in which these files are searched for is implementation-specific and should not be relied on by web applications.\n• Each TLD file is examined. If it has a element, then a new element is created, with a subelement whose value is that of the element, and with a subelement that refers to the TLD file.\n• If the created element has a different to any in the taglib map, it is added. This mechanism provides an automatic URI to TLD mapping as well as supporting multiple TLDs within a packaged JAR. Note that this functionality does not require explicitly naming the location of the TLD file, which would require a mechanism like the . Note also that the mechanism does not add duplicated entries. The Container may also add additional entries to the taglib map. As in the previous case, the entries are only added for URIs that are not present in the map. Conceptually the entries correspond to TLD describing these tag libraries. These implicit map entries correspond to libraries that are known to the Container, who is responsible for providing their implementation, either through tag handlers, or via the mechanism described in Section 7.3.9, “Well-Known URIs”. The TLD resource path can be determined from the attribute of a directive as described below. In the explanation below an absolute URI is one that starts with a protocol and host, while a relative URI specification is as in section 2.5.2, i.e. one without the protocol and host part. All steps are described as if they were taken, but an implementation can use a different implementation strategy as long as the result is preserved. The taglib map generated in Sections Section 7.3.3, “Taglib Map in web.xml”, Section 7.3.4, “Implicit Map Entries from TLDs” and Section 7.3.5, “Implicit Map Entries from the Container” may contain one or more entries. Each entry is identified by a , which is the value of the subelement. This may be an absolute URI, or a relative URI spec starting with or one not starting with . Each entry also defines a as follows:\n• If the subelement is some relative URI specification that starts with a the is this URI.\n• If the subelement is some relative URI specification that does not start with , the is the resolution of the URI relative to (the result of this resolution is a relative URI specification that starts with ). The following describes how to resolve a directive to compute the TLD resource path. It is based on the value of the attribute of the directive.\n• If is , an absolute URI\n\n Look in the taglib map for an entry whose is . If found, the corresponding is the TLD resource path. If not found, a translation error is raised.\n• If is , a relative URI that starts with \n\n Look in the taglib map for an entry whose is . If found, the corresponding is the TLD resource path. If no such entry is found, is the TLD resource path.\n• If is , a relative URI that does not start with \n\n Look in the for an entry whose is . If found, the corresponding is the TLD resource path. If no such entry is found, resolve relative to the current JSP page where the directive appears; that value (by definition, this is a relative URI specification that starts with ) is the TLD resource path. For example, if references\n\n , then if there is no that matches\n\n , the TLD resource path would be . The explicit map provides a explicit description of the tag libraries that are being used in a web application. The implicit map from TLDs means that a JAR file implementing a tag library can be dropped in and used immediatedly through its stable URIs. The use of relative URI specifications in the taglib map enables very short names in the directive. For example, if the map is: Finally, the fallback rule allows a directive to refer directly to the TLD. This arrangement is very convenient for quick development at the expense of less flexibility and accountability. For example, in the case above, it enables: The set of classes available at translation time is the same as that available at runtime: the classes in the underlying Java platform, those in the JSP container, and those in the class files in , in the JAR files in , and, indirectly those indicated through the use of the attribute in the file of these JAR files. As part of the process of assembling a web application, the Application Assembler will create a directory, with appropriate and subdirectories, place JSP pages, servlet classes, auxiliary classes, and tag libraries in the proper places, and create a that ties everything together. Tag libraries that have been delivered in the standard JAR format can be dropped directly into . This automatically adds all the TLDs inside the JAR, making their URIs advertised in their elements visible to the URI to TLD map. The assembler may create entries in for each of the libraries that are to be used. Part of the assembly (and later the deployment) may create and/or change information that customizes a tag library; see Section 7.5.3, “Customizing a Tag Library”. A JSP container may “know of” some specific URIs and may provide alternate implementations for the tag libraries described by these URIs, but the user must see the behavior as that described by the required, portable tag library description described by the URI. A JSP container must always use the mapping specified for a URI in the deployment descriptor if present. If the deployer wants to use the platform-specific implementation of the well-known URI, the mapping for that URI should be removed at deployment time. The JSP 2.0 Tag Library Descriptor supports the notion of Tag Extension Elements and Tag Library Extension Elements. These are elements added to the TLD by the tag library developer that provide additional information about the tag, using a schema defined outside of the JSP specification. The information contained in these extensions is intended to be used by tools only, and is not accessible at compile-time, deployment-time, or run-time. JSP containers must not alter their behavior based on the content, the presence, or the absence of a particular Tag or Tag Library Extension Element. In addition, JSP containers must consider invalid any tag library that specifies for any Tag or Tag Library Extension element. Any attempt to use an invalid tag library must produce a translation error. This is to preserve application compatibility across containers. The JSP container may use schema to validate the structure of the Tag Library Descriptor. If it does so, any new content injected into Tag or Tag Library Extension elements must not be validated by the JSP Container. Tag Library Extension Elements provide extension information at the tag library level, and are specified by adding a element as a child of . Tag Extension Elements provide extension information at the tag level, and are specified by adding a element as a child of . To use these elements, an XML namespace must first be defined and the namespace must be imported into the TLD. In the following non-normative example, a fictitious company called ACME has decided to enhance the page author’s experience by defining a set of Tag and Tag Library Extension elements that cause sounds to be played when inserting tags in a document. In this hypothetical example, ACME has published an XML Schema at that defines the extensions, and has provided plug-ins for various JSP-capable IDEs to recognize these extension elements. The following example tag library uses ACME’s extensions to provide helpful voice annotations that describe how to use each tag in the tag library. <taglib xmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:acme=\"http://acme.com/\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/j2ee/web-jsptaglibrary_2_1.xsd http://acme.com/ http://acme.com/acme.xsd\" version=\"2.1\"> <description> Simple Math Tag Library. Contains ACME sound extensions with helpful voice annotations that describe how to use the tags in this library. </description> <tlib-version>1.0</tlib-version> <short-name>math</short-name> <tag> <description>Adds two numbers</description> <display-name>add</display-name> <name>add</name> <tag-class>com.foobar.tags.AddTag</tag-class> <body-content>empty</body-content> <attribute> <name>x</name> <type>java.lang.Double</type> </attribute> <attribute> <name>y</name> <type>java.lang.Double</type> </attribute> <tag-extension namespace=\"http://acme.com/\"> <!-- Extensions for tag sounds --> <extension-element xsi:type=\"acme:acme-soundsType\"> <acme:version>1.5</acme:version> <!-- Sound played for help on the add tag --> <acme:tag-sound>sounds/add.au</acme:tag-sound> <!-- Sound played for help on the x attribute --> <acme:attribute-sound name=\"x\"> sounds/add-x.au </acme:attribute-sound> <!-- Sound that’s played for help on the yattribute --> <acme:attribute-sound name=\"y\"> sounds/add-y.au </acme:attribute-sound> </extension-element> </tag-extension> </tag> <taglib-extension namespace=\"http://acme.com/\"> <!-- Extensions for taglibrary sounds--> <extension-element xsi:type=\"acme:acme-soundsType\"> <acme:version>1.5</acme:version> <!-- Sound played when author imports this taglib --> <acme:import-sound>sounds/intro.au</acme:import-sound> </extension-element> </taglib-extension> </taglib> The corresponding file would look something like: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <xsd:schema targetNamespace=\"http://acme.com/\" xmlns:j2ee=\"http://java.sun.com/xml/ns/j2ee\" xmlns:acme=\"http://acme.com/\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xml=\"http://www.w3.org/XML/1998/namespace\" elementFormDefault=\"qualified\" attributeFormDefault=\"unqualified\" version=\"1.0\"> <xsd:annotation> <xsd:documentation> This an XML Schema for sample Acme taglib extensibility elements, used to test TLD extensibility. </xsd:documentation> </xsd:annotation> <!-- **************************************************** --> <xsd:import namespace=\"http://java.sun.com/xml/ns/j2ee\" schemaLocation=\"../web-jsptaglibrary_2_0.xsd\" /> <!-- **************************************************** --> <xsd:complexType name=\"acme-soundsType\"> <xsd:annotation> <xsd:documentation> Extension for sounds associated with a tag </xsd:documentation> </xsd:annotation> <xsd:complexContent> <xsd:extension base=\"j2ee:extensibleType\"> <xsd:sequence> <xsd:element name=\"version\" type=\"xsd:string\"/> <xsd:element name=\"tag-sound\" type=\"xsd:string\" minOccurs=\"0\" maxOccurs=\"unbounded\"/> <xsd:element name=\"attribute-sound\" minOccurs=\"0\" maxOccurs=\"unbounded\"> <xsd:complexType> <xsd:simpleContent> <xsd:extension base=\"xsd:string\"> <xsd:attribute name=\"name\" use=\"required\" type=\"xsd:string\" /> </xsd:extension> </xsd:simpleContent> </xsd:complexType> </xsd:element> <xsd:element name=\"import-sound\" type=\"xsd:string\" minOccurs=\"0\" maxOccurs=\"unbounded\"/> </xsd:sequence> </xsd:extension> </xsd:complexContent> </xsd:complexType> </xsd:schema> There are a number of reasons why the structure of a JSP page should conform to some validation rules:\n• Request-time semantics; e.g. a subelement may require the information from some enclosing element at request-time.\n• Authoring-tool support; e.g. a tool may require an ordering in the actions.\n• Methodological constraints; e.g. a development group may want to constrain the way some features are used. Validation can be done either at translation-time or at request-time. In general translation-time validation provides a better user experience, and the JSP 3.0 specification provides a very flexible translation-time validation mechanism. Some translation-time validation is represented in the Tag Library Descriptor. In some cases a class needs to be provided to supplement this information. The Tag Library Descriptor contains the basic syntactic information. In particular, the attributes are described including their name, whether they are optional or mandatory, and whether they accept request-time expressions. Additionally the element can be used to indicate that an action must be empty. All constraints described in the TLD must be enforced. A tag library author can assume that the tag handler instance corresponds to an action that satisfies all constraints indicated in the TLD. A class may be listed in the TLD for a tag library to request that a JSP page be validated. The XML view of a JSP page is exposed through a class, and the validator class can do any checks the tag library author may have found appropriate. The JSP container must uniquely identify all XML elements in the XML view of a JSP page through a attribute. This attribute can be used to provide better information on the location of an error. A can be passed some initialization parameters in the TLD. This eases the reuse of validator classes. We expect that validator classes will be written based on different XML schema mechanisms (DTDs, XSchema, Relaxx, others). Standard validator classes may be incorporated into a later version of the JSP specification if a clear schema standard appears at some point. Additional translation-time validation can be done using the method in the class. The method is invoked at translation-time and is passed a instance as its argument. As of JSP 2.0, the default behavior of is to call the method. The mechanism was the original validation mechanism introduced in JSP 1.1 with the rest of the Tag Extension machinery. Tag libraries that are designed to run in JSP 1.2 containers or higher should use the validator class mechanism. Tag libraries that are designed to run in JSP 2.0 containers or higher that wish to use the validation mechanism are encouraged to implement the method in favor of the method due to the ability to provide better validation messages. Either method will work, though implementing both is not recommended. In some cases, additional request-time validation will be done dynamically within the methods in the tag handler. If an error is discovered, an instance of can be thrown. If uncaught, this object will invoke the errorpage mechanism of the JSP specification. This section is not normative, although it reflects good design practices. 7.5.1. How to Define New Implicit Objects We advocate the following style for the introduction of implicit objects:\n• Add an action called to define the desired objects. The JSP page can make these objects available as follows: This approach has the advantage of requiring no new machinery and of making very explicit the dependency. In some cases there may be an implementation dependency in making these objects available. For example, they may be providing access to some functionality that exists only in a particular implementation. This can be done by having the tag extension class test at run-time for the existence of some implementation dependent feature and raise a run-time error (this, of course, makes the page not Jakarta EE compliant). This mechanism, together with the access to metadata information allows for vendors to innovate within the standard. If a feature is added to a JSP specification, and a vendor also provides that feature through its vendor-specific mechanism, the standard mechanism, as indicated in the JSP specification will “win”. This means that vendor-specific mechanisms can slowly migrate into the specification as they prove their usefulness. If a vendor wants to associate some information that is not described in the current version of the TLD with some tag library, it can do so by inserting the information in a document it controls, inserting the document in the portion of the Web Application where the Tag Library resides, and using the standard Servlet 5.0 mechanisms to access that information. A tag library can be customized at assembly and deployment time. For example, a tag library that provides access to databases may be customized with login and password information. There is no convenient place in in the Servlet 5.0 spec for customization information. A standardized mechanism is probably going to be part of a forthcoming JSP specification, but in the meantime the suggestion is that a tag library author place this information in a well-known location at some resource in the portion of the Web Application and access it via the call on the .\n\nThis chapter describes the details of tag files, a JSP 2.0 onwards facility that allows page authors to author tag extensions using only JSP syntax. In the past, the ability to encapsulate presentation logic into reusable, full-featured tag libraries was only available to developers that had a reasonable amount of Java experience. Tag files bring the power of reuse to the basic page author, who are not required to know Java. When used together with JSP Fragments and Simple Tag Handlers, these concepts have the ability to simplify JSP development substantially, even for developers who do know Java. As of JSP version 2.0, the JSP Compiler is required to recognize tag files. A tag file is a source file that provides a way for a page author to abstract a segment of JSP code and make it reusable via a custom action. Tag files allow a JSP page author to create tag libraries using JSP syntax. This means that page authors no longer need to know Java or ask someone who knows Java to write a tag extension. Even for page authors or tag library developers who know Java, writing tag files is more convenient when developing tags that primarily output template text. The required file extension for a tag file are or As is the case with JSP files, the actual tag may be composed of a top file that includes other files that contain either a complete tag or a segment of a tag file. Just as the recommended extension for a segment of a JSP file is , the recommended extension for a segment of a tag file is . The syntax of a tag file is similar to that of a JSP page, with the following exceptions:\n• Directives - Some directives are not available or have limited availability, and some tag file specific directives are available. See Section 8.5, “Tag File Directives” for a discussion on tag file directives.\n• The and standard actions can only be used in Tag Files. The EBNF grammar in Section 1.3.10, “JSP Syntax Grammar” describes the syntax of tag files. The root production for a tag files is . See Section 8.6, “Tag Files in XML Syntax” for details on tag files in XML syntax. For each tag file in the web application, a tag handler is made available to JSP pages and other tag files. The specifics of how this is done are left up to the Container implementation. For example, some Containers may choose to compile tag files into Java tag handlers, whereas others may decide to interpret the tag handlers. However the Container chooses to prepare the tag handler, the following conditions must hold true for all tag handlers defined as tag files:\n• The tag file implementation must keep a copy of the instance passed to it by the invoking page via the method. This is called the Invoking JSP Context.\n• The tag file implementation must create and maintain a second instance of called a JSP Context Wrapper. If the Invoking JSP Context is an instance of , the JSP Context Wrapper must also be an instance of . This wrapper must be returned when is called.\n• For each invocation to the tag, the JSP Context Wrapper must present a clean page scope containing no initial elements. All scopes other than the page scope must be identical to those in the Invoking JSP Context and must be modified accordingly when updates are made to those scopes in the JSP Context Wrapper. Any modifications to the page scope, however, must not affect the Invoking JSP Context.\n• For each attribute declared and specified, a page-scoped variable must be created in the page scope of the JSP Context Wrapper, unless the attribute is a deferred value or a deferred method, in which case the VariableMapper obtained from the ELContext in the current pageContext is used to map the deferred expression to the attribute name. The name of the variable must be the same as the declared attribute name. The value of the variable must be the value of the attribute passed in during invocation. For each attribute declared as optional and not specified, no variable is created. If the tag accepts dynamic attributes, then the names and values of those dynamic attributes must be exposed to the tag file as specified in Table JSP.8-2 , “Details of directive attributes”.\n\n If the attribute is a deferred-value, it is directly mapped. If the attribute is a deferred-method, it is wrapped in a , and the resulting is mapped.\n\n There are two implications here. They are best illustrated by examples. Suppose we have a tag file tagf.tag: First, in , will cause the immediate evaluation of the deferred expression. Secondly, since the is used to resolve variables at EL parse time, a deferred expression such as is not dependent on anymore, so that it can be evaluated long after the end of life of the tag file’s pageContext. This is very useful for Jakarta Server Faces applications.\n\n Since the EL syntax does not allow for invocation of the method in a , the only allowable use of is to pass it to another tag that has a deferred-method attribute, in the form of “#{attr2}”.\n• For all intents and purposes other than for synchronizing the , , and scripting variables, the effective for the tag file is the JSP Context Wrapper. For example, the scripting variable must point to the JSP Context Wrapper instead of the invoking JSP Context.\n• The tag handler must behave as though a tag library descriptor entry was defined for it, in accordance with the , , and directives that appear in the tag file translation unit. It is legal for a tag file to forward to a page via the standard action. Just as for JSP pages, the forward is handled through the request dispatcher. Upon return from the method, the generated tag handler must stop processing of the tag file and throw . Similarly, if a tag file invokes a Classic Tag Handler which returns from the method, or if it invokes a Simple Tag Handler which throws in the method, the generated tag handler must terminate and must be thrown. In either of these two cases, the and methods must be called on enclosing tags that implement the interface before returning. The methods of enclosing classic tags must not be called. Care should be taken when invoking a classic tag handler from a tag file. In general, SimpleTag Extensions can be used in environments other than servlet environments. However, because the interface relies on , which in turn assumes a servlet environment, using classic tag handlers indirectly binds the use of the tag file to servlet environments. Nonetheless, the JSP container must allow such an invocation to occur. When a tag file attempts to invoke a classic tag handler (i.e. one that implements the interface), it must cast the passed to the into a . In the event that the class cast fails, the invocation of the classic tag fails, and a must be thrown. If a tag file in XML syntax contains a jsp:root element, the value of that element’s version attribute must match the tag file’s JSP version. See Section 8.4.2, “Packaging in a JAR”, and Section 8.4.3, “Packaging Directly in a Web Application”, for how the JSP version of a tag file is determined. One of the goals of tag files as a technology is to make it as easy to write a tag handler as it is to write a JSP. Traditionally, writing tag handlers has been a tedious task, with a lot of time spent compiling and packaging the tag handlers and writing a TLD to provide information to tools and page authors about the custom actions. The rules for packaging tag files are designed to make it very simple and fast to write simple tags, while still providing as much power and flexibility as classic tag handlers have. Tag extensions written in JSP using tag files can be placed in one of two locations. The first possibility is in the directory (or a subdirectory of ) in a file installed in the directory of the web application. Tags placed here are typically part of a reusable library of tags that can be easily dropped into any web application. The second possibility is in the directory (or a subdirectory of ) of the web application. Tags placed here are within easy reach and require little packaging. Only files with a or extension are recognized by the container to be tag files. Tag files that appear in any other location are not considered tag extensions and must be ignored by the JSP container. For example, a tag file that appears in the root of a web application would be treated as content to be served. To be accessible, tag files bundled in a require a Tag Library Descriptor. Tag files that appear in a JAR but are not defined in a TLD must be ignored by the JSP container. JSP 2.0 added an additional TLD element to describe tags within a tag library, namely . The element requires and subelements, which define the tag name and the full path of the tag file from the root of the JAR, respectively. In a JAR file, the element must always begin with . The values for the other subelements of override the defaults specified in the tag directive. Tag files packaged in a JAR inherit the JSP version of the TLD that references them. Note that it is possible to combine both classic tag handlers and tag handlers implemented using tag files in the same tag library by combining the use of and elements under the element. This means that in most instances the client is unaware of how the tag extension was implemented. Given that and share a namespace, a tag library is considered invalid and must be rejected by the container if a element has a subelement with the same content as a subelement in a element. Any attempt to use an invalid tag library must trigger a translation error. Tag files placed in the directory of the web application, or a subdirectory, are made easily accessible to JSPs without the need to explicitly write a Tag Library Descriptor. This makes it convenient for page authors to quickly abstract reusable JSP code by simply creating a new file and placing the code inside of it. The JSP container must interpret the directory and each subdirectory under it, as another implicitly defined tag library containing tag handlers defined by the tag files that appear in that directory. There are no special relationships between subdirectories - they are allowed simply for organizational purposes. For example, the following web application contains three tag libraries: The JSP container must generate an implicit tag library for each directory under and including . This tag library can be imported only via the attribute of the directive (see Section 1.10.2, “The Directive”), and has the following hard-wired values:\n• for the tag library defaults to 1.0\n• is derived from the directory name. If the directory is , the short name is simply . Otherwise, the full directory path (relative to the web application) is taken, minus the prefix. Then, all characters are replaced with , which yields the short name. Note that short names are not guaranteed to be unique (as in versus or versus )\n• A element is considered to exist for each tag file in this directory, with the following sub-elements:\n• The for each is the filename of the tag file, without the or extension.\n• The for each is the path of the tag file, relative to the root of the web application. For the above example, the implicit Tag Library Descriptor for the directory would be: The JSP version of an implicit tag library defaults to 2.0. The JSP version and tlib-version of an implicit tag library may be configured by placing a TLD with the reserved name implicit.tld in the same directory as the implicit tag library’s constituent tag files. A JSP 2.1 onwards container must consider only the JSP version and tlib-version specified by an implicit.tld file, and ignore its short-name element. Any additional elements in an implicit.tld file must cause a translation error. The JSP version specified in an implicit.tld file must be equal to or greater than 2.0, or else a translation error must be reported. Upon deployment, the JSP container must search for and process all tag files appearing in these directories and subdirectories. In processing a tag file, the container makes the custom actions defined in these tags available to JSP files. If a directory contains two files with the same tag name (e.g. and ), it is considered to be the same as having a TLD file with two elements whose sub-elements are identical. The tag library is therefore considered invalid. Despite the existence of an implicit tag library, a Tag Library Descriptor in the web application can still create additional tags from the same tag files. This is accomplished by adding a element with a that points to the tag file. In this case, the value of must start with . It a tag file is referenced by both a TLD as well as an implicit TLD, the JSP versions of the TLD and implicit TLD do not need to match. Tag files can also be compiled into Java classes and bundled as a tag library. This is useful for the situation where a tag library developer wishes to distribute a binary version of the tag library without the original source. Tag library developers that choose this form of packaging must use a tool that produces portable JSP code that uses only standard APIs. Containers are not required to provide such a tool. This section describes the directives available within tag files, which define Simple Tag Handlers. Table JSP.8-1 , “Directives available to tag files” outlines which directives are available in tag files: A tag file is not a . The directive must be used instead. If this directive is used in a tag file, a translation error must result. Identical to JSP pages. Note that if the included file contains syntax unsuitable for tag files, a translation error must occur. Only applicable to tag files. An attempt to use this directive in JSP pages will result in a translation error. Only applicable to tag files. An attempt to use this directive in JSP pages will result in a translation error. Only applicable to tag files. An attempt to use this directive in JSP pages will result in a translation error. The directive is similar to the directive, but applies to tag files instead of JSPs. Like the directive, a translation unit can contain more than one instance of the directive, all the attributes will apply to the complete translation unit (i.e. directives are position independent). There shall be only one occurrence of any attribute/value defined by this directive in a given translation unit, unless the values for the duplicate attributes are identical for all occurrences. The and attributes are exempt from this rule and can appear multiple times. Multiple uses of the attribute are cumulative (with ordered set union semantics). Other such multiple attribute/value (re)definitions result in a fatal translation error if the values do not match. The attribute/value namespace is reserved for use by this, and subsequent, JSP specifications. The details of the attributes are as follows: (optional) A short name that is intended to be displayed by tools. Defaults to the name of the tag file, without the extension. (optional) Provides information on the content of the body of this tag. Can be either , , or . A translation error will result if or any other value is used. Defaults to . (optional) The presence of this attribute indicates this tag supports additional attributes with dynamic names. If present, the generated tag handler must implement the interface, and the container must treat the tag as if its corresponding TLD entry contained . The implementation must not reject any attribute names. The value identifies a page scoped attribute in which to place a containing the names and values of the dynamic attributes passed during this invocation. The must contain each dynamic attribute name as the key and the dynamic attribute value as the corresponding value. Only dynamic attributes with no are to be present in the ; all other dynamic attributes are ignored. A translation error will result if there is a tag directive with a attribute equal to the value of a attribute of a directive or equal to the value of a attribute of an directive in this translation unit. (optional) Either a context-relative path, or a path relative to the tag source file, of an image file containing a small icon that can be used by tools. Defaults to no small icon. (optional) Either a context-relative path, or a path relative to the tag source file, of an image file containing a large icon that can be used by tools. Defaults to no large icon. (optional) Defines an arbitrary string that describes this tag. Defaults to no description. (optional) Defines an arbitrary string that presents an informal description of an example of a use of this action. Defaults to no example. (optional) Carries the same syntax and semantics of the language attribute of the page directive. (optional) Carries the same syntax and semantics of the import attribute of the page directive. (optional) Carries the same syntax and semantics of the attribute in the directive. However, there is no corresponding global configuration element in . The attribute cannot be used in tag files in XML syntax. (optional) Carries the same syntax and semantics of the attribute of the directive. However, there is no corresponding global configuration element in . (optional) Carries the same syntax and semantics of the deferredSyntaxAllowedAsLiteral attribute of the directive. However, there is no corresponding global configuration element in . Causes a translation error if specified in a tag file with a JSP version less than 2.1. (optional) Carries the same syntax and semantics of the trimDirectiveWhitespaces attribute of the directive. However, there is no corresponding global configuration element in . The directive is analogous to the element in the Tag Library Descriptor, and allows for the declaration of custom action attributes. The details of the attributes are as follows: (required) The unique name of the attribute being declared. A translation error must result if more than one directive appears in the same translation unit with the same . A translation error will result if there is an directive with a attribute equal to the value of the attribute of a directive or the attribute of a directive in this translation unit. (optional) Whether this attribute is required ( ) or optional ( ). Defaults to if not specified. (optional) Whether this attribute is a fragment to be evaluated by the tag handler ( ) or a normal attribute to be evaluated by the container prior to being passed to the tag handler. If this attribute is , the type attribute is fixed at and a translation error will result if the attribute is specified. Also, if this attribute is , the attribute is fixed at and a translation error will result if the attribute is specified. Defaults to . (optional) Whether the attribute’s value may be dynamically calculated at runtime by a scriptlet expression. Unlike the corresponding TLD element, this attribute defaults to . (optional) The runtime type of the attribute’s value. Defaults to if not specified. It is a translation error to specify a primitive type. (optional) Description of the attribute. Defaults to no description. (optional) Whether the attribute’s value represents a deferred value expression. Only one of or may be true. If is specified, default is true, otherwise default is false. Causes a translation error if specified in a tag file with a JSP version less than 2.1. (optional) The expected type resulting from the evaluation of the attribute’s value expression. If both and are specified, must be true. If is true, default is . Causes a translation error if specified in a tag file with a JSP version less than 2.1. (optional) Whether the attribute’s value represents a deferred method expression. Only one of or may be true. If is specified, default is true, otherwise default is false. Causes a translation error if specified in a tag file with a JSP version less than 2.1. (optional) The signature, as defined in the Java Language Specification, of the method to be invoked in the attribute’s method expression. If both and are specified, must be true. If is true and is not specified, it defaults to . Causes a translation error if specified in a tag file with a JSP version less than 2.1. The directive is analogous to the element in the Tag Library descriptor, and defines the details of a variable exposed by the tag handler to the calling page. See Section 7.1.4.7, “Actions Defining Scripting Variables” for more details. <%@ variable name-given=\"sum\" variable-class=\"java.lang.Integer\" scope=\"NESTED\" declare=\"true\" description=\"The sum of the two operands\" %> The details of the attributes are as follows: Defines a scripting variable to be defined in the page invoking this tag. Either the attribute or the attribute must be specified. Specifying neither or both will result in a translation error. A translation error will result if two directives have the same . A translation error will result if there is a directive with a attribute equal to the value of the attribute of an directive or the attribute of a directive in this translation unit. Defines a scripting variable to be defined in the page invoking this tag. The specified name is the of an attribute whose (translation-time) value at of the start of the tag invocation will give the name of the variable. A translation error will result if there is no directive with a attribute equal to the value of this attribute that is of type , is and not an . Either the attribute or the attribute must be specified. Specifying neither or both will result in a translation error. A translation error will result if two variable directives have the same . Defines a locally scoped attribute to hold the value of this variable. The container will synchronize this value with the variable whose name is given in . Required when is specified. A translation error must occur if used without . A translation error must occur if the value of is the same as the value of a attribute of an directive or the attribute of a directive in the same translation unit. (optional) The name of the class of the variable. The default is . (optional) Whether the variable is declared or not in the calling page/tag file, after this tag invocation. is the default. (optional) The scope of the scripting variable defined. Can be either , , or . Defaults to . (optional) An optional description of this variable. Defaults to no description. Tag files can be authored using the XML syntax, as described in Chapter 6, JSP Documents. This section describes the few distinctions from the case of JSP documents. Tag files in XML syntax must have the extension . All files with extension according to the rules in Section 8.4.1, “Location of Tag Files” are tag files in XML syntax. Conversely, files with extension are not in XML syntax. The element can, but needs not, appear in tag files in XML syntax. A element cannot appear in a tag file in JSP syntax. As indicated in Section 5.16, “<jsp:output>”, the default for tag files, in either syntax, is not to generate the xml declaration. The element can be used to change that default for tag files in XML syntax. Finally, the directive in a tag file in XML syntax cannot include a attribute; the encoding is inferred using the conventions for XML documents. Using the attribute shall result in a translation-time error. Similar to JSP pages, tag files have an equivalent XML document, the XML view of a tag file, that is exposed to the translation phase for validation. During the translation phase for a tag file, a tag XML view is created and passed to all tag library validators declared in all tag libraries declared in the tag file. The XML view of a tag file is identical to the XML view of a JSP, except that there are additional XML elements defined to handle tag file specific features. The XML view of a tag file is obtained in the same way that the XML view of a JSP page is obtained (see Chapter 10, XML View). Tag library developers writing tag files have access to certain implicit objects that are always available for use within scriptlets and expressions through scripting variables that are declared implicitly at the beginning of the tag handler implementation. All scripting languages are required to provide access to these objects. Each implicit object has a class or interface type defined in a core Java technology or Jakarta Servlet API package, as shown in Table JSP.8-5 , “Implicit Objects Available in Tag Files”. The response to the request.\n\n scope. The for this tag file.\n\n scope. The session object created for the requesting client (if any).\n\n This variable is only valid for HTTP protocols.\n\n scope. The servlet context obtained from the servlet configuration object (as in the call ).\n\n scope. An object that writes into the output stream.\n\n scope. The for this JSP page.\n\n scope. Object names with prefixes , , and , in any combination of upper and lower case, are reserved by the JSP specification. Just as is the case for all tag handlers, a tag file is able to communicate with its calling page via variables. As mentioned earlier, in tag files, variables are declared using the directive. Though the scopes of variables are similar to those in classic tag handlers, the semantics are slightly different. The intent is to be able to emulate IN and OUT parameters using attributes and variables, which appear as page-scoped attributes local to the tag file, and are synchronized with the calling page’s at various points. The and attributes of the directive can be used to allow the caller to customize the name of the variable in the calling page while referring to a constant name in the tag file. When using these attributes, the name of the variable in the calling page is derived from the value of at the time the tag was called. The name of the corresponding variable in the tag file is the value of .\n• IN parameters - Use attributes. For each attribute, a page-scoped attribute is made available in the of the tag file. The page-scoped attribute is initialized to the value of the attribute when the tag is called. No further synchronization is performed.\n• OUT parameters - Use variables with scope or . For each or variable, a page-scoped attribute is made available in the of the tag file. The scoped attribute is not initialized. Synchronization is performed at the end of the tag for and and also before the invocation of a fragment for . See Table JSP.8-6 , “Variable synchronization behavior” for details.\n• Nested parameters - Use variables with scope or . For each or variable, a page-scoped attribute is made available in the of the tag file. The scoped attribute is not initialized. Synchronization is performed before each fragment invocation for and , and also after the end of the tag for . See Table JSP.8-6 , “Variable synchronization behavior” for details. The JSP container is required to generate code to handle the synchronization of each declared variable. The details of how and when each variable is synchronized varies by the variable’s scope, as per Table JSP.8-6 , “Variable synchronization behavior”. The following list describes what each synchronization action means. If is used, the name of the variable in the calling page (referred to as P) and the name of the variable in the tag file (referred to as T) are the same and are equal to the value of . If is used, the name of P is equal to the value of the attribute (at the time the page was called) specified by the value of and the name of T is equal to the value of the attribute.\n• tag → page - For this variable, if T exists in the tag file, create/update P in the calling page. If a T does not exist in the tag file, and P does exist in the calling page, P is removed from the calling page’s page scope. If the declare attribute for this variable is set to true , a corresponding scripting variable is declared in the calling page or tag file, as with any other tag handler. If this scripting variable would not be accessible in the context in which it is defined, the container need not declare the scripting variable (for example in a scriptless body).\n• save - For this variable, save the value of P, for later restoration. If P did not exist, remember that fact.\n• restore - For this variable, restore the value of P in the calling page, from the value saved earlier. If P did not exist before, ensure it does not exist now. All variable synchronization and restoration that occurs at the end of a tag file must occur regardless of whether an exception is thrown inside the tag file. All variable synchronization that occurs after the invocation of a fragment must occur regardless of whether an exception occured while invoking the fragment. The following examples help illustrate how variable synchronization works between a tag file and its calling page. In this example, the scope is used to pass a variable to the tag’s body, and make it available to the calling page at the end of the tag invocation. 8.9.2.2. Example of AT_BEGIN and name-from-attribute Like the previous example, in this example the scope is used to pass a variable to the tag’s body, and make it available to the calling page at the end of the tag invocation. The name of the attribute is customized via . In this example, the scope is used to make a private variable available to the calling page. The original value is restored when the tag is done. In this example, the AT_END scope is used to return a value to the page. The body of the tag is not affected. This example illustrates how the tag file can remove objects from the page scope of the calling page during synchronization. The expected output of this example is: 2 '1' '' 2"
    },
    {
        "link": "https://support.microsoft.com/en-us/office/database-design-basics-eb2159cf-1e30-401a-8084-bd4f9c9ca1f5",
        "document": "To find and organize the information required, start with your existing information. For example, you might record purchase orders in a ledger or keep customer information on paper forms in a file cabinet. Gather those documents and list each type of information shown (for example, each box that you fill in on a form). If you don't have any existing forms, imagine instead that you have to design a form to record the customer information. What information would you put on the form? What fill-in boxes would you create? Identify and list each of these items. For example, suppose you currently keep the customer list on index cards. Examining these cards might show that each card holds a customers name, address, city, state, postal code and telephone number. Each of these items represents a potential column in a table. As you prepare this list, don’t worry about getting it perfect at first. Instead, list each item that comes to mind. If someone else will be using the database, ask for their ideas, too. You can fine-tune the list later. Next, consider the types of reports or mailings you might want to produce from the database. For instance, you might want a product sales report to show sales by region, or an inventory summary report that shows product inventory levels. You might also want to generate form letters to send to customers that announces a sale event or offers a premium. Design the report in your mind, and imagine what it would look like. What information would you place on the report? List each item. Do the same for the form letter and for any other report you anticipate creating. Giving thought to the reports and mailings you might want to create helps you identify items you will need in your database. For example, suppose you give customers the opportunity to opt in to (or out of) periodic e-mail updates, and you want to print a listing of those who have opted in. To record that information, you add a “Send e-mail” column to the customer table. For each customer, you can set the field to Yes or No. The requirement to send e-mail messages to customers suggests another item to record. Once you know that a customer wants to receive e-mail messages, you will also need to know the e-mail address to which to send them. Therefore you need to record an e-mail address for each customer. It makes good sense to construct a prototype of each report or output listing and consider what items you will need to produce the report. For instance, when you examine a form letter, a few things might come to mind. If you want to include a proper salutation — for example, the \"Mr.\", \"Mrs.\" or \"Ms.\" string that starts a greeting, you will have to create a salutation item. Also, you might typically start a letter with “Dear Mr. Smith”, rather than “Dear. Mr. Sylvester Smith”. This suggests you would typically want to store the last name separate from the first name. A key point to remember is that you should break each piece of information into its smallest useful parts. In the case of a name, to make the last name readily available, you will break the name into two parts — First Name and Last Name. To sort a report by last name, for example, it helps to have the customer's last name stored separately. In general, if you want to sort, search, calculate, or report based on an item of information, you should put that item in its own field. Think about the questions you might want the database to answer. For instance, how many sales of your featured product did you close last month? Where do your best customers live? Who is the supplier for your best-selling product? Anticipating these questions helps you zero in on additional items to record. After gathering this information, you are ready for the next step.\n\nTo divide the information into tables, choose the major entities, or subjects. For example, after finding and organizing information for a product sales database, the preliminary list might look like this: The major entities shown here are the products, the suppliers, the customers, and the orders. Therefore, it makes sense to start out with these four tables: one for facts about products, one for facts about suppliers, one for facts about customers, and one for facts about orders. Although this doesn’t complete the list, it is a good starting point. You can continue to refine this list until you have a design that works well. When you first review the preliminary list of items, you might be tempted to place them all in a single table, instead of the four shown in the preceding illustration. You will learn here why that is a bad idea. Consider for a moment, the table shown here: In this case, each row contains information about both the product and its supplier. Because you can have many products from the same supplier, the supplier name and address information has to be repeated many times. This wastes disk space. Recording the supplier information only once in a separate Suppliers table, and then linking that table to the Products table, is a much better solution. A second problem with this design comes about when you need to modify information about the supplier. For example, suppose you need to change a supplier's address. Because it appears in many places, you might accidentally change the address in one place but forget to change it in the others. Recording the supplier’s address in only one place solves the problem. When you design your database, always try to record each fact just once. If you find yourself repeating the same information in more than one place, such as the address for a particular supplier, place that information in a separate table. Finally, suppose there is only one product supplied by Coho Winery, and you want to delete the product, but retain the supplier name and address information. How would you delete the product record without also losing the supplier information? You can't. Because each record contains facts about a product, as well as facts about a supplier, you cannot delete one without deleting the other. To keep these facts separate, you must split the one table into two: one table for product information, and another table for supplier information. Deleting a product record should delete only the facts about the product, not the facts about the supplier. Once you have chosen the subject that is represented by a table, columns in that table should store facts only about the subject. For instance, the product table should store facts only about products. Because the supplier address is a fact about the supplier, and not a fact about the product, it belongs in the supplier table.\n\nTo determine the columns in a table, decide what information you need to track about the subject recorded in the table. For example, for the Customers table, Name, Address, City-State-Zip, Send e-mail, Salutation and E-mail address comprise a good starting list of columns. Each record in the table contains the same set of columns, so you can store Name, Address, City-State-Zip, Send e-mail, Salutation and E-mail address information for each record. For example, the address column contains customers’ addresses. Each record contains data about one customer, and the address field contains the address for that customer. Once you have determined the initial set of columns for each table, you can further refine the columns. For example, it makes sense to store the customer name as two separate columns: first name and last name, so that you can sort, search, and index on just those columns. Similarly, the address actually consists of five separate components, address, city, state, postal code, and country/region, and it also makes sense to store them in separate columns. If you want to perform a search, filter or sort operation by state, for example, you need the state information stored in a separate column. You should also consider whether the database will hold information that is of domestic origin only, or international, as well. For instance, if you plan to store international addresses, it is better to have a Region column instead of State, because such a column can accommodate both domestic states and the regions of other countries/regions. Similarly, Postal Code makes more sense than Zip Code if you are going to store international addresses. The following list shows a few tips for determining your columns.\n• In most cases, you should not store the result of calculations in tables. Instead, you can have Access perform the calculations when you want to see the result. For example, suppose there is a Products On Order report that displays the subtotal of units on order for each category of product in the database. However, there is no Units On Order subtotal column in any table. Instead, the Products table includes a Units On Order column that stores the units on order for each product. Using that data, Access calculates the subtotal each time you print the report. The subtotal itself should not be stored in a table.\n• You may be tempted to have a single field for full names, or for product names along with product descriptions. If you combine more than one kind of information in a field, it is difficult to retrieve individual facts later. Try to break down information into logical parts; for example, create separate fields for first and last name, or for product name, category, and description. Once you have refined the data columns in each table, you are ready to choose each table's primary key.\n\nEach table should include a column or set of columns that uniquely identifies each row stored in the table. This is often a unique identification number, such as an employee ID number or a serial number. In database terminology, this information is called the primary key of the table. Access uses primary key fields to quickly associate data from multiple tables and bring the data together for you. If you already have a unique identifier for a table, such as a product number that uniquely identifies each product in your catalog, you can use that identifier as the table’s primary key — but only if the values in this column will always be different for each record. You cannot have duplicate values in a primary key. For example, don’t use people’s names as a primary key, because names are not unique. You could easily have two people with the same name in the same table. A primary key must always have a value. If a column's value can become unassigned or unknown (a missing value) at some point, it can't be used as a component in a primary key. You should always choose a primary key whose value will not change. In a database that uses more than one table, a table’s primary key can be used as a reference in other tables. If the primary key changes, the change must also be applied everywhere the key is referenced. Using a primary key that will not change reduces the chance that the primary key might become out of sync with other tables that reference it. Often, an arbitrary unique number is used as the primary key. For example, you might assign each order a unique order number. The order number's only purpose is to identify an order. Once assigned, it never changes. If you don’t have in mind a column or set of columns that might make a good primary key, consider using a column that has the AutoNumber data type. When you use the AutoNumber data type, Access automatically assigns a value for you. Such an identifier is factless; it contains no factual information describing the row that it represents. Factless identifiers are ideal for use as a primary key because they do not change. A primary key that contains facts about a row — a telephone number or a customer name, for example — is more likely to change, because the factual information itself might change. 1. A column set to the AutoNumber data type often makes a good primary key. No two product IDs are the same. In some cases, you may want to use two or more fields that, together, provide the primary key of a table. For example, an Order Details table that stores line items for orders would use two columns in its primary key: Order ID and Product ID. When a primary key employs more than one column, it is also called a composite key. For the product sales database, you can create an AutoNumber column for each of the tables to serve as primary key: ProductID for the Products table, OrderID for the Orders table, CustomerID for the Customers table, and SupplierID for the Suppliers table.\n\nNow that you have divided your information into tables, you need a way to bring the information together again in meaningful ways. For example, the following form includes information from several tables. 1. Information in this form comes from the Customers table... Access is a relational database management system. In a relational database, you divide your information into separate, subject-based tables. You then use table relationships to bring the information together as needed. Consider this example: the Suppliers and Products tables in the product orders database. A supplier can supply any number of products. It follows that for any supplier represented in the Suppliers table, there can be many products represented in the Products table. The relationship between the Suppliers table and the Products table is, therefore, a one-to-many relationship. To represent a one-to-many relationship in your database design, take the primary key on the \"one\" side of the relationship and add it as an additional column or columns to the table on the \"many\" side of the relationship. In this case, for example, you add the Supplier ID column from the Suppliers table to the Products table. Access can then use the supplier ID number in the Products table to locate the correct supplier for each product. The Supplier ID column in the Products table is called a foreign key. A foreign key is another table’s primary key. The Supplier ID column in the Products table is a foreign key because it is also the primary key in the Suppliers table. You provide the basis for joining related tables by establishing pairings of primary keys and foreign keys. If you are not sure which tables should share a common column, identifying a one-to-many relationship ensures that the two tables involved will, indeed, require a shared column. Consider the relationship between the Products table and Orders table. A single order can include more than one product. On the other hand, a single product can appear on many orders. Therefore, for each record in the Orders table, there can be many records in the Products table. And for each record in the Products table, there can be many records in the Orders table. This type of relationship is called a many-to-many relationship because for any product, there can be many orders; and for any order, there can be many products. Note that to detect many-to-many relationships between your tables, it is important that you consider both sides of the relationship. The subjects of the two tables — orders and products — have a many-to-many relationship. This presents a problem. To understand the problem, imagine what would happen if you tried to create the relationship between the two tables by adding the Product ID field to the Orders table. To have more than one product per order, you need more than one record in the Orders table per order. You would be repeating order information for each row that relates to a single order — resulting in an inefficient design that could lead to inaccurate data. You run into the same problem if you put the Order ID field in the Products table — you would have more than one record in the Products table for each product. How do you solve this problem? The answer is to create a third table, often called a junction table, that breaks down the many-to-many relationship into two one-to-many relationships. You insert the primary key from each of the two tables into the third table. As a result, the third table records each occurrence or instance of the relationship. Each record in the Order Details table represents one line item on an order. The Order Details table’s primary key consists of two fields — the foreign keys from the Orders and the Products tables. Using the Order ID field alone doesn’t work as the primary key for this table, because one order can have many line items. The Order ID is repeated for each line item on an order, so the field doesn’t contain unique values. Using the Product ID field alone doesn’t work either, because one product can appear on many different orders. But together, the two fields always produce a unique value for each record. In the product sales database, the Orders table and the Products table are not related to each other directly. Instead, they are related indirectly through the Order Details table. The many-to-many relationship between orders and products is represented in the database by using two one-to-many relationships:\n• The Orders table and Order Details table have a one-to-many relationship. Each order can have more than one line item, but each line item is connected to only one order.\n• The Products table and Order Details table have a one-to-many relationship. Each product can have many line items associated with it, but each line item refers to only one product. From the Order Details table, you can determine all of the products on a particular order. You can also determine all of the orders for a particular product. After incorporating the Order Details table, the list of tables and fields might look something like this: Another type of relationship is the one-to-one relationship. For instance, suppose you need to record some special supplementary product information that you will need rarely or that only applies to a few products. Because you don't need the information often, and because storing the information in the Products table would result in empty space for every product to which it doesn’t apply, you place it in a separate table. Like the Products table, you use the ProductID as the primary key. The relationship between this supplemental table and the Product table is a one-to-one relationship. For each record in the Product table, there exists a single matching record in the supplemental table. When you do identify such a relationship, both tables must share a common field. When you detect the need for a one-to-one relationship in your database, consider whether you can put the information from the two tables together in one table. If you don’t want to do that for some reason, perhaps because it would result in a lot of empty space, the following list shows how you would represent the relationship in your design:\n• If the two tables have the same subject, you can probably set up the relationship by using the same primary key in both tables.\n• If the two tables have different subjects with different primary keys, choose one of the tables (either one) and insert its primary key in the other table as a foreign key. Determining the relationships between tables helps you ensure that you have the right tables and columns. When a one-to-one or one-to-many relationship exists, the tables involved need to share a common column or columns. When a many-to-many relationship exists, a third table is needed to represent the relationship.\n\nOnce you have the tables, fields, and relationships you need, you should create and populate your tables with sample data and try working with the information: creating queries, adding new records, and so on. Doing this helps highlight potential problems — for example, you might need to add a column that you forgot to insert during your design phase, or you may have a table that you should split into two tables to remove duplication. See if you can use the database to get the answers you want. Create rough drafts of your forms and reports and see if they show the data you expect. Look for unnecessary duplication of data and, when you find any, alter your design to eliminate it. As you try out your initial database, you will probably discover room for improvement. Here are a few things to check for:\n• Did you forget any columns? If so, does the information belong in the existing tables? If it is information about something else, you may need to create another table. Create a column for every information item you need to track. If the information can’t be calculated from other columns, it is likely that you will need a new column for it.\n• Are any columns unnecessary because they can be calculated from existing fields? If an information item can be calculated from other existing columns — a discounted price calculated from the retail price, for example — it is usually better to do just that, and avoid creating new column.\n• Are you repeatedly entering duplicate information in one of your tables? If so, you probably need to divide the table into two tables that have a one-to-many relationship.\n• Do you have tables with many fields, a limited number of records, and many empty fields in individual records? If so, think about redesigning the table so it has fewer fields and more records.\n• Has each information item been broken into its smallest useful parts? If you need to report, sort, search, or calculate on an item of information, put that item in its own column.\n• Does each column contain a fact about the table's subject? If a column does not contain information about the table's subject, it belongs in a different table.\n• Are all relationships between tables represented, either by common fields or by a third table? One-to-one and one-to- many relationships require common columns. Many-to-many relationships require a third table. Suppose that each product in the product sales database falls under a general category, such as beverages, condiments, or seafood. The Products table could include a field that shows the category of each product. Suppose that after examining and refining the design of the database, you decide to store a description of the category along with its name. If you add a Category Description field to the Products table, you have to repeat each category description for each product that falls under the category — this is not a good solution. A better solution is to make Categories a new subject for the database to track, with its own table and its own primary key. You can then add the primary key from the Categories table to the Products table as a foreign key. The Categories and Products tables have a one-to-many relationship: a category can include more than one product, but a product can belong to only one category. When you review your table structures, be on the lookout for repeating groups. For example, consider a table containing the following columns: Here, each product is a repeating group of columns that differs from the others only by adding a number to the end of the column name. When you see columns numbered this way, you should revisit your design. Such a design has several flaws. For starters, it forces you to place an upper limit on the number of products. As soon as you exceed that limit, you must add a new group of columns to the table structure, which is a major administrative task. Another problem is that those suppliers that have fewer than the maximum number of products will waste some space, since the additional columns will be blank. The most serious flaw with such a design is that it makes many tasks difficult to perform, such as sorting or indexing the table by product ID or name. Whenever you see repeating groups review the design closely with an eye on splitting the table in two. In the above example it is better to use two tables, one for suppliers and one for products, linked by supplier ID.\n\nYou can apply the data normalization rules (sometimes just called normalization rules) as the next step in your design. You use these rules to see if your tables are structured correctly. The process of applying the rules to your database design is called normalizing the database, or just normalization. Normalization is most useful after you have represented all of the information items and have arrived at a preliminary design. The idea is to help you ensure that you have divided your information items into the appropriate tables. What normalization cannot do is ensure that you have all the correct data items to begin with. You apply the rules in succession, at each step ensuring that your design arrives at one of what is known as the \"normal forms.\" Five normal forms are widely accepted — the first normal form through the fifth normal form. This article expands on the first three, because they are all that is required for the majority of database designs. First normal form states that at every row and column intersection in the table there, exists a single value, and never a list of values. For example, you cannot have a field named Price in which you place more than one Price. If you think of each intersection of rows and columns as a cell, each cell can hold only one value. Second normal form requires that each non-key column be fully dependent on the entire primary key, not on just part of the key. This rule applies when you have a primary key that consists of more than one column. For example, suppose you have a table containing the following columns, where Order ID and Product ID form the primary key: This design violates second normal form, because Product Name is dependent on Product ID, but not on Order ID, so it is not dependent on the entire primary key. You must remove Product Name from the table. It belongs in a different table (Products). Third normal form requires that not only every non-key column be dependent on the entire primary key, but that non-key columns be independent of each other. Another way of saying this is that each non-key column must be dependent on the primary key and nothing but the primary key. For example, suppose you have a table containing the following columns: Assume that Discount depends on the suggested retail price (SRP). This table violates third normal form because a non-key column, Discount, depends on another non-key column, SRP. Column independence means that you should be able to change any non-key column without affecting any other column. If you change a value in the SRP field, the Discount would change accordingly, thus violating that rule. In this case Discount should be moved to another table that is keyed on SRP."
    },
    {
        "link": "https://reddit.com/r/Database/comments/rrwmtv/are_there_any_good_rules_of_thumb_dos_and_donts",
        "document": "I am trying to design databases and I am not sure how I should go about doing that, and I am wondering if there are any good rules of thumb to keep in mind when designing a database"
    },
    {
        "link": "https://knack.com/blog/how-to-design-an-effective-relational-database",
        "document": "Relational databases are the workhorses of data storage; they excel at organizing large amounts of information into a structured format, making it easy to store, retrieve, and manage. Whether you’re managing a company’s customer records, tracking inventory in a store, or building a personal library of movies, a relational database can be your secret weapon.\n\nThis comprehensive guide is designed to equip you with the knowledge and tools to build a relational database. We’ll outline the technical aspects of creating tables, defining relationships, and querying data and also explore the theoretical foundations of relational database design.\n\nSo, whether you’re a seasoned developer or just starting your journey into the world of data, this guide is here to empower you. Let’s dive in and build a solid foundation for your data management needs.\n• A relational database is a structured collection of data organized into tables (or relations) that are connected by defined relationships. This allows for efficient storage, retrieval, and management of data using SQL (Structured Language Query).\n• A relational database management system (RDBMS) is software that enables users to create, manage, and interact with relational databases by organizing data into tables with relationships. It ensures data integrity, supports SQL for querying, and enables efficient data storage and retrieval.\n• Relational databases are used for storing, organizing, and managing large amounts of structured data, such as customer information, transaction records, and inventory management. They can be used in industries like finance, healthcare, and e-commerce.\n\nA relational database is a structured system for storing and organizing data in a way that allows for efficient retrieval and manipulation. It follows the relational model, which emphasizes data organization into tables and the establishment of relationships between those tables.\n\nHere’s why relational databases are crucial for data management:\n• Organization: They provide a clear and structured way to store large amounts of data, making it easy to find specific information quickly.\n• Data Integrity: The relational model enforces data consistency and reduces redundancy, ensuring the accuracy and reliability of your information.\n• Scalability: They can efficiently handle large datasets and accommodate growing data volumes.\n• Data Sharing: The structured format allows for easy sharing and manipulation of data across different applications and processes.\n\nThe concept of relational databases is rooted in the mathematical theory of relations. A relational database table can be seen as a mathematical relation where each row is a tuple, and the columns represent the attributes of that data.\n\nUnderstanding this connection helps us grasp the core principles of relational databases:\n• Tables: Correspond to mathematical relations, with rows and columns representing tuples and attributes.\n• Primary Keys: Uniquely identify each row in a table, similar to how mathematical relations avoid duplicate entries.\n• Relationships: Established between tables using foreign keys, which link data points across tables, reflecting the connections between sets in mathematical relations.\n\nBy leveraging these mathematical concepts, relational databases ensure data organization, minimize redundancy and enable powerful data manipulation techniques.\n\nBefore getting involved in table structures and queries, it’s crucial to establish clear goals for your relational database. Your ideal database should be efficient, adaptable, and perfectly suited to the unique needs of your organization.\n• Focus: Clearly defined goals help you focus on the data that truly matters for your organization’s needs.\n• Scalability: Goals that consider future growth ensure your database can adapt to accommodate evolving data needs.\n\nHow to Define Your Database Goals:\n• Identify Data Users: Who will be using this database, and for what purposes? Understanding their needs is key. (e.g., Marketing team, Sales department, Customer support)\n• Data Requirements: What specific data points are essential to capture and manage?\n• Desired Functionality: What kind of operations need to be performed on the data? (e.g., Reporting, Data Analysis, Searching)\n• Future Considerations: How might your data needs change over time? Will you need to integrate with other systems?\n\nThe true power of relational databases lies in their ability to establish connections between different tables.\n\nHere’s a breakdown of the different types of data relationships:\n• One-to-One (1:1): In this relationship, a single record in one table corresponds to exactly one record in another table. This is less common but can be used in specific scenarios.\n• One-to-Many (1:N): This is the most fundamental and widely used relationship. A single record in one table (the “one” side) can be linked to multiple records in another table (the “many” side). This is often achieved through the use of a foreign key, which references the primary key of the “one” side table.\n• Many-to-Many (N:N): Here, multiple records in one table can be associated with multiple records in another table. Relational databases cannot directly represent this relationship, but we can create a workaround using an associative table. This associative table has foreign keys referencing both the original tables and establishes the many-to-many connection.\n\nUnderstanding these relationships is key to designing an efficient and organized database structure. By properly defining relationships, you can:\n• Minimize Data Redundancy: Store data only once and avoid duplication across tables.\n• Maintain Data Integrity: Ensure consistency and accuracy of information by linking related data points.\n• Simplify Data Retrieval: Perform complex queries that span multiple tables to retrieve the information you need.\n\nNow that you understand the core concepts of relational databases, let’s explore some best practices for designing them effectively. Following these principles will ensure your database is functional, efficient, manageable, and future-proof.\n• Normalization: This is a set of techniques to minimize data redundancy and improve data integrity. Normalization involves breaking down tables into smaller, focused tables with well-defined relationships.\n• Clear and Consistent Naming: Use descriptive and consistent names for tables, columns, and constraints. This enhances the readability and maintainability of your database.\n• Data Types: Choose appropriate data types for each column, such as integers for numbers, dates for time-based data, and text for descriptive information. This ensures data accuracy and efficient storage.\n• Constraints: Utilize constraints like primary keys and foreign keys to enforce data integrity and prevent invalid entries.\n• Clear Documentation: Document your database design clearly, including table structures, relationships, and the purpose of each field. This is crucial for future maintenance and collaboration.\n• Data Duplication: Avoid storing the same data in multiple places. This can lead to inconsistencies and maintenance headaches.\n• Poor Data Naming: Cryptic or inconsistent naming conventions can make the database difficult to understand and navigate.\n• Inflexible Design: Don’t anticipate every future need, but design with some level of flexibility to accommodate potential growth and changes.\n• Security Oversights: Implement proper access controls and security measures to safeguard your sensitive data.\n• Lack of Testing: Thoroughly test your database design and queries before deploying them to real-world use cases.\n\nThe Role of SQL in Database Creation in Relational Databases\n\nSQL (Structured Query Language) is the cornerstone of interacting with relational databases. It’s a powerful and standardized language that allows you to create, manipulate, and retrieve data from your database.\n\nHere’s a glimpse into how SQL empowers you to manage your relational database:\n• Database Creation: SQL commands like CREATE TABLE enable you to define the structure of your tables, specifying columns, data types, and constraints.\n• Data Manipulation: SQL provides a rich set of commands for inserting, updating, and deleting data within your tables. (e.g., INSERT, UPDATE, DELETE)\n• Data Retrieval: The SELECT statement is the heart of data retrieval in SQL. You can use it to extract specific data points or entire rows based on various criteria and filter conditions.\n• Data Relationships: SQL allows you to establish relationships between tables using foreign keys. This is often achieved through the FOREIGN KEY constraint within the CREATE TABLE statement.\n\nNow that we’ve explored the fundamental concepts and design principles, let’s build a simple relational database using SQL.\n\nStep 1: Define Your Purpose and Data Needs\n\nHere, you’ll identify the purpose of your database and the specific data points you want to manage.\n• What kind of information will you be storing? (e.g., Customer information, Product inventory, Library of books)\n• Who will be using this database, and how? (e.g., Sales team, Marketing department, Personal reference)\n\nBy answering these questions, you can determine the tables you need and the attributes (columns) within those tables.\n\nCreating an Entity-Relationship Model (ERM) can be a helpful visualization tool, especially for complex data structures. An ERM is a diagram that represents the entities (tables) in your database and the relationships between them.\n\nOnce you have a clear understanding of your data, it’s time to translate that knowledge into SQL commands to create the tables in your database. Here’s a breakdown of the process:\n• Use the CREATE TABLE statement: This command defines the structure of your table, specifying its name and the columns it will contain.\n• Define Columns: For each column, specify its name, data type (e.g., text, integer, date), and any constraints like primary key or foreign key.\n\nThis code creates a table named “Customers” with four columns:\n• customer_id: An auto-incrementing integer that uniquely identifies each customer (primary key).\n• first_name: Customer’s first name (text, not null).\n• last_name: Customer’s last name (text, not null).\n\nNow that you have your tables defined, it’s time to establish relationships between them if necessary. Foreign keys are used to link data points across tables, enforcing referential integrity and preventing inconsistencies.\n\nHere, the Orders table is created with a foreign key customer_id that references the primary key of the Customers table. This ensures that each order has a valid customer associated with it.\n\nWith your tables in place, you can start inserting data using the INSERT statement in SQL.\n\nFinally, the power of your relational database lies in its ability to retrieve specific data. SQL’s SELECT statement allows you to query your database based on various criteria, filtering and sorting data to answer your questions and generate reports.\n\nIt’s crucial to identify your data requirements in great detail. This initial phase allows for a structured and efficient database that perfectly aligns with your application or business needs.\n\nHere, we’ll explore a step-by-step process to identify your data requirements:\n• What problem are you trying to solve, or what information do you need to manage? (e.g., Tracking customer orders for an e-commerce store, Managing employee information for a company)\n• Who will be using this data, and for what purposes? (e.g., Sales team, Marketing department, Human Resources)\n• Entities are the core building blocks of your data model. They represent the real-world objects or concepts you want to store information about. Think of them as the “things” in your data universe. (e.g., In an e-commerce store, entities could be Customers, Products, or Orders)\n• Once you have your entities identified, it’s time to determine the specific characteristics or details you want to capture for each one. These characteristics become the attributes (columns) of your database tables. Ask yourself: What information do you need to know about each entity?\n• In the real world, entities rarely exist in isolation. They often have connections with each other. Your data model needs to reflect these relationships to accurately represent the information structure. Analyze how your entities interact and identify the nature of those relationships. A Product can be included in many Orders, and an Order can contain multiple Products (Many-to-Many relationship – typically requiring an associative table).)\n• Involve Stakeholders: Get input from different departments or users who will be working with the data.\n• Start Simple, Iterate Often: Begin with a core set of entities and attributes. As your understanding evolves, refine and expand your data model.\n• Document Everything: Clearly document your data requirements, including entity definitions, attribute details, and relationship descriptions.\n\nKnack’s no-code platform offers a user-friendly interface for building database schemas without writing code. Here’s a step-by-step guide on designing your schema using Knack:\n• Log in to your Knack account or create a free trial.\n• Click on “New App” to initiate the app-building process.\n• Knack uses tables to represent your data entities.\n• Click on “Add Table” to create a new table.\n• Give your table a descriptive name that reflects the entity it represents (e.g., Customers, Products, Orders).\n• Each table will contain fields that represent the attributes of your entity.\n• Click on “Add Field” to define a new field.\n• Choose the appropriate data type for your field based on the information you want to store (e.g., Text for names, Number for prices, Date for order dates).\n• Give your field a clear and concise name that reflects its purpose (e.g., first_name, product_price, order_date).\n• Knack allows you to establish relationships between tables using connected fields.\n• To create a relationship, navigate to the table containing the “one” side of the relationship (e.g., the Customers table).\n• Select the table representing the “many” side of the relationship (e.g., Orders table). This creates a foreign key connection.\n• Choose the field in the “one” table that will be used for linking (e.g., customer_id in the Customers table).\n\nA well-designed relational database is a powerful tool, but like any tool, it needs proper maintenance. Here are best practices to keep your database running smoothly as your data volume and user base grow.\n• Indexing Key Fields: Indexes act like reference catalogs in a library, allowing for faster data retrieval. Identify frequently used columns in your queries and create indexes on those fields. This significantly improves query execution speed.\n• Optimize Queries: Write efficient SQL queries that avoid unnecessary operations or filtering conditions. Analyze slow queries and identify areas for improvement.\n• Hardware Optimization: Ensure your database server has sufficient resources (CPU, RAM) to handle the workload. Consider upgrading hardware if performance bottlenecks arise.\n• Denormalization (Strategic): In some cases, denormalization can improve read performance by duplicating certain data points across tables.\n• Archiving Old Data: Don’t overload your database with inactive or historical data. Regularly archive to keep your active tables lean and efficient.\n• Horizontal Scaling (Sharding): For massive datasets, consider horizontal scaling. This involves distributing data across multiple servers.\n• Choose the Right Database Engine: Select a database engine that fits your specific needs and anticipated data growth. Consider factors like performance, scalability, and available features.\n• Design for Growth: While building your schema, factor in potential future needs. Leave room for adding new tables or fields without compromising the overall structure.\n• Regular Monitoring: Proactively monitor your database performance and identify potential bottlenecks before they become critical issues. Regularly analyze query execution times, storage usage, and user activity.\n\nBuilding a relational database is an iterative process. Just like any software development project, thorough testing and continuous improvement can create a robust and user-friendly system.\n• Data Integrity: Testing helps identify data inconsistencies, invalid entries, and potential breaches of referential integrity. This ensures your data remains accurate and reliable.\n• Functionality: Verify that your database functions as intended. Test different queries, data manipulation operations, and user workflows to identify any bugs or shortcomings.\n• Performance: Evaluate the performance of your database under various load conditions. This helps pinpoint areas for optimization and ensures the system can handle real-world usage.\n• Unit Testing: Test individual components of your database schema, such as table structures and queries, in isolation. This helps isolate issues early in the development process.\n• Integration Testing: Test how different parts of your database interact with each other, ensuring smooth data flow and consistency across tables.\n• User Acceptance Testing (UAT): Involve your end-users in testing the database. Their feedback is invaluable for identifying usability issues and ensuring the system meets their needs effectively.\n\nChoosing and Setting Up Primary Fields in a Relational Database\n\nThe primary key is a fundamental concept in relational databases. It acts as a unique identifier for each row in a table, ensuring data integrity and efficient data retrieval. Choosing the right primary key is crucial for establishing a solid foundation for your database.\n• Uniqueness: The primary key value must be unique for every row in the table. No two rows can have the same primary key value.\n• Not Null: The primary key field should not allow null values. Every row must have a defined primary key value.\n• Simplicity and Efficiency: Ideally, the primary key should be concise and allow for efficient retrieval of data.\n• Auto-Incrementing Integers: This is a popular choice for primary keys. The database automatically generates a unique integer for each new row, ensuring uniqueness and simplicity. (e.g., customer_id in a Customers table)\n• Unique Natural Keys: In some cases, a natural attribute of an entity can serve as a unique identifier. For example, a Social Security number (assuming appropriate privacy considerations) could be a primary key in an Employee table, provided duplicates are strictly controlled.\n• Composite Keys: When no single attribute is inherently unique, a combination of two or more attributes can be used as a composite primary key. This is often used for tables linking multiple entities. (e.g., A combination of order_id and product_id in an Order_Details table linking Orders and Products tables)\n\nRelational databases require regular maintenance, so you may need to address issues in your design as your data needs evolve. Here’s how:\n• Data Redundancy: Avoid storing the same data in multiple places. Normalize your tables and use foreign keys to create relationships.\n• Performance Issues: Optimize queries, create indexes on frequently used fields, and consider hardware upgrades if needed.\n• Scalability Challenges: Plan for growth! Denormalize strategically, archive old data, and explore horizontal scaling for massive datasets.\n• Testing Oversights: Thoroughly test your database design! Involve users, identify bottlenecks, and iterate based on feedback.\n\nThis comprehensive guide has equipped you with the knowledge and best practices to navigate the world of relational databases. You’ve learned how to:\n• Define your data requirements and identify key entities and relationships.\n• Optimize your database for performance and scalability to handle growing needs.\n• Implement a rigorous testing and iteration process to ensure data integrity and user satisfaction.\n\nBuilding a database with Knack is far simpler than doing this from scratch. Following our “working with records” guide will give you everything you need to know about building your table, fields, and records to start building custom software applications.\n\nKnack uses tables and fields to define your data. Tables are used to separate your data into common groups. You can think of a table like a spreadsheet or a database table. Fields are used to define specific attributes of a table. Think of a field as a spreadsheet column. You’ll want to add a field for each attribute you want to store for a given table.\n\nOnce you’ve signed up for Knack, you can access your tables by clicking on the “Data” button in the top left of the Builder (inside your new project):\n\nFrom here, you can start defining your database records, tables, fields, and overall schema to build your application. This makes the process of building relational databases much easier.\n\nStart building your relational database for free today with Knack!\n\nWhat is a relational database, and why is it important for businesses?\n\nA relational database is a type of database that organizes data into tables with rows and columns and establishes relationships between these tables based on common fields. It’s important for businesses because it provides a structured and efficient way to store, manage, and retrieve data.\n\nHow does Knack facilitate the building of relational databases?\n\nKnack provides a user-friendly platform that allows users to build custom relational databases without writing any code. With its intuitive drag-and-drop interface and customizable templates, Knack empowers users to design database schemas, define relationships between tables, and create forms and views for data entry and retrieval, all without the need for technical expertise.\n\nWhat are the key components of a relational database built with Knack?\n\nKey components of a relational database built with Knack include tables, which store data in rows and columns; fields, which represent the attributes of the data stored in each table; relationships, which define connections between tables based on common fields; forms, which allow users to enter and edit data; and views, which display data in different formats for analysis and reporting.\n\nCan I import existing data into a relational database built with Knack?\n\nYes, Knack allows users to import existing data from various sources, including spreadsheets, CSV files, and other databases, into their relational databases. Users can map fields from their data sources to fields in their Knack databases, enabling them to quickly populate their databases with existing data and start using Knack’s features for data management and analysis.\n\nHow scalable are relational databases built with Knack?\n\nRelational databases built with Knack are highly scalable and can accommodate growing data volumes and user bases. Knack offers flexible pricing plans that allow users to scale their databases and applications as needed, with options for additional storage, users, and features."
    },
    {
        "link": "https://vertabelo.com/blog/database-design-principles",
        "document": "A well-designed data model must support database integrity, performance, scalability, and security. To achieve these qualities, pay attention to the top 12 database design principles explained in this guide.\n\nDatabase design principles are essential to creating efficient, reliable, and scalable databases. A database created following these fundamental design principles ensures that its data will be stored in it in an organized and structured manner. It will facilitate database administration and allow users to obtain accurate results.\n\nFollowing the best practices for data modeling allows a database to meet a series of key objectives:\n• Integrity: Observing principles such as non-redundancy and the use of constraints, primary and foreign keys, data validation, and referential integrity ensures that the information stored in the database maintains its integrity and consistency.\n• Performance: Normalization principles optimize data access and eliminate redundancy. In turn, indexing principles contribute to speeding up result times.\n• Scalability: A database must be able to grow without compromising its performance and the integrity of its data. The same design principles that contribute to integrity and performance also make scalability possible.\n• Security: Database design principles aimed at controlling access to information – as well as encrypting and protecting sensitive data – make information security possible.\n• Maintainability: Using naming conventions and maintaining up-to-date documentation ensure that databases are easy to use, update, and modify.\n\nIf you’re new to database design, you may want to start with our article on how to learn database design. Then, you can come back to the top 12 database design principles that will help you achieve the above objectives. Additionally, consider enrolling in database training courses to gain a comprehensive understanding and practical skills in database design.\n\nRedundant information in a database schema can cause several problems. It can lead to inconsistencies: if the same data is stored in multiple tables, there is a risk that it will be updated in one table and not in the others – generating discrepancies in the results.\n\nRedundancy also unnecessarily increases the storage space required by the database and can negatively affect query performance and data update operations. The normalization principle (see below) is used to eliminate redundancy in database models.\n\nAbove, we see an example of unwanted redundancy in a data model: the column (dependent on ) is in both the table and the table. This creates the risk that it contains different information in each table.\n\nAn exception to the principle of non-redundancy occurs in dimensional schemas, which are used for analytical processing and data warehousing. In dimensional schemas, a certain degree of data redundancy can be used to reduce complexity in combined queries. Otherwise, such queries would be more resource intensive.\n\nAbove we have an example of a dimensional schema. The columns and are repeated in the table to avoid the need to join tables when querying these columns.\n\nEvery table must have a primary key. The primary key is essential because it guarantees the uniqueness of each row within the table. In addition, the primary key is used to establish relationships with other tables in the database.\n\nWithout a primary key, a table would have no reliable way to identify individual records. This can lead to data integrity problems, issues with query accuracy, and difficulties in updating that table. If you leave a table without a primary key in your schema, you run the risk of that table containing duplicate rows that will cause incorrect query results and application errors.\n\nOn the other hand, primary keys make it easier to interpret your data model. By seeing the primary keys of every table in an entity-relationship diagram (ERD), the programmer writing a query will know how to access each table and how to join it with others.\n\nHaving a primary key in each table ensures that relationships can be maintained between tables.\n\nIn relational databases, null values indicate unknown, missing, or non-applicable data. When defining each column in a table, you must establish whether it supports null values. You should only allow the column to support null values if you’re certain that, at some point, the value of that column may be unknown, missing, or not applicable.\n\nIt is also important to differentiate null values from “empty” values, such as the number zero or a zero-length string. Read these tips on how to make good use of nullable columns for more information.\n• A null value can be applied to columns of any data type (as long as the column supports null values).\n• Null values are ignored in unique and foreign key constraints.\n• In SQL, a null value is different from any value – even from another null value.\n• Any SQL operation involving a null value will result in another null value. The exception is mathematical aggregate functions like SUM(), where null values are treated as zeros.\n\nReferential integrity guarantees that columns involved in the relationship between two tables (e.g. primary and foreign keys) will have shared values. This means that the values of the columns in the secondary (child or dependent) table must exist in the corresponding columns of the primary (parent) table.\n\nFurthermore, referential integrity requires that such column values are unique in the primary table – ideally, they should constitute the primary key of the primary table. In the secondary table, the columns involved in the relationship must constitute a foreign key. Read What Is a Foreign Key? for more information.\n\nBy establishing relationships between tables using foreign keys and primary keys, we make use of the database engine’s resources to ensure data integrity. This also improves the performance of queries and other database operations. Foreign and primary keys facilitate the creation of indexes to speed up table lookups; we’ll discuss this more in the indexing section of this article.\n\nIn this example, referential integrity ensures that each booking is associated with a passenger and a room. It also ensures a room type is specified for each room.\n\nSometimes, you may feel tempted to have a single column for complex or compound data. For example, the table below stores complete addresses and full names in single fields:\n\nIn this example, the column clearly violates the atomicity principle, since it stores composite data that could be divided into smaller pieces. The field also violates this principle, as it stores first and last names in the same value.\n\nIf you combine more than one piece of information in one column, it will be difficult to access individual data later. For example, what if you wanted to address the customer by their first name or see which customers live in the state of Oklahoma? The query would get very complicated!\n\nTry to divide the information into logical parts; in our example, you could create separate fields for first name and last name and for address, city, state, and postal code. This principle is made explicit in the first normal form, which we’ll discuss next.\n\nIn any data model for transactional applications or processes – such as an online banking or e-commerce site – it is crucial to avoid anomalies in the processes of inserting, updating, or deleting data.\n\nThis is where normalization techniques are applied; they seek to eliminate clutter, disorganization, and inconsistencies in data models. Normalization is a formal method for correcting data models that you can intuitively sense, just by looking at them, that there is something wrong. Below, the orange table is an example of a non-normalized model:\n\nAny experienced database designer will quickly observe that the orange table has many defects that could be solved by normalization. This would result in the green (normalized) tables on the right.\n\nIn practice, normalizing a model is a matter of bringing it to the third normal form (3NF). Bringing a data model to 3NF maintains data integrity, reduces redundancy, and optimizes the storage space occupied by the database. The lower normal forms (1NF and 2NF) are intermediate steps towards 3NF, while the higher normal forms (4NF and 5NF) are rarely used. Read our article on normalization and the three normal forms for more information.\n\nWhen designing a schema, you must choose the appropriate data type for each column of each table. You’ll choose a data type according to the nature and format of the information expected to be stored in that column.\n\nIf, for example, you are creating a column where telephone numbers will be stored, you could associate a numeric data type to it (such as INT) if it will only store numbers. But, if it must also store other characters – such as parentheses, hyphens, or spaces – the data type should be VARCHAR.\n\nOn the other hand, if you create a column to store dates, common database modeling tips suggest that the data type should be DATE, DATETIME, or TIMESTAMP. If you define this column as VARCHAR, dates can be stored in very different formats (e.g. '18-Jan-2023', '2023-01-18', '01/18/23'); it will be impossible to use this column as a search criterion or as a filter for queries.\n\nAn example of a VARCHAR column used to store dates.\n\nOn the other hand, the data type of a column must support the entire universe of possible data that can be assigned to that column, either by users or by applications.\n\nIf you create a column to store product prices, for example, you must make sure that it supports the maximum number of integers and decimals with which those prices can be expressed. There are no approximate sizes in this. If you define the data type with less capacity than necessary, you will cause errors in the use of the database. And if you define it with more capacity than necessary, you will be wasting storage space and opening the door to performance problems.\n\nIndexes are data structures that make it easy for the database engine to locate the row(s) that meet a given search criteria. Each index is associated with a table and contains the values of one or more columns of that table. Read our article What Is a Database Index? for more information.\n\nSince indexes are created mainly to speed up searches and queries, we don’t always know which indexes should be created during the database design process. This is why index creation is often part of database maintenance rather than design.\n\nHowever, database designers can anticipate the needs of the applications that will access the database and include indexes that promise the greatest benefits. To determine what those indexes are, you can follow these data modeling best practices:\n• Create an index for all tables’ primary keys. This is usually done automatically by relational database management systems (RDBMSs), so you don’t usually need to go to the trouble of doing it yourself.\n• Create an index for each foreign key constraint that exists in the model of the secondary (child) table of each relationship. Some RDBMSs also do this automatically.\n• Create indexes based on use cases. Analyze how the applications will use the database to anticipate the need for indexes that include the lookup columns detailed in those cases.\n• Create indexes for fields that are frequently used for querying or filtering data, but avoid creating unnecessary or duplicate indexes.\n• Consider columns’ cardinality before you index. If you create an index for a column with low cardinality, it will rarely speed up queries. This is because it will not significantly reduce the query’s search space.\n• Prioritize “narrow” columns for indexes. Indexes take up storage space, so including very wide columns in an index (such as complete addresses, full names, product descriptions, etc.) will cause the index to take up a lot of space.\n\nThese rules are just some starting points for creating indexes. Even when your model design is finished and the database is up and running, you (or a colleague) may still need to create indexes to help maintain optimal performance.\n\nLarge schemas are difficult to read and manage when the totality of their tables exceeds the dimensions of a medium-sized poster or a couple of screens. At this point, partitioning the schema becomes necessary so that the schema can be visualized by sections.\n\nThe criterion used to partition a large schema is up to the designer and the consumers of that schema (developers, DBAs, database architects, etc.). The idea is to choose the partitioning criteria that is most useful.\n\nFor example, a schema can be partitioned according to access permissions. In this case, we’d separate tables with limited read/write permissions from those with free access for any user. Or you might partition a model by update frequency. This calls for separating tables that are updated constantly, tables that are updated occasionally, and tables that are never updated.\n\nThese ways of partitioning will be helpful both for DBAs (who need to manage storage assignment, roles, and permissions) and developers (who need to know what kind of interaction applications can have with each table).\n\nA schema that’s partitioned according to the update frequency of each table.\n\nControlling access to a system through user authentication is one of the most basic principles to prevent data misuse and promote information security.\n\nUser authentication is very familiar to all of us; it’s how a system, database, program, etc. ensures that 1) a user is who they say they are, and 2) the user only accesses the information or parts of the system they are entitled to see.\n\nA schema intended to provide authentication and access control must allow for registering new users, supporting different authentication factors, and providing options for recovering passwords. It must also protect authentication data from unauthorized users and define user permissions by roles and levels.\n\nIdeally, every schema should support these functionalities. Even if you are asked to design a database without support for security mechanisms – because, for example, it will be used by only one person – it is highly recommended that you include everything necessary to manage access restrictions. If you are asked to leave aside any form of access control to reduce costs or development time, you must make sure that the project stakeholders understand that the design you’ll create will be vulnerable to attackers.\n\nFor more details on how to create a data model that supports authentication and access control, read these Best Practices for Designing an Authentication Module.\n\nEvery database must be prepared to resist hacking and attempts to access data by unauthorized users. Even if security mechanisms such as firewalls and password policies are in place, database design is the last line of defense to protect information when all other methods fail.\n\nThere are several things that you, as a designer, can do to minimize the risks of unauthorized access to information. One of them is to provide columns that support encrypted or hashed data. String encryption and hashing techniques alter the length of character strings and the set of characters that can be allowed. When you’re defining VARCHAR columns to store data that can be encrypted or hashed, you must take into account both the maximum length and the range of characters they can have.\n\nA common belief is that deliberately using non-obvious names for objects in a database is an effective method of preventing unauthorized users from gaining access to the data. This practice (known as security by obscurity), can make an attacker's “job” somewhat more difficult. But those who know cybersecurity do not recommend this; for an experienced cybercriminal, it only represents a small obstacle to overcome.\n\nWhen it comes to naming objects in a database, it is essential to have a naming convention for the database objects and to respect it so that no table or column will be named randomly.\n\nGood design practices for security and user authentication include not storing keys, even encrypted ones. All encrypted data carries the risk of being decrypted. For this reason, hash functions that are not bijective are used to protect keys. This means that there is no way to use a hash function result to obtain the original data. Instead of storing the encrypted key, only the hash of that key is stored.\n\nA hashed key, even if it does not allow finding the original key, serves as an authentication mechanism: if the hash of a password entered during a login session matches the hash stored for the user trying to log in, then there is no doubt that the password entered is the correct one.\n\nIt is important to restrict write permissions for the table and column where a hashed password is stored. This helps prevent potential attackers from altering the stored hash to one that corresponds to a known password.\n\nThroughout this article, we have seen the importance of database design principles. We’ve made a tour through 12 fundamental principles that ensure efficient, scalable, maintainable, and secure data models. To wrap things up and to deepen your knowledge of database design principles, I recommend the following books:\n• Logical Database Design Principles (Foundations of Database Design) by John Garmany, Jeff Walker, and Terry Clark.\n• Database Principles: Fundamentals of Design, Implementation, and Management by Carlos Coronel, Steven Morris, et al.\n• Principles of Database Management: The Practical Guide to Storing, Managing and Analyzing Big and Small Data by Wilfried Lemahieu, Seppe vanden Broucke, and Bart Baesens.\n\nI hope this article inspires you to learn more about the best practices of database design. You can also check out the database design section of Vertabelo’s blog for more ideas."
    },
    {
        "link": "https://lucidchart.com/blog/database-design-best-practices",
        "document": "Designing a database the right way requires some analysis of your data and planning around how you want to structure it. There are many different database types, models, and customizations you can use to achieve your goals. Here’s how to plan your database and start designing it. How will you use your database? Today, organizations are using data as part of their business intelligence gathering in end-customer products and services, for forecasting, and to inform real-time business decisions. Databases don’t have to be digital—technically, a notebook counts—but digital databases mean you can work with Big Data and use data analytics much more effectively. In the past, much of the data that businesses collected was discarded or not used in meaningful ways to drive business decisions. Think, for instance, of all of the retailers who collected purchasing data to process in-store sales at the checkout register and didn’t have an efficient way to keep point-of-sale data or perform useful analysis of it. Now we know that data can be extraordinarily valuable for organizations, and we have more and more means of leveraging and visualizing data than ever before. What does good database design look like? How you’re using the data and knowledge your organization collects is one important consideration when you’re developing your goals. Database design is usually dictated by how you’re using your data today and how your organization plans to use it in the future. If you already have a database with existing data, then you have to consider how you’ll migrate as well. For every database use case, there are different types of databases, database software, and specific designs. The database design you use today may not fit all of your needs tomorrow. This is why databases aren’t chosen randomly but represent a carefully-researched decision at most companies.\n\nGood database design is driven by several core principles:\n• Minimize redundancy: To save resources, create an efficient database, and simplify how the database works, data redundancy is minimized and duplication is avoided.\n• Protect accuracy: Your database should keep information accurate and reduce the likelihood of accidentally damaging information.\n• Be accessible: The business intelligence systems that need reading and writing access should have it. Your database should provide access while also protecting data security.\n• Meet expectations: Of course, you keep a database to fulfill a specific purpose—so the database design must successfully support your data processing expectations. Your database should take into consideration what stakeholders in your organization need from their data. For this reason, it’s a good practice to include them in your database design process. Determining your goals for your database Who should you invite feedback from on your database design? Think about end-users within your organization, including team members, project managers, devs, and other internal stakeholders, as well as your external stakeholders such as business customers or power users. Before you get too far into mapping out your goals and beginning the design process, think about stakeholders who should be involved and how to involve them. This stakeholder involvement not only prevents possible backlash by avoiding designs that others in your organization would see as a bad fit. It also brings you more ideas, best practices, and experience to potentially draw from that can save resources and improve the outcome. Gather information to help with your decision Ask yourself some pointed questions to determine the database you need. First, though, you should start gathering information that will help you with this process and decision.\n• Forms: Collect the forms using data that will go in the database.\n• Processes: Review each process involved in collecting or processing data for the database. You’ll need to have these processes available for reference as you plan your database.\n• Types of data: Any data fields you’d gather and store in your database, such as customer contact information for a database of customers: name, email address, address, city, state, and zip code. Your data should be broken down into basic pieces, removing any complexity. Structured Query Language (SQL) allows you to interact with a database and make meaningful use of its data. Often, databases are categorized as SQL or NoSQL (Not Only SQL). NewSQL has properties of both. There are unique pros and cons to these options, so think about how your database’s characteristics enable or restrict how you use them. Otherwise known as a relational database, SQL databases are made up of tables of data along with the relationships among the data fields. These are traditional databases, and they’re popular for many different database use cases, but they’re also difficult to scale vertically. You can horizontally scale SQL databases, but this isn’t appropriate for every database use. Today, many types of data need to be stored and managed in a more streamlined way—with databases that don’t have the same requirements and expectations associated with SQL and ACID compliance. One example of where SQL gets into trouble with large-scale data is with atomicity. A relational database can’t function well without restricting “write” activity and managing it carefully with bookkeeping in the background to ensure data integrity. As you scale, these management activities can be difficult to expand and adapt, which can be a problem for certain Big Data projects. As noted earlier, it’s “not only SQL” rather than “no SQL,” so you can have a NoSQL database with some relational components that are structured with SQL. NoSQL databases run the gamut in terms of how data is stored and structured. With NoSQL, though, you do have some component of your database that’s not managed by SQL. Aside from choosing SQL or NoSQL, you need to think about the data model you’ll use:\n• Relational database: All relationships are already defined in a relational database, connecting together tables with columns and rows of data. With this type of database, you can use your data in many different ways without rearranging it. This is great for many complex use cases involving situations where you need to store data with many different relationships, such as product names along with product information.\n• Hierarchical database: A one-to-many, tree-like data structure. For a hierarchy (hence the name), hierarchical databases make a lot of sense. You could create a database with department names, and each department can be associated with a list of employees who work there.\n• Network database: Like hierarchical databases, network databases can have a parent record associated with multiple child records. Network databases can also have multiple parents associated with a single child, however, adding flexibility for some uses. If you visualize a network database, it will look something like a net or web of interconnected records.\n• Object-oriented database: This last type of database uses objects rather than tables, which relational databases use. With object-oriented databases, object-oriented programmers can purposely build the databases they need. When you’re ready to design your database, keep these best practices in mind. As you design, think about your users. Put usability at the forefront and ensure that everything is as easy and straightforward as possible for the end-user, even if that means more work for you upfront.\n• Use standardization: Stay consistent with naming conventions and avoid abbreviations. You want to create a standard and stick with it throughout your database.\n• Consider future modifications: The database is a living thing in the sense that it should be modifiable later.\n• Keep technical debt in check: Don’t leave too many potential messes for users to workaround or for future devs to resolve."
    }
]