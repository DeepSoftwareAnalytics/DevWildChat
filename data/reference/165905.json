[
    {
        "link": "https://keras.io/guides/sequential_model",
        "document": "Author: fchollet\n\n Date created: 2020/04/12\n\n Last modified: 2023/06/25\n\n Description: Complete guide to the Sequential model.\n\nWhen to use a Sequential model\n\nA model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nis equivalent to this function:\n\nA Sequential model is not appropriate when:\n• Your model has multiple inputs or multiple outputs\n• Any of your layers has multiple inputs or multiple outputs\n• You need to do layer sharing\n\nYou can create a Sequential model by passing a list of layers to the Sequential constructor:\n\nIts layers are accessible via the attribute:\n\nYou can also create a Sequential model incrementally via the method:\n\nNote that there's also a corresponding method to remove layers: a Sequential model behaves very much like a list of layers.\n\nAlso note that the Sequential constructor accepts a argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nSpecifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn't \"built\": it has no weights (and calling results in an error stating just this). The weights are created when the model first sees some input data:\n\nOnce a model is \"built\", you can call its method to display its contents:\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an object to your model, so that it knows its input shape from the start:\n\nNote that the object is not displayed as part of , since it isn't a layer:\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\n\nIn general, it's a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.\n\nWhen building a new Sequential architecture, it's useful to incrementally stack layers with and frequently print model summaries. For instance, this enables you to monitor how a stack of and layers is downsampling image feature maps:\n\nWhat to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n• Train your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\n• Save your model to disk and restore it. See our guide to serialization & saving.\n\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an and attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\nHere's a similar example that only extract features from one layer:\n\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren't familiar with it, make sure to read our guide to transfer learning.\n\nHere are two common transfer learning blueprint involving Sequential models.\n\nFirst, let's say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over and set on each layer, except the last one. Like this:\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:\n\nIf you do transfer learning, you will probably find yourself frequently using these two patterns.\n\nThat's about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n• Guide to making new Layers & Models via subclassing"
    },
    {
        "link": "https://keras.io/api/models/sequential",
        "document": "# Note that you can also omit the initial `Input`.\n\n# In that case the model doesn't have any weights until the first call\n\n# to a training/evaluation method (since it isn't yet built):\n\n# Whereas if you specify an `Input`, the model gets built\n\n# continuously as you are adding layers:\n\n# When using the delayed-build pattern (no input shape specified), you can\n\n# choose to manually build your model by calling\n\n# Note that when using the delayed-build pattern (no input shape specified),\n\n# the model gets built the first time you call `fit`, `eval`, or `predict`,\n\n# or the first time you call the model on some input data.\n\n# This builds the model for the first time:"
    },
    {
        "link": "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras",
        "document": "Keras is a powerful and easy-to-use free open source Python library for developing and evaluating deep learning models.\n\nIt is part of the TensorFlow library and allows you to define and train neural network models in just a few lines of code.\n\nIn this tutorial, you will discover how to create your first deep learning neural network model in Python using Keras.\n\nKick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\n• Update Feb/2017: Updated prediction example, so rounding works in Python 2 and 3.\n• Update Mar/2017: Updated example for the latest versions of Keras and TensorFlow.\n• Update Jul/2019: Expanded and added more useful resources.\n\nThere is not a lot of code required, but we will go over it slowly so that you will know how to create your own models in the future.\n\nThe steps you will learn in this tutorial are as follows:\n• Tie It All Together\n\nThis Keras tutorial makes a few assumptions. You will need to have:\n• Keras and a backend (Theano or TensorFlow) installed and configured\n\nIf you need help with your environment, see the tutorial:\n• How to Setup a Python Environment for Deep Learning\n\nCreate a new file called keras_first_network.py and type or copy-and-paste the code into the file as you go.\n\nThe first step is to define the functions and classes you intend to use in this tutorial.\n\nYou will use the NumPy library to load your dataset and two classes from the Keras library to define your model.\n\nThe imports required are listed below.\n\nYou can now load our dataset.\n\nIn this Keras tutorial, you will use the Pima Indians onset of diabetes dataset. This is a standard machine learning dataset from the UCI Machine Learning repository. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.\n\nAs such, it is a binary classification problem (onset of diabetes as 1 or not as 0). All of the input variables that describe each patient are numerical. This makes it easy to use directly with neural networks that expect numerical input and output values and is an ideal choice for our first neural network in Keras.\n\nThe dataset is available here:\n\nDownload the dataset and place it in your local working directory, the same location as your Python file.\n\nSave it with the filename:\n\nTake a look inside the file; you should see rows of data like the following:\n\nYou can now load the file as a matrix of numbers using the NumPy function loadtxt().\n\nThere are eight input variables and one output variable (the last column). You will be learning a model to map rows of input variables (X) to an output variable (y), which is often summarized as y = f(X).\n\nThe variables can be summarized as follows:\n• Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n\nOnce the CSV file is loaded into memory, you can split the columns of data into input and output variables.\n\nThe data will be stored in a 2D array where the first dimension is rows and the second dimension is columns, e.g., [rows, columns].\n\nYou can split the array into two arrays by selecting subsets of columns using the standard NumPy slice operator or “:”. You can select the first eight columns from index 0 to index 7 via the slice 0:8. We can then select the output column (the 9th variable) via index 8.\n\nYou are now ready to define your neural network model.\n\nNote: The dataset has nine columns, and the range 0:8 will select columns from 0 to 7, stopping before index 8. If this is new to you, then you can learn more about array slicing and ranges in this post:\n• How to Index, Slice, and Reshape NumPy Arrays for Machine Learning in Python\n\nModels in Keras are defined as a sequence of layers.\n\nWe create a Sequential model and add layers one at a time until we are happy with our network architecture.\n\nThe first thing to get right is to ensure the input layer has the correct number of input features. This can be specified when creating the first layer with the input_shape argument and setting it to for presenting the eight input variables as a vector.\n\nHow do we know the number of layers and their types?\n\nThis is a tricky question. There are heuristics that you can use, and often the best network structure is found through a process of trial and error experimentation (I explain more about this here). Generally, you need a network large enough to capture the structure of the problem.\n\nIn this example, let’s use a fully-connected network structure with three layers.\n\nFully connected layers are defined using the Dense class. You can specify the number of neurons or nodes in the layer as the first argument and the activation function using the activation argument.\n\nAlso, you will use the rectified linear unit activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer.\n\nIt used to be the case that Sigmoid and Tanh activation functions were preferred for all layers. These days, better performance is achieved using the ReLU activation function. Using a sigmoid on the output layer ensures your network output is between 0 and 1 and is easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n\nYou can piece it all together by adding each layer:\n• The model expects rows of data with 8 variables (the input_shape=(8,) argument).\n• The first hidden layer has 12 nodes and uses the relu activation function.\n• The second hidden layer has 8 nodes and uses the relu activation function.\n• The output layer has one node and uses the sigmoid activation function.\n\nNote: The most confusing thing here is that the shape of the input to the model is defined as an argument on the first hidden layer. This means that the line of code that adds the first Dense layer is doing two things, defining the input or visible layer and the first hidden layer.\n\nNow that the model is defined, you can compile it.\n\nCompiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow. The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU, GPU, or even distributed.\n\nWhen compiling, you must specify some additional properties required when training the network. Remember training a network means finding the best set of weights to map inputs to outputs in your dataset.\n\nYou must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network, and any optional metrics you want to collect and report during training.\n\nIn this case, use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as “binary_crossentropy“. You can learn more about choosing loss functions based on your problem here:\n• How to Choose Loss Functions When Training Deep Learning Neural Networks\n\nWe will define the optimizer as the efficient stochastic gradient descent algorithm “adam“. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems. To learn more about the Adam version of stochastic gradient descent, see the post:\n• Gentle Introduction to the Adam Optimization Algorithm for Deep Learning\n\nFinally, because it is a classification problem, you will collect and report the classification accuracy defined via the metrics argument.\n\nYou have defined your model and compiled it to get ready for efficient computation.\n\nNow it is time to execute the model on some data.\n\nYou can train or fit your model on your loaded data by calling the fit() function on the model.\n\nTraining occurs over epochs, and each epoch is split into batches.\n• Epoch: One pass through all of the rows in the training dataset\n• Batch: One or more samples considered by the model within an epoch before weights are updated\n\nOne epoch comprises one or more batches, based on the chosen batch size, and the model is fit for many epochs. For more on the difference between epochs and batches, see the post:\n• What is the Difference Between a Batch and an Epoch in a Neural Network?\n\nThe training process will run for a fixed number of epochs (iterations) through the dataset that you must specify using the epochs argument. You must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size, and set using the batch_size argument.\n\nThis problem will run for a small number of epochs (150) and use a relatively small batch size of 10.\n\nThese configurations can be chosen experimentally by trial and error. You want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration. This is called model convergence.\n\nThis is where the work happens on your CPU or GPU.\n\nNo GPU is required for this example, but if you’re interested in how to run large models on GPU hardware cheaply in the cloud, see this post:\n• How to Setup Amazon AWS EC2 GPUs to Train Keras Deep Learning Models\n\nYou have trained our neural network on the entire dataset, and you can evaluate the performance of the network on the same dataset.\n\nThis will only give you an idea of how well you have modeled the dataset (e.g., train accuracy), but no idea of how well the algorithm might perform on new data. This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n\nYou can evaluate your model on your training dataset using the evaluate() function and pass it the same input and output used to train the model.\n\nThis will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n\nThe evaluate() function will return a list with two values. The first will be the loss of the model on the dataset, and the second will be the accuracy of the model on the dataset. You are only interested in reporting the accuracy so ignore the loss value.\n\n6. Tie It All Together\n\nYou have just seen how you can easily create your first neural network model in Keras.\n\nLet’s tie it all together into a complete code example.\n\nYou can copy all the code into your Python file and save it as “keras_first_network.py” in the same directory as your data file “pima-indians-diabetes.csv“. You can then run the Python file as a script from your command line (command prompt) as follows:\n\nRunning this example, you should see a message for each of the 150 epochs, printing the loss and accuracy, followed by the final evaluation of the trained model on the training dataset.\n\nIt takes about 10 seconds to execute on my workstation running on the CPU.\n\nIdeally, you would like the loss to go to zero and the accuracy to go to 1.0 (e.g., 100%). This is not possible for any but the most trivial machine learning problems. Instead, you will always have some error in your model. The goal is to choose a model configuration and training configuration that achieve the lowest loss and highest accuracy possible for a given dataset.\n\nNote: If you try running this example in an IPython or Jupyter notebook, you may get an error.\n\nThe reason is the output progress bars during training. You can easily turn these off by setting verbose=0 in the call to the fit() and evaluate() functions; for example:\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWhat score did you get?\n\n Post your results in the comments below.\n\nNeural networks are stochastic algorithms, meaning that the same algorithm on the same data can train a different model with different skill each time the code is run. This is a feature, not a bug. You can learn more about this in the post:\n\nThe variance in the performance of the model means that to get a reasonable approximation of how well your model is performing, you may need to fit it many times and calculate the average of the accuracy scores. For more on this approach to evaluating neural networks, see the post:\n• How to Evaluate the Skill of Deep Learning Models\n\nFor example, below are the accuracy scores from re-running the example five times:\n\nYou can see that all accuracy scores are around 77%, and the average is 76.924%.\n\nThe number one question I get asked is:\n\nYou can adapt the above example and use it to generate predictions on the training dataset, pretending it is a new dataset you have not seen before.\n\nMaking predictions is as easy as calling the predict() function on the model. You are using a sigmoid activation function on the output layer, so the predictions will be a probability in the range between 0 and 1. You can easily convert them into a crisp binary prediction for this classification task by rounding them.\n\nAlternately, you can convert the probability into 0 or 1 to predict crisp classes directly; for example:\n\nThe complete example below makes predictions for each example in the dataset, then prints the input data, predicted class, and expected class for the first five examples in the dataset.\n\nRunning the example does not show the progress bar as before, as the verbose argument has been set to 0.\n\nAfter the model is fit, predictions are made for all examples in the dataset, and the input rows and predicted class value for the first five examples is printed and compared to the expected class value.\n\nYou can see that most rows are correctly predicted. In fact, you can expect about 76.9% of the rows to be correctly predicted based on your estimated performance of the model in the previous section.\n\nIf you would like to know more about how to make predictions with Keras models, see the post:\n• How to Make Predictions with Keras\n\nIn this post, you discovered how to create your first neural network model using the powerful Keras Python library for deep learning.\n\nSpecifically, you learned the six key steps in using Keras to create a neural network or deep learning model step-by-step, including:\n• How to define a neural network in Keras\n• How to compile a Keras model using the efficient numerical backend\n• How to train a model on data\n• How to evaluate a model on data\n• How to make predictions with the model\n\nDo you have any questions about Keras or about this tutorial?\n\n Ask your question in the comments, and I will do my best to answer.\n\nWell done, you have successfully developed your first neural network using the Keras deep learning library in Python.\n\nThis section provides some extensions to this tutorial that you might want to explore.\n• Tune the Model. Change the configuration of the model or training process and see if you can improve the performance of the model, e.g., achieve better than 76% accuracy.\n• Save the Model. Update the tutorial to save the model to a file, then load it later and use it to make predictions (see this tutorial).\n• Summarize the Model. Update the tutorial to summarize the model and create a plot of model layers (see this tutorial).\n• Separate, Train, and Test Datasets. Split the loaded dataset into a training and test set (split based on rows) and use one set to train the model and the other set to estimate the performance of the model on new data.\n• Plot Learning Curves. The fit() function returns a history object that summarizes the loss and accuracy at the end of each epoch. Create line plots of this data, called learning curves (see this tutorial).\n• Learn a New Dataset. Update the tutorial to use a different tabular dataset, perhaps from the UCI Machine Learning Repository.\n• Use Functional API. Update the tutorial to use the Keras Functional API for defining the model (see this tutorial).\n\nAre you looking for some more Deep Learning tutorials with Python and Keras?\n\nTake a look at some of these:\n• Regression Tutorial with the Keras Deep Learning Library in Python\n• How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras\n\nHow did you go? Do you have any questions about deep learning?\n\n Post your questions in the comments below, and I will do my best to help."
    },
    {
        "link": "https://tensorflow.org/guide/keras/sequential_model",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nWhen to use a Sequential model\n\nA model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n\nis equivalent to this function:\n\nA Sequential model is not appropriate when:\n• Your model has multiple inputs or multiple outputs\n• Any of your layers has multiple inputs or multiple outputs\n• You need to do layer sharing\n\nYou can create a Sequential model by passing a list of layers to the Sequential constructor:\n\nIts layers are accessible via the attribute:\n\nYou can also create a Sequential model incrementally via the method:\n\nNote that there's also a corresponding method to remove layers: a Sequential model behaves very much like a list of layers.\n\nAlso note that the Sequential constructor accepts a argument, just like any layer or model in Keras. This is useful to annotate TensorBoard graphs with semantically meaningful names.\n\nSpecifying the input shape in advance\n\nGenerally, all layers in Keras need to know the shape of their inputs in order to be able to create their weights. So when you create a layer like this, initially, it has no weights:\n\nIt creates its weights the first time it is called on an input, since the shape of the weights depends on the shape of the inputs:\n\nNaturally, this also applies to Sequential models. When you instantiate a Sequential model without an input shape, it isn't \"built\": it has no weights (and calling results in an error stating just this). The weights are created when the model first sees some input data:\n\nOnce a model is \"built\", you can call its method to display its contents:\n\nHowever, it can be very useful when building a Sequential model incrementally to be able to display the summary of the model so far, including the current output shape. In this case, you should start your model by passing an object to your model, so that it knows its input shape from the start:\n\nNote that the object is not displayed as part of , since it isn't a layer:\n\nA simple alternative is to just pass an argument to your first layer:\n\nModels built with a predefined input shape like this always have weights (even before seeing any data) and always have a defined output shape.\n\nIn general, it's a recommended best practice to always specify the input shape of a Sequential model in advance if you know what it is.\n\nWhen building a new Sequential architecture, it's useful to incrementally stack layers with and frequently print model summaries. For instance, this enables you to monitor how a stack of and layers is downsampling image feature maps:\n\nWhat to do once you have a model\n\nOnce your model architecture is ready, you will want to:\n• Train your model, evaluate it, and run inference. See our guide to training & evaluation with the built-in loops\n• Save your model to disk and restore it. See our guide to serialization & saving.\n• Speed up model training by leveraging multiple GPUs. See our guide to multi-GPU and distributed training.\n\nOnce a Sequential model has been built, it behaves like a Functional API model. This means that every layer has an and attribute. These attributes can be used to do neat things, like quickly creating a model that extracts the outputs of all intermediate layers in a Sequential model:\n\nHere's a similar example that only extract features from one layer:\n\nTransfer learning consists of freezing the bottom layers in a model and only training the top layers. If you aren't familiar with it, make sure to read our guide to transfer learning.\n\nHere are two common transfer learning blueprint involving Sequential models.\n\nFirst, let's say that you have a Sequential model, and you want to freeze all layers except the last one. In this case, you would simply iterate over and set on each layer, except the last one. Like this:\n\nAnother common blueprint is to use a Sequential model to stack a pre-trained model and some freshly initialized classification layers. Like this:\n\nIf you do transfer learning, you will probably find yourself frequently using these two patterns.\n\nThat's about all you need to know about Sequential models!\n\nTo find out more about building models in Keras, see:\n• Guide to making new Layers & Models via subclassing"
    },
    {
        "link": "https://medium.com/nerd-for-tech/ways-to-build-keras-models-198f6a643944",
        "document": "Good to be back after a long break, I had a lot of stuff going on in parallel so I took a break from this series, but I am back again with a basic but super important and interesting topic.\n\nTill now we learned about Deep learning, how can we implement it using neural networks, intuition and math behind the algorithm but now an important question arises that do we have a prescribed structure or any specific pattern to build neural network models?\n\nThe answer is Yes, there are some patterns which are to be followed while building neural networks and anyone who works with neural network knows them, if not they should definitely read this blog.\n\nKeras is an open source library used for building neural networks and using in deep learning. It allows us to build, train and evaluate the model. Two basic patterns for building models are Sequential API and Functional API models.\n\nIt is the basic and the easiest model which can be build and evaluated using keras. As the name suggests, it allows us to build models in a sequential form, in layer after layer style. Here we create a proper cell for the model which contains all the layers in the order we want for our model. This model cannot be used for random input layer and random output layer format, it follows the complete sequential order in which it is build.\n\nThis is a snippet of how a sequential model is build. This is the same model which I trained for the cifar-10 dataset in my last blog. You all can see the pattern followed while building this type of model, start with importing libraries like tensorflow and keras and calling tf.keras.Sequential. Then we have to build the desired model using our layers, such as in this example we used the Input layer with an image size of 32x32 in rgb format given as input, this is followed by the complete architecture.\n\nHere we can visualize that all layers are stacked one over other and follow a prescribed sequence and will train on the given data accordingly.\n\nIn this type of model, all inputs can be directly connected to the output layer. In this type we simply build a network with different layers and can manually declare the input and output layer,not necessary that the last layer has to be the output layer and the training will take place according to our indications of input and output layer regardless of the architecture.\n\nThis type of model is mainly used in case of Transfer learning where if we do not want to go till the last layer and revert the values before that so that that can be used in another model can be easily done, we are not required to go through the complete model again, we can just simply transfer weights.\n\nThis is an example of how we build models using Functional API on the same dataset cifar-10 with using the same architecture as used above. You can clearly see while writing the code we gave the previous layer output as the next layer input which caused in the formation of a series like pattern where layers are stacked one after other. There is no restriction in the order, you can fix any order in which you want your network to work.\n\nHere is another example of Functional API model which will easily explain what multiple input and multiple output model means. We can easily visualize that this model contains two different input layers and two output layers, also has concatenate layers in between which connects two different layers. Just a note that shapes of both layers entering in the concatenate layer should be same so that they can be concatenated. Also at the end when we defined the model, we have declared both inputs and both outputs in separate lists.\n\nThis is the formed model which easily shows us the architecture including all connections. Functional models have a vast and important role while building models which is it allows having multiple inputs and outputs which is required for complex networks helps us in optimizing our problem.\n\nSo I hope now you guys understood the concept of Sequential and Functional API and now you can build your neural network models according to your requirements but as I always advise everyone to read the official documentation properly to gain complete knowledge of any concept, I would again suggest the same here and will drop the link for keras.io where you can read the documentation for these types of models and learn in depth about them."
    },
    {
        "link": "https://networkx.org/documentation/stable/reference/algorithms/dag.html",
        "document": "Note that most of these functions are only guaranteed to work for DAGs. In general, these functions do not check for acyclic-ness, so it is up to the user to check for that.\n\nReturns all nodes having a path to in . Returns all nodes reachable from in . Returns a generator of _all_ topological sorts of the directed graph G. Generate the nodes in the unique lexicographical topological sort order. Returns True if the graph is a directed acyclic graph (DAG) or False if not. Returns a branching representing all (overlapping) paths from root nodes to leaf nodes in the given directed acyclic graph. Yields 3-node tuples that represent the v-structures in . Yields 3-node tuples that represent the colliders in . Yields 3-node tuples that represent the v-structures in ."
    },
    {
        "link": "https://mungingdata.com/python/dag-directed-acyclic-graph-networkx",
        "document": "Directed Acyclic Graphs (DAGs) are a critical data structure for data science / data engineering workflows. DAGs are used extensively by popular projects like Apache Airflow and Apache Spark.\n\nThis blog post will teach you how to build a DAG in Python with the networkx library and run important graph algorithms.\n\nOnce you're comfortable with DAGs and see how easy they are to work with, you'll find all sorts of analyses that are good candidates for DAGs. DAGs are just as important as data structures like dictionaries and lists for a lot of analyses.\n\nConsider the following DAG:\n\nroot, a, b, c, d, and e are referred to as nodes. The arrows that connect the nodes are called edges. A graph is a collection of nodes that are connected by edges. A directed acyclic graph is a special type of graph with properties that'll be explained in this post.\n\nHere's how we can construct our sample graph with the networkx library.\n\nThe directed graph is modeled as a list of tuples that connect the nodes. Remember that these connections are referred to as \"edges\" in graph nomenclature. Take another look at the graph image and observe how all the arguments to match up with the arrows in the graph.\n\nnetworkx is smart enough to infer the nodes from a collection of edges.\n\nAlgorithms let you perform powerful analyses on graphs. This blog post focuses on how to use the built-in networkx algorithms.\n\nThe shortest path between two nodes in a graph is the quickest way to travel from the start node to the end node.\n\nLet's use the shortest path algorithm to calculate the quickest way to get from root to e.\n\nYou could also go from root => a => b => d => e to get from root to e, but that'd be longer.\n\nThe method returns the longest path in a DAG.\n\nNodes in a DAG can be topologically sorted such that for every directed edge uv from node u to node v, u comes before v in the ordering.\n\nOur graph has nodes (a, b, c, etc.) and directed edges (ab, bc, bd, de, etc.). Here's a couple of requirements that our topological sort need to satisfy:\n• for ab, a needs to come before b in the ordering\n• for bc, b needs to come before c\n• for bd, b needs to come before d\n• for de, d needs to come before e\n\nLet's run the algorithm and see if all our requirements are met:\n\nObserve that a comes before b, b comes before c, b comes before d, and d comes before e. The topological sort meets all the ordering requirements.\n\nWe can check to make sure the graph is directed.\n\nWe can also make sure it's a directed acyclic graph.\n\nLet's make a graph that's directed, but not acyclic. A \"not acyclic graph\" is more commonly referred to as a \"cyclic graph\".\n\nAn acyclic graph is when a node can't reach itself. This graph isn't acyclic because nodes can reach themselves (for example 3 can take this trip 3 => 4 => 1 => 2 => 3 and arrive back at itself.\n\nDirected graphs that aren't acyclic can't be topologically sorted.\n\nLet's revisit the topological sorting requirements and examine why cyclic directed graphs can't be topologically sorted. Our graph has nodes 1, 2, 3, 4 and directed edges 12, 23, 34, and 41. Here are the requirements for topological sorting:\n• for 12, 1 needs to come before 2 in the ordering\n• for 23, 2 needs to come before 3\n• for 34, 3 needs to come before 4\n• for 41, 4 needs to come before 1\n\nThe first three requirements are easy to meet and can be satisfied with a 1, 2, 3 sorting. But the final requirement is impossible to meet. 4 needs to be before 1, but 4, 1, 2, 3 isn't possible because 3 needs to come before 4.\n\nGraph that's neither directed nor acyclic\n\nWe've been using the class to make graphs that are directed thus far. You can use the class to make undirected graphs. All the edges in an undirected graph are bidirectional, so arrows aren't needed in visual representations of undirected graphs.\n\nYou need to use different algorithms when interacting with bidirectional graphs. Stick with DAGs while you're getting started ;)\n\nA directed graph can have multiple valid topological sorts. m, n, o, p, q is another way to topologically sort this graph.\n\nIt's easy to visualized networkx graphs with matplotlib.\n\nHere's how we can visualize the first DAG from this blog post:\n\nHere's how to visualize our directed, cyclic graph.\n\nNow that you're familiar with DAGs and can see how easy they are to create and manage with networkx, you can easily start incorporating this data structure in your projects.\n\nI recently created a project called unicron that models PySpark transformations in a DAG, to give users an elegant interface for running order dependent functions. You'll be able to make nice abstractions like these when you're comfortable with the DAG data structure.\n\nCheck out this blog post on setting up a PySpark project with Poetry if you're interested in learning how to process massive datasets with PySpark and use networkx algorithms at scale."
    },
    {
        "link": "https://networkx.org/documentation/stable/auto_examples/graph/plot_dag_layout.html",
        "document": "# `multipartite_layout` expects the layer as a node attribute, so add the # Compute the multipartite_layout using the \"layer\" node attribute"
    },
    {
        "link": "https://stackoverflow.com/questions/54903222/implementing-a-dag-in-python",
        "document": "I am implementing a DAG in python. I am using a dictionary to implement the DAG. Each key represents a node in the graph. And the value associated with a key represents a set of nodes dependent on the node at that key.\n\nIs it necessary to use an orderedDict instead of a Dict for implementing the DAG. The orderedDict preserves the order of insertion of the keys. I am wondering why would one want to preserve the insertion order of nodes in the DAG when the value at each key represents a set of nodes dependent of the node at that corresponding key?"
    },
    {
        "link": "https://networkx.org/documentation/stable/tutorial.html",
        "document": "This guide can help you start working with NetworkX.\n\nThe graph can be grown in several ways. NetworkX includes many graph generator functions and facilities to read and write graphs in many formats. To get started though we’ll look at simple manipulations. You can add one node at a time, or add nodes from any iterable container, such as a list You can also add nodes along with node attributes if your container yields 2-tuples of the form : Node attributes are discussed further below. Nodes from one graph can be incorporated into another: now contains the nodes of as nodes of . In contrast, you could use the graph as a node in . The graph now contains as a node. This flexibility is very powerful as it allows graphs of graphs, graphs of files, graphs of functions and much more. It is worth thinking about how to structure your application so that the nodes are useful entities. Of course you can always use a unique identifier in and have a separate dictionary keyed by identifier to the node information if you prefer. You should not change the node object if the hash depends on its contents.\n\ncan also be grown by adding one edge at a time, or by adding any ebunch of edges. An ebunch is any iterable container of edge-tuples. An edge-tuple can be a 2-tuple of nodes or a 3-tuple with 2 nodes followed by an edge attribute dictionary, e.g., . Edge attributes are discussed further below. There are no complaints when adding existing nodes or edges. For example, after removing all nodes and edges, we add new nodes/edges and NetworkX quietly ignores any that are already present. At this stage the graph consists of 8 nodes and 3 edges, as can be seen by: The order of adjacency reporting (e.g., , , ) is the order of edge addition. However, the order of G.edges is the order of the adjacencies which includes both the order of the nodes and each node’s adjacencies. See example below:\n\nWe can examine the nodes and edges. Four basic graph properties facilitate reporting: , , and . These are set-like views of the nodes, edges, neighbors (adjacencies), and degrees of nodes in a graph. They offer a continually updated read-only view into the graph structure. They are also dict-like in that you can look up node and edge data attributes via the views and iterate with data attributes using methods , . If you want a specific container type instead of a view, you can specify one. Here we use lists, though sets, dicts, tuples and other containers may be better in other contexts. # the number of edges incident to 1 One can specify to report the edges and degree from a subset of all nodes using an nbunch. An nbunch is any of: (meaning all nodes), a node, or an iterable container of nodes that is not itself a node in the graph.\n\nWhat to use as nodes and edges# You might notice that nodes and edges are not specified as NetworkX objects. This leaves you free to use meaningful items as nodes and edges. The most common choices are numbers or strings, but a node can be any hashable object (except ), and an edge can be associated with any object using . As an example, and could be protein objects from the RCSB Protein Data Bank, and could refer to an XML record of publications detailing experimental observations of their interaction. We have found this power quite useful, but its abuse can lead to surprising behavior unless one is familiar with Python. If in doubt, consider using to obtain a more traditional graph with integer labels.\n\nThe class provides additional methods and properties specific to directed edges, e.g., , , , etc. To allow algorithms to work with both classes easily, the directed versions of is equivalent to while reports the sum of and even though that may feel inconsistent at times. Some algorithms work only for directed graphs and others are not well defined for directed graphs. Indeed the tendency to lump directed and undirected graphs together is dangerous. If you want to treat a directed graph as undirected for some measurement you should probably convert it using or with\n\nNetworkX can be configured to use separate thrid-party backends to improve performance and add functionality. Backends are optional, installed separately, and can be enabled either directly in the user’s code or through environment variables. Several backends are available to accelerate NetworkX–often significantly–using GPUs, parallel processing, and other optimizations, while other backends add additional features such as graph database integration. Multiple backends can be used together to compose a NetworkX runtime environment optimized for a particular system or use case. Refer to the Backends section to see a list of available backends known to work with the current stable release of NetworkX. NetworkX uses backends by dispatching function calls at runtime to corresponding functions provided by backends, either automatically via configuration variables, or explicitly by hard-coded arguments to functions. Automatic dispatch is possibly the easiest and least intrusive means by which a user can use backends with NetworkX code. This technique is useful for users that want to write portable code that runs on systems without specific backends, or simply want to use backends for existing code without modifications. The example below configures NetworkX to automatically dispatch to a backend named for all NetworkX functions that supports.\n• None If does not support a NetworkX function used by the application, the default NetworkX implementation for that function will be used.\n• None If is not installed on the system running this code, an exception will be raised. # runs using backend from NETWORKX_BACKEND_PRIORITY, if set The equivalent configuration can be applied to NetworkX directly to the code through the NetworkX global parameters, which may be useful if environment variables are not suitable. This will override the corresponding environment variable allowing backends to be enabled programatically in Python code. However, the tradeoff is slightly less portability as updating the backend specification may require a small code change instead of simply updating an environment variable. Automatic dispatch using the environment variable or the global config also allows for the specification of multiple backends, ordered based on the priority which NetworkX should attempt to dispatch to. The following examples both configure NetworkX to dispatch functions first to if it supports the function, then if does not, then finally the default NetworkX implementation if no backend specified can handle the call. NetworkX includes debug logging calls using Python’s standard logging mechanism. These can be enabled to help users understand when and how backends are being used. To enable debug logging only in NetworkX modules: or to enable it globally: Backends can also be used explicitly on a per-function call basis by specifying a backend using the keyword argument. This technique not only requires that the backend is installed, but also requires that the backend implement the function, since NetworkX will not fall back to the default NetworkX implementation if a backend is specified with . This is possibly the least portable option, but has the advantage that NetworkX will raise an exception if cannot be used, which is useful for users that require a specific implementation. Explicit dispatch can also provide a more interactive experience and is especially useful for demonstrations, experimentation, and debugging. The NetworkX dispatcher allows users to use backends for NetworkX code in very specific ways not covered in this tutorial. Refer to the Backends reference section for details on topics such as:\n• None Control of how specific function types (algorithms vs. generators) are dispatched to specific backends\n• None Details on automatic conversions to/from backend and NetworkX graphs for dispatch and fallback"
    }
]