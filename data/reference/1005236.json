[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html",
        "document": "Standardize features by removing the mean and scaling to unit variance.\n\nThe standard score of a sample is calculated as:\n\nwhere is the mean of the training samples or zero if , and is the standard deviation of the training samples or one if .\n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using .\n\nStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nis sensitive to outliers, and the features may scale differently from each other in the presence of outliers. For an example visualization, refer to Compare StandardScaler with other scalers.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing to avoid breaking the sparsity structure of the data.\n\nRead more in the User Guide.\n\nIf False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. If True, scale the data to unit variance (or equivalently, unit standard deviation). scale_ ndarray of shape (n_features,) or None Per feature relative scaling of the data to achieve zero mean and unit variance. Generally this is calculated using . If a variance is zero, we can’t achieve unit variance, and the data is left as-is, giving a scaling factor of 1. is equal to when . mean_ ndarray of shape (n_features,) or None The mean value for each feature in the training set. Equal to when and . var_ ndarray of shape (n_features,) or None The variance for each feature in the training set. Used to compute . Equal to when and . Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings. The number of samples processed by the estimator for each feature. If there are no missing samples, the will be an integer, otherwise it will be an array of dtype int. If are used it will be a float (if no missing data) or an array of dtype float that sums the weights seen so far. Will be reset on new calls to fit, but increments across calls.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform.\n\nWe use a biased estimator for the standard deviation, equivalent to . Note that the choice of is unlikely to affect model performance.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/preprocessing.html",
        "document": "The package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n\nIn general, many learning algorithms such as linear models benefit from standardization of the data set (see Importance of Feature Scaling). If some outliers are present in the set, robust scalers or other transformers can be more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in Compare the effect of different scalers on data with outliers.\n\nStandardization, or mean removal and variance scaling# Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation. For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) may assume that all features are centered around zero or have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. The module provides the utility class, which is a quick and easy way to perform the following operation on an array-like dataset: Scaled data has zero mean and unit variance: This class implements the API to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. This class is hence suitable for use in the early steps of a : It is possible to disable either centering or scaling by either passing or to the constructor of . An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using or , respectively. The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data. Here is an example to scale a toy data matrix to the range: The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data: It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data: If is given an explicit the full formula is: works in a very similar fashion, but scales in a way that the training data lies within the range by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data. Here is how to use the toy data from the previous example with this scaler: Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales. was specifically designed for scaling sparse data, and is the recommended way to go about this. However, can accept matrices as input, as long as is explicitly passed to the constructor. Otherwise a will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. cannot be fitted to sparse inputs, but you can use the method on sparse inputs. Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see and ). Any other sparse input will be converted to the Compressed Sparse Rows representation. To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream. Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the method of sparse matrices is another option. If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use as a drop-in replacement instead. It uses more robust estimates for the center and range of your data. Further discussion on the importance of centering and scaling data is available on this FAQ: Should I normalize/standardize/rescale the data? It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features. To address this issue you can use with to further remove the linear correlation across features. If you have a kernel matrix of a kernel \\(K\\) that computes a dot product in a feature space (possibly implicitly) defined by a function \\(\\phi(\\cdot)\\), a can transform the kernel matrix so that it contains inner products in the feature space defined by \\(\\phi\\) followed by the removal of the mean in that space. In other words, computes the centered Gram matrix associated to a positive semidefinite kernel \\(K\\). We can have a look at the mathematical formulation now that we have the intuition. Let \\(K\\) be a kernel matrix of shape computed from \\(X\\), a data matrix of shape , during the step. \\(K\\) is defined by \\(\\phi(X)\\) is a function mapping of \\(X\\) to a Hilbert space. A centered kernel \\(\\tilde{K}\\) is defined as: where \\(\\tilde{\\phi}(X)\\) results from centering \\(\\phi(X)\\) in the Hilbert space. Thus, one could compute \\(\\tilde{K}\\) by mapping \\(X\\) using the function \\(\\phi(\\cdot)\\) and center the data in this new space. However, kernels are often used because they allows some algebra calculations that avoid computing explicitly this mapping using \\(\\phi(\\cdot)\\). Indeed, one can implicitly center as shown in Appendix B in [Scholkopf1998]: \\(1_{\\text{n}_{samples}}\\) is a matrix of where all entries are equal to \\(\\frac{1}{\\text{n}_{samples}}\\). In the step, the kernel becomes \\(K_{test}(X, Y)\\) defined as: \\(Y\\) is the test dataset of shape and thus \\(K_{test}\\) is of shape . In this case, centering \\(K_{test}\\) is done as: \\(1'_{\\text{n}_{samples}}\\) is a matrix of shape where all entries are equal to \\(\\frac{1}{\\text{n}_{samples}}\\).\n\nTwo types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature. Quantile transforms put all features into the same desired distribution based on the formula \\(G^{-1}(F(X))\\) where \\(F\\) is the cumulative distribution function of the feature and \\(G^{-1}\\) the quantile function of the desired output distribution \\(G\\). This formula is using the two following facts: (i) if \\(X\\) is a random variable with a continuous cumulative distribution function \\(F\\) then \\(F(X)\\) is uniformly distributed on \\([0,1]\\); (ii) if \\(U\\) is a random variable with uniform distribution on \\([0,1]\\) then \\(G^{-1}(U)\\) has distribution \\(G\\). By performing a rank transformation, a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features. Power transforms are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution. provides a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1: This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined: This can be confirmed on a independent testing set with similar remarks: In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness. currently provides two such power transformations, the Yeo-Johnson transform and the Box-Cox transform. Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by \\(\\lambda\\), which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution: While the above example sets the option to , will apply zero-mean, unit-variance normalization to the transformed output by default. Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation. It is also possible to map data to a normal distribution using by setting . Using the earlier example with the iris dataset: Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input’s minimum and maximum — corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively — do not become infinite under the transformation.\n\nNormalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. This assumption is the base of the Vector Space Model often used in text classification and clustering contexts. The function provides a quick and easy way to perform this operation on a single array-like dataset, either using the , , or norms: The module further provides a utility class that implements the same operation using the API (even though the method is useless in this case: the class is stateless as this operation treats samples independently). This class is hence suitable for use in the early steps of a : The normalizer instance can then be used on sample vectors as any transformer: Note: L2 normalization is also known as spatial sign preprocessing. and accept both dense array-like and sparse matrices from scipy.sparse as input. For sparse input the data is converted to the Compressed Sparse Rows representation (see ) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.\n\nDiscretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models. For more advanced possibilities, in particular smooth ones, see Generating polynomial features further below. By default the output is one-hot encoded into a sparse matrix (See Encoding categorical features) and this can be configured with the parameter. For each feature, the bin edges are computed during and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as: Based on these bin intervals, is transformed as follows: The resulting dataset contains ordinal attributes which can be further used in a . Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins. implements different binning strategies, which can be selected with the parameter. The ‘uniform’ strategy uses constant-width bins. The ‘quantile’ strategy uses the quantiles values to have equally populated bins in each feature. The ‘kmeans’ strategy defines bins based on a k-means clustering procedure performed on each feature independently. Be aware that one can specify custom bins by passing a callable defining the discretization strategy to . For instance, we can use the Pandas function :\n• None Demonstrating the different strategies of KBinsDiscretizer Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the . It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice. As for the , the utility class is meant to be used in the early stages of . The method does nothing as each sample is treated independently of others: It is possible to adjust the threshold of the binarizer: As for the class, the preprocessing module provides a companion function to be used when the transformer API is not necessary. Note that the is similar to the when , and when the bin edge is at the value . and accept both dense array-like and sparse matrices from scipy.sparse as input. For sparse input the data is converted to the Compressed Sparse Rows representation (see ). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.\n\nOften it’s useful to add complexity to a model by considering nonlinear features of the input data. We show two possibilities that are both based on polynomials: The first one uses pure polynomials, the second one uses splines, i.e. piecewise polynomials. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented in : The features of X have been transformed from \\((X_1, X_2)\\) to \\((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\\). In some cases, only interaction terms among features are required, and it can be gotten with the setting : The features of X have been transformed from \\((X_1, X_2, X_3)\\) to \\((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\\). Note that polynomial features are used implicitly in kernel methods (e.g., , ) when using polynomial Kernel functions. See Polynomial and Spline interpolation for Ridge regression using created polynomial features. Another way to add nonlinear terms instead of pure polynomials of features is to generate spline basis functions for each feature with the . Splines are piecewise polynomials, parametrized by their polynomial degree and the positions of the knots. The implements a B-spline basis, cf. the references below. The treats each feature separately, i.e. it won’t give you interaction terms. Some of the advantages of splines over polynomials are:\n• None B-splines are very flexible and robust if you keep a fixed low degree, usually 3, and parsimoniously adapt the number of knots. Polynomials would need a higher degree, which leads to the next point.\n• None B-splines do not have oscillatory behaviour at the boundaries as have polynomials (the higher the degree, the worse). This is known as Runge’s phenomenon.\n• None B-splines provide good options for extrapolation beyond the boundaries, i.e. beyond the range of fitted values. Have a look at the option .\n• None B-splines generate a feature matrix with a banded structure. For a single feature, every row contains only non-zero elements, which occur consecutively and are even positive. This results in a matrix with good numerical properties, e.g. a low condition number, in sharp contrast to a matrix of polynomials, which goes under the name Vandermonde matrix. A low condition number is important for stable algorithms of linear models. The following code snippet shows splines in action: As the is sorted, one can easily see the banded matrix output. Only the three middle diagonals are non-zero for . The higher the degree, the more overlapping of the splines. Interestingly, a of is the same as with and if .\n• None Eilers, P., & Marx, B. (1996). Flexible Smoothing with B-splines and Penalties. Statist. Sci. 11 (1996), no. 2, 89–121.\n• None Perperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. A review of spline function procedures in R. BMC Med Res Methodol 19, 46 (2019)."
    },
    {
        "link": "https://scikit-learn.org/0.24/modules/generated/sklearn.preprocessing.StandardScaler.html",
        "document": "Standardize features by removing the mean and scaling to unit variance\n\nThe standard score of a sample is calculated as:\n\nwhere is the mean of the training samples or zero if , and is the standard deviation of the training samples or one if .\n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using .\n\nStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing to avoid breaking the sparsity structure of the data.\n\nRead more in the User Guide.\n\nIf False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. If True, scale the data to unit variance (or equivalently, unit standard deviation). scale_ ndarray of shape (n_features,) or None Per feature relative scaling of the data to achieve zero mean and unit variance. Generally this is calculated using . If a variance is zero, we can’t achieve unit variance, and the data is left as-is, giving a scaling factor of 1. is equal to when . mean_ ndarray of shape (n_features,) or None The mean value for each feature in the training set. Equal to when . var_ ndarray of shape (n_features,) or None The variance for each feature in the training set. Used to compute . Equal to when . The number of samples processed by the estimator for each feature. If there are no missing samples, the will be an integer, otherwise it will be an array of dtype int. If are used it will be a float (if no missing data) or an array of dtype float that sums the weights seen so far. Will be reset on new calls to fit, but increments across calls.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform.\n\nWe use a biased estimator for the standard deviation, equivalent to . Note that the choice of is unlikely to affect model performance.\n\nFor a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py.\n\nCompute the mean and std to be used for later scaling. Fit to data, then transform it. Get parameters for this estimator. Scale back the data to the original representation Online computation of mean and std on X for later scaling. Set the parameters of this estimator."
    },
    {
        "link": "https://digitalocean.com/community/tutorials/standardscaler-function-in-python",
        "document": "Hello, readers! In this article, we will be focusing on one of the most important pre-processing techniques in Python - Standardization using StandardScaler() function.\n\nSo, let us begin!!\n\nBefore getting into Standardization, let us first understand the concept of Scaling.\n\nScaling of Features is an essential step in modeling the algorithms with the datasets. The data that is usually used for the purpose of modeling is derived through various means such as:\n\nSo, the data obtained contains features of various dimensions and scales altogether. Different scales of the data features affect the modeling of a dataset adversely.\n\nIt leads to a biased outcome of predictions in terms of misclassification error and accuracy rates. Thus, it is necessary to Scale the data prior to modeling.\n\nThis is when standardization comes into picture.\n\nStandardization is a scaling technique wherein it makes the data scale-free by converting the statistical distribution of the data into the below format:\n\nBy this, the entire data set scales with a zero mean and unit variance, altogether.\n\nLet us now try to implement the concept of Standardization in the upcoming sections.\n\nPython sklearn library offers us with StandardScaler() function to standardize the data values into a standard format.\n\nAccording to the above syntax, we initially create an object of the function. Further, we use along with the assigned object to transform the data and standardize it.\n\nNote: Standardization is only applicable on the data values that follows Normal Distribution.\n\nHave a look at the below example!\n• Import the necessary libraries required. We have imported sklearn library to use the StandardScaler function.\n• Load the dataset. Here we have used the IRIS dataset from sklearn.datasets library. You can find the dataset here.\n• Set an object to the StandardScaler() function.\n• Segregate the independent and the target variables as shown above.\n• Apply the function onto the dataset using the fit_transform() function.\n\nBy this, we have come to the end of this topic. Feel free to comment below, in case you come across any question.\n\nFor more posts related to Python, Stay tuned @ Python with JournalDev and till then, Happy Learning!! :)"
    },
    {
        "link": "https://datascience.stackexchange.com/questions/78489/feature-scaling-for-mlp-neural-network-sklearn",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    },
    {
        "link": "https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn",
        "document": "Finding an accurate machine learning model is not the end of the project.\n\nIn this post you will discover how to save and load your machine learning model in Python using scikit-learn.\n\nThis allows you to save your model to file and load it later in order to make predictions.\n\nKick-start your project with my new book Machine Learning Mastery With Python, including step-by-step tutorials and the Python source code files for all examples.\n• Update Jan/2017: Updated to reflect changes to the scikit-learn API in version 0.18.\n• Update Mar/2018: Added alternate link to download the dataset as the original appears to have been taken down.\n\nThis tutorial is divided into 3 parts, they are:\n\nPickle is the standard way of serializing objects in Python.\n\nYou can use the pickle operation to serialize your machine learning algorithms and save the serialized format to a file.\n\nLater you can load this file to deserialize your model and use it to make new predictions.\n\nThe example below demonstrates how you can train a logistic regression model on the Pima Indians onset of diabetes dataset, save the model to file and load it to make predictions on the unseen test set (download from here).\n\nRunning the example saves the model to finalized_model.sav in your local working directory.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nLoad the saved model and evaluating it provides an estimate of accuracy of the model on unseen data.\n\nJoblib is part of the SciPy ecosystem and provides utilities for pipelining Python jobs.\n\nIt provides utilities for saving and loading Python objects that make use of NumPy data structures, efficiently.\n\nThis can be useful for some machine learning algorithms that require a lot of parameters or store the entire dataset (like K-Nearest Neighbors).\n\nThe example below demonstrates how you can train a logistic regression model on the Pima Indians onset of diabetes dataset, saves the model to file using joblib and load it to make predictions on the unseen test set.\n\nRunning the example saves the model to file as finalized_model.sav and also creates one file for each NumPy array in the model (four additional files).\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nAfter the model is loaded an estimate of the accuracy of the model on unseen data is reported.\n\nThis section lists some important considerations when finalizing your machine learning models.\n• Python Version. Take note of the python version. You almost certainly require the same major (and maybe minor) version of Python used to serialize the model when you later load it and deserialize it.\n• Library Versions. The version of all major libraries used in your machine learning project almost certainly need to be the same when deserializing a saved model. This is not limited to the version of NumPy and the version of scikit-learn.\n• Manual Serialization. You might like to manually output the parameters of your learned model so that you can use them directly in scikit-learn or another platform in the future. Often the algorithms used by machine learning algorithms to make predictions are a lot simpler than those used to learn the parameters can may be easy to implement in custom code that you have control over.\n\nTake note of the version so that you can re-create the environment if for some reason you cannot reload your model on another machine or another platform at a later time.\n\nIn this post you discovered how to persist your machine learning algorithms in Python with scikit-learn.\n\nYou learned two techniques that you can use:\n• The joblib API for efficiently serializing Python objects with NumPy arrays.\n\nDo you have any questions about saving and loading your model?\n\n Ask your questions in the comments and I will do my best to answer them."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2023/02/how-to-save-and-load-machine-learning-models-in-python-using-joblib-library",
        "document": "How to Save and Load Machine Learning Models in Python Using Joblib Library?\n\nMachine Learning models require large datasets to get high accuracy, so in order to train a machine learning model with a large-size dataset, we also need a reasonable amount of time. So we use the joblib library to get rid of training the model again and again, instead, what we do is just train the model once and then save it using the joblib library, and then we use the same model.\n\nThis post will look at using Python’s joblib package to save and load machine learning models. For this project, Google Colab is used.\n\nJoblib is a Python library for running computationally intensive tasks in parallel. It provides a set of functions for performing operations in parallel on large data sets and for caching the results of computationally expensive functions. Joblib is especially useful for machine learning models because it allows you to save the state of your computation and resume your work later or on a different machine.\n• Understanding the importance of the Joblib library and why saving our machine learning models is useful.\n• How to use the joblib library for saving and loading our trained machine learning model?\n• Understanding the different functions that save and load models, including functions like “save” and “load.”\n\nThis article was published as a part of the Data Science Blogathon.\n\nWhy Should you Use Joblib?\n\nCompared to other techniques of storing and loading machine learning models, using Joblib has a number of benefits. Since data is stored as byte strings rather than objects, it may be stored quickly and easily in a smaller amount of space than traditional pickling. Moreover, it automatically corrects errors when reading or writing files, making it more dependable than manual pickling. Last but not least, using joblib enables you to save numerous iterations of the same model, making it simpler to contrast them and identify the most accurate one.\n\nJoblib enables multiprocessing across several machines or cores on a single machine, which enables programmers to parallelize jobs across numerous machines. This makes it simple for programmers to utilize distributed computing resources like clusters or GPUs to accelerate their model training process.\n\nImport joblib using the following code:\n\nIf the above code gives an error, you don’t have joblib installed in your environment.\n\ninstall joblib using the following code:\n\nWe will make a logistic regression model for this purpose and use the iris dataset present in sklearn. datasets.\n\nThe Iris dataset is a well-known dataset in the field of machine learning and statistics. It contains 150 observations of iris flowers and the measurements of their sepals and petals. The dataset includes 50 observations for each of three species of iris flowers (Iris setosa, Iris virginica, and Iris versicolor). The measurements included in the dataset are sepal length, sepal width, petal length, and petal width. The Iris dataset is commonly used as a benchmark for classification algorithms as it is small, well-understood, and multi-class.\n\nLogistic Regression is a type of statistical method used for binary classification problems. It is used to model the relationship between a dependent variable and one or more independent variables. Logistic regression aims to estimate the probability of an event occurring based on the values of the independent variables. The output of logistic regression is a probability between 0 and 1, which can then be thresholded to make a binary decision about the class of the event. Logistic regression is widely used in various fields, including medicine, marketing, and finance, due to its simplicity, interpretability, and ability to handle various data types and distributions. Despite its simplicity, logistic regression is a powerful tool for solving many binary classification problems and is often a good starting point for more complex machine learning models.\n\nSaving our trained machine learning model using the dump function of the joblib library.\n\nBelow given image show the current working directory before saving the model using the joblib library.\n\nBelow is the screenshot after saving the model using the joblib dump method.\n\nYou can clearly notice that after running the code: joblib.dump(reg, ‘regression_model.joblib’), a new file has been saved in the current directory as ‘regression_model.joblib’.\n\nLoading the regression_model.joblib for using it for making predictions.\n\nMaking predictions for the test dataset using our trained ML model.\n\nJoblib library is very useful when we want to use machine learning models in applications and websites.\n\nJoblib can be useful in development in several ways:\n\n1. Debugging and Profiling: It might be challenging to identify which sections of code are taking the longest to execute when creating a large application with several functions. Joblib offers simple tools for profiling the performance of your code, allowing you to locate and speed up the areas of your application that are the slowest.\n\n2. Reproducibility: Doing the same calculations several times might be time-consuming when working with huge datasets. In order to reuse the results of time-consuming computations without having to run the code again, Joblib offers a means to cache the results. By doing this, you can save time and guarantee the reproducibility of your results.\n\n3. Testing: Writing tests is crucial when creating a complex program since they ensure that the code performs as intended. Joblib offers a means to run tests concurrently so you can learn more quickly about the state of your code. This can speed up your development process and enable you to write and execute more tests in less time.\n\n4. Experimentation: Running several iterations of the code simultaneously can be useful when creating a new algorithm or testing out various strategies. Joblib offers a straightforward method for running various iterations of your code concurrently so you can rapidly compare their outcomes and determine which strategy works best.\n\nIn conclusion, Joblib can be helpful in development by offering instruments for debugging and profiling, ensuring repeatability, accelerating the testing procedure, and enabling experimentation. With the aid of these features, you can create more substantial and intricate apps with greater productivity and efficiency.\n\nThe key takeaways of this article are as follows:\n• It offers Python-based machine learning frameworks like scikit-learn and TensorFlow developers an effective approach to instantly save and load their learned models without having to redo the time-consuming and expensive process of training from scratch each time they require them.\n• It also allows developers to take advantage of parallelization techniques such as multiprocessing across multiple machines or cores on a single machine making higher performance levels achievable at lower costs.\n• So if you’re looking for an easy way to optimize your model creation and storage processes in Python, look no further than JobLib!\n\nThe media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion."
    },
    {
        "link": "https://stackoverflow.com/questions/56107259/how-to-save-a-trained-model-by-scikit-learn",
        "document": "You can use:\n\nIn the specific case of scikit-learn, it may be better to use joblib’s replacement of pickle (dump & load), which is more efficient on objects that carry large numpy arrays internally as is often the case for fitted scikit-learn estimators, but can only pickle to the disk and not to a string:"
    },
    {
        "link": "https://medium.com/@merilainen.vili/save-and-load-models-on-disk-in-scikit-learn-python-a-complete-guide-e7e743863893",
        "document": "By reading this article you will learn two methods for saving trained Scikit-Learn ML models to disk. These two methods use different libraries, which are:\n\nYou will also learn how to load these models from the disk. By doing this, you ensure that your models are preserved for future use or for sharing with other people!\n\nIf you only want the quick and easy answer in 10 seconds, here it is:\n• How to Save a Classifier to Disk: Introduction to Model Preservation\n• How to load a model from disk\n• Pros and Cons of pickle and Joblib\n• Considerations and Best Practices for Model Management\n• Common Issues and Tips for Model Management in Scikit-learn\n\n1. How to Save a Classifier to Disk: Introduction to Model Preservation\n\nCreating a ML model is merely the first part of the ML journey. Once a model is trained, tested, and fine-tuned to provide desirable predictions or classifications, it needs to be preserved.\n\nParticularly, saving a model to disk enables developers and data scientists to utilize it across different projects, share it with peers, or deploy it into production. All without the necessity to retrain it — a process that can be computationally expensive and time-consuming.\n\nThe task of saving a model, also known as model serialization, encapsulates the model into a format suitable for storage or transmission.\n\nIn the next sections we will learn techniques for saving a Scikit-Learn classifier. We look at two different methods, using Pickle and Joblib. Both are popular libraries in Python for object serialization.\n\nBefore we get to saving a model, let’s train a simple classifier using Scikit-Learn. Below is a basic example where we’ll use the famous Iris dataset to train a decision tree classifier.\n\nIn the code snippet above:\n• loads the Iris dataset, widely used for classification tasks.\n• creates a decision tree classifier, which is then fitted with data using method.\n\nPickle is a module in Python standard library that allows objects to be serialized to a byte stream and de-serialized back.\n\nSerializing a model (or classifier) means converting it into a format that can be stored or transferred, and subsequently, de-serializing it means recovering the original model from the stored format.\n\nHere’s a straightforward guide on how to use Pickle to save your Scikit-Learn classifier to disk:\n\nPython’s Pickle module should be available by default as it is part of the standard library. There’s typically no need to install it separately. You can simply import it to get started:\n\nOnce we have a trained model, we can use Pickle to save it to disk. Here’s how we can do this:\n• opens a file in write-binary mode, where is the file name and indicates that the file should be opened in binary mode for writing.\n• writes the byte stream to the file, effectively saving the classifier to disk.\n\nIf you have saved the aforementioned code to a file called “ ”, and then run the code in this file, you should now have the following files in your folder:\n\nAs you can see, the trained classifier is now saved in the file “ ”.\n\nNow we have learned how to train a basic decision tree classifier using Scikit-Learn and save it to disk using Pickle. The saved model can be later loaded and utilized to make predictions.\n\nIn subsequent sections, we’ll explore how to load the saved model back into your Python environment.\n\nLet’s next check out how to do the same thing, but with a different library called Joblib:\n\nWhile Pickle provides general utilities for serialization in Python, is particularly optimized for efficiently storing large numpy arrays, making it more suitable for models that need to store a large amount of data, which is often the case with Scikit-Learn models.\n\nBefore utilizing joblib, ensure that it is installed in your Python environment. You can do this through the command line using the Python package installer .\n\nFor a direct check, you might use followed by the package name to get information about that specific package, if installed:\n\nIf is installed, this command will display a summary, including the version number and other details about the package. If it's not installed, there will be no output, and you can install it using :\n\nOnce joblib is installed, we can proceed to save our previously trained classifier (as per the example in the previous section) using joblib. Below is a simple example:\n• : This function takes two arguments. The first argument, , is the trained model we wish to save. The second argument is the name we want to give to our saved model file. In this case, will be the name of the file where the model is saved.\n\nIf you have saved the aforementioned code to a file called “ ”, and then run the code in this file, you should now have the following files in your folder:\n\nAs you can see, the trained classifier is now saved in the file “ ”.\n\nNote on Efficiency: Joblib is often more efficient than Pickle when dealing with large numpy arrays (common in Scikit-learn models) because it stores them in a binary format and doesn’t need to serialize them. The reduction in serialization time and disk space usage can be quite significant with large models.\n\nSimilar to Pickle, when using Joblib to save models, it’s advisable to be wary of compatibility issues, especially when sharing models between different platforms or Python versions.\n\nIn the next section, we will explore how to load the saved model and leverage it to make predictions.\n\n2. How to load a model from disk\n\nThis section provides a concise guide to retrieving and utilizing stored machine learning models using both Pickle and Joblib.\n\nTo load the saved model, we need to read the stored byte stream from the file and deserialize it back into a Python object. Here’s how this can be done:\n• : Here, is used to open the file where the model is saved, ‘ ‘, in read-binary mode ( ). Using creates a context (via context management) in which the subsequent code block is run. Once the block is exited, is automatically closed, ensuring clean management of system resources.\n• : This function deserializes the byte stream from the file, converting it back into a Python object, in this case, the saved classifier. This object is assigned to the variable for further use.\n\nNote on Compatibility and Security:\n\nEnsure that the model is loaded with the same version of Scikit-Learn with which it was saved to prevent incompatibilities. Moreover, it is imperative to note that can execute arbitrary code. Only load models from trusted sources to mitigate security risks.\n\nOnce the classifier is loaded, it can be utilized just like the original trained model for making predictions or further training:\n\nHere, is used to make predictions using the loaded model, and is an example input to the model, representing (in this case random) feature values. With the aforementioned steps for training, saving, and loading with pickle, and this code, you should get the following output:\n\nLoading a model that was saved with Joblib involves reading the file from disk and deserializing it back into a Python object. Below is a simple guide:\n• : This function is utilized to read and deserialize the model saved in ‘ ‘. The resulting object, which is our trained classifier, is assigned to . Joblib takes care of handling NumPy arrays efficiently and is particularly well-suited for deserializing large models.\n\nEnsure to use the same version of Scikit-Learn and related dependencies while loading the model as were used during saving to prevent potential issues related to model compatibility. In terms of security, while Joblib does not execute arbitrary code during loading (like Pickle) it’s always wise to load models only from trusted sources to maintain the integrity of your applications and data.\n\nThe can now be used for making predictions or any further operations as needed:\n\nAgain, is used to make predictions using the loaded model. I’m just using some random values as the input for the model as an example.\n\nWith the aforementioned steps for training, saving, and loading with joblib, and this code, you should get the following output:\n\n3. Pros and Cons of pickle and Joblib\n\nPickle and Joblib present two approaches for saving and loading Scikit-Learn machine learning models. While both offer convenience and are widely utilized, they exhibit distinctive advantages and downsides for varying use-cases.\n• Universality: Being Python’s intrinsic object persistence mechanism, pickle has the ability to serialize a broad range of Python objects.\n• Ease of Use: With its incorporation into Python and a straightforward API, pickle is user-friendly and requires no additional installations.\n• Efficiency: Pickle does not excel in efficiency when dealing with objects heavily reliant on NumPy arrays, a common scenario in scikit-learn.\n• Security Concerns: Pickle poses security risks as it can execute arbitrary code during loading. Always be wary of unpickling data from unverified sources.\n• Compatibility Issues: Pickle lacks compatibility assurance between different Python versions, sometimes hindering the interchangeability of pickle files among them.\n• Efficiency: Tailored for objects utilizing NumPy data structures, joblib presents a more efficient alternative to pickle in contexts involving sizable NumPy arrays.\n• Disk Usage: Joblib is adept at managing disk usage, particularly with large NumPy arrays, often outperforming pickle in this regard.\n• Compressed Serialization: Joblib provides built-in functionalities for compressing serialized objects, aiding in further disk space conservation.\n• Scope: Joblib may not be as universal as pickle for other object types.\n• Installation: Joblib requires a separate installation as it is not an in-built Python feature.\n• Compatibility: Similar to pickle, joblib does not ensure backward compatibility, causing potential issues when attempting to load a model saved with a different version.\n\nNote on Security: Regardless of the method, never unpickle/load data from untrusted sources due to the risk of executing malicious code.\n\nIn summation, while Pickle and Joblib both offer valuable functionalities for serializing Python objects, the specific use-case, data structure characteristics, and model deployment contexts should dictate the optimal choice. Special attention should also be paid to maintaining security and facilitating future usability when saving and sharing models.\n\n4. Considerations and Best Practices for Model Management\n• Metadata Management: Preserving additional metadata such as training data, Python source code, version details of scikit-learn and dependencies, and cross-validation scores is imperative. This facilitates verifying consistency in performance metrics and supports rebuilding models with future scikit-learn versions.\n• Deployment: Often, models are deployed in production environments using containers (e.g., Docker) to encapsulate and preserve the execution environment and dependencies, enhancing portability and mitigating issues related to version inconsistencies.\n• Optimization Through Data Type Selection: Opt for efficient data types to minimize your models’ size. For example, using 32-bit or 16-bit data types in place of 64-bit ones, when applicable, can reduce memory usage without sacrificing performance, thus improving load times and operational efficiency.\n• Implement Model Optimization Strategies: Explore techniques such as model pruning and quantization to create more efficient models. Pruning involves removing non-essential weights, while quantization decreases parameter precision, both of which can reduce model size and improve loading times without substantially impacting performance.\n• Employ Compression Techniques for Models: Utilize libraries like joblib and pickle to compress your models when storing them, significantly minimizing the required disk space. This is especially valuable for large models or in scenarios where storage space is a crucial factor.\n• Employ Parallel Loading: When dealing with large or multiple models, parallel loading can be an effective strategy to minimize loading time. The feasibility of this approach is subject to your system’s I/O capabilities.\n• Take Advantage of Hardware Accelerators: Ensure that your models are configured to leverage hardware accelerators like GPUs or TPUs, if available. Doing so can markedly enhance loading speeds and training and prediction times by utilizing the specialized resources offered by these devices.\n• Emphasis on Version Control and File Management: Effectively managing versions and files is essential to streamline development and deployment processes. Implement version control systems to track changes and manage different iterations of your models, ensuring reproducibility and efficient collaboration among developers.\n• Ensuring Model Security: It’s important to safeguard your models, especially when they contain sensitive data or are proprietary. Employ encryption for your model files to obstruct unauthorized access and always adhere to prevalent security protocols when sharing your models.\n\nThese points offer a coherent and comprehensive approach to model management, encompassing security, efficiency, and operational effectiveness.\n\n5. Common Issues and Tips for Model Management in Scikit-learn\n• Scikit-learn Updates: Test and, if necessary, retrain and save models using the updated version of scikit-learn post-upgrade to avoid compatibility issues.\n• Inconsistent Versions Warning: When an estimator, pickled with a certain scikit-learn version, is unpickled with a different version, an is raised. Ensure consistent version usage to mitigate this issue. Here's an example of handling such a warning:\n• Large File Sizes and compression: Manage and minimize storage issues and slow loading times by compressing models using joblib or pickle, which can notably reduce file sizes. Prefer using joblib for saving and loading scikit-learn models due to its efficiency with NumPy data structures.\n• Custom Functions or Objects: If your model incorporates custom functions or objects that are vital for model deserialization, ensure that those are defined or imported in the same scope when loading a model. Alternatively, upon modifying custom functions or objects, check compatibility and retrain the classifier if needed, ensuring no adverse impacts on saved models.\n• Version Control: Employ systems like Git or Weights & Biases Artifacts for tracking model changes, enabling easy rollbacks and maintaining consistency.\n• Metadata Management: Log comprehensive records of hyperparameters, preprocessing steps, and metrics. This can be done using tools like the Weights & Biases Artifacts API for example.\n• Python Version: Ensure the Python version is consistent during serialization and deserialization of models.\n• Library Versions: Maintain consistent versions of all major libraries, including NumPy and scikit-learn, during model deserialization.\n• Manual Serialization: Optionally, manually output model parameters for use in scikit-learn or other platforms, providing control and simplicity for implementing algorithms used in predictions.\n\nIncorporating these practices aids in effective model management and promotes consistent and reproducible machine learning project outcomes."
    },
    {
        "link": "https://scikit-learn.org/stable/model_persistence.html",
        "document": "After training a scikit-learn model, it is desirable to have a way to persist the model for future use without having to retrain. Based on your use-case, there are a few different ways to persist a scikit-learn model, and here we help you decide which one suits you best. In order to make a decision, you need to answer the following questions:\n• None Do you need the Python object after persistence, or do you only need to persist in order to serve the model and get predictions out of it?\n\nIf you only need to serve the model and no further investigation on the Python object itself is required, then ONNX might be the best fit for you. Note that not all models are supported by ONNX.\n\nIn case ONNX is not suitable for your use-case, the next question is:\n• None Do you absolutely trust the source of the model, or are there any security concerns regarding where the persisted model comes from?\n\nIf you have security concerns, then you should consider using skops.io which gives you back the Python object, but unlike based persistence solutions, loading the persisted model doesn’t automatically allow arbitrary code execution. Note that this requires manual investigation of the persisted file, which allows you to do.\n\nThe other solutions assume you absolutely trust the source of the file to be loaded, as they are all susceptible to arbitrary code execution upon loading the persisted file since they all use the pickle protocol under the hood.\n• None Do you care about the performance of loading the model, and sharing it between processes where a memory mapped object on disk is beneficial?\n\nIf yes, then you can consider using joblib. If this is not a major concern for you, then you can use the built-in module.\n• None Did you try or and found that the model cannot be persisted? It can happen for instance when you have user defined functions in your model.\n\nIf yes, then you can use cloudpickle which can serialize certain objects which cannot be serialized by or .\n\nIn a typical workflow, the first step is to train the model using scikit-learn and scikit-learn compatible libraries. Note that support for scikit-learn and third party estimators varies across the different persistence methods. Creating an appropriate model depends on your use-case. As an example, here we train a on the iris dataset: Once the model is trained, you can persist it using your desired method, and then you can load the model in a separate environment and get predictions from it given input data. Here there are two major paths depending on how you persist and plan to serve the model:\n• None ONNX: You need an runtime and an environment with appropriate dependencies installed to load the model and use the runtime to get predictions. This environment can be minimal and does not necessarily even require Python to be installed to load the model and compute predictions. Also note that typically requires much less RAM than Python to compute predictions from small models.\n• None , , , cloudpickle: You need a Python environment with the appropriate dependencies installed to load the model and get predictions from it. This environment should have the same packages and the same versions as the environment where the model was trained. Note that none of these methods support loading a model trained with a different version of scikit-learn, and possibly different versions of other dependencies such as and . Another concern would be running the persisted model on a different hardware, and in most cases you should be able to load your persisted model on a different hardware.\n\n, or Open Neural Network Exchange format is best suitable in use-cases where one needs to persist the model and then use the persisted artifact to get predictions without the need to load the Python object itself. It is also useful in cases where the serving environment needs to be lean and minimal, since the runtime does not require . is a binary serialization of the model. It has been developed to improve the usability of the interoperable representation of data models. It aims to facilitate the conversion of the data models between different machine learning frameworks, and to improve their portability on different computing architectures. More details are available from the ONNX tutorial. To convert scikit-learn model to sklearn-onnx has been developed. However, not all scikit-learn models are supported, and it is limited to the core scikit-learn and does not support most third party estimators. One can write a custom converter for third party or custom estimators, but the documentation to do that is sparse and it might be challenging to do so. To convert the model to format, you need to give the converter some information about the input as well, about which you can read more here: You can load the model in Python and use the runtime to get predictions:\n\n(and and by extension), has many documented security vulnerabilities by design and should only be used if the artifact, i.e. the pickle-file, is coming from a trusted and verified source. You should never load a pickle file from an untrusted source, similarly to how you should never execute code from an untrusted source. Also note that arbitrary computations can be represented using the format, and it is therefore recommended to serve models using in a sandboxed environment to safeguard against computational and memory exploits. Also note that there are no supported ways to load a model trained with a different version of scikit-learn. While using , , , or cloudpickle, models saved using one version of scikit-learn might load in other versions, however, this is entirely unsupported and inadvisable. It should also be kept in mind that operations performed on such data could give different and unexpected results, or even crash your Python process. In order to rebuild a similar model with future versions of scikit-learn, additional metadata should be saved along the pickled model:\n• None The training data, e.g. a reference to an immutable snapshot\n• None The Python source code used to generate the model\n• None The versions of scikit-learn and its dependencies\n• None The cross validation score obtained on the training data This should make it possible to check that the cross-validation score is in the same range as before. Aside for a few exceptions, persisted models should be portable across operating systems and hardware architectures assuming the same versions of dependencies and Python are used. If you encounter an estimator that is not portable, please open an issue on GitHub. Persisted models are often deployed in production using containers like Docker, in order to freeze the environment and dependencies. If you want to know more about these issues, please refer to these talks:\n• None Adrin Jalali: Let’s exploit pickle, and skops to the rescue! | PyData Amsterdam 2023.\n• None Alex Gaynor: Pickles are for Delis, not Software - PyCon 2014. If the versions of the dependencies used may differ from training to production, it may result in unexpected behaviour and errors while using the trained model. To prevent such situations it is recommended to use the same dependencies and versions in both the training and production environment. These transitive dependencies can be pinned with the help of package management tools like , , , , , , etc. It is not always possible to load an model trained with older versions of the scikit-learn library and its dependencies in an updated software environment. Instead, you might need to retrain the model with the new versions of the all the libraries. So when training a model, it is important to record the training recipe (e.g. a Python script) and training set information, and metadata about all the dependencies to be able to automatically reconstruct the same training environment for the updated software. When an estimator is loaded with a scikit-learn version that is inconsistent with the version the estimator was pickled with, a is raised. This warning can be caught to obtain the original version the estimator was pickled with: The last step after training a scikit-learn model is serving the model. Once the trained model is successfully loaded, it can be served to manage different prediction requests. This can involve deploying the model as a web service using containerization, or other model deployment strategies, according to the specifications.\n\nBased on the different approaches for model persistence, the key points for each approach can be summarized as follows:\n• None : It provides a uniform format for persisting any machine learning or deep learning model (other than scikit-learn) and is useful for model inference (predictions). It can however, result in compatibility issues with different frameworks.\n• None : Trained scikit-learn models can be easily shared and put into production using . It is more secure compared to alternate approaches based on because it does not load arbitrary code unless explicitly asked for by the user. Such code needs to be packaged and importable in the target Python environment.\n• None : Efficient memory mapping techniques make it faster when using the same persisted model in multiple Python processes when using . It also gives easy shortcuts to compress and decompress the persisted object without the need for extra code. However, it may trigger the execution of malicious code when loading a model from an untrusted source as any other pickle-based persistence mechanism.\n• None : It is native to Python and most Python objects can be serialized and deserialized using , including custom Python classes and functions as long as they are defined in a package that can be imported in the target environment. While can be used to easily save and load scikit-learn models, it may trigger the execution of malicious code while loading a model from an untrusted source. can also be very efficient memorywise if the model was persisted with but it does not support memory mapping.\n• None cloudpickle: It has comparable loading efficiency as and (without memory mapping), but offers additional flexibility to serialize custom Python code such as lambda expressions and interactively defined functions and classes. It might be a last resort to persist pipelines with custom Python components such as a that wraps a function defined in the training script itself or more generally outside of any importable Python package. Note that cloudpickle offers no forward compatibility guarantees and you might need the same version of cloudpickle to load the persisted model along with the same version of all the libraries used to define the model. As the other pickle-based persistence mechanisms, it may trigger the execution of malicious code while loading a model from an untrusted source."
    }
]