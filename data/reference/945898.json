[
    {
        "link": "https://medium.com/@mickael.boillaud/denoising-diffusion-model-from-scratch-using-pytorch-658805d293b4",
        "document": "I Introduction\n\n I.1 VAE\n\n I.2 GAN\n\n I.3 Why we need to get another model architecture\n\n I.4 What we are going to cover | Expected Knowledge\n\nII The goal of this blog\n\nIII DDPM Theory\n\n III.1 Forward Process\n\n III.2 Backward Diffusion Process\n\n III.3 Training Algo\n\n III.4 Sampling\n\n III.5 Before We Conclude with the Theory\n\n III.6 Conclusion\n\nIV Model Implementation\n\n IV.1 A bit about the model\n\n IV.2 ConvNext Block\n\n IV.3 Sinusoidale timestamp Embedding\n\n IV.4 DownSample & UpSample\n\n IV.5 Time Multi Layer Perceptron\n\n IV.6 Attention is always all we need\n\n IV.7 Putting all together\n\nV. Diffusion Implementation\n\nVI. Training Key Points\n\nVII. Annexe\n\nVIII. Sources\n\nBefore you begin your reading, feel free to follow me for more Deep Learning blogs and connect with me on LinkedIn to not miss any content.\n\nBefore delving into the nitty-gritty details of how the Denoising Diffusion Probabilistic Model (DDPM) works, let’s take a look at some historical perspectives on Generative AI.\n\nThe first notable model resulting from research is the VAE — Variational Autoencoder. Basically, it employs what we call an encoder, a probabilistic latent space, and a decoder. During training, the encoder predicts mean and variance for each image. These values are then sampled from a Gaussian distribution and passed into the decoder, where the inputted image is expected to be similar to the outputted one. The process involves using KL Divergence to compute the loss. One remarkable strength of VAEs lies in their capacity to generate a diverse range of images. During the sampling phase, we simply sample from a Gaussian distribution, and the decoder creates a new images.\n\nYou can find here a simple implementation of only the VAE using Keras with tensorflow backend\n\nAfter a short year of utilising Variational Autoencoders (VAEs), a groundbreaking generative family model emerged — Generative Adversarial Networks (GANs). This introduction marked the inception of a new class of generative models characterized by the collaboration of two neural networks: a generator and a discriminator, involved in an adversarial training process. The generator’s objective is to craft realistic data, such as images, from random noise, while the discriminator endeavours to differentiate between authentic and generated data. Throughout the training phase, the generator and discriminator continually refine their capabilities through a competitive learning process. The generator generates increasingly convincing data to outsmart the discriminator, which, in turn, sharpens its ability to discern between real and generated samples. This adversarial interplay culminates in the generator producing high-quality, lifelike data. In the sampling phase, following GAN training, the generator can create fresh samples by inputting random noise. It transforms this noise into data that often mirrors real examples, showcasing GANs’ proficiency in generating novel and realistic data.\n\nI.3 Why we need to get another model architecture\n\nBoth models suffer from different issues, such as the possible non-convergence of GANs. However, a crucial point propels researchers to explore further. While GANs excel in generating realistic images that closely look like the one in the training set, VAEs specialize in creating a diverse range of images, albeit with a tendency towards producing blurry ones. Yet, no existing models have successfully combined both capabilities — creating images that are both highly realistic and diverse. This challenge has posed a significant hurdle for researchers to tackle and resolve.\n\nSix years after the release of the first GAN paper, and seven years after the release of the VAE’s one, a groundbreaking model emerged: the Denoising Diffusion Probabilistic Model (DDPM). DDPM combines the strengths of both worlds, excelling in the creation of diverse and realistic images post-training.\n\nI.4 What we are going to cover | Expected Knowledge\n\nIn this article, we will delve into the intricacies of DDPM, covering its training processes, including both the forward and backward processes, and exploring how the sampling is executed. Throughout this exploration, we will construct a DDPM from scratch using PyTorch, complete with its full training pipeline.\n\nWe assume that you are already familiar with the basics of deep learning and have a solid foundation in deep computer vision. We will not be introducing these fundamental concepts. Additionally, a basic understanding of probabilities is required.\n\nTime to grab a cup of coffee, sit back, and let’s embark on this journey.\n\nII The goal of this blog\n\nNow that we’ve embarked on an introductory journey into the realm of generative models, let’s establish why we are delving into this study. The context for this exploration is rooted in my recent personal project, which also serves as my final school project. For details on personal motivations and project management, you can find the link to the blog here. Today’s task centers around the creation of deepfake aerial satellite imagery of Earth. Despite its initial complexity, fear not, as each step will be thoroughly explained in the modeling theory, complemented by code snippets. Our objective is both ambitious and thrilling — to generate images where humans are convinced of their authenticity, achieving an impressive 80% credibility across all the generated images.\n\nDataset exemple used for the task can be found in the Annexe section\n\nDenoising Diffusion Probabilistic Models (DDPM) represent a cutting-edge approach in the realm of generative models. Unlike traditional models relying on explicit likelihood functions, DDPM operates by iteratively denoising a diffusion process. This involves gradually adding noise to an image and attempting to remove that noise. The underlying theory is grounded in the idea that transforming a simple distribution, such as a Gaussian, through a series of diffusion steps can result in a complex and expressive image data distribution. In other words, by shifting the sample from its original image distribution into the Gaussian distribution, we can create a model to reverse this process. This enables us to start from the full Gaussian distribution and end up with the image distribution, effectively generating a new image.\n\nThe training of DDPM involves two essential steps: the fixed and non-learnable forward process, designed to create noisy images, and the subsequent backward process. The primary objective of the backward process is to de-noise the images using a specialised machine learning model.\n\nAs mentioned earlier, the forward process is a fixed and non-learnable step. However, it does require some pre-defined settings. Before delving into these settings, let’s understand how it works.\n\nThe core concept of the process is to start with a clear image. Over a specified number of steps denoted by ‘T,’ a small amount of noise is gradually introduced, following a Gaussian distribution. Let’s take a closer look at how this process unfolds.\n\nAs observed in the image, noise is incrementally added at each step. Now, let’s dig into the mathematical representation of this noise.\n\nAs previously mentioned, the noise is sampled from a Gaussian distribution. To introduce a small amount of noise at each step, we use a Markov Chain. This means that to generate the image at the current timestamp, we only need the image from the last timestamp. The Markov Chain concept is pivotal here and will prove crucial for the subsequent mathematical details.\n\nIt’s worth noting to mention that a Markov Chain is a stochastic process where the probability of transitioning to any particular state depends solely on the current state and time elapsed, not on the sequence of events that preceded it. This property simplifies the modelling of the noise addition process, making it more manageable for mathematical analysis.\n\nHere, the variance parameter, denoted as beta, is intentionally set to a very small value. This choice aims to introduce only a minimal amount of noise at each step.\n\nAs previously mentioned, the step parameter ‘T’ dictates the number of steps the process undergoes to generate a fully noisy image. In the paper, this parameter is set to 1000, which might appear substantial. A valid concern arises: do we truly need to create 1000 noisy images for each original image in our dataset? The Markov Chain aspect proves instrumental in addressing this concern. Since we only need the image from the last step to predict the next one, and the noise added at each step remains constant, we can streamline computations by generating the noisy image for a specific timestamp. Moreover, employing a re-parametrization trick for betas allows us to further simplify the equations.\n\nThe result is obtained through the development of equation (2) by incorporating the new parameters introduced in equation (3).\n\nNow that we have introduced noise to all the images, it’s time to perform the inverse operation. As it stands, conducting the backward process to denoise the images cannot be achieved mathematically unless we know the initial condition, i.e., the un-noised image at t = 0. However, our goal is to sample directly from the noise to create a new image, and thus, we lack information about the outcome. Consequently, we need to devise a method to denoise images step by step without prior knowledge of the outcome. What better solution than employing a deep learning model to approximate this intricate mathematical function?\n\nWith a bit of mathematical context, the model will approximate the equation (5). A noteworthy detail is that we will adhere to the DDPM original paper and keep the variance fixed, although it’s also possible to make the model learn it.\n\nAs evident, the model’s task is to predict the mean of the noise added between the current timestamp and the previous one. By doing so, we can effectively withdraw the noise, achieving the desired outcome. However, what if, in the end, we don’t want the model to predict the operation of noise added between two steps? Instead, what if our goal is for the model to predict the noise added from the “original image” to the last timestamp? While this involves a bit of mathematics, let’s delve into the reasoning behind this choice.\n\nAs discussed earlier, performing the backward process mathematically is challenging unless we know the initial image without noise. Let’s start by defining the post-variance, denoted as beta tilde, and explore potential solutions.\n\nHence, our model’s task is clear: predict the noise added from the initial image to the image at timestamp ‘t.’ Remarkably, our forward process enables us to execute this operation, starting from a clear image and progressing to a noisy image at timestamp ‘t.’ What a fortuitous coincidence!\n\nSince we will dig into and explain the reasoning behind this choice shortly, let’s assume that the model architecture employed for making predictions will be a U-Net.\n\nNow, for the training phase, our objective is to, for each image in the dataset, randomly select a timestamp within the range [0, T] and compute the Forward Diffusion Process. This yields a clear and somewhat noisy image, along with the actual noise used. Leveraging our understanding of the Backward Process, we will employ the model to predict the noise added to the image. With both the real and predicted noise available, it appears we’ve stepped into a supervised machine learning problem. This brings a sense of familiarity — a known territory.\n\nBut now, which loss function should we use to train our model? Since we are working with a probabilistic latent space, it becomes evident that the Kullback-Leibler (KL) Divergence is a suitable choice. \n\nThe KL Divergence measures the difference between two probability distributions, in our case, the distribution predicted by the model and the expected distribution. In our scenario, incorporating KL Divergence in the loss function guides the model not only to produce accurate predictions but also to ensure that the latent space representation adheres to the desired probabilistic structure.\n\nComplex mathematical details that won’t be displayed here but are available in the sources section, the KL Divergence in this case can be approximated to an L2 loss function. Consequently, we arrive at the following loss function:\n\nGreat, it seems that we have finally arrived at the training algorithm presented in the paper.\n\nNow that we have a good understanding of how we can execute the backward process, it’s time to apply it. Let’s begin with a fully random image at time T and use the backward process T times to finally arrive at time 0. This constitutes the second algorithm outlined in the paper\n\nIII.5 Before We Conclude with the Theory\n\nIt appears we have a multitude of different parameters — beta, beta tildes, alpha, alpha bar, etc. However, we haven’t yet provided information on how these parameters are chosen. The only parameter known at this point is T, which is set to 1000.\n\nFor all the listed parameters, their selection is contingent on beta. Beta, in a sense, determines the amount of noise we aim to add at each step. Consequently, to ensure the success of the algorithm, it is crucial to carefully choose values for betas.\n\nVarious methods of sampling beta were explored during the experimentation phase of the paper. The original approach, linear sampling, raised concerns due to two plateaus — one at the beginning and a big one at the end — where the images either received insufficient noise or became excessively noisy. To address this issue, an alternative, more commonly used method known as cosine sampling was adopted. Cosine sampling provides a smoother and more consistent addition of noise.\n\nWhat a journey! We’ve covered the entire diffusion process and delved into the underlying mathematics. We’ve made great progress! However, we are only halfway through our journey as we still need to implement all the knowledge we’ve gained into our IDE as Python code. Refill your coffee cup, and let’s the second round start!\n\nAs introduced in the last section, it’s no secret that we will utilize a U-Net architecture for noise prediction. The choice was driven by the fact that U-Net is an ideal architecture for image processing, capturing spatial and feature maps, and providing an output size identical to the input. The selection of U-Net came naturally.\n\nGiven the complexity of the task and the requirement to use the same model for every step (where the model needs to be capable, with the same weights, of denoising both a fully noised image and a slightly noisy image), tuning the model is essential. This involves incorporating more complex blocks and introducing an awareness of the used timestamps through a pre-sinusoidal embedding step. These enhancements aim to make the model an expert at the denoising task. We’ll explore each building block before moving on to constructing the full model. A more complex version of the model can be found in the codebase, linked to the blog. You’ll also have access to the training steps and denoising process.\n\nTo meet the need for enhancing model complexity, the fundamental convolution blocks play a crucial role. Instead of solely relying on the basic block outlined in the u-net paper, we will incorporate another inspired by the ConvNext paper. This includes utilising residual and timestamps subnets to align with the specified requirements."
    },
    {
        "link": "https://github.com/lucidrains/denoising-diffusion-pytorch",
        "document": "Implementation of Denoising Diffusion Probabilistic Model in Pytorch. It is a new approach to generative modeling that may have the potential to rival GANs. It uses denoising score matching to estimate the gradient of the data distribution, followed by Langevin sampling to sample from the true distribution.\n\nThis implementation was inspired by the official Tensorflow version here\n\nUpdate: Turns out none of the technicalities really matters at all | \"Cold Diffusion\" paper | Muse\n\nOr, if you simply want to pass in a folder name and the desired image dimensions, you can use the class to easily train a model.\n\nSamples and model checkpoints will be logged to periodically\n\nThe class is now equipped with 🤗 Accelerator. You can easily do multi-gpu training in two steps using their CLI\n\nAt the project root directory, where the training script is, run\n\nThen, in the same directory\n\n, , , ( , ( , , , ), ) ( , , , ) . ( , , ) # features are normalized from 0 to 1 ( ) . () ( ) # this is just an example, but you can formulate your own Dataset and pass it into the `Trainer1D` below ( , , , , , , , , ) . () . ( ) .\n\ndoes not evaluate the generated samples in any way since the type of data is not known.\n\nYou could consider adding a suitable metric to the training loop yourself after doing an editable install of this package ."
    },
    {
        "link": "https://medium.com/data-science/diffusion-model-from-scratch-in-pytorch-ddpm-9d9760528946",
        "document": "A diffusion model in general terms is a type of generative deep learning model that creates data from a learned denoising process. There are many variations of diffusion models with the most popular ones usually being text conditional models that can generate a certain image based on a prompt. Some diffusion models (Control-Net) can even blend images with certain artistic styles. Here is an example below here:\n\nIf you don’t know what's so special about the image, try moving farther away from the screen or squinting your eyes to see the secret hidden in the image.\n\nThere are many different applications and types of diffusion models, but in this tutorial we are going to build the foundational unconditional diffusion model, DDPM (Denoising Diffusion Probabilistic Models) [1]. We will start by looking into how the algorithm works intuitively under the hood, and then we will build it from scratch in PyTorch. Also, this tutorial will focus primarily on the intuitive idea behind the algorithm and the specific implementation details. For the mathematical derivations and background, this book [2] is a great reference.\n\nLast Notes: This implementation was built for workflows that contain a single GPU with CUDA compatibility. In addition, the complete code repository can be found here https://github.com/nickd16/Diffusion-Models-from-Scratch\n\nHow it Works -> The Forward and Reverse Process\n\nThe diffusion process includes a forward and a reverse process. The forward process is a predetermined Markov chain based on a noise schedule. The noise schedule is a set of variances B1, B2, … BT that govern the conditional normal distributions that make up the Markov chain.\n\nThis formula is the mathematical representation of the forward process, but intuitively we can understand it as a sequence where we gradually map our data examples X to pure noise. Our first term in the forward process is just our initial data example. At an intermediate time step t, we have a noised version of X, and at our final time step T, we arrive at pure noise that is approximately governed by a standard normal distribution. When we build a diffusion model, we choose our noise schedule. In DDPM for example, our noise schedule features 1000 time steps of linearly increasing variances starting at 1e-4 to 0.02. It is also important to note that our forward process is static, meaning we choose our noise schedule as a hyperparameter to our diffusion model and we do not train the forward process as it is already defined explicitly.\n\nThe final key detail we have to know about the forward process is that because the distributions are normal, we can mathematically derive a distribution known as the “Diffusion Kernel” which is the distribution of any intermediate value in our forward process given our initial data point. This allows us to bypass all of the intermediate steps of iteratively adding t-1 levels of noise in the forward process to get an image with t noise which will come in handy later when we train our model. This is mathematically represented as:\n\nwhere alpha at time t is defined as the cumulative product (1-B) from our initial time step to our current time step.\n\nThe reverse process is the key to a diffusion model. The reverse process is essentially the undoing of the forward process by gradually removing amounts of noise from a pure noisy image to generate new images. We do this by starting at purely noised data, and for each time step t we subtract the amount of noise that would have theoretically been added by the forward process for that time step. We keep removing noise until eventually we have something that resembles our original data distribution. The bulk of our work is training a model to carefully approximate the forward process in order to estimate a reverse process that can generate new samples.\n\nTo train such a model to estimate the reverse diffusion process, we can follow the algorithm in the image defined below:\n• Take a randomly sampled data point from our training dataset\n• Add the noise from that time step to our data, simulating the forward diffusion process through the “diffusion kernel”\n• Pass our defused image into our model to predict the noise we added\n• Compute the mean squared error between the predicted noise and the actual noise and optimize our model’s parameters through that objective function\n\nMathematically, the exact formula in the algorithm might look a little strange at first without seeing the full derivation, but intuitively its a reparameterization of the diffusion kernel based on the alpha values of our noise schedule and its simply the squared difference of predicted noise and the actual noise we added to an image.\n\nIf our model can successfully predict the amount of noise based on a specific time step of our forward process, we can iteratively start from noise at time step T and gradually remove noise based on each time step until we recover data that resembles a generated sample from our original data distribution.\n\nThe sampling algorithm is summarized in the following:\n\nFor each timestep starting from our last timestep and moving backwards:\n\n2. Update Z by estimating the reverse process distribution with mean parameterized by Z from the previous step and variance parameterized by the noise our model estimates at that timestep\n\n3. Add a small amount of the noise back for stability (explanation below)\n\n4. And repeat until we arrive at time step 0, our recovered image!\n\nThe algorithm to then sample and generate images might look mathematically complicated but it intuitively boils down to an iterative process where we start with pure noise, estimate the noise that theoretically was added at time step t, and subtract it. We do this until we arrive at our generated sample. The only small detail we should be mindful of is after we subtract the estimated noise, we add back a small amount of it to keep the process stable. For example, estimating and subtracting the total amount of noise in the beginning of the iterative process all at once leads to very incoherent samples, so in practice adding a bit of the noise back and iterating through every time step has empirically been shown to generate better samples.\n\nThe authors of the DDPM paper used the UNET architecture originally designed for medical image segmentation to build a model to predict the noise for the diffusion reverse process. The model we are going to use in this tutorial is meant for 32x32 images perfect for datasets such as MNIST, but the model can be scaled to also handle data of much higher resolutions. There are many variations of the UNET, but the overview of the model architecture we will build is in the image below.\n\nThe UNET for DDPM is similar to the classic UNET because it contains both a down sampling stream and an up sampling stream that lightens the computational burden of the network, while also having skip connections between the two streams to merge the information from both the shallow and deep features of the model.\n\nThe main differences between the DDPM UNET and the classic UNET is that the DDPM UNET features attention in the 16x16 dimensional layers and sinusoidal transformer embeddings in every residual block. The meaning behind the sinusoidal embeddings is to tell the model which time step we are trying to predict the noise. This helps the model predict the noise at each time step by injecting positional information on where the model is on our noise schedule. For example, if we had a schedule of noise that had a lot of noise in certain time steps, the model understanding what time step it has to predict can help the model’s prediction on that noise for the corresponding time step. More general information on attention and embeddings can be found here [3] for those not already familiar with them from the transformer architecture.\n\nIn our implementation of the model, we will start by defining our imports (possible pip install commands commented for reference) and coding our sinusoidal time step embeddings. Intuitively, the sinusoidal embeddings are different sin and cos frequencies that can be added directly to our inputs to give the model additional positional/sequential understanding. As you can see from the image below, each sinusoidal wave is unique which will give the model awareness on its location in our noise schedule.\n\nThe residual blocks in each layer of the UNET will be equivalent to the ones used in the original DDPM paper. Each residual block will have a sequence of group-norm, the ReLU activation, a 3x3 “same” convolution, dropout, and a skip-connection.\n\nIn DDPM, the authors used 2 residual blocks per layer (resolution scale) of the UNET and for the 16x16 dimension layers, we include the classic transformer attention mechanism between the two residual blocks. We will now implement the attention mechanism for the UNET:\n\nThe attention implementation is straight forward. We reshape our data such that the h*w dimensions are combined into a “sequence” dimension like the classic input for a transformer model and the channel dimension turns into the embedding feature dimension. In this implementation we utilize torch.nn.functional.scaled_dot_product_attention because this implementation contains flash attention, which is an optimized version of attention which is still mathematically equivalent to classic transformer attention. For more information on flash attention you can refer to these papers: [4], [5].\n\nFinally at this point, we can define a complete layer of the UNET:\n\nEach layer in DDPM as previously discussed has 2 residual blocks and may contain an attention mechanism, and we additionally pass our embeddings into each residual block. Also, we return both the downsampled or upsampled value as well as the value prior which we will store and use for our residual concatenated skip connections.\n\nFinally, we can finish the UNET Class:\n\nThe implementation is straight forward based on the classes we have already created. The only difference in this implementation is that our channels for the up-stream are slightly larger than the typical channels of the UNET. I found that this architecture trained more efficiently on a single GPU with 16GB of VRAM.\n\nCoding the noise/variance scheduler for DDPM is also very straightforward. In DDPM, our schedule will start, as previously mentioned, at 1e-4 and end at 0.02 and increase linearly.\n\nWe return both the beta (variance) values and the alpha values since we the formulas for training and sampling use both based on their mathematical derivations.\n\nAdditionally (not required) this function defines a training seed. This means that if you want to reproduce a specific training instance you can use a set seed such that the random weight and optimizer initializations are the same each time you use the same seed.\n\nFor our implementation, we will create a model to generate MNIST data (hand written digits). Since these images are 28x28 by default in pytorch, we pad the images to 32x32 to follow the original paper trained on 32x32 images.\n\nFor optimization, we use Adam with initial learning rate of 2e-5. We also use EMA (Exponential Moving Average) to aid in generation quality. EMA is a weighted average of the model’s parameters that in inference time can create smoother, less noisy samples. For this implementation I use the library timm’s EMAV3 out of the box implementation with weight 0.9999 as used in the DDPM paper.\n\nTo summarize our training, we simply follow the psuedo-code above. We pick random time steps for our batch, noise our data in the batch based on our schedule at those time steps, and we input that batch of noised images into the UNET along with the time steps themselves to guide the sinusoidal embeddings. We use the formulas in the pseudo-code based on the “diffusion kernel” to noise the images. We then take our model’s prediction of how much noise we added and compare to the actual noise we added and optimize the mean squared error of the noise. We also implemented basic checkpointing to pause and resume training on different epochs.\n\nFor inference, we exactly follow again the other part of the pseudo code. Intuitively, we are just reversing the forward process. We are starting from pure noise, and our now trained model can predict the estimated noise at each time step and can then generate brand new samples iteratively. Each different starting point for the noise, we can generate a different unique sample that is similar to our original data distribution but unique. The formulas for inference were not derived in this article but the reference linked in the beginning can help guide readers who want a deeper understanding.\n\nAlso note, I included a helper function to view the diffused images so you can visualize how well the model learned the reverse process.\n\nAfter training for 75 epochs with the experimental details listed above, we obtain these results:\n\nAt this point we have just coded DDPM from scratch in PyTorch!\n\n[3] Attention is All You Need https://arxiv.org/abs/1706.03762"
    },
    {
        "link": "https://github.com/huggingface/blog/blob/main/annotated-diffusion.md",
        "document": "In this blog post, we'll take a deeper look into Denoising Diffusion Probabilistic Models (also known as DDPMs, diffusion models, score-based generative models or simply autoencoders) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include GLIDE and DALL-E 2 by OpenAI, Latent Diffusion by the University of Heidelberg and ImageGen by Google Brain.\n\nWe'll go over the original DDPM paper by (Ho et al., 2020), implementing it step-by-step in PyTorch, based on Phil Wang's implementation - which itself is based on the original TensorFlow implementation. Note that the idea of diffusion for generative modeling was actually already introduced in (Sohl-Dickstein et al., 2015). However, it took until (Song et al., 2019) (at Stanford University), and then (Ho et al., 2020) (at Google Brain) who independently improved the approach.\n\nNote that there are several perspectives on diffusion models. Here, we employ the discrete-time (latent variable model) perspective, but be sure to check out the other perspectives as well.\n\nWe'll install and import the required libraries first (assuming you have PyTorch installed).\n\nA (denoising) diffusion model isn't that complex if you compare it to other generative models such as Normalizing Flows, GANs or VAEs: they all convert noise from some simple distribution to a data sample. This is also the case here where a neural network learns to gradually denoise data starting from pure noise.\n\nIn a bit more detail for images, the set-up consists of 2 processes:\n• a fixed (or predefined) forward diffusion process \\(q\\) of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise\n• a learned reverse denoising diffusion process \\(p_\\theta\\), where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n\nBoth the forward and reverse process indexed by \\(t\\) happen for some number of finite time steps \\(T\\) (the DDPM authors use \\(T=1000\\)). You start with \\(t=0\\) where you sample a real image \\(\\mathbf{x}_0\\) from your data distribution (let's say an image of a cat from ImageNet), and the forward process samples some noise from a Gaussian distribution at each time step \\(t\\), which is added to the image of the previous time step. Given a sufficiently large \\(T\\) and a well behaved schedule for adding noise at each time step, you end up with what is called an isotropic Gaussian distribution at \\(t=T\\) via a gradual process.\n\nLet's write this down more formally, as ultimately we need a tractable loss function which our neural network needs to optimize.\n\nLet \\(q(\\mathbf{x}_0)\\) be the real data distribution, say of \"real images\". We can sample from this distribution to get an image, \\(\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)\\). We define the forward diffusion process \\(q(\\mathbf{x}t | \\mathbf{x}{t-1})\\) which adds Gaussian noise at each time step \\(t\\), according to a known variance schedule \\(0 < \\beta_1 < \\beta_2 < ... < \\beta_T < 1\\) as $$ q(\\mathbf{x}t | \\mathbf{x}{t-1}) = \\mathcal{N}(\\mathbf{x}t; \\sqrt{1 - \\beta_t} \\mathbf{x}{t-1}, \\beta_t \\mathbf{I}). $$\n\nRecall that a normal distribution (also called Gaussian distribution) is defined by 2 parameters: a mean \\(\\mu\\) and a variance \\(\\sigma^2 \\geq 0\\). Basically, each new (slightly noisier) image at time step \\(t\\) is drawn from a conditional Gaussian distribution with \\(\\mathbf{\\mu}t = \\sqrt{1 - \\beta_t} \\mathbf{x}{t-1}\\) and \\(\\sigma^2_t = \\beta_t\\), which we can do by sampling \\(\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) and then setting \\(\\mathbf{x}t = \\sqrt{1 - \\beta_t} \\mathbf{x}{t-1} + \\sqrt{\\beta_t} \\mathbf{\\epsilon}\\).\n\nNote that the \\(\\beta_t\\) aren't constant at each time step \\(t\\) (hence the subscript) --- in fact one defines a so-called \"variance schedule\", which can be linear, quadratic, cosine, etc. as we will see further (a bit like a learning rate schedule).\n\nSo starting from \\(\\mathbf{x}_0\\), we end up with \\(\\mathbf{x}_1, ..., \\mathbf{x}_t, ..., \\mathbf{x}_T\\), where \\(\\mathbf{x}_T\\) is pure Gaussian noise if we set the schedule appropriately.\n\nNow, if we knew the conditional distribution \\(p(\\mathbf{x}_{t-1} | \\mathbf{x}_t)\\), then we could run the process in reverse: by sampling some random Gaussian noise \\(\\mathbf{x}_T\\), and then gradually \"denoise\" it so that we end up with a sample from the real distribution \\(\\mathbf{x}_0\\).\n\nHowever, we don't know \\(p(\\mathbf{x}{t-1} | \\mathbf{x}t)\\). It's intractable since it requires knowing the distribution of all possible images in order to calculate this conditional probability. Hence, we're going to leverage a neural network to approximate (learn) this conditional probability distribution, let's call it \\(p\\theta (\\mathbf{x}{t-1} | \\mathbf{x}_t)\\), with \\(\\theta\\) being the parameters of the neural network, updated by gradient descent.\n\nOk, so we need a neural network to represent a (conditional) probability distribution of the backward process. If we assume this reverse process is Gaussian as well, then recall that any Gaussian distribution is defined by 2 parameters:\n\nso we can parametrize the process as $$ p_\\theta (\\mathbf{x}{t-1} | \\mathbf{x}t) = \\mathcal{N}(\\mathbf{x}{t-1}; \\mu\\theta(\\mathbf{x}{t},t), \\Sigma\\theta (\\mathbf{x}_{t},t))$$ where the mean and variance are also conditioned on the noise level \\(t\\).\n\nHence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to keep the variance fixed, and let the neural network only learn (represent) the mean \\(\\mu_\\theta\\) of this conditional probability distribution. From the paper:\n\nThis was then later improved in the Improved diffusion models paper, where a neural network also learns the variance of this backwards process, besides the mean.\n\nSo we continue, assuming that our neural network only needs to learn/represent the mean of this conditional probability distribution.\n\nTo derive an objective function to learn the mean of the backward process, the authors observe that the combination of \\(q\\) and \\(p_\\theta\\) can be seen as a variational auto-encoder (VAE) (Kingma et al., 2013). Hence, the variational lower bound (also called ELBO) can be used to minimize the negative log-likelihood with respect to ground truth data sample \\(\\mathbf{x}_0\\) (we refer to the VAE paper for details regarding ELBO). It turns out that the ELBO for this process is a sum of losses at each time step \\(t\\), \\(L = L_0 + L_1 + ... + L_T\\). By construction of the forward \\(q\\) process and backward process, each term (except for \\(L_0\\)) of the loss is actually the KL divergence between 2 Gaussian distributions which can be written explicitly as an L2-loss with respect to the means!\n\nA direct consequence of the constructed forward process \\(q\\), as shown by Sohl-Dickstein et al., is that we can sample \\(\\mathbf{x}_t\\) at any arbitrary noise level conditioned on \\(\\mathbf{x}_0\\) (since sums of Gaussians is also Gaussian). This is very convenient: we don't need to apply \\(q\\) repeatedly in order to sample \\(\\mathbf{x}_t\\). We have that\n\nwith \\(\\alpha_t := 1 - \\beta_t\\) and \\(\\bar{\\alpha}t := \\Pi{s=1}^{t} \\alpha_s\\). Let's refer to this equation as the \"nice property\". This means we can sample Gaussian noise and scale it appropriately and add it to \\(\\mathbf{x}_0\\) to get \\(\\mathbf{x}_t\\) directly. Note that the \\(\\bar{\\alpha}_t\\) are functions of the known \\(\\beta_t\\) variance schedule and thus are also known and can be precomputed. This then allows us, during training, to optimize random terms of the loss function \\(L\\) (or in other words, to randomly sample \\(t\\) during training and optimize \\(L_t\\)).\n\nAnother beauty of this property, as shown in Ho et al. is that one can (after some math, for which we refer the reader to this excellent blog post) instead reparametrize the mean to make the neural network learn (predict) the added noise (via a network \\(\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\)) for noise level \\(t\\) in the KL terms which constitute the losses. This means that our neural network becomes a noise predictor, rather than a (direct) mean predictor. The mean can be computed as follows:\n\nThe final objective function \\(L_t\\) then looks as follows (for a random time step \\(t\\) given \\(\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\) ):\n\nHere, \\(\\mathbf{x}0\\) is the initial (real, uncorrupted) image, and we see the direct noise level \\(t\\) sample given by the fixed forward process. \\(\\mathbf{\\epsilon}\\) is the pure noise sampled at time step \\(t\\), and \\(\\mathbf{\\epsilon}\\theta (\\mathbf{x}_t, t)\\) is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.\n\nThe training algorithm now looks as follows:\n• we take a random sample \\(\\mathbf{x}_0\\) from the real unknown and possibily complex data distribution \\(q(\\mathbf{x}_0)\\)\n• we sample a noise level \\(t\\) uniformly between \\(1\\) and \\(T\\) (i.e., a random time step)\n• we sample some noise from a Gaussian distribution and corrupt the input by this noise at level \\(t\\) (using the nice property defined above)\n• the neural network is trained to predict this noise based on the corrupted image \\(\\mathbf{x}_t\\) (i.e. noise applied on \\(\\mathbf{x}_0\\) based on known schedule \\(\\beta_t\\))\n\nIn reality, all of this is done on batches of data, as one uses stochastic gradient descent to optimize neural networks.\n\nThe neural network needs to take in a noised image at a particular time step and return the predicted noise. Note that the predicted noise is a tensor that has the same size/resolution as the input image. So technically, the network takes in and outputs tensors of the same shape. What type of neural network can we use for this?\n\nWhat is typically used here is very similar to that of an Autoencoder, which you may remember from typical \"intro to deep learning\" tutorials. Autoencoders have a so-called \"bottleneck\" layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the \"bottleneck\", and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.\n\nIn terms of architecture, the DDPM authors went for a U-Net, introduced by (Ronneberger et al., 2015) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in He et al., 2015).\n\nAs can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.\n\nBelow, we implement this network, step-by-step.\n\nFirst, we define some helper functions and classes which will be used when implementing the neural network. Importantly, we define a module, which simply adds the input to the output of a particular function (in other words, adds a residual connection to a particular function).\n\nWe also define aliases for the up- and downsampling operations.\n\nAs the parameters of the neural network are shared across time (noise level), the authors employ sinusoidal position embeddings to encode \\(t\\), inspired by the Transformer (Vaswani et al., 2017). This makes the neural network \"know\" at which particular time step (noise level) it is operating, for every image in a batch.\n\nThe module takes a tensor of shape as input (i.e. the noise levels of several noisy images in a batch), and turns this into a tensor of shape , with being the dimensionality of the position embeddings. This is then added to each residual block, as we will see further.\n\nNext, we define the core building block of the U-Net model. The DDPM authors employed a Wide ResNet block (Zagoruyko et al., 2016), but Phil Wang has replaced the standard convolutional layer by a \"weight standardized\" version, which works better in combination with group normalization (see (Kolesnikov et al., 2019) for details).\n\nNext, we define the attention module, which the DDPM authors added in between the convolutional blocks. Attention is the building block of the famous Transformer architecture (Vaswani et al., 2017), which has shown great success in various domains of AI, from NLP and vision to protein folding. Phil Wang employs 2 variants of attention: one is regular multi-head self-attention (as used in the Transformer), the other one is a linear attention variant (Shen et al., 2018), whose time- and memory requirements scale linear in the sequence length, as opposed to quadratic for regular attention.\n\nFor an extensive explanation of the attention mechanism, we refer the reader to Jay Allamar's wonderful blog post.\n\nThe DDPM authors interleave the convolutional/attention layers of the U-Net with group normalization (Wu et al., 2018). Below, we define a class, which will be used to apply groupnorm before the attention layer, as we'll see further. Note that there's been a debate about whether to apply normalization before or after attention in Transformers.\n\nNow that we've defined all building blocks (position embeddings, ResNet blocks, attention and group normalization), it's time to define the entire neural network. Recall that the job of the network \\(\\mathbf{\\epsilon}_\\theta(\\mathbf{x}_t, t)\\) is to take in a batch of noisy images and their respective noise levels, and output the noise added to the input. More formally:\n• the network takes a batch of noisy images of shape and a batch of noise levels of shape as input, and returns a tensor of shape\n\nThe network is built up as follows:\n• first, a convolutional layer is applied on the batch of noisy images, and position embeddings are computed for the noise levels\n• next, a sequence of downsampling stages are applied. Each downsampling stage consists of 2 ResNet blocks + groupnorm + attention + residual connection + a downsample operation\n• at the middle of the network, again ResNet blocks are applied, interleaved with attention\n• next, a sequence of upsampling stages are applied. Each upsampling stage consists of 2 ResNet blocks + groupnorm + attention + residual connection + an upsample operation\n• finally, a ResNet block followed by a convolutional layer is applied.\n\nUltimately, neural networks stack up layers as if they were lego blocks (but it's important to understand how they work).\n\nThe forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps \\(T\\). This happens according to a variance schedule. The original DDPM authors employed a linear schedule:\n\nHowever, it was shown in (Nichol et al., 2021) that better results can be achieved when employing a cosine schedule.\n\nBelow, we define various schedules for the \\(T\\) timesteps (we'll choose one later on).\n\nTo start with, let's use the linear schedule for \\(T=300\\) time steps and define the various variables from the \\(\\beta_t\\) which we will need, such as the cumulative product of the variances \\(\\bar{\\alpha}_t\\). Each of the variables below are just 1-dimensional tensors, storing values from \\(t\\) to \\(T\\). Importantly, we also define an function, which will allow us to extract the appropriate \\(t\\) index for a batch of indices.\n\nWe'll illustrate with a cats image how noise is added at each time step of the diffusion process.\n\nNoise is added to PyTorch tensors, rather than Pillow Images. We'll first define image transformations that allow us to go from a PIL image to a PyTorch tensor (on which we can add the noise), and vice versa.\n\nThese transformations are fairly simple: we first normalize images by dividing by \\(255\\) (such that they are in the \\([0,1]\\) range), and then make sure they are in the \\([-1, 1]\\) range. From the DPPM paper:\n\nWe also define the reverse transform, which takes in a PyTorch tensor containing values in \\([-1, 1]\\) and turn them back into a PIL image:\n\nWe can now define the forward diffusion process as in the paper:\n\nLet's test it on a particular time step:\n\nLet's visualize this for various time steps:\n\nThis means that we can now define the loss function given the model as follows:\n\nThe will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n\nHere we define a regular PyTorch Dataset. The dataset simply consists of images from a real dataset, like Fashion-MNIST, CIFAR-10 or ImageNet, scaled linearly to \\([−1, 1]\\).\n\nEach image is resized to the same size. Interesting to note is that images are also randomly horizontally flipped. From the paper:\n\nHere we use the 🤗 Datasets library to easily load the Fashion MNIST dataset from the hub. This dataset consists of images which already have the same resolution, namely 28x28.\n\nNext, we define a function which we'll apply on-the-fly on the entire dataset. We use the functionality for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the \\([-1,1]\\) range.\n\nAs we'll sample from the model during training (in order to track progress), we define the code for that below. Sampling is summarized in the paper as Algorithm 2:\n\nGenerating new images from a diffusion model happens by reversing the diffusion process: we start from \\(T\\), where we sample pure noise from a Gaussian distribution, and then use our neural network to gradually denoise it (using the conditional probability it has learned), until we end up at time step \\(t = 0\\). As shown above, we can derive a slighly less denoised image \\(\\mathbf{x}_{t-1 }\\) by plugging in the reparametrization of the mean, using our noise predictor. Remember that the variance is known ahead of time.\n\nIdeally, we end up with an image that looks like it came from the real data distribution.\n\nThe code below implements this.\n\nNote that the code above is a simplified version of the original implementation. We found our simplification (which is in line with Algorithm 2 in the paper) to work just as well as the original, more complex implementation, which employs clipping.\n\nNext, we train the model in regular PyTorch fashion. We also define some logic to periodically save generated images, using the method defined above.\n\nBelow, we define the model, and move it to the GPU. We also define a standard optimizer (Adam).\n\nTo sample from the model, we can just use our sample function defined above:\n\nSeems like the model is capable of generating a nice T-shirt! Keep in mind that the dataset we trained on is pretty low-resolution (28x28).\n\nWe can also create a gif of the denoising process:\n\nNote that the DDPM paper showed that diffusion models are a promising direction for (un)conditional image generation. This has since then (immensely) been improved, most notably for text-conditional image generation. Below, we list some important (but far from exhaustive) follow-up works:\n• Improved Denoising Diffusion Probabilistic Models (Nichol et al., 2021): finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance\n• Cascaded Diffusion Models for High Fidelity Image Generation (Ho et al., 2021): introduces cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis\n• Diffusion Models Beat GANs on Image Synthesis (Dhariwal et al., 2021): show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models by improving the U-Net architecture, as well as introducing classifier guidance\n• Classifier-Free Diffusion Guidance (Ho et al., 2021): shows that you don't need a classifier for guiding a diffusion model by jointly training a conditional and an unconditional diffusion model with a single neural network\n• Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) (Ramesh et al., 2022): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image\n• Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) (Saharia et al., 2022): shows that combining a large pre-trained language model (e.g. T5) with cascaded diffusion works well for text-to-image synthesis\n\nNote that this list only includes important works until the time of writing, which is June 7th, 2022.\n\nFor now, it seems that the main (perhaps only) disadvantage of diffusion models is that they require multiple forward passes to generate an image (which is not the case for generative models like GANs). However, there's research going on that enables high-fidelity generation in as few as 10 denoising steps."
    },
    {
        "link": "https://huggingface.co/blog/annotated-diffusion",
        "document": "We'll go over the original DDPM paper by (Ho et al., 2020), implementing it step-by-step in PyTorch, based on Phil Wang's implementation - which itself is based on the original TensorFlow implementation. Note that the idea of diffusion for generative modeling was actually already introduced in (Sohl-Dickstein et al., 2015). However, it took until (Song et al., 2019) (at Stanford University), and then (Ho et al., 2020) (at Google Brain) who independently improved the approach.\n\nNote that there are several perspectives on diffusion models. Here, we employ the discrete-time (latent variable model) perspective, but be sure to check out the other perspectives as well.\n\nWe'll install and import the required libraries first (assuming you have PyTorch installed).\n\nA (denoising) diffusion model isn't that complex if you compare it to other generative models such as Normalizing Flows, GANs or VAEs: they all convert noise from some simple distribution to a data sample. This is also the case here where a neural network learns to gradually denoise data starting from pure noise.\n\nIn a bit more detail for images, the set-up consists of 2 processes:\n• a fixed (or predefined) forward diffusion process of our choosing, that gradually adds Gaussian noise to an image, until you end up with pure noise\n• a learned reverse denoising diffusion process , where a neural network is trained to gradually denoise an image starting from pure noise, until you end up with an actual image.\n\nBoth the forward and reverse process indexed by t happen for some number of finite time steps T (the DDPM authors use T=1000). You start with t=0 where you sample a real image x0​ from your data distribution (let's say an image of a cat from ImageNet), and the forward process samples some noise from a Gaussian distribution at each time step t, which is added to the image of the previous time step. Given a sufficiently large T and a well behaved schedule for adding noise at each time step, you end up with what is called an isotropic Gaussian distribution at t=T via a gradual process.\n\nLet's write this down more formally, as ultimately we need a tractable loss function which our neural network needs to optimize.\n\nLet q(x0​) be the real data distribution, say of \"real images\". We can sample from this distribution to get an image, x0​∼q(x0​). We define the forward diffusion process q(xt​∣xt−1​) which adds Gaussian noise at each time step t, according to a known variance schedule 0<β1​<β2​<...<βT​<1 as q(xt​∣xt−1​)=N(xt​;1−βt​ ​xt−1​,βt​I).\n\nRecall that a normal distribution (also called Gaussian distribution) is defined by 2 parameters: a mean μ and a variance σ2≥0. Basically, each new (slightly noisier) image at time step t is drawn from a conditional Gaussian distribution with μt​=1−βt​ ​xt−1​ and σt2​=βt​, which we can do by sampling ϵ∼N(0,I) and then setting xt​=1−βt​ ​xt−1​+βt​ ​ϵ.\n\nNote that the βt​ aren't constant at each time step t (hence the subscript) --- in fact one defines a so-called \"variance schedule\", which can be linear, quadratic, cosine, etc. as we will see further (a bit like a learning rate schedule).\n\nSo starting from x0​, we end up with x1​,...,xt​,...,xT​, where xT​ is pure Gaussian noise if we set the schedule appropriately.\n\nNow, if we knew the conditional distribution p(xt−1​∣xt​), then we could run the process in reverse: by sampling some random Gaussian noise xT​, and then gradually \"denoise\" it so that we end up with a sample from the real distribution x0​.\n\nHowever, we don't know p(xt−1​∣xt​). It's intractable since it requires knowing the distribution of all possible images in order to calculate this conditional probability. Hence, we're going to leverage a neural network to approximate (learn) this conditional probability distribution, let's call it pθ​(xt−1​∣xt​), with θ being the parameters of the neural network, updated by gradient descent.\n\nOk, so we need a neural network to represent a (conditional) probability distribution of the backward process. If we assume this reverse process is Gaussian as well, then recall that any Gaussian distribution is defined by 2 parameters:\n\nso we can parametrize the process as pθ​(xt−1​∣xt​)=N(xt−1​;μθ​(xt​,t),Σθ​(xt​,t)) where the mean and variance are also conditioned on the noise level t.\n\nHence, our neural network needs to learn/represent the mean and variance. However, the DDPM authors decided to keep the variance fixed, and let the neural network only learn (represent) the mean μθ​ of this conditional probability distribution. From the paper:\n\nThis was then later improved in the Improved diffusion models paper, where a neural network also learns the variance of this backwards process, besides the mean.\n\nSo we continue, assuming that our neural network only needs to learn/represent the mean of this conditional probability distribution.\n\nDefining an objective function (by reparametrizing the mean)\n\nTo derive an objective function to learn the mean of the backward process, the authors observe that the combination of q and pθ​ can be seen as a variational auto-encoder (VAE) (Kingma et al., 2013). Hence, the variational lower bound (also called ELBO) can be used to minimize the negative log-likelihood with respect to ground truth data sample x0​ (we refer to the VAE paper for details regarding ELBO). It turns out that the ELBO for this process is a sum of losses at each time step t, L=L0​+L1​+...+LT​. By construction of the forward q process and backward process, each term (except for L0​) of the loss is actually the KL divergence between 2 Gaussian distributions which can be written explicitly as an L2-loss with respect to the means!\n\nA direct consequence of the constructed forward process q, as shown by Sohl-Dickstein et al., is that we can sample xt​ at any arbitrary noise level conditioned on x0​ (since sums of Gaussians is also Gaussian). This is very convenient: we don't need to apply q repeatedly in order to sample xt​. We have that q(xt​∣x0​)=N(xt​;αˉt​ ​x0​,(1−αˉt​)I)\n\nwith αt​:=1−βt​ and αˉt​:=Πs=1t​αs​. Let's refer to this equation as the \"nice property\". This means we can sample Gaussian noise and scale it appropriately and add it to x0​ to get xt​ directly. Note that the αˉt​ are functions of the known βt​ variance schedule and thus are also known and can be precomputed. This then allows us, during training, to optimize random terms of the loss function L (or in other words, to randomly sample t during training and optimize Lt​).\n\nAnother beauty of this property, as shown in Ho et al. is that one can (after some math, for which we refer the reader to this excellent blog post) instead reparametrize the mean to make the neural network learn (predict) the added noise (via a network ϵθ​(xt​,t)) for noise level t in the KL terms which constitute the losses. This means that our neural network becomes a noise predictor, rather than a (direct) mean predictor. The mean can be computed as follows:\n\nThe final objective function Lt​ then looks as follows (for a random time step t given ϵ∼N(0,I) ):\n\nHere, x0​ is the initial (real, uncorrupted) image, and we see the direct noise level t sample given by the fixed forward process. ϵ is the pure noise sampled at time step t, and ϵθ​(xt​,t) is our neural network. The neural network is optimized using a simple mean squared error (MSE) between the true and the predicted Gaussian noise.\n\nThe training algorithm now looks as follows:\n• we take a random sample from the real unknown and possibily complex data distribution\n• we sample a noise level uniformly between and (i.e., a random time step)\n• we sample some noise from a Gaussian distribution and corrupt the input by this noise at level (using the nice property defined above)\n• the neural network is trained to predict this noise based on the corrupted image (i.e. noise applied on based on known schedule )\n\nIn reality, all of this is done on batches of data, as one uses stochastic gradient descent to optimize neural networks.\n\nThe neural network needs to take in a noised image at a particular time step and return the predicted noise. Note that the predicted noise is a tensor that has the same size/resolution as the input image. So technically, the network takes in and outputs tensors of the same shape. What type of neural network can we use for this?\n\nWhat is typically used here is very similar to that of an Autoencoder, which you may remember from typical \"intro to deep learning\" tutorials. Autoencoders have a so-called \"bottleneck\" layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the \"bottleneck\", and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.\n\nIn terms of architecture, the DDPM authors went for a U-Net, introduced by (Ronneberger et al., 2015) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in He et al., 2015).\n\nAs can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.\n\nBelow, we implement this network, step-by-step.\n\nFirst, we define some helper functions and classes which will be used when implementing the neural network. Importantly, we define a module, which simply adds the input to the output of a particular function (in other words, adds a residual connection to a particular function).\n\nWe also define aliases for the up- and downsampling operations.\n\nAs the parameters of the neural network are shared across time (noise level), the authors employ sinusoidal position embeddings to encode t, inspired by the Transformer (Vaswani et al., 2017). This makes the neural network \"know\" at which particular time step (noise level) it is operating, for every image in a batch.\n\nThe module takes a tensor of shape as input (i.e. the noise levels of several noisy images in a batch), and turns this into a tensor of shape , with being the dimensionality of the position embeddings. This is then added to each residual block, as we will see further.\n\nNext, we define the core building block of the U-Net model. The DDPM authors employed a Wide ResNet block (Zagoruyko et al., 2016), but Phil Wang has replaced the standard convolutional layer by a \"weight standardized\" version, which works better in combination with group normalization (see (Kolesnikov et al., 2019) for details).\n\nNext, we define the attention module, which the DDPM authors added in between the convolutional blocks. Attention is the building block of the famous Transformer architecture (Vaswani et al., 2017), which has shown great success in various domains of AI, from NLP and vision to protein folding. Phil Wang employs 2 variants of attention: one is regular multi-head self-attention (as used in the Transformer), the other one is a linear attention variant (Shen et al., 2018), whose time- and memory requirements scale linear in the sequence length, as opposed to quadratic for regular attention.\n\nFor an extensive explanation of the attention mechanism, we refer the reader to Jay Allamar's wonderful blog post.\n\nThe DDPM authors interleave the convolutional/attention layers of the U-Net with group normalization (Wu et al., 2018). Below, we define a class, which will be used to apply groupnorm before the attention layer, as we'll see further. Note that there's been a debate about whether to apply normalization before or after attention in Transformers.\n\nNow that we've defined all building blocks (position embeddings, ResNet blocks, attention and group normalization), it's time to define the entire neural network. Recall that the job of the network ϵθ​(xt​,t) is to take in a batch of noisy images and their respective noise levels, and output the noise added to the input. More formally:\n• the network takes a batch of noisy images of shape and a batch of noise levels of shape as input, and returns a tensor of shape\n\nThe network is built up as follows:\n• first, a convolutional layer is applied on the batch of noisy images, and position embeddings are computed for the noise levels\n• next, a sequence of downsampling stages are applied. Each downsampling stage consists of 2 ResNet blocks + groupnorm + attention + residual connection + a downsample operation\n• at the middle of the network, again ResNet blocks are applied, interleaved with attention\n• next, a sequence of upsampling stages are applied. Each upsampling stage consists of 2 ResNet blocks + groupnorm + attention + residual connection + an upsample operation\n• finally, a ResNet block followed by a convolutional layer is applied.\n\nUltimately, neural networks stack up layers as if they were lego blocks (but it's important to understand how they work).\n\nThe forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps T. This happens according to a variance schedule. The original DDPM authors employed a linear schedule:\n\nHowever, it was shown in (Nichol et al., 2021) that better results can be achieved when employing a cosine schedule.\n\nBelow, we define various schedules for the T timesteps (we'll choose one later on).\n\nTo start with, let's use the linear schedule for T=300 time steps and define the various variables from the βt​ which we will need, such as the cumulative product of the variances αˉt​. Each of the variables below are just 1-dimensional tensors, storing values from t to T. Importantly, we also define an function, which will allow us to extract the appropriate t index for a batch of indices.\n\nWe'll illustrate with a cats image how noise is added at each time step of the diffusion process.\n\nNoise is added to PyTorch tensors, rather than Pillow Images. We'll first define image transformations that allow us to go from a PIL image to a PyTorch tensor (on which we can add the noise), and vice versa.\n\nThese transformations are fairly simple: we first normalize images by dividing by 255 (such that they are in the [0,1] range), and then make sure they are in the [−1,1] range. From the DPPM paper:\n\nWe also define the reverse transform, which takes in a PyTorch tensor containing values in [−1,1] and turn them back into a PIL image:\n\nWe can now define the forward diffusion process as in the paper:\n\nLet's test it on a particular time step:\n\nLet's visualize this for various time steps:\n\nThis means that we can now define the loss function given the model as follows:\n\nThe will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.\n\nHere we define a regular PyTorch Dataset. The dataset simply consists of images from a real dataset, like Fashion-MNIST, CIFAR-10 or ImageNet, scaled linearly to [−1,1].\n\nEach image is resized to the same size. Interesting to note is that images are also randomly horizontally flipped. From the paper:\n\nHere we use the 🤗 Datasets library to easily load the Fashion MNIST dataset from the hub. This dataset consists of images which already have the same resolution, namely 28x28.\n\nNext, we define a function which we'll apply on-the-fly on the entire dataset. We use the functionality for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the [−1,1] range.\n\nAs we'll sample from the model during training (in order to track progress), we define the code for that below. Sampling is summarized in the paper as Algorithm 2:\n\nGenerating new images from a diffusion model happens by reversing the diffusion process: we start from T, where we sample pure noise from a Gaussian distribution, and then use our neural network to gradually denoise it (using the conditional probability it has learned), until we end up at time step t=0. As shown above, we can derive a slighly less denoised image xt−1​ by plugging in the reparametrization of the mean, using our noise predictor. Remember that the variance is known ahead of time.\n\nIdeally, we end up with an image that looks like it came from the real data distribution.\n\nThe code below implements this.\n\nNote that the code above is a simplified version of the original implementation. We found our simplification (which is in line with Algorithm 2 in the paper) to work just as well as the original, more complex implementation, which employs clipping.\n\nNext, we train the model in regular PyTorch fashion. We also define some logic to periodically save generated images, using the method defined above.\n\nBelow, we define the model, and move it to the GPU. We also define a standard optimizer (Adam).\n\nTo sample from the model, we can just use our sample function defined above:\n\nSeems like the model is capable of generating a nice T-shirt! Keep in mind that the dataset we trained on is pretty low-resolution (28x28).\n\nWe can also create a gif of the denoising process:\n\nNote that the DDPM paper showed that diffusion models are a promising direction for (un)conditional image generation. This has since then (immensely) been improved, most notably for text-conditional image generation. Below, we list some important (but far from exhaustive) follow-up works:\n• Improved Denoising Diffusion Probabilistic Models (Nichol et al., 2021): finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance\n• Cascaded Diffusion Models for High Fidelity Image Generation (Ho et al., 2021): introduces cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis\n• Diffusion Models Beat GANs on Image Synthesis (Dhariwal et al., 2021): show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models by improving the U-Net architecture, as well as introducing classifier guidance\n• Classifier-Free Diffusion Guidance (Ho et al., 2021): shows that you don't need a classifier for guiding a diffusion model by jointly training a conditional and an unconditional diffusion model with a single neural network\n• Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) (Ramesh et al., 2022): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image\n• Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) (Saharia et al., 2022): shows that combining a large pre-trained language model (e.g. T5) with cascaded diffusion works well for text-to-image synthesis\n\nNote that this list only includes important works until the time of writing, which is June 7th, 2022.\n\nFor now, it seems that the main (perhaps only) disadvantage of diffusion models is that they require multiple forward passes to generate an image (which is not the case for generative models like GANs). However, there's research going on that enables high-fidelity generation in as few as 10 denoising steps."
    },
    {
        "link": "https://tensorflow.org/tutorials/customization/custom_training_walkthrough",
        "document": "This tutorial shows you how to train a machine learning model with a custom training loop to categorize penguins by species. In this notebook, you use TensorFlow to accomplish the following:\n• Use the trained model to make predictions\n\nThis tutorial demonstrates the following TensorFlow programming tasks:\n• Building models and layers with the Keras API\n\nImagine you are an ornithologist seeking an automated way to categorize each penguin you find. Machine learning provides many algorithms to classify penguins statistically. For instance, a sophisticated machine learning program could classify penguins based on photographs. The model you build in this tutorial is a little simpler. It classifies penguins based on their body weight, flipper length, and beaks, specifically the length and width measurements of their culmen.\n\nThere are 18 species of penguins, but in this tutorial you will only attempt to classify the following three:\n\nFortunately, a research team has already created and shared a dataset of 334 penguins with body weight, flipper length, beak measurements, and other data. This dataset is also conveniently available as the penguins TensorFlow Dataset.\n\nInstall the package for the penguins dataset. The package is the nightly released version of the TensorFlow Datasets (TFDS). For more information on TFDS, see TensorFlow Datasets overview.\n\nThen select Runtime > Restart Runtime from the Colab menu to restart the Colab runtime.\n\nDo not proceed with the rest of this tutorial without first restarting the runtime.\n\nImport TensorFlow and the other required Python modules.\n\nThe default penguins/processed TensorFlow Dataset is already cleaned, normalized, and ready for building a model. Before you download the processed data, preview a simplified version to get familiar with the original penguin survey data.\n\nDownload the simplified version of the penguins dataset ( ) using the TensorFlow Datasets method. There are 344 data records in this dataset. Extract the first five records into a object to inspect a sample of the values in this dataset:\n\nThe numbered rows are data records, one example per line, where:\n• The first six fields are features: these are the characteristics of an example. Here, the fields hold numbers representing penguin measurements.\n• The last column is the label: this is the value you want to predict. For this dataset, it's an integer value of 0, 1, or 2 that corresponds to a penguin species name.\n\nIn the dataset, the label for the penguin species is represented as a number to make it easier to work with in the model you are building. These numbers correspond to the following penguin species:\n\nCreate a list containing the penguin species names in this order. You will use this list to interpret the output of the classification model:\n\nFor more information about features and labels, refer to the ML Terminology section of the Machine Learning Crash Course.\n\nNow, download the preprocessed penguins dataset ( ) with the method, which returns a list of objects. Note that the dataset doesn't come with its own test set, so use an 80:20 split to slice the full dataset into the training and test sets. You will use the test dataset later to verify your model.\n\nNotice that this version of the dataset has been processed by reducing the data down to four normalized features and a species label. In this format, the data can be quickly used to train a model without further processing.\n\nYou can visualize some clusters by plotting a few features from the batch:\n\nA model is a relationship between features and the label. For the penguin classification problem, the model defines the relationship between the body mass, flipper and culmen measurements and the predicted penguin species. Some simple models can be described with a few lines of algebra, but complex machine learning models have a large number of parameters that are difficult to summarize.\n\nCould you determine the relationship between the four features and the penguin species without using machine learning? That is, could you use traditional programming techniques (for example, a lot of conditional statements) to create a model? Perhaps—if you analyzed the dataset long enough to determine the relationships between body mass and culmen measurements to a particular species. And this becomes difficult—maybe impossible—on more complicated datasets. A good machine learning approach determines the model for you. If you feed enough representative examples into the right machine learning model type, the program figures out the relationships for you.\n\nNext you need to select the kind of model to train. There are many types of models and picking a good one takes experience. This tutorial uses a neural network to solve the penguin classification problem. Neural networks can find complex relationships between features and the label. It is a highly-structured graph, organized into one or more hidden layers. Each hidden layer consists of one or more neurons. There are several categories of neural networks and this program uses a dense, or fully-connected neural network: the neurons in one layer receive input connections from every neuron in the previous layer. For example, Figure 2 illustrates a dense neural network consisting of an input layer, two hidden layers, and an output layer:\n\nWhen you train the model from Figure 2 and feed it an unlabeled example, it yields three predictions: the likelihood that this penguin is the given penguin species. This prediction is called inference. For this example, the sum of the output predictions is 1.0. In Figure 2, this prediction breaks down as: for Adelie, for Chinstrap, and for Gentoo species. This means that the model predicts—with 95% probability—that an unlabeled example penguin is a Chinstrap penguin.\n\nThe TensorFlow API is the preferred way to create models and layers. This makes it easy to build models and experiment while Keras handles the complexity of connecting everything together.\n\nThe model is a linear stack of layers. Its constructor takes a list of layer instances, in this case, two layers with 10 nodes each, and an output layer with 3 nodes representing your label predictions. The first layer's parameter corresponds to the number of features from the dataset, and is required:\n\nThe activation function determines the output shape of each node in the layer. These non-linearities are important—without them the model would be equivalent to a single layer. There are many , but ReLU is common for hidden layers.\n\nThe ideal number of hidden layers and neurons depends on the problem and the dataset. Like many aspects of machine learning, picking the best shape of the neural network requires a mixture of knowledge and experimentation. As a rule of thumb, increasing the number of hidden layers and neurons typically creates a more powerful model, which requires more data to train effectively.\n\nLet's have a quick look at what this model does to a batch of features:\n\nHere, each example returns a logit for each class.\n\nTo convert these logits to a probability for each class, use the softmax function:\n\nTaking the across classes gives us the predicted class index. But, the model hasn't been trained yet, so these aren't good predictions:\n\nTraining is the stage of machine learning when the model is gradually optimized, or the model learns the dataset. The goal is to learn enough about the structure of the training dataset to make predictions about unseen data. If you learn too much about the training dataset, then the predictions only work for the data it has seen and will not be generalizable. This problem is called overfitting—it's like memorizing the answers instead of understanding how to solve a problem.\n\nThe penguin classification problem is an example of supervised machine learning: the model is trained from examples that contain labels. In unsupervised machine learning, the examples don't contain labels. Instead, the model typically finds patterns among the features.\n\nBoth training and evaluation stages need to calculate the model's loss. This measures how off a model's predictions are from the desired label, in other words, how bad the model is performing. You want to minimize, or optimize, this value.\n\nYour model will calculate its loss using the function which takes the model's class probability predictions and the desired label, and returns the average loss across the examples.\n\nUse the context to calculate the gradients used to optimize your model:\n\nAn optimizer applies the computed gradients to the model's parameters to minimize the function. You can think of the loss function as a curved surface (refer to Figure 3) and you want to find its lowest point by walking around. The gradients point in the direction of steepest ascent—so you'll travel the opposite way and move down the hill. By iteratively calculating the loss and gradient for each batch, you'll adjust the model during training. Gradually, the model will find the best combination of weights and bias to minimize the loss. And the lower the loss, the better the model's predictions.\n\nTensorFlow has many optimization algorithms available for training. In this tutorial, you will use the that implements the stochastic gradient descent (SGD) algorithm. The parameter sets the step size to take for each iteration down the hill. This rate is a hyperparameter that you'll commonly adjust to achieve better results.\n\nInstantiate the optimizer with a learning rate of , a scalar value that is multiplied by the gradient at each iteration of the training:\n\nThen use this object to calculate a single optimization step:\n\nWith all the pieces in place, the model is ready for training! A training loop feeds the dataset examples into the model to help it make better predictions. The following code block sets up these training steps:\n• Iterate each epoch. An epoch is one pass through the dataset.\n• Within an epoch, iterate over each example in the training grabbing its features ( ) and label ( ).\n• Using the example's features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n• Use an to update the model's parameters.\n• Keep track of some stats for visualization.\n\nThe variable is the number of times to loop over the dataset collection. In the code below, is set to 201 which means this training loop will run 201 times. Counter-intuitively, training a model longer does not guarantee a better model. is a hyperparameter that you can tune. Choosing the right number usually requires both experience and experimentation:\n\nAlternatively, you could use the built-in Keras method to train your model.\n\nWhile it's helpful to print out the model's training progress, you can visualize the progress with TensorBoard - a visualization and metrics tool that is packaged with TensorFlow. For this simple example, you will create basic charts using the module.\n\nInterpreting these charts takes some experience, but in general you want to see the loss decrease and the accuracy increase:\n\nNow that the model is trained, you can get some statistics on its performance.\n\nEvaluating means determining how effectively the model makes predictions. To determine the model's effectiveness at penguin classification, pass some measurements to the model and ask the model to predict what penguin species they represent. Then compare the model's predictions against the actual label. For example, a model that picked the correct species on half the input examples has an accuracy of . Figure 4 shows a slightly more effective model, getting 4 out of 5 predictions correct at 80% accuracy:\n\nEvaluating the model is similar to training the model. The biggest difference is the examples come from a separate test set rather than the training set. To fairly assess a model's effectiveness, the examples used to evaluate a model must be different from the examples used to train the model.\n\nThe penguin dataset doesn't have a separate test dataset so in the previous Download the dataset section, you split the original dataset into test and train datasets. Use the dataset for the evaluation.\n\nEvaluate the model on the test dataset\n\nUnlike the training stage, the model only evaluates a single epoch of the test data. The following code iterates over each example in the test set and compare the model's prediction against the actual label. This comparison is used to measure the model's accuracy across the entire test set:\n\nYou can also use the keras function to get accuracy information on your test dataset.\n\nBy inspecting the last batch, for example, you can observe that the model predictions are usually correct.\n\nUse the trained model to make predictions\n\nYou've trained a model and \"proven\" that it's good—but not perfect—at classifying penguin species. Now let's use the trained model to make some predictions on unlabeled examples; that is, on examples that contain features but not labels.\n\nIn real-life, the unlabeled examples could come from lots of different sources including apps, CSV files, and data feeds. For this tutorial, manually provide three unlabeled examples to predict their labels. Recall, the label numbers are mapped to a named representation as:"
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/pytorch_with_examples.html",
        "document": "This tutorial introduces the fundamental concepts of PyTorch through self-contained examples.\n\nAt its core, PyTorch provides two main features:\n• None An n-dimensional Tensor, similar to numpy but can run on GPUs\n\nWe will use a problem of fitting \\(y=\\sin(x)\\) with a third order polynomial as our running example. The network will have four parameters, and will be trained with gradient descent to fit random data by minimizing the Euclidean distance between the network output and the true output.\n\nBefore introducing PyTorch, we will first implement the network using numpy. Numpy provides an n-dimensional array object, and many functions for manipulating these arrays. Numpy is a generic framework for scientific computing; it does not know anything about computation graphs, or deep learning, or gradients. However we can easily use numpy to fit a third order polynomial to sine function by manually implementing the forward and backward passes through the network using numpy operations: # Backprop to compute gradients of a, b, c, d with respect to loss Numpy is a great framework, but it cannot utilize GPUs to accelerate its numerical computations. For modern deep neural networks, GPUs often provide speedups of 50x or greater, so unfortunately numpy won’t be enough for modern deep learning. Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical to a numpy array: a Tensor is an n-dimensional array, and PyTorch provides many functions for operating on these Tensors. Behind the scenes, Tensors can keep track of a computational graph and gradients, but they’re also useful as a generic tool for scientific computing. Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their numeric computations. To run a PyTorch Tensor on GPU, you simply need to specify the correct device. Here we use PyTorch Tensors to fit a third order polynomial to sine function. Like the numpy example above we need to manually implement the forward and backward passes through the network: # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # Backprop to compute gradients of a, b, c, d with respect to loss\n\nIn the above examples, we had to manually implement both the forward and backward passes of our neural network. Manually implementing the backward pass is not a big deal for a small two-layer network, but can quickly get very hairy for large complex networks. Thankfully, we can use automatic differentiation to automate the computation of backward passes in neural networks. The autograd package in PyTorch provides exactly this functionality. When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients. This sounds complicated, it’s pretty simple to use in practice. Each Tensor represents a node in a computational graph. If is a Tensor that has then is another Tensor holding the gradient of with respect to some scalar value. Here we use PyTorch Tensors and autograd to implement our fitting sine wave with third order polynomial example; now we no longer need to manually implement the backward pass through the network: # We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__ # such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU. # By default, requires_grad=False, which indicates that we do not need to # compute gradients with respect to these Tensors during the backward pass. # Create random Tensors for weights. For a third order polynomial, we need # Setting requires_grad=True indicates that we want to compute gradients with # respect to these Tensors during the backward pass. # Compute and print loss using operations on Tensors. # Now loss is a Tensor of shape (1,) # loss.item() gets the scalar value held in the loss. # Use autograd to compute the backward pass. This call will compute the # gradient of loss with respect to all Tensors with requires_grad=True. # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding # the gradient of the loss with respect to a, b, c, d respectively. # because weights have requires_grad=True, but we don't need to track this # Manually zero the gradients after updating weights Under the hood, each primitive autograd operator is really two functions that operate on Tensors. The forward function computes output Tensors from input Tensors. The backward function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value. In PyTorch we can easily define our own autograd operator by defining a subclass of and implementing the and functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data. In this example we define our model as \\(y=a+b P_3(c+dx)\\) instead of \\(y=a+bx+cx^2+dx^3\\), where \\(P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)\\) is the Legendre polynomial of degree three. We write our own custom autograd function for computing forward and backward of \\(P_3\\), and use it to implement our model: We can implement our own custom autograd Functions by subclassing torch.autograd.Function and implementing the forward and backward passes In the forward pass we receive a Tensor containing the input and return a Tensor containing the output. ctx is a context object that can be used to stash information for backward computation. You can cache arbitrary objects for use in the backward pass using the ctx.save_for_backward method. In the backward pass we receive a Tensor containing the gradient of the loss with respect to the output, and we need to compute the gradient of the loss with respect to the input. # device = torch.device(\"cuda:0\") # Uncomment this to run on GPU # By default, requires_grad=False, which indicates that we do not need to # compute gradients with respect to these Tensors during the backward pass. # Create random Tensors for weights. For this example, we need # 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized # not too far from the correct result to ensure convergence. # Setting requires_grad=True indicates that we want to compute gradients with # respect to these Tensors during the backward pass. # To apply our Function, we use Function.apply method. We alias this as 'P3'. # Use autograd to compute the backward pass. # Manually zero the gradients after updating weights\n\nComputational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level. When building neural networks we frequently think of arranging the computation into layers, some of which have learnable parameters which will be optimized during learning. In TensorFlow, packages like Keras, TensorFlow-Slim, and TFLearn provide higher-level abstractions over raw computational graphs that are useful for building neural networks. In PyTorch, the package serves this same purpose. The package defines a set of Modules, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters. The package also defines a set of useful loss functions that are commonly used when training neural networks. In this example we use the package to implement our polynomial model network: # For this example, the output y is a linear function of (x, x^2, x^3), so # we can consider it as a linear layer neural network. Let's prepare the # In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape # (3,), for this case, broadcasting semantics will apply to obtain a tensor # Use the nn package to define our model as a sequence of layers. nn.Sequential # is a Module which contains other Modules, and applies them in sequence to # produce its output. The Linear Module computes output from input using a # linear function, and holds internal Tensors for its weight and bias. # The Flatten layer flatens the output of the linear layer to a 1D tensor, # to match the shape of `y`. # The nn package also contains definitions of popular loss functions; in this # case we will use Mean Squared Error (MSE) as our loss function. # Forward pass: compute predicted y by passing x to the model. Module objects # override the __call__ operator so you can call them like functions. When # doing so you pass a Tensor of input data to the Module and it produces # Compute and print loss. We pass Tensors containing the predicted and true # values of y, and the loss function returns a Tensor containing the # Zero the gradients before running the backward pass. # Backward pass: compute gradient of the loss with respect to all the learnable # parameters of the model. Internally, the parameters of each Module are stored # in Tensors with requires_grad=True, so this call will compute gradients for # all learnable parameters in the model. # Update the weights using gradient descent. Each parameter is a Tensor, so # we can access its gradients like we did before. # You can access the first layer of `model` like accessing the first item of a list # For linear layer, its parameters are stored as `weight` and `bias`. Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters with . This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like , , , and other. The package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms. In this example we will use the package to define our model as before, but we will optimize the model using the algorithm provided by the package: # Use the nn package to define our model and loss function. # Use the optim package to define an Optimizer that will update the weights of # the model for us. Here we will use RMSprop; the optim package contains many other # optimization algorithms. The first argument to the RMSprop constructor tells the # optimizer which Tensors it should update. # Forward pass: compute predicted y by passing x to the model. # Before the backward pass, use the optimizer object to zero all of the # gradients for the variables it will update (which are the learnable # weights of the model). This is because by default, gradients are # accumulated in buffers( i.e, not overwritten) whenever .backward() # is called. Checkout docs of torch.autograd.backward for more details. # Backward pass: compute gradient of the loss with respect to model # Calling the step function on an Optimizer makes an update to its Sometimes you will want to specify models that are more complex than a sequence of existing Modules; for these cases you can define your own Modules by subclassing and defining a which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors. In this example we implement our third order polynomial as a custom Module subclass: In the constructor we instantiate four parameters and assign them as In the forward function we accept a Tensor of input data and we must return a Tensor of output data. We can use Modules defined in the constructor as well as arbitrary operators on Tensors. Just like any class in Python, you can also define custom method on PyTorch modules # Construct our model by instantiating the class defined above # Construct our loss function and an Optimizer. The call to model.parameters() # in the SGD constructor will contain the learnable parameters (defined # with torch.nn.Parameter) which are members of the model. # Forward pass: Compute predicted y by passing x to the model # Zero gradients, perform a backward pass, and update the weights. As an example of dynamic graphs and weight sharing, we implement a very strange model: a third-fifth order polynomial that on each forward pass chooses a random number between 3 and 5 and uses that many orders, reusing the same weights multiple times to compute the fourth and fifth order. For this model we can use normal Python flow control to implement the loop, and we can implement weight sharing by simply reusing the same parameter multiple times when defining the forward pass. We can easily implement this model as a Module subclass: In the constructor we instantiate five parameters and assign them as members. For the forward pass of the model, we randomly choose either 4, 5 and reuse the e parameter to compute the contribution of these orders. Since each forward pass builds a dynamic computation graph, we can use normal Python control-flow operators like loops or conditional statements when defining the forward pass of the model. Here we also see that it is perfectly safe to reuse the same parameter many Just like any class in Python, you can also define custom method on PyTorch modules # Construct our model by instantiating the class defined above # Construct our loss function and an Optimizer. Training this strange model with # vanilla stochastic gradient descent is tough, so we use momentum # Forward pass: Compute predicted y by passing x to the model # Zero gradients, perform a backward pass, and update the weights."
    },
    {
        "link": "https://realpython.com/pytorch-vs-tensorflow",
        "document": "PyTorch vs TensorFlow: What’s the difference? Both are open-source Python libraries that use graphs to perform numerical computations on data in deep learning applications. Both are used extensively in academic research and commercial code. Both are extended by a variety of APIs, cloud computing platforms, and model repositories.\n\nIf they’re so similar, then how do you decide which one is best for your project?\n• What the differences are between PyTorch and TensorFlow\n• What tools and resources are available for each\n• How to choose the best option for your specific use case\n\nYou’ll start by taking a close look at both platforms, beginning with the slightly older TensorFlow. Then, you’ll explore PyTorch and some considerations to help you determine which choice is best for your project. Let’s get started!\n\nTensorFlow was developed by Google and released as open-source in 2015. It grew out of Google’s homegrown machine learning software, which was refactored and optimized for use in production. The name “TensorFlow” describes how you organize and perform operations on data. The basic data structure for both TensorFlow and PyTorch is a tensor. When you use TensorFlow, you perform operations on the data in these tensors by building a stateful dataflow graph, kind of like a flowchart that remembers past events. TensorFlow has a reputation for being a production-grade deep learning library. It has a large and active user base and a proliferation of official and third-party tools and platforms for training, deploying, and serving models. After PyTorch was released in 2016, TensorFlow declined in popularity. But in late 2019, Google released TensorFlow 2.0—a major update that simplified the library and made it more user-friendly, leading to renewed interest among the machine learning community. In TensorFlow 2.0, you can use eager execution, which is how Python normally works. Eager execution evaluates operations immediately, so you can write your code using Python control flow rather than graph control flow. To see this in action, you’ll take a look at how you would multiply two tensors using both Tensorflow 1.0 and 2.0. To start, here’s an example of how to multiply tensors using TensorFlow 2.0. With eager execution, all you need is : In this code, you declare your tensors using Python’s list notation, and executes the element-wise multiplication immediately when you call it. Before TensorFlow 2.0, you had to manually stitch together an abstract syntax tree by making API calls. You then had to pass a set of output tensors and input tensors to a call and manually compile the model. A object is a class for running TensorFlow operations. It contains the environment in which objects are evaluated and objects are executed, and it can own resources like objects. The most common way to use a is as a context manager. To see how a is used in this way, here’s an example of how you multiply two tensors using the old TensorFlow 1.0 method: This code uses TensorFlow 2.x’s API to access TensorFlow 1.x methods and disable eager execution. You first declare the input tensors and using tensor objects. Then you define the operation to perform on them. Note that nothing has been calculated at this point. Next, using the object as a context manager, you create a container to encapsulate the runtime environment and do the multiplication by feeding real values into the placeholders with a . Finally, still inside the session, you the result. Note: Keep in mind that because TensorFlow 1.0 has been deprecated, it probably isn’t the best option for your future projects, and you should stick with using TensorFlow 2.0. One other improvement in TensorFlow 2.0 is that it comes with a Python interface called Keras. Keras has simpler APIs, rolls common use cases into prefabricated components for you, and provides better error messages than base TensorFlow. The Advantage of Using Keras To give you an idea of how Keras’s layers can make your job easier, have a look at the code below. With only a few lines of code, you create a regression model—a model that learns patterns from a dataset of numbers and predicts unseen numbers based on those patterns. Assume your features are saved under , and that each element of corresponds to the target label for the corresponding sample in : With only a few lines of code, you did quite a complex task! You created a two-layer regression model with a stochastic gradient descent (SGD) optimizer and mean squared error loss function. Take a look at the code snippet below to see how you can accomplish the same task without using Keras: In this code snippet, you can see how it’s training a two-layer regression model with an SGD optimizer and mean squared error loss function. However, without Keras, you need to do a lot more work! For example, unlike the last approach, you need a custom function for training your model and a custom loop for calculating the error of your model. If your use case doesn’t fall into one of the Core API applications, like building tools on top of TensorFlow or developing your own high-performance platform, you should prefer Keras. TensorFlow has a large and well-established user base and a plethora of tools to help productionize machine learning. For mobile development, it has APIs for JavaScript and Swift, and TensorFlow Lite lets you compress and optimize models for Internet of Things (IoT) devices. Another benefit of TensorFlow is that you can start using it quickly because of the wealth of data, pretrained models, and Google Colab notebooks that both Google and third-parties provide. Also, many popular machine learning algorithms and datasets are built into TensorFlow and are ready to use. In addition to the built-in datasets, you can access Google Research Datasets or use Google’s Dataset Search to find even more. As you’ve already seen, Keras makes it easier for you to get models up and running, so you can try out new techniques in less time. Indeed, Keras is the most used deep learning framework among the top five winningest teams on Kaggle. One drawback is that the update from TensorFlow 1.0 to TensorFlow 2.0 changed so many features that you might find yourself confused. Upgrading code is tedious and error-prone. Many resources, like tutorials, might contain outdated advice. Some highlights of the APIs, extensions, and useful tools of the TensorFlow extended ecosystem include:\n• Model Garden: An official collection of models that use TensorFlow’s high-level APIs.\n• Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: A comprehensive introduction to machine learning using TensorFlow.\n• KerasNLP: A natural language processing library that supports workflows built from modular components that have state-of-the-art preset weights and architectures.\n\nPyTorch was developed by Facebook and was first publicly released in 2016. It was created to offer production optimizations similar to TensorFlow while making models easier to write. Because Python programmers found it so natural to use, PyTorch rapidly gained users, inspiring the TensorFlow team to adopt many of PyTorch’s most popular features in TensorFlow 2.0. PyTorch has a reputation for being more widely used in research than in production. However, since its release the year after TensorFlow, PyTorch has seen a sharp increase in usage by professional developers. The 2023 Stack Overflow Developer Survey list of the most popular “Other Frameworks, Libraries, and Tools” shows that 8.41% of developers use TensorFlow and 7.89% use PyTorch. For context on PyTorch’s growth, the 2020 Stack Overflow Developer Survey indicated that 10.4 percent of professional developers use TensorFlow, while only 4.1 percent use PyTorch. Moreover, the 2018 survey reported that TensorFlow was used by 7.6 percent of developers, compared to just 1.6 percent for PyTorch. As for research, PyTorch is a popular choice, and computer science programs like Stanford’s now use it to teach deep learning. PyTorch is based on Torch, a framework for doing fast computation that is written in C. Torch has a Lua wrapper for constructing models. PyTorch wraps the same C back end in a Python interface. But it’s more than just a wrapper. Developers built it from the ground up to make models easy to write for Python programmers. The underlying, low-level C and C++ code is optimized for running Python code. Because of this tight integration, you get: This means you can write highly customized neural network components directly in Python without having to use a lot of low-level functions. It’s also worth noting that PyTorch’s eager execution, which evaluates tensor operations immediately and dynamically, inspired TensorFlow 2.0, so the APIs for both look a lot alike. Converting NumPy objects to tensors is baked into PyTorch’s core data structures. That means you can easily switch back and forth between objects and objects. For example, you can use PyTorch’s native support for converting NumPy arrays to tensors to create two objects, turn each into a object using , and then take their element-wise product: As you can see in this example, using lets you print out the result of the multiplication, which is a object converted to a object. The most important difference between a object and a object is that the class has different methods and attributes, such as which computes the gradient, and CUDA compatibility. One of the main special features of PyTorch is that it adds a C++ module for autodifferentiation to the Torch backend. Autodifferentiation automatically calculates the gradient of the functions defined in during backpropagation. By default, PyTorch uses eager mode computation. You can run a neural net as you build it, line by line, which makes it easier to debug. It also makes it possible to construct neural nets with conditional execution. This dynamic execution is more intuitive for most Python programmers. Some highlights of the APIs, extensions, and useful tools of the PyTorch extended ecosystem include:\n• fast.ai: An API that makes it straightforward to build models quickly.\n• TorchServe: An open-source model server developed in collaboration between AWS and Facebook.\n• TorchElastic: A framework for training deep neural networks at scale using Kubernetes.\n• PyTorch Hub: An active community for sharing and extending cutting-edge models.\n• TorchVison: A library dedicated to computer vision tasks that offers datasets, model architectures, and common image transformations.\n• TorchText: A natural language processing library that provides data processing utilities and popular datasets for the NLP field.\n\nDeciding which library to use for a project depends on several factors. These include your personal style, the types of models and data you’ll use, and your project goal. When you start your project with a little research on which library best supports these considerations, you’ll set yourself up for success! If you’re a Python programmer and you’re used to the Python style of doing things, then PyTorch will feel easy to pick up. It works the way you’d expect it to, right out of the box. On the other hand, more coding languages are supported in TensorFlow than in PyTorch, which has a C++ API. You can use TensorFlow in both JavaScript and Swift. If you don’t want to write much low-level code, then Keras abstracts away a lot of the details for common use cases so you can build TensorFlow models without sweating the details. What models are you using? If you want to use a specific pretrained model, like BERT or DeepDream, then you should research what it’s compatible with. Some pretrained models are available in only one library or the other, and some are available in both. The Model Garden and the PyTorch and TensorFlow hubs are also good resources to check. Another important question to ask yourself is about the data you’ll be working with. What type of data do you need, or what kind of data do you have? Depending on your dataset requirements and the specific data formats you’re dealing with, PyTorch and TensorFlow both offer built-in functionality and extension libraries. There’s no one-size-fits-all solution, so it makes sense to look at the data formats individually. The main data formats are: In the next few sections, you’ll look at each of these data types and learn how both PyTorch and TensorFlow support them. If your focus is audio data alone, then PyTorch is a good choice. PyTorch offers TorchAudio, which is an exclusive library for handling audio data. With TorchAudio, the complexity of audio processing for machine learning is reduced, thanks to necessary transformations like resampling and short-time Fourier transform. TorchAudio also helps you to access essential datasets, comprehensive feature extraction methods like Mel spectrograms, and efficient file management for different audio types. TensorFlow provides audio processing functionalities mainly through its core API, tf.audio, and additional libraries like TensorFlow I/O and TensorFlow Signal. The API includes capabilities such as decoding WAV files and making spectrograms. With TensorFlow Signal, you get more advanced options such as short-time Fourier transform and Mel spectrograms for deeper signal processing. It also includes audio data augmentation techniques like stretching sounds or changing pitch to build stronger models. So, if you want an easy-to-use library for audio tasks like changing sample rates or creating Mel spectrograms, go with PyTorch’s TorchAudio. It’s made for audio and simplifies a lot of the work. But, if your project needs to handle audio along with other data types, or if you’re after more advanced audio processing, TensorFlow with its , TensorFlow I/O, and TensorFlow Signal might be the better fit. If you’re working with text data, then TorchText is a great choice. TorchText is part of the PyTorch ecosystem and is designed to handle text data for natural language processing tasks. It offers tools for managing data loading, applying text transformations, and integrating with datasets. This library simplifies tasks like text classification and language modeling by making vocabulary creation and tokenization more straightforward. TorchText also provides pretrained components, which let you prototype faster. Because of this, TorchText is really popular in experimentation, yet its modular architecture also offers adaptability for development requirements. TensorFlow has two main companions that extend its capabilities to process text data efficiently—TensorFlow Text and KerasNLP. KerasNLP provides high-level text processing modules that are available as layers or models. If you need access to lower-level tools, you can use TensorFlow Text. It includes advanced tokenization and preprocessing tools, as well as text classification and text generation. So, if you’re looking for something that’s easy to use, quick to set up, and great for trying out new ideas, go with PyTorch’s TorchText. It’s perfect for getting NLP projects off the ground fast. But, if you need to do more detailed work with text, like fine-tuning how words are split up or handling complex text processing, then TensorFlow is the way to go. PyTorch’s TorchVision is made for working with images. It comes with ready-to-use datasets like CIFAR, MNIST, and ImageNet that you can use to test and train your models. You also get access to pretrained models through for tasks like image classification, object detection, and segmentation. This is useful for getting a head start through transfer learning, where you can tweak these models for your specific needs without having to start from zero. TorchVision also includes all the important image transformations you’d need, like cropping, rotating, and normalizing. Plus, there are utilities that simplify the process of loading and showing images in batches, making your workflow smoother. With TensorFlow’s TF Image, part of TensorFlow’s core, you get all the basics for resizing, cropping, and flipping images to get them ready for your models. Then there’s TensorFlow Datasets (TFDS), which offers a huge selection of real-world image datasets that are easy to plug into your projects. When it comes to pretrained models, TensorFlow Hub steps in with models that you can use right away, like ResNet and MobileNet. You can save time by using these models to recognize patterns or objects in images so you don’t have to start from scratch. For image preprocessing, there’s Keras preprocessing layers for preprocessing images directly within your model setups, making everything from adjusting image sizes to enhancing image quality a breeze. When you’re deciding between PyTorch and TensorFlow for your work with images and computer vision tasks, it comes down to what your project needs. Sometimes, the exact tool or data you want to use might only be available in one of these options. Also, consider the ease of integration with your existing workflow and the availability of community support for your tasks, as these factors can significantly impact your project’s success. The final question to ask yourself is about your project goal. Where will your model live? If you want to deploy a model on mobile devices, then TensorFlow is a good bet because of TensorFlow Lite and its Swift API. For serving models, TensorFlow has tight integration with Google Cloud, but PyTorch is integrated into TorchServe on AWS. If you want to enter Kaggle competitions, then Keras will let you quickly iterate over experiments. Think about these questions and examples at the outset of your project. Nail down the two or three most important components, and either TensorFlow or PyTorch will emerge as the right choice. Get Your Decision Guide: Click here to download the free decision guide that will help you choose between PyTorch and TensorFlow for your Python deep learning project."
    },
    {
        "link": "https://github.com/ahsan-83/Deep-Learning-Specialization-Coursera/blob/main/Improving%20Deep%20Neural%20Network/Assignment/Week%203-Programming%20Assignment%20TensorFlow%20Introduction/Tensorflow_introduction.ipynb",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://pub.aimind.so/custom-model-building-in-tensorflow-keras-36bf04da3f3f",
        "document": "The TensorFlow framework accommodates the creation of models spanning a spectrum from straightforward to intricate architectures. In the realm of deep learning, practitioners may possess diverse backgrounds and experience levels. TensorFlow recognizes this diversity and offers various approaches, catering to both beginners and seasoned individuals. One such approach is tf.keras, which represents TensorFlow’s implementation of the Keras API specification. Keras has evolved into a high-level interface for constructing and training models in TensorFlow versions 2.0 and beyond.\n\nIn the Sequential API, neural network layers are organized in a linear, sequential manner, with data flowing through them in a single direction. This approach is widely adopted for crafting straightforward neural network architectures, where layers are stacked one after another. It’s an ideal starting point for beginners due to its user-friendly nature, allowing you to construct models by connecting building blocks in a straightforward manner. However, one limitation of the Sequential API is its restriction to creating Keras models with only one input tensor and one output tensor, making it unsuitable for more complex architectures with multiple inputs or outputs. The Sequential API in Keras is a simple and easy-to-use way to create neural networks, but it does come with some drawbacks:\n• Limited Model Architectures: The Sequential API is designed for linear, sequential models where layers are stacked on top of each other in a single, straight path. This restricts its ability to create more complex network architectures, such as multi-branch models or models with shared layers.\n• Inflexibility: The Sequential API doesn’t provide the flexibility to reuse layers easily. Once you’ve added a layer to the model, you can’t easily connect it to multiple parts of the network or reuse it in different ways within the same model. The Functional API is better suited for such scenarios.\n• Limited Access to Intermediate Layers: In some advanced use cases, you might want to access intermediate layers or their outputs for various purposes like feature extraction or implementing custom loss functions. The Sequential API doesn’t make this as straightforward as the Functional API, where you can easily access intermediate outputs.\n• Complex Models are Challenging: As your models become more complex, you might find it challenging to express them using only the Sequential API. If you need to implement intricate skip connections, shared layers, or models with multiple inputs and outputs, you will need to resort to the Functional API or Subclassing API, which offers greater flexibility.\n• Debugging and Visualization: Debugging complex models or visualizing the architecture can be more difficult with the Sequential API compared to the Functional API, where you have a clear, explicit graph of the model. This can be especially challenging when dealing with models that have branches or skip connections. In summary, while the Sequential API is excellent for simple, feedforward neural networks, it can be limiting for more complex and customized model architectures. In such cases, the Functional API or Subclassing API in Keras provides more flexibility and control.\n\nThis approach empowers you to construct neural network models featuring multiple inputs and outputs, as well as the capability to incorporate shared layers. It not only encompasses all the functionality of the sequential API but also offers enhanced control over how layers interact with one another. The Functional API adopts a Lego-like concept, where components can be assembled to construct a framework for your final model. Think of these components as Lego bricks that can be arranged and stacked in various configurations to define your model’s architecture. Once these components are created, they can be reused multiple times, providing the advantage of reusability. def CNN_block(x,filters, kernel_size=3):\n\n # Defining a function which returns an operation of Convolution, BatchNormalisation and Relu.\n\n # Here x is the input.\n\n x = layers.Conv2D(filters, kernel_size=3)(x)\n\n x = layers.BatchNormalization()(x)\n\n\n\n return tf.nn.relu(x) def inception_block(x, filter1, filter2):\n\n # Defining a function which applies two CNN blocks. The output of both these blocks are then concatenated\n\n conv1 = CNN_block(x, filter1)\n\n conv2 = CNN_block(x, filter2)\n\n x = keras.layers.concatenate([conv1, conv2])\n\n\n\n return x In this context, the inception_block leverages the functionality of two branches from the CNN_block. These outputs originating from these dual branches are subsequently merged by employing two instances of the CNN_block, arranged as a list and processed through the layers.concatenate layer. This capability is referred to as Shared Layers, wherein the same block is employed within a new block or Lego-like structure. By sharing information across multiple inputs, shared layers enable you to train a model effectively even when you have limited data. Now that all the necessary building blocks for the model have been defined, you can proceed to construct its architecture. The Functional API effectively bridges the gap by enabling the creation of more intricate models. This approach facilitates the incorporation of multiple inputs and outputs, the utilization of shared layers, the implementation of branching structures, and the promotion of reusability. However, within this approach, operations are performed on pre-existing built-in layers. The Keras API offers a wide array of these built-in layers, including but not limited to:\n• BatchNormalization, Dropout, Embedding, and many others. However, if you prefer not to use the predefined built-in layers and desire the flexibility to design your custom layers, then the third approach — Model Subclassing — offers an elegant and tailored solution.\n\nIn TensorFlow, if you desire to create custom layers, such as your own dense layer, or if you intend to craft unique activation functions like softmax or ReLU, you have the freedom to do so. Typically, when developing machine learning models, you would use the provided built-in layers, which are designed to address conventional tasks efficiently. However, if your objective is to work at a lower level, manipulating individual operations and variables rather than relying on a higher level of abstraction, you can achieve this by creating classes that inherit from the tf.keras.Layer class. This approach liberates you from being solely dependent on the pre-existing layers offered by the Keras API. Let’s explore how the concept of subclassing empowers you to construct novel and custom layers. In Keras, a fundamental building block is the Layer class. A layer comprises two essential components: a state, which encompasses the layer’s “weights,” and a transformation that maps inputs to outputs, often referred to as a “call” representing the layer’s forward pass. For instance, consider a densely-connected layer. Within this layer, there exists a state, specifically the variables ‘w’ and ‘b.’ class custom_dense(keras.layers.Layer):\n\n def __init__(self, units, input_dim):\n\n super().__init__()\n\n self.w = self.add_weight( # specifying this is a weight and trainable.\n\n name=\"w\",\n\n shape=(input_dim, units),\n\n initializer=\"random_normal\",\n\n trainable=True,\n\n )\n\n self.b = self.add_weight(\n\n name=\"b\", shape=(units,), initializer=\"zeros\", trainable=True\n\n )\n\n\n\n def call(self, inputs):\n\n return tf.matmul(inputs, self.w) + self.b The init() defines all the instances of the custom layers that will be utilised in building the model. Once all the instances are defined, you can create the call() method which overrides how the computation should happen between the instances & other layers.\n\nIn summary, custom datasets in TensorFlow offer the flexibility and customization required to work with a wide range of data sources, formats, and machine learning tasks, making them a fundamental component of TensorFlow workflows. When working with image-based models, the typical workflow involves gathering data from memory, applying essential preprocessing and transformations to each image, and then randomly selecting batches of these images for training the model. Likewise, for text-based models, the process entails extracting meaningful information from raw data, cleaning and converting them into tokens, implementing necessary preprocessing steps, and grouping them into batches suitable for training text models. The TensorFlow tf.data API is a valuable tool for constructing and managing intricate input pipelines. It empowers you to efficiently handle extensive datasets and perform intricate data transformations. To illustrate this process, let’s create a dataset using the tf.data API with the MNIST dataset as an example.\n• The tf.data API provides the tf.data.Dataset.prefetch transformation. It can be used to decouple the time when data is produced from the time when data is consumed.’\n• The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step.\n• You could either manually tune this value, or set it to tf.data.AUTOTUNE, which will prompt the tf.data runtime to tune the value dynamically at runtime. The input pipeline starts from importing the data and creating a dataset from the data stored in the memory. For this, you can use tf.data.Dataset.from_tensor_slices(), which creates a tf.data.Dataset the object whose elements are slices of the passed tensors. Once you have created the object, you can transform it by applying different operations to the dataset object. (for example, Dataset.map() orDataset.batch()). Once you have built the dataset, the rest of the process remains the same as you have done while building your custom layers & models. Important Note: All three modeling approaches — Sequential, Functional, and Subclassed models — can seamlessly interact with each other. This means that within your Subclassed model, you have the flexibility to incorporate a Sequential or Functional model to define the architecture of your neural network.\n\nThe most intriguing aspect of model training lies in crafting a custom training loop. While the fit() method offers a seamless training experience, it’s essential to understand the inner workings. Beneath the surface of the fit() method lies the backpropagation process, where your model’s parameters are optimized to capture the relationship between input data and target values. This process can be distilled into these fundamental steps:\n• Compute the loss by comparing these predictions to the target values.\n• Conduct error backpropagation by tracking gradients in your computations.\n• Apply these gradients to your model’s parameters using an optimizer. TensorFlow equips us with the GradientTape() functionality, enabling meticulous monitoring of operations and facilitating gradient computation, thereby providing precise control over every detail of the training process. For automatic differentiation, TensorFlow must maintain a record of the sequence of operations during the forward pass. During the subsequent backward pass, TensorFlow traverses this operation list in reverse order to compute gradients.\n• In the context of eager execution, TensorFlow computes tensor values as they are encountered in code. Consequently, it does not precompute a static graph that relies on input placeholders.\n• Therefore, to propagate errors backward, you must manually track the gradients of your computations and subsequently apply these gradients to an optimizer.\n• If you’ve created a custom layer, you have precise control over the operations within that layer, including the computation of gradients and the accumulation of loss. The Gradient Tape feature grants direct access to individual gradients within the layer. Once you’ve recorded relevant operations, you can use GradientTape.gradient(target, sources) to compute the gradient of a specific target (typically a loss) with respect to specific sources (often the model’s variables). Just as you’ve harnessed the capabilities of tf.keras.Model and tf.keras.layers.Layer for crafting your custom model and layers, you similarly want to leverage the convenient features offered by the fit() method. This method empowers you to retain control over fine-grained details and each operation while preserving a high-level abstraction and simplicity. In the process of customizing the fit() method, you need to override the train_step function within your Model class, akin to how we’ve overridden the call() method to customize the forward pass. The train_step function is automatically invoked when you apply the fit() method to your model. Consequently, this approach seamlessly integrates customization within the framework of the fit() method. In essence, you can succinctly summarize the entire custom training procedure in these steps:\n• Return a dictionary containing metric names along with their respective values. class Custom_fit(keras.Model):\n\n def __init__(self, model):\n\n super().__init__()\n\n self.model = model\n\n\n\n def compile(self, optimizer, loss, metric):\n\n super(Custom_fit, self).compile()\n\n self.optimizer = optimizer\n\n self.loss = loss\n\n self.metric = metric\n\n\n\n def train_step(self,data):\n\n # Unpack the data\n\n x, y = data\n\n with tf.GradientTape() as tape:\n\n # Compute predictions\n\n y_pred = self.model(x, training=True)\n\n # Calculating loss\n\n loss = self.loss(y, y_pred)\n\n \n\n # Tracking gradients\n\n training_vars = self.trainable_variables\n\n gradients = tape.gradient(loss, training_vars)\n\n\n\n #Update optimizer & metrics\n\n self.optimizer.apply_gradients(zip(gradients, training_vars))\n\n self.metric.update_state(y, y_pred)\n\n\n\n return {\"Train_loss for Custom_train\": loss, \"Train_accuracy for Custom_train\": self.metric.result()}\n\n\n\n\n\n def test_step(self, data):\n\n # Unpack the data\n\n x, y = data\n\n # Compute predictions\n\n y_pred = self.model(x, training=False)\n\n # Calculating loss\n\n loss = self.loss(y, y_pred)\n\n #Update optimizer & metrics\n\n self.metric.update_state(y, y_pred)\n\n return {\"Test_loss for Custom_test\": loss, \"Test_accuracy for Custom_test\": self.metric.result()}"
    }
]