[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html",
        "document": "Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.\n\nWith sparse inputs, the ARPACK implementation of the truncated SVD can be used (i.e. through ). Alternatively, one may consider where the data are not centered.\n\nNotice that this class only supports sparse inputs for some solvers such as “arpack” and “covariance_eigh”. See for an alternative with sparse data.\n\nFor a usage example, see Principal Component Analysis (PCA) on Iris Dataset\n\nRead more in the User Guide.\n\nNumber of components to keep. if n_components is not set all components are kept: If and , Minka’s MLE is used to guess the dimension. Use of will interpret as . If and , select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components. If , the number of components must be strictly less than the minimum of n_features and n_samples. Hence, the None case results in: If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead. When True (False by default) the vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions. The solver is selected by a default ‘auto’ policy is based on and : if the input data has fewer than 1000 features and more than 10 times as many samples, then the “covariance_eigh” solver is used. Otherwise, if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient “randomized” method is selected. Otherwise the exact “full” SVD is computed and optionally truncated afterwards. Run exact full SVD calling the standard LAPACK solver via and select the components by postprocessing Precompute the covariance matrix (on centered data), run a classical eigenvalue decomposition on the covariance matrix typically using LAPACK and select the components by postprocessing. This solver is very efficient for n_samples >> n_features and small n_features. It is, however, not tractable otherwise for large n_features (large memory footprint required to materialize the covariance matrix). Also note that compared to the “full” solver, this solver effectively doubles the condition number and is therefore less numerical stable (e.g. on input data with a large range of singular values). Run SVD truncated to calling ARPACK solver via . It requires strictly Run randomized SVD by the method of Halko et al. Tolerance for singular values computed by svd_solver == ‘arpack’. Must be of range [0.0, infinity). Number of iterations for the power method computed by svd_solver == ‘randomized’. Must be of range [0, infinity). This parameter is only relevant when . It corresponds to the additional number of random vectors to sample the range of so as to ensure proper conditioning. See for more details. Power iteration normalizer for randomized SVD solver. Not used by ARPACK. See for more details. Used when the ‘arpack’ or ‘randomized’ solvers are used. Pass an int for reproducible results across multiple function calls. See Glossary. Principal axes in feature space, representing the directions of maximum variance in the data. Equivalently, the right singular vectors of the centered input data, parallel to its eigenvectors. The components are sorted by decreasing . The amount of variance explained by each of the selected components. The variance estimation uses degrees of freedom. Equal to n_components largest eigenvalues of the covariance matrix of X. Percentage of variance explained by each of the selected components. If is not set then all components are stored and the sum of the ratios is equal to 1.0. The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the variables in the lower-dimensional space. Per-feature empirical mean, estimated from the training set. The estimated number of components. When n_components is set to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None. Number of samples in the training data. The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to compute the estimated data covariance and score samples. Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X. Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nFor n_components == ‘mle’, this class uses the method from: Minka, T. P.. “Automatic choice of dimensionality for PCA”. In NIPS, pp. 598-604\n\nImplements the probabilistic PCA model from: Tipping, M. E., and Bishop, C. M. (1999). “Probabilistic principal component analysis”. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods.\n\nFor svd_solver == ‘randomized’, see: Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). “Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions”. SIAM review, 53(2), 217-288. and also Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). “A randomized algorithm for the decomposition of matrices”. Applied and Computational Harmonic Analysis, 30(1), 47-68."
    },
    {
        "link": "https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nThis example compares different (linear) dimensionality reduction methods applied on the Digits data set. The data set contains images of digits from 0 to 9 with approximately 180 samples of each class. Each image is of dimension 8x8 = 64, and is reduced to a two-dimensional data point.\n\nPrincipal Component Analysis (PCA) applied to this data identifies the combination of attributes (principal components, or directions in the feature space) that account for the most variance in the data. Here we plot the different samples on the 2 first principal components.\n\nLinear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes. In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.\n\nNeighborhood Components Analysis (NCA) tries to find a feature space such that a stochastic nearest neighbor algorithm will give the best accuracy. Like LDA, it is a supervised method.\n\nOne can see that NCA enforces a clustering of the data that is visually meaningful despite the large reduction in dimension.\n\n# Use a nearest neighbor classifier to evaluate the methods # Make a list of the methods to be compared # Compute the nearest neighbor accuracy on the embedded test set # Embed the data set in 2 dimensions using the fitted model # Plot the projected points and show the evaluation score"
    },
    {
        "link": "https://stackoverflow.com/questions/46702935/scikit-learn-principal-component-analysis-pca-for-dimension-reduction",
        "document": "I want to perform principal component analysis for dimension reduction and data integration.\n\nI have 3 features(variables) and 5 samples like below. I want to integrate them into 1-dimensional(1 feature) output by transforming them(computing 1st PC). I want to use transformed data for further statistical analysis, because I believe that it displays the 'main' characteristics of 3 input features.\n\nI first wrote a test code with python using scikit-learn like below. It is the simple case that the values of 3 features are all equivalent. In other word, I applied PCA for three same vector, [0, 1, 2, 1, 0].\n• Is taking 1st PCA after dimension reduction proper approach for data integration?\n\n1-2. For example, if features are like [power rank, speed rank], and power have roughly negative correlation with speed, when it is a 2-feature case. I want to know the sample which have both 'high power' and 'high speed'. It is easy to decide that [power 1, speed 1] is better than [power 2, speed 2], but difficult for the case like [power 4, speed 2] vs [power 3, speed 3]. So I want to apply PCA to 2-dimensional 'power and speed' dataset, and take 1st PC, then use the rank of '1st PC'. Is this kind of approach still proper?\n• In this case, I think the output should also be [0, 1, 2, 1, 0] which is the same as the input. But output was [-1.38564065, 0.34641016, 2.07846097, 0.34641016, -1.38564065]. Are there any problem with the code, or is it the right answer?"
    },
    {
        "link": "https://kdnuggets.com/2023/05/principal-component-analysis-pca-scikitlearn.html",
        "document": "By, KDnuggets Contributing Editor & Technical Content Specialist on May 16, 2023 in Machine Learning\n\nLearn how to perform principal component analysis (PCA) in Python using the scikit-learn library.\n\nIf you’re familiar with the unsupervised learning paradigm, you’d have come across dimensionality reduction and the algorithms used for dimensionality reduction such as the principal component analysis (PCA). Datasets for machine learning typically contain a large number of features, but such high-dimensional feature spaces are not always helpful.\n\nIn general, all the features are not equally important and there are certain features that account for a large percentage of variance in the dataset. Dimensionality reduction algorithms aim to reduce the dimension of the feature space to a fraction of the original number of dimensions. In doing so, the features with high variance are still retained—but are in the transformed feature space. And principal component analysis (PCA) is one of the most popular dimensionality reduction algorithms.\n\nIn this tutorial, we’ll learn how principal component analysis (PCA) works and how to implement it using the scikit-learn library.\n\nBefore we go ahead and implement principal component analysis (PCA) in scikit-learn, it’s helpful to understand how PCA works.\n\nAs mentioned, principal component analysis is a dimensionality reduction algorithm. Meaning it reduces the dimensionality of the feature space. But how does it achieve this reduction?\n\nThe motivation behind the algorithm is that there are certain features that capture a large percentage of variance in the original dataset. So it's important to find the directions of maximum variance in the dataset. These directions are called principal components. And PCA is essentially a projection of the dataset onto the principal components.\n\nSo how do we find the principal components?\n\nSuppose the data matrix X is of dimensions num_observations x num_features, we perform eigenvalue decomposition on the covariance matrix of X.\n\nIf the features are all zero mean, then the covariance matrix is given by X.T X. Here, X.T is the transpose of the matrix X. If the features are not all zero mean initially, we can subtract the mean of column i from each entry in that column and compute the covariance matrix. It’s simple to see that the covariance matrix is a square matrix of order num_features.\n\nThe first k principal components are the eigenvectors corresponding to the k largest eigenvalues.\n\nSo the steps in PCA can be summarized as follows:\n\n \n\n\n\nBecause the covariance matrix is a symmetric and positive semi-definite, the eigendecomposition takes the following form:\n\nWhere, D is the matrix of eigenvectors and Λ is a diagonal matrix of eigenvalues.\n\nAnother matrix factorization technique that can be used to compute principal components is singular value decomposition or SVD.\n\nSingular value decomposition (SVD) is defined for all matrices. Given a matrix X, SVD of X gives: X = U Σ V.T. Here, U, Σ, and V are the matrices of left singular vectors, singular values, and right singular vectors, respectively. V.T. is the transpose of V.\n\nSo the SVD of the covariance matrix of X is given by:\n\n \n\n\n\nComparing the equivalence of the two matrix decompositions:\n\nWe have the following:\n\nThere are computationally efficient algorithms for calculating the SVD of a matrix. The scikit-learn implementation of PCA also uses SVD under the hood to compute the principal components.\n\nNow that we’ve learned the basics of principal component analysis, let’s proceed with the scikit-learn implementation of the same.\n\nTo understand how to implement principal component analysis, let’s use a simple dataset. In this tutorial, we’ll use the wine dataset available as part of scikit-learn's datasets module.\n\nLet’s start by loading and preprocessing the dataset:\n\nIt has 13 features and 178 records in all.\n\nAs a next step, let's preprocess the dataset. The features are all on different scales. To bring them all to a common scale, we’ll use the that transforms the features to have zero mean and unit variance:\n\nTo find the principal components, we can use the PCA class from scikit-learn’s decomposition module.\n\nLet’s instantiate a PCA object by passing in the number of principal components to the constructor.\n\nThe number of principal components is the number of dimensions that you’d like to reduce the feature space to. Here, we set the number of components to 3.\n\nInstead of calling the method, you can also call followed by the method.\n\nNotice how the steps in principal component analysis such as computing the covariance matrix, performing eigendecomposition or singular value decomposition on the covariance matrix to get the principal components have all been abstracted away when we use scikit-learn’s implementation of PCA.\n\nStep 4 – Examining Some Useful Attributes of the PCA Object\n\nThe PCA instance that we created has several useful attributes that help us understand what is going on under the hood.\n\nThe attribute stores the directions of maximum variance (the principal components).\n\nWe mentioned that the principal components are directions of maximum variance in the dataset. But how do we measure how much of the total variance is captured in the number of principal components we just chose?\n\nThe attribute captures the ratio of the total variance each principal component captures. Sowe can sum up the ratios to get the total variance in the chosen number of components.\n\nHere, we see that three principal components capture over 66.5% of total variance in the dataset.\n\nWe can try running principal component analysis by varying the number of components .\n\nTo visualize the for the number of components, let’s plot the two quantities as shown:\n\nWhen we use all the 13 components, the is 1.0 indicating that we’ve captured 100% of the variance in the dataset.\n\nIn this example, we see that with 6 principal components, we'll be able to capture more than 80% of variance in the input dataset.\n\n \n\n\n\nI hope you’ve learned how to perform principal component analysis using built-in functionality in the scikit-learn library. Next, you can try to implement PCA on a dataset of your choice. If you’re looking for good datasets to work with, check out this list of websites to find datasets for your data science projects.\n\n[1] Computational Linear Algebra, fast.ai\n\n \n\n \n\n Bala Priya C is a developer and technical writer from India. She likes working at the intersection of math, programming, data science, and content creation. Her areas of interest and expertise include DevOps, data science, and natural language processing. She enjoys reading, writing, coding, and coffee! Currently, she's working on learning and sharing her knowledge with the developer community by authoring tutorials, how-to guides, opinion pieces, and more."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/unsupervised_reduction.html",
        "document": "If your number of features is high, it may be useful to reduce it with an unsupervised step prior to supervised steps. Many of the Unsupervised learning methods implement a method that can be used to reduce the dimensionality. Below we discuss two specific example of this pattern that are heavily used.\n\nThe unsupervised data reduction and the supervised estimator can be chained in one step. See Pipeline: chaining estimators.\n\napplies Hierarchical clustering to group together features that behave similarly. Note that if features have very different scaling or statistical properties, may not be able to capture the links between related features. Using a can be useful in these settings."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html",
        "document": "Read more in the User Guide.\n\nThe number of clusters to form as well as the number of centroids to generate. For an example of how to choose an optimal value for refer to Selecting the number of clusters with silhouette analysis on KMeans clustering.\n• None ‘k-means++’ : selects initial cluster centroids using sampling based on an empirical probability distribution of the points’ contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is “greedy k-means++”. It differs from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them.\n• None ‘random’: choose observations (rows) at random from data for the initial centroids.\n• None If an array is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n• None If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization. For an example of how to use the different strategies, see A demo of K-Means clustering on the handwritten digits data. For an evaluation of the impact of initialization, see the example Empirical evaluation of the impact of k-means initialization. Number of times the k-means algorithm is run with different centroid seeds. The final results is the best output of consecutive runs in terms of inertia. Several runs are recommended for sparse high-dimensional problems (see Clustering sparse data with k-means). When , the number of runs depends on the value of init: 10 if using or is a callable; 1 if using or is an array-like. Changed in version 1.4: Default value for changed to . Maximum number of iterations of the k-means algorithm for a single run. Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence. Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See Glossary. When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. Note that if the original data is not C-contiguous, a copy will be made even if copy_x is False. If the original data is sparse, but not in CSR format, a copy will be made even if copy_x is False. K-means algorithm to use. The classical EM-style algorithm is . The variation can be more efficient on some datasets with well-defined clusters, by using the triangle inequality. However it’s more memory intensive due to the allocation of an extra array of shape . Changed in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”. Changed “auto” to use “lloyd” instead of “elkan”. Coordinates of cluster centers. If the algorithm stops before fully converging (see and ), these will not be consistent with . Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided. Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nThe k-means problem is solved using either Lloyd’s or Elkan’s algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of samples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. Refer to “How slow is the k-means method?” D. Arthur and S. Vassilvitskii - SoCG2006. for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That’s why it can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of or ), and will not be consistent, i.e. the will not be the means of the points in each cluster. Also, the estimator will reassign after the last iteration to make consistent with on the training set.\n\nFor examples of common problems with K-Means and how to address them see Demonstration of k-means assumptions.\n\nFor a demonstration of how K-Means can be used to cluster text documents see Clustering text documents using k-means.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example Comparison of the K-Means and MiniBatchKMeans clustering algorithms.\n\nFor a comparison between K-Means and BisectingKMeans refer to example Bisecting K-Means and Regular K-Means Performance Comparison.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/clustering.html",
        "document": "Clustering of unlabeled data can be performed with the module .\n\nEach clustering algorithm comes in two variants: a class, that implements the method to learn the clusters on train data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the attribute.\n\ncreates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given. Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the preference, which controls how many exemplars are used, and the damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages. The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \\(O(N^2 T)\\), where \\(N\\) is the number of samples and \\(T\\) is the number of iterations until convergence. Further, the memory complexity is of the order \\(O(N^2)\\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets. The messages sent between points belong to one of two categories. The first is the responsibility \\(r(i, k)\\), which is the accumulated evidence that sample \\(k\\) should be the exemplar for sample \\(i\\). The second is the availability \\(a(i, k)\\) which is the accumulated evidence that sample \\(i\\) should choose sample \\(k\\) to be its exemplar, and considers the values for all other samples that \\(k\\) should be an exemplar. In this way, exemplars are chosen by samples if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves. More formally, the responsibility of a sample \\(k\\) to be the exemplar of sample \\(i\\) is given by: Where \\(s(i, k)\\) is the similarity between samples \\(i\\) and \\(k\\). The availability of sample \\(k\\) to be the exemplar of sample \\(i\\) is given by: To begin with, all values for \\(r\\) and \\(a\\) are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor \\(\\lambda\\) is introduced to iteration process: where \\(t\\) indicates the iteration times.\n• None Demo of affinity propagation clustering algorithm: Affinity Propagation on a synthetic 2D datasets with 3 classes\n• None Visualizing the stock market structure Affinity Propagation on financial time series to find groups of companies\n\nThe algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, and , which define formally what we mean when we say dense. Higher or lower indicate higher density necessary to form a cluster. More formally, we define a core sample as being a sample in the dataset such that there exist other samples within a distance of , which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster. Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least in distance from any core sample, is considered an outlier by the algorithm. While the parameter primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below). In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below. The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering. The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see . This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g., with sparse matrices). This matrix will consume \\(n^2\\) floats. A couple of mechanisms for getting around this are:\n• None Use OPTICS clustering in conjunction with the method. OPTICS clustering also calculates the full pairwise matrix, but only keeps one row in memory at a time (memory complexity n).\n• None A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with . See .\n• None The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a when fitting DBSCAN.\n• None A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n• None DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.\n\nThe algorithm shares many similarities with the algorithm, and can be considered a generalization of DBSCAN that relaxes the requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a distance, and a spot within the cluster attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for , then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given value using the method. Setting to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points. The reachability distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining reachability distances and data set produces a reachability plot, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. ‘Cutting’ the reachability plot at a single value produces DBSCAN like results; all points above the ‘cut’ are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter . There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster. The results from OPTICS method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise. Note that for any single value of , DBSCAN will tend to have a shorter run time than OPTICS; however, for repeated runs at varying values, a single run of OPTICS may require less cumulative runtime than DBSCAN. It is also important to note that OPTICS’ output is close to DBSCAN’s only if and are close. Spatial indexing trees are used to avoid calculating the full distance matrix, and allow for efficient memory usage on large sets of samples. Different distance metrics can be supplied via the keyword. For large datasets, similar (but not identical) results can be obtained via . The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain \\(n\\) (as opposed to \\(n^2\\)) memory scaling; however, tuning of the parameter will likely need to be used to give a solution in a reasonable amount of wall time.\n• None “OPTICS: ordering points to identify the clustering structure.” Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999."
    },
    {
        "link": "https://datacamp.com/tutorial/k-means-clustering-python",
        "document": "In this course, you will be introduced to unsupervised learning through techniques such as hierarchical and k-means clustering using the SciPy library."
    },
    {
        "link": "https://stackoverflow.com/questions/19197715/scikit-learn-k-means-elbow-criterion",
        "document": "If the true label is not known in advance(as in your case), then can be evaluated using either Elbow Criterion or Silhouette Coefficient.\n\nThe idea behind elbow method is to run k-means clustering on a given dataset for a range of values of k ( , e.g k=1 to 10), and for each value of k, calculate sum of squared errors (SSE).\n\nAfter that, plot a line graph of the SSE for each value of k. If the line graph looks like an arm - a red circle in below line graph (like angle), the \"elbow\" on the arm is the value of optimal k (number of cluster). Here, we want to minimize SSE. SSE tends to decrease toward 0 as we increase k (and SSE is 0 when k is equal to the number of data points in the dataset, because then each data point is its own cluster, and there is no error between it and the center of its cluster).\n\nSo the goal is to choose a that still has a low SSE, and the elbow usually represents where we start to have diminishing returns by increasing k.\n\nWe can see in plot, 3 is the optimal number of clusters (encircled red) for iris dataset, which is indeed correct.\n\nA higher Silhouette Coefficient score relates to a model with better-defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores: `\n\nThe Silhouette Coefficient is for a single sample is then given as:\n\nNow, to find the optimal value of for , loop through 1..n for n_clusters in and calculate Silhouette Coefficient for each sample.\n\nA higher Silhouette Coefficient indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n\nFor n_clusters=2, The Silhouette Coefficient is 0.680813620271\n\n For n_clusters=3, The Silhouette Coefficient is 0.552591944521\n\n For n_clusters=4, The Silhouette Coefficient is 0.496992849949 \n\n For n_clusters=5, The Silhouette Coefficient is 0.488517550854 \n\n For n_clusters=6, The Silhouette Coefficient is 0.370380309351\n\n For n_clusters=7, The Silhouette Coefficient is 0.356303270516\n\n For n_clusters=8, The Silhouette Coefficient is 0.365164535737\n\n For n_clusters=9, The Silhouette Coefficient is 0.346583642095\n\n For n_clusters=10, The Silhouette Coefficient is 0.328266088778\n\n\n\nAs we can see, n_clusters=2 has highest Silhouette Coefficient. This means that 2 should be the optimal number of cluster, Right?\n\nIris dataset has 3 species of flower, which contradicts the 2 as an optimal number of cluster. So despite n_clusters=2 having highest Silhouette Coefficient, We would consider n_clusters=3 as optimal number of cluster due to -\n• n_clusters=3 has the 2nd highest value of Silhouette Coefficient.\n\nSo choosing n_clusters=3 is the optimal no. of cluster for iris dataset.\n\nChoosing optimal no. of the cluster will depend on the type of datasets and the problem we are trying to solve. But most of the cases, taking highest Silhouette Coefficient will yield an optimal number of cluster."
    },
    {
        "link": "https://realpython.com/k-means-clustering-python",
        "document": "The k-means clustering method is an unsupervised machine learning technique used to identify clusters of data objects in a dataset. There are many different types of clustering methods, but k-means is one of the oldest and most approachable. These traits make implementing k-means clustering in Python reasonably straightforward, even for novice programmers and data scientists.\n\nIf you’re interested in learning how and when to implement k-means clustering in Python, then this is the right place. You’ll walk through an end-to-end example of k-means clustering using Python, from preprocessing the data to evaluating results.\n• When to use k-means clustering to analyze your data\n• How to implement k-means clustering in Python with scikit-learn\n• How to select a meaningful number of clusters\n\nClick the link below to download the code you’ll use to follow along with the examples in this tutorial and implement your own k-means clustering pipeline:\n\nClustering is a set of techniques used to partition data into groups, or clusters. Clusters are loosely defined as groups of data objects that are more similar to other objects in their cluster than they are to data objects in other clusters. In practice, clustering helps identify two qualities of data: Meaningful clusters expand domain knowledge. For example, in the medical field, researchers applied clustering to gene expression experiments. The clustering results identified groups of patients who respond differently to medical treatments. Useful clusters, on the other hand, serve as an intermediate step in a data pipeline. For example, businesses use clustering for customer segmentation. The clustering results segment customers into groups with similar purchase histories, which businesses can then use to create targeted advertising campaigns. Note: You’ll learn about unsupervised machine learning techniques in this tutorial. If you’re interested in learning more about supervised machine learning techniques, then check out Logistic Regression in Python. There are many other applications of clustering, such as document clustering and social network analysis. These applications are relevant in nearly every industry, making clustering a valuable skill for professionals working with data in any field. You can perform clustering using many different approaches—so many, in fact, that there are entire categories of clustering algorithms. Each of these categories has its own unique strengths and weaknesses. This means that certain clustering algorithms will result in more natural cluster assignments depending on the input data. Note: If you’re interested in learning about clustering algorithms not mentioned in this section, then check out A Comprehensive Survey of Clustering Algorithms for an excellent review of popular techniques. Selecting an appropriate clustering algorithm for your dataset is often difficult due to the number of choices available. Some important factors that affect this decision include the characteristics of the clusters, the features of the dataset, the number of outliers, and the number of data objects. You’ll explore how these factors help determine which approach is most appropriate by looking at three popular categories of clustering algorithms: It’s worth reviewing these categories at a high level before jumping right into k-means. You’ll learn the strengths and weaknesses of each category to provide context for how k-means fits into the landscape of clustering algorithms. Partitional clustering divides data objects into nonoverlapping groups. In other words, no object can be a member of more than one cluster, and every cluster must have at least one object. These techniques require the user to specify the number of clusters, indicated by the variable k. Many partitional clustering algorithms work through an iterative process to assign subsets of data points into k clusters. Two examples of partitional clustering algorithms are k-means and k-medoids. These algorithms are both nondeterministic, meaning they could produce different results from two separate runs even if the runs were based on the same input.\n• They work well when clusters have a spherical shape. They also have several weaknesses:\n• They’re not well suited for clusters with complex shapes and different sizes.\n• They break down when used with clusters of different densities. Hierarchical clustering determines cluster assignments by building a hierarchy. This is implemented by either a bottom-up or a top-down approach:\n• Agglomerative clustering is the bottom-up approach. It merges the two points that are the most similar until all points have been merged into a single cluster.\n• Divisive clustering is the top-down approach. It starts with all points as one cluster and splits the least similar clusters at each step until only single data points remain. These methods produce a tree-based hierarchy of points called a dendrogram. Similar to partitional clustering, in hierarchical clustering the number of clusters (k) is often predetermined by the user. Clusters are assigned by cutting the dendrogram at a specified depth that results in k groups of smaller dendrograms. Unlike many partitional clustering techniques, hierarchical clustering is a deterministic process, meaning cluster assignments won’t change when you run an algorithm twice on the same input data. The strengths of hierarchical clustering methods include the following:\n• They often reveal the finer details about the relationships between data objects. The weaknesses of hierarchical clustering methods include the following: Density-based clustering determines cluster assignments based on the density of data points in a region. Clusters are assigned where there are high densities of data points separated by low-density regions. Unlike the other clustering categories, this approach doesn’t require the user to specify the number of clusters. Instead, there is a distance-based parameter that acts as a tunable threshold. This threshold determines how close points must be to be considered a cluster member. Examples of density-based clustering algorithms include Density-Based Spatial Clustering of Applications with Noise, or DBSCAN, and Ordering Points To Identify the Clustering Structure, or OPTICS. The strengths of density-based clustering methods include the following:\n• They excel at identifying clusters of nonspherical shapes. The weaknesses of density-based clustering methods include the following:\n• They aren’t well suited for clustering in high-dimensional spaces.\n• They have trouble identifying clusters of varying densities.\n\nHow to Perform K-Means Clustering in Python In this section, you’ll take a step-by-step tour of the conventional version of the k-means algorithm. Understanding the details of the algorithm is a fundamental step in the process of writing your k-means clustering pipeline in Python. What you learn in this section will help you decide if k-means is the right choice to solve your clustering problem. Conventional k-means requires only a few steps. The first step is to randomly select k centroids, where k is equal to the number of clusters you choose. Centroids are data points representing the center of a cluster. The main element of the algorithm works by a two-step process called expectation-maximization. The expectation step assigns each data point to its nearest centroid. Then, the maximization step computes the mean of all the points for each cluster and sets the new centroid. Here’s what the conventional version of the k-means algorithm looks like: The quality of the cluster assignments is determined by computing the sum of the squared error (SSE) after the centroids converge, or match the previous iteration’s assignment. The SSE is defined as the sum of the squared Euclidean distances of each point to its closest centroid. Since this is a measure of error, the objective of k-means is to try to minimize this value. The figure below shows the centroids and SSE updating through the first five iterations from two different runs of the k-means algorithm on the same dataset: The purpose of this figure is to show that the initialization of the centroids is an important step. It also highlights the use of SSE as a measure of clustering performance. After choosing a number of clusters and the initial centroids, the expectation-maximization step is repeated until the centroid positions reach convergence and are unchanged. The random initialization step causes the k-means algorithm to be nondeterministic, meaning that cluster assignments will vary if you run the same algorithm twice on the same dataset. Researchers commonly run several initializations of the entire k-means algorithm and choose the cluster assignments from the initialization with the lowest SSE. Writing Your First K-Means Clustering Code in Python Thankfully, there’s a robust implementation of k-means clustering in Python from the popular machine learning package scikit-learn. You’ll learn how to write a practical implementation of the k-means algorithm using the scikit-learn version of the algorithm. Note: If you’re interested in gaining a deeper understanding of how to write your own k-means algorithm in Python, then check out the Python Data Science Handbook. The code in this tutorial requires some popular external Python packages and assumes that you’ve installed Python with Anaconda. For more information on setting up your Python environment for machine learning in Windows, read through Setting Up Python for Machine Learning on Windows. Otherwise, you can begin by installing the required packages: The code is presented so that you can follow along in an console or Jupyter Notebook. Click the prompt ( ) at the top right of each code block to see the code formatted for copy-paste. You can also download the source code used in this article by clicking on the link below: Download the sample code: Click here to get the code you’ll use to learn how to write a k-means clustering pipeline in this tutorial. This step will import the modules needed for all the code in this section: You can generate the data from the above GIF using , a convenience function in scikit-learn used to generate synthetic clusters. uses these parameters:\n• is the total number of samples to generate.\n• is the number of centers to generate.\n• A two-dimensional NumPy array with the x- and y-values for each of the samples\n• A one-dimensional NumPy array containing the cluster labels for each sample Note: Many scikit-learn algorithms rely heavily on NumPy in their implementations. If you want to learn more about NumPy arrays, check out Look Ma, No Loops: Array Programming With NumPy. Nondeterministic machine learning algorithms like k-means are difficult to reproduce. The parameter is set to an integer value so you can follow the data presented in the tutorial. In practice, it’s best to leave as the default value, . Here’s a look at the first five elements for each of the variables returned by : Data sets usually contain numerical features that have been measured in different units, such as height (in inches) and weight (in pounds). A machine learning algorithm would consider weight more important than height only because the values for weight are larger and have higher variability from person to person. Machine learning algorithms need to consider all features on an even playing field. That means the values for all features must be transformed to the same scale. The process of transforming numerical features to use the same scale is known as feature scaling. It’s an important data preprocessing step for most distance-based machine learning algorithms because it can have a significant impact on the performance of your algorithm. There are several approaches to implementing feature scaling. A great way to determine which technique is appropriate for your dataset is to read scikit-learn’s preprocessing documentation. In this example, you’ll use the class. This class implements a type of feature scaling called standardization. Standardization scales, or shifts, the values for each numerical feature in your dataset so that the features have a mean of 0 and standard deviation of 1: Take a look at how the values have been scaled in : Now the data are ready to be clustered. The estimator class in scikit-learn is where you set the algorithm parameters before fitting the estimator to the data. The scikit-learn implementation is flexible, providing several parameters that can be tuned. Here are the parameters used in this example:\n• controls the initialization technique. The standard version of the k-means algorithm is implemented by setting to . Setting this to employs an advanced trick to speed up convergence, which you’ll use later.\n• sets k for the clustering step. This is the most important parameter for k-means.\n• sets the number of initializations to perform. This is important because two runs can converge on different cluster assignments. The default behavior for the scikit-learn algorithm is to perform ten k-means runs and return the results of the one with the lowest SSE.\n• sets the number of maximum iterations for each initialization of the k-means algorithm. Instantiate the class with the following arguments: The parameter names match the language that was used to describe the k-means algorithm earlier in the tutorial. Now that the k-means class is ready, the next step is to fit it to the data in . This will perform ten runs of the k-means algorithm on your data with a maximum of iterations per run: Statistics from the initialization run with the lowest SSE are available as attributes of after calling : # The number of iterations required to converge Finally, the cluster assignments are stored as a one-dimensional NumPy array in . Here’s a look at the first five predicted labels: Note that the order of the cluster labels for the first two data objects was flipped. The order was in but in even though those data objects are still members of their original clusters in . This behavior is normal, as the ordering of cluster labels is dependent on the initialization. Cluster 0 from the first run could be labeled cluster 1 in the second run and vice versa. This doesn’t affect clustering evaluation metrics. Choosing the Appropriate Number of Clusters In this section, you’ll look at two methods that are commonly used to evaluate the appropriate number of clusters: These are often used as complementary evaluation techniques rather than one being preferred over the other. To perform the elbow method, run several k-means, increment with each iteration, and record the SSE: # A list holds the SSE values for each k The previous code block made use of Python’s dictionary unpacking operator ( ). To learn more about this powerful Python operator, check out How to Iterate Through a Dictionary in Python. When you plot SSE as a function of the number of clusters, notice that SSE continues to decrease as you increase . As more centroids are added, the distance from each point to its closest centroid will decrease. There’s a sweet spot where the SSE curve starts to bend known as the elbow point. The x-value of this point is thought to be a reasonable trade-off between error and number of clusters. In this example, the elbow is located at : The above code produces the following plot: Determining the elbow point in the SSE curve isn’t always straightforward. If you’re having trouble choosing the elbow point of the curve, then you could use a Python package, kneed, to identify the elbow point programmatically: The silhouette coefficient is a measure of cluster cohesion and separation. It quantifies how well a data point fits into its assigned cluster based on two factors:\n• How close the data point is to other points in the cluster\n• How far away the data point is from points in other clusters Silhouette coefficient values range between and . Larger numbers indicate that samples are closer to their clusters than they are to other clusters. In the scikit-learn implementation of the silhouette coefficient, the average silhouette coefficient of all the samples is summarized into one score. The function needs a minimum of two clusters, or it will raise an exception. Loop through values of again. This time, instead of computing SSE, compute the silhouette coefficient: # A list holds the silhouette coefficients for each k # Notice you start at 2 clusters for silhouette coefficient Plotting the average silhouette scores for each shows that the best choice for is since it has the maximum score: The above code produces the following plot: Ultimately, your decision on the number of clusters to use should be guided by a combination of domain knowledge and clustering evaluation metrics. The elbow method and silhouette coefficient evaluate clustering performance without the use of ground truth labels. Ground truth labels categorize data points into groups based on assignment by a human or an existing algorithm. These types of metrics do their best to suggest the correct number of clusters but can be deceiving when used without context. Note: In practice, it’s rare to encounter datasets that have ground truth labels. When comparing k-means against a density-based approach on nonspherical clusters, the results from the elbow method and silhouette coefficient rarely match human intuition. This scenario highlights why advanced clustering evaluation techniques are necessary. To visualize an example, import these additional modules: This time, use to generate synthetic data in the shape of crescents: Fit both a k-means and a DBSCAN algorithm to the new data and visually assess the performance by plotting the cluster assignments with Matplotlib: # Fit the algorithms to the features # Compute the silhouette scores for each algorithm Print the silhouette coefficient for each of the two algorithms and compare them. A higher silhouette coefficient suggests better clusters, which is misleading in this scenario: The silhouette coefficient is higher for the k-means algorithm. The DBSCAN algorithm appears to find more natural clusters according to the shape of the data: This suggests that you need a better method to compare the performance of these two clustering algorithms. If you’re interested, you can find the code for the above plot by expanding the box below. To learn more about plotting with Matplotlib and Python, check out Python Plotting with Matplotlib (Guide). Here’s how you can plot the comparison of the two algorithms in the crescent moons example: Since the ground truth labels are known, it’s possible to use a clustering metric that considers labels in its evaluation. You can use the scikit-learn implementation of a common metric called the adjusted rand index (ARI). Unlike the silhouette coefficient, the ARI uses true cluster assignments to measure the similarity between true and predicted labels. Compare the clustering results of DBSCAN and k-means using ARI as the performance metric: The ARI output values range between and . A score close to indicates random assignments, and a score close to indicates perfectly labeled clusters. Based on the above output, you can see that the silhouette coefficient was misleading. ARI shows that DBSCAN is the best choice for the synthetic crescents example as compared to k-means. There are several metrics that evaluate the quality of clustering algorithms. Reading through the implementations in scikit-learn will help you select an appropriate clustering evaluation metric.\n\nHow to Build a K-Means Clustering Pipeline in Python Now that you have a basic understanding of k-means clustering in Python, it’s time to perform k-means clustering on a real-world dataset. These data contain gene expression values from a manuscript authored by The Cancer Genome Atlas (TCGA) Pan-Cancer analysis project investigators. There are samples (rows) representing five distinct cancer subtypes. Each sample has gene expression values for genes (columns). The dataset is available from the UC Irvine Machine Learning Repository, but you can use the Python code below to obtain the data programmatically. To follow along with the examples below, you can download the source code by clicking on the following link: Download the sample code: Click here to get the code you’ll use to learn how to write a k-means clustering pipeline in this tutorial. In this section, you’ll build a robust k-means clustering pipeline. Since you’ll perform multiple transformations of the original input data, your pipeline will also serve as a practical clustering framework. Assuming you want to start with a fresh namespace, import all the modules needed to build and evaluate the pipeline, including pandas and seaborn for more advanced visualizations: Download and extract the TCGA dataset from UCI: # Extract the data from the archive After the download and extraction is completed, you should have a directory that looks like this: The class in scikit-learn requires a NumPy array as an argument. The NumPy package has a helper function to load the data from the text file into memory as NumPy arrays: Check out the first three columns of data for the first five samples as well as the labels for the first five samples: The variable contains all the gene expression values from genes. The are the cancer types for each of the samples. The first record in corresponds with the first label in . The labels are strings containing abbreviations of cancer types: To use these labels in the evaluation methods, you first need to convert the abbreviations to integers with : Since the has been fitted to the data, you can see the unique classes represented using . Store the length of the array to the variable for later use: In practical machine learning pipelines, it’s common for the data to undergo multiple sequences of transformations before it feeds into a clustering algorithm. You learned about the importance of one of these transformation steps, feature scaling, earlier in this tutorial. An equally important data transformation technique is dimensionality reduction, which reduces the number of features in the dataset by either removing or combining them. Dimensionality reduction techniques help to address a problem with machine learning algorithms known as the curse of dimensionality. In short, as the number of features increases, the feature space becomes sparse. This sparsity makes it difficult for algorithms to find data objects near one another in higher-dimensional space. Since the gene expression dataset has over features, it qualifies as a great candidate for dimensionality reduction. Principal Component Analysis (PCA) is one of many dimensionality reduction techniques. PCA transforms the input data by projecting it into a lower number of dimensions called components. The components capture the variability of the input data through a linear combination of the input data’s features. Note: A full description of PCA is out of scope for this tutorial, but you can learn more about it in the scikit-learn user guide. The next code block introduces you to the concept of scikit-learn pipelines. The scikit-learn class is a concrete implementation of the abstract idea of a machine learning pipeline. Your gene expression data aren’t in the optimal format for the class, so you’ll need to build a preprocessing pipeline. The pipeline will implement an alternative to the class called for feature scaling. You use when you do not assume that the shape of all your features follows a normal distribution. The next step in your preprocessing pipeline will implement the class to perform dimensionality reduction: Now that you’ve built a pipeline to process the data, you’ll build a separate pipeline to perform k-means clustering. You’ll override the following default arguments of the class:\n• init: You’ll use instead of to ensure centroids are initialized with some distance between them. In most cases, this will be an improvement over .\n• n_init: You’ll increase the number of initializations to ensure you find a stable solution.\n• max_iter: You’ll increase the number of iterations per initialization to ensure that k-means will converge. Build the k-means clustering pipeline with user-defined arguments in the constructor: The class can be chained to form a larger pipeline. Build an end-to-end k-means clustering pipeline by passing the and pipelines to : Calling with as the argument performs all the pipeline steps on the : The pipeline performs all the necessary steps to execute k-means clustering on the gene expression data! Depending on your Python REPL, may print a summary of the pipeline. Objects defined inside pipelines are accessible using their step name. Evaluate the performance by calculating the silhouette coefficient: Calculate ARI, too, since the ground truth cluster labels are available: As mentioned earlier, the scale for each of these clustering performance metrics ranges from -1 to 1. A silhouette coefficient of 0 indicates that clusters are significantly overlapping one another, and a silhouette coefficient of 1 indicates clusters are well-separated. An ARI score of 0 indicates that cluster labels are randomly assigned, and an ARI score of 1 means that the true labels and predicted labels form identical clusters. Since you specified in the PCA step of the k-means clustering pipeline, you can also visualize the data in the context of the true labels and predicted labels. Plot the results using a pandas DataFrame and the seaborn plotting library: Here’s what the plot looks like: The visual representation of the clusters confirms the results of the two clustering evaluation metrics. The performance of your pipeline was pretty good. The clusters only slightly overlapped, and cluster assignments were much better than random. Your first k-means clustering pipeline performed well, but there’s still room to improve. That’s why you went through the trouble of building the pipeline: you can tune the parameters to get the most desirable clustering results. The process of parameter tuning consists of sequentially altering one of the input values of the algorithm’s parameters and recording the results. At the end of the parameter tuning process, you’ll have a set of performance scores, one for each new value of a given parameter. Parameter tuning is a powerful method to maximize performance from your clustering pipeline. By setting the parameter , you squished all the features into two components, or dimensions. This value was convenient for visualization on a two-dimensional plot. But using only two components means that the PCA step won’t capture all of the explained variance of the input data. Explained variance measures the discrepancy between the PCA-transformed data and the actual input data. The relationship between and explained variance can be visualized in a plot to show you how many components you need in your PCA to capture a certain percentage of the variance in the input data. You can also use clustering performance metrics to evaluate how many components are necessary to achieve satisfactory clustering results. In this example, you’ll use clustering performance metrics to identify the appropriate number of components in the PCA step. The class is powerful in this situation. It allows you to perform basic parameter tuning using a loop. Iterate over a range of and record evaluation metrics for each iteration: # This set the number of components for pca, Plot the evaluation metrics as a function of to visualize the relationship between adding components and the performance of the k-means clustering results: The above code generates the a plot showing performance metrics as a function of : There are two takeaways from this figure:\n• The silhouette coefficient decreases linearly. The silhouette coefficient depends on the distance between points, so as the number of dimensions increases, the sparsity increases.\n• The ARI improves significantly as you add components. It appears to start tapering off after , so that would be the value to use for presenting the best clustering results from this pipeline. Like most machine learning decisions, you must balance optimizing clustering evaluation metrics with the goal of the clustering task. In situations when cluster labels are available, as is the case with the cancer dataset used in this tutorial, ARI is a reasonable choice. ARI quantifies how accurately your pipeline was able to reassign the cluster labels. The silhouette coefficient, on the other hand, is a good choice for exploratory clustering because it helps to identify subclusters. These subclusters warrant additional investigation, which can lead to new and important insights.\n\nYou now know how to perform k-means clustering in Python. Your final k-means clustering pipeline was able to cluster patients with different cancer types using real-world gene expression data. You can use the techniques you learned here to cluster your own data, understand how to get the best clustering results, and share insights with others. In this tutorial, you learned:\n• What the popular clustering techniques are and when to use them\n• What the k-means algorithm is\n• How to implement k-means clustering in Python\n• How to evaluate the performance of clustering algorithms\n• How to build and tune a robust k-means clustering pipeline in Python\n• How to analyze and present clustering results from the k-means algorithm You also took a whirlwind tour of scikit-learn, an accessible and extensible tool for implementing k-means clustering in Python. If you’d like to reproduce the examples you saw above, then be sure to download the source code by clicking on the following link: Download the sample code: Click here to get the code you’ll use to learn how to write a k-means clustering pipeline in this tutorial. You’re now ready to perform k-means clustering on datasets you find interesting. Be sure to share your results in the comments below! Note: The dataset used in this tutorial was obtained from the UCI Machine Learning Repository. Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. The original dataset is maintained by The Cancer Genome Atlas Pan-Cancer analysis project."
    }
]