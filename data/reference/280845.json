[
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/api/winbase/nf-winbase-createsemaphorea",
        "document": "To specify an access mask for the object, use the CreateSemaphoreEx function.\n\nA pointer to a SECURITY_ATTRIBUTES structure. If this parameter is NULL, the handle cannot be inherited by child processes.\n\nThe lpSecurityDescriptor member of the structure specifies a security descriptor for the new semaphore. If this parameter is NULL, the semaphore gets a default security descriptor. The ACLs in the default security descriptor for a semaphore come from the primary or impersonation token of the creator.\n\nThe initial count for the semaphore object. This value must be greater than or equal to zero and less than or equal to lMaximumCount. The state of a semaphore is signaled when its count is greater than zero and nonsignaled when it is zero. The count is decreased by one whenever a wait function releases a thread that was waiting for the semaphore. The count is increased by a specified amount by calling the ReleaseSemaphore function.\n\nThe maximum count for the semaphore object. This value must be greater than zero.\n\nThe name of the semaphore object. The name is limited to MAX_PATH characters. Name comparison is case sensitive.\n\nIf lpName matches the name of an existing named semaphore object, this function requests the SEMAPHORE_ALL_ACCESS access right. In this case, the lInitialCount and lMaximumCount parameters are ignored because they have already been set by the creating process. If the lpSemaphoreAttributes parameter is not NULL, it determines whether the handle can be inherited, but its security-descriptor member is ignored.\n\nIf lpName is NULL, the semaphore object is created without a name.\n\nIf lpName matches the name of an existing event, mutex, waitable timer, job, or file-mapping object, the function fails and the GetLastError function returns ERROR_INVALID_HANDLE. This occurs because these objects share the same namespace.\n\nThe name can have a \"Global\" or \"Local\" prefix to explicitly create the object in the global or session namespace. The remainder of the name can contain any character except the backslash character (\\). For more information, see Kernel Object Namespaces. Fast user switching is implemented using Terminal Services sessions. Kernel object names must follow the guidelines outlined for Terminal Services so that applications can support multiple users.\n\nThe object can be created in a private namespace. For more information, see Object Namespaces.\n\nIf the function succeeds, the return value is a handle to the semaphore object. If the named semaphore object existed before the function call, the function returns a handle to the existing object and GetLastError returns ERROR_ALREADY_EXISTS.\n\nIf the function fails, the return value is NULL. To get extended error information, call GetLastError.\n\nThe handle returned by CreateSemaphore has the SEMAPHORE_ALL_ACCESS access right; it can be used in any function that requires a handle to a semaphore object, provided that the caller has been granted access. If a semaphore is created from a service or a thread that is impersonating a different user, you can either apply a security descriptor to the semaphore when you create it, or change the default security descriptor for the creating process by changing its default DACL. For more information, see Synchronization Object Security and Access Rights.\n\nThe state of a semaphore object is signaled when its count is greater than zero, and nonsignaled when its count is equal to zero. The lInitialCount parameter specifies the initial count. The count can never be less than zero or greater than the value specified in the lMaximumCount parameter.\n\nAny thread of the calling process can specify the semaphore-object handle in a call to one of the wait functions. The single-object wait functions return when the state of the specified object is signaled. The multiple-object wait functions can be instructed to return either when any one or when all of the specified objects are signaled. When a wait function returns, the waiting thread is released to continue its execution. Each time a thread completes a wait for a semaphore object, the count of the semaphore object is decremented by one. When the thread has finished, it calls the ReleaseSemaphore function, which increments the count of the semaphore object.\n\nMultiple processes can have handles of the same semaphore object, enabling use of the object for interprocess synchronization. The following object-sharing mechanisms are available:\n• A child process created by the CreateProcess function can inherit a handle to a semaphore object if the lpSemaphoreAttributes parameter of CreateSemaphore enabled inheritance.\n• A process can specify the semaphore-object handle in a call to the DuplicateHandle function to create a duplicate handle that can be used by another process.\n• A process can specify the name of a semaphore object in a call to the [OpenSemaphore](/windows/win32/api/synchapi/nf-synchapi-opensemaphorew) or CreateSemaphore function.\n\nUse the CloseHandle function to close the handle. The system closes the handle automatically when the process terminates. The semaphore object is destroyed when its last handle has been closed.\n\nFor an example that uses CreateSemaphore, see Using Semaphore Objects."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/sync/using-semaphore-objects",
        "document": "The following example uses a semaphore object to limit the number of threads that can perform a particular task. First, it uses the CreateSemaphore function to create the semaphore and to specify initial and maximum counts, then it uses the CreateThread function to create the threads.\n\nBefore a thread attempts to perform the task, it uses the WaitForSingleObject function to determine whether the semaphore's current count permits it to do so. The wait function's time-out parameter is set to zero, so the function returns immediately if the semaphore is in the nonsignaled state. WaitForSingleObject decrements the semaphore's count by one.\n\nWhen a thread completes the task, it uses the ReleaseSemaphore function to increment the semaphore's count, thus enabling another waiting thread to perform the task."
    },
    {
        "link": "https://stackoverflow.com/questions/6858652/inter-process-semaphores-in-windows",
        "document": "is there any kind of semaphore for processes in Windows API? I found this one\n\nbut it is only for thread as I understood, thanks in advance for any help"
    },
    {
        "link": "https://cs.gmu.edu/~rcarver/ModernMultithreading/LectureNotes/chapter3noteswin32-2up.pdf",
        "document": ""
    },
    {
        "link": "http://moodle.eece.cu.edu.eg/pluginfile.php/2951/mod_resource/content/1/ELC467-L6c_notes.pdf",
        "document": ""
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/api/winnt/nf-winnt-interlockedexchange",
        "document": "Sets a 32-bit variable to the specified value as an atomic operation.\n\nTo operate on a pointer variable, use the InterlockedExchangePointer function.\n\nTo operate on a 16-bit variable, use the InterlockedExchange16 function.\n\nTo operate on a 64-bit variable, use the InterlockedExchange64 function.\n\nA pointer to the value to be exchanged. The function sets this variable to Value, and returns its prior value.\n\nThe value to be exchanged with the value pointed to by Target.\n\nThe function returns the initial value of the Target parameter.\n\nThe interlocked functions provide a simple mechanism for synchronizing access to a variable that is shared by multiple threads. This function is atomic with respect to calls to other interlocked functions.\n\nThis function is implemented using a compiler intrinsic where possible. For more information, see the WinBase.h header file and _InterlockedExchange.\n\nThis function generates a full memory barrier (or fence) to ensure that memory operations are completed in order.\n\nItanium-based systems: For performance-critical applications, use InterlockedExchangeAcquire instead.\n\nNote This function is supported on Windows RT-based systems."
    },
    {
        "link": "https://stackoverflow.com/questions/208490/when-should-the-win32-interlockedexchange-function-be-used",
        "document": "is both a write and a read -- it returns the previous value.\n\nThis is necessary to ensure another thread didn't write a different value just after you did. For example, say you're trying to increment a variable. You can read the value, add 1, then set the new value with . The value returned by must match the value you originally read, otherwise another thread probably incremented it at the same time, and you need to loop around and try again."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/sync/interlocked-variable-access",
        "document": "Applications must synchronize access to variables that are shared by multiple threads. Applications must also ensure that operations on these variables are performed atomically (performed in their entirety or not at all.)\n\nSimple reads and writes to properly-aligned 32-bit variables are atomic operations. In other words, you will not end up with only one portion of the variable updated; all bits are updated in an atomic fashion. However, access is not guaranteed to be synchronized. If two threads are reading and writing from the same variable, you cannot determine if one thread will perform its read operation before the other performs its write operation.\n\nSimple reads and writes to properly aligned 64-bit variables are atomic on 64-bit Windows. Reads and writes to 64-bit values are not guaranteed to be atomic on 32-bit Windows. Reads and writes to variables of other sizes are not guaranteed to be atomic on any platform.\n\nThe interlocked functions provide a simple mechanism for synchronizing access to a variable that is shared by multiple threads. They also perform operations on variables in an atomic manner. The threads of different processes can use these functions if the variable is in shared memory.\n\nThe InterlockedIncrement and InterlockedDecrement functions combine the steps involved in incrementing or decrementing a variable into an atomic operation. This feature is useful in a multitasking operating system, in which the system can interrupt one thread's execution to grant a slice of processor time to another thread. Without such synchronization, two threads could read the same value, increment it by 1, and store the new value for a total increase of 1 instead of 2. The interlocked variable-access functions protect against this kind of error.\n\nThe InterlockedExchange and InterlockedExchangePointer functions atomically exchange the values of the specified variables. The InterlockedExchangeAdd function combines two operations: adding two variables together and storing the result in one of the variables.\n\nThe InterlockedCompareExchange, InterlockedCompare64Exchange128, and InterlockedCompareExchangePointer functions combine two operations: comparing two values and storing a third value in one of the variables, based on the outcome of the comparison.\n\nThe InterlockedAnd, InterlockedOr, and InterlockedXor functions atomically perform AND, OR, and XOR operations, respectively.\n\nThere are functions that are specifically designed to perform interlocked variable access on 64-bit memory values and addresses, and are optimized for use on 64-bit Windows. Each of these functions contains \"64\" in the name; for example, InterlockedDecrement64 and InterlockedCompareExchangeAcquire64.\n\nMost of the interlocked functions provide full memory barriers on all Windows platforms. There are also functions that combine the basic interlocked variable access operations with the acquire and release memory ordering semantics supported by certain processors. Each of these functions contains the word \"Acquire\" or \"Release\" in their names; for example, InterlockedDecrementAcquire and InterlockedDecrementRelease. Acquire memory semantics specify that the memory operation being performed by the current thread will be visible before any other memory operations are attempted. Release memory semantics specify that the memory operation being performed by the current thread will be visible after all other memory operations have been completed. These semantics allow you to force memory operations to be performed in a specific order. Use acquire semantics when entering a protected region and release semantics when leaving it."
    },
    {
        "link": "https://stackoverflow.com/questions/34956224/synchronize-threads-interlockedexchange",
        "document": "There is a lot of background to cover for your question. We don't know for example what tool chain you are using so I am going to answer it as a winapi question. I further assume you have some something in mind like this:\n\nThere are many things wrong here. For starters the does very little here. When happens it will eventually be visible to the other thread because it is backed by a global variable. This is so because it will at least make it into the cache and the cache has ways to tell other processors that a given line (which is a range of addresses) is dirty. The only way it would not make it into the cache is that if the compiler makes a super crazy optimization in which stays in the cpu as a register. That could actually happen but not in this particular code example.\n\nSo volatile tells the compiler to never keep the variable as a register. That is what it is, every time you see a volatile variable you can translate it as \"never enregister this variable\". Its use here is just basically a paranoid move.\n\nIf this code is what you had in mind then this looping over a flag pattern is called a and this one is a really poor one. It is almost never the right thing to do in a user mode program.\n\nBefore we go into better approaches let me tackle your Interlocked question. What people usually mean is this pattern\n\nAssume the means similar code as before. What the is doing is forcing the write to memory to happen in a deterministic, \"broadcast the change now\", kind of way and the typical way to read it in the same \"bypass the cache\" way is via .\n\nOne problem with them is that they generate more traffic on the system bus. That is, the bus now being used to broadcast cache synchronization packets among the cpus on the system.\n\nwould be the modern, C++11 way to do the same, but still not what you really want to do.\n\nI added the call there to point to the real problem. When you wait for a memory address to change you are using cpu resources that would be better used somewhere else, for example in the actual work (!!). If you actually yield the processor there is at least a chance that the OS will give it to the , but in a multicore machine it will quickly go back to polling the variable. In a modern machine you will be checking this millions of times per second, with the yield, probably 200000 times per second. Terrible waste either way.\n\nWhat you want to do here is to leverage Windows to do a zero-cost wait, or at least a low cost as you want to:\n\nWhen you return from the worker thread the thread handle get signaled and the wait it satisfied. While stuck in you don't consume any cpu cycles. If you want to do a periodic activity in the main() function while you wait you can replace with 1000, which will release the main thread every second. In that case you need to check the return value of to tell the timeout from thread being done case.\n\nIf you need to actually know when work started, you need an additional waitable object, for example, a Windows event which is obtained via and can be waited on using the same .\n\nNow that we can see the code you have in mind, you don't need atomics, works just fine. The is protected by the cs mutex anyhow for the case.\n\nIf I might suggest, you can use and cs to accomplish the same without at all:"
    },
    {
        "link": "https://community.osr.com/t/how-does-interlockedexchange-work/48347",
        "document": "The InterlockedExchange uses the Intel XCHG instruction. XCHG returns the\n\n previous value of the location so it is a read/write sequence which is\n\n locked to be atomic. The XCHG instruction will guarantee that one of the InterlockedExchange\n\n operations in your example will complete before the other executes. We do\n\n not know if CPU1 or CPU2 will be the first to complete but we do know that\n\n each exchange will be atomic, the second can not start until the first is\n\n complete. That is the second to execute will return the value set by the\n\n first to execute (5 or 3) . The CPUs have a hardware line for interprocessor synchronization, the LOCK\n\n signal. The LOCK signal is asserted during the execution of a few\n\n instructions by default and can be selected by the LOCK instruction Prefix\n\n on a few more instructions. This allows the synchronization. Some instructions are atomic by default: The Intel486 processor (and newer processors since) guarantees that the\n\n following basic memory operations will always be carried out atomically: The Pentium processor (and newer processors since) guarantees that the\n\n following additional memory operations will always be carried out\n\n atomically: . 16-bit accesses to uncached memory locations that fit within a 32-bit data\n\n bus The P6 family processors (and newer processors since) guarantee that the\n\n following additional memory operation will always be carried out atomically: . Unaligned 16-, 32-, and 64-bit accesses to cached memory that fit within a\n\n cache line Other operations and collections of operations are not atomic by default.\n\n The LOCK prefix is allowed for some instructions to form LOCKED\n\n instructions. We can use LOCKED instructions to synchronize the other operations and\n\n groups of operations. We also use locked instructions to implement locking\n\n for much longer sequences of instructions. (For example the simplest form\n\n of spinlock can be constructed with the XCHG instruction.) The reference in the stackoverflow reply is the definitive reference for\n\n multiprocessor operations: \"See “Bus Locking” in Chapter 8, \"Multiple-Processor Management,“of the\n\n IntelR 64 and IA-32 Architectures Software Developer’s Manual, Volume 3A,\n\n for more information on bus locking.” -----Original Message-----\n\n From: xxxxx@lists.osr.com\n\n [mailto:xxxxx@lists.osr.com] On Behalf Of xxxxx@gmx.de\n\n Sent: Friday, October 11, 2013 11:54 AM\n\n To: Windows System Software Devs Interest List\n\n Subject: [ntdev] How does InterlockedExchange work I have a Windows 7 x86/amd64 driver where I want to synchronize access to a\n\n variable. Can I use InterlockedExchange for it? My current understanding of InterlockedExchange is, that InterlockedExchange\n\n is done via compiler intrinsics. That means, the read (InterlockedExchange\n\n returns the old value) and the write is done in one clock cycle. The\n\n interlocked functions are atomic only when the variable is always accessed\n\n via an interlocked function. But what happens in this case: StatusVariable is written in the same clock cycle on two CPU cores. Does the\n\n function notice that the variable is accessed and defer the write to a\n\n different clock cycle? Or is it undefined which value the variable has after\n\n the write? Is it also possible that the variable contains garbage? This is a cross-post from stackoverflow, because I didn’t get satisfying\n\n answers there. http:cores>\n\nhttp://stackoverflow.com/questions/18076416/interlockedexchange-on-two-cpu-c\n\nores \n\n\n\n—\n\n\n\nNTDEV is sponsored by OSR\n\n\n\nVisit the list at: http:\n\nhttp://www.osronline.com/showlists.cfm?list=ntdev\n\n\n\nOSR is HIRING!! See http:\n\nhttp://www.osr.com/careers\n\n\n\nFor our schedule of WDF, WDM, debugging and other seminars visit: \n\n\n\nhttp: http://www.osr.com/seminars\n\n\n\nTo unsubscribe, visit the List Server section of OSR Online at\n\nhttp:\n\nhttp://www.osronline.com/page.cfm?name=ListServer</http:></http:></http:></http:></http:>\n\nThe LOCK prefix (or an implicitly-locked instruction) is a single\n\n instruction cycle. So I’m not sure what you meant by “groups of\n\n operations” in the explanation below; if you execute three LOCKED\n\n instructions in a row, the combination of accidental sequencing and the\n\n built-in lock logic means they may be interleaved in every possible order\n\n between two or more cores, even if the first instruction is granted to\n\n CPU0, the lock does not persist across the next instruction, even if it is\n\n LOCKed as well. So the only “groups” of operations that can be done are\n\n those that involve a single instruction (E.g., InterlockedIncrement,\n\n which reads, increments, and stores in one operation, or\n\n InterlockedCompareAndExchange, which does a lot more, but again, only\n\n within one instruction.\n\n joe The InterlockedExchange uses the Intel XCHG instruction. XCHG returns the\n\n previous value of the location so it is a read/write sequence which is\n\n locked to be atomic. The XCHG instruction will guarantee that one of the InterlockedExchange\n\n operations in your example will complete before the other executes. We do\n\n not know if CPU1 or CPU2 will be the first to complete but we do know that\n\n each exchange will be atomic, the second can not start until the first is\n\n complete. That is the second to execute will return the value set by the\n\n first to execute (5 or 3) . The CPUs have a hardware line for interprocessor synchronization, the\n\n LOCK\n\n signal. The LOCK signal is asserted during the execution of a few\n\n instructions by default and can be selected by the LOCK instruction Prefix\n\n on a few more instructions. This allows the synchronization. Some instructions are atomic by default: The Intel486 processor (and newer processors since) guarantees that the\n\n following basic memory operations will always be carried out atomically: The Pentium processor (and newer processors since) guarantees that the\n\n following additional memory operations will always be carried out\n\n atomically: . 16-bit accesses to uncached memory locations that fit within a 32-bit\n\n data\n\n bus The P6 family processors (and newer processors since) guarantee that the\n\n following additional memory operation will always be carried out\n\n atomically: . Unaligned 16-, 32-, and 64-bit accesses to cached memory that fit within\n\n a\n\n cache line Other operations and collections of operations are not atomic by default.\n\n The LOCK prefix is allowed for some instructions to form LOCKED\n\n instructions. We can use LOCKED instructions to synchronize the other operations and\n\n groups of operations. We also use locked instructions to implement\n\n locking\n\n for much longer sequences of instructions. (For example the simplest form\n\n of spinlock can be constructed with the XCHG instruction.) The reference in the stackoverflow reply is the definitive reference for\n\n multiprocessor operations: \"See “Bus Locking” in Chapter 8, \"Multiple-Processor Management,“of the\n\n IntelR 64 and IA-32 Architectures Software Developer’s Manual, Volume 3A,\n\n for more information on bus locking.” -----Original Message-----\n\n From: xxxxx@lists.osr.com\n\n [mailto:xxxxx@lists.osr.com] On Behalf Of xxxxx@gmx.de\n\n Sent: Friday, October 11, 2013 11:54 AM\n\n To: Windows System Software Devs Interest List\n\n Subject: [ntdev] How does InterlockedExchange work I have a Windows 7 x86/amd64 driver where I want to synchronize access to\n\n a\n\n variable. Can I use InterlockedExchange for it? My current understanding of InterlockedExchange is, that\n\n InterlockedExchange\n\n is done via compiler intrinsics. That means, the read (InterlockedExchange\n\n returns the old value) and the write is done in one clock cycle. The\n\n interlocked functions are atomic only when the variable is always accessed\n\n via an interlocked function. But what happens in this case: StatusVariable is written in the same clock cycle on two CPU cores. Does\n\n the\n\n function notice that the variable is accessed and defer the write to a\n\n different clock cycle? Or is it undefined which value the variable has\n\n after\n\n the write? Is it also possible that the variable contains garbage? This is a cross-post from stackoverflow, because I didn’t get satisfying\n\n answers there. http:> cores>\n\n> http://stackoverflow.com/questions/18076416/interlockedexchange-on-two-cpu-c\n\n> ores\n\n>\n\n>\n\n>\n\n> —\n\n>\n\n> NTDEV is sponsored by OSR\n\n>\n\n>\n\n>\n\n> Visit the list at: http:\n\n> http://www.osronline.com/showlists.cfm?list=ntdev\n\n>\n\n>\n\n>\n\n> OSR is HIRING!! See http:\n\n> http://www.osr.com/careers\n\n>\n\n>\n\n>\n\n> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n>\n\n> http: http://www.osr.com/seminars\n\n>\n\n>\n\n>\n\n> To unsubscribe, visit the List Server section of OSR Online at\n\n> http:\n\n> http://www.osronline.com/page.cfm?name=ListServer\n\n>\n\n>\n\n> —\n\n> NTDEV is sponsored by OSR\n\n>\n\n> Visit the list at: http://www.osronline.com/showlists.cfm?list=ntdev\n\n>\n\n> OSR is HIRING!! See http://www.osr.com/careers\n\n>\n\n> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n> http://www.osr.com/seminars\n\n>\n\n> To unsubscribe, visit the List Server section of OSR Online at\n\n> http://www.osronline.com/page.cfm?name=ListServer</http:></http:></http:></http:></http:>\n\nI was referring to the LOCK mechanism as the basis for multiprocessor\n\n synchronization. One uses a LOCKED instruction to form a lock to protect the sequence of\n\n instructions (a simple spinlock). To elaborate on the simple spinlock we could create a lock variable.\n\n Initialize it to 0. To acquire the lock we do an XCHG with an argument of\n• If the previous value returned is 0 we have the lock, if it is 1 we do\n\n not and should delay and try again until we get the lock. To release the\n\n lock we do the XCHG with an argument of 0. The longer sequence of code is\n\n between the acquire and release and is protected by the (implicit) LOCK on\n\n the XCHG instruction. It is of course better to use the Windows SpinLock. -----Original Message-----\n\n From: xxxxx@lists.osr.com\n\n [mailto:xxxxx@lists.osr.com] On Behalf Of xxxxx@flounder.com\n\n Sent: Friday, October 11, 2013 2:08 PM\n\n To: Windows System Software Devs Interest List\n\n Subject: RE: [ntdev] How does InterlockedExchange work The LOCK prefix (or an implicitly-locked instruction) is a single\n\n instruction cycle. So I’m not sure what you meant by “groups of operations”\n\n in the explanation below; if you execute three LOCKED instructions in a row,\n\n the combination of accidental sequencing and the built-in lock logic means\n\n they may be interleaved in every possible order between two or more cores,\n\n even if the first instruction is granted to CPU0, the lock does not persist\n\n across the next instruction, even if it is LOCKed as well. So the only\n\n “groups” of operations that can be done are those that involve a single\n\n instruction (E.g., InterlockedIncrement, which reads, increments, and\n\n stores in one operation, or InterlockedCompareAndExchange, which does a lot\n\n more, but again, only within one instruction. The InterlockedExchange uses the Intel XCHG instruction. XCHG returns the previous value of the location so it is a read/write sequence which is locked to be atomic. The XCHG instruction will guarantee that one of the InterlockedExchange operations in your example will complete before the other executes. We do not know if CPU1 or CPU2 will be the first to complete but we do know that each exchange will be atomic, the second can not start until the first is complete. That is the second to execute will return the value set by the first to execute (5 or 3) . The CPUs have a hardware line for interprocessor synchronization, the LOCK signal. The LOCK signal is asserted during the execution of a few instructions by default and can be selected by the LOCK instruction Prefix on a few more instructions. This allows the Some instructions are atomic by default: The Intel486 processor (and newer processors since) guarantees that the following basic memory operations will always be carried out\n\n atomically: The Pentium processor (and newer processors since) guarantees that the following additional memory operations will always be carried out . 16-bit accesses to uncached memory locations that fit within a The P6 family processors (and newer processors since) guarantee that the following additional memory operation will always be carried out . Unaligned 16-, 32-, and 64-bit accesses to cached memory that fit Other operations and collections of operations are not atomic by default. The LOCK prefix is allowed for some instructions to form LOCKED We can use LOCKED instructions to synchronize the other operations and groups of operations. We also use locked instructions to implement locking for much longer sequences of instructions. (For example the simplest form of spinlock can be constructed with the XCHG The reference in the stackoverflow reply is the definitive reference Volume 3A, for more information on bus locking.\" http:\n\nhttp://download.intel.com/design/processor/manuals/253668.pdf\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> -----Original Message-----\n\n\n\n> From: mailto:xxxxx\n\nxxxxx@lists.osr.com\n\n\n\n> [mailto:xxxxx\n\nmailto:xxxxx@lists.osr.com] On Behalf Of \n\n\n\n> mailto:xxxxx xxxxx@gmx.de\n\n\n\n> Sent: Friday, October 11, 2013 11:54 AM\n\n\n\n> To: Windows System Software Devs Interest List\n\n\n\n> Subject: [ntdev] How does InterlockedExchange work\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> Hi,\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> I have a Windows 7 x86/amd64 driver where I want to synchronize access \n\n\n\n> to a variable. Can I use InterlockedExchange for it?\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> My current understanding of InterlockedExchange is, that \n\n\n\n> InterlockedExchange is done via compiler intrinsics. That means, the \n\n\n\n> read (InterlockedExchange returns the old value) and the write is done \n\n\n\n> in one clock cycle. The interlocked functions are atomic only when the \n\n\n\n> variable is always accessed via an interlocked function.\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> But what happens in this case:\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> CPU1: InterlockedExchange(&Adapter->StatusVariable, 5);\n\n\n\n> \n\n\n\n> CPU2: InterlockedExchange(&Adapter->StatusVariable, 3);\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> StatusVariable is written in the same clock cycle on two CPU cores. \n\n\n\n> Does the function notice that the variable is accessed and defer the \n\n\n\n> write to a different clock cycle? Or is it undefined which value the \n\n\n\n> variable has after the write? Is it also possible that the variable \n\n\n\n> contains garbage?\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> This is a cross-post from stackoverflow, because I didn’t get \n\n\n\n> satisfying answers there.\n\n\n\n> \n\n\n\n> \n\n\n\n> http:\n\n> o-cpu-\n\n\n\n> cores>\n\n\n\n> http:\n\nhttp://stackoverflow.com/questions/18076416/interlockedexchange-on-two\n\n\n\n> -cpu-c\n\n\n\n> ores\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> —\n\n\n\n> \n\n\n\n> NTDEV is sponsored by OSR\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> Visit the list at: \n\n\n\n> < http:\n\nhttp://www.osronline.com/showlists.cfm?list=ntdev>\n\n\n\n> http:\n\nhttp://www.osronline.com/showlists.cfm?list=ntdev\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> OSR is HIRING!! See < http:\n\nhttp://www.osr.com/careers> \n\n\n\n> http: http://www.osr.com/careers\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n\n\n> \n\n\n\n> < http: http://www.osr.com/seminars>\n\nhttp: http://www.osr.com/seminars\n\n\n\n> \n\n\n\n> \n\n\n\n> \n\n\n\n> To unsubscribe, visit the List Server section of OSR Online at \n\n\n\n> < http:\n\nhttp://www.osronline.com/page.cfm?name=ListServer>\n\n\n\n> http:\n\nhttp://www.osronline.com/page.cfm?name=ListServer\n\n\n\n> \n\n\n\n> \n\n\n\n> —\n\n\n\n> NTDEV is sponsored by OSR\n\n\n\n> \n\n\n\n> Visit the list at: http:\n\nhttp://www.osronline.com/showlists.cfm?list=ntdev\n\n\n\n> \n\n\n\n> OSR is HIRING!! See http:\n\nhttp://www.osr.com/careers\n\n\n\n> \n\n\n\n> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n\n\n> http: http://www.osr.com/seminars\n\n\n\n> \n\n\n\n> To unsubscribe, visit the List Server section of OSR Online at \n\n\n\n> http:\n\nhttp://www.osronline.com/page.cfm?name=ListServer\n\n\n\n—\n\n\n\nNTDEV is sponsored by OSR\n\n\n\nVisit the list at: http:\n\nhttp://www.osronline.com/showlists.cfm?list=ntdev\n\n\n\nOSR is HIRING!! See http:\n\nhttp://www.osr.com/careers\n\n\n\nFor our schedule of WDF, WDM, debugging and other seminars visit: \n\n\n\nhttp: http://www.osr.com/seminars\n\n\n\nTo unsubscribe, visit the List Server section of OSR Online at\n\nhttp:\n\nhttp://www.osronline.com/page.cfm?name=ListServer</http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></mailto:xxxxx></mailto:xxxxx></mailto:xxxxx></http:>\n\nNote also the suggested usage of the PAUSE instruction in a spin-lock wait.\n\n joe I was referring to the LOCK mechanism as the basis for multiprocessor\n\n synchronization. One uses a LOCKED instruction to form a lock to protect the sequence of\n\n instructions (a simple spinlock). To elaborate on the simple spinlock we could create a lock variable.\n\n Initialize it to 0. To acquire the lock we do an XCHG with an argument\n\n of\n• If the previous value returned is 0 we have the lock, if it is 1 we\n\n do\n\n not and should delay and try again until we get the lock. To release the\n\n lock we do the XCHG with an argument of 0. The longer sequence of code is\n\n between the acquire and release and is protected by the (implicit) LOCK on\n\n the XCHG instruction. It is of course better to use the Windows SpinLock. -----Original Message-----\n\n From: xxxxx@lists.osr.com\n\n [mailto:xxxxx@lists.osr.com] On Behalf Of\n\n xxxxx@flounder.com\n\n Sent: Friday, October 11, 2013 2:08 PM\n\n To: Windows System Software Devs Interest List\n\n Subject: RE: [ntdev] How does InterlockedExchange work The LOCK prefix (or an implicitly-locked instruction) is a single\n\n instruction cycle. So I’m not sure what you meant by “groups of\n\n operations”\n\n in the explanation below; if you execute three LOCKED instructions in a\n\n row,\n\n the combination of accidental sequencing and the built-in lock logic means\n\n they may be interleaved in every possible order between two or more cores,\n\n even if the first instruction is granted to CPU0, the lock does not\n\n persist\n\n across the next instruction, even if it is LOCKed as well. So the only\n\n “groups” of operations that can be done are those that involve a single\n\n instruction (E.g., InterlockedIncrement, which reads, increments, and\n\n stores in one operation, or InterlockedCompareAndExchange, which does a\n\n lot\n\n more, but again, only within one instruction. > The InterlockedExchange uses the Intel XCHG instruction. XCHG returns > the previous value of the location so it is a read/write sequence > which is locked to be atomic. > The XCHG instruction will guarantee that one of the > InterlockedExchange operations in your example will complete before > the other executes. We do not know if CPU1 or CPU2 will be the first > to complete but we do know that each exchange will be atomic, the > second can not start until the first is complete. That is the second > to execute will return the value set by the first to execute (5 or 3) . > The CPUs have a hardware line for interprocessor synchronization, the > LOCK signal. The LOCK signal is asserted during the execution of a > few instructions by default and can be selected by the LOCK > instruction Prefix on a few more instructions. This allows the > Some instructions are atomic by default: > The Intel486 processor (and newer processors since) guarantees that > the following basic memory operations will always be carried out\n\n atomically: > The Pentium processor (and newer processors since) guarantees that the > following additional memory operations will always be carried out > . 16-bit accesses to uncached memory locations that fit within a > The P6 family processors (and newer processors since) guarantee that > the following additional memory operation will always be carried out > . Unaligned 16-, 32-, and 64-bit accesses to cached memory that fit > Other operations and collections of operations are not atomic by\n\n > default. > The LOCK prefix is allowed for some instructions to form LOCKED > We can use LOCKED instructions to synchronize the other operations and > groups of operations. We also use locked instructions to implement > locking for much longer sequences of instructions. (For example the > simplest form of spinlock can be constructed with the XCHG > The reference in the stackoverflow reply is the definitive reference > Volume 3A, for more information on bus locking.\" > http:\n\n> http://download.intel.com/design/processor/manuals/253668.pdf\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> -----Original Message-----\n\n>\n\n>> From: mailto:xxxxx\n\n> xxxxx@lists.osr.com\n\n>\n\n>> [mailto:xxxxx\n\n> mailto:xxxxx@lists.osr.com] On Behalf Of\n\n>\n\n>> mailto:xxxxx xxxxx@gmx.de\n\n>\n\n>> Sent: Friday, October 11, 2013 11:54 AM\n\n>\n\n>> To: Windows System Software Devs Interest List\n\n>\n\n>> Subject: [ntdev] How does InterlockedExchange work\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> Hi,\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> I have a Windows 7 x86/amd64 driver where I want to synchronize access\n\n>\n\n>> to a variable. Can I use InterlockedExchange for it?\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> My current understanding of InterlockedExchange is, that\n\n>\n\n>> InterlockedExchange is done via compiler intrinsics. That means, the\n\n>\n\n>> read (InterlockedExchange returns the old value) and the write is done\n\n>\n\n>> in one clock cycle. The interlocked functions are atomic only when the\n\n>\n\n>> variable is always accessed via an interlocked function.\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> But what happens in this case:\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> CPU1: InterlockedExchange(&Adapter->StatusVariable, 5);\n\n>\n\n>>\n\n>\n\n>> CPU2: InterlockedExchange(&Adapter->StatusVariable, 3);\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> StatusVariable is written in the same clock cycle on two CPU cores.\n\n>\n\n>> Does the function notice that the variable is accessed and defer the\n\n>\n\n>> write to a different clock cycle? Or is it undefined which value the\n\n>\n\n>> variable has after the write? Is it also possible that the variable\n\n>\n\n>> contains garbage?\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> This is a cross-post from stackoverflow, because I didn’t get\n\n>\n\n>> satisfying answers there.\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> http:>\n\n>> o-cpu-\n\n>\n\n>> cores>\n\n>\n\n>> http:\n\n> http://stackoverflow.com/questions/18076416/interlockedexchange-on-two\n\n>\n\n>> -cpu-c\n\n>\n\n>> ores\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> —\n\n>\n\n>>\n\n>\n\n>> NTDEV is sponsored by OSR\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> Visit the list at:\n\n>\n\n>> < http:\n\n> http://www.osronline.com/showlists.cfm?list=ntdev>\n\n>\n\n>> http:\n\n> http://www.osronline.com/showlists.cfm?list=ntdev\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> OSR is HIRING!! See < http:\n\n> http://www.osr.com/careers>\n\n>\n\n>> http: http://www.osr.com/careers\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n>\n\n>>\n\n>\n\n>> < http: http://www.osr.com/seminars>\n\n> http: http://www.osr.com/seminars\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> To unsubscribe, visit the List Server section of OSR Online at\n\n>\n\n>> < http:\n\n> http://www.osronline.com/page.cfm?name=ListServer>\n\n>\n\n>> http:\n\n> http://www.osronline.com/page.cfm?name=ListServer\n\n>\n\n>>\n\n>\n\n>>\n\n>\n\n>> —\n\n>\n\n>> NTDEV is sponsored by OSR\n\n>\n\n>>\n\n>\n\n>> Visit the list at: http:\n\n> http://www.osronline.com/showlists.cfm?list=ntdev\n\n>\n\n>>\n\n>\n\n>> OSR is HIRING!! See http:\n\n> http://www.osr.com/careers\n\n>\n\n>>\n\n>\n\n>> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n>\n\n>> http: http://www.osr.com/seminars\n\n>\n\n>>\n\n>\n\n>> To unsubscribe, visit the List Server section of OSR Online at\n\n>\n\n>> http:\n\n> http://www.osronline.com/page.cfm?name=ListServer\n\n>\n\n>\n\n>\n\n>\n\n>\n\n>\n\n>\n\n> —\n\n>\n\n> NTDEV is sponsored by OSR\n\n>\n\n>\n\n>\n\n> Visit the list at: http:\n\n> http://www.osronline.com/showlists.cfm?list=ntdev\n\n>\n\n>\n\n>\n\n> OSR is HIRING!! See http:\n\n> http://www.osr.com/careers\n\n>\n\n>\n\n>\n\n> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n>\n\n> http: http://www.osr.com/seminars\n\n>\n\n>\n\n>\n\n> To unsubscribe, visit the List Server section of OSR Online at\n\n> http:\n\n> http://www.osronline.com/page.cfm?name=ListServer\n\n>\n\n>\n\n> —\n\n> NTDEV is sponsored by OSR\n\n>\n\n> Visit the list at: http://www.osronline.com/showlists.cfm?list=ntdev\n\n>\n\n> OSR is HIRING!! See http://www.osr.com/careers\n\n>\n\n> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n> http://www.osr.com/seminars\n\n>\n\n> To unsubscribe, visit the List Server section of OSR Online at\n\n> http://www.osronline.com/page.cfm?name=ListServer</http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></http:></mailto:xxxxx></mailto:xxxxx></mailto:xxxxx></http:>\n\nEven in late 70s and early 80s, it was an exercise to do the atomic OPS for machine that don’t have test/set ops… Also you reminded me those old heors: O.J.Dahl, Parnas, Hoere, Dijkstra to name a few for the advancement of CS & Soft Engg… Date: Fri, 11 Oct 2013 21:34:05 -0400\n\n Subject: RE: [ntdev] How does InterlockedExchange work\n\n From: xxxxx@flounder.com\n\n To: xxxxx@lists.osr.com Interlocked instructions are mot required to do locking. As a homework\n\n exercise in my operating systems ourse in 1967, we had to implement\n\n interprocessor/interthread locks in an ordinary programming language (in\n\n those days it was Algol; today it would be C). None of us got the right\n\n answer, but the professor (a little-known new faculty, one David L.\n\n Parnas) presented the correct answer, complete with a proof of\n\n correctness. By te way, the solution is sufficiently complex and inefficient that it\n\n makes it obvious ehy you need interlocked instructions.\n\n joe > Particularly when the underlying system is NUMA…\n\n >\n\n >\n\n >\n\n >\n\n >\n\n > From: xxxxx@outlook.com\n\n > To: xxxxx@lists.osr.com\n\n > Subject: RE: [ntdev] How does InterlockedExchange work\n\n > Date: Fri, 11 Oct 2013 20:15:07 -0500\n\n >\n\n >\n\n >\n\n >\n\n > It is interesting, and subtle. But important on the face of\n\n > muti-processors systems and cache.\n\n >\n\n >\n\n > in later case\n\n >\n\n >\n\n > For the first part, it is better to return the exact old value …\n\n >\n\n >\n\n > -pro\n\n >\n\n >> From: xxxxx@hotmail.com\n\n >> Subject: Re:[ntdev] How does InterlockedExchange work\n\n >> Date: Fri, 11 Oct 2013 21:02:56 -0400\n\n >> To: xxxxx@lists.osr.com\n\n >>\n\n >> In this case, as every other case, a compiler intrinsic is merely a way\n\n >> for\n\n >> your code to be compiled to a binary that does not include the\n\n >> instruction\n\n >> sequence for a function call to an external library. The code may be\n\n >> condensed to an inline operation, or converted to a local function call\n\n >> instead depending on the compiler and the intrinsic.\n\n >>\n\n >> In the specific case of the various interlocked operations, as others\n\n >> have\n\n >> intimated, the API relies on features of the hardware to ensure\n\n >> atomicity of\n\n >> the operation and the compiler intrinsic does the same for a specific\n\n >> platform. IMHO this paradigm is erroneously called ‘lock free’, but\n\n >> really\n\n >> is is hardware lock based instead of software lock based. Note that\n\n >> nearly\n\n >> all software locks rely on hardware locks of some description, so what\n\n >> this\n\n >> paradigm really involves is using the constructs usually used to\n\n >> implement a\n\n >> software lock to directly manipulate a logical construct (i.e. a linked\n\n >> list\n\n >> or other data structure)\n\n >>\n\n >> Note that cache aware versions of these algorithms are the subject of\n\n >> conjectural patents in jurisdictions where such patents are possible,\n\n >> but\n\n >> mostly treated as either prior art or not copyright infringement in all\n\n >> sane\n\n >> countries (i.e. those not currently having a problem with a debt\n\n >> ceiling)\n\n >>\n\n >> If you wish to investigate further, there is a significant amount of\n\n >> publicized research on the topic and there won?t be any lack of pointers\n\n >> to\n\n >> good resources from this group\n\n >>\n\n >> wrote in message news:xxxxx@ntdev…\n\n >>\n\n >> Hi,\n\n >>\n\n >> I have a Windows 7 x86/amd64 driver where I want to synchronize access\n\n >> to a\n\n >> variable. Can I use InterlockedExchange for it?\n\n >>\n\n >> My current understanding of InterlockedExchange is, that\n\n >> InterlockedExchange\n\n >> is done via compiler intrinsics. That means, the read\n\n >> (InterlockedExchange\n\n >> returns the old value) and the write is done in one clock cycle. The\n\n >> interlocked functions are atomic only when the variable is always\n\n >> accessed\n\n >> via an interlocked function.\n\n >>\n\n >> But what happens in this case:\n\n >>\n\n >> CPU1: InterlockedExchange(&Adapter->StatusVariable, 5);\n\n >> CPU2: InterlockedExchange(&Adapter->StatusVariable, 3);\n\n >>\n\n >> StatusVariable is written in the same clock cycle on two CPU cores. Does\n\n >> the\n\n >> function notice that the variable is accessed and defer the write to a\n\n >> different clock cycle? Or is it undefined which value the variable has\n\n >> after\n\n >> the write? Is it also possible that the variable contains garbage?\n\n >>\n\n >> This is a cross-post from stackoverflow, because I didn’t get satisfying\n\n >> answers there.\n\n >> http://stackoverflow.com/questions/18076416/interlockedexchange-on-two-cpu-cores\n\n >>\n\n >>\n\n >> —\n\n >> NTDEV is sponsored by OSR\n\n >>\n\n >> Visit the list at: http://www.osronline.com/showlists.cfm?list=ntdev\n\n >>\n\n >> OSR is HIRING!! See http://www.osr.com/careers\n\n >>\n\n >> For our schedule of WDF, WDM, debugging and other seminars visit:\n\n >> http://www.osr.com/seminars\n\n >>\n\n >> To unsubscribe, visit the List Server section of OSR Online at\n\n >> http://www.osronline.com/page.cfm?name=ListServer\n\n >\n\n >\n\n > —\n\n >\n\n >\n\n > NTDEV is sponsored by OSR\n\n >\n\n >\n\n >\n\n >\n\n >\n\n > Visit the list at: http://www.osronline.com/showlists.cfm?list=ntdev\n\n >\n\n >\n\n >\n\n >\n\n >\n\n > OSR is HIRING!! See http://www.osr.com/careers\n\n >\n\n >\n\n >\n\n >\n\n >\n\n > For our schedule of WDF, WDM, debugging and other seminars visit:\n\n >\n\n >\n\n > http://www.osr.com/seminars\n\n >\n\n >\n\n >\n\n >\n\n >\n\n > To unsubscribe, visit the List Server section of OSR Online at\n\n > http://www.osronline.com/page.cfm?name=ListServer\n\n > —\n\n > NTDEV is sponsored by OSR\n\n >\n\n > Visit the list at: http://www.osronline.com/showlists.cfm?list=ntdev\n\n >\n\n > OSR is HIRING!! See http://www.osr.com/careers\n\n >\n\n > For our schedule of WDF, WDM, debugging and other seminars visit:\n\n > http://www.osr.com/seminars\n\n >\n\n > To unsubscribe, visit the List Server section of OSR Online at\n\n > http://www.osronline.com/page.cfm?name=ListServer For our schedule of WDF, WDM, debugging and other seminars visit:\n\n http://www.osr.com/seminars To unsubscribe, visit the List Server section of OSR Online at http://www.osronline.com/page.cfm?name=ListServer\n\nIn the past, many architectures supported what was called\n\n “read-pause-write”, the equivalent of LOCK, on certain instructions. For\n\n example, on the PDP-11 (and, consequently, almost certainly on the VAX),\n\n INC and DEC were always done with read-pause-write. The problem with this\n\n it that it wastes memory bandwidth, hence the optional LOCK prefix on the\n\n x86 family. It is also nice that the LOCK prefix can apply to many\n\n different instructions. There is no such thing as a “lock free” algorithm, although some limited\n\n cases (a circular buffer accessed by at most two threads comes to mind)\n\n can be made to work without explicit locking. However, like the “locked\n\n access” example we had to do in Algol, an algorithm that requires\n\n simultaneous access must have some known invariant. The circular buffer\n\n case usually works because the “head” is accessed by exactly one thread\n\n and the “tail” is accessed by exactly one thread, meaning that locking was\n\n not needed. But the number of “lock-free” algorithms I’ve seen always\n\n seem to depend at some level on instruction atomicity, or are so\n\n incredibly fragile that attempts to adopt them usually lead to some error\n\n that invalidates the complex set of preconditions that make them work.\n\n And all too often, it is an attempt to generalize a two-thread-safe model\n\n to an N-thread-safe model. I’ve seen a few of these spectacular failures,\n\n caused by people who felt that those synchronization primitives like spin\n\n locks, mutexes, and semaphores were “too complicated” and they had a\n\n “simple solution”. Yeah, right.\n\n joe > IMHO this paradigm is erroneously called ‘lock free’, but really is is\n\n > hardware lock based instead\n\n > of software lock based. In order to realize how erroneous calling these functions lock-free is you\n\n just have to keep in mind that\n\n not all architectures that support MP provide the exhaustive sets of\n\n operations that may use bus locking.\n\n Don’t forget that on many architectures (they are known as load-and-store\n\n ones) you cannot perform arithmetic and logic operations directly on\n\n memory operands - instead, you have to load operands to registers,\n\n perform an operation in registers, and then store the results in memory. It is (hopefully) obvious that you cannot implement functions like\n\n InterlockedIncrement() on such an architecture like that in one go,\n\n because ‘LOCK INC [.memory]’ -like operation is obviously impossible on\n\n it. Instead, you have to, first, obtain a test-and-set-based lock, then\n\n update the target variable, and then release the lock… For our schedule of WDF, WDM, debugging and other seminars visit:\n\n http://www.osr.com/seminars To unsubscribe, visit the List Server section of OSR Online at\n\n http://www.osronline.com/page.cfm?name=ListServer\n\nThis problem with high contention on busses is one of the reason that\n\n queued spin locks were invented. And it is worth studying why a “spin\n\n lock” should include a PAUSE instruction to reduce power consumption and\n\n bus contention. The memory system runs asynchronously with the CPU, and is mediated by the\n\n multilevel caching system and cache interlock logic as well. The “cost”\n\n of accessing a memory location will depend on a huge number of conditions.\n\n If the lock is in a cached location, it might take as little as 0 clock\n\n cycles to read it. If the lock is in a cached location that has been\n\n modified in another cache, it can take hundreds of clock cycles, depending\n\n on the relative speeds of the CPU clock and the memory system. After\n\n that, if there’s a cache miss, or some cache has a modified copy of the\n\n location being accessed, the costs can only go up. The bus is locked for\n\n the entire cycle of the instruction, however long that takes.\n\n joe Your notion of how it works in some fixed number of “clock cycles” is\n\n what’s causing your confusion. The instruction in question may take many\n\n (for a very loose definition of many) cycles, depending on how much\n\n contention there is on the bus. If you had 16 cores trying to do the same\n\n thing on the same address simultaneously, the last one to get the bus is\n\n going to take many more cycles than the first one. The information below should help you comprehend this. Not speaking for LogRhythm\n\n Phil Barila | Senior Software Engineer\n\n 720.881.5364 (w)\n\n LogRhythm, Inc.\n\n A LEADER 2013 SIEM Magic Quadrant\n\n Perfect 5-Star Rating in SC Magazine for 5 Consecutive Years -----Original Message-----\n\n From: xxxxx@lists.osr.com\n\n [mailto:xxxxx@lists.osr.com] On Behalf Of\n\n xxxxx@yahoo.com\n\n Sent: Friday, October 11, 2013 10:37 AM\n\n To: Windows System Software Devs Interest List\n\n Subject: RE:[ntdev] How does InterlockedExchange work InterlockedExchange is implemented with lock prefix, which is part of x86\n\n instruction set. You can read about the prefix in Intel or AMD manuals. Execution of ‘locked’ instruction guarantees that no other entity (another\n\n CPU, core and probably DMA controllers) can access the memory address\n\n during execution of the instruction. So, having instruction ‘inc dword [SomeAddress]’ (which is of\n\n ‘read-modify-write’) executing in parallel on two cores, the following\n\n sequence may occur: So we have the variable incremented only once. Having instruction ‘lock inc dword [SomeAddress]’ the sequence will look\n\n like this: Which is what you wanted to do. Not all the instructions allowed to have the prefix, some of instructions\n\n implicitly have the prefix. I also don’t remember granularity of the lock.\n\n Manuals should contain all the details. For our schedule of WDF, WDM, debugging and other seminars visit:\n\n http://www.osr.com/seminars To unsubscribe, visit the List Server section of OSR Online at\n\n http://www.osronline.com/page.cfm?name=ListServer"
    }
]