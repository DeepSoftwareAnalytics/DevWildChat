[
    {
        "link": "https://tensorflow.org/api_docs/python/tf/keras/layers/Embedding",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nUsed in the notebooks\n\nThis layer can only be used on positive integer inputs of a fixed range.\n\n# The model will take as input an integer matrix of size (batch, # input_length), and the largest integer (i.e. word index) in the input # should be no larger than 999 (vocabulary size). # Now model.output_shape is (None, 10, 64), where `None` is the batch\n\nThis method is the reverse of , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by )."
    },
    {
        "link": "https://restack.io/p/embeddings-keras-answer-tutorial-embedding-layers-cat-ai",
        "document": "Learn how to effectively use embedding layers in Keras for deep learning applications with practical examples.\n\nText embedding models are crucial for transforming unstructured textual data into numerical vectors, which are essential for various tasks such as semantic search and clustering. This section delves into the practical applications of embeddings in machine learning, particularly focusing on their role as feature encoders. Text embeddings serve as a bridge between raw text and machine learning algorithms. By converting text into a format that models can understand, embeddings enhance the performance of machine learning tasks. Here are some key points to consider:\n• Dimensionality Reduction: Techniques like SVD or PCA can be applied to embeddings, but caution is advised. Reducing dimensionality by even 10% can lead to a decline in performance for specific tasks due to the loss of information.\n• Feature Encoding: Embeddings can be utilized as both free-text feature encoders and categorical feature encoders. This is particularly beneficial when dealing with meaningful categorical variables, such as job titles. To create embeddings using the library, you can follow this simple example: import taskingai embedding_result = taskingai.inference.text_embedding( model_id=\"YOUR_MODEL_ID\", input=\"Machine learning is a subfield of artificial intelligence (AI) that involves the development of algorithms that allow computers to learn from and make decisions or predictions based on data.\" ) For processing multiple text inputs, the following code snippet demonstrates how to create embeddings for a list: embedding_list = taskingai.inference.text_embedding( model_id=\"YOUR_MODEL_ID\", input=[ \"Machine learning is a subfield of artificial intelligence (AI) that involves the development of algorithms that allow computers to learn from and make decisions or predictions based on data.\", \"Michael Jordan, often referred to by his initials MJ, is considered one of the greatest players in the history of the National Basketball Association (NBA).\" ] ) Embeddings can effectively predict numerical values, such as star ratings from reviews. The following code illustrates how to implement regression using embeddings: For classification tasks, embeddings can categorize reviews into star ratings. The following code demonstrates this approach: Best Practices for Using Embeddings\n• Preprocessing: Always preprocess your text to eliminate noise and enhance relevance.\n• Input Size: Ensure that the text size is manageable for the model to process effectively. By leveraging these techniques and best practices, you can significantly enhance the performance of your machine learning models through effective use of text embeddings.\n• None Learn how to effectively embed Keras layers in your models for improved performance and accuracy.\n• None Explore embedding lookup techniques in the context of embeddings, enhancing your understanding of vector representations.\n• None Explore TensorFlow embeddings with Word2Vec for efficient word representation and semantic understanding in NLP tasks."
    },
    {
        "link": "https://keras.io/api/layers",
        "document": "Layers are the basic building blocks of neural networks in Keras. A layer consists of a tensor-in tensor-out computation function (the layer's method) and some state, held in TensorFlow variables (the layer's weights).\n\nA Layer instance is callable, much like a function:\n\nUnlike a function, though, layers maintain a state, updated when the layer receives data during training, and stored in :\n\nWhile Keras offers a wide range of built-in layers, they don't cover ever possible use case. Creating custom layers is very common, and very easy.\n\nSee the guide Making new layers and models via subclassing for an extensive overview, and refer to the documentation for the base class."
    },
    {
        "link": "https://tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nThe class: the combination of state (weights) and some computation\n\nOne of the central abstractions in Keras is the class. A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs (a \"call\", the layer's forward pass).\n\nHere's a densely-connected layer. It has a state: the variables and .\n\nYou would use a layer by calling it on some tensor input(s), much like a Python function.\n\nNote that the weights and are automatically tracked by the layer upon being set as layer attributes:\n\nBesides trainable weights, you can add non-trainable weights to a layer as well. Such weights are meant not to be taken into account during backpropagation, when you are training the layer.\n\nHere's how to add and use a non-trainable weight:\n\nIt's part of , but it gets categorized as a non-trainable weight:\n\nBest practice: deferring weight creation until the shape of the inputs is known\n\nOur layer above took an argument that was used to compute the shape of the weights and in :\n\nIn many cases, you may not know in advance the size of your inputs, and you would like to lazily create weights when that value becomes known, some time after instantiating the layer.\n\nIn the Keras API, we recommend creating layer weights in the method of your layer. Like this:\n\nThe method of your layer will automatically run build the first time it is called. You now have a layer that's lazy and thus easier to use:\n\nImplementing separately as shown above nicely separates creating weights only once from using weights in every call. However, for some advanced custom layers, it can become impractical to separate the state creation and computation. Layer implementers are allowed to defer weight creation to the first , but need to take care that later calls use the same weights. In addition, since is likely to be executed for the first time inside a , any variable creation that takes place in should be wrapped in a .\n\nIf you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer.\n\nWe recommend creating such sublayers in the method and leave it to the first to trigger building their weights.\n\nWhen writing the method of a layer, you can create loss tensors that you will want to use later, when writing your training loop. This is doable by calling :\n\nNotice that can take the result of plain TensorFlow operations. There is no need to call a object here.\n\nThese losses (including those created by any inner layer) can be retrieved via . This property is reset at the start of every to the top-level layer, so that always contains the loss values created during the last forward pass.\n\nIn addition, the property also contains regularization losses created for the weights of any inner layer:\n\nThese losses are meant to be taken into account when writing training loops, like this:\n\nFor a detailed guide about writing training loops, see the guide to writing a training loop from scratch.\n\nThese losses also work seamlessly with (they get automatically summed and added to the main loss, if any):\n\nYou can optionally enable serialization on your layers\n\nIf you need your custom layers to be serializable as part of a Functional model, you can optionally implement a method:\n\nNote that the method of the base class takes some keyword arguments, in particular a and a . It's good practice to pass these arguments to the parent class in and to include them in the layer config:\n\nIf you need more flexibility when deserializing the layer from its config, you can also override the class method. This is the base implementation of :\n\nTo learn more about serialization and saving, see the complete guide to saving and serializing models.\n\nSome layers, in particular the layer and the layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a (boolean) argument in the method.\n\nBy exposing this argument in , you enable the built-in training and evaluation loops (e.g. ) to correctly use the layer in training and inference.\n\nThe other privileged argument supported by is the argument.\n\nYou will find it in all Keras RNN layers. A mask is a boolean tensor (one boolean value per timestep in the input) used to skip certain input timesteps when processing timeseries data.\n\nKeras will automatically pass the correct argument to for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the layer configured with , and the layer.\n\nTo learn more about masking and how to write masking-enabled layers, please check out the guide \"understanding padding and masking\".\n\nIn general, you will use the class to define inner computation blocks, and will use the class to define the outer model -- the object you will train.\n\nFor instance, in a ResNet50 model, you would have several ResNet blocks subclassing , and a single encompassing the entire ResNet50 network.\n\nThe class has the same API as , with the following differences:\n• It exposes the list of its inner layers, via the property.\n\nEffectively, the class corresponds to what we refer to in the literature as a \"layer\" (as in \"convolution layer\" or \"recurrent layer\") or as a \"block\" (as in \"ResNet block\" or \"Inception block\").\n\nMeanwhile, the class corresponds to what is referred to in the literature as a \"model\" (as in \"deep learning model\") or as a \"network\" (as in \"deep neural network\").\n\nSo if you're wondering, \"should I use the class or the class?\", ask yourself: will I need to call on it? Will I need to call on it? If so, go with . If not (either because your class is just a block in a bigger system, or because you are writing training & saving code yourself), use .\n\nFor instance, we could take our mini-resnet example above, and use it to build a that we could train with , and that we could save with :\n\nPutting it all together: an end-to-end example\n\nHere's what you've learned so far:\n• A encapsulate a state (created in or ) and some computation (defined in ).\n• Layers can be recursively nested to create new, bigger computation blocks.\n• Layers can create and track losses (typically regularization losses) via .\n• The outer container, the thing you want to train, is a . A is just like a , but with added training and serialization utilities.\n\nLet's put all of these things together into an end-to-end example: we're going to implement a Variational AutoEncoder (VAE). We'll train it on MNIST digits.\n\nOur VAE will be a subclass of , built as a nested composition of layers that subclass . It will feature a regularization loss (KL divergence).\n\nNote that since the VAE is subclassing , it features built-in training loops. So you could also have trained it like this:"
    },
    {
        "link": "https://restack.io/p/embeddings-knowledge-embedding-layers-cat-ai",
        "document": "Learn how to effectively use embedding layers in TensorFlow for improved model performance and representation learning."
    },
    {
        "link": "https://tensorflow.org/tutorials/keras/regression",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nIn a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. Contrast this with a classification problem, where the aim is to select a class from a list of classes (for example, where a picture contains an apple or an orange, recognizing which fruit is in the picture).\n\nThis tutorial uses the classic Auto MPG dataset and demonstrates how to build models to predict the fuel efficiency of the late-1970s and early 1980s automobiles. To do this, you will provide the models with a description of many automobiles from that time period. This description includes attributes like cylinders, displacement, horsepower, and weight.\n\nThis example uses the Keras API. (Visit the Keras tutorials and guides to learn more.)\n\nThe dataset is available from the UCI Machine Learning Repository.\n\nFirst download and import the dataset using pandas:\n\nThe dataset contains a few unknown values:\n\nDrop those rows to keep this initial tutorial simple:\n\nThe column is categorical, not numeric. So the next step is to one-hot encode the values in the column with pd.get_dummies.\n\nSplit the data into training and test sets\n\nNow, split the dataset into a training set and a test set. You will use the test set in the final evaluation of your models.\n\nReview the joint distribution of a few pairs of columns from the training set.\n\nThe top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other.\n\nLet's also check the overall statistics. Note how each feature covers a very different range:\n\nSeparate the target value—the \"label\"—from the features. This label is the value that you will train the model to predict.\n\nIn the table of statistics it's easy to see how different the ranges of each feature are:\n\nIt is good practice to normalize features that use different scales and ranges.\n\nOne reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n\nAlthough a model might converge without feature normalization, normalization makes training much more stable.\n\nThe is a clean and simple way to add feature normalization into your model.\n\nThe first step is to create the layer:\n\nThen, fit the state of the preprocessing layer to the data by calling :\n\nCalculate the mean and variance, and store them in the layer:\n\nWhen the layer is called, it returns the input data, with each feature independently normalized:\n\nBefore building a deep neural network model, start with linear regression using one and several variables.\n\nBegin with a single-variable linear regression to predict from .\n\nTraining a model with typically starts by defining the model architecture. Use a model, which represents a sequence of steps.\n\nThere are two steps in your single-variable linear regression model:\n• Normalize the input features using the preprocessing layer.\n\nThe number of inputs can either be set by the argument, or automatically when the model is run for the first time.\n\nFirst, create a NumPy array made of the features. Then, instantiate the and fit its state to the data:\n\nThis model will predict from .\n\nRun the untrained model on the first 10 'Horsepower' values. The output won't be good, but notice that it has the expected shape of :\n\nOnce the model is built, configure the training procedure using the Keras method. The most important arguments to compile are the and the , since these define what will be optimized ( ) and how (using the ).\n\nUse Keras to execute the training for 100 epochs:\n\nVisualize the model's training progress using the stats stored in the object:\n\nCollect the results on the test set for later:\n\nSince this is a single variable regression, it's easy to view the model's predictions as a function of the input:\n\nYou can use an almost identical setup to make predictions based on multiple inputs. This model still does the same \\(y = mx+b\\) except that \\(m\\) is a matrix and \\(x\\) is a vector.\n\nCreate a two-step Keras Sequential model again with the first layer being ( ) you defined earlier and adapted to the whole dataset:\n\nWhen you call on a batch of inputs, it produces outputs for each example:\n\nWhen you call the model, its weight matrices will be built—check that the weights (the \\(m\\) in \\(y=mx+b\\)) have a shape of :\n\nConfigure the model with Keras and train with for 100 epochs:\n\nUsing all the inputs in this regression model achieves a much lower training and validation error than the , which had one input:\n\nCollect the results on the test set for later:\n\nIn the previous section, you implemented two linear models for single and multiple inputs.\n\nHere, you will implement single-input and multiple-input DNN models.\n\nThe code is basically the same except the model is expanded to include some \"hidden\" non-linear layers. The name \"hidden\" here just means not directly connected to the inputs or outputs.\n\nThese models will contain a few more layers than the linear model:\n• The normalization layer, as before (with for a single-input model and for a multiple-input model).\n• Two hidden, non-linear, layers with the ReLU ( ) activation function nonlinearity.\n\nBoth models will use the same training procedure, so the method is included in the function below.\n\nCreate a DNN model with only as input and (defined earlier) as the normalization layer:\n\nThis model has quite a few more trainable parameters than the linear models:\n\nThis model does slightly better than the linear single-input :\n\nIf you plot the predictions as a function of , you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:\n\nCollect the results on the test set for later:\n\nRepeat the previous process using all the inputs. The model's performance slightly improves on the validation dataset.\n\nCollect the results on the test set:\n\nSince all models have been trained, you can review their test set performance:\n\nThese results match the validation error observed during training.\n\nYou can now make predictions with the on the test set using Keras and review the loss:\n\nIt appears that the model predicts reasonably well.\n\nIf you're happy with the model, save it for later use with :\n\nIf you reload the model, it gives identical output:\n\nThis notebook introduced a few techniques to handle a regression problem. Here are a few more tips that may help:\n• Mean squared error (MSE) ( ) and mean absolute error (MAE) ( ) are common loss functions used for regression problems. MAE is less sensitive to outliers. Different loss functions are used for classification problems.\n• Similarly, evaluation metrics used for regression differ from classification.\n• When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n• Overfitting is a common problem for DNN models, though it wasn't a problem for this tutorial. Visit the Overfit and underfit tutorial for more help with this.\n\n# Permission is hereby granted, free of charge, to any person obtaining a # copy of this software and associated documentation files (the \"Software\"), # to deal in the Software without restriction, including without limitation # the rights to use, copy, modify, merge, publish, distribute, sublicense, # and/or sell copies of the Software, and to permit persons to whom the # Software is furnished to do so, subject to the following conditions: # The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the Software. # THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, # FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL # THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER # LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING # FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER"
    },
    {
        "link": "https://tensorflow.org/guide/keras/preprocessing_layers",
        "document": "Save and categorize content based on your preferences.\n\nStay organized with collections Save and categorize content based on your preferences.\n\nThe Keras preprocessing layers API allows developers to build Keras-native input processing pipelines. These input processing pipelines can be used as independent preprocessing code in non-Keras workflows, combined directly with Keras models, and exported as part of a Keras SavedModel.\n\nWith Keras preprocessing layers, you can build and export models that are truly end-to-end: models that accept raw images or raw structured data as input; models that handle feature normalization or feature value indexing on their own.\n• : turns raw strings into an encoded representation that can be read by an layer or layer.\n• : performs categorical feature hashing, also known as the \"hashing trick\".\n• : turns string categorical values into an encoded representation that can be read by an layer or layer.\n• : turns integer categorical values into an encoded representation that can be read by an layer or layer.\n\nThese layers are for standardizing the inputs of an image model.\n• : rescales and offsets the values of a batch of images (e.g. go from inputs in the range to inputs in the range.\n\nThese layers apply random augmentation transforms to a batch of images. They are only active during training.\n\nSome preprocessing layers have an internal state that can be computed based on a sample of the training data. The list of stateful preprocessing layers is:\n• and : hold a mapping between input values and integer indices.\n• : holds the mean and standard deviation of the features.\n\nCrucially, these layers are non-trainable. Their state is not set during training; it must be set before training, either by initializing them from a precomputed constant, or by \"adapting\" them on data.\n\nYou set the state of a preprocessing layer by exposing it to training data, via the method:\n\nThe method takes either a Numpy array or a object. In the case of and , you can also pass a list of strings:\n\nIn addition, adaptable layers always expose an option to directly set state via constructor arguments or weight assignment. If the intended state values are known at layer construction time, or are calculated outside of the call, they can be set without relying on the layer's internal computation. For instance, if external vocabulary files for the , , or layers already exist, those can be loaded directly into the lookup tables by passing a path to the vocabulary file in the layer's constructor arguments.\n\nHere's an example where you instantiate a layer with precomputed vocabulary:\n\nPreprocessing data before the model or inside the model\n\nThere are two ways you could be using preprocessing layers:\n\nOption 1: Make them part of the model, like this:\n\nWith this option, preprocessing will happen on device, synchronously with the rest of the model execution, meaning that it will benefit from GPU acceleration. If you're training on a GPU, this is the best option for the layer, and for all image preprocessing and data augmentation layers.\n\nOption 2: apply it to your , so as to obtain a dataset that yields batches of preprocessed data, like this:\n\nWith this option, your preprocessing will happen on a CPU, asynchronously, and will be buffered before going into the model. In addition, if you call on your dataset, the preprocessing will happen efficiently in parallel with training:\n\nThis is the best option for , and all structured data preprocessing layers. It can also be a good option if you're training on a CPU and you use image preprocessing layers.\n\nNote that the layer can only be executed on a CPU, as it is mostly a dictionary lookup operation. Therefore, if you are training your model on a GPU or a TPU, you should put the layer in the pipeline to get the best performance.\n\nWhen running on a TPU, you should always place preprocessing layers in the pipeline (with the exception of and , which run fine on a TPU and are commonly used as the first layer in an image model).\n\nBenefits of doing preprocessing inside the model at inference time\n\nEven if you go with option 2, you may later want to export an inference-only end-to-end model that will include the preprocessing layers. The key benefit to doing this is that it makes your model portable and it helps reduce the training/serving skew.\n\nWhen all data preprocessing is part of the model, other people can load and use your model without having to be aware of how each feature is expected to be encoded & normalized. Your inference model will be able to process raw images or raw structured data, and will not require users of the model to be aware of the details of e.g. the tokenization scheme used for text, the indexing scheme used for categorical features, whether image pixel values are normalized to or to , etc. This is especially powerful if you're exporting your model to another runtime, such as TensorFlow.js: you won't have to reimplement your preprocessing pipeline in JavaScript.\n\nIf you initially put your preprocessing layers in your pipeline, you can export an inference model that packages the preprocessing. Simply instantiate a new model that chains your preprocessing layers and your training model:\n\nPreprocessing layers are compatible with the tf.distribute API for running training across multiple machines.\n\nIn general, preprocessing layers should be placed inside a and called either inside or before the model as discussed above.\n\nFor more details, refer to the Data preprocessing section of the Distributed input tutorial.\n\nNote that image data augmentation layers are only active during training (similarly to the layer).\n\nYou can see a similar setup in action in the example image classification from scratch.\n\nNote that, here, index 0 is reserved for out-of-vocabulary values (values that were not seen during ).\n\nYou can see the in action in the Structured data classification from scratch example.\n\nNote that index 0 is reserved for missing values (which you should specify as the value 0), and index 1 is reserved for out-of-vocabulary values (values that were not seen during ). You can configure this by using the and constructor arguments of .\n\nYou can see the in action in the example structured data classification from scratch.\n\nApplying the hashing trick to an integer categorical feature\n\nIf you have a categorical feature that can take many different values (on the order of 1e4 or higher), where each value only appears a few times in the data, it becomes impractical and ineffective to index and one-hot encode the feature values. Instead, it can be a good idea to apply the \"hashing trick\": hash the values to a vector of fixed size. This keeps the size of the feature space manageable, and removes the need for explicit indexing.\n\nThis is how you should preprocess text to be passed to an layer.\n\nYou can see the layer in action, combined with an mode, in the example text classification from scratch.\n\nNote that when training such a model, for best performance, you should always use the layer as part of the input pipeline.\n\nEncoding text as a dense matrix of N-grams with multi-hot encoding\n\nThis is how you should preprocess text to be passed to a layer.\n\n# Define some text data to adapt the layer \"The Brain is wider than the Sky\" \"For put them side by side\" \"The one the other will contain\" \"With ease and You beside\" \"The Brain is deeper than the sea\" \"The Brain is deeper than the sea\" \"for if they are held Blue to Blue\" # Preprocess the string inputs, turning them into int sequences # Train the model on the int sequences # For inference, you can export a model that accepts strings as input # Call the end-to-end model on test data (which includes unknown tokens) \"The one the other will absorb\"\n\nEncoding text as a dense matrix of N-grams with TF-IDF weighting\n\nThis is an alternative way of preprocessing text before passing it to a layer.\n\nWorking with lookup layers with very large vocabularies\n\nYou may find yourself working with a very large vocabulary in a , a layer, or an layer. Typically, a vocabulary larger than 500MB would be considered \"very large\".\n\nIn such a case, for best performance, you should avoid using . Instead, pre-compute your vocabulary in advance (you could use Apache Beam or TF Transform for this) and store it in a file. Then load the vocabulary into the layer at construction time by passing the file path as the argument.\n\nUsing lookup layers on a TPU pod or with .\n\nThere is an outstanding issue that causes performance to degrade when using a , , or layer while training on a TPU pod or on multiple machines via . This is slated to be fixed in TensorFlow 2.7."
    },
    {
        "link": "https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python",
        "document": "Keras is a deep learning library that wraps the efficient numerical libraries Theano and TensorFlow.\n\nIn this post, you will discover how to develop and evaluate neural network models using Keras for a regression problem.\n\nAfter completing this step-by-step tutorial, you will know:\n• How to load a CSV dataset and make it available to Keras\n• How to create a neural network model with Keras for a regression problem\n• How to use scikit-learn with Keras to evaluate models using cross-validation\n• How to perform data preparation in order to improve skill with Keras models\n• How to tune the network topology of models with Keras\n\nKick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\n• Update Mar/2018: Added alternate link to download the dataset as the original appears to have been taken down\n\nThe problem that we will look at in this tutorial is the Boston house price dataset.\n\nYou can download this dataset and save it to your current working directly with the file name housing.csv (update: download data from here).\n\nThe dataset describes 13 numerical properties of houses in Boston suburbs and is concerned with modeling the price of houses in those suburbs in thousands of dollars. As such, this is a regression predictive modeling problem. Input attributes include crime rate, the proportion of nonretail business acres, chemical concentrations, and more.\n\nThis is a well-studied problem in machine learning. It is convenient to work with because all the input and output attributes are numerical, and there are 506 instances to work with.\n\nReasonable performance for models evaluated using Mean Squared Error (MSE) is around 20 in thousands of dollars squared (or $4,500 if you take the square root). This is a nice target to aim for with our neural network model.\n\nIn this section, you will create a baseline neural network model for the regression problem.\n\nLet’s start by including all the functions and objects you will need for this tutorial.\n\nYou can now load your dataset from a file in the local directory.\n\nThe dataset is, in fact, not in CSV format in the UCI Machine Learning Repository. The attributes are instead separated by whitespace. You can load this easily using the pandas library. Then split the input (X) and output (Y) attributes, making them easier to model with Keras and scikit-learn.\n\nYou can create Keras models and evaluate them with scikit-learn using handy wrapper objects provided by the Keras library. This is desirable, because scikit-learn excels at evaluating models and will allow you to use powerful data preparation and model evaluation schemes with very few lines of code.\n\nThe Keras wrappers require a function as an argument. This function you must define is responsible for creating the neural network model to be evaluated.\n\nBelow, you will define the function to create the baseline model to be evaluated. It is a simple model with a single, fully connected hidden layer with the same number of neurons as input attributes (13). The network uses good practices such as the rectifier activation function for the hidden layer. No activation function is used for the output layer because it is a regression problem, and you are interested in predicting numerical values directly without transformation.\n\nThe efficient ADAM optimization algorithm is used, and a mean squared error loss function is optimized. This will be the same metric you will use to evaluate the performance of the model. It is a desirable metric because taking the square root gives an error value you can directly understand in the context of the problem (thousands of dollars).\n\nIf you are new to Keras or deep learning, see this Keras tutorial.\n\nThe Keras wrapper object used in scikit-learn as a regression estimator is called KerasRegressor. You create an instance and pass it both the name of the function to create the neural network model and some parameters to pass along to the fit() function of the model later, such as the number of epochs and batch size. Both of these are set to sensible defaults.\n\nThe final step is to evaluate this baseline model. You will use 10-fold cross validation to evaluate the model.\n\nAfter tying this all together, the complete example is listed below.\n\nRunning this code gives you an estimate of the model’s performance on the problem for unseen data.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nNote: The mean squared error is negative because scikit-learn inverts so that the metric is maximized instead of minimized. You can ignore the sign of the result.\n\nThe result reports the mean squared error, including the average and standard deviation (average variance) across all ten folds of the cross validation evaluation.\n\nAn important concern with the Boston house price dataset is that the input attributes all vary in their scales because they measure different quantities.\n\nIt is almost always good practice to prepare your data before modeling it using a neural network model.\n\nContinuing from the above baseline model, you can re-evaluate the same model using a standardized version of the input dataset.\n\nYou can use scikit-learn’s Pipeline framework to perform the standardization during the model evaluation process within each fold of the cross validation. This ensures that there is no data leakage from each test set cross validation fold into the training data.\n\nThe code below creates a scikit-learn pipeline that first standardizes the dataset and then creates and evaluates the baseline neural network model.\n\nAfter tying this together, the complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example provides an improved performance over the baseline model without standardized data, dropping the error.\n\nA further extension of this section would be to similarly apply a rescaling to the output variable, such as normalizing it to the range of 0-1 and using a Sigmoid or similar activation function on the output layer to narrow output predictions to the same range.\n\nMany concerns can be optimized for a neural network model.\n\nPerhaps the point of biggest leverage is the structure of the network itself, including the number of layers and the number of neurons in each layer.\n\nIn this section, you will evaluate two additional network topologies in an effort to further improve the performance of the model. You will look at both a deeper and a wider network topology.\n\nOne way to improve the performance of a neural network is to add more layers. This might allow the model to extract and recombine higher-order features embedded in the data.\n\nIn this section, you will evaluate the effect of adding one more hidden layer to the model. This is as easy as defining a new function to create this deeper model, copied from your baseline model above. You can then insert a new line after the first hidden layer—in this case, with about half the number of neurons.\n\nYour network topology now looks like this:\n\nYou can evaluate this network topology in the same way as above, while also using the standardization of the dataset shown above to improve performance.\n\nAfter tying this together, the complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning this model shows a further improvement in performance from 28 down to 24 thousand squared dollars.\n\nAnother approach to increasing the representational capability of the model is to create a wider network.\n\nIn this section, you will evaluate the effect of keeping a shallow network architecture and nearly doubling the number of neurons in the one hidden layer.\n\nAgain, all you need to do is define a new function that creates your neural network model. Here, you will increase the number of neurons in the hidden layer compared to the baseline model from 13 to 20.\n\nYour network topology now looks like this:\n\nYou can evaluate the wider network topology using the same scheme as above:\n\nAfter tying this together, the complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nBuilding the model reveals a further drop in error to about 21 thousand squared dollars. This is not a bad result for this problem.\n\nIt might have been hard to guess that a wider network would outperform a deeper network on this problem. The results demonstrate the importance of empirical testing in developing neural network models.\n\nIn this post, you discovered the Keras deep learning library for modeling regression problems.\n\nThrough this tutorial, you learned how to develop and evaluate neural network models, including:\n• How to load data and develop a baseline model\n• How to lift performance using data preparation techniques like standardization\n• How to design and evaluate networks with different varying topologies on a problem\n\nDo you have any questions about the Keras deep learning library or this post? Ask your questions in the comments, and I will do my best to answer."
    },
    {
        "link": "https://stackoverflow.com/questions/77280976/change-of-categorical-data-to-numeric-data-in-the-required-columns-so-that-a-lin",
        "document": "Some of the columns on the given dataset contains categorical data. I have to change the data to the numeric so that I can apply simple linear regression to predict the score. The columns name are city, batting_team, bowling_team, batsman, non_striker, bowler. I have to use the features to predict the score by applying simple linear regression.\n\nI used one hot encoder to change the datatype but I'm not able to write perfect code for it."
    },
    {
        "link": "https://stackoverflow.com/questions/55250124/mixing-numerical-and-categorical-data-into-keras-sequential-model-with-dense-lay",
        "document": "I have a training set in a Pandas dataframe, and I pass this data frame into with . Here is some information about the df:\n\nAs you can see, rows in the df contain 5 columns, 4 of which contain numerical values (either int or float), and one which contains a hot encoded array representing some categorical data. I am creating my keras model as seen below:\n\nis just a 1D array of 0s and 1s. So I believe I do need a Dense(1) sigmoid layer at the end, as well as 'binary_crossentropy' loss.\n\nThis model works excellent if I only pass numerical data. But as soon as I introduce hot encodings (categorical data), I get this error:\n\nPlease do not suggest expanding out each value in the one_hot arrays into their own columns. This example is a trimmed down version of my dataset, which contains 6-8 categorical columns, some of the one_hots are arrays of 5000+ size. So this is not a feasible solution for me. I'm looking to perhaps refine my Sequential model (or overhaul the keras model completely) in order to process categorical data along with numerical data.\n\nRemember, the training labels are 1D array of 0/1 values. I need both numerical/categorical training sets predicting one set of outcomes, I can't have one set of predictions from the numerical data and one set of predictions from the categorical data."
    }
]