[
    {
        "link": "https://librosa.org/doc/main/generated/librosa.effects.time_stretch.html",
        "document": "Stretch factor. If , then the signal is sped up. If , then the signal is slowed down."
    },
    {
        "link": "https://librosa.org/doc-playground/latest/generated/librosa.effects.time_stretch.html",
        "document": "Stretch factor. If , then the signal is sped up. If , then the signal is slowed down."
    },
    {
        "link": "https://librosa.org/doc/main/tutorial.html",
        "document": "You're reading the documentation for a development version. For the latest released version, please have a look at 0.11.0 .\n\nThis section covers the fundamentals of developing with librosa, including a package overview, basic and advanced usage, and integration with the scikit-learn package. We will assume basic familiarity with Python and NumPy/SciPy.\n\nBefore diving into the details, we’ll walk through a brief example program # 1. Get the file path to an included audio example # 4. Convert the frame indices of beat events into timestamps The first step of the program: gets the path to an audio example file included with librosa. After this step, will be a string variable containing the path to the example audio file. loads and decodes the audio as a time series , represented as a one-dimensional NumPy floating point array. The variable sr contains the sampling rate of , that is, the number of samples per second of audio. By default, all audio is mixed to mono and resampled to 22050 Hz at load time. This behavior can be overridden by supplying additional arguments to . Next, we run the beat tracker: The output of the beat tracker is an estimate of the tempo (in beats per minute), and an array of frame numbers corresponding to detected beat events. Frames here correspond to short windows of the signal ( ), each separated by samples. librosa uses centered frames, so that the kth frame is centered around sample . The next operation converts the frame numbers into timings: Now, will be an array of timestamps (in seconds) corresponding to detected beat events. The contents of should look something like this:\n\nHere we’ll cover a more advanced example, integrating harmonic-percussive separation, multiple spectral features, and beat-synchronous feature aggregation. # Separate harmonics and percussives into two waveforms # This time, we'll use the mean value (default) instead of median # We'll use the median value of each feature between beat frames This example builds on tools we’ve already covered in the quickstart example, so here we’ll focus just on the new parts. The first difference is the use of the effects module for time-series harmonic-percussive separation: The result of this line is that the time series has been separated into two time series, containing the harmonic (tonal) and percussive (transient) portions of the signal. Each of and have the same shape and duration as . The motivation for this kind of operation is two-fold: first, percussive elements tend to be stronger indicators of rhythmic content, and can help provide more stable beat tracking results; second, percussive elements can pollute tonal feature representations (such as chroma) by contributing energy across all frequency bands, so we’d be better off without them. Next, we introduce the feature module and extract the Mel-frequency cepstral coefficients from the raw signal : The output of this function is the matrix , which is a of shape (where denotes the track duration in frames). Note that we use the same here as in the beat tracker, so the detected values correspond to columns of . The first type of feature manipulation we introduce is , which computes (smoothed) first-order differences among columns of its input: The resulting matrix has the same shape as the input . The second type of feature manipulation is , which aggregates columns of its input between sample indices (e.g., beat frames): Here, we’ve vertically stacked the and matrices together. The result of this operation is a matrix with the same number of rows as its input, but the number of columns depends on . Each column will be the average of input columns between and . ( will be expanded to span the full range so that all data is accounted for.) Next, we compute a chromagram using just the harmonic component: After this line, will be a of shape , and each row corresponds to a pitch class (e.g., C, C#, etc.). Each column of is normalized by its peak value, though this behavior can be overridden by setting the parameter. Once we have the chromagram and list of beat frames, we again synchronize the chroma between beat events: This time, we’ve replaced the default aggregate operation (average, as used above for MFCCs) with the median. In general, any statistical summarization function can be supplied here, including , , , etc. Finally, the all features are vertically stacked again:"
    },
    {
        "link": "https://restack.io/p/open-source-ai-libraries-knowledge-audio-processing-cat-ai",
        "document": "Librosa is a powerful Python library designed for audio and music signal analysis. It provides a wide range of functionalities that are essential for offline audio processing, making it a go-to choice for developers and researchers alike. Below are some of the key features and functions that Librosa offers:\n\nLibrosa allows you to load audio files easily with the function. This function reads audio files and returns the audio time series and its sampling rate. Here’s how you can use it:\n\nLibrosa excels in extracting various audio features that are crucial for analysis. Some of the most commonly used features include:\n• Mel-frequency cepstral coefficients (MFCCs): Useful for speech and audio classification tasks.\n• Chroma features: Represent the energy distribution across different pitch classes.\n• Spectral features: Such as spectral centroid, bandwidth, and roll-off, which provide insights into the frequency content of the audio.\n\nYou can extract MFCCs using the following code:\n\nLibrosa also provides functions for time stretching and pitch shifting, which are essential for audio manipulation. The function allows you to change the speed of an audio signal without affecting its pitch, while lets you shift the pitch of an audio signal. Here’s an example:\n\nVisualizing audio data is crucial for understanding its characteristics. Librosa provides several functions for plotting audio waveforms and spectrograms. For instance, you can visualize the waveform using:\n\nLibrosa is an indispensable tool for anyone working with audio data in Python. Its comprehensive set of features for audio loading, processing, and visualization makes it a top choice for offline Python libraries for audio processing. For more detailed information, you can refer to the official documentation at Librosa Documentation."
    },
    {
        "link": "https://librosa.org/librosa",
        "document": ""
    },
    {
        "link": "https://librosa.org/doc-playground/0.7.2/generated/librosa.output.write_wav.html",
        "document": "Note: only mono or stereo, floating-point data is supported.\n\nThis function is deprecated in librosa 0.7.0. It will be removed in 0.8. Usage of should be replaced by .\n\nNote that only floating-point values are supported. enable amplitude normalization. For floating point y, scale the data to the range [-1, +1].\n\nTrim a signal to 5 seconds and save it back"
    },
    {
        "link": "https://stackoverflow.com/questions/60105626/split-audio-on-timestamps-librosa",
        "document": "librosa is first and foremost a library for audio analysis, not audio synthesis or processing. The support for writing simple audio files is given (see here), but it is also stated there:\n\nThis function is deprecated in librosa 0.7.0. It will be removed in 0.8. Usage of write_wav should be replaced by soundfile.write.\n\nGiven this information, I'd rather use a tool like sox to split audio files.\n\nFrom \"Split mp3 file to TIME sec each using SoX\":\n\nYou can run SoX like this:\n\nIt will create a series of files with a 2-second chunk of the audio each.\n\nIf you'd rather stay within Python, you might want to use pysox for the job."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2024/01/hands-on-guide-to-librosa-for-handling-audio-files",
        "document": "Librosa is a powerful Python library that offers a wide range of tools and functionalities for handling audio files. Whether you’re a music enthusiast, a data scientist, or a machine learning engineer, Librosa can be a valuable asset in your toolkit. In this hands-on guide, we will explore the importance of Librosa for audio file handling and its benefits and provide an overview of the library itself.\n\nUnderstanding the Importance of Librosa for Audio File Handling\n\nAudio file handling is crucial in various domains, including music analysis, speech recognition, and sound processing. Librosa simplifies working with audio files by providing a high-level interface and a comprehensive set of functions. It allows users to perform audio data preprocessing, feature extraction, visualization, analysis, and even advanced techniques like music genre classification and audio source separation.\n\nBenefits of Using Librosa for Audio Analysis\n\nLibrosa offers several benefits that make it a preferred choice for audio analysis:\n• Easy Installation and Setup: Installing Librosa is a breeze, thanks to its availability on popular package managers like pip and conda. Once installed, you can quickly import it into your Python environment and start working with audio files.\n• Extensive Functionality: Librosa provides various functions for various audio processing tasks. Whether you need to resample audio, extract features, visualize waveforms, or perform advanced techniques, Librosa has got you covered.\n• Integration with Other Libraries: Librosa integrates with popular Python libraries such as NumPy, SciPy, and Matplotlib. This allows users to leverage the power of these libraries in conjunction with Librosa for more advanced audio analysis tasks.\n\nBefore diving into the practical aspects of using Librosa, let’s briefly overview the library’s structure and critical components.\n\nLibrosa is built on top of NumPy and SciPy, which are fundamental libraries for scientific computing in Python. It provides a set of modules and submodules that cater to different aspects of audio file handling. Some of the key modules include:\n• Core: This module contains the core functionality of Librosa, including functions for loading audio files, resampling, and time stretching.\n• Feature Extraction: This module extracts audio features such as mel spectrogram, spectral contrast, chroma features, zero crossing rate, and temporal centroid.\n• Visualization: As the name suggests, this module provides functions for visualizing audio waveforms, spectrograms, and other related visualizations.\n• Effects: This module offers functions for audio processing and manipulation, such as time and pitch shifting, noise reduction, and audio segmentation.\n• Advanced Techniques: This module covers advanced techniques like music genre classification, speech emotion recognition, and audio source separation.\n\nNow that we have a basic understanding let’s dive into the practical aspects of using this powerful library.\n\nTo begin using Librosa, install it in your Python environment. The installation process is straightforward and can be done using popular package managers like pip or conda. Once installed, you can import Librosa into your Python script or Jupyter Notebook.\n\nBefore diving into audio analysis, it is essential to preprocess the audio data to ensure its quality and compatibility with the desired analysis techniques. It provides several functions for audio data preprocessing, including resampling, time stretching, audio normalization, scaling, and handling missing data.\n\nFor example, let’s say you have an audio file with a sample rate of 44100 Hz, but you want to resample it to 22050 Hz. You can use the `librosa.resample()` function to achieve this:\n\nFeature extraction is a crucial step in audio analysis, as it helps capture the audio signal’s relevant characteristics. Librosa offers various functions for extracting audio features, such as mel spectrogram, spectral contrast, chroma features, zero crossing rate, and temporal centroid. These features can be used for music genre classification, speech recognition, and sound event detection.\n\nFor example, let’s extract the mel spectrogram of an audio file using Librosa:\n\nVisualizing audio data can provide valuable insights into its characteristics and help understand the underlying patterns. Librosa provides functions for visualizing audio waveforms, spectrograms, and other related visualizations. It also offers tools for analyzing audio signal envelopes onsets and identifying key and pitch estimation.\n\nFor example, let’s visualize the waveform of an audio file using Librosa:\n\nLibrosa enables users to perform various audio processing and manipulation tasks. This includes time and pitch shifting, noise reduction, audio denoising, and audio segmentation. These techniques can be helpful in applications like audio enhancement, audio synthesis, and sound event detection.\n\nFor example, let’s perform time stretching on an audio file using Librosa:\n\nIf you want to listen to or save the stretched audio, you can use the following code:\n\nLibrosa goes beyond fundamental audio analysis and offers advanced techniques for specialized tasks. This includes music genre classification, speech emotion recognition, and audio source separation. These techniques leverage machine learning algorithms and signal processing techniques to achieve accurate results.\n\nLibrosa is a versatile and powerful library for handling audio files in Python. It provides a comprehensive set of tools and functionalities for audio data preprocessing, feature extraction, visualization, analysis, and advanced techniques. By following this hands-on guide, you can leverage the power to handle audio files effectively and unlock valuable insights from audio data."
    },
    {
        "link": "https://stackoverflow.com/questions/30619740/downsampling-wav-audio-file",
        "document": "To downsample (also called decimate) your signal (it means to reduce the sampling rate), or upsample (increase the sampling rate) you need to interpolate between your data.\n\nThe idea is that you need to somehow draw a curve between your points, and then take values from this curve at the new sampling rate. This is because you want to know the value of the sound wave at some time that wasn't sampled, so you have to guess this value by one way or an other. The only case where subsampling would be easy is when you divide the sampling rate by an integer $k$. In this case, you just have to take buckets of $k$ samples and keep only the first one. But this won't answer your question. See the picture below where you have a curve sampled at two different scales.\n\nYou could do it by hand if you understand the principle, but I strongly recommend you to use a library. The reason is that interpolating the right way isn't easy or either obvious.\n\nYou could use a linear interpolation (connect points with a line) or a binomial interpolation (connect three points with a piece of polynom) or (sometimes the best for sound) use a Fourier transform and interpolate in the space of frequency. Since fourier transform isn't something you want to re-write by hand, if you want a good subsampling/supsampling, See the following picture for two curves of upsampling using a different algorithm from scipy. The \"resampling\" function use fourier transform.\n\nI was indeed in the case I was loading a 44100Hz wave file and required a 48000Hz sampled data, so I wrote the few following lines to load my data:\n\nNotice you can also use the method decimate in the case you are only doing downsampling and want something faster than fourier."
    },
    {
        "link": "https://comet.com/site/blog/working-with-audio-data-for-machine-learning-in-python",
        "document": "Most of the attention, when it comes to machine learning or deep learning models, is given to computer vision or natural language sub-domain problems.\n\nHowever, there’s an ever-increasing need to process audio data, with emerging advancements in technologies like Google Home and Alexa that extract information from voice signals. As such, working with audio data has become a new trend and area of study.\n\nThe possible applications extend to voice recognition, music classification, tagging, and generation, and are paving the way for audio use cases to become the new era of deep learning.\n\nSound are pressure waves, and these waves can be represented by numbers over a time period. These air pressure differences communicates with the brain. Audio files are generally stored in .wav format and need to be digitized, using the concept of sampling.\n\nLoading and Visualizing an audio file in Python\n\nis a Python library that helps us work with audio data. For complete documentation, you can also refer to this link.\n• Loading the file: The audio file is loaded into a NumPy array after being sampled at a particular sample rate (sr).\n\n3. Playing Audio : Using, , we can play the audio file in a Jupyter Notebook, using the command\n\n4. Waveform visualization : To visualize the sampled signal and plot it, we need two Python libraries—Matplotlib and Librosa. The following code depicts the waveform visualization of the amplitude vs the time representation of the signal.\n\n5. Spectrogram : A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. They are time-frequency portraits of signals. Using a spectrogram, we can see how energy levels (dB) vary over time.\n\n6. Log-frequency axis: Features can be obtained from a spectrogram by converting the linear frequency axis, as shown above, into a logarithmic axis. The resulting representation is also called a log-frequency spectrogram. The code we need to write here is:\n\nCreating an audio signal and saving it\n\nA digitized audio signal is a NumPy array with a specified frequency and sample rate. The analog wave format of the audio signal represents a function (i.e. sine, cosine etc). We need to save the composed audio signal generated from the NumPy array. This kind of audio creation could be used in applications that require voice-to-text translation in audio-enabled bots or search engines.\n\nSo far, so good. Easy and fun to learn. But data pre-processing steps can be difficult and memory-consuming, as we’ll often have to deal with audio signals that are longer than 1 second. Compared to the images or number of pixels in each training item in popular datasets such as MNIST or CIFAR, the number of data points in digital audio is much higher. This may lead to memory issues."
    }
]