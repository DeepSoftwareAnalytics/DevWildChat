[
    {
        "link": "https://docs.python.org/3/library/subprocess.html",
        "document": "The module allows you to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. This module intends to replace several older modules and functions:\n\nInformation about how the module can be used to replace these modules and functions can be found in the following sections.\n\nInstances of the class have the following methods: Check if child process has terminated. Set and return attribute. Otherwise, returns . Wait for child process to terminate. Set and return attribute. If the process does not terminate after timeout seconds, raise a exception. It is safe to catch this exception and retry the wait. This will deadlock when using or and the child process generates enough output to a pipe such that it blocks waiting for the OS pipe buffer to accept more data. Use when using pipes to avoid that. When the parameter is not , then (on POSIX) the function is implemented using a busy loop (non-blocking call and short sleeps). Use the module for an asynchronous wait: see . Interact with process: Send data to stdin. Read data from stdout and stderr, until end-of-file is reached. Wait for process to terminate and set the attribute. The optional input argument should be data to be sent to the child process, or , if no data should be sent to the child. If streams were opened in text mode, input must be a string. Otherwise, it must be bytes. returns a tuple . The data will be strings if streams were opened in text mode; otherwise, bytes. Note that if you want to send data to the process’s stdin, you need to create the Popen object with . Similarly, to get anything other than in the result tuple, you need to give and/or too. If the process does not terminate after timeout seconds, a exception will be raised. Catching this exception and retrying communication will not lose any output. The child process is not killed if the timeout expires, so in order to cleanup properly a well-behaved application should kill the child process and finish communication: The data read is buffered in memory, so do not use this method if the data size is large or unlimited. Sends the signal signal to the child. Do nothing if the process completed. On Windows, SIGTERM is an alias for . CTRL_C_EVENT and CTRL_BREAK_EVENT can be sent to processes started with a creationflags parameter which includes . Stop the child. On POSIX OSs the method sends to the child. On Windows the Win32 API function is called to stop the child. Kills the child. On POSIX OSs the function sends SIGKILL to the child. On Windows is an alias for . The following attributes are also set by the class for you to access. Reassigning them to new values is unsupported: The args argument as it was passed to – a sequence of program arguments or else a single string. If the stdin argument was , this attribute is a writeable stream object as returned by . If the encoding or errors arguments were specified or the text or universal_newlines argument was , the stream is a text stream, otherwise it is a byte stream. If the stdin argument was not , this attribute is . If the stdout argument was , this attribute is a readable stream object as returned by . Reading from the stream provides output from the child process. If the encoding or errors arguments were specified or the text or universal_newlines argument was , the stream is a text stream, otherwise it is a byte stream. If the stdout argument was not , this attribute is . If the stderr argument was , this attribute is a readable stream object as returned by . Reading from the stream provides error output from the child process. If the encoding or errors arguments were specified or the text or universal_newlines argument was , the stream is a text stream, otherwise it is a byte stream. If the stderr argument was not , this attribute is . Use rather than , or to avoid deadlocks due to any of the other OS pipe buffers filling up and blocking the child process. The process ID of the child process. Note that if you set the shell argument to , this is the process ID of the spawned shell. The child return code. Initially , is set by a call to the , , or methods if they detect that the process has terminated. A value indicates that the process hadn’t yet terminated at the time of the last method call. A negative value indicates that the child was terminated by signal (POSIX only).\n\nThe class and following constants are only available on Windows. Partial support of the Windows STARTUPINFO structure is used for creation. The following attributes can be set by passing them as keyword-only arguments. A bit field that determines whether certain attributes are used when the process creates a window. If specifies , this attribute is the standard input handle for the process. If is not specified, the default for standard input is the keyboard buffer. If specifies , this attribute is the standard output handle for the process. Otherwise, this attribute is ignored and the default for standard output is the console window’s buffer. If specifies , this attribute is the standard error handle for the process. Otherwise, this attribute is ignored and the default for standard error is the console window’s buffer. If specifies , this attribute can be any of the values that can be specified in the parameter for the ShowWindow function, except for . Otherwise, this attribute is ignored. is provided for this attribute. It is used when is called with . A dictionary of additional attributes for process creation as given in , see UpdateProcThreadAttribute. Sequence of handles that will be inherited. close_fds must be true if non-empty. The handles must be temporarily made inheritable by when passed to the constructor, else will be raised with Windows error (87). In a multithreaded process, use caution to avoid leaking handles that are marked inheritable when combining this feature with concurrent calls to other process creation functions that inherit all handles such as . This also applies to standard handle redirection, which temporarily creates inheritable handles. The module exposes the following constants. The standard input device. Initially, this is the console input buffer, . The standard output device. Initially, this is the active console screen buffer, . The standard error device. Initially, this is the active console screen buffer, . Hides the window. Another window will be activated. Specifies that the , , and attributes contain additional information. Specifies that the attribute contains additional information. A parameter to specify that the Working in Background mouse cursor will be displayed while a process is launching. This is the default behavior for GUI processes. A parameter to specify that the mouse cursor will not be changed when launching a process. The new process has a new console, instead of inheriting its parent’s console (the default). A parameter to specify that a new process group will be created. This flag is necessary for using on the subprocess. This flag is ignored if is specified. A parameter to specify that a new process will have an above average priority. A parameter to specify that a new process will have a below average priority. A parameter to specify that a new process will have a high priority. A parameter to specify that a new process will have an idle (lowest) priority. A parameter to specify that a new process will have a normal priority. (default) A parameter to specify that a new process will have realtime priority. You should almost never use REALTIME_PRIORITY_CLASS, because this interrupts system threads that manage mouse input, keyboard input, and background disk flushing. This class can be appropriate for applications that “talk” directly to hardware or that perform brief tasks that should have limited interruptions. A parameter to specify that a new process will not create a window. A parameter to specify that a new process will not inherit its parent’s console. This value cannot be used with CREATE_NEW_CONSOLE. A parameter to specify that a new process does not inherit the error mode of the calling process. Instead, the new process gets the default error mode. This feature is particularly useful for multithreaded shell applications that run with hard errors disabled. A parameter to specify that a new process is not associated with the job.\n\nPrior to Python 3.5, these three functions comprised the high level API to subprocess. You can now use in many cases, but lots of existing code calls these functions. Run the command described by args. Wait for command to complete, then return the attribute. Code needing to capture stdout or stderr should use instead: To suppress stdout or stderr, supply a value of . The arguments shown above are merely some common ones. The full function signature is the same as that of the constructor - this function passes all supplied arguments other than timeout directly through to that interface. Do not use or with this function. The child process will block if it generates enough output to a pipe to fill up the OS pipe buffer as the pipes are not being read from. Changed in version 3.12: Changed Windows shell search order for . The current directory and are replaced with and . As a result, dropping a malicious program named into a current directory no longer works. Run command with arguments. Wait for command to complete. If the return code was zero then return, otherwise raise . The object will have the return code in the attribute. If was unable to start the process it will propagate the exception that was raised. Code needing to capture stdout or stderr should use instead: To suppress stdout or stderr, supply a value of . The arguments shown above are merely some common ones. The full function signature is the same as that of the constructor - this function passes all supplied arguments other than timeout directly through to that interface. Do not use or with this function. The child process will block if it generates enough output to a pipe to fill up the OS pipe buffer as the pipes are not being read from. Changed in version 3.12: Changed Windows shell search order for . The current directory and are replaced with and . As a result, dropping a malicious program named into a current directory no longer works. Run command with arguments and return its output. If the return code was non-zero it raises a . The object will have the return code in the attribute and any output in the attribute. The arguments shown above are merely some common ones. The full function signature is largely the same as that of - most arguments are passed directly through to that interface. One API deviation from behavior exists: passing will behave the same as (or , depending on other arguments) rather than using the parent’s standard input file handle. By default, this function will return the data as encoded bytes. The actual encoding of the output data may depend on the command being invoked, so the decoding to text will often need to be handled at the application level. This behaviour may be overridden by setting text, encoding, errors, or universal_newlines to as described in Frequently Used Arguments and . To also capture standard error in the result, use : 'ls: non_existent_file: No such file or directory\n\n' Changed in version 3.4: Support for the input keyword argument was added. Changed in version 3.6: encoding and errors were added. See for details. Added in version 3.7: text was added as a more readable alias for universal_newlines. Changed in version 3.12: Changed Windows shell search order for . The current directory and are replaced with and . As a result, dropping a malicious program named into a current directory no longer works.\n\nIn this section, “a becomes b” means that b can be used as a replacement for a. All “a” functions in this section fail (more or less) silently if the executed program cannot be found; the “b” replacements raise instead. In addition, the replacements using will fail with a if the requested operation produces a non-zero return code. The output is still available as the attribute of the raised exception. In the following examples, we assume that the relevant functions have already been imported from the module. # Allow p1 to receive a SIGPIPE if p2 exits. The call after starting the p2 is important in order for p1 to receive a SIGPIPE if p2 exits before p1. Alternatively, for trusted input, the shell’s own pipeline support may still be used directly:\n• None Calling the program through the shell is usually not required.\n• None The return value is encoded differently to that of .\n• None The function ignores SIGINT and SIGQUIT signals while the command is running, but the caller must do this separately when using the module. A more realistic example would look like this: If the cmd argument to popen2 functions is a string, the command is executed through /bin/sh. If it is a list, the command is directly executed. and basically work as , except that:\n• None raises an exception if the execution fails.\n• None The capturestderr argument is replaced with the stderr argument.\n• None and must be specified.\n• None popen2 closes all file descriptors by default, but you have to specify with to guarantee this behavior on all platforms or past Python versions.\n\nConverting an argument sequence to a string on Windows¶ On Windows, an args sequence is converted to a string that can be parsed using the following rules (which correspond to the rules used by the MS C runtime):\n• None Arguments are delimited by white space, which is either a space or a tab.\n• None A string surrounded by double quotation marks is interpreted as a single argument, regardless of white space contained within. A quoted string can be embedded in an argument.\n• None A double quotation mark preceded by a backslash is interpreted as a literal double quotation mark.\n• None Backslashes are interpreted literally, unless they immediately precede a double quotation mark.\n• None If backslashes immediately precede a double quotation mark, every pair of backslashes is interpreted as a literal backslash. If the number of backslashes is odd, the last backslash escapes the next double quotation mark as described in rule 3. Module which provides function to parse and escape command lines. Disabling use of or ¶ On Linux, defaults to using the system call internally when it is safe to do so rather than . This greatly improves performance. If you ever encounter a presumed highly unusual situation where you need to prevent from being used by Python, you can set the attribute to a false value. Setting this has no impact on use of which could use internally within its libc implementation. There is a similar attribute if you need to prevent use of that. It is safe to set these to false on any Python version. They will have no effect on older versions when unsupported. Do not assume the attributes are available to read. Despite their names, a true value does not indicate that the corresponding function will be used, only that it may be. Please file issues any time you have to use these private knobs with a way to reproduce the issue you were seeing. Link to that issue from a comment in your code."
    },
    {
        "link": "https://python.land/operating-system/python-subprocess",
        "document": "Despite the many libraries on PyPI, sometimes you need to run an external command from your Python code. The built-in Python subprocess module makes this relatively easy. In this article, you’ll learn some basics about processes and sub-processes.\n\nWe’ll use the Python subprocess module to safely execute external commands, capture the output, and optionally feed them with input from standard in. If you’re familiar with the theory of processes and sub-processes, you can safely skip the first section.\n\nA program that is executed on a computer is also called a process. But what is a process, exactly? Let’s define it more formally:\n\nA process can have multiple Python threads, this is called multi-threading. In turn, a computer can run multiple processes at once. These processes can be different programs, but they can also be multiple instances of the same program. Our article on concurrency with Python explains this in great detail. The following images come from that article, too:\n\nIf you want to run an external command, it means you need to create a new process from your Python process. Such a process is often called a child process or a sub-process. Visually, this is what happens when one process spawns two sub-processes:\n\nWhat happens internally (inside the OS kernel) is what’s called a fork. The process forks itself, meaning a new copy of the process is created and started. This can be useful if you want to parallelize your code and utilize multiple CPUs on your machine. That’s what we call multiprocessing.\n\nHowever, we can utilize this same technique to start another process. First, the process forks itself, creating a copy. That copy, in turn, replaces itself with another process: the process you were looking to execute.\n\nWe can go the low-level way and do much of this ourselves using the Python subprocess module, but luckily, Python also offers a wrapper that will take care of all the nitty-gritty details and do so safely, too. Thanks to the wrapper, running an external command comes down to calling a function. This wrapper is the function run() from the subprocess package, and that’s what we’ll use in this article.\n\nI thought it would be nice for you to know what’s going on internally, but if you feel confused, rest assured that you don’t need this knowledge to do what you want: running an external command with the Python subprocess module.\n\nEnough with the theory; it’s time to get our hands dirty and write some code to execute external commands.\n\nFirst of all, you need to import the subprocess library. Since it is part of Python 3, you don’t need to install it separately. From this library, we’ll work with the run command. This command was added in Python 3.5. Make sure you have at least that Python version, but preferably you should be running the latest version. Check our detailed Python installation instructions if you need help with that.\n\nLet’s start with a simple call to ls, to list the current directories and files:\n\nIn fact, we can call Python, the binary, from our Python code. Let’s request the version of the default python3 installation on our system next:\n• Run a subprocess, in this case the python3 binary, with one argument:\n• Inspect the result variable, which is of the type\n\nThe process returned code 0, meaning it was executed successfully. Any other return code would mean there was some kind of error. It depends on the process you called what the different return code means.\n\nAs you can see in the output, the Python binary printed its version number on standard out, which is usually your terminal. Your result may vary because your Python version will likely be different. Perhaps, you’ll even get an error looking like this: FileNotFoundError: [Errno 2] No such file or directory: 'python3' . In this case, make sure the Python binary is called python3 on your system too, and that it’s in the PATH.\n\nIf you run an external command, you’ll likely want to capture the output of that command. We can achieve this with the capture_output=True option:\n\nAs you can see, Python didn’t print its version to our terminal this time. The subprocess.run command redirected the standard out and standard error streams so it could capture them and store the result for us. After inspecting the result variable, we see that the Python version was captured from standard out. Since there were no errors, stderr is empty.\n\nI also added the option encoding=’UTF-8′. If you don’t, assumes the output is a stream of bytes because it doesn’t have this information. Try it, if you want. As a result, and will be byte arrays. Hence, if you know the output will be ASCII text or UTF-8 text, you’re better off specifying it so the run function encodes the captured output accordingly as well.\n\nAlternatively, you can also use the option text=True without specifying the encoding. Python will capture the output as text. I’d recommend specifying the encoding explicitly if you know it.\n\nIf the external command expects data on standard input, we can do so easily as well with the option of Python’s function. Please note that I’m not going into streaming data here. We’ll build on the previous examples here:\n\nWe just used Python to execute some Python code with the python3 binary. It’s completely useless but (hopefully) very instructive!\n\nThe code variable is a multi-line Python string, and we assign it as input to the command using the option.\n\nIf you are looking to execute shell commands on Unix-like systems, by which I mean anything you would normally type into a Bash-like shell, you need to realize that these are often not external binaries that are executed. For example, expressions like and loops, or pipes and other operators, are interpreted by the shell itself.\n\nPython often has alternatives in the form of built-in libraries, which you should prefer. But if you need to execute a shell command, for whatever reason, will happily do so when you use the option. It allows you to enter commands just as if you were entering them in a Bash compatible shell:\n\nBut a warning is in place: using this method is prone to command injection attacks (see: caveats).\n\nCaveats to look out for\n\nRunning external commands is not without risks. Please read this section very carefully.\n\nYou might see code examples where is used to execute a command. The module is more powerful, though, and the official Python docs recommend using it over . Another issue with is that it is more prone to command injection.\n\nA common attack, or exploit, is to inject extra commands to gain control over a computer system. For example, if you ask your user for input and use that input in a call to or a call to , you’re at risk of a command injection attack.\n\nTo demonstrate, the following code allows us to run any shell command:\n\nBecause we directly used the user input, the user can run any command simply by appending it with a semicolon. E.g., the following input will list the / directory and echo a text. Try it for yourself:\n\nThe solution is not to try and clean the user input. You might be tempted to start looking for semicolons and rejecting the input of you find one. Don’t; hackers can think of at least 5 other ways to append a command in this situation. It’s an uphill battle.\n\nThe better solution is to not use , and feed the command in a list as in the earlier examples. Input like this will fail in such cases because the subprocess module will ensure the input is an argument to the program you’re executing instead of a new command.\n\nWith the same input but with , you will get this:\n\nThe command is treated as an argument to , which in turn tells us that It can’t find that file or directory.\n\nIn fact, using user input is always dangerous, not just because of command injection. For example, suppose you allow a user to input a file name. After this, we read the file and show it to the user. Although this might seem harmless, a user could enter something like this:\n\nWhere might contain your database password… oops! You always need to sanitize and check user input properly. However, how to do that properly is beyond the scope of this article.\n\nThe following related resources will help you dive even deeper in this subject:\n• The official documentation has all the details about the subprocess library\n• Our article on Python concurrency explains more about processes and threads\n• Our section on using the Unix shell might come in handy"
    },
    {
        "link": "https://datacamp.com/tutorial/python-subprocess",
        "document": "Let's now take a look at some Python subprocess examples.\n\nThe method is a convenient way to run a subprocess and wait for it to complete. It lets you choose the command to run and add options like arguments, environment variables, and input/output redirections. Once the subprocess is started, the method blocks until the subprocess completes and returns a object, which contains the return code and output of the subprocess.\n\nThe method takes several arguments, some of which are:\n• : The command to run and its arguments, passed as a list of strings.\n• : When set to True, will capture the standard output and standard error.\n• : When set to True, will return the stdout and stderr as string, otherwise as bytes.\n• : a boolean value that indicates whether to check the return code of the subprocess, if check is true and the return code is non-zero, then subprocess is raised.\n• : A value in seconds that specifies how long to wait for the subprocess to complete before timing out.\n• : A boolean value that indicates whether to run the command in a shell. This means that the command is passed as a string, and shell-specific features, such as wildcard expansion and variable substitution, can be used.\n\nThe method also returns a object, which contains the following attributes:\n• : The command and arguments that were run.\n• : The return code of the subprocess.\n• : The standard output of the subprocess, as a bytes object.\n• : The standard error of the subprocess, as a bytes object.\n\nLinux or mac users replace to and get rid of the argument.\n\nYou can also run a python script using the method. Let’s start by creating a simple Python script in file\n\nSave this file as . Now, you can use the module to run this file:\n\nFor simple use-cases, you can directly pass a python command in the function. Here is how:\n\nIn the list, the first element is a path to executable Python (your path may be different). The second element, is a Python tag that allows the user to write Python code as text to the command line. The third element, , is the Python command itself.\n\nExample 4: Using the check argument\n\nThe check argument is an optional argument of the function in the Python subprocess module. It is a boolean value that controls whether the function should check the return code of the command being run.\n\nWhen check is set to , the function will check the return code of the command and raise a exception if the return code is non-zero. The exception will have the return code, , , and as attributes.\n\nWhen check is set to (default), the function will not check the return code and will not raise an exception, even if the command fails.\n\nNotice that the command failed because does not exist. As opposed to when you set , your process won’t fail; instead, you will get the error message in .\n\nis a lower-level interface to running subprocesses, while is a higher-level wrapper around that is intended to be more convenient to use.\n\nallows you to start a new process and interact with its standard input, output, and error streams. It returns a handle to the running process that can be used to wait for the process to complete, check its return code, or terminate it.\n\nis a more convenient function that allows you to run a command and capture its output in a single call, without having to create a object and manage the streams yourself. It also allows you to specify various options for running the command, such as whether to raise an exception if the command fails.\n\nIn general, you should use if you just need to run a command and capture its output and if you need more control over the process, such as interacting with its input and output streams.\n\nThe class takes the same arguments as , including the args that specify the command to be run and other optional arguments such as , , , , , and . Also, the class has several methods that allow you to interact with the process, such as , , , , and .\n\nThis will run the command and create a new object, which is stored in the variable . The standard output and error of the command are captured using the method and stored in the variables output and errors, respectively.\n\nis useful when you want more control over the process, such as sending input to it, receiving output from it, or waiting for it to complete.\n\nis a function in the Python subprocess module that is used to run a command in a separate process and wait for it to complete. It returns the return code of the command, which is zero if the command was successful, and non-zero if it failed.\n\nThe function takes the same arguments as , including the args which specify the command to be run, and other optional arguments, such as , , , , , and .\n\nThe standard output and error of the command are sent to the same and as the parent process unless you redirect them using and arguments.\n\nThis will run the command in a separate process and wait for it to complete. The command's return code will be stored in the variable, which will be zero if the command was successful, and non-zero if it failed.\n\nis useful when you want to run a command and check the return code, but do not need to capture the output.\n\nis a function in the subprocess module that is similar to , but it only returns the standard output of the command, and raises a exception if the return code is non-zero.\n\nThe function takes the same arguments as , including the args which specify the command to be run, and other optional arguments, such as , , , , and .\n\nThe function returns the standard output of the command as a bytes object or string, if is passed.\n\nPython subprocess module provides a way to create and interact with child processes, which can be used to run other programs or commands. One of the features of the subprocess module is the ability to create pipes, which allow communication between the parent and child processes.\n\nA pipe is a unidirectional communication channel that connects one process's standard output to another's standard input. A pipe can connect the output of one command to the input of another, allowing the output of the first command to be used as input to the second command.\n\nPipes can be created using the subprocess module with the Popen class by specifying the stdout or stdin argument as .\n\nFor example, the following code creates a pipe that connects the output of the ls command to the input of the grep command, which filters the output to show only the lines that contain the word :\n\nIn this example, the class is used to create two child processes, one for the ls command and one for the grep command. The stdout of the ls command is connected to the stdin of the grep command using , which creates a pipe between the two processes. The method is used to send the output of the command to the command and retrieve the filtered output.\n\nThe Python module provides a powerful and flexible way to create and interact with child processes, allowing you to run other programs or issue commands from within your Python script. From simple commands like to more advanced features like pipes, redirecting input and output, and passing environment variables, the subprocess module has something to offer for almost every use case. It is a great way to automate repetitive tasks, run system commands, and even interact with other programming languages and platforms.\n\nWhile working with the subprocess module, it's important to remember that running external commands poses a security risk, especially when using the parameter or passing unsanitized input. It's always a good practice to use the function that allows you to specify various options for how the command should be run, such as whether to raise an exception if the command fails.\n\nIf you are interested in doing a deep dive into endless possibilities for command line automation through Python, check out our Command Line Automation in Python course. In this course, you will learn to write an automation code that will browse a filesystem, look for files that follow a pattern, and then determine whether files are duplicates in one of the numerous cases. After finishing the course, you'll be able to manage and interact with Unix processes as well as automate a variety of routine file system activities.\n\nBoost Your Team’s Python Skills with DataCamp for Business\n\nIf you or your team is looking to enhance your skills in Python and command line automation, consider exploring DataCamp for Business. DataCamp offers tailored learning solutions for teams of all sizes, helping businesses stay ahead in the rapidly evolving tech landscape. With DataCamp for Business, you can upskill your team with courses and custom learning tracks designed to build expertise in Python, automation, and other essential data science tools. Whether you're a startup or an enterprise, DataCamp for Business provides the resources and flexibility to achieve your team's learning goals. Request a demo today to learn more."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2024/01/python-subprocess-a-comprehensive-guide-to-executing-external-commands",
        "document": "Python subprocess is a powerful module that allows you to execute external commands from within your Python script. It provides a way to interact with the command line and run shell commands, system commands, external scripts, and more. In this comprehensive guide, we will explore the various features and functionalities of the subprocess module, along with best practices and tips for using it effectively.\n\nYou can import it into your Python script using the following line of code:\n\nNow that you have imported the subprocess module let’s dive into running basic commands with the subprocess.\n\nThe subprocess module provides several functions for executing external commands. One of the simplest ways to run a command is by using the `subprocess.run()` function. This function takes the command as a string and executes it. Here’s an example:\n\nIn the above example, we are running the `ls` command to list the files and directories in the current directory. The `shell=True` argument tells the subprocess to use the shell to execute the command. The `capture_output=True` argument captures the output of the command, and the `text=True` argument ensures that the output is returned as a string.\n\nApart from the `subprocess.run()` function, the subprocess module provides other functions for executing external commands. Let’s explore some of them.\n\nThe `subprocess.run()` function is versatile and allows you to run a command and capture its output. It returns a `CompletedProcess` object that contains information about the command’s execution. Here’s an example:\n\nIn the above example, we are running the `echo` command to print “Hello, World!”. The command is passed as a list of strings, where each element represents a part of the command. The output of the command is captured and printed using the `result.stdout` attribute.\n\nThe subprocess module provides various ways to capture the output of a command. In addition to the `subprocess.run()` function, you can use the `subprocess.check_output()` function to capture the output as a byte string, or the `subprocess.PIPE` constant to capture the output as a file-like object. Here’s an example:\n\nIn the above example, we are using the `subprocess.check_output()` function to capture the output of the `ls` command. The `shell=True` argument tells the subprocess to use the shell to execute the command. The output is returned as a byte string, which we decode and print using the `output.decode()` method.\n\nWhen executing external commands, you often need to pass command line arguments. The subprocess module allows you to pass arguments as a list of strings, where each element represents an argument. Here’s an example:\n\nIn the above example, we are passing the arguments “Hello,” and “World!” to the `echo` command. Each argument is represented as a separate element in the list.\n\nThe subprocess module also provides the `subprocess.Popen()` function, which allows you to run commands asynchronously. This means that you can start a command and continue with other tasks without waiting for the command to complete. Here’s an example:\n\nIn the above example, we are using the `subprocess.Popen()` function to start the `ping` command to google.com. The command is started in the background, and we can continue with other tasks while it is running.\n\nIn addition to the basic functionalities, the subprocess module provides advanced techniques for interacting with external commands. Let’s explore some of them.\n\nThe subprocess module allows you to redirect the input and output streams of a command. You can redirect the input from a file or a string, and redirect the output to a file or a variable. Here’s an example:\n\nIn the above example, we are redirecting the input of the `grep` command from a file named “input.txt”. We are also redirecting the output of the `ls` command to a file named “output.txt”.\n\nThe subprocess module allows you to set environment variables for the command’s execution. You can pass a dictionary of environment variables to the `subprocess.run()` function using the `env` argument. Here’s an example:\n\nIn the above example, we are setting the `PATH` environment variable to “/usr/local/bin” for the execution of the `echo` command. The `shell=True` argument tells the subprocess to use the shell to execute the command.\n\nWhen executing external commands, it is important to handle errors and exceptions. The subprocess module provides various ways to handle errors, such as checking the return code of the command, capturing the error output, and raising exceptions. Here’s an example:\n\nIn the above example, we are running an invalid command and using the `check=True` argument to raise an exception if the command fails. We catch the `subprocess.CalledProcessError` exception and print the return code and error output.\n\nThe subprocess module allows you to interact with the process of the executed command. You can send input to the process, receive output from the process, and even terminate the process if needed. Here’s an example:\n\nIn the above example, we are starting a Python interpreter as a subprocess and sending the command `print(‘Hello, World!’)` to it. We then read the output of the process and print it. Finally, we terminate the process using the `process.terminate()` method.\n\nThe subprocess module allows you to run commands in the background, without blocking the execution of your Python script. You can use the `subprocess.Popen()` function with the `subprocess.DEVNULL` constant to redirect the input and output streams to null. Here’s an example:\n\nIn the above example, we are running a Python script named “script.py” in the background. The input, output, and error streams of the script are redirected to null, so they do not interfere with the execution of the main script.\n\nThe subprocess module is widely used in various scenarios. Let’s explore some common use cases for using the Python subprocess.\n\nPython subprocess allows you to run shell commands from within your Python script. This can be useful for automating tasks, executing system commands, and interacting with the command line. Here’s an example:\n\nIn the above example; we are running the `ls` command to list the files and directories in the current directory.\n\nPython subprocess allows you to execute system commands, such as installing packages, updating software, and managing system resources. Here’s an example:\n\nIn the above example, we are using the `apt-get` command to install a package.\n\nThe Python subprocess allows you to run external scripts, such as shell scripts, Python scripts, and other executable scripts. Here’s an example:\n\nIn the above example, we are running a Python script named “script.py”.\n\nPython subprocess allows you to run commands with elevated privileges, such as running commands as an administrator or root user. Here’s an example:\n\nIn the above example, we are running a command with elevated privileges using the `sudo` command.\n\nPython subprocess allows you to run commands that require user input, such as interactive programs and command line prompts. Here’s an example:\n\nIn the above example, we are running the Python interpreter in interactive mode, which allows user input.\n\nBest Practices and Tips for Using Python subprocess\n\nWhen using Python subprocess, it is important to follow best practices and consider certain factors. Here are some tips for using Python subprocess effectively.\n\nWhen executing external commands, it is important to ensure command security and sanitization. Always validate and sanitize user input before passing it to subprocess functions to prevent command injection attacks.\n\nPython subprocess works differently on different operating systems. Take into account the differences in command syntax, environment variables, and system commands when writing cross-platform code.\n\nIn complex applications, managing subprocesses can be challenging. Consider using higher-level libraries and frameworks, such as Celery or asyncio, to manage subprocesses efficiently.\n\nWhen encountering issues with subprocesses, it is important to debug and troubleshoot them effectively. Use logging, error handling, and debugging techniques to identify and resolve any problems.\n\nPython subprocess is a powerful module that allows you to execute external commands from within your Python script. It provides various functionalities, including running commands, capturing output, handling errors, and interacting with the command’s process. By following best practices and considering various factors, you can use Python subprocess effectively in your projects.\n\nReady to supercharge your AI & ML journey? Join our Certified AI & ML BlackBelt Plus Program today! Gain a competitive edge with personalized learning paths, 1:1 mentorship, and hands-on guided projects. Power ahead in your AI & ML career! Enroll now for on-demand doubt-clearing sessions and embark on the path to becoming an AI & ML expert with Python. Don’t miss out – the future of tech awaits you!"
    },
    {
        "link": "https://realpython.com/python-subprocess",
        "document": "Python’s module allows you to run shell commands and manage external processes directly from your Python code. By using , you can execute shell commands like or , launch applications, and handle both input and output streams. This module provides tools for error handling and process communication, making it a flexible choice for integrating command-line operations into your Python projects.\n\nBy the end of this tutorial, you’ll understand that:\n• The Python module is used to run shell commands and manage external processes.\n• You run a shell command using by calling with the command as a list of arguments.\n• , , and differ in how they execute commands and handle process output and return codes.\n• is for parallel execution within Python, while manages external processes.\n• To execute multiple commands in sequence using , you can chain them by using pipes or running them consecutively.\n\nRead on to learn how to use Python’s module to automate shell tasks, manage processes, and integrate command-line operations into your applications.\n\nOnce you have the basics down, you’ll be exploring some practical ideas for how to leverage Python’s . You’ll also dip your toes into advanced usage of Python’s by experimenting with the underlying constructor.\n\nFirst off, you might be wondering why there’s a in the Python module name. And what exactly is a process, anyway? In this section, you’ll answer these questions. You’ll come away with a high-level mental model for thinking about processes. If you’re already familiar with processes, then you might want to skip directly to basic usage of the Python module. Whenever you use a computer, you’ll always be interacting with programs. A process is the operating system’s abstraction of a running program. So, using a computer always involve processes. Start menus, app bars, command-line interpreters, text editors, browsers, and more—every application comprises one or more processes. A typical operating system will report hundreds or even thousands of running processes, which you’ll get to explore shortly. However, central processing units (CPUs) typically only have a handful of cores, which means that they can only run a handful of instructions simultaneously. So, you may wonder how thousands of processes can appear to run at the same time. In short, the operating system is a marvelous multitasker—as it has to be. The CPU is the brain of a computer, but it operates at the nanosecond timescale. Most other components of a computer are far slower than the CPU. For instance, a magnetic hard disk read takes thousands of times longer than a typical CPU operation. If a process needs to write something to the hard drive, or wait for a response from a remote server, then the CPU would sit idle most of the time. Multitasking keeps the CPU busy. Part of what makes the operating system so great at multitasking is that it’s fantastically organized too. The operating system keeps track of processes in a process table or process control block. In this table, you’ll find the process’s file handles, security context, references to its address spaces, and more. The process table allows the operating system to abandon a particular process at will, because it has all the information it needs to come back and continue with the process at a later time. A process may be interrupted many thousands of times during execution, but the operating system always finds the exact point where it left off upon returning. An operating system doesn’t boot up with thousands of processes, though. Many of the processes you’re familiar with are started by you. In the next section, you’ll look into the lifetime of a process. Think of how you might start a Python application from the command line. This is an instance of your command-line process starting a Python process: The process that starts another process is referred to as the parent, and the new process is referred to as the child. The parent and child processes run mostly independently. Sometimes the child inherits specific resources or contexts from the parent. As you learned in Processes and the Operating System, information about processes is kept in a table. Each process keeps track of its parents, which allows the process hierarchy to be represented as a tree. You’ll be exploring your system’s process tree in the next section. Note: The precise mechanism for creating processes differs depending on the operating system. For a brief overview, the Wikipedia article on process management has a short section on process creation. For more details about the Windows mechanism, check out the win32 API documentation page on creating processes On UNIX-based systems, processes are typically created by using to copy the current process and then replacing the child process with one of the family of functions. The parent-child relationship between a process and its subprocess isn’t always the same. Sometimes the two processes will share specific resources, like inputs and outputs, but sometimes they won’t. Sometimes child processes live longer than the parent. A child outliving the parent can lead to orphaned or zombie processes, though more discussion about those is outside the scope of this tutorial. When a process has finished running, it’ll usually end. Every process, on exit, should return an integer. This integer is referred to as the return code or exit status. Zero is synonymous with success, while any other value is considered a failure. Different integers can be used to indicate the reason why a process has failed. In the same way that you can return a value from a function in Python, the operating system expects an integer return value from a process once it exits. This is why the canonical C function usually returns an integer: This example shows a minimal amount of C code necessary for the file to compile with without any warnings. It has a function that returns an integer. When this program runs, the operating system will interpret its execution as successful since it returns zero. So, what processes are running on your system right now? In the next section, you’ll explore some of the tools that you can use to take a peek at your system’s process tree. Being able to see what processes are running and how they’re structured will come in handy when visualizing how the module works. You may be curious to see what processes are running on your system right now. To do that, you can use platform-specific utilities to track them: There are many tools available for Windows, but one which is easy to get set up, is fast, and will show you the process tree without much effort is Process Hacker. You can install Process Hacker by going to the downloads page or with Chocolatey: Open the application, and you should immediately see the process tree. One of the native commands that you can use with PowerShell is , which lists the active processes on the command line. is a command prompt utility that does the same. The official Microsoft version of Process Hacker is part of the Sysinternals utilities, namely Process Monitor and Process Explorer. You also get PsList, which is a command-line utility similar to on UNIX. You can install Sysinternals by going to the downloads page or by using Chocolatey: You can also use the more basic, but classic, Task Manager—accessible by pressing + and selecting the Task Manager. For UNIX-based systems, there are many command-line utilities to choose from:\n• : The classic process and resource monitor, often installed by default. Once it’s running, to see the tree view, also called the forest view, press . The forest view may not work on the default macOS .\n• : More advanced and user-friendly version of .\n• : Another version of with more information, but more technical. On macOS, you also have the Activity Monitor application in your utilities. In the View menu, if you select All Processes, Hierarchically, you should be able to see your process tree. You can also explore the Python psutil library, which allows you to retrieve running process information on both Windows and UNIX-based systems. One universal attribute of process tracking across systems is that each process has a process identification number, or PID, which is a unique integer to identify the process within the context of the operating system. You’ll see this number on most of the utilities listed above. Along with the PID, it’s typical to see the resource usage, such as CPU percentage and amount of RAM that a particular process is using. This is the information that you look for if a program is hogging all your resources. The resource utilization of processes can be useful for developing or debugging scripts that use the module, even though you don’t need the PID, or any information about what resources processes are using in the code itself. While playing with the examples that are coming up, consider leaving a representation of the process tree open to see the new processes pop up. You now have a bird’s-eye view of processes. You’ll deepen your mental model throughout the tutorial, but now it’s time to see how to start your own processes with the Python module.\n\nIn this section, you’ll take a look at some of the most basic examples demonstrating the usage of the module. You’ll start by exploring a bare-bones command-line timer program with the function. If you want to follow along with the examples, then create a new folder. All the examples and programs can be saved in this folder. Navigate to this newly created folder on the command line in preparation for the examples coming up. All the code in this tutorial is standard library Python—with no external dependencies required—so a virtual environment isn’t necessary. To come to grips with the Python module, you’ll want a bare-bones program to run and experiment with. For this, you’ll use a program written in Python: The timer program uses to accept an integer as an argument. The integer represents the number of seconds that the timer should wait until exiting, which the program uses to achieve. It’ll play a small animation representing each passing second until it exits: It’s not much, but the key is that it serves as a cross-platform process that runs for a few seconds and which you can easily tinker with. You’ll be calling it with as if it were a separate executable. Note: Calling Python programs with the Python module doesn’t make much sense—there’s usually no need for other Python modules to be in separate processes since you can just import them. The main reason you’ll be using Python programs for most of the examples in this tutorial is that they’re cross-platform, and you most likely already have Python installed! You may be tempted to think that starting a new process could be a neat way to achieve concurrency, but that’s not the intended use case for the module. Maybe what you need are other Python modules dedicated to concurrency, covered in a later section. The module is mainly for calling programs other than Python. But, as you can see, you can call Python too if you want! For more discussion on the use cases of , check out the section where this is discussed in more depth, or one of the later examples. Okay, ready to get stuck in! Once you have the program ready, open a Python interactive session and call the timer with : With this code, you should’ve seen the animation playing right in the REPL. You imported and then called the function with a list of strings as the one and only argument. This is the parameter of the function. On executing , the timer process starts, and you can see its output in real time. Once it’s done, it returns an instance of the class. On the command line, you might be used to starting a program with a single string: However, with you need to pass the command as a sequence, as shown in the example. Each item in the sequence represents a token which is used for a system call to start a new process. Note: Calling isn’t the same as calling programs on the command line. The function makes a system call, foregoing the need for a shell. You’ll cover interaction with the shell in a later section. Shells typically do their own tokenization, which is why you just write the commands as one long string on the command line. With the Python module, though, you have to break up the command into tokens manually. For instance, executable names, flags, and arguments will each be one token. Note: You can use the module to help you out if you need, just bear in mind that it’s designed for POSIX compliant systems and may not work well in Windows environments: The function divides a typical command into the different tokens needed. The module can come in handy when it may be not obvious how to divide up more complex commands that have special characters, like spaces: You’ll note that the message, which contains spaces, is preserved as a single token, and the extra quotation marks are no longer needed. The extra quotation marks on the shell serve to group the token together, but since uses sequences, it’s always unambiguous which parts should be interpreted as one token. Now that you’re familiar with some of the very basics of starting new processes with the Python module, coming up you’ll see that you can run any kind of process, not just Python or text-based programs. The Use of to Run Any App With , you aren’t limited to text-based applications like the shell. You can call any application that you can with the Start menu or app bar, as long as you know the precise name or path of the program that you want to run: Depending on your Linux distribution, you may have a different text editor, such as , , , or . These commands should open up a text editor window. Usually won’t get returned until you close the editor window. Yet in the case of macOS, since you need to run the launcher process to launch TextEdit, the gets returned straight away. Launcher processes are in charge of launching a specific process and then ending. Sometimes programs, such as web browsers, have them built in. The mechanics of launcher processes is out of the scope of this tutorial, but suffice to say that they’re able to manipulate the operating system’s process tree to reassign parent-child relationships. Note: There are many problems that you might initially reach for to solve, but then you’ll find a specific module or library that solves it for you. This tends to be a theme with since it is quite a low-level utility. An example of something that you might want to do with is to open a web browser to a specific page. However, for that, it’s probably best to use the Python module . The module uses under the hood but handles all the finicky cross-platform and browser differences that you might encounter. Then again, can be a remarkably useful tool to get something done quickly. If you don’t need a full-fledged library, then can be your Swiss Army knife. It all depends on your use case. More discussion on this topic will come later. You’ve successfully started new processes using Python! That’s at its most basic. Next up, you’ll take a closer look at the object that’s returned from . When you use , the return value is an instance of the class. As the name suggests, returns the object only once the child process has ended. It has various attributes that can be helpful, such as the that were used for the process and the . To see this clearly, you can assign the result of to a variable, and then access its attributes such as : timer.py: error: the following arguments are required: time The process has a return code that indicates failure, but it doesn’t raise an exception. Typically, when a process fails, you’ll always want an exception to be raised, which you can do by passing in a argument: timer.py: error: the following arguments are required: time : There are various ways to deal with failures, some of which will be covered in the next section. The important point to note for now is that won’t necessarily raise an exception if the process fails unless you’ve passed in a argument. The also has a few attributes relating to input/output (I/O), which you’ll cover in more detail in the communicating with processes section. Before communicating with processes, though, you’ll learn how to handle errors when coding with .\n\nIntroduction to the Shell and Text-Based Programs With Some of the most popular use cases of the module are to interact with text-based programs, typically available on the shell. That’s why in this section, you’ll start to explore all the moving parts involved when interacting with text-based programs, and perhaps question if you need the shell at all! The shell is typically synonymous with the command-line interface or CLI, but this terminology isn’t entirely accurate. There are actually two separate processes that make up the typical command-line experience:\n• The interpreter, which is typically thought of as the whole CLI. Common interpreters are Bash on Linux, Zsh on macOS, or PowerShell on Windows. In this tutorial, the interpreter will be referred to as the shell.\n• The interface, which displays the output of the interpreter in a window and sends user keystrokes to the interpreter. The interface is a separate process from the shell, sometimes called a terminal emulator. When on the command line, it’s common to think that you’re interacting directly with the shell, but you’re really interacting with the interface. The interface takes care of sending your commands to the shell and displaying the shell’s output back to you. With this important distinction in mind, it’s time to turn your attention to what is actually doing. It’s common to think that calling is somehow the same as typing a command in a terminal interface, but there are important differences. While all new process are created with the same system calls, the context from which the system call is made is different. The function can make a system call directly and doesn’t need to go through the shell to do so: In fact, many programs that are thought of as shell programs, such as Git, are really just text-based programs that don’t need a shell to run. This is especially true of UNIX environments, where all of the familiar utilities like , , , and are actually separate executables that can be called directly: There are some tools that are specific to shells, though. Finding tools embedded within the shell is far more common on Windows shells like PowerShell, where commands like are part of the shell itself and not separate executables like they are in a UNIX environment: : [WinError 2] The system cannot find the file specified In PowerShell, is the default alias for , but calling that won’t work either because isn’t a separate executable—it’s part of PowerShell itself. The fact that many text-based programs can operate independently from the shell may make you wonder if you can cut out the middle process—namely, the shell—and use directly with the text-based programs typically associated with the shell. Use Cases for the Shell and There are a few common reasons why you might want to call the shell with the subprocess module:\n• When you know certain commands are only available via the shell, which is more common in Windows\n• When you’re experienced in writing shell scripts with a particular shell, so you want to leverage your ability there to do certain tasks while still working primarily in Python\n• When you’ve inherited a large shell script that might do nothing that Python couldn’t do, but would take a long time to reimplement in Python You might use the shell to wrap programs or to do some text processing. However, the syntax can be very cryptic when compared to Python. With Python, text processing workflows are easier to write, easier to maintain, generally more performant, and cross-platform to boot. So it’s well worth considering going without the shell. What often happens, though, is that you just don’t have the time or it’s not worth the effort to reimplement existing shell scripts in Python. In those cases, using for some sloppy Python isn’t a bad thing! Common reasons for using itself are similar in nature to using the shell with :\n• When you have to use or analyze a black box, or even a white box\n• When you want a wrapper for an application\n• When you need to launch another application\n• As an alternative to basic shell scripts Note: A black box could be a program that can be freely used but whose source code isn’t available, so there’s no way to know exactly what it does and no way to modify its internals. Similarly, a white box could be a program whose source code is available but can’t be changed. It could also be a program whose source code you could change, but its complexity means that it would take you a long time to get your head around it to be able to change it. In these cases, you can use to wrap your boxes of varying opacity, bypassing any need to change or reimplement things in Python. Often you’ll find that for use cases, there will be a dedicated library for that task. Later in the tutorial, you’ll examine a script that creates a Python project, complete with a virtual environment and a fully initialized Git repository. However, the Cookiecutter and Copier libraries already exist for that purpose. Even though specific libraries might be able to do your task, it may still be worth doing things with . For one, it might be much faster for you to execute what you already know how to do, rather than learning a new library. Additionally, if you’re sharing this script with friends or colleagues, it’s convenient if your script is pure Python without any other dependencies, especially if your script needs to go on minimal environments like servers or embedded systems. However, if you’re using instead of to read and write a few files with Bash, you might want to consider learning how to read and write with Python. Learning how to read and write files doesn’t take long, and it’ll definitely be worth it for such a common task. With that out of the way, it’s time to get familiar with the shell environments on both Windows and UNIX-based systems. To run a shell command using , the should contain the shell that you want to use, the flag to indicate that you want it to run a specific command, and the command that you’re passing in: Here a common shell command is demonstrated. It uses piped into to filter some of the entries. The shell is handy for this kind of operation because you can take advantage of the pipe operator ( ). You’ll cover pipes in more detail later. You can replace with the shell of your choice. The flag stands for command, but may be different depending on the shell that you’re using. This is almost the exact equivalent of what happens when you add the argument: The argument uses behind the scenes, so it’s almost the equivalent of the previous example. Note: On UNIX-based systems, the shell was traditionally the Bourne shell. That said, the Bourne shell is now quite old, so many operating systems use as a link to Bash or Dash. This can often be different from the shell used with the terminal interface that you interact with. For instance, since macOS Catalina, the default shell that you’ll find on the command-line app has changed from Bash to Zsh, yet often still points to Bash. Likewise, on Ubuntu, points to Dash, but the default that you typically interact with on the command-line application is still Bash. So, calling on your system may result in a different shell than what is found in this tutorial. Nevertheless, the examples should all still work. You’ll note that the token after should be one single token, with all the spaces included. Here you’re giving control to the shell to parse the command. If you were to include more tokens, this would be interpreted as more options to pass to the shell executable, not as additional commands to run inside the shell. In this section, you’ll cover basic use of the shell with in a Windows environment. To run a shell command using , the should contain the shell that you want to use, the flag to indicate that you want it to run a specific command, and the command that you’re passing in: Note that and both work. If you don’t have PowerShell Core, then you can call or . You’ll note that the token after should be one single token, with all the spaces included. Here you’re giving control to the shell to parse the command. If you were to include more tokens, this would be interpreted as more options to pass to the shell executable, not as additional commands to run inside the shell. If you need the Command Prompt, then the executable is or , and the flag to indicate that the following token is a command is : Volume in drive C has no label. This last example is the exact equivalent of calling with . Said in another way, using the argument is like prepending and to your argument list. Note: Windows’ evolution has been very different from that of UNIX-based systems. The most widely known shell is the Windows Command Prompt which is by now a legacy shell. The Command Prompt was made to emulate the pre-Windows MS-DOS environment. Many shell scripts, or batch scripts, were written for this environment which are still in use today. The function with the parameter will almost always end up using the Command Prompt. The module uses the Windows environment variable, which in almost all cases will point to , the Command Prompt. By now, there are so many programs that equate to that changing it would cause much breakage in unexpected places! So, changing is generally not advised. At this point, you should know about an important security concern that you’ll want to be aware of if you have user-facing elements in your Python program, regardless of the operating system. It’s a vulnerability that’s not confined to . Rather, it can be exploited in many different areas. If at any point you plan to get user input and somehow translate that to a call to , then you have to be very careful of injection attacks. That is, take into account potential malicious actors. There are many ways to cause havoc if you just let people run code on your machine. To use a very simplistic example, where you take user input and send it, unfiltered, to subprocess to run on the shell: You can imagine the intended use case is to wrap and add something to it. So the expected user behavior is to provide a path like . However, if a malicious actor realized what was happening, they could execute almost any code they wanted. Take the following, for instance, but be careful with this: C:\\RealPython; echo 'You could've been hacked: rm -Recurse -Force C:\\' Again, beware! These innocent-looking lines could try and delete everything on the system! In this case the malicious part is in quotes, so it won’t run, but if the quotes were not there, you’d be in trouble. The key part that does this is the call to with the relevant flags to recursively delete all files, folders, and subfolders, and it’ll work to force the deletion through. It can run the and potentially the as entirely separate commands by adding semicolons, which act as command separators allowing what would usually be multiple lines of code to run on one line. Running these malicious commands would cause irreparable damage to the file system, and would require reinstalling the operating system. So, beware! Luckily, the operating system wouldn’t let you do this to some particularly important files. The command would need to use in UNIX-based systems, or be run as an administrator in Windows to be completely successful in its mayhem. The command would probably delete a lot of important stuff before stopping, though. So, make sure that if you’re dynamically building user inputs to feed into a call, then you’re very careful! With that warning, coming up you’ll be covering using the outputs of commands and chaining commands together—in short, how to communicate with processes once they’ve started.\n\nYou’ve used the module to execute programs and send basic commands to the shell. But something important is still missing. For many tasks that you might want to use for, you might want to dynamically send inputs or use the outputs in your Python code later. To communicate with your process, you first should understand a little bit about how processes communicate in general, and then you’ll take a look at two examples to come to grips with the concepts. A stream at its most basic represents a sequence of elements that aren’t available all at once. When you read characters and lines from a file, you’re working with a stream in the form of a file object, which at its most basic is a file descriptor. File descriptors are often used for streams. So, it’s not uncommon to see the terms stream, file, and file-like used interchangeably. When processes are initialized, there are three special streams that a process makes use of. A process does the following: These are the standard streams—a cross-platform pattern for process communication. Sometimes the child process inherits these streams from the parent. This is what’s happening when you use in the REPL and are able to see the output of the command. The of the Python interpreter is inherited by the subprocess. When you’re in a REPL environment, you’re looking at a command-line interface process, complete with the three standard I/O streams. The interface has a shell process as a child process, which itself has a Python REPL as a child. In this situation, unless you specify otherwise, comes from the keyboard, while and are displayed on-screen. The interface, the shell, and the REPL share the streams: You can think of the standard I/O streams as byte dispensers. The subprocess fills up and , and you fill up . Then you read the bytes in and , and the subprocess reads from . As with a dispenser, you can stock before it gets linked up to a child process. The child process will then read from as and when it needs to. Once a process has read from a stream, though, the bytes are dispensed. You can’t go back and read them again: These three streams, or files, are the basis for communicating with your process. In the next section, you’ll start to see this in action by getting the output of a magic number generator program. Often, when using the module, you’ll want to use the output for something and not just display the output as you have been doing so far. In this section, you’ll use a magic number generator that outputs, well, a magic number. Imagine that the magic number generator is some obscure program, a black box, inherited across generations of sysadmins at your job. It outputs a magic number that you need for your secret calculations. You’ll read from the of and use it in your wrapper Python program: Okay, not really so magical. That said, it’s not the magic number generator that you’re interested in—it’s interacting with a hypothetical black box with that’s interesting. To grab the number generator’s output to use later, you can pass in a argument to : Passing a argument of to makes the output of the process available at the attribute of the completed process object. You’ll note that it’s returned as a bytes object, so you need to be mindful of encodings when reading it. Also note that the attribute of the is no longer a stream. The stream has been read, and it’s stored as a bytes object in the attribute. With the output available, you can use more than one subprocess to grab values and operate on them in your code: In this example, you start two magic number processes that fetch two magic numbers and then add them together. For now, you rely on the automatic decoding of the bytes object by the constructor. In the next section, though, you’ll learn how to decode and encode explicitly. Processes communicate in bytes, and you have a few different ways to deal with encoding and decoding these bytes. Beneath the surface, has a few ways of getting into text mode. Text mode means that will try to take care of encoding itself. To do that, it needs to know what character encoding to use. Most of the options for doing this in will try to use the default encoding. However, you generally want to be explicit about what encoding to use to prevent a bug that would be hard to find in the future. You can pass a argument for Python to take care of encodings using the default encoding. But, as mentioned, it’s always safer to specify the encodings explicitly using the argument, as not all systems work with the nearly universal UTF-8: If in text mode, the attribute on a is now a string and not a bytes object. You can also decode the bytes returned by calling the method on the attribute directly, without requiring text mode at all: There are other ways to put into text mode. You can also set a value for or , which will also put into text mode. This may seem redundant, but much of this is kept for backwards compatibility, seeing as the module has changed over the years. Now that you know how to read and decode the output of a process, it’s time to take a look at writing to the input of a process. In this section, you’ll use to interact with a command-line game. It’s a basic program that’s designed to test a human’s reaction time. With your knowledge of standard I/O streams, though, you’ll be able to hack it! The source code of the game makes use of the and module: The program starts, asks for the user to press enter, and then after a random amount of time will ask the user to press enter again. It measures from the time the message appears to the time the user presses enter, or at least that’s what the game developer thinks: The function will read from until it reaches a newline, which means an keystroke in this context. It returns everything it consumed from except the newline. With that knowledge, you can use to interact with this game: A reaction time of 0 milliseconds! Not bad! Considering the average human reaction time is around 270 milliseconds, your program is definitely superhuman. Note that the game rounds its output, so 0 milliseconds doesn’t mean it’s instantaneous. The argument passed to is a string consisting of two newlines. The parameter is set to , which puts into text mode. This sets up the process for it to receive the input you that give it. Before the program starts, is stocked, waiting for the program to consume the newlines it contains. One newline is consumed to start the game, and the next newline is consumed to react to . Now that you know what’s happening—namely that can be stocked, as it were—you can hack the program yourself without . If you start the game and then press a few times, that’ll stock up with a few newlines that the program will automatically consume once it gets to the line. So your reaction time is really only the time it takes for the reaction game to execute and consume an input: The game developer gets wise to this, though, and vows to release another version, which will guard against this exploit. In the meantime, you’ll peek a bit further under the hood of and learn about how it wires up the standard I/O streams.\n\nTo really understand subprocesses and the redirection of streams, you really need to understand pipes and what they are. This is especially true if you want to wire up two processes together, feeding one into another process’s , for instance. In this section, you’ll be coming to grips with pipes and how to use them with the module. A pipe, or pipeline, is a special stream that, instead of having one file handle as most files do, has two. One handle is read-only, and the other is write-only. The name is very descriptive—a pipe serves to pipe a byte stream from one process to another. It’s also buffered, so a process can write to it, and it’ll hold onto those bytes until it’s read, like a dispenser. You may be used to seeing pipes on the command line, as you did in the section on shells: This command tells the shell to create an process to list all the files in . The pipe operator ( ) tells the shell to create a pipe from the of the process and feed it into the of the process. The process filters out all the lines that don’t contain the string . Windows doesn’t have , but a rough equivalent of the same command would be as follows: However, on Windows PowerShell, things work very differently. As you learned in the Windows shell section of this tutorial, the different commands are not separate executables. Therefore, PowerShell is internally redirecting the output of one command into another without starting new processes. Note: If you don’t have access to a UNIX-based operating system but have Windows 10 or above, then you actually do have access to a UNIX-based operating system! Check out Windows Subsystem for Linux, which will give you access to a fully featured Linux shell. You can use pipes for different processes on PowerShell, though getting into the intricacies of which ones is outside the scope of this tutorial. For more information on PowerShell pipes, check out the documentation. So, for the rest of the pipe examples, only UNIX-based examples will be used, as the basic mechanism is the same for both systems. They’re not nearly as common on Windows, anyway. If you want to let the shell take care of piping processes into one another, then you can just pass the whole string as a command into : This way, you can let your chosen shell take care of piping one process into another, instead of trying to reimplement things in Python. This is a perfectly valid choice in certain situations. Later in the tutorial, you’ll also come to see that you can’t pipe processes directly with . For that, you’ll need the more complicated . Actual piping is demonstrated in Connecting Two Porcesses Together With Pipes, near the end of the tutorial. Whether you mean to pipe one process into another with the module or not, the module makes extensive use of pipes behind the scenes. The Python module uses pipes extensively to interact with the processes that it starts. In a previous example, you used the parameter to be able to access : is equivalent to explicitly setting the and parameters to the constant: The constant is nothing special. It’s just a number that indicates to that a pipe should be created. The function then creates a pipe to link up to the of the subprocess, which the function then reads into the object’s attribute. By the time it’s a , it’s no longer a pipe, but a bytes object that can be accessed multiple times. Note: Pipe buffers have a limited capacity. Depending on the system you are running on, you may easily run into that limit if you plan on holding large quantities of data in the buffer. To work around this limit, you can use normal files. You can also pass a file object to any of the standard stream parameters: You can’t pass a bytes object or a string directly to the argument, though. It needs to be something file-like. Note that the that gets returned first is from the call to which returns the new stream position, which in this case is the start of the stream. The parameter is similar to the parameter in that it’s a shortcut. Using the parameter will create a buffer to store the contents of , and then link the file up to the new process to serve as its . To actually link up two processes with a pipe from within is something that you can’t do with . Instead, you can delegate the plumbing to the shell, as you did earlier in the Introduction to the Shell and Text Based Programs with section. If you needed to link up different processes without delegating any of the work to the shell, then you could do that with the underlying constructor. You’ll cover in a later section. In the next section, though, you’ll be simulating a pipe with because in most cases, it’s not vital for processes to be linked up directly. Though you can’t actually link up two processes together with a pipe by using the function, at least not without delegating it to the shell, you can simulate piping by judicious use of the attribute. If you’re on a UNIX-based system where almost all typical shell commands are separate executables, then you can just set the of the second process to the attribute of the first : Here the attribute of the object of is set to the of the . It’s important that it’s set to rather than . This is because the attribute isn’t a file-like object. It’s a bytes object, so it can’t be used as an argument to . As an alternative, you can operate directly with files too, setting them to the standard stream parameters. When using files, you set the file object as the argument to , instead of using the parameter: As you learned in the previous section, for Windows PowerShell, doing something like this doesn’t make a whole lot of sense because most of the time, these utilities are part of PowerShell itself. Because you aren’t dealing with separate executables, piping becomes less of a necessity. However, the pattern for piping is still the same if something like this needs to be done. With most of the tools out the way, it’s now time to think about some practical applications for .\n\nWhen you have an issue that you want to solve with Python, sometimes the module is the easiest way to go, even though it may not be the most correct. Using is often tricky to get working across different platforms, and it has inherent dangers. But even though it may involve some sloppy Python, using can be a very quick and efficient way to solve a problem. As mentioned, for most tasks you can imagine doing with , there’s usually a library out there that’s dedicated to that specific task. The library will almost certainly use , and the developers will have worked hard to make the code reliable and to cover all the corner cases that can make using difficult. So, even though dedicated libraries exist, it can often be simpler to just use , especially if you’re in an environment where you need to limit your dependencies. In the following sections, you’ll be exploring a couple of practical ideas. Creating a New Project: An Example Say you often need to create new local projects, each complete with a virtual environment and initialized as a Git repository. You could reach for the Cookiecutter library, which is dedicated to that task, and that wouldn’t be a bad idea. However, using Cookiecutter would mean learning Cookiecutter. Imagine you didn’t have much time, and your environment was extremely minimal anyway—all you could really count on was Git and Python. In these cases, can quickly set up your project for you: This is a command-line tool that you can call to start a project. It’ll take care of creating a file and a file, and then it’ll run a few commands to create a virtual environment, initialize a git repository, and perform your first commit. It’s even cross-platform, opting to use to create the files and folders, which abstracts away the operating system differences. Could this be done with Cookiecutter? Could you use GitPython for the part? Could you use the module to create the virtual environment? Yes to all. But if you just need something quick and dirty, using commands you already know, then just using can be a great option. If you use Dropbox, you may not know that there’s a way to ignore files when syncing. For example, you can keep virtual environments in your project folder and use Dropbox to sync the code, but keep the virtual environment local. That said, it’s not as easy as adding a file. Rather, it involves adding special attributes to files, which can be done from the command line. These attributes are different between UNIX-like systems and Windows: There are some UNIX-based projects, like dropboxignore, that use shell scripts to make it easier to ignore files and folders. The code is relatively complex, and it won’t work on Windows. With the module, you can wrap the different shell commands quite easily to come up with your own utility: This is a simplified snippet from the author’s dotDropboxIgnore repository. The function detects the operating system with the module and returns an object that’s an abstraction around the system-specific shell. The code hasn’t implemented the behavior on macOS, so it raises a if it detects it’s running on macOS. The shell object allows you to call an method with a list of objects to set Dropbox to ignore those files. On the class, the constructor tests to see if PowerShell Core is available, and if not, will fall back to the older Windows PowerShell, which is installed by default on Windows 10. In the next section, you’ll review some of the other modules that might be interesting to keep in mind when deciding whether to use .\n\nAs mentioned, the underlying class for the whole module is the class and the constructor. Each function in calls the constructor under the hood. Using the constructor gives you lots of control over the newly started subprocesses. As a quick summary, is basically the class constructor, some setup, and then a call to the method on the newly initialized object. The method is a blocking method that returns the and data once the process has ended. The name of comes from a similar UNIX command that stands for pipe open. The command creates a pipe and then starts a new process that invokes the shell. The module, though, doesn’t automatically invoke the shell. The function is a blocking function, which means that interacting dynamically with a process isn’t possible with it. However, the constructor starts a new process and continues, leaving the process running in parallel. The developer of the reaction game that you were hacking earlier has released a new version of their game, one in which you can’t cheat by loading with newlines: \"A letter will appear on screen after a random amount of time, \"when it appears, type the letter as fast as possible \" \"Press enter when you are ready\" Now the program will display a random character, and you need to press that exact character to have the game register your reaction time: What’s to be done? First, you’ll need to come to grips with using with basic commands, and then you’ll find another way to exploit the reaction game. Using the constructor is very similar in appearance to using . If there’s an argument that you can pass to , then you’ll generally be able to pass it to . The fundamental difference is that it’s not a blocking call—rather than waiting until the process is finished, it’ll run the process in parallel. So you need to take this non-blocking nature into account if you want to read the new process’s output: This program calls the timer process in a context manager and assigns to a pipe. Then it runs the method on the object and reads its . The method is a basic method to check if a process is still running. If it is, then returns . Otherwise, it’ll return the process’s exit code. Then the program uses to try and read as many bytes as are available at . Note: If you put the object into text mode and then called on , the call to would be blocking until it reached a newline. In this case, a newline would coincide with the end of the timer program. This behavior isn’t desired in this situation. To read as many bytes as are available at that time, disregarding newlines, you need to read with . It’s important to note that is only available on byte streams, so you need to make sure to deal with encodings manually and not use text mode. The output of this program first prints because the process hasn’t yet finished. The program then prints what is available in so far, which is the starting message and the first character of the animation. After three seconds, the timer hasn’t finished, so you get again, along with two more characters of the animation. After another three seconds, the process has ended, so produces , and you get the final characters of the animation and : Output from poll: None Output from stdout: Starting timer of 5 seconds . Output from poll: None Output from stdout: .. Output from poll: 0 Output from stdout: ..Done! In this example, you’ve seen how the constructor works very differently from . In most cases, you don’t need this kind of fine-grained control. That said, in the next sections, you’ll see how you can pipe one process into another, and how you can hack the new reaction game. Connecting Two Processes Together With Pipes As mentioned in a previous section, if you need to connect processes together with pipes, you need to use the constructor. This is mainly because is a blocking call, so by the time the next process starts, the first one has ended, meaning that you can’t directly link up to its . This procedure will only be demonstrated for UNIX systems, because piping in Windows is far less common, as mentioned in the simulating a pipe section: In this example, the two processes are started in parallel. They are joined with a common pipe, and the loop takes care of reading the pipe at to output the lines. A key point to note is that in contrast to , which returns a object, the constructor returns a object. The standard stream attributes of a point to bytes objects or strings, but the same attributes of a object point to the actual streams. This allows you to communicate with processes as they’re running. Whether you really need to pipe processes into one another, though, is another matter. Ask yourself if there’s much to be lost by mediating the process with Python and using exclusively. There are some situations in which you really need , though, such as hacking the new version of the reaction time game. Now that you know you can use to interact with a process dynamically as it runs, it’s time to turn that knowledge toward exploiting the reaction time game again: With this script, you’re taking complete control of the buffering of a process, which is why you pass in arguments such as to the Python process and to . These arguments are to ensure that no extra buffering is taking place. The script works by using a function that’ll search for one of a list of strings by grabbing one character at a time from the process’s . As each character comes through, the script will search for the string. Note: To make this work on both Windows and UNIX-based systems, two strings are searched for: either or . The Windows-style carriage return along with the typical newline is required on Windows systems. After the script has found one of the target strings, which in this case is the sequence of characters before the target letter, it’ll then grab the next character and write that letter to the process’s followed by a newline: At one millisecond, it’s not quite as good as the original hack, but it’s still very much superhuman. Well done! With all this fun aside, interacting with processes using can be very tricky and is prone to errors. First, see if you can use exclusively before resorting to the constructor. If you really need to interact with processes at this level, the module has a high-level API to create and manage subprocesses. The subprocess functionality is intended for more complex uses of where you may need to orchestrate various processes. This might be the case if you’re performing complex processing of many image, video, or audio files, for example. If you’re using at this level, then you’re probably building a library."
    },
    {
        "link": "https://trac.ffmpeg.org/wiki/Encode/MP3",
        "document": "This page describes how to use the external libmp3lame encoding library within to create MP3 audio files ( has no native MP3 encoder). See also other codecs you could use, and FFmpeg AAC Encoding Guide if you want AAC instead, and the ​official documentation.\n\nExample to encode VBR MP3 audio with using the libmp3lame library:\n\nControl quality with (or the alias ). Values are encoder specific, so for libmp3lame the range is 0-9 where a lower value is a higher quality. 0-3 will normally produce transparent results, 4 (default) should be close to perceptual transparency, and 6 produces an \"acceptable\" quality. The option is mapped to the option in the standalone command-line interface tool.\n\nIn our example above, we selected , meaning we used LAME's option , which gives us a VBR MP3 audio stream with an average stereo bitrate of 170-210 kBit/s.\n\nIf you need constant bitrate (CBR) MP3 audio, you need to use the option instead of . Here you can specify the number of bits per second, for example if you want 256 Kbit/s (25.6 KB/s) audio. Available options are: 8, 16, 24, 32, 40, 48, 64, 80, 96, 112, 128, 160, 192, 224, 256, or 320 (add a after each to get that rate). So to get the highest quality setting use (but see note below).\n\nABR is something of a mixture between VBR and CBR, see the ​official documentation for details on use.\n\nSometimes the output will consist of fewer bits per second than requested:\n• With CBR it could be due to the chosen value being in-between allowable settings (it defaults down to the next lower acceptable bitrate).\n• With VBR it could be due to the input already in being a lower bitrate than requested, in which case, it basically just re-encodes it at the bitrate of the input.\n• Or possibly that the input was CBR, and the VBR aspect is able to reduce bitrate by a considerable amount."
    },
    {
        "link": "https://ffmpeg.org/ffmpeg.html",
        "document": "is a universal media converter. It can read a wide variety of inputs - including live grabbing/recording devices - filter, and transcode them into a plethora of output formats.\n\nreads from an arbitrary number of inputs (which can be regular files, pipes, network streams, grabbing devices, etc.), specified by the option, and writes to an arbitrary number of outputs, which are specified by a plain output url. Anything found on the command line which cannot be interpreted as an option is considered to be an output url.\n\nEach input or output can, in principle, contain any number of elementary streams of different types (video/audio/subtitle/attachment/data), though the allowed stream counts and/or types may be limited by the container format. Selecting which streams from which inputs will go into which output is either done automatically or with the option (see the Stream selection chapter).\n\nTo refer to inputs/outputs in options, you must use their indices (0-based). E.g. the first input is , the second is , etc. Similarly, streams within an input/output are referred to by their indices. E.g. refers to the fourth stream in the third input or output. Also see the Stream specifiers chapter.\n\nAs a general rule, options are applied to the next specified file. Therefore, order is important, and you can have the same option on the command line multiple times. Each occurrence is then applied to the next input or output file. Exceptions from this rule are the global options (e.g. verbosity level), which should be specified first.\n\nDo not mix input and output files – first specify all input files, then all output files. Also do not mix options which belong to different files. All options apply ONLY to the next input or output file and are reset between files.\n• Convert an input media file to a different format, by re-encoding media streams:\n• Set the video bitrate of the output file to 64 kbit/s:\n• Force the frame rate of the output file to 24 fps:\n• Force the frame rate of the input file (valid for raw formats only) to 1 fps and the frame rate of the output file to 24 fps:\n\nThe format option may be needed for raw input files.\n\nbuilds a transcoding pipeline out of the components listed below. The program’s operation then consists of input data chunks flowing from the sources down the pipes towards the sinks, while being transformed by the components they encounter along the way.\n\nThe following kinds of components are available:\n• Demuxers (short for \"demultiplexers\") read an input source in order to extract\n• global properties such as metadata or chapters;\n• list of input elementary streams and their properties One demuxer instance is created for each option, and sends encoded packets to decoders or muxers. In other literature, demuxers are sometimes called splitters, because their main function is splitting a file into elementary streams (though some files only contain one elementary stream). A schematic representation of a demuxer looks like this: ┌──────────┬───────────────────────┐ │ demuxer │ │ packets for stream 0 ╞══════════╡ elementary stream 0 ├──────────────────────► │ │ │ │ global ├───────────────────────┤ │properties│ │ packets for stream 1 │ and │ elementary stream 1 ├──────────────────────► │ metadata │ │ │ ├───────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├───────────────────────┤ │ │ │ packets for stream N │ │ elementary stream N ├──────────────────────► │ │ │ └──────────┴───────────────────────┘ ▲ │ │ read from file, network stream, │ grabbing device, etc. │\n• Decoders receive encoded (compressed) packets for an audio, video, or subtitle elementary stream, and decode them into raw frames (arrays of pixels for video, PCM for audio). A decoder is typically associated with (and receives its input from) an elementary stream in a demuxer, but sometimes may also exist on its own (see Loopback decoders). A schematic representation of a decoder looks like this:\n• Filtergraphs process and transform raw audio or video frames. A filtergraph consists of one or more individual filters linked into a graph. Filtergraphs come in two flavors - simple and complex, configured with the and options, respectively. A simple filtergraph is associated with an output elementary stream; it receives the input to be filtered from a decoder and sends filtered output to that output stream’s encoder. A simple video filtergraph that performs deinterlacing (using the deinterlacer) followed by resizing (using the filter) can look like this: ┌────────────────────────┐ │ simple filtergraph │ frames from ╞════════════════════════╡ frames for a decoder │ ┌───────┐ ┌───────┐ │ an encoder ────────────►├─►│ yadif ├─►│ scale ├─►│────────────► │ └───────┘ └───────┘ │ └────────────────────────┘ A complex filtergraph is standalone and not associated with any specific stream. It may have multiple (or zero) inputs, potentially of different types (audio or video), each of which receiving data either from a decoder or another complex filtergraph’s output. It also has one or more outputs that feed either an encoder or another complex filtergraph’s input. The following example diagram represents a complex filtergraph with 3 inputs and 2 outputs (all video): Frames from second input are overlaid over those from the first. Frames from the third input are rescaled, then the duplicated into two identical streams. One of them is overlaid over the combined first two inputs, with the result exposed as the filtergraph’s first output. The other duplicate ends up being the filtergraph’s second output.\n• Encoders receive raw audio, video, or subtitle frames and encode them into encoded packets. The encoding (compression) process is typically lossy - it degrades stream quality to make the output smaller; some encoders are lossless, but at the cost of much higher output size. A video or audio encoder receives its input from some filtergraph’s output, subtitle encoders receive input from a decoder (since subtitle filtering is not supported yet). Every encoder is associated with some muxer’s output elementary stream and sends its output to that muxer. A schematic representation of an encoder looks like this:\n• Muxers (short for \"multiplexers\") receive encoded packets for their elementary streams from encoders (the transcoding path) or directly from demuxers (the streamcopy path), interleave them (when there is more than one elementary stream), and write the resulting bytes into the output file (or pipe, network stream, etc.). A schematic representation of a muxer looks like this: ┌──────────────────────┬───────────┐ packets for stream 0 │ │ muxer │ ──────────────────────►│ elementary stream 0 ╞═══════════╡ │ │ │ ├──────────────────────┤ global │ packets for stream 1 │ │properties │ ──────────────────────►│ elementary stream 1 │ and │ │ │ metadata │ ├──────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├──────────────────────┤ │ packets for stream N │ │ │ ──────────────────────►│ elementary stream N │ │ │ │ │ └──────────────────────┴─────┬─────┘ │ write to file, network stream, │ grabbing device, etc. │ │ ▼\n\nThe simplest pipeline in is single-stream streamcopy, that is copying one input elementary stream’s packets without decoding, filtering, or encoding them. As an example, consider an input file called with 3 elementary streams, from which we take the second and write it to file . A schematic representation of such a pipeline looks like this:\n\nThe above pipeline can be constructed with the following commandline:\n• there are no input options for this input;\n• there are two output options for this output:\n• selects the input stream to be used - from input with index 0 (i.e. the first one) the stream with index 1 (i.e. the second one);\n• selects the encoder, i.e. streamcopy with no decoding or encoding.\n\nStreamcopy is useful for changing the elementary stream count, container format, or modifying container-level metadata. Since there is no decoding or encoding, it is very fast and there is no quality loss. However, it might not work in some cases because of a variety of factors (e.g. certain information required by the target container is not available in the source). Applying filters is obviously also impossible, since filters work on decoded frames.\n\nMore complex streamcopy scenarios can be constructed - e.g. combining streams from two input files into a single output:\n\nthat can be built by the commandline\n\nThe output option is used twice here, creating two streams in the output file - one fed by the first input and one by the second. The single instance of the option selects streamcopy for both of those streams. You could also use multiple instances of this option together with Stream specifiers to apply different values to each stream, as will be demonstrated in following sections.\n\nA converse scenario is splitting multiple streams from a single input into multiple outputs:\n\nNote how a separate instance of the option is needed for every output file even though their values are the same. This is because non-global options (which is most of them) only apply in the context of the file before which they are placed.\n\nThese examples can of course be further generalized into arbitrary remappings of any number of inputs into any number of outputs.\n\nTranscoding is the process of decoding a stream and then encoding it again. Since encoding tends to be computationally expensive and in most cases degrades the stream quality (i.e. it is lossy), you should only transcode when you need to and perform streamcopy otherwise. Typical reasons to transcode are:\n• you want to feed the stream to something that cannot decode the original codec.\n\nNote that will transcode all audio, video, and subtitle streams unless you specify for them.\n\nConsider an example pipeline that reads an input file with one audio and one video stream, transcodes the video and copies the audio into a single output file. This can be schematically represented as follows\n\nand implemented with the following commandline:\n\nNote how it uses stream specifiers and to select input streams and apply different values of the option to them; see the Stream specifiers section for more details.\n\nWhen transcoding, audio and video streams can be filtered before encoding, with either a simple or complex filtergraph.\n\nSimple filtergraphs are those that have exactly one input and output, both of the same type (audio or video). They are configured with the per-stream option (with and aliases for (video) and (audio) respectively). Note that simple filtergraphs are tied to their output stream, so e.g. if you have multiple audio streams, will create a separate filtergraph for each one.\n\nTaking the trancoding example from above, adding filtering (and omitting audio, for clarity) makes it look like this:\n\nComplex filtergraphs are those which cannot be described as simply a linear processing chain applied to one stream. This is the case, for example, when the graph has more than one input and/or output, or when output stream type is different from input. Complex filtergraphs are configured with the option. Note that this option is global, since a complex filtergraph, by its nature, cannot be unambiguously associated with a single stream or file. Each instance of creates a new complex filtergraph, and there can be any number of them.\n\nA trivial example of a complex filtergraph is the filter, which has two video inputs and one video output, containing one video overlaid on top of the other. Its audio counterpart is the filter.\n\nWhile decoders are normally associated with demuxer streams, it is also possible to create \"loopback\" decoders that decode the output from some encoder and allow it to be fed back to complex filtergraphs. This is done with the directive, which takes as a parameter the index of the output stream that should be decoded. Every such directive creates a new loopback decoder, indexed with successive integers starting at zero. These indices should then be used to refer to loopback decoders in complex filtergraph link labels, as described in the documentation for .\n\nDecoding AVOptions can be passed to loopback decoders by placing them before , analogously to input/output options.\n\nE.g. the following example:\n• (line 2) encodes it with at low quality;\n• (line 4) places decoded video side by side with the original input video;\n• (line 5) combined video is then losslessly encoded and written into .\n\nSuch a transcoding pipeline can be represented with the following diagram:\n\nprovides the option for manual control of stream selection in each output file. Users can skip and let ffmpeg perform automatic stream selection as described below. The options can be used to skip inclusion of video, audio, subtitle and data streams respectively, whether manually mapped or automatically selected, except for those streams which are outputs of complex filtergraphs.\n\nThe sub-sections that follow describe the various rules that are involved in stream selection. The examples that follow next show how these rules are applied in practice.\n\nWhile every effort is made to accurately reflect the behavior of the program, FFmpeg is under continuous development and the code may have changed since the time of this writing.\n\nIn the absence of any map options for a particular output file, ffmpeg inspects the output format to check which type of streams can be included in it, viz. video, audio and/or subtitles. For each acceptable stream type, ffmpeg will pick one stream, when available, from among all the inputs.\n\nIt will select that stream based upon the following criteria:\n• for video, it is the stream with the highest resolution,\n• for audio, it is the stream with the most channels,\n• for subtitles, it is the first subtitle stream found but there’s a caveat. The output format’s default subtitle encoder can be either text-based or image-based, and only a subtitle stream of the same type will be chosen.\n\nIn the case where several streams of the same type rate equally, the stream with the lowest index is chosen.\n\nData or attachment streams are not automatically selected and can only be included using .\n\nWhen is used, only user-mapped streams are included in that output file, with one possible exception for filtergraph outputs described below.\n\nIf there are any complex filtergraph output streams with unlabeled pads, they will be added to the first output file. This will lead to a fatal error if the stream type is not supported by the output format. In the absence of the map option, the inclusion of these streams leads to the automatic stream selection of their types being skipped. If map options are present, these filtergraph streams are included in addition to the mapped streams.\n\nComplex filtergraph output streams with labeled pads must be mapped once and exactly once.\n\nStream handling is independent of stream selection, with an exception for subtitles described below. Stream handling is set via the option addressed to streams within a specific output file. In particular, codec options are applied by ffmpeg after the stream selection process and thus do not influence the latter. If no option is specified for a stream type, ffmpeg will select the default encoder registered by the output file muxer.\n\nAn exception exists for subtitles. If a subtitle encoder is specified for an output file, the first subtitle stream found of any type, text or image, will be included. ffmpeg does not validate if the specified encoder can convert the selected stream or if the converted stream is acceptable within the output format. This applies generally as well: when the user sets an encoder manually, the stream selection process cannot check if the encoded stream can be muxed into the output file. If it cannot, ffmpeg will abort and all output files will fail to be processed.\n\nThe following examples illustrate the behavior, quirks and limitations of ffmpeg’s stream selection methods.\n\nThey assume the following three input files.\n\nThere are three output files specified, and for the first two, no options are set, so ffmpeg will select streams for these two files automatically.\n\nis a Matroska container file and accepts video, audio and subtitle streams, so ffmpeg will try to select one of each type.\n\n For video, it will select from , which has the highest resolution among all the input video streams.\n\n For audio, it will select from , since it has the greatest number of channels.\n\n For subtitles, it will select from , which is the first subtitle stream from among and .\n\naccepts only audio streams, so only from is selected.\n\nFor , since a option is set, no automatic stream selection will occur. The option will select all audio streams from the second input . No other streams will be included in this output file.\n\nFor the first two outputs, all included streams will be transcoded. The encoders chosen will be the default ones registered by each output format, which may not match the codec of the selected input streams.\n\nFor the third output, codec option for audio streams has been set to , so no decoding-filtering-encoding operations will occur, or can occur. Packets of selected streams shall be conveyed from the input file and muxed within the output file.\n\nAlthough is a Matroska container file which accepts subtitle streams, only a video and audio stream shall be selected. The subtitle stream of is image-based and the default subtitle encoder of the Matroska muxer is text-based, so a transcode operation for the subtitles is expected to fail and hence the stream isn’t selected. However, in , a subtitle encoder is specified in the command and so, the subtitle stream is selected, in addition to the video stream. The presence of disables audio stream selection for .\n\nA filtergraph is setup here using the option and consists of a single video filter. The filter requires exactly two video inputs, but none are specified, so the first two available video streams are used, those of and . The output pad of the filter has no label and so is sent to the first output file . Due to this, automatic selection of the video stream is skipped, which would have selected the stream in . The audio stream with most channels viz. in , is chosen automatically. No subtitle stream is chosen however, since the MP4 format has no default subtitle encoder registered, and the user hasn’t specified a subtitle encoder.\n\nThe 2nd output file, , only accepts text-based subtitle streams. So, even though the first subtitle stream available belongs to , it is image-based and hence skipped. The selected stream, in , is the first text-based subtitle stream.\n\nThe above command will fail, as the output pad labelled has been mapped twice. None of the output files shall be processed.\n\nThis command above will also fail as the hue filter output has a label, , and hasn’t been mapped anywhere.\n\nThe command should be modified as follows,\n\nThe video stream from is sent to the hue filter, whose output is cloned once using the split filter, and both outputs labelled. Then a copy each is mapped to the first and third output files.\n\nThe overlay filter, requiring two video inputs, uses the first two unused video streams. Those are the streams from and . The overlay output isn’t labelled, so it is sent to the first output file , regardless of the presence of the option.\n\nThe aresample filter is sent the first unused audio stream, that of . Since this filter output is also unlabelled, it too is mapped to the first output file. The presence of only suppresses automatic or manual stream selection of audio streams, not outputs sent from filtergraphs. Both these mapped streams shall be ordered before the mapped stream in .\n\nThe video, audio and subtitle streams mapped to are entirely determined by automatic stream selection.\n\nconsists of the cloned video output from the hue filter and the first audio stream from . \n\n\n\nAll the numerical options, if not specified otherwise, accept a string representing a number as input, which may be followed by one of the SI unit prefixes, for example: ’K’, ’M’, or ’G’.\n\nIf ’i’ is appended to the SI unit prefix, the complete prefix will be interpreted as a unit prefix for binary multiples, which are based on powers of 1024 instead of powers of 1000. Appending ’B’ to the SI unit prefix multiplies the value by 8. This allows using, for example: ’KB’, ’MiB’, ’G’ and ’B’ as number suffixes.\n\nOptions which do not take arguments are boolean options, and set the corresponding value to true. They can be set to false by prefixing the option name with \"no\". For example using \"-nofoo\" will set the boolean option with name \"foo\" to false.\n\nOptions that take arguments support a special syntax where the argument given on the command line is interpreted as a path to the file from which the actual argument value is loaded. To use this feature, add a forward slash ’/’ immediately before the option name (after the leading dash). E.g.\n\nwill load a filtergraph description from the file named .\n\nSome options are applied per-stream, e.g. bitrate or codec. Stream specifiers are used to precisely specify which stream(s) a given option belongs to.\n\nA stream specifier is a string generally appended to the option name and separated from it by a colon. E.g. contains the stream specifier, which matches the second audio stream. Therefore, it would select the ac3 codec for the second audio stream.\n\nA stream specifier can match several streams, so that the option is applied to all of them. E.g. the stream specifier in matches all audio streams.\n\nAn empty stream specifier matches all streams. For example, or would copy all the streams without reencoding.\n\nPossible forms of stream specifiers are:\n\nThese options are shared amongst the ff* tools.\n\nThese options are provided directly by the libavformat, libavdevice and libavcodec libraries. To see the list of available AVOptions, use the option. They are separated into two categories:\n\nFor example to write an ID3v2.3 header instead of a default ID3v2.4 to an MP3 file, use the private option of the MP3 muxer:\n\nAll codec AVOptions are per-stream, and thus a stream specifier should be attached to them:\n\nIn the above example, a multichannel audio stream is mapped twice for output. The first instance is encoded with codec ac3 and bitrate 640k. The second instance is downmixed to 2 channels and encoded with codec aac. A bitrate of 128k is specified for it using absolute index of the output stream.\n\nNote: the syntax cannot be used for boolean AVOptions, use / .\n\nNote: the old undocumented way of specifying per-stream AVOptions by prepending v/a/s to the options name is now obsolete and will be removed soon.\n\nForce input or output file format. The format is normally auto detected for input files and guessed from the file extension for output files, so this option is not needed in most cases. Do not overwrite output files, and exit immediately if a specified output file already exists. Set number of times input stream shall be looped. Loop 0 means no loop, loop -1 means infinite loop. Allow forcing a decoder of a different media type than the one detected or designated by the demuxer. Useful for decoding media data muxed as data streams. Select an encoder (when used before an output file) or a decoder (when used before an input file) for one or more streams. is the name of a decoder/encoder or a special value (output only) to indicate that the stream is not to be re-encoded. encodes all video streams with libx264 and copies all audio streams. For each stream, the last matching option is applied, so will copy all the streams except the second video, which will be encoded with libx264, and the 138th audio, which will be encoded with libvorbis. When used as an input option (before ), limit the of data read from the input file. When used as an output option (before an output url), stop writing the output after its duration reaches . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. -to and -t are mutually exclusive and -t has priority. Stop writing the output or reading the input at . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. -to and -t are mutually exclusive and -t has priority. Set the file size limit, expressed in bytes. No further chunk of bytes is written after the limit is exceeded. The size of the output file is slightly more than the requested file size. When used as an input option (before ), seeks in this input file to . Note that in most formats it is not possible to seek exactly, so will seek to the closest seek point before . When transcoding and is enabled (the default), this extra segment between the seek point and will be decoded and discarded. When doing stream copy or when is used, it will be preserved. When used as an output option (before an output url), decodes but discards input until the timestamps reach . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. Like the option but relative to the \"end of file\". That is negative values are earlier in the file, 0 is at EOF. This will take the difference between the start times of the target and reference inputs and offset the timestamps of the target file by that difference. The source timestamps of the two inputs should derive from the same clock source for expected results. If is set then must also be set. If either of the inputs has no starting timestamp then no sync adjustment is made. Acceptable values are those that refer to a valid ffmpeg input index. If the sync reference is the target index itself or , then no adjustment is made to target timestamps. A sync reference may not itself be synced to any other input. must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. The offset is added to the timestamps of the input files. Specifying a positive offset means that the corresponding streams are delayed by the time duration specified in . Set the recording timestamp in the container. must be a date specification, see (ffmpeg-utils)the Date section in the ffmpeg-utils(1) manual. An optional may be given to set metadata on streams, chapters or programs. See documentation for details. This option overrides metadata set with . It is also possible to delete metadata by using an empty value. For example, for setting the title in the output file: To set the language of the first audio stream: Default value: by default, all disposition flags are copied from the input stream, unless the output stream this option applies to is fed by a complex filtergraph - in that case no disposition flags are set by default. is a sequence of disposition flags separated by ’+’ or ’-’. A ’+’ prefix adds the given disposition, ’-’ removes it. If the first flag is also prefixed with ’+’ or ’-’, the resulting disposition is the default value updated by . If the first flag is not prefixed, the resulting disposition is . It is also possible to clear the disposition by setting it to 0. If no options were specified for an output file, ffmpeg will automatically set the ’default’ disposition flag on the first stream of each type, when there are multiple streams of this type in the output file and no stream of that type is already marked as default. The option lists the known disposition flags. For example, to make the second audio stream the default stream: To make the second subtitle stream the default stream and remove the default disposition from the first subtitle stream: To add the ’original’ and remove the ’comment’ disposition flag from the first audio stream without removing its other disposition flags: To remove the ’original’ and add the ’comment’ disposition flag to the first audio stream without removing its other disposition flags: To set only the ’original’ and ’comment’ disposition flags on the first audio stream (and remove its other disposition flags): To remove all disposition flags from the first audio stream: Not all muxers support embedded thumbnails, and those who do, only support a few formats, like JPEG or PNG. Creates a program with the specified , and adds the specified (s) to it. Creates a stream group of the specified and , or by ping an input group, adding the specified (s) and/or previously defined (s) to it. can be one of the following: Groups s that belong to the same IAMF Audio Element For this group , the following options are available The Audio Element type. The following values are supported: Demixing information used to reconstruct a scalable channel audio representation. This option must be separated from the rest with a ’,’, and takes the following key=value options An identifier parameters blocks in frames may refer to Recon gain information used to reconstruct a scalable channel audio representation. This option must be separated from the rest with a ’,’, and takes the following key=value options An identifier parameters blocks in frames may refer to A layer defining a Channel Layout in the Audio Element. This option must be separated from the rest with a ’,’. Several ’,’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options The following flags are available: Wether to signal if recon_gain is present as metadata in parameter blocks within frames Which channels output_gain applies to. The following flags are available: The ambisonics mode. This has no effect if audio_element_type is set to channel. The following values are supported: Each ambisonics channel is coded as an individual mono stream in the group Groups s that belong to all IAMF Audio Element the same IAMF Mix Presentation references For this group , the following options are available A sub-mix within the Mix Presentation. This option must be separated from the rest with a ’,’. Several ’,’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options An identifier parameters blocks in frames may refer to, for post-processing the mixed audio signal to generate the audio signal for playback The sample rate duration fields in parameters blocks in frames that refer to this are expressed as Default mix gain value to apply when there are no parameter blocks sharing the same for a given frame References an Audio Element used in this Mix Presentation to generate the final output audio signal for playback. This option must be separated from the rest with a ’|’. Several ’|’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options: The for an Audio Element which this sub-mix refers to An identifier parameters blocks in frames may refer to, for applying any processing to the referenced and rendered Audio Element before being summed with other processed Audio Elements The sample rate duration fields in parameters blocks in frames that refer to this are expressed as Default mix gain value to apply when there are no parameter blocks sharing the same for a given frame A key=value string describing the sub-mix element where \"key\" is a string conforming to BCP-47 that specifies the language for the \"value\" string. \"key\" must be the same as the one in the mix’s Indicates whether the input channel-based Audio Element is rendered to stereo loudspeakers or spatialized with a binaural renderer when played back on headphones. This has no effect if the referenced Audio Element’s is set to channel. The following values are supported: Specifies the layouts for this sub-mix on which the loudness information was measured. This option must be separated from the rest with a ’|’. Several ’|’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options: The layout follows the loudspeaker sound system convention of ITU-2051-3. Channel layout matching one of Sound Systems A to J of ITU-2051-3, plus 7.1.2 and 3.1.2 This has no effect if is set to binaural. The program integrated loudness information, as defined in ITU-1770-4. The digital (sampled) peak value of the audio signal, as defined in ITU-1770-4. The true peak of the audio signal, as defined in ITU-1770-4. The Dialogue loudness information, as defined in ITU-1770-4. The Album loudness information, as defined in ITU-1770-4. A key=value string string describing the mix where \"key\" is a string conforming to BCP-47 that specifies the language for the \"value\" string. \"key\" must be the same as the ones in all sub-mix element’s s E.g. to create an scalable 5.1 IAMF file from several WAV input files To copy the two stream groups (Audio Element and Mix Presentation) from an input IAMF file with four streams into an mp4 output Specify target file type ( , , , , ). may be prefixed with , or to use the corresponding standard. All the format options (bitrate, codecs, buffer sizes) are then set automatically. You can just type: Nevertheless you can specify additional options as long as you know they do not conflict with the standard, as in: The parameters set for each target are as follows. The target is identical to the target except that the pixel format set is for all three standards. Any user-set value for a parameter above will override the target preset value. In that case, the output may not comply with the target standard. As an input option, blocks all data streams of a file from being filtered or being automatically selected or mapped for any output. See option to disable streams individually. As an output option, disables data recording i.e. automatic selection or mapping of any data stream. For full manual control see the option. Set the number of data frames to output. This is an obsolete alias for , which you should use instead. Stop writing to the stream after frames. Use fixed quality scale (VBR). The meaning of / is codec-dependent. If is used without a then it applies only to the video stream, this is to maintain compatibility with previous behavior and as specifying the same codec specific value to 2 different codecs that is audio and video generally is not what is intended when no stream_specifier is used. Create the filtergraph specified by and use it to filter the stream. is a description of the filtergraph to apply to the stream, and must have a single input and a single output of the same type of the stream. In the filtergraph, the input is associated to the label , and the output to the label . See the ffmpeg-filters manual for more information about the filtergraph syntax. See the -filter_complex option if you want to create filtergraphs with multiple inputs and/or outputs. This boolean option determines if the filtergraph(s) to which this stream is fed gets reinitialized when input frame parameters change mid-stream. This option is enabled by default as most video and all audio filters cannot handle deviation in input frame properties. Upon reinitialization, existing filter state is lost, like e.g. the frame count reference available in some filters. Any frames buffered at time of reinitialization are lost. The properties where a change triggers reinitialization are, for video, frame resolution or pixel format; for audio, sample format, sample rate, channel count or channel layout. Defines how many threads are used to process a filter pipeline. Each pipeline will produce a thread pool with this many threads available for parallel processing. The default is the number of available CPUs. Specify the preset for matching stream(s). Log encoding progress/statistics as \"info\"-level log (see ). It is on by default, to explicitly disable it you need to specify . Set period at which encoding progress/statistics are updated. Default is 0.5 seconds. Progress information is written periodically and at the end of the encoding process. It is made of \" = \" lines. consists of only alphanumeric characters. The last key of a sequence of progress information is always \"progress\" with the value \"continue\" or \"end\". The update period is set using . For example, log progress information to stdout: Enable interaction on standard input. On by default unless standard input is used as an input. To explicitly disable interaction you need to specify . Disabling interaction on standard input is useful, for example, if ffmpeg is in the background process group. Roughly the same result can be achieved with but it requires a shell. Print timestamp/latency information. It is off by default. This option is mostly useful for testing and debugging purposes, and the output format may change from one version to another, so it should not be employed by portable scripts. See also the option . Add an attachment to the output file. This is supported by a few formats like Matroska for e.g. fonts used in rendering subtitles. Attachments are implemented as a specific type of stream, so this option will add a new stream to the file. It is then possible to use per-stream options on this stream in the usual way. Attachment streams created with this option will be created after all the other streams (i.e. those created with or automatic mappings). Note that for Matroska you also have to set the mimetype metadata tag: (assuming that the attachment stream will be third in the output file). Extract the matching attachment stream into a file named . If is empty, then the value of the metadata tag will be used. E.g. to extract the first attachment to a file named ’out.ttf’: To extract all attachments to files determined by the tag: Technical note – attachments are implemented as codec extradata, so this option can actually be used to extract extradata from any stream, not just attachments.\n\nSet pixel format. Use to show all the supported pixel formats. If the selected pixel format can not be selected, ffmpeg will print a warning and select the best pixel format supported by the encoder. If is prefixed by a , ffmpeg will exit with an error if the requested pixel format can not be selected, and automatic conversions inside filtergraphs are disabled. If is a single , ffmpeg selects the same pixel format as the input (or graph output) and automatic conversions are disabled. Set default flags for the libswscale library. These flags are used by automatically inserted filters and those within simple filtergraphs, if not overridden within the filtergraph definition. See the (ffmpeg-scaler)ffmpeg-scaler manual for a list of scaler options. Rate control override for specific intervals, formatted as \"int,int,int\" list separated with slashes. Two first values are the beginning and end frame numbers, last one is quantizer to use if positive, or quality factor if negative. Dump video coding statistics to . See the vstats file format section for the format description. Dump video coding statistics to . See the vstats file format section for the format description. Specify which version of the vstats format to use. Default is . See the vstats file format section for the format description. Force video tag/fourcc. This is an alias for . can take arguments of the following form: If the argument consists of timestamps, ffmpeg will round the specified times to the nearest output timestamp as per the encoder time base and force a keyframe at the first frame having timestamp equal or greater than the computed timestamp. Note that if the encoder time base is too coarse, then the keyframes may be forced on frames with timestamps lower than the specified time. The default encoder time base is the inverse of the output framerate but may be set otherwise via . If one of the times is \" [ ]\", it is expanded into the time of the beginning of all chapters in the file, shifted by , expressed as a time in seconds. This option can be useful to ensure that a seek point is present at a chapter mark or any other designated place in the output file. For example, to insert a key frame at 5 minutes, plus key frames 0.1 second before the beginning of every chapter: If the argument is prefixed with , the string is interpreted like an expression and is evaluated for each frame. A key frame is forced in case the evaluation is non-zero. The expression in can contain the following constants: the number of current processed frame, starting from 0 the number of the previous forced frame, it is when no keyframe was forced yet the time of the previous forced frame, it is when no keyframe was forced yet the time of the current processed frame For example to force a key frame every 5 seconds, you can specify: To force a key frame 5 seconds after the time of the last forced one, starting from second 13: If the argument is , ffmpeg will force a key frame if the current frame being encoded is marked as a key frame in its source. In cases where this particular source frame has to be dropped, enforce the next available frame to become a key frame instead. Note that forcing too many keyframes is very harmful for the lookahead algorithms of certain encoders: using fixed-GOP options or similar would be more efficient. Automatically crop the video after decoding according to file metadata. Default is all. Apply both codec and container level croppping. This is the default mode. When doing stream copy, copy also non-key frames found at the beginning. Initialise a new hardware device of type called , using the given device parameters. If no name is specified it will receive a default name of the form \" %d\". The meaning of and the following arguments depends on the device type: is the number of the CUDA device. The following options are recognized: If set to 1, uses the primary device context instead of creating a new one. Choose the second device on the system. Choose the first device and use the primary device context. is the number of the Direct3D 9 display adapter. is the number of the Direct3D 11 display adapter. If not specified, it will attempt to use the default Direct3D 11 display adapter or the first Direct3D 11 display adapter whose hardware VendorId is specified by ‘ ’. Create a d3d11va device on the Direct3D 11 display adapter specified by index 1. Create a d3d11va device on the first Direct3D 11 display adapter whose hardware VendorId is 0x8086. is either an X11 display name, a DRM render node or a DirectX adapter index. If not specified, it will attempt to open the default X11 display ($DISPLAY) and then the first DRM render node (/dev/dri/renderD128), or the default DirectX adapter on Windows. The following options are recognized: When is not specified, use this option to specify the name of the kernel driver associated with the desired device. This option is available only when the hardware acceleration method drm and vaapi are enabled. When and are not specified, use this option to specify the vendor id associated with the desired device. This option is available only when the hardware acceleration method drm and vaapi are enabled and kernel_driver is not specified. Create a vaapi device on a device associated with kernel driver ‘ ’. Create a vaapi device on a device associated with vendor id ‘ ’. is an X11 display name. If not specified, it will attempt to open the default X11 display ($DISPLAY). selects a value in ‘ ’. Allowed values are: If not specified, ‘ ’ is used. (Note that it may be easier to achieve the desired result for QSV by creating the platform-appropriate subdevice (‘ ’ or ‘ ’ or ‘ ’) and then deriving a QSV device from that.) The following options are recognized: Specify a DRM render node on Linux or DirectX adapter on Windows. Choose platform-appropriate subdevice type. On Windows ‘ ’ is used as default subdevice type when is specified at configuration time, ‘ ’ is used as default subdevice type when is specified at configuration time. On Linux user can use ‘ ’ only as subdevice type. Choose the GPU subdevice with type ‘ ’ and create QSV device with ‘ ’. Choose the GPU subdevice with type ‘ ’ and create QSV device with ‘ ’. Create a QSV device with ‘ ’ on DirectX adapter 1 with subdevice type ‘ ’. Create a VAAPI device called ‘ ’ on , then derive a QSV device called ‘ ’ from device ‘ ’. selects the platform and device as platform_index.device_index. The set of devices can also be filtered using the key-value pairs to find only devices matching particular platform or device strings. The strings usable as filters are: The indices and filters must together uniquely select a device. Choose the second device on the first platform. Choose the device with a name containing the string Foo9000. Choose the GPU device on the second platform supporting the cl_khr_fp16 extension. If is an integer, it selects the device by its index in a system-dependent list of devices. If is any other string, it selects the first device with a name containing that string as a substring. The following options are recognized: If set to 1, enables the validation layer, if installed. If set to 1, images allocated by the hwcontext will be linear and locally mappable. A plus separated list of additional instance extensions to enable. A plus separated list of additional device extensions to enable. Choose the second device on the system. Choose the first device with a name containing the string RADV. Choose the first device and enable the Wayland and XCB instance extensions. Initialise a new hardware device of type called , deriving it from the existing device with the name . List all hardware device types supported in this build of ffmpeg. Pass the hardware device called to all filters in any filter graph. This can be used to set the device to upload to with the filter, or the device to map to with the filter. Other filters may also make use of this parameter when they require a hardware device. Note that this is typically only required when the input is not already in hardware frames - when it is, filters will derive the device they require from the context of the frames they receive as input. This is a global setting, so all filters will receive the same device. Use hardware acceleration to decode the matching stream(s). The allowed values of are: Do not use any hardware acceleration (the default). Use VDPAU (Video Decode and Presentation API for Unix) hardware acceleration. Use the Intel QuickSync Video acceleration for video transcoding. Unlike most other values, this option does not enable accelerated decoding (that is used automatically whenever a qsv decoder is selected), but accelerated transcoding, without copying the frames into the system memory. For it to work, both the decoder and the encoder must support QSV acceleration and no filters must be used. This option has no effect if the selected hwaccel is not available or not supported by the chosen decoder. Note that most acceleration methods are intended for playback and will not be faster than software decoding on modern CPUs. Additionally, will usually need to copy the decoded frames from the GPU memory into the system memory, resulting in further performance loss. This option is thus mainly useful for testing. Select a device to use for hardware acceleration. This option only makes sense when the option is also specified. It can either refer to an existing device created with by name, or it can create a new device as if ‘ ’ : were called immediately before. List all hardware acceleration components enabled in this build of ffmpeg. Actual runtime availability depends on the hardware and its suitable driver being installed. Set a specific output video stream as the heartbeat stream according to which to split and push through currently in-progress subtitle upon receipt of a random access packet. This lowers the latency of subtitles for which the end packet or the following subtitle has not yet been received. As a drawback, this will most likely lead to duplication of subtitle events in order to cover the full duration, so when dealing with use cases where latency of when the subtitle event is passed on to output is not relevant this option should not be utilized. Requires to be set for the relevant input subtitle stream for this to have any effect, as well as for the input subtitle stream having to be directly mapped to the same output in which the heartbeat stream resides.\n\nCreate one or more streams in the output file. This option has two forms for specifying the data source(s): the first selects one or more streams from some input file (specified with ), the second takes an output from some complex filtergraph (specified with ). In the first form, an output stream is created for every stream from the input file with the index . If is given, only those streams that match the specifier are used (see the Stream specifiers section for the syntax). A character before the stream identifier creates a \"negative\" mapping. It disables matching streams from already created mappings. An optional may be given after the stream specifier, which for multiview video specifies the view to be used. The view specifier may have one of the following formats: select a view by its ID; may be set to ’all’ to use all the views interleaved into one stream; select a view by its index; i.e. 0 is the base view, 1 is the first non-base view, etc. select a view by its display position; may be or The default for transcoding is to only use the base view, i.e. the equivalent of . For streamcopy, view specifiers are not supported and all views are always copied. A trailing after the stream index will allow the map to be optional: if the map matches no streams the map will be ignored instead of failing. Note the map will still fail if an invalid input file index is used; such as if the map refers to a non-existent input. An alternative form will map outputs from complex filter graphs (see the option) to the output file. must correspond to a defined output link label in the graph. This option may be specified multiple times, each adding more streams to the output file. Any given input stream may also be mapped any number of times as a source for different output streams, e.g. in order to use different encoding options and/or filters. The streams are created in the output in the same order in which the options are given on the commandline. Using this option disables the default mappings for this output file. To map ALL streams from the first input file to output If you have two audio streams in the first input file, these streams are identified by and . You can use to select which streams to place in an output file. For example: will map the second input stream in to the (single) output stream in . To select the stream with index 2 from input file (specified by the identifier ), and stream with index 6 from input (specified by the identifier ), and copy them to the output file : To select all video and the third audio stream from an input file: To map all the streams except the second audio, use negative mappings To map the video and audio streams from the first input, and using the trailing , ignore the audio mapping if no audio streams exist in the first input: Ignore input streams with unknown type instead of failing if copying such streams is attempted. Allow input streams with unknown type to be copied instead of failing if copying such streams is attempted. Set metadata information of the next output file from . Note that those are file indices (zero-based), not filenames. Optional parameters specify, which metadata to copy. A metadata specifier can have the following forms: global metadata, i.e. metadata that applies to the whole file per-stream metadata. is a stream specifier as described in the Stream specifiers chapter. In an input metadata specifier, the first matching stream is copied from. In an output metadata specifier, all matching streams are copied to. If metadata specifier is omitted, it defaults to global. By default, global metadata is copied from the first input file, per-stream and per-chapter metadata is copied along with streams/chapters. These default mappings are disabled by creating any mapping of the relevant type. A negative file index can be used to create a dummy mapping that just disables automatic copying. For example to copy metadata from the first stream of the input file to global metadata of the output file: To do the reverse, i.e. copy global metadata to all audio streams: Note that simple would work as well in this example, since global metadata is assumed by default. Copy chapters from input file with index to the next output file. If no chapter mapping is specified, then chapters are copied from the first input file with at least one chapter. Use a negative file index to disable any chapter copying. Show benchmarking information at the end of an encode. Shows real, system and user time used and maximum memory consumption. Maximum memory consumption is not supported on all systems, it will usually display as 0 if not supported. Show benchmarking information during the encode. Shows real, system and user time used in various steps (audio/video encode/decode). Exit after ffmpeg has been running for seconds in CPU user time. When dumping packets, also dump the payload. Its value is a floating-point positive number which represents the maximum duration of media, in seconds, that should be ingested in one second of wallclock time. Default value is zero and represents no imposed limitation on speed of ingestion. Value represents real-time speed and is equivalent to . Mainly used to simulate a capture device or live input stream (e.g. when reading from a file). Should not be used with a low value when input is an actual capture device or live stream as it may cause packet loss. It is useful for when flow speed of output packets is important, such as live streaming. Read input at native frame rate. This is equivalent to setting . Set an initial read burst time, in seconds, after which will be enforced. If either the input or output is blocked leading to actual read speed falling behind the specified readrate, then this rate takes effect till the input catches up with the specified readrate. Must not be lower than the primary readrate. Set video sync method / framerate mode. vsync is applied to all output video streams but can be overridden for a stream by setting fps_mode. vsync is deprecated and will be removed in the future. For compatibility reasons some of the values for vsync can be specified as numbers (shown in parentheses in the following table). Each frame is passed with its timestamp from the demuxer to the muxer. Frames will be duplicated and dropped to achieve exactly the requested constant frame rate. Frames are passed through with their timestamp or dropped so as to prevent 2 frames from having the same timestamp. Chooses between cfr and vfr depending on muxer capabilities. This is the default method. Note that the timestamps may be further modified by the muxer, after this. For example, in the case that the format option is enabled. With -map you can select from which stream the timestamps should be taken. You can leave either video or audio unchanged and sync the remaining stream(s) to the unchanged one. Frame drop threshold, which specifies how much behind video frames can be before they are dropped. In frame rate units, so 1.0 is one frame. The default is -1.1. One possible usecase is to avoid framedrops in case of noisy timestamps or to increase frame drop precision in case of exact timestamps. Pad the output audio stream(s). This is the same as applying . Argument is a string of filter parameters composed the same as with the filter. must be set for this output for the option to take effect. Do not process input timestamps, but keep their values without trying to sanitize them. In particular, do not remove the initial start time offset value. Note that, depending on the option or on specific muxer processing (e.g. in case the format option is enabled) the output timestamps may mismatch with the input timestamps even when this option is selected. When used with , shift input timestamps so they start at zero. This means that using e.g. will make output timestamps start at 50 seconds, regardless of what timestamp the input file started at. Specify how to set the encoder timebase when stream copying. is an integer numeric value, and can assume one of the following values: The time base is copied to the output encoder from the corresponding input demuxer. This is sometimes required to avoid non monotonically increasing timestamps when copying video streams with variable frame rate. The time base is copied to the output encoder from the corresponding input decoder. Try to make the choice automatically, in order to generate a sane output. Set the encoder timebase. can assume one of the following values: Assign a default value according to the media type. For video - use 1/framerate, for audio - use 1/samplerate. Use the timebase from the demuxer. Use the timebase from the filtergraph. Use the provided number as the timebase. This field can be provided as a ratio of two integers (e.g. 1:24, 1:48000) or as a decimal number (e.g. 0.04166, 2.0833e-5) Note that this option may require buffering frames, which introduces extra latency. The maximum amount of this latency may be controlled with the option. The option may require buffering potentially large amounts of data when at least one of the streams is \"sparse\" (i.e. has large gaps between frames – this is typically the case for subtitles). This option controls the maximum duration of buffered frames in seconds. Larger values may allow the option to produce more accurate results, but increase memory use and latency. The default value is 10 seconds. The timestamp discontinuity correction enabled by this option is only applied to input formats accepting timestamp discontinuity (for which the flag is enabled), e.g. MPEG-TS and HLS, and is automatically disabled when employing the option (unless wrapping is detected). If a timestamp discontinuity is detected whose absolute value is greater than , ffmpeg will remove the discontinuity by decreasing/increasing the current DTS and PTS by the corresponding delta value. The timestamp correction enabled by this option is only applied to input formats not accepting timestamp discontinuity (for which the flag is not enabled). If a timestamp discontinuity is detected whose absolute value is greater than , ffmpeg will drop the PTS/DTS timestamp value. The default value is (30 hours), which is arbitrarily picked and quite conservative. Assign a new stream-id value to an output stream. This option should be specified prior to the output filename to which it applies. For the situation where multiple output files exist, a streamid may be reassigned to a different value. For example, to set the stream 0 PID to 33 and the stream 1 PID to 36 for an output mpegts file: Apply bitstream filters to matching streams. The filters are applied to each packet as it is received from the demuxer (when used as an input option) or before it is sent to the muxer (when used as an output option). is a comma-separated list of bitstream filter specifications, each of the form Any of the ’,=:’ characters that are to be a part of an option value need to be escaped with a backslash. Use the option to get the list of bitstream filters. applies the bitstream filter (which converts MP4-encapsulated H.264 stream to Annex B) to the input video stream. applies the bitstream filter (which extracts text from MOV subtitles) to the output subtitle stream. Note, however, that since both examples use , it matters little whether the filters are applied on input or output - that would change if transcoding was happening. Specify Timecode for writing. is ’:’ for non drop timecode and ’;’ (or ’.’) for drop. Define a complex filtergraph, i.e. one with arbitrary number of inputs and/or outputs. For simple graphs – those with one input and one output of the same type – see the options. is a description of the filtergraph, as described in the “Filtergraph syntax” section of the ffmpeg-filters manual. This option may be specified multiple times - each use creates a new complex filtergraph. Inputs to a complex filtergraph may come from different source types, distinguished by the format of the corresponding link label:\n• To connect an input stream, use (i.e. the same syntax as ). If matches multiple streams, the first one will be used. For multiview video, the stream specifier may be followed by the view specifier, see documentation for the option for its syntax.\n• To connect a loopback decoder use [dec: ], where is the index of the loopback decoder to be connected to given input. For multiview video, the decoder index may be followed by the view specifier, see documentation for the option for its syntax.\n• To connect an output from another complex filtergraph, use its link label. E.g the following example:\n• (line 2) uses a complex filtergraph with one input and two outputs to scale the video to 1920x1080 and duplicate the result to both outputs;\n• (line 3) encodes one scaled output with and writes the result to ;\n• (line 5) places the output of the loopback decoder (i.e. the -encoded video) side by side with the scaled original input;\n• (line 6) combined video is then losslessly encoded and written into . Note that the two filtergraphs cannot be combined into one, because then there would be a cycle in the transcoding pipeline (filtergraph output goes to encoding, from there to decoding, then back to the same graph), and such cycles are not allowed. An unlabeled input will be connected to the first unused input stream of the matching type. Output link labels are referred to with . Unlabeled outputs are added to the first output file. Note that with this option it is possible to use only lavfi sources without normal input files. For example, to overlay an image over video Here refers to the first video stream in the first input file, which is linked to the first (main) input of the overlay filter. Similarly the first video stream in the second input is linked to the second (overlay) input of overlay. Assuming there is only one video stream in each input file, we can omit input labels, so the above is equivalent to Furthermore we can omit the output label and the single output from the filter graph will be added to the output file automatically, so we can simply write As a special exception, you can use a bitmap subtitle stream as input: it will be converted into a video with the same size as the largest video in the file, or 720x576 if no video is present. Note that this is an experimental and temporary solution. It will be removed once libavfilter has proper support for subtitles. For example, to hardcode subtitles on top of a DVB-T recording stored in MPEG-TS format, delaying the subtitles by 1 second: To generate 5 seconds of pure red video using lavfi source: Defines how many threads are used to process a filter_complex graph. Similar to filter_threads but used for graphs only. The default is the number of available CPUs. Define a complex filtergraph, i.e. one with arbitrary number of inputs and/or outputs. Equivalent to . This option enables or disables accurate seeking in input files with the option. It is enabled by default, so seeking is accurate when transcoding. Use to disable it, which may be useful e.g. when copying some streams and transcoding the others. This option enables or disables seeking by timestamp in input files with the option. It is disabled by default. If enabled, the argument to the option is considered an actual timestamp, and is not offset by the start time of the file. This matters only for files which do not start from timestamp 0, such as transport streams. For input, this option sets the maximum number of queued packets when reading from the file or device. With low latency / high rate live streams, packets may be discarded if they are not read in a timely manner; setting this value can force ffmpeg to use a separate input thread and read packets as soon as they arrive. By default ffmpeg only does this if multiple inputs are specified. For output, this option specified the maximum number of packets that may be queued to each muxing thread. Print sdp information for an output stream to . This allows dumping sdp information when at least one output isn’t an rtp stream. (Requires at least one of the output formats to be rtp). Allows discarding specific streams or frames from streams. Any input stream can be fully discarded, using value whereas selective discarding of frames from a stream occurs at the demuxer and is not supported by all demuxers. Stop and abort on various conditions. The following flags are available: No packets were passed to the muxer, the output is empty. No packets were passed to the muxer in some of the output streams. Set fraction of decoding frame failures across all inputs which when crossed ffmpeg will return exit code 69. Crossing this threshold does not terminate processing. Range is a floating-point number between 0 to 1. Default is 2/3. When transcoding audio and/or video streams, ffmpeg will not begin writing into the output until it has one packet for each such stream. While waiting for that to happen, packets for other streams are buffered. This option sets the size of this buffer, in packets, for the matching output stream. The default value of this option should be high enough for most uses, so only touch this option if you are sure that you need it. This is a minimum threshold until which the muxing queue size is not taken into account. Defaults to 50 megabytes per stream, and is based on the overall size of packets passed to the muxer. Enable automatically inserting format conversion filters in all filter graphs, including those defined by , , and . If filter format negotiation requires a conversion, the initialization of the filters will fail. Conversions can still be performed by inserting the relevant conversion filter (scale, aresample) in the graph. On by default, to explicitly disable it you need to specify . Declare the number of bits per raw sample in the given output stream to be . Note that this option sets the information provided to the encoder/muxer, it does not change the stream to conform to this value. Setting values that do not match the stream properties may result in encoding failures or invalid output files. Write per-frame encoding information about the matching streams into the file given by . writes information about raw video or audio frames right before they are sent for encoding, while writes information about encoded packets as they are received from the encoder. writes information about packets just as they are about to be sent to the muxer. Every frame or packet produces one line in the specified file. The format of this line is controlled by / / . When stats for multiple streams are written into a single file, the lines corresponding to different streams will be interleaved. The precise order of this interleaving is not specified and not guaranteed to remain stable between different invocations of the program, even with the same options. Specify the format for the lines written with / / . is a string that may contain directives of the form . is backslash-escaped — use \\{, \\}, and \\\\ to write a literal {, }, or \\, respectively, into the output. The directives given with may be one of the following: Index of the output stream in the file. Frame number. Pre-encoding: number of frames sent to the encoder so far. Post-encoding: number of packets received from the encoder so far. Muxing: number of packets submitted to the muxer for this stream so far. Input frame number. Index of the input frame (i.e. output by a decoder) that corresponds to this output frame or packet. -1 if unavailable. Timebase in which this frame/packet’s timestamps are expressed, as a rational number . Note that encoder and muxer may use different timebases. Timebase for , as a rational number . Available when is available, otherwise. Presentation timestamp of the frame or packet, as an integer. Should be multiplied by the timebase to compute presentation time. Presentation timestamp of the input frame (see ), as an integer. Should be multiplied by to compute presentation time. Printed as (2^63 - 1 = 9223372036854775807) when not available. Presentation time of the frame or packet, as a decimal number. Equal to multiplied by . Presentation time of the input frame (see ), as a decimal number. Equal to multiplied by . Printed as inf when not available. Decoding timestamp of the packet, as an integer. Should be multiplied by the timebase to compute presentation time. Decoding time of the frame or packet, as a decimal number. Equal to multiplied by . Number of audio samples sent to the encoder so far. Number of audio samples in the frame. Size of the encoded packet in bytes. Current bitrate in bits per second. Average bitrate for the whole stream so far, in bits per second, -1 if it cannot be determined at this point. Character ’K’ if the packet contains a keyframe, character ’N’ otherwise. Directives tagged with packet may only be used with and . Directives tagged with frame may only be used with . Directives tagged with audio may only be used with audio streams. In the future, new items may be added to the end of the default formatting strings. Users who depend on the format staying exactly the same, should prescribe it manually. Note that stats for different streams written into the same file may have different formats.\n\nA preset file contains a sequence of = pairs, one for each line, specifying a sequence of options which would be awkward to specify on the command line. Lines starting with the hash (’#’) character are ignored and are used to provide comments. Check the directory in the FFmpeg source tree for examples.\n\nThere are two types of preset files: ffpreset and avpreset files.\n\nffpreset files are specified with the , , , and options. The option takes the filename of the preset instead of a preset name as input and can be used for any kind of codec. For the , , and options, the options specified in a preset file are applied to the currently selected codec of the same type as the preset option.\n\nThe argument passed to the , , and preset options identifies the preset file to use according to the following rules:\n\nFirst ffmpeg searches for a file named .ffpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ) or in a folder along the executable on win32, in that order. For example, if the argument is , it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named - .ffpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\navpreset files are specified with the option. They work similar to ffpreset files, but they only allow encoder- specific options. Therefore, an = pair specifying an encoder cannot be used.\n\nWhen the option is specified, ffmpeg will look for files with the suffix .avpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ), in that order.\n\nFirst ffmpeg searches for a file named - .avpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named .avpreset in the same directories.\n\nThe and options enable generation of a file containing statistics about the generated video outputs.\n\nThe option controls the format version of the generated file.\n\nWith version the format is:\n\nWith version the format is:\n\nThe value corresponding to each key is described below:\n\nSee also the -stats_enc options for an alternative way to show encoding statistics.\n\nIf you specify the input format and device then ffmpeg can grab video and audio directly.\n\nOr with an ALSA audio source (mono input, card id 1) instead of OSS:\n\nNote that you must activate the right video source and channel before launching ffmpeg with any TV viewer such as xawtv by Gerd Knorr. You also have to set the audio recording levels correctly with a standard mixer.\n\nGrab the X11 display with ffmpeg via\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable.\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable. 10 is the x-offset and 20 the y-offset for the grabbing.\n\nAny supported file format and protocol can serve as input to ffmpeg:\n• You can use YUV files as input: It will use the files: The Y files use twice the resolution of the U and V files. They are raw files, without header. They can be generated by all decent video decoders. You must specify the size of the image with the option if ffmpeg cannot guess it.\n• You can input from a raw YUV420P file: test.yuv is a file containing raw YUV planar data. Each frame is composed of the Y plane followed by the U and V planes at half vertical and horizontal resolution.\n• You can output to a raw YUV420P file:\n• You can set several input files and output files: Converts the audio file a.wav and the raw YUV video file a.yuv to MPEG file a.mpg.\n• You can also do audio and video conversions at the same time:\n• You can encode to several formats at the same time and define a mapping from input stream to output streams: Converts a.wav to a.mp2 at 64 kbits and to b.mp2 at 128 kbits. ’-map file:index’ specifies which input stream is used for each output stream, in the order of the definition of output streams.\n• You can transcode decrypted VOBs: This is a typical DVD ripping example; the input is a VOB file, the output an AVI file with MPEG-4 video and MP3 audio. Note that in this command we use B-frames so the MPEG-4 stream is DivX5 compatible, and GOP size is 300 which means one intra frame every 10 seconds for 29.97fps input video. Furthermore, the audio stream is MP3-encoded so you need to enable LAME support by passing to configure. The mapping is particularly useful for DVD transcoding to get the desired audio language. NOTE: To see the supported input formats, use .\n• You can extract images from a video, or create a video from many images: This will extract one video frame per second from the video and will output them in files named , , etc. Images will be rescaled to fit the new WxH values. If you want to extract just a limited number of frames, you can use the above command in combination with the or option, or in combination with -ss to start extracting from a certain point in time. For creating a video from many images: The syntax specifies to use a decimal number composed of three digits padded with zeroes to express the sequence number. It is the same syntax supported by the C printf function, but only formats accepting a normal integer are suitable. When importing an image sequence, -i also supports expanding shell-like wildcard patterns (globbing) internally, by selecting the image2-specific option. For example, for creating a video from filenames matching the glob pattern :\n• You can put many streams of the same type in the output: The resulting output file will contain the first four streams from the input files in reverse order.\n• The four options lmin, lmax, mblmin and mblmax use ’lambda’ units, but you may use the QP2LAMBDA constant to easily convert from ’q’ units:\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 22, 2025 using makeinfo."
    },
    {
        "link": "https://stackoverflow.com/questions/3255674/convert-audio-files-to-mp3-using-ffmpeg",
        "document": "You could use this command:\n\nExplanation of the used arguments in this example:\n• None - Disable video, to make sure no video (including album cover image) is included if the source would be a video file\n• None - Set the audio sampling frequency. For output streams it is set by default to the frequency of the corresponding input stream. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options.\n• None - Set the number of audio channels. For output streams it is set by default to the number of input audio channels. For input streams this option only makes sense for audio grabbing devices and raw demuxers and is mapped to the corresponding demuxer options. So used here to make sure it is stereo (2 channels)\n• None - Converts the audio bit-rate to be exact 192 KB/s (192 kibibit per second). But maybe use instead, which allows the encoder to pick from 170 to 210 KB/s quality-range (average 192 KB/s). But format may not be compatible with some old player-hardware.\n\nNote to see docs about bit-rate argument's differences. Because maybe that option is the most important one, as it decides the \"quality\" versus \"output size\" versus \"old mp3-player compatibility\"."
    },
    {
        "link": "https://stackoverflow.com/questions/36012173/ffmpeg-convert-audio-files-to-mp3-using-ffmpeg-same-quality",
        "document": "it is impossible. FFmpeg cannot search for you input file's bit rate and then cannot copy funded bit rate to output. We need to find the input file's bit rate then we need to convert to mp3. This code converts audio files to mp3 with 320 kbps bit rate."
    },
    {
        "link": "https://img.ly/blog/ultimate-guide-to-ffmpeg",
        "document": "In this guide, we'll go through the hot topics of FFmpeg. But before that, we'll cover some base ground to help you understand basic media concepts and FFmpeg. Feel free to skip the parts that are already trivial for you!\n\nFFmpeg.org's definition is the following: \"FFmpeg is the leading multimedia framework, able to decode, encode, transcode, mux, demux, stream, filter and play pretty much anything that humans and machines have created. It supports the most obscure ancient formats up to the cutting edge. No matter if they were designed by some standards committee, the community or a corporation.\"\n\nI think of FFmpeg as the go-to application for audio/video manipulation in an automated or scripted manner.\n\nWhen you need to implement a service that manipulates video, or just have 300 media files that need to be converted into a different format, FFmpeg is your - nerdy - friend.\n\nFFmpeg can do large chunks of the basic functionalities of a modern Non-linear (NLE) video editors, e.g., Davinci Resolve Studio or Premiere Pro. But, it does not have a graphical interface in that sense as those behemoths do, and unarguably it is way less friendly.\n\nIn a general NLE, you might do things like these:\n• Drop it into the timeline\n• Add an extra audio track to the mix\n\nOr, to achieve the exact same thing, you could also execute this command:\n\nYes, it isn't friendly at all, but it is very, very powerful once you become friends with FFmpeg.\n\nCheck out this comparison of the original and the edited one:\n\nIf you want to try this command out, get the example files and see it for yourself!\n\nFFmpeg is available for most common and even uncommon platforms and architectures. You can be on Linux, Mac OS X or Microsoft Windows, and you'll be able to run or link to FFmpeg.\n\nInstalling FFmpeg is easy on most platforms! There is no installer, usually just a compressed archive you need to get for your platform and architecture.\n\nIn the case of Linux, most distributions include a pre-built FFmpeg in their software repositories. Therefore, you can install FFmpeg from those even more quickly.\n\nThe project was started in 2000 by the awesome Fabrice Bellard. The name is a concatenation of \"FF\" meaning \"fast-forward\" and MPEG, the name of a video standards group. It has been very well, active and alive since then, releasing a new release about every three months.\n\nThe default FFmpeg shipped with my Ubuntu Linux distribution supports about 460 codecs and 370 formats.\n\nSee it for yourself:\n\nKeep in mind that the supported codecs and formats (and filters, demuxers, muxers, input and output methods, etc.) are highly dependent on the so-called compilation flags.\n\nThis means that the above number only represents the fact that it supports at least this many codecs and formats. Still, there are even more that the package builders excluded for various reasons, e.g.: licensing, architecture, size considerations, etc.\n\nSince FFmpeg is open source, you can compile FFmpeg for yourself at any time.\n\nSuppose for example, that you care about your layer's size (therefore the bootstrap speed) in AWS Lambda. In this case, you can compile an FFmpeg binary that only contains the mp3 encoder for example, and nothing else.\n\nAlso, you might not want to run into licensing issues and leave out stuff that would cause problems for your use case. Therefore you choose to leave out particular codecs/formats. I highly recommend checking out the \"--enable-gpl\", \"--enable-nonfree\" and \"--enable-version3\" compilation flags in this case, as well as this.\n\nOr you might want to have a standalone FFmpeg binary in your project (e.g.: embedded, or some cloud instance), that does not depend on any operating system libraries. Then you want to make a so-called static build, that compiles in all the libraries into a single binary file, and does not depend on your OS' libraries and the runtime loading of other FFmpeg libraries. Search around for \"--enable-static\" in this case.\n\nFinally, you can find pre-built static FFmpeg builds right here too.\n\nFFmpeg reads and writes most video and audio formats that matter for most of us. It is a very capable and high-performance tool for converting and manipulating these formats.\n\nBut FFmpeg can do even more!\n\nFFmpeg has vast amounts of filters for audio and video. Therefore, video manipulation is also a key feature of FFmpeg.\n\nIt does support many kinds of hardware accelerations! Video encoding is a very resource-intensive operation, and you might come across quite a few hardware devices or features that might speed up your process!\n\nMost notably, if you have an NVIDIA card, you can increase your H.264 or H.265 encoding and decoding throughput by multipliers compared to your CPU. But other things, such as VDPAU, VAAPI, or OpenCL, can be leveraged to boost your pipeline's throughput.\n\nLearn more about the supported hardware acceleration methods here.\n\nFFmpeg is also very capable when it comes to accessing input and output data.\n\nJust to name a few: it can use your webcam, record from your microphone, grab your screen, or capture from your Blackmagic DeckLink. But FFmpeg can download directly from a web address, open all kinds of streams, read from a pipe, a socket, and of course, from files.\n\nThe same holds true for outputting the data. It can write to your webcam, play audio on your microphone... Just kidding:) It can output to files, streams, pipes, sockets and so on.\n\nThis article is full of FFmpeg commands that are working examples. The reason for that is that you could test these out for yourself! But the command line interfaces of different operating systems are slightly different, so the commands in this article are meant to be executed in a Linux bash shell.\n\nTo adopt these command lines to Microsoft Windows, you might need to:\n• Change (cd) into the directory where you extracted the ffmpeg.exe. Alternatively, add that directory to the path to make it callable from anywhere.\n• You might need to replace \"ffmpeg\" to \"ffmpeg.exe\"\n• You will need to replace \"\\\"-s (backslashes) at the end of the lines with \"^\"-s (hats)\n• You'll need to replace the argument's value to something like this: to get commands with the drawtext filter working.\n\nMacOS users will need steps #1 and #4.\n\nNow let's have a quick overview of media concepts. These concepts will be vital for us if we want to understand the latter sections of this article and FFmpeg's workings. To keep this section brief, it is a higher-level, simplified explanation of these concepts.\n\nWe'll briefly cover the following terms:\n\nThe sampling rate is the factor that shows how many times we measure/scan/sample the input data stream.\n\nThe image below shows the measurement windows (quantization) as gray bars.\n\nWhy does this matter? Because it is a balancing act. If we measure the signal less often, we'll lose more details (bad). Also, by having fewer samples, we'll have less data in the end. Therefore the file size will be smaller (good).\n\nHere are some ballpark values:\n• 88.2 kHz (Insane - usually for production only)\n• 96 kHz (Insane - usually for production only)\n\nThere are no definite \"right answers\" here. The question is what is \"good enough\" for your use case? GSM focuses on speech, and not even quality but understandability and the least possible amount of data. Therefore, they found that 8 kHz is enough (there are quite a few more tricks), for their purposes.\n\nThe \"CD quality\" aimed for high quality. Therefore they chose 44.1 kHz, that number has some history in it, but the main reason for aiming above 40 kHz lies in physics and how the human ear works.\n\nThere were two very smart guys whose theorem basically says that if you want a quite good signal representation, you have to sample it at twice the speed as its original frequency. Human hearing generally works up until about 20 kHz, so if you want \"good quality\", you should aim for at least 40 kHz. And 40 kHz + some headroom + some more physics + historical reasons = 44.1 kHz! :)\n\nAs for the higher rates, those are only used when very high-quality audio editing is needed.\n\nBitrate represents the amount of data per second that results from our transcoding/quantization process. If it is 1411 kbit/s, that means that for every second of audio data, about 1411 kbit of output data will be produced.\n\nTherefore, you can say that 1 minute of audio with 1411 kbit/sec will require:\n\nNow, it is only easy like that with raw audio data and with a few simple codecs, e.g. PCM in WAVs.\n\nCodecs compressing hard might throw your numbers around a little, as input data might be compressible with different rates. Variable bitrate is usually happening to save space. The encoder might output a lower bitrate if the data is \"simple\" and does not require high precision.\n\nHere are some ballpark values:\n\nInside of most audio formats, you can have more audio channels. This means multiple, separated audio streams can be in the same file.\n\nMany times, multiple channels have their own name:\n• If you have a single microphone, you will most probably record it into a single channel called Mono.\n• General music from the FM radio or streaming services usually has two channels in a so-called \"Stereo\" configuration.\n\nWith stereo, there could be several methods how the audio \"image\" can be made richer by leveraging audio panning, time and phase-shifting and much more. There is a special recording technique too, called Binaural recording, which is super awesome. Wear headphones for this, and don't be scared:)\n\nFor example, here are Big Buck Bunny's audio waveforms in Audacity:\n\nYou can see that there are two lines of waveforms and also that they are pretty similar. That is normal, as you usually hear the same thing with your two ears, but the matter is in the subtle differences between the two. That's where directionality, richness, and all kinds of other effects lie.\n\nBut why stop at two? The list continues:\n• 2.1, as it is often called, means three channels: 2 for stereo and one for the LFE (\"low-frequency effects\" a.k.a.: \"bass\").\n• 5.1 is similar, with five directional channels (2 front, 1 center, 2 rear) and the LFE.\n\nSo channels are just separate \"recordings\" or \"streams\" of audio signals.\n\nFor images, there are quite a few parameters, but we'll check out only these:\n\nAn image consists of pixels, single points that have a single color. The resolution of an image determines how many columns and rows of pixels are in an image. In other words: an image has a width and a height.\n\nThis image shows the first 10 pixels in the first row.\n\nHere are some ballpark values for resolution:\n• \"HD\" or \"Full HD\" or \"1K\" or \"1080p\" means 1920x1080 pixels.\n• \"4K\" could mean a few values, but it should be about 3840x2160 pixels.\n• A regular 16mp photo you make of your cat is about 4608x3456 pixels.\n\nBit-depth represents the number of bits used for storing a single pixel's color value. This is the same balancing game, and you need to decide between quality or file size.\n\nThese last two sometimes are referred to as \"8 bit\" or \"10 bit\" respectively, especially when talking about videos. That means 8/10 bits per single color channel.\n\nSome image formats support an additional channel together with the red, green, and blue components: the alpha channel. The alpha channel determines how transparent a single pixel is, and it can have different bit-depths, it is usually either 1, 8 or 16 bits.\n\nIf the alpha channel is 1 bit, then the format can encode a pixel to be either transparent or non-transparent. If it is 8 or more bits, then the format can encode 256 or more steps of transparency.\n\nVideo data is built by single images shown right after each other. This brings in most attributes of images and a few more!\n\nSo a video has a that is its width and height.\n\nThen the first obvious parameter of a video is the , which defines how many images are shown in a second. Common values for this are 24, 25, 30, or 60.\n\nA video file also has a assigned to it, which is the format describing how all those images were compressed into this video file. There are many more attributes of videos, but this is a good start.\n\nCompression is a super important thing when it comes to video because you have thousands of images to keep together. If you aren't doing it in a smart way, then the resulting video will be very, very large.\n\nJust imagine a 2-minute video, with 30 fps. That means it will have 60 s * 2 * 30 fps = 3600 frames! I have just taken a screenshot of an HD video, which was 730 kbyte in JPEG format. Now 3600 frame * 730 kbyte equals 2.5 gigabytes!\n\nCan you imagine that? I hope not, and that's because compression brings that way, way down, to the level of tens of megabytes. These days a video of that size is quite high quality and about 2 hours long. Also, don't forget, that JPEG is already compressed, a single frame would be 6 mbyte when uncompressed. Now that 2-minute video would be 21 gigabytes if we'd store it uncompressed.\n\nStandard codecs such as H.264 and H.265 are doing very clever and complex operations to achieve high compression ratios with good quality.\n\nJust think about that, most frames in a video are quite similar, only containing small differences. So if we could only store that little difference between frames, we'd won a huge bonus! And that's just one of the many tricks codecs do.\n\nCodec designers are also exploiting the weaknesses and features of the human eye. Such as the fact that we are more sensitive to light intensity changes than color changes (say hello to YUV). And they can get away with lower quality details for parts that are moving fast, and so on.\n\nBecause why lose precious bits for things that you can't even notice?!\n\nThere are many codecs out there, with different goals in mind, although the majority focus on keeping the file size low.\n• H.264, H.265: These are the most common ones, with the widest support in browsers, phones, players, etc. It focuses on small file sizes with good quality. (At the cost of resource intensiveness.)\n• Apple ProRes, DNxHD: These are common formats for production. They focus on quality and ease of processing and not on file size.\n\nThe goal of audio codecs is the same as what we saw with the video codecs. It is just harder to demonstrate it as audio does not consist of single image frames but audio frames/packets. So an analog audio signal is of an almost infinite, or at least very high quality if you think of it.\n\nAt the lowest level, the speed and amplitude resolution is very high. We could say \"atomic\", as we need to measure and store the speed and direction of atoms. So if you want to store that exactly, that will require a super high-quality measurement, which will also result in a very high bitrate data stream.\n\nThankfully, the sound is at least not propagating with light speed so we can save quite a lot just by that fact. (There's no need for an extreme sampling rate.) Then our hearing is very limited if we take the previous paragraph as a scale, so we win there again. We don't need most of that high precision that is there.\n\nBut still, if we take our hearing capability and want to store raw audio data with about 44.1 kHz of sample rate with about 1 Mbit/sec bitrate, we'd still get quite a lot of data. Check the calculations in the audio bitrate section above.\n\nSo raw audio can be compressed further, which is what many popular codecs do. They also exploit the human senses, but this time the human ear. We started with the basics that the human ear has a limit on the frequencies it can detect. Therefore, we can save a lot by cutting out the range of frequencies outside our hearing range. Unless you are a bat, you are fine between 20-20khz! :)\n\nBut there are other tricks, for example, auditory masking. That means that the presence of one frequency can affect your capability to detect a different frequency. From the codec's viewpoint, it can skip encoding a few frequencies if it is smart enough to know which ones you'll not notice. I'm sure there are a lot more tricks, let me know if you know about a few more interesting ones!\n\nHere is a list of common codecs:\n• PCM (e.g. in a WAV container), FLAC: These are lossless formats.\n• MIDI: It is a funny format. It is like a music sheet that might sound different on different players or settings. It is usually not made from real audio data, but from recording a digital keyboard or as an output from an audio composing software.\n\nNow we got through the fundamental building blocks, the image, the video, the video codecs, and the audio codecs, and we reached the top of this iceberg: the containers.\n\nA container is a format specification, that combines all these streams into a single file format. It defines how to put all these data together, how to attach metadata (e.g. author, description, etc), how to synchronize these streams, and sometimes a container even contains indexes to aid seeking.\n\nSo, for example, a MOV container can contain an H.264 video stream and an AAC audio stream together.\n\nI will use these example materials as inputs in the following parts of this article. If you'd like to follow along, save these files for yourself!\n\nAnd we will make our own audio file by extracting the audio from the Big Buck Bunny movie! We'll use this file as an example, so after downloading the video file, please execute this:\n\nBy the middle of this article, you'll understand this command, but for now, just make sure to have the WAV file next to your video file to test out the commands later in the article.\n\nWe'll use these files in the following parts of this article. Therefore make sure to get them!\n\nFFmpeg is the name of the main binary and the project itself, but it is shipped together with two other binaries, ffplay and ffprobe.\n\nLet's check them out quickly, right in the command line!\n\nFFplay is a basic video player, that can be used for playing media. It's not a friendly video player, but it is a good testing ground for various things.\n\nTo execute it, just simply supply a media file:\n\nIf you want to test this exact command, you'll need to get the example files.\n\nFor example, it can be used to preview filters (we'll discuss those later), but let's see an example:\n\nFFprobe, as its name implies, is a tool for getting information about media files.\n\nWill return us some general information about the video file:\n\nI have abbreviated it heavily, as we'll check this out later.\n\nBut FFprobe is way more powerful than just this!\n\nWith the following command, we can get the same listing in JSON format, which is machine-readable!\n\nThe explanation of this command is the following:\n• \"-v error -hide_banner\": This part hides extra output, such as headers and the default build information.\n• \"-print_format json\": Obviously, this causes ffprobe to output a JSON.\n• \"-show_streams\" is the main switch that requests the stream information.\n\nIn this output, you can see three streams of data in this video file. The first (index: 0) is a video stream, that is an HD video with an H.264 codec. Then we have two audio streams, the first (index: 1) is a simple mp3 stream with stereo audio, and the second (index: 2) is an ac3 stream with 6 channels, most likely in an 5.1 configuration.\n\nI have removed quite a lot of output for brevity, but you can get way more information out of these streams, e.g. fps for the video stream and so on.\n\nOther than -show_streams, there are 3 more: -show_format, -show_packets and -show_frames. Unless you are really deep in the rabbit hole, you'll not need the last two, but -show_format could be useful:\n\nThis is an overview of \"what is this file\". As we see, it is a MOV file (format_name), with three streams (nb_streams), and it is 634 seconds long. Also, there are some tags where we can see the title, the artist, and other information.\n\nHere is a quick intro to how FFmpeg actually works!\n\nFor those who are just joining in: please get the example assets if you want to test out the commands shown in this chapter!\n\nFFmpeg opens the file, decodes it into memory, then encodes the in-memory packets back and puts them into some container: some output file. The term \"codec\" is a mix of the words \"coder & encoder\". Those are the magic parts before and after the \"decoded frames\".\n\nThe decoded frames are uncompressed images in-memory, e.g. the most basic pixel format for video frames is called \"rgb24\". This just stores red, green, and blue values right after each other in 3x8 bits, or 3x1 byte, which could hold 16m colors.\n\nThe importance of this is that other than a few exceptions, you can only manipulate or encode the decoded frames. So when we get to different audio/video filters or transcoding, you'll need the decoded frames for all that. But don't worry, FFmpeg does this automatically for you.\n\nSo you see and probably guessed, that FFmpeg must access the input data somehow. FFmpeg knows how to handle most media files, as the awesome people who develop FFmpeg and the related libraries made encoders and decoders for most formats available!\n\nDon't think that it is a trivial thing. Many formats are reverse engineered, a hard task requiring brilliant people.\n\nSo although we often refer to input files, the input could come from many sources, such as the network, a hardware device and so on. We'll learn more about that later on in this article.\n\nMany media files are containers for different streams, meaning that a single file might contain multiple streams of content.\n\nFor example, a .mov file might contain one or more streams:\n• audio tracks (e.g. for the different languages or audio formats such as stereo or 5.1)\n\nAll these are streams of data from the viewpoint of FFmpeg. Input files and their streams are numerically differentiated with a 0-based index. So, for example, 1:0 means the first(0) stream of the second(1) input file. We'll learn more about that later too!\n\nImportant to note that FFmpeg can open any number of input files simultaneously, and the filtering and mapping will decide what it will do with those. Again more on that later!\n\nAs we have seen in the previous section, streams are the fundamental building blocks of containers. So every input file must have at least one stream. And that's what you can list by the simple command for example.\n\nA stream might contain an audio format such as MP3, or a video format such as an H.264 stream.\n\nAlso, a stream, depending on the codec, might contain multiple \"things\". For example, an mp3 or a WAV stream might include various audio channels.\n\nSo the building block hierarchy, in this case is: File → Stream → Channels.\n\nOf course, an output could be a local file, but it doesn't need to be. It could be a socket, a stream and so on. In the same way as with inputs, you could have multiple outputs, and the mapping determines what goes into which output file.\n\nThe output also must have some format or container. Most of the time FFmpeg can and will guess that for us, mostly from the extension, but we can specify it too.\n\nMapping refers to the act of connecting input file streams with output file streams. So if you give 3 input files and 4 output files to FFmpeg, you must also define what should go to where.\n\nIf you give a single input and a single output, then FFmpeg will guess it for you without specifying any mapping, but make sure you know how exactly that happens, to avoid surprises. More on all that later!\n\nFiltering stands for the feature of FFmpeg to modify the decoded frames (audio or video). Other applications might call them effects, but i'm sure there is a reason why FFmpeg calls them filters.\n\nThere are two kinds of filtering supported by FFmpeg, simple and complex. In this article we'll only discuss the complex filters, as it is a superset of the simple filters, and this way, we avoid confusion and redundant content.\n\nSimple filters are a single chain of filters between a single input and output. Complex filters can have more chains of filters, with any number of inputs and outputs.\n\nThe following figure extends the previous overview image with the filtering module:\n\nA is built from , which are built from .\n\nSo a single filter does a single thing, for example, changes the volume. This filter is quite trivial, it has a single input, changes the volume, and it has a single output.\n\nFor video, we could check out the scale filter, which is also quite straightforward: it has a single input, scales the incoming frames, and it has a single output too.\n\nYou can chain these filters, meaning that you connect the output of one to the input of the next one! So you can have a volume filter after an echo filter, for example, and this way, you'll add echo, and then you change the volume.\n\nThis way, your chain will have a single input, and it will do several things with it and will output something at the end.\n\nNow, the \"complex\" comes in when you have multiple chains of these filters!\n\nBut before we go there, you should also know that some single filters might have multiple inputs or outputs!\n• The overlay filter puts 2 video streams above each other and will output a single video stream.\n• The split filter splits a single video stream into 2+ video streams (by copying).\n\nSo let's discuss a complex example from a bird's eye view! I have two video files, I want to put them above each other, and I want the output in two files/sizes, 720p and 1080p.\n\nNow, that's where complex filtering will be faithful to its name: to achieve this, you'll need several filter chains!\n\nAs you see, you can connect chains, and you can connect chains to output files. There is a rule that you can only consume a chain once, and that's why we used split instead of the same input for chains 2 and 3.\n\nThe takeaway is this: with complex filter graphs (and mapping), you can:\n\nFor those who are just joining in: please get the example assets if you want to test out the commands shown in this chapter!\n\nFinally, we arrived at FFmpeg, and trust me, we'll execute it quite a lot of times! Let's see how FFmpeg's command line options are organized, as that is the first tricky part we need to understand!\n\nFFmpeg mostly thinks about input and output files and their options together with global options. You specify input files with the \"-i\" flag followed by a file name. For the output file, specify it as-is without any preceding CLI (command line interface) flag.\n\nLet's specify just an input file:\n\nThe following image helps to understand the output:\n• First, you get the \"banner\", where you see the build information and lib versions. If you watch closely, you'll see the compilation flags, starting with --, e.g. --enable-shared.\n• Then you get the same output as we have seen with ffprobe earlier.\n• And then you get a complaint that there is no output file(s) specified. That's fine for now.\n\nYou can remove the banner here with \"-hide_banner\", but for brevity's sake I'll not include that anymore in the commands here, and I will leave it out from the outputs too.\n\nNow, let's get brave, and specify an output file!\n\nAs I've said earlier, the output file is understood by FFmpeg as it is just a filename. But more specifically, it is after the input(s) specifications, and it is not a value of any other switches.\n\nDon't be confused for now, but yes, FFmpeg can have as many inputs and outputs as you'd like. We'll cover that in more detail soon!\n\nBefore taking a look at the output, let me congratulate you! You have just converted a video file into an audio file, by keeping just the audio content!\n\nThis is how you transcode! Of course, you'll want to specify more parameters later on.\n\nSo, here is the output:\n\n(1) First, we have our input metadata printing, which we saw many times already.\n\n(2) Then we have something called \"stream mapping\". We forced FFmpeg into a decision situation, as we specified an input file with 1 video and 2 audio streams. We said we wanted an audio output (guessed from the .wav extension). But we didn't specify which audio stream we wanted, so let's see what FFmpeg decided:\n• \"Stream #0:2\" means \"The first input file's third stream\" or \"input file index 0's stream with index 2.\" This is the input.\n• \"-> #0:0\" means the first output file's first stream. This is the output.\n• Here you can learn more about how FFmpeg decide this.\n• Later on, we'll manually override the mapping.\n• Summary: FFmpeg decided to convert the third stream in the input file (the ac3 5.1 audio) into the first stream of the output file.\n\n(3) Then we have our output metadata information. This reveals what FFmpeg will output. It usually copies most of the metadata, and here you also see the container/format information too.\n\n(4) And then we see the output summary. For example, the transcoding was 181x faster than the playback speed. Nice!\n\nBefore going further, let's understand FFmpeg's command line arguments from a bird's eye view!\n\nIn the manual, you'll see this:\n\nThis is the general outline of how to specify inputs, outputs, input options, output options, and global options. The order matters, but it is easy to remember: global options, inputs and outputs. Also, i/o options come BEFORE the i/o specification.\n\nLet's put these into pseudo command line options, to understand it better:\n\nAs for the global options, these are the ones you might care about:\n• -y: To overwrite the output even if it exists.\n\nFor example, you can run this as many times as you want:\n\nAnd it will overwrite the output and be less verbose than earlier.\n\nWithout explaining the options themselves, let's just see some real-world examples with options:\n\nAnd here it is with two inputs and two outputs:\n\nWe saw above that this command:\n\n... will result in an audio file that contains one of the audio streams from the input video chosen by FFmpeg. This automatic stream selection is usually handy when it is trivial. For example, when you have one stream as input and one output file, you don't need to specify any mapping manually.\n\nBut in cases where it is not so trivial, you are usually better off manually specifying what you really want to do.\n\nThe following image summarises what our current situation is:\n\nThe video stream was not matched, as the output format was an audio file (.wav). But then FFmpeg chose Stream #2, because it has more channels.\n\nSo what if we'd like to get the stereo track instead? That is where mapping comes in! The mapping is a parameter of the OUTPUT file. Therefore the mapping arguments should come right before our output file definition!\n\nThe argument -map 0:1 means, that in the (since we specify it as an output option) we'd like to have 's (the first input file) !\n\nLet's see the relevant parts from the output!\n\nThe \"Stream #0:1 -> #0:0\" part means that we have successfully overridden the mapping, to get the mp3 stream (0:1) into our output! Also, the output metadata reveals that we'll get a stereo result instead of the 5.1 earlier.\n\nYou can have multiple outputs from a single input, let's see when that might be useful!\n\nLet's say, we want to extract BOTH audio streams into two separate WAV files! It is super easy:\n\nSee? I have just specified two output files with two mapping specifications! Also, I have sneaked in the \"-y\" to have it overwrite our previous file!\n\nLet's check out the relevant parts of the output!\n\nNow the mapping reveals two lines, as we have two outputs! And indeed, you'll get two .wav files as the output, one is stereo, and one is 5.1!\n\nThere might be several other reasons why you'd want to get multiple outputs. Let's briefly check out a few!\n\nWow, did you catch that? We just created a WAV and an mp3 in a single command line! I've reverted to the automatic stream selection for brevity's sake.\n\nA bit closer to real-life needs, you might want different output qualities:\n\nHere -b:a 320k means \"bitrate of audio should be around 320 kbit/sec\". So I have requested FFmpeg to make two mp3s for me, from the stereo stream of the input.\n\nChecking on the files, this is what we got:\n\nOne more common reason for having multiple outputs or using mapping is when we introduce filters into our pipeline, but that will be discussed later!\n\nNow you understand the foundations of how to communicate your basic requirements to FFmpeg via its command line! Great job! Now we can dive even deepert.\n\nIn this section, we will discover and even try out some common features of FFmpeg!\n\nFor those who are just joining in: please get the example assets if you want to test out the commands shown in this chapter!\n\nLet's see the common ways FFmpeg is fed with different data!\n\nOf course, you have already seen that if you have a local file on your filesystem, FFmpeg is happy to read it!\n\nThis command which is exactly the same as one of our previous ones just reads a local file. Really, that's it.\n\nDid you know, that FFmpeg can open a file directly on the network?!\n\nThe command above opens the file directly from the network and saves the first 5 seconds into a local file!\n\nI wanted to spare bandwidth for these awesome guys over renderfarming.net, so I added the duration flag: -t 5. FFmpeg doesn't even download the full video for this operation. Isn't that wonderful?!\n\nFFmpeg can also open your webcam!\n\nThis is an example command for Linux:\n\nThis would record 10 seconds of your webcam!\n\nAccessing the webcam happens differently on different platforms. Also specifying parameters is different for each platform, so for this reason, if you'd like to access your webcam with FFmpeg, please refer to the documentation:\n\nLet's record some audio directly from your microphone!\n\nThis command was meant to work on Linux, but you can check out how to do that on Microsoft Windows or macOS.\n\nFinally, FFmpeg can read from a pipe, and also output to a pipe.\n\nOn Linux, you could do something like this:\n\nThis command would use the cat program to simply read in the video file and output it to its standard output. Then this output is piped INTO FFmpeg, through its standard input. The combination \"-i -\" means \"read from standard input\". By the way, standard input would be your keyboard otherwise, if we wouldn't use any redirection here.\n\nThen we specify the required output format for FFmpeg, with \"-f wav\". This is needed because now we'll have no output file name, and FFmpeg will not be able to guess the format. Then we specify \"pipe:1\" as an output, meaning we'd like FFmpeg to output to its standard output.\n\nFrom then, we pipe the data into a program called \"pv\", it is just a metering tool, that dumps information on the throughput (from its stdin to its stdout). Finally, we redirect pv's output into a WAV file.\n\nYou might ask why we'd want to do that, why we talk about this. Piping can be useful if you build a complex pipeline from different programs or if you want to spare reading and writing to a local file.\n\nFor example, the node package fluent-ffmpeg can leverage this functionality by supplying input and output streams. For example, you can read from an S3 bucket and write to one directly.\n\nBut be warned, hell is awaiting you on that road. No kidding. You need to research the limitations of this technique. For example, many formats can not be streamed in this manner, as they need random access to the output data to write the indices at the beginning of the file after processing.\n\nFFmpeg can output into many protocols, from local file storage and ftp to message queue protocols all the way to streaming protocols.\n\nFor more information, check out the documentation here.\n\nIn this chapter, we'll be going to see how to transcode into audio with FFmpeg!\n\nFFmpeg is quite smart, and by the extension, it can determine which codec to use. If you specify \"audio.wav\" or \"audio.mp3\" for example, FFmpeg will use the appropriate codec to do the encoding.\n\nIt is perfectly guessing most of the time. But if you want to specify the format manually, then the \"-f\" flag is your friend.\n\nFor this, you might want to consult the list of formats:\n\nSo, these three commands will do exactly the same, but the last two requires the -f flag.\n\nIn most cases. you want to specify the target bitrate you expect from your codec to output. If you are unsure what bitrate is, please read this article's audio bitrate section.\n\nTo specify the audio bitrate, use the \"-b:a\" option with a corresponding value, e.g.:\n• -b:a 320k: For the mp3 codec this is considered high quality.\n\nYou may want to specify the sample rate to ensure quality or low output file size. Half the sample rate could mean half the output file size. If you are unsure what the sample rate is, please read the \"audio sample rate\" section of this article.\n\nTo specify the audio sample rate, use the \"-ar\" option with a corresponding value, e.g.:\n• -ar 22500: A bit of a compromise, not recommended for music, but for speech, it might be enough.\n• -ar 8000: Low quality, e.g. if you only want \"understandable\" speech.\n\nSetting the channel count can be useful, for example, if you have a stereo recording of a single person's speech. In that case, you might be content with just a mono output half the size of the original recording.\n\nIf you are unsure what an audio channel is, please read the \"audio channels\" section of this article.\n\nTo specify the channel count use the \"-ac\" option with a corresponding value, e.g.:\n\nThis is how you produce a high-quality output:\n\nCheck out this documentation about good quality audio transcoding too!.\n\nIf you want to convert audio into a lossless format, here are a few choices for you:\n\nIt's good if you know that flac results in a smaller file than WAV, as WAV doesn't actually compress by default:\n\nWAV is generally thought of as a lossless format, but keep in mind that the WAV container can contain lossy content too, but by default FFmpeg uses the pcm_s16le format, which is the 16 bit PCM, that could be understood as lossless.\n\nLearn more here and here.\n\nIn this chapter, we'll be going to see how to transcode a video file into the two most common formats!\n\nH264 is one of the most popular video codecs. Most devices, browsers and video players understand how to play it. It is efficient in storing video content, but as with most advanced video codecs, it is a resource intensive-process to encode and decode.\n\nA complete command line for a high-quality H.264 transcoding with high-quality AAC audio is the following:\n\nMake sure to understand this command and to customize it to match your needs.\n\nTo help you do that, let's dissect this command!\n\n-preset slow: libx264 has a lot of variables that you can be tune, and most of them balance the coding speed and the resulting file size. To make your life easier, there are presets by which you can easily declare what you need: small size or speed.\n\n-crf 22: This is the constant rate factor, the main option for setting image quality. It is a number between 0-51, where 0 is lossless, and 51 is the worst quality. Generally, you want something between 17 and 28. This is the option to tune the balance between image quality and file size. Check my comparison video here.\n\n-profile:v main -g 250 -pix_fmt yuv420p: These are advanced options, guaranteeing you a quite backward compatible result. (See this, this, and this.)\n\n-map 0:0 -map 0:1: You might not need this: these options are selecting the correct video and audio streams. In our case, we have two audio streams, and we need the stereo one to avoid some issues with our aac stream.\n\n-acodec aac: Select the AAC (Advanced Audio Coding) codec for the audio in the output. We need to be more specific than just -f for the format. We need to specify the audio codec here manually.\n\n-ar 44100: Set the audio sampling rate (learn more about that in previous chapters of this article).\n\n-b:a 320k: Set the audio bitrate (learn more about that in previous chapters of this article).\n\n30seconds_of_bb.mkv: The output file name. All the options since the last -i (or the last output file) considered to be a modifier for this output.\n\nFrom this, we understand that FFmpeg chose the mp3 stream from the input file because we told it to do so. (Remember, it has two audio streams in it, a stereo mp3 and a 5.1 ac3.) We also see that my machine could transcode with 35fps (0.58 times the playback speed), and our settings resulted in an average video bitrate of 4200 kbit/s.\n\nThe video bitrate is an interesting question in this mode. With the CRF option, we specify the \"constant visual quality\" we want. To reach a constant visual quality, the encoder works hard to guess how much it can compress certain parts of every frame, and the result of that guess defines the final average video bitrate.\n\nIf you want even better results with H.264, and you can afford a bit more processing time and a bit more complicated process, check out the 2-pass encoding instead of the constant rate factor method introduced above.\n\nTo learn more about these two different rate control methods, read the awesome Understanding Rate Control Modes article. And to learn more about the intricacies of H.264 encoding, check out the H264 encoding guide.\n\nFinally, later on, I will show you a comparison video that shows how different CRF values perform!\n\nH.265 is the successor of H.264, according to the official FFmpeg manual, it offers 25-50% bitrate savings while retaining the same visual quality.\n\nA complete command line for a high-quality H.265 transcoding with high-quality AAC audio is the following:\n\nAnd the result is:\n\nH.265 also has multiple rate control algorithms, I used the CRF method here. If you want to use a different rate control algorithm, then you may check out the H.265 encoding guide. Also, check out the next section, where I'll reveal how different CRF values perform!\n\nThis command is almost the same as what we used in the H.264 example above, so please refer to that section to understand the arguments.\n\nIf we compare H.264 and H.265 with our commands above, taking into account this 10-minute long video on my system, these are the results:\n\nI have created a video for your convenience, that shows the different crf values in action. The selected frame had some movement on it with the leaves in the bunny's hand. Movement is important with video codecs, as usually that's where quality losses are first visible.\n\nThis video shows how the different CRF values perform, from 0-51 with the H.264 and H.265 formats!\n\nIn this section, we'll achieve basic editing tasks by using FFmpeg only!\n\nWe'll just get a basic mp4 with default settings in these examples to keep things simple. But to encode the result in a proper, high quality way, please check the earlier sections where we learned how to encode into H.264 and H.265!\n\nTrimming from the beginning of the clip\n\nIt is possible to specify an in-point for a media file. By doing that, you essentially cut off the specified amount from the beginning of the input file. Therefore, FFmpeg will skip the first part of the file and only transcode the remainder!\n\nFor this, you need the \"-ss\" flag! The value can be specified in seconds (5 or 5.2) or as a timestamp (HOURS:MM:SS.MILLISECONDS).\n\nTo get the outro only, we could seek all the way to the end of the video! (It is 00:10:34.53 or 635 seconds long!)\n\nSeeking can be a bit tricky, so you may want to learn more about seeking here.\n\nTrimming from the end of the clip\n\nYou can also set an out-point for an input file, therefore shortening it. There are two options for this:\n• -to: This sets the timestamp where the input video should stop.\n\nThese two are mutually exclusive, and also they do the same if no -ss is specified. The value can be specified in seconds (5 or 5.2) or as a timestamp (HOURS:MM:SS.MILLISECONDS).\n\nAll four above commands result in exactly the same video. (For nerds: even the md5sum is the same.)\n\nBut let's see how they perform when we introduce seeking!\n\nThe first command will result in a 30 second long video, while the second command will be 20 seconds long only!\n\nThe figure below shows the difference:\n\nFFmpeg can do something I'm not aware of in any other popular NLE: it can edit videos without reencoding them!\n\nThe usual workflow is to decode the data frames (a/v) into memory, modify them as much as we like and then encode them into a new video file. The problem with this is that unless you work with raw or lossless codecs, you'll lose some quality in the process. Another issue with this approach is that it is computationally intensive.\n\nFor certain operations, you can configure FFmpeg, to keep the data frames intact, and this way, you can avoid decoding and encoding them! This is incredibly faster than regular transcoding, usually hundreds of times faster.\n\nThe \"certain operations\" are those that don't need to modify the data frames themselves. For example, you can cut and trim this way. Also, you can manipulate streams while keeping others, like you can replace the audio track without touching the video frames.\n\nAll this is a bit of magic, and there are caveats you need to prepare for, but it is good if you know about this, as it is often handy!\n\nThe trick lies in two options:\n\nRemove audio while keeping the video without reencoding\n\nHere, we used the \"-an\" option, which removes all audio streams. I remembered it as \"audio no\", but that is just my mnemonic:)\n\nLet's see how fast it was:\n\nSo It processed the whole 10 minutes of video in 2 seconds, 349x faster than playback, with 20950 fps!\n\nRemove video while keeping the audio without reencoding\n\nHere, we used the \"-vn\" option, which removes all video streams. I remembered it as \"video no\".\n\nLet's see how fast it was:\n\n776x faster than playback, finished in about a second, not bad!\n\nThere could be precision issues with seeking while you do this, so you may want to learn more about seeking and copying here.\n\nWe have removed audio and video already, but what if we want to swap them?\n\nThere is quite a lot going on in here, so let's explain the parts!\n\nFirst, we have two inputs (-i), meaning we are better off manually specifying the mapping. The command would work without the \"-map\" options, but it would ignore our second input.\n\nmeans that please use the first file's (first) video stream and the second file's (first) audio stream.\n\nWith , we require FFmpeg to copy the already encoded data packets without touching them. Therefore FFmpeg's work is mostly really just copying bytes, no decoding, no encoding.\n\nNot surprisingly, that's what we see in the stream mapping too:\n\nAnd since it is just copying, it was crazy fast, 162x of the playback speed, or almost 10k frames per second!\n\nExecute the exact same command, but with \"bbb_with_replaced_audio.mp4\" (.mp4 container instead of .mov) as an output file! You'll get this:\n\nThe message is quite clear. You can not have a pcm_s16le (raw WAV, say that 10 times:)) stream in an MP4 container. I'm not sure if it is FFmpeg's or the container's lack of support, but we need to solve this. If you run into this situation, you might consider two solutions:\n• Change the container: I've just tried MOV, and it worked.\n• Encode the audio: We still copy the video data, and encoding audio isn't that painful.\n\nI just showed you option #1, so let's see option #2:\n\nThis copies the video frames and encodes our WAV into a supported codec to be held in the mp4 container. You can refer back to the audio encoding section if you want to learn more about that.\n\nHere is the output:\n\n\"Only\" 36x faster than playback, 2176 fps, still not that bad!\n\nFFmpeg supports many audio and video filters. Currently, there are 116 audio and 286 video filters, but there are a bit more if we count the hardware accelerated ones too.\n\nSo how do we leverage them?\n\nThere are two ways to define filters, but I'm going to explain the complex filter, as the difference is not much, but it is more versatile. So there is a global option for FFmpeg, called: . With quite a weird syntax, you can specify all your filters and their parameters right after this option.\n\nYou can imagine the process with the following image:\n\nBasically, your filter graph can access all the inputs (-i a.mp4 -i b.mp4 -i c.mp4), and it can produce as many outputs as you like (-map might be needed).\n\nLet's take a look at a simple, basic example:\n\nAlthough is a global option, I like to put it after the inputs and before the outputs as it is a bit easier to overlook the whole command that way. Thankfully the command line parser of FFmpeg is smart enough, and it works.\n\nThe command above produces a 5-second-long video, where the text \"HELLO THERE\" is overlaid on the intro of Big Buck Bunny.\n\nLet's understand the weird format for specifying filters!\n\nWe'll go bottom-up, and we'll build it from there. So the most basic format is this:\n\nThe first thing before the first equal (=) sign is the filter's name, which is the drawtext filter in this case. Then we have our first argument, \"text\" and its value \"'HELLO THERE'\". Right after that, separated with a colon (:) comes the next argument, \"y\" with a value of \"20\".\n\nYou can guess what each of the text, y, x, fontsize and fontfile arguments do, as it is quite self-explaining. But especially for the first time, you'll heavily rely on the filtering documentation to understand every filter and every argument.\n\nAlso, several characters are reserved, such as: and a few others depending on your environment, so sooner or later you need to learn about escaping too.\n\nTo recap, our pipeline looks like this now:\n\nThis previous command is a single filter chain that consists of a single filter only, but you could have more filters put right after each other! It means that the output of one filter will be the input for the next! The way to do this is by separating them with a comma!\n\nLet's draw two boxes with the drawbox filter!\n\nSee? The output of the first filter is passed to the output of the second filter!\n\nNow, we have skipped something this far, because for simple uses FFmpeg is smart enough to do it for us. And this is the specification of a chain's input and output pads!\n\nLet's draw just a single rectangle for now:\n\nFFmpeg sees that the input for our filter chain is a single video file, and the output is a single output video file. Therefore, it safely assumes that we want that single input as the input of our single filter chain. And that single output should be the single output of our single output chain.\n\nThat's really nice, as, in simple situations like this, we don't need to assign and map inputs and outputs manually! But when we get more inputs, filter chains, or outputs, it is no longer possible. Therefore, we need to understand how to assign inputs and outputs!\n\nFirst of all, let's compare the following two command lines. They result in exactly the same result, but the second one represents what FFmpeg does internally (roughly):\n\nDo you see the difference? Before our filter chain, an \"input pad\" is defined: . The expected format between the square brackets is documented in the stream selection section of the official documentation, and this article already covered it.\n• 0:v: This means the first video stream of the first input file.\n• 0:v:0: Means exactly the same thing but in a long form.\n• 0:0: Means the first stream of the first input file (not recommended, as it could be anything in theory. It could be a subtitle stream, a thumbnail, a video or an audio stream...)\n• 0:a: This means the first audio stream of the first input file.\n• 0:a:0: Means exactly the same thing but in a long form.\n• 0:a:1: Means the second (index #1) audio stream of the first input file.\n\nSo we can specify which input file should be connected to which input of the filter graph!\n\nAlso, something similar is going on at the end! Do you see, the output pad definition at the end of our filter chain?\n\nThe naming here is easier, as basically you can specify any arbitrary name in here. It roughly means, \"please store the output data under this name\".\n\nAnd when you specify your output file, you can or need to map it by selecting one of your filter graph outputs! Therefore, we must add the -map \"[out_link_0]\" option before our output file.\n\nThis map option means this: \"Please save the data stream with this name into the following output file.\"\n\nThis is how you can visualize this input/output mapping:\n\nComing from the previous sections, you are now ready to see and understand an even more complicated configuration, which has multiple input files, output files, and filter chains!\n\nLet's see the output (two files next to each other):\n\nWe had two inputs, and we got two output files, an image, and a video, with a red rectangle on them, with a single command!\n\nAre you still here? I hope! Let's understand what happened in that crazy command! We have two input files:\n• -t 5 -i bbb_sunflower_1080p_60fps_normal.mp4: Our video file, but to make it quick, just the first five seconds of it\n\nThen the first thing to note is that we have two filter chains! They are separated with a \";\".\n\nOur first filter graph is this:\n• This requests the first input file as an input\n• Saves the output into the \"train_box\" output pad\n\nOur second filter graph is this:\n• This requests the second input file as an input\n• Saves the output into the \"bbb_box\" output pad\n\nAnd finally, we got two outputs, each mapping to one of the outputs of the filter graph:\n\nHere is the same thing visually:\n\nIf you are thinking about making it even more complex and making filter graphs that combine multiple inputs into one for example, you are on the right track! It is possible, and we will get to that!\n\nThis was the introduction to the filtering system and its syntax.\n\nNow let's get to know a few filters and make some interesting stuff!\n\nThe scale filter is a simple one, yet it is quite powerful!\n\nThe arguments speak for themselves, but a few things:\n• Specifying -1 to either width or height means rescaling while keeping the aspect ratio.\n• \"force_original_aspect_ratio\" can be , . Meaning it will increase or decrease the image to fit the specified bounding box while keeping the aspect ratio.\n\nWe have already covered this a little, so let's dive deeper!\n\nThis is what we used earlier:\n\nNow let's discover how to align the text!\n\nMany filters, including drawtext, support variables in some of its argument's values. If you scroll down in the documentation of drawtext, you'll find this:\n\nAnd after this part, you'll see many variables which you can include in your x and y variables!\n\nAnd this is what we'll get in the end:\n\nI need to mention one good trick that might not be obvious at first. So the variable is a tricky one, because different text will be of different height! E.g.: \"____\" and \"WWW\" will result in a different height.\n\nFor this reason, you do not always want to use text_h or even just a constant y=value expression but rather, you need to align text by its baseline. So just remember to use the \"ascent\" variable whenever you need to align text vertically!\n\nCheck out these two examples! Each has two drawtext filters printing \"_\" and \"_H\":\n\nSee? This is the difference between aligning the \"top left\" or the \"baseline\" of the text!\n\nOverlaying is a very interesting thing to do with FFmpeg. Let's jump right in!\n\nOf course, the overlay filter has a ton of options, but I wanted to demonstrate the easiest possible command line. We don't even need to mess with input/output pads, as FFmpeg automatically understands the situation: two inputs for the overlay filter and its single output into a single output.\n\nBut just to exercise, we could have executed it like this:\n\nAnd this would result in the same output! Check it out, now I have specified the two inputs for the overlay: !\n\nLet's align the smiley into the center!\n\nAs we have seen with the drawtext, the overlay filter's arguments also support a few dynamic variables. We'll use those to achieve what we want!\n\nI want to make it smaller, and I also want to blur it!\n\nNow pause for a minute, and think about it, how you'd do that?!\n\nFor this we needed to have two filter graphs!\n\nThe first one is this:\n• Then the scaled output is also blurred.\n• Then the output is saved into the output pad named \"smiley\".\n\nThen, we have our second filter graph:\n• This takes as input the first input file (the video).\n• This also takes as input the output pad named \"smiley\". (We are connecting two chains this time!)\n• Then the overlay filter does its overlaying thing, and we trust FFmpeg to pair the unnamed output with the single output file we specified.\n\nLet's do one more, a really complicated one!\n\nLet's have the outro overlaid over the intro!\n\nWe could have achieved it in several ways, e.g. we could use the trim filter, but to keep it easy, we just open the same file twice and seek/trim them.\n• -t 5 -i bbb_sunflower_1080p_60fps_normal.mp4: Open the video, and keep the first five seconds of it.\n• -t 5 -ss 00:09:40 -i bbb_sunflower_1080p_60fps_normal.mp4: Open the same video again, but seek to the end and keep five seconds from there.\n\nThen we have two filter graphs again, one scales down the outro, and the second is just an overlay.\n\nAre you excited?:) I hope these made-up examples opened up your eye for the possibilities, and I hope you'll create very creative stuff with this knowledge!\n\nIn this section, we'll use chroma keying to remove the background from Big Buck Bunny's intro, and then we will put the transparent logo over the original video, as if it would be some kind of a logo overlay!\n\nSo just to recap, Big Buck Bunny's first few seconds are like this:\n\nAnd this is the result:\n\nAlso, the butterfly moves its wings repeatedly!\n• -ss 0.5 -t 2 -i bbb_sunflower_1080p_60fps_normal.mp4: We read in the intro from 0.5 to 2.5 seconds.\n• -ss 10 -i bbb_sunflower_1080p_60fps_normal.mp4: We read in the video, starting from the 10th second.\n\nThen we have two filter graphs, the first being this:\n\nAs we see, we have three filters in here!\n• chromakey: This one takes a color and a few parameters as input, and outputs transparent frames. The specified color + the blended areas will be the transparent sections. In our case we replaced the white-ish (#fdfdfd) background color with transparency.\n• scale: We resize the full 1080p image into something around 300px high.\n• loop: With the loop filter, we repeat all the 2 seconds worth of 120 frames (60*2) over and over again, to have the butterfly move its wings continuously.\n\nAnd then, finally we have the second filter graph:\n\nNothing fancy, just an overlay of the original video and our chrome keyed intro.\n\nYou might want to check out a few more filters, that I didn't cover here.\n\nHere are just a few interesting ones:\n\nIn this chapter, we'll be going to check out some audio manipulation techniques with FFmpeg!\n\nFirst of all, let's see our example file:\n\nIt is a voice recording, and it is intentionally... well, quite bad.\n\nFrom the waveform, it is obvious that there are very different volume ranges in it. This is an example recording where each sentence was read in different strengths: \"normal\", \"whisper\" or \"powerful\", this is why you see repeating patterns of amplitude ranges on the image.\n\nIt isn't visible, but it has some noise too, and of course, it is not normalized or enhanced in any way. Yet.\n\nPlease note that there are different scenarios, requirements, and ways to enhance audio. This is a simplified method to show the outline of the process in this article. I'm not an audio engineer, although I have some experience in the area. So if you know it better, feel free to fine-tune it for yourself even more, or contact me and recommend improvements!\n\nI'm showing an example here with a very rough input, one that you'd just reject in real life as it would be useless due to its quality. But it is an excellent example to show the different steps of the enhancing process and to see what can be done to it!\n\nThe following steps are built upon each other, and we'll reach the complete command at the end!\n\nDon't forget that these settings are specific to this voice recording. Sadly this can not be generalized too much.\n\nA gate is like a switch that opens only if the signal is stronger than the threshold. So if the signal level is lower than the threshold, it cuts to complete silence. Although you might soften or delay this cut with the knee, attack, and release arguments.\n\nWe'll use this filter as a basic noise reduction method now! This helps us remove the noise between words and sentences by cutting it to silence. It doesn't remove noise in any other way, e.g. it doesn't touch the static on the voice itself.\n\nAnd let's see it:\n\nAs you can see, the \"silent\" parts were attenuated heavily, while the above-the-threshold parts remained similar. Those parts were still affected by the knee, attack, and release arguments determining how hard (knee) and quick (attack/release) the cut is.\n\nI've left a quite high release timeout here to avoid sudden dips in the amplitude.\n\nThis is where we are right now:\n\nThe silent parts are more silent than before, but still, the amplitude range or the dynamic range is quite high. You must change your volume levels to hear everything and void blowing your speakers/brain out.\n\nBefore fixing that, let's do a bit more housekeeping. Let's do some equalization and frequency filtering!\n\nWe'll use these filters:\n\nThis command gradually attenuates frequencies below 100hz, as there are not much valuable content in there, but it can really lower the clarity of the speech.\n\nThen we do the same, but for frequencies above 10 kHz. This is mostly needed because we have a lot of high-frequency noise, so this is a workaround for those. Also, a male voice is generally deeper than a woman's, so you might want to pay attention to how low you can put the bar.\n\nThen comes anequalizer, which has a crazy an exceptional way of setting its arguments:\n• at 250hz with a width of 100hz boost by 2 db, with Chebyshev type 1 filter on channel 0.\n• at 700hz with a width of 500hz attenuate by 5 db, with Chebyshev type 1 filter on channel 0.\n• at 2000hz with a width of 1000hz attenuate by 2 db, with Chebyshev type 1 filter on channel 0.\n\nI agree. You might have used a friendlier equalizer in your life than this one:)\n\nThose values are based on experimentation and common recommendations for voice. Feel free to tune it for your own needs!\n\nLet's compare the frequency plots before and after:\n\nTip: To see the frequency plot in Audacity, open a file, select all, and choose Analyze → Plot spectrum!\n\nThe compressor filter applies dynamic range compression on the incoming audio data. To simplify this, the compressor varies the attenuation based on the incoming signal level. Basically, when you watch a badly mastered movie, this is what you are doing. When it is way too loud in some action scene, you reach for the remote control or mouse to lower the volume, but in the next moment, you will not hear what your heroes are saying, so you increase it back again.\n\nDynamic range compression roughly does the same. You may set it up in a way so that it would attenuate louder parts, therefore keeping the overall volume range relatively small.\n\nIt often happens that performers on the stage use a high dynamic range. Many performers will shout at one moment and then whisper in the next to increase drama or keep the attention. If you want to avoid manually adjusting the volume in real-time (while blowing off your speakers and pulling your hair out), then a compressor will save you in these situations!\n\nThis is why our example audio consists of different speaking strengths, so that we could see the dramatic effect of this filter.\n\nAnd let's compare the result of this with the original waveform!\n\nFirst, sets the input gain. It is 1 by default, but since our example, audio is extremely silent at places, we boost up the whole thing before processing.\n\nThen defines that everything above 0.025 should be attenuated.\n\nBased on the image below, I've decided to cut at this point, as this is above most of the whispering, which cuts hard pops and \"s\"-es even in the \"whisper zone\".\n\nThen means 1:20 in attenuation ratio, which means that if the level rises 20 dB above the threshold, it will be only 1 dB above the line after the attenuation. Basically, this is a very strong compression ratio, it is almost a limiter.\n\nThis far, we boosted the signal, then turned down everything that was above our \"whisper line\" with a quite strong ratio, and now, everything is basically at the whisper level, even the parts that are shouting.\n\nFinally, with the we just bring back everything to the level where the \"normal\" parts were before.\n\nLet's take a look back now, to understand why we used the gate and did the equalization before the compressor.\n\nGenerally, you want to remove unneeded parts and frequencies before compression, as the compressor will likely increase those too! So by removing most of the noise in the gaps, we avoided to increase them too! And the same goes for the high- and lowpass filtering.\n\nNow, if we want to make the result a bit louder, we could increase the previous step's argument, or leverage the volume filter.\n\nWhile we are at it, let's cut the first 4 seconds too with .\n\nExcuse me for that title:)\n\nSo as I've described earlier, compression can amplify the noises, so you might want to run the result through a gate again:\n\nIn this case, I've used a softer gate, with . Because of this, I could use shorter attack and release delays too, as the attenuation is not that strong, it isn't causing hard dips in the audio.\n\nPutting it all together\n\nJust a single command could have achieved all the steps above:\n\nI just copy-pasted all the filters right after each other with a comma between them.\n\nIsn't it beautiful? Yeah, it isn't, but it is very practical:)\n\nFor the last time, check out the difference:\n\nIt has less noise, more clear voice, and a small volume range. Therefore it is easy on your ears!\n\nYou might want to check out a few more filters that I didn't cover here.\n\nHere are just a few interesting ones:\n\nFor your convenience, let me list the most important documentations that might be important for you! Most of these were already linked many times in this article.\n\nIf you got this far from top to bottom, then you are a true hero! I hope you enjoyed this, and I also hope that it inspired you to create something awesome with FFmpeg! Please consider donating to FFmpeg – they are fantastic.\n\n\n\nIf you're looking to take your creative projects to the next level, check out our products - Creative Editor SDK, Video Editor SDK, and Photo Editor SDK. These versatile tools empower you to bring your vision to life, whether you're editing images, crafting stunning videos, or unleashing your artistic talents.\n\n\n\nThanks for reading! Let us know what you think on Twitter! To stay in the loop, subscribe to our Newsletter."
    }
]