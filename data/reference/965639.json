[
    {
        "link": "https://spacy.io/usage/linguistic-features",
        "document": "After tokenization, spaCy can parse and tag a given . This is where the trained pipeline and its statistical models come in, which enable spaCy to make predictions of which tag or label most likely applies in this context. A trained component includes binary data that is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following “the” in English is most likely a noun. Linguistic annotations are available as attributes. Like many NLP libraries, spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore to its name:\n• Lemma: The base form of the word.\n• is alpha: Is the token an alpha character?\n• is stop: Is the token part of a stop list, i.e. the most common words of the language? Most of the tags and labels look pretty abstract, and they vary between languages. will show you a short description – for example, returns “verb, 3rd person singular present”. Using spaCy’s built-in displaCy visualizer, here’s what our example sentence and its dependencies look like: For a list of the fine-grained and coarse-grained part-of-speech tags assigned by spaCy’s models across different languages, see the label schemes documented in the models directory.\n\nThe context manager lets you merge and split tokens. Modifications to the tokenization are stored and performed all at once when the context manager exits. To merge several tokens into one single token, pass a to . An optional dictionary of lets you set attributes that will be assigned to the merged token – for example, the lemma, part-of-speech tag or entity type. By default, the merged token will receive the same attributes as the merged span’s root.\n• Inspect the attribute with and without setting the . You’ll see that the lemma defaults to “New”, the lemma of the span’s root.\n• Overwrite other attributes like the . Since “New York” is also recognized as a named entity, this change will also be reflected in the . If you need to merge named entities or noun chunks, check out the built-in and pipeline components. When added to your pipeline using , they’ll take care of merging the spans automatically. If an attribute in the is a context-dependent token attribute, it will be applied to the underlying . For example , or only apply to a word in context, so they’re token attributes. If an attribute is a context-independent lexical attribute, it will be applied to the underlying , the entry in the vocabulary. For example, or apply to all words of the same spelling, regardless of the context. If you’re trying to merge spans that overlap, spaCy will raise an error because it’s unclear how the result should look. Depending on the application, you may want to match the shortest or longest possible span, so it’s up to you to filter them. If you’re looking for the longest non-overlapping span, you can use the helper: The method allows splitting one token into two or more tokens. This can be useful for cases where tokenization rules alone aren’t sufficient. For example, you might want to split “its” into the tokens “it” and “is” – but not the possessive pronoun “its”. You can write rule-based logic that can find only the correct “its” to split, but by that time, the will already be tokenized. This process of splitting a token requires more settings, because you need to specify the text of the individual tokens, optional per-token attributes and how the tokens should be attached to the existing syntax tree. This can be done by supplying a list of – either the token to attach the newly split token to, or a tuple if the newly split token should be attached to another subtoken. In this case, “New” should be attached to “York” (the second split subtoken) and “York” should be attached to “in”.\n• Assign different attributes to the subtokens and compare the result.\n• Change the heads so that “New” is attached to “in” and “York” is attached to “New”.\n• Split the token into three tokens instead of two – for example, . Specifying the heads as a list of or tuples allows attaching split subtokens to other subtokens, without having to keep track of the token indices after splitting. Attach this token to the second subtoken (index ) that will be split into, i.e. “York”. Attach this token to in the original , i.e. “in”. If you don’t care about the heads (for example, if you’re only running the tokenizer and not the parser), you can attach each subtoken to itself: When splitting tokens, the subtoken texts always have to match the original token text – or, put differently always needs to hold true. If this wasn’t the case, splitting tokens could easily end up producing confusing and unexpected results that would contradict spaCy’s non-destructive tokenization policy. If you’ve registered custom extension attributes, you can overwrite them during tokenization by providing a dictionary of attribute names mapped to new values as the key in the . For merging, you need to provide one dictionary of attributes for the resulting merged token. For splitting, you need to provide a list of dictionaries with custom attributes, one per split subtoken. To set extension attributes during retokenization, the attributes need to be registered using the method and they need to be writable. This means that they should either have a default value that can be overwritten, or a getter and setter. Method extensions or extensions with only a getter are computed dynamically, so their values can’t be overwritten. For more details, see the extension attribute docs.\n• Add another custom extension – maybe ? – and overwrite it.\n• Change the extension attribute to use only a function. You should see that spaCy raises an error, because the attribute is not writable anymore.\n• Rewrite the code to split a token with . Remember that you need to provide a list of extension attribute values as the property, one for each split subtoken.\n\nSimilarity is determined by comparing word vectors or “word embeddings”, multi-dimensional meaning representations of a word. Word vectors can be generated using an algorithm like word2vec and usually look like this: To make them compact and fast, spaCy’s small pipeline packages (all packages that end in ) don’t ship with word vectors, and only include context-sensitive tensors. This means you can still use the methods to compare documents, spans and tokens – but the result won’t be as good, and individual tokens won’t have any vectors assigned. So in order to use real word vectors, you need to download a larger pipeline package: Pipeline packages that come with built-in word vectors make them available as the attribute. and will default to an average of their token vectors. You can also check if a token has a vector assigned, and get the L2 norm, which can be used to normalize vectors.\n• has vector: Does the token have a vector representation?\n• Vector norm: The L2 norm of the token’s vector (the square root of the sum of the values squared) The words “dog”, “cat” and “banana” are all pretty common in English, so they’re part of the pipeline’s vocabulary, and come with a vector. The word “afskfsd” on the other hand is a lot less common and out-of-vocabulary – so its vector representation consists of 300 dimensions of , which means it’s practically nonexistent. If your application will benefit from a large vocabulary with more vectors, you should consider using one of the larger pipeline packages or loading in a full vector package, for example, , which includes 685k unique vectors. spaCy is able to compare two objects, and make a prediction of how similar they are. Predicting similarity is useful for building recommendation systems or flagging duplicates. For example, you can suggest a user content that’s similar to what they’re currently looking at, or label a support ticket as a duplicate if it’s very similar to an already existing one. Each , , and comes with a method that lets you compare it with another object, and determine the similarity. Of course similarity is always subjective – whether two words, spans or documents are similar really depends on how you’re looking at it. spaCy’s similarity implementation usually assumes a pretty general-purpose definition of similarity.\n• Compare two different tokens and try to find the two most dissimilar tokens in the texts with the lowest similarity score (according to the vectors).\n• Compare the similarity of two objects, entries in the vocabulary. You can get a lexeme via the attribute of a token. You should see that the similarity results are identical to the token similarity. What to expect from similarity results Computing similarity scores can be helpful in many situations, but it’s also important to maintain realistic expectations about what information it can provide. Words can be related to each other in many ways, so a single “similarity” score will always be a mix of different signals, and vectors trained on different data can produce very different results that may not be useful for your purpose. Here are some important considerations to keep in mind:\n• There’s no objective definition of similarity. Whether “I like burgers” and “I like pasta” is similar depends on your application. Both talk about food preferences, which makes them very similar – but if you’re analyzing mentions of food, those sentences are pretty dissimilar, because they talk about very different foods.\n• The similarity of and objects defaults to the average of the token vectors. This means that the vector for “fast food” is the average of the vectors for “fast” and “food”, which isn’t necessarily representative of the phrase “fast food”.\n• Vector averaging means that the vector of multiple tokens is insensitive to the order of the words. Two documents expressing the same meaning with dissimilar wording will return a lower similarity score than two documents that happen to contain the same words while expressing different meanings. is a library developed by us that builds on top of spaCy and lets you train and query more interesting and detailed word vectors. It combines noun phrases like “fast food” or “fair game” and includes the part-of-speech tags and entity labels. The library also includes annotation recipes for our annotation tool Prodigy that let you evaluate vectors and create terminology lists. For more details, check out our blog post. To explore the semantic similarities across all Reddit comments of 2015 and 2019, see the interactive demo. Custom word vectors can be trained using a number of open-source libraries, such as Gensim, FastText, or Tomas Mikolov’s original Word2vec implementation. Most word vector libraries output an easy-to-read text-based format, where each line consists of the word followed by its vector. For everyday use, we want to convert the vectors into a binary format that loads faster and takes up less space on disk. The easiest way to do this is the command-line utility. This will output a blank spaCy pipeline in the directory , giving you access to some nice Latin vectors. You can then pass the directory path to or use it in the of your config when you train a model. To help you strike a good balance between coverage and memory usage, spaCy’s class lets you map multiple keys to the same row of the table. If you’re using the command to create a vocabulary, pruning the vectors will be taken care of automatically if you set the flag. You can also do it manually in the following steps:\n• Start with a word vectors package that covers a huge vocabulary. For instance, the package provides 300-dimensional GloVe vectors for 685k terms of English.\n• If your vocabulary has values set for the attribute, the lexemes will be sorted by descending probability to determine which vectors to prune. Otherwise, lexemes will be sorted by their order in the .\n• Call with the number of vectors you want to keep. reduces the current vector table to a given number of unique entries, and returns a dictionary containing the removed words, mapped to tuples, where is the entry the removed word was mapped to and the similarity score between the two words. In the example above, the vector for “Shore” was removed and remapped to the vector of “coast”, which is deemed about 73% similar. “Leaving” was remapped to the vector of “leaving”, which is identical. If you’re using the command, you can set the option to easily reduce the size of the vectors as you add them to a spaCy pipeline: This will create a blank spaCy pipeline with vectors for the first 10,000 words in the vectors. All other words in the vectors are mapped to the closest vector among those retained. The attribute is a read-only numpy or cupy array (depending on whether you’ve configured spaCy to use GPU memory), with dtype . The array is read-only so that spaCy can avoid unnecessary copy operations where possible. You can modify the vectors via the or table. Using the method is often the easiest approach if you have vectors in an arbitrary format, as you can read in the vectors with your own logic, and just set them with a simple loop. This method is likely to be slower than approaches that work with the whole vectors table at once, but it’s a great approach for once-off conversions before you save out your object to disk.\n\nEvery language is different – and usually full of exceptions and special cases, especially amongst the most common words. Some of these exceptions are shared across languages, while others are entirely specific – usually so specific that they need to be hard-coded. The module contains all language-specific data, organized in simple Python files. This makes the data easy to update and extend. The shared language data in the directory root includes rules that can be generalized across languages – for example, rules for basic punctuation, emoji, emoticons and single-letter abbreviations. The individual language data in a submodule contains rules that are only relevant to a particular language. It also takes care of putting together all components and creating the subclass – for example, or . The values are defined in the . List of most common words of a language that are often useful to filter out, for example “and” or “I”. Matching tokens will return for . Special-case rules for the tokenizer, for example, contractions like “can’t” and abbreviations with punctuation, like “U.K.”. Regular expressions for splitting tokens, e.g. on punctuation or special characters like emoji. Includes rules for prefixes, suffixes and infixes. Character classes to be used in regular expressions, for example, Latin characters, quotes, hyphens or icons. Custom functions for setting lexical attributes on tokens, e.g. , which includes language-specific words like “ten” or “hundred”. Functions that compute views of a object based on its syntax. At the moment, only used for noun chunks. If you want to customize multiple components of the language data or add support for a custom language or domain-specific “dialect”, you can also implement your own language subclass. The subclass should define two attributes: the (unique language code) and the defining the language data. For an overview of the available attributes that can be overwritten, see the documentation. The decorator lets you register a custom language class and assign it a string name. This means that you can call with your custom language name, and even train pipelines with it and refer to it in your training config. After registering your custom language class using the registry, you can refer to it in your training config. This means spaCy will train your pipeline using the custom subclass. In order to resolve to your subclass, the registered function needs to be available during training. You can load a Python file containing the code using the argument:"
    },
    {
        "link": "https://realpython.com/natural-language-processing-spacy-python",
        "document": "spaCy is a robust open-source library for Python, ideal for natural language processing (NLP) tasks. It offers built-in capabilities for tokenization, dependency parsing, and named-entity recognition, making it a popular choice for processing and analyzing text. With spaCy, you can efficiently represent unstructured text in a computer-readable format, enabling automation of text analysis and extraction of meaningful insights.\n\nBy the end of this tutorial, you’ll understand that:\n• You can use spaCy for natural language processing tasks such as part-of-speech tagging, and named-entity recognition.\n• spaCy is often preferred over NLTK for production environments due to its performance and modern design.\n• spaCy provides integration with transformer models, such as BERT.\n• You handle tokenization in spaCy by breaking text into tokens using its efficient built-in tokenizer.\n• Dependency parsing in spaCy helps you understand grammatical structures by identifying relationships between headwords and dependents.\n\nUnstructured text is produced by companies, governments, and the general population at an incredible scale. It’s often important to automate the processing and analysis of text that would be impossible for humans to process. To automate the processing and analysis of text, you need to represent the text in a format that can be understood by computers. spaCy can help you do that.\n\nIf you’re new to NLP, don’t worry! Before you start using spaCy, you’ll first learn about the foundational terms and concepts in NLP. You should be familiar with the basics in Python, though. The code in this tutorial contains dictionaries, lists, tuples, loops, comprehensions, object oriented programming, and lambda functions, among other fundamental Python concepts.\n\nIn this section, you’ll install spaCy into a virtual environment and then download data and models for the English language. You can install spaCy using , a Python package manager. It’s a good idea to use a virtual environment to avoid depending on system-wide packages. To learn more about virtual environments and , check out Using Python’s pip to Manage Your Projects’ Dependencies and Python Virtual Environments: A Primer. First, you’ll create a new virtual environment, activate it, and install spaCy. Select your operating system below to learn how: With spaCy installed in your virtual environment, you’re almost ready to get started with NLP. But there’s one more thing you’ll have to install: There are various spaCy models for different languages. The default model for the English language is designated as . Since the models are quite large, it’s best to install them separately—including all languages in one package would make the download too massive. Once the model has finished downloading, open up a Python REPL and verify that the installation has been successful: If these lines run without any errors, then it means that spaCy was installed and that the models and data were successfully downloaded. You’re now ready to dive into NLP with spaCy!\n\nIn this section, you’ll use spaCy to deconstruct a given input string, and you’ll also read the same text from a file. First, you need to load the language model instance in spaCy: The function returns a callable object, which is commonly assigned to a variable called . To start processing your input, you construct a object. A object is a sequence of objects representing a lexical token. Each object has information about a particular piece—typically one word—of text. You can instantiate a object by calling the object with the input string as an argument: \"This tutorial is about Natural Language Processing in spaCy.\" ['This', 'tutorial', 'is', 'about', 'Natural', 'Language', In the above example, the text is used to instantiate a object. From there, you can access a whole bunch of information about the processed text. For instance, you iterated over the object with a list comprehension that produces a series of objects. On each object, you called the attribute to get the text contained within that token. You won’t typically be copying and pasting text directly into the constructor, though. Instead, you’ll likely be reading it from a file: ['This', 'tutorial', 'is', 'about', 'Natural', 'Language', In this example, you read the contents of the file with the method of the object. Since the file contains the same information as the previous example, you’ll get the same result.\n\nSentence detection is the process of locating where sentences start and end in a given text. This allows you to you divide a text into linguistically meaningful units. You’ll use these units when you’re processing your text to perform tasks such as part-of-speech (POS) tagging and named-entity recognition, which you’ll come to later in the tutorial. In spaCy, the property is used to extract sentences from the object. Here’s how you would extract the total number of sentences and the sentences themselves for a given input: \" company. He is interested in learning\" He is interested in learning... In the above example, spaCy is correctly able to identify the input’s sentences. With , you get a list of objects representing individual sentences. You can also slice the objects to produce sections of a sentence. You can also customize sentence detection behavior by using custom delimiters. Here’s an example where an ellipsis ( ) is used as a delimiter, in addition to the full stop, or period ( ): \"Gus, can you, ... never mind, I forgot\" \" what I was saying. So, do you think\" \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\" never mind, I forgot what I was saying. So, do you think we should ... For this example, you used the decorator to define a new function that takes a object as an argument. The job of this function is to identify tokens in that are the beginning of sentences and mark their attribute to . Once done, the function must return the object again. Then, you can add the custom boundary function to the object by using the method. Parsing text with this modified object will now treat the word after an ellipse as the start of a new sentence.\n\nBuilding the container involves tokenizing the text. The process of tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as objects. As you’ve already seen, with spaCy, you can print the tokens by iterating over the object. But objects also have other attributes available for exploration. For instance, the token’s original index position in the string is still available as an attribute on : \" company. He is interested in learning\" In this example, you iterate over , printing both and the attribute, which represents the starting position of the token in the original text. Keeping this information could be useful for in-place word replacement down the line, for example. spaCy provides various other attributes for the class: Text with Whitespace Is Alphanum? Is Punctuation? Is Stop Word? In this example, you use f-string formatting to output a table accessing some common attributes from each in :\n• prints the token text along with any trailing space, if present.\n• indicates whether the token consists of alphabetic characters or not.\n• indicates whether the token is a punctuation symbol or not.\n• indicates whether the token is a stop word or not. You’ll be covering stop words a bit later in this tutorial. As with many aspects of spaCy, you can also customize the tokenization process to detect tokens on custom characters. This is often used for hyphenated words such as London-based. To customize tokenization, you need to update the property on the callable object with a new object. To see what’s involved, imagine you had some text that used the symbol instead of the usual hyphen ( ) as an infix to link words together. So, instead of London-based, you had London@based: \" company. He is interested in learning\" In this example, the default parsing read the London@based text as a single token, but if you used a hyphen instead of the symbol, then you’d get three tokens. To include the symbol as a custom infix, you need to build your own object: In this example, you first instantiate a new object. To build a new , you generally provide it with:\n• : A storage container for special cases, which is used to handle cases like contractions and emoticons.\n• : A function that handles preceding punctuation, such as opening parentheses.\n• : A function that handles succeeding punctuation, such as closing parentheses.\n• : A function that handles non-whitespace separators, such as hyphens.\n• : An optional Boolean function that matches strings that should never be split. It overrides the previous rules and is useful for entities like URLs or numbers. The functions involved are typically regex functions that you can access from compiled regex objects. To build the regex objects for the prefixes and suffixes—which you don’t want to customize—you can generate them with the defaults, shown on lines 5 to 10. To make a custom infix function, first you define a new list on line 12 with any regex patterns that you want to include. Then, you join your custom list with the object’s attribute, which needs to be cast to a list before joining. You want to do this to include all the existing infixes. Then you pass the extended tuple as an argument to to obtain your new regex object for infixes. When you call the constructor, you pass the method on the prefix and suffix regex objects, and the function on the infix regex object. Now you can replace the tokenizer on the object. After that’s done, you’ll see that the symbol is now tokenized separately.\n\nLemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form, or root word, is called a lemma. For example, organizes, organized and organizing are all forms of organize. Here, organize is the lemma. The inflection of a word allows you to express different grammatical categories, like tense (organized vs organize), number (trains vs train), and so on. Lemmatization is necessary because it helps you reduce the inflected forms of a word so that they can be analyzed as a single item. It can also help you normalize the text. spaCy puts a attribute on the class. This attribute has the lemmatized form of the token: \" and several internal talks at his workplace.\" In this example, you check to see if the original word is different from the lemma, and if it is, you print both the original word and its lemma. You’ll note, for instance, that reduces to its lemma form, . If you don’t lemmatize the text, then and will be counted as different tokens, even though they both refer to the same concept. Lemmatization helps you avoid duplicate words that may overlap conceptually.\n\nYou can now convert a given text into tokens and perform statistical analysis on it. This analysis can give you various insights, such as common words or unique words in the text: \" working for a London-based Fintech company. He is\" \" There is a developer conference happening on 21 July\" ' 2019 in London. It is titled \"Applications of Natural' \" available at +44-1234567891. Gus is helping organize it.\" \" He keeps organizing local Python meetups and several\" \" internal talks at his workplace. Gus is also presenting\" ' a talk. The talk will introduce the reader about \"Use' \" Apart from his work, he is very passionate about music.\" \" Gus is learning to play the Piano. He has enrolled\" \" himself in the weekend batch of Great Piano Academy.\" \" Great Piano Academy is situated in Mayfair or the City\" \" of London and has world-class piano instructors.\" By looking just at the common words, you can probably assume that the text is about , , and . That’s a significant finding! If you can just look at the most common words, that may save you a lot of reading, because you can immediately tell if the text is about something that interests you or not. That’s not to say this process is guaranteed to give you good results. You are losing some information along the way, after all. That said, to illustrate why removing stop words can be useful, here’s another example of the same text including stop words: [('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)] Four out of five of the most common words are stop words that don’t really tell you much about the summarized text. This is why stop words are often considered noise for many applications.\n\nRule-based matching is one of the steps in extracting information from unstructured text. It’s used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech). While you can use regular expressions to extract entities (such as phone numbers), rule-based matching in spaCy is more powerful than regex alone, because you can include semantic or grammatical filters. For example, with rule-based matching, you can extract a first name and a last name, which are always proper nouns: \" company. He is interested in learning\" In this example, is a list of objects that defines the combination of tokens to be matched. Both POS tags in it are (proper noun). So, the consists of two objects in which the POS tags for both tokens should be . This pattern is then added to with the method, which takes a identifier and a list of patterns. Finally, matches are obtained with their starting and end indexes. You can also use rule-based matching to extract phone numbers: \" happening on 21 July 2019 in London. It is titled\" \" There is a helpline number available\" In this example, the pattern is updated in order to match phone numbers. Here, some attributes of the token are also used:\n• matches the exact text of the token.\n• transforms the token string to show orthographic features, standing for digit.\n• defines operators. Using as a value means that the pattern is optional, meaning it can match 0 or 1 times. Chaining together these dictionaries gives you a lot of flexibility to choose your matching criteria. Note: For simplicity, in the example, phone numbers are assumed to be of a particular format: . You can change this depending on your use case. Again, rule-based matching helps you identify and extract tokens and phrases by matching according to lexical patterns and grammatical features. This can be useful when you’re looking for a particular entity."
    },
    {
        "link": "https://spacy.io/models/zh",
        "document": "To find out more about this model, see the overview of the latest model releases.\n\nTo find out more about this model, see the overview of the latest model releases.\n\nTo find out more about this model, see the overview of the latest model releases.\n\nTo find out more about this model, see the overview of the latest model releases."
    },
    {
        "link": "https://restack.io/p/semantic-parsing-answer-spacy-dependency-parsing-cat-ai",
        "document": ""
    },
    {
        "link": "https://medium.com/@meenavyas/spacy-named-entity-and-dependency-parsing-visualizers-3c64f0174938",
        "document": "I was searching for some pre-trained models that would read text and extract entities out of it like cities, places, time and date etc. automatically as training a model manually is time consuming and needs a lot of data to train if somebody has already done it why not reuse it.\n\nNamed-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a sub-task of information extraction that seeks to locate and classify named entities in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n\nspaCy is the leading open-source library for advanced NLP. spaCy has excellent pre-trained named-entity recognizers in a number of models. Note that we used “en_core_web_sm” model. I have read that some spaCy models are case-sensitive.\n\nI tried converting text of a random news article into Named Entities using this visualization tool “displaCy Named Entity Visualizer“. You can look at the results in the link here\n\nHere is the output of the paragraph I had entered in the tool\n\nIf you look at spaCy documentation, it gives the explanation of these entity types\n• PERSON (People, including fictional): It classified “AI”, “CAGR”, “Tencent” wrongly as person in our context.\n• NORP (Nationalities or religious or political groups): It classified ‘Asian’ and “Chinese” correctly as nationality.\n• GPE (Countries, cities, states): It classified country “U.S.” correctly but misclassified “Alibaba” and “AI” in our context.\n• ORG (Companies, agencies, institutions etc): It classified “Baidu”, “Google”, “IBM”, and “Microsoft” correctly.\n• CARDINAL (Numerals that do not fall under another type): It classified “one” and “three” correctly.\n• DATE (Absolute or relative dates or periods): “2017” was classified correctly.\n\nFor the entry types which are not correct , we need to re-train the model with our own contextual data as training set.\n\nA Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc.\n\nA dependency parser analyzes the grammatical structure of a sentence, establishing relationships between “head” words and words which modify those heads.\n\nThe figure below shows a snapshot of dependency parser of the paragraph above. Full image can be viewed in Dependency Visualizers here.\n\nDependency Parsers can read various forms of plain text input and can output various analysis formats, including part-of-speech tagged text, phrase structure trees, and a grammatical relations (typed dependency) format.\n\nDependency Parsing can be used to solve various complex NLP (Natural Language Processing) problems like Named Entity Recognition, Relation Extraction, translation. For more details on Dependency parsing, watch this Stanford video.\n\nRead about Parsey McParseface (and SyntaxNet), open source dependency parser here"
    },
    {
        "link": "https://github.com/pyg-team/pytorch_geometric",
        "document": "PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n\nIt consists of various methods for deep learning on graphs and other irregular structures, also known as geometric deep learning, from a variety of published papers. In addition, it consists of easy-to-use mini-batch loaders for operating on many small and single giant graphs, multi GPU-support, support, support, a large number of common benchmark datasets (based on simple interfaces to create your own), and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.\n\nClick here to join our Slack community!\n\nWhether you are a machine learning researcher or first-time user of machine learning toolkits, here are some reasons to try out PyG for machine learning on graph-structured data.\n• Easy-to-use and unified API: All it takes is 10-20 lines of code to get started with training a GNN model (see the next section for a quick tour). PyG is PyTorch-on-the-rocks: It utilizes a tensor-centric API and keeps design principles close to vanilla PyTorch. If you are already familiar with PyTorch, utilizing PyG is straightforward.\n• Comprehensive and well-maintained GNN models: Most of the state-of-the-art Graph Neural Network architectures have been implemented by library developers or authors of research papers and are ready to be applied.\n• Great flexibility: Existing PyG models can easily be extended for conducting your own research with GNNs. Making modifications to existing models or creating new architectures is simple, thanks to its easy-to-use message passing API, and a variety of operators and utility functions.\n• Large-scale real-world GNN models: We focus on the need of GNN applications in challenging real-world scenarios, and support learning on diverse types of graphs, including but not limited to: scalable GNNs for graphs with millions of nodes; dynamic GNNs for node predictions over time; heterogeneous GNNs with multiple node types and edge types.\n\nIn this quick tour, we highlight the ease of creating and training a GNN model with only a few lines of code.\n\nIn the first glimpse of PyG, we implement the training of a GNN for classifying papers in a citation graph. For this, we load the Cora dataset, and create a simple 2-layer GCN model using the pre-defined :\n\nMore information about evaluating final model performance can be found in the corresponding example.\n\nIn addition to the easy application of existing GNNs, PyG makes it simple to implement custom Graph Neural Networks (see here for the accompanying tutorial). For example, this is all it takes to implement the edge convolutional layer from Wang et al.:\n\nPyG provides a multi-layer framework that enables users to build Graph Neural Network solutions on both low and high levels. It comprises of the following components:\n• The PyG engine utilizes the powerful PyTorch deep learning framework with full and TorchScript support, as well as additions of efficient CPU/CUDA libraries for operating on sparse data, e.g., .\n• The PyG storage handles data processing, transformation and loading pipelines. It is capable of handling and processing large-scale graph datasets, and provides effective solutions for heterogeneous graphs. It further provides a variety of sampling solutions, which enable training of GNNs on large-scale graphs.\n• The PyG operators bundle essential functionalities for implementing Graph Neural Networks. PyG supports important GNN building blocks that can be combined and applied to various parts of a GNN model, ensuring rich flexibility of GNN design.\n• Finally, PyG provides an abundant set of GNN models, and examples that showcase GNN models on standard graph benchmarks. Thanks to its flexibility, users can easily build and modify custom GNN models to fit their specific needs.\n\nWe list currently supported PyG models, layers and operators according to category:\n\nGNN layers: All Graph Neural Network layers are implemented via the interface. A GNN layer specifies how to perform message passing, i.e. by designing different message, aggregation and update functions as defined here. These GNN layers can be stacked together to create Graph Neural Network models.\n• GCNConv from Kipf and Welling: Semi-Supervised Classification with Graph Convolutional Networks (ICLR 2017) [Example]\n• ChebConv from Defferrard et al.: Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (NIPS 2016) [Example]\n• GATConv from Veličković et al.: Graph Attention Networks (ICLR 2018) [Example]\n\nPooling layers: Graph pooling layers combine the vectorial representations of a set of nodes in a graph (or a subgraph) into a single vector representation that summarizes its properties of nodes. It is commonly applied to graph-level tasks, which require combining node features into a single graph representation.\n• Top-K Pooling from Gao and Ji: Graph U-Nets (ICML 2019), Cangea et al.: Towards Sparse Hierarchical Graph Classifiers (NeurIPS-W 2018) and Knyazev et al.: Understanding Attention and Generalization in Graph Neural Networks (ICLR-W 2019) [Example]\n• DiffPool from Ying et al.: Hierarchical Graph Representation Learning with Differentiable Pooling (NeurIPS 2018) [Example]\n\nGNN models: Our supported GNN models incorporate multiple message passing layers, and users can directly use these pre-defined models to make predictions on graphs. Unlike simple stacking of GNN layers, these models could involve pre-processing, additional learnable parameters, skip connections, graph coarsening, etc.\n• SchNet from Schütt et al.: SchNet: A Continuous-filter Convolutional Neural Network for Modeling Quantum Interactions (NIPS 2017) [Example]\n• DimeNet and DimeNetPlusPlus from Klicpera et al.: Directional Message Passing for Molecular Graphs (ICLR 2020) and Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules (NeurIPS-W 2020) [Example]\n• Node2Vec from Grover and Leskovec: node2vec: Scalable Feature Learning for Networks (KDD 2016) [Example]\n• Deep Multiplex Graph Infomax from Park et al.: Unsupervised Attributed Multiplex Network Embedding (AAAI 2020) [Example]\n• Masked Label Prediction from Shi et al.: Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification (CoRR 2020) [Example]\n• PMLP from Yang et al.: Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs (ICLR 2023)\n\nGNN operators and utilities: PyG comes with a rich set of neural network operators that are commonly used in many GNN models. They follow an extensible design: It is easy to apply these operators and graph utilities to existing GNN layers and models to further enhance model performance.\n• DropEdge from Rong et al.: DropEdge: Towards Deep Graph Convolutional Networks on Node Classification (ICLR 2020)\n• DropNode, MaskFeature and AddRandomEdge from You et al.: Graph Contrastive Learning with Augmentations (NeurIPS 2020)\n• GraphNorm from Cai et al.: GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training (ICML 2021)\n• GDC from Klicpera et al.: Diffusion Improves Graph Learning (NeurIPS 2019) [Example]\n\nScalable GNNs: PyG supports the implementation of Graph Neural Networks that can scale to large-scale graphs. Such application is challenging since the entire graph, its associated features and the GNN parameters cannot fit into GPU memory. Many state-of-the-art scalability approaches tackle this challenge by sampling neighborhoods for mini-batch training, graph clustering and partitioning, or by using simplified GNN models. These approaches have been implemented in PyG, and can benefit from the above GNN layers, operators and models.\n• NeighborLoader from Hamilton et al.: Inductive Representation Learning on Large Graphs (NIPS 2017) [Example1, Example2, Example3]\n• ClusterGCN from Chiang et al.: Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks (KDD 2019) [Example1, Example2]\n• GraphSAINT from Zeng et al.: GraphSAINT: Graph Sampling Based Inductive Learning Method (ICLR 2020) [Example]\n\nPyG is available for Python 3.9 to Python 3.12.\n\nYou can now install PyG via Anaconda for all major OS/PyTorch/CUDA combinations 🤗 If you have not yet installed PyTorch, install it via as described in the official PyTorch documentation. Given that you have PyTorch installed ( ), simply run\n\nFrom PyG 2.3 onwards, you can install and use PyG without any external library required except for PyTorch. For this, simply run\n\nIf you want to utilize the full set of features from PyG, there exists several additional libraries you may want to install:\n\nThese packages come with their own CPU and GPU kernel implementations based on the PyTorch C++/CUDA/hip(ROCm) extension interface. For a basic usage of PyG, these dependencies are fully optional. We recommend to start with a minimal installation, and install additional dependencies once you start to actually need them.\n\nFor ease of installation of these extensions, we provide wheels for all major OS/PyTorch/CUDA combinations, see here.\n\nTo install the binaries for PyTorch 2.5.0, simply run\n\nwhere should be replaced by either , , , or depending on your PyTorch installation.\n\nTo install the binaries for PyTorch 2.4.0, simply run\n\nwhere should be replaced by either , , , or depending on your PyTorch installation.\n\nNote: Binaries of older versions are also provided for PyTorch 1.4.0, PyTorch 1.5.0, PyTorch 1.6.0, PyTorch 1.7.0/1.7.1, PyTorch 1.8.0/1.8.1, PyTorch 1.9.0, PyTorch 1.10.0/1.10.1/1.10.2, PyTorch 1.11.0, PyTorch 1.12.0/1.12.1, PyTorch 1.13.0/1.13.1, PyTorch 2.0.0/2.0.1, PyTorch 2.1.0/2.1.1/2.1.2, PyTorch 2.2.0/2.2.1/2.2.2, and PyTorch 2.3.0/2.3.1 (following the same procedure). For older versions, you might need to explicitly specify the latest supported version number or install via in order to prevent a manual installation from source. You can look up the latest supported version number here.\n\nNVIDIA provides a PyG docker container for effortlessly training and deploying GPU accelerated GNNs with PyG, see here.\n\nIn case you want to experiment with the latest PyG features which are not fully released yet, either install the nightly version of PyG via\n\nor install PyG from master via\n\nThe external repository provides wheels and detailed instructions on how to install PyG for ROCm. If you have any questions about it, please open an issue here.\n\nPlease cite our paper (and the respective papers of the methods used) if you use this code in your own work:\n\nFeel free to email us if you wish your work to be listed in the external resources. If you notice anything unexpected, please open an issue and let us know. If you have any questions or are missing a specific feature, feel free to discuss them with us. We are motivated to constantly make PyG even better."
    },
    {
        "link": "https://pytorch-geometric.readthedocs.io/en/2.5.1/modules/nn.html",
        "document": "An extension of the container in order to define a sequential GNN model. Since GNN operators take in multiple input arguments, additionally expects both global input arguments, and function header definitions of individual operators. If omitted, an intermediate module will operate on the output of its preceding module: Here, defines the input arguments of , and defines the function header, i.e. input arguments and return types of . In particular, this also allows to create more sophisticated models, such as utilizing :\n• None input_args (str) – The input arguments of the model.\n• None modules ([(str, Callable) or Callable]) – A list of modules (with optional function header definitions). Alternatively, an of modules (and function header definitions) can be passed."
    },
    {
        "link": "https://medium.com/cj-express-tech-tildi/first-timers-guide-to-pytorch-geometric-part-1-the-basic-1b6006e1f4db",
        "document": "Part 1 — The Basics of building datasets with graph-based information and plugging them into models\n\nHere’s my first attempt with Pytorch-geometric (PyG) and Graph Neural Network (GNN) models. Perhaps this post will help you through some tricky spots that I have struggled with.\n\nIn the basic section, I will go through the steps of passing graph-based information into a PyG-style dataset, splitting them into minibatches, and using it in GNN models.\n\nThis requires that you\n• have designed and built your graph data with networkx or equivalent (I added the links to networkx documentation)\n\nI found that most tutorials for PyG are using the ready-made Dataset. I have my own graph built in networkx (see an example of an output from networkx’s node_link_data()). So, my next step is to transform the graph-based information into something Pytorch can easily use.\n\nPyG offers a quick tutorial for us here. We’ll pay attention to the InMemoryDataset class, from which we’ll inherit and adapt to our own use cases.\n\nFirst, let’s take a closer look at the initiation step. InMemoryDataset will check whether the “root” directory, specified with the “root” argument in __init__(), has already contained the file(s) in the “processed_file_names” list. If it does, then it will skip the “process” method and go straight to loading “self.data” and “self.slices”. (Somehow I’ve never got to use “slices”, but I’ll just go with the flow for now.)\n\nThe important part is the “process” method. This is where you decide what happens before you save the transformed data. There are 3 pre-defined functions for you to use: transform, pre_transform, and pre-filter. The detailed description is on the tutorial page. However, for first-timers, we can also just define the steps directly and make sure things work the first time and refactor them later. Below is the flow I used.\n\nHere is an example of what the process method looks like for my case.\n\nHere is another example with more bells and whistles. Note that the Data object I created at the end has a few more attributes.\n\nNow, you can actually use the Dataset object in some GNN models, but you may not want to. With larger graphs, you would want to split the graph into several batches. Luckily, there are many ways to do mini-batch with graph data already built in the PyG library. For well-known models in the GNN field, chances are there is already some form of graph-splitting implementation for you. See more discussion on mini-batching with graphs in PyG here.\n\nIf you remain confused after reading that page, fear not. That was my reaction when I read that page as well. Turns out there are many kinds of Loader and each one is generally tied to an implementation in a certain paper, including heterogeneous graph transformer, cluster-GCN, and GraphSAINT. Note that the table in the link contains both “Loader” and “Sampler” classes. So, if your goal is to use some ready-made models, you may be in luck. There are also some examples with a variety of datasets, models, and tasks to choose from here.\n\nIn my case, I wanted to try out the “GraphSage” model from the paper “Inductive Representation Learning on Large Graphs”. There are loaders for this paper, so I just had to choose the appropriate one and set the correct parameters.\n\nFor GraphSage, I have NeighborLoader and LinkNeighborLoader to choose from. For a given batch_size parameters, the NeighborLoader will pick that exact number of nodes, whereas the LinkNeighborLoader will pick the exact number of edges. Since I wanted to do a link prediction task later, I would like to have a uniform number of edges in each batch, so I picked the latter.\n\nSome interesting parameters are the following:\n• batch_size: for edge samplers, this will be the number of edges in a single batch (for other Loaders that sample nodes, it will be the number of nodes in a batch.)\n• num_neighbors: this is specific to this Loader for GraphSage. It specifies the number of neighbors to sample for each node in each iteration. (See more details in the GraphSage paper — Appendix)\n• edge_label_index: this is how you pass the graph-based information. The Loader will use this\n• edge_label: optional, only use this for labelled data. Must be the same length as the .\n• neg_sampling_ratio: usually your graph data will only contain positive edges, so you can add negative edges with these parameters. The negative edges will have the label 0. The positive edges will have the label 1 by default. (Alternatively, you can also build graphs with negative edges in the first place by adding the label as the edge property, but that will make your graph rather messy. You may want to keep the graph clean and deal with the negative sample here.)\n\nLet’s see how to tie together the steps from processing raw data into Dataset, loading and splitting them into mini-batches, and using the batches in a model. Below is a partial code from this tutorial. Comments are my own.\n\nSo that was the usual flow for creating Dataset with graph-based information, splitting them into minibatches, and plugging them with GNN models. The next part will talk about some specific details for a promotion-customer link prediction model I built, particularly, how to select a specific subset of edges for prediction tasks and how to deal with different graphs at training time and at serving time.\n\nSee First-timer’s Guide to Pytorch-geometric — Part 2 The Applied for more implementation details.\n• How to pass your Dataset in a mini-batch Loader and then use it in a GNN model (GraphSAGE) and a LogisticRegression model at the end."
    },
    {
        "link": "https://wandb.ai/syllogismos/machine-learning-with-graphs/reports/9-Graph-Neural-Networks-with-Pytorch-Geometric--VmlldzozNzcwMTE",
        "document": ""
    },
    {
        "link": "https://kaggle.com/code/iogbonna/introduction-to-graph-neural-network-with-pytorch",
        "document": ""
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html",
        "document": "Read more in the User Guide.\n\nThe number of clusters to form as well as the number of centroids to generate. For an example of how to choose an optimal value for refer to Selecting the number of clusters with silhouette analysis on KMeans clustering.\n• None ‘k-means++’ : selects initial cluster centroids using sampling based on an empirical probability distribution of the points’ contribution to the overall inertia. This technique speeds up convergence. The algorithm implemented is “greedy k-means++”. It differs from the vanilla k-means++ by making several trials at each sampling step and choosing the best centroid among them.\n• None ‘random’: choose observations (rows) at random from data for the initial centroids.\n• None If an array is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.\n• None If a callable is passed, it should take arguments X, n_clusters and a random state and return an initialization. For an example of how to use the different strategies, see A demo of K-Means clustering on the handwritten digits data. For an evaluation of the impact of initialization, see the example Empirical evaluation of the impact of k-means initialization. Number of times the k-means algorithm is run with different centroid seeds. The final results is the best output of consecutive runs in terms of inertia. Several runs are recommended for sparse high-dimensional problems (see Clustering sparse data with k-means). When , the number of runs depends on the value of init: 10 if using or is a callable; 1 if using or is an array-like. Changed in version 1.4: Default value for changed to . Maximum number of iterations of the k-means algorithm for a single run. Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence. Determines random number generation for centroid initialization. Use an int to make the randomness deterministic. See Glossary. When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True (default), then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. Note that if the original data is not C-contiguous, a copy will be made even if copy_x is False. If the original data is sparse, but not in CSR format, a copy will be made even if copy_x is False. K-means algorithm to use. The classical EM-style algorithm is . The variation can be more efficient on some datasets with well-defined clusters, by using the triangle inequality. However it’s more memory intensive due to the allocation of an extra array of shape . Changed in version 1.1: Renamed “full” to “lloyd”, and deprecated “auto” and “full”. Changed “auto” to use “lloyd” instead of “elkan”. Coordinates of cluster centers. If the algorithm stops before fully converging (see and ), these will not be consistent with . Sum of squared distances of samples to their closest cluster center, weighted by the sample weights if provided. Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nThe k-means problem is solved using either Lloyd’s or Elkan’s algorithm.\n\nThe average complexity is given by O(k n T), where n is the number of samples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. Refer to “How slow is the k-means method?” D. Arthur and S. Vassilvitskii - SoCG2006. for more details.\n\nIn practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That’s why it can be useful to restart it several times.\n\nIf the algorithm stops before fully converging (because of or ), and will not be consistent, i.e. the will not be the means of the points in each cluster. Also, the estimator will reassign after the last iteration to make consistent with on the training set.\n\nFor examples of common problems with K-Means and how to address them see Demonstration of k-means assumptions.\n\nFor a demonstration of how K-Means can be used to cluster text documents see Clustering text documents using k-means.\n\nFor a comparison between K-Means and MiniBatchKMeans refer to example Comparison of the K-Means and MiniBatchKMeans clustering algorithms.\n\nFor a comparison between K-Means and BisectingKMeans refer to example Bisecting K-Means and Regular K-Means Performance Comparison.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://datacamp.com/tutorial/k-means-clustering-python",
        "document": "In this course, you will be introduced to unsupervised learning through techniques such as hierarchical and k-means clustering using the SciPy library."
    },
    {
        "link": "https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nThis example is meant to illustrate situations where k-means produces unintuitive and possibly undesirable clusters.\n\nThe previously generated data is now used to show how behaves in the following scenarios:\n• None Non-optimal number of clusters: in a real setting there is no uniquely defined true number of clusters. An appropriate number of clusters has to be decided from data-based criteria and knowledge of the intended goal.\n• None Anisotropically distributed blobs: k-means consists of minimizing sample’s euclidean distances to the centroid of the cluster they are assigned to. As a consequence, k-means is more appropriate for clusters that are isotropic and normally distributed (i.e. spherical gaussians).\n• None Unequal variance: k-means is equivalent to taking the maximum likelihood estimator for a “mixture” of k gaussian distributions with the same variances but with possibly different means.\n• None Unevenly sized blobs: there is no theoretical result about k-means that states that it requires similar cluster sizes to perform well, yet minimizing euclidean distances does mean that the more sparse and high-dimensional the problem is, the higher is the need to run the algorithm with different centroid seeds to ensure a global minimal inertia."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/clustering.html",
        "document": "Clustering of unlabeled data can be performed with the module .\n\nEach clustering algorithm comes in two variants: a class, that implements the method to learn the clusters on train data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the attribute.\n\ncreates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given. Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the preference, which controls how many exemplars are used, and the damping factor which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages. The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order \\(O(N^2 T)\\), where \\(N\\) is the number of samples and \\(T\\) is the number of iterations until convergence. Further, the memory complexity is of the order \\(O(N^2)\\) if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets. The messages sent between points belong to one of two categories. The first is the responsibility \\(r(i, k)\\), which is the accumulated evidence that sample \\(k\\) should be the exemplar for sample \\(i\\). The second is the availability \\(a(i, k)\\) which is the accumulated evidence that sample \\(i\\) should choose sample \\(k\\) to be its exemplar, and considers the values for all other samples that \\(k\\) should be an exemplar. In this way, exemplars are chosen by samples if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves. More formally, the responsibility of a sample \\(k\\) to be the exemplar of sample \\(i\\) is given by: Where \\(s(i, k)\\) is the similarity between samples \\(i\\) and \\(k\\). The availability of sample \\(k\\) to be the exemplar of sample \\(i\\) is given by: To begin with, all values for \\(r\\) and \\(a\\) are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor \\(\\lambda\\) is introduced to iteration process: where \\(t\\) indicates the iteration times.\n• None Demo of affinity propagation clustering algorithm: Affinity Propagation on a synthetic 2D datasets with 3 classes\n• None Visualizing the stock market structure Affinity Propagation on financial time series to find groups of companies\n\nThe algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of core samples, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, and , which define formally what we mean when we say dense. Higher or lower indicate higher density necessary to form a cluster. More formally, we define a core sample as being a sample in the dataset such that there exist other samples within a distance of , which are defined as neighbors of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of their neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster. Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least in distance from any core sample, is considered an outlier by the algorithm. While the parameter primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter is crucial to choose appropriately for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as for “noise”). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below). In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below. The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order. However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order. This would happen when a non-core sample has a distance lower than to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering. The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see . This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g., with sparse matrices). This matrix will consume \\(n^2\\) floats. A couple of mechanisms for getting around this are:\n• None Use OPTICS clustering in conjunction with the method. OPTICS clustering also calculates the full pairwise matrix, but only keeps one row in memory at a time (memory complexity n).\n• None A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with . See .\n• None The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a when fitting DBSCAN.\n• None A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n• None DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19.\n\nThe algorithm shares many similarities with the algorithm, and can be considered a generalization of DBSCAN that relaxes the requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph, which assigns each sample both a distance, and a spot within the cluster attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of inf set for , then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given value using the method. Setting to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points. The reachability distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining reachability distances and data set produces a reachability plot, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. ‘Cutting’ the reachability plot at a single value produces DBSCAN like results; all points above the ‘cut’ are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter . There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster. The results from OPTICS method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise. Note that for any single value of , DBSCAN will tend to have a shorter run time than OPTICS; however, for repeated runs at varying values, a single run of OPTICS may require less cumulative runtime than DBSCAN. It is also important to note that OPTICS’ output is close to DBSCAN’s only if and are close. Spatial indexing trees are used to avoid calculating the full distance matrix, and allow for efficient memory usage on large sets of samples. Different distance metrics can be supplied via the keyword. For large datasets, similar (but not identical) results can be obtained via . The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain \\(n\\) (as opposed to \\(n^2\\)) memory scaling; however, tuning of the parameter will likely need to be used to give a solution in a reasonable amount of wall time.\n• None “OPTICS: ordering points to identify the clustering structure.” Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999."
    },
    {
        "link": "https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html",
        "document": "In the previous few sections, we have explored one category of unsupervised machine learning models: dimensionality reduction. Here we will move on to another class of unsupervised machine learning models: clustering algorithms. Clustering algorithms seek to learn, from the properties of the data, an optimal division or discrete labeling of groups of points. Many clustering algorithms are available in Scikit-Learn and elsewhere, but perhaps the simplest to understand is an algorithm known as k-means clustering, which is implemented in . We begin with the standard imports:\n\nThe k-means algorithm searches for a pre-determined number of clusters within an unlabeled multidimensional dataset. It accomplishes this using a simple conception of what the optimal clustering looks like:\n• The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n• Each point is closer to its own cluster center than to other cluster centers. Those two assumptions are the basis of the k-means model. We will soon dive into exactly how the algorithm reaches this solution, but for now let's take a look at a simple dataset and see the k-means result. First, let's generate a two-dimensional dataset containing four distinct blobs. To emphasize that this is an unsupervised algorithm, we will leave the labels out of the visualization\n\nExpectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science. k-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here. In short, the expectation–maximization approach here consists of the following procedure:\n• Repeat until converged\n• M-Step: set the cluster centers to the mean Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to. The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster. The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics. We can visualize the algorithm as shown in the following figure. For the particular initialization shown here, the clusters converge in just three iterations. For an interactive version of this figure, refer to the code in the Appendix."
    }
]