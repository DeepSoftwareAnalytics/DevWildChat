[
    {
        "link": "https://okx.com/docs-v5/en",
        "document": "Welcome to our V5 API documentation. OKX provides REST and WebSocket APIs to suit your trading needs.\n• For users who complete registration on my.okx.com, please visit https://my.okx.com/docs-v5/en/ for the V5 API documentation.\n• For users who complete registration on app.okx.com, please visit https://app.okx.com/docs-v5/en/ for the V5 API documentation.\n• Learn how to trade with V5 API: Best practice to OKX’s v5 API\n• Get access to our market maker python sample code Python market maker sample\n• Please take 1 minute to help us improve: V5 API Satisfaction Survey\n• If you have any questions, please consult online customer service\n\nPlease refer to my api page regarding V5 API Key creation.\n\nCreate an API Key on the website before signing any requests. After creating an APIKey, keep the following information safe:\n\nThe system returns randomly-generated APIKeys and SecretKeys. You will need to provide the Passphrase to access the API. We store the salted hash of your Passphrase for authentication. We cannot recover the Passphrase if you have lost it. You will need to create a new set of APIKey.\n\n\n\n\n\nThere are three permissions below that can be associated with an API key. One or more permission can be assigned to any key.\n• Read : Can request and view account info such as bills and order history which need read permission\n• Trade : Can place and cancel orders, funding transfer, make settings which need write permission\n\nAll private REST requests must contain the following headers:\n• None The Base64-encoded signature (see Signing Messages subsection for details).\n• None The UTC timestamp of your request .e.g : 2020-12-08T09:08:57.715Z\n• None The passphrase you specified when creating the APIKey.\n\nRequest bodies should have content type and be in valid JSON format.\n\nThe header is generated as follows:\n• Sign the prehash string with the SecretKey using the HMAC SHA256.\n• Encode the signature in the Base64 format.\n\nThe value is the same as the header with millisecond ISO format, e.g. .\n\nThe request method should be in UPPERCASE: e.g. and .\n\nThe is the path of requesting an endpoint.\n\nThe refers to the String of the request body. It can be omitted if there is no request body (frequently the case for requests).\n\nThe SecretKey is generated when you create an APIKey.\n\nWebSocket is a new HTML5 protocol that achieves full-duplex data transmission between the client and server, allowing data to be transferred effectively in both directions. A connection between the client and server can be established with just one handshake. The server will then be able to push data to the client according to preset rules. Its advantages include:\n• The WebSocket request header size for data transmission between client and server is only 2 bytes.\n• Either the client or server can initiate data transmission.\n• There's no need to repeatedly create and delete TCP connections, saving resources on bandwidth and server.\n\nConnection limit: 3 requests per second (based on IP)\n\nWhen subscribing to a public channel, use the address of the public service. When subscribing to a private channel, use the address of the private service\n\nThe total number of 'subscribe'/'unsubscribe'/'login' requests per connection is limited to 480 times per hour.\n\nThe limit will be set at 30 WebSocket connections per specific WebSocket channel per sub-account. Each WebSocket connection is identified by the unique .\n\nThe WebSocket channels subject to this limitation are as follows:\n\nIf users subscribe to the same channel through the same WebSocket connection through multiple arguments, for example, by using and , it will be counted once only. If users subscribe to the listed channels (such as orders and accounts) using either the same or different connections, it will not affect the counting, as these are considered as two different channels. The system calculates the number of WebSocket connections per channel.\n\nThe platform will send the number of active connections to clients through the event message to new channel subscriptions.\n\nWhen the limit is breached, generally the latest connection that sends the subscription request will be rejected. Client will receive the usual subscription acknowledgement followed by the from the connection that the subscription has been terminated. In exceptional circumstances the platform may unsubscribe existing connections.\n\nOrder operations through WebSocket, including place, amend and cancel orders, are not impacted through this change.\n\napiKey: Unique identification for invoking API. Requires user to apply one manually.\n\ntimestamp: the Unix Epoch time, the unit is seconds, e.g. 1704876947\n\nsign: signature string, the signature algorithm is as follows:\n\nFirst concatenate , , , strings, then use HMAC SHA256 method to encrypt the concatenated string with SecretKey, and then perform Base64 encoding.\n\nsecretKey: The security key generated when the user applies for APIKey, e.g. : 22582BD0CFF14C41EDBF1AB98506286D\n\nWebSocket channels are divided into two categories: and channels.\n\n-- No authentication is required, include tickers channel, K-Line channel, limit price channel, order book channel, and mark price channel etc.\n\n-- including account channel, order channel, and position channel, etc -- require log in.\n\nUsers can choose to subscribe to one or more channels, and the total length of multiple channels cannot exceed 64 KB.\n\nBelow is an example of subscription parameters. The requirement of subscription parameters for each channel is different. For details please refer to the specification of each channels.\n\nUnsubscribe from one or more channels.\n\nClient will receive the information in the following scenarios:\n\n30 seconds prior to the upgrade of the WebSocket service, the notification message will be sent to users indicating that the connection will soon be disconnected. Users are encouraged to establish a new connection to prevent any disruptions caused by disconnection.\n\n\n\n\n\n The feature is supported by WebSocket Public (/ws/v5/public) and Private (/ws/v5/private) for now.\n\nTo facilitate your trading experience, please set the appropriate account mode before starting trading.\n\nIn the trading account trading system, 4 account modes are supported: , , , and .\n\nYou need to set on the Web/App for the first set of every account mode.\n\nCurrently, the V5 API works for Demo Trading, but some functions are not supported, such as , , , etc.\n\nOKX account can be used for login on Demo Trading. If you already have an OKX account, you can log in directly.\n\nStart API Demo Trading by the following steps:\n\n Login OKX —> Trade —> Demo Trading —> Personal Center —> Demo Trading API -> Create Demo Trading V5 API Key —> Start your Demo Trading\n\nYou need to sign in to your OKX account before accessing the explorer. The interface only allow access to the demo trading environment.\n• None Clicking Try it out button in Parameters Panel and editing request parameters.\n• None Clicking button to send your request. You can check response in Responses panel.\n\nThe rules for placing orders at the exchange level are as follows:\n• The maximum number of pending orders (including post only orders, limit orders and taker orders that are being processed): 4,000\n• None The maximum number of pending orders per trading symbol is 500, the limit of 500 pending orders applies to the following order types:\n• Limit and market orders triggered under the order types below:\n• None The maximum number of pending spread orders: 500 across all spreads\n\nThe rules for trading are as follows:\n• When the number of maker orders matched with a taker order exceeds the maximum number limit of 1000, the taker order will be canceled.\n• The limit orders will only be executed with a portion corresponding to 1000 maker orders and the remainder will be canceled.\n• Fill or Kill (FOK) orders will be canceled directly.\n\nThe rules for the returning data are as follows:\n• None and represent the request result or error reason when the return data has , and has not ;\n• None It is and that represent the request result or error reason when the return data has rather than and .\n• There are no difference between and :\n• For BTC-USD-SWAP, and are both BTC-USD. For BTC-USDC-SWAP, and are both BTC-USDC.\n• If you set the request parameter \"uly\" as BTC-USD, you will get the data for BTC-USD (coin-margined) contracts.\n• If you set the request parameter \"instFamily\" as BTC-USD, then you also will get data for BTC-USD (coin-margined) contracts.\n• You can look up the corresponding instFamily of each instrument from the \"Get instruments\" endpoint.\n\nOrders may not be processed in time due to network delay or busy OKX servers. You can configure the expiry time of the request using if you want the order request to be discarded after a specific time.\n\nIf is specified in the requests for Place (multiple) orders or Amend (multiple) orders, the request will not be processed if the current system time of the server is after the .\n\nSet the following parameters in the request header\n\nThe following endpoints are supported:\n\nThe following parameters are set in the request\n\nThe following endpoints are supported:\n\nOur REST and WebSocket APIs use rate limits to protect our APIs against malicious usage so our trading platform can operate reliably and fairly.\n\n When a request is rejected by our system due to rate limits, the system returns error code 50011 (Rate limit reached. Please refer to API documentation and throttle requests accordingly).\n\n The rate limit is different for each endpoint. You can find the limit for each endpoint from the endpoint details. Rate limit definitions are detailed below:\n• None WebSocket login and subscription rate limits are based on connection.\n• None Private REST rate limits are based on User ID (sub-accounts have individual User IDs).\n• None WebSocket order management rate limits are based on User ID (sub-accounts have individual User IDs).\n\nFor Trading-related APIs (place order, cancel order, and amend order) the following conditions apply:\n• None Rate limits are shared across the REST and WebSocket channels.\n• None Rate limits for placing orders, amending orders, and cancelling orders are independent from each other.\n• None Rate limits are defined on the Instrument ID level (except Options)\n• None Rate limits for Options are defined based on the Instrument Family level. Refer to the Get instruments endpoint to view Instrument Family information.\n• None Rate limits for a multiple order endpoint and a single order endpoint are also independent, with the exception being when there is only one order sent to a multiple order endpoint, the order will be counted as a single order and adopt the single order rate limit.\n\nAt the sub-account level, we allow a maximum of 1000 order requests per 2 seconds. Only new order requests and amendment order requests will be counted towards this limit. The limit encompasses all requests from the endpoints below. For batch order requests consisting of multiple orders, each order will be counted individually. Error code 50061 is returned when the sub-account rate limit is exceeded. The existing rate limit rule per instrument ID remains unchanged and the existing rate limit and sub-account rate limit will operate in parallel. If clients require a higher rate limit, clients can trade via multiple sub-accounts.\n\nThis is only applicable to >= VIP5 customers. \n\n As an incentive for more efficient trading, the exchange will offer a higher sub-account rate limit to clients with a high trade fill ratio. \n\n\n\n The exchange calculates two ratios based on the transaction data from the past 7 days at 00:00 UTC.\n• Sub-account fill ratio: This ratio is determined by dividing (the trade volume in USDT of the sub-account) by (sum of (new and amendment request count per symbol * symbol multiplier) of the sub-account). Note that the master trading account itself is also considered as a sub-account in this context.\n• Master account aggregated fill ratio: This ratio is calculated by dividing (the trade volume in USDT on the master account level) by (the sum (new and amendment count per symbol * symbol multiplier] of all sub-accounts).\n\nThe symbol multiplier allows for fine-tuning the weight of each symbol. A smaller symbol multiplier (<1) is used for smaller pairs that require more updates per trading volume. All instruments have a default symbol multiplier, and some instruments will have overridden symbol multipliers.\n\nThe fill ratio computation excludes block trading, spread trading, MMP and fiat orders for order count; and excludes block trading, spread trading for trade volume. Only successful order requests (sCode=0) are considered.\n\nAt 08:00 UTC, the system will use the maximum value between the sub-account fill ratio and the master account aggregated fill ratio based on the data snapshot at 00:00 UTC to determine the sub-account rate limit based on the table below. For broker (non-disclosed) clients, the system considers the sub-account fill ratio only.\n\nIf there is an improvement in the fill ratio and rate limit to be uplifted, the uplift will take effect immediately at 08:00 UTC. However, if the fill ratio decreases and the rate limit needs to be lowered, a one-day grace period will be granted, and the lowered rate limit will only be implemented on T+1 at 08:00 UTC. On T+1, if the fill ratio improves, the higher rate limit will be applied accordingly. In the event of client demotion to VIP4, their rate limit will be downgraded to Tier 1, accompanied by a one-day grace period.\n\nIf the 7-day trading volume of a sub-account is less than 1,000,000 USDT, the fill ratio of the master account will be applied to it.\n\nFor newly created sub-accounts, the Tier 1 rate limit will be applied at creation until T+1 8am UTC, at which the normal rules will be applied.\n\nBlock trading, spread trading, MMP and spot/margin orders are exempted from the sub-account rate limit.\n\nThe exchange offers GET / Account rate limit endpoint that provides ratio and rate limit data, which will be updated daily at 8am UTC. It will return the sub-account fill ratio, the master account aggregated fill ratio, current sub-account rate limit and sub-account rate limit on T+1 (applicable if the rate limit is going to be demoted). \n\n\n\n The fill ratio and rate limit calculation example is shown below. Client has 3 accounts, symbol multiplier for BTC-USDT-SWAP = 1 and XRP-USDT = 0.1.\n\nIf you require a higher request rate than our rate limit, you can set up different sub-accounts to batch request rate limits. We recommend this method for throttling or spacing out requests in order to maximize each accounts' rate limit and avoid disconnections or rejections.\n\nHigh-caliber trading teams are welcomed to work with OKX as market makers in providing a liquid, fair, and orderly platform to all users. OKX market makers could enjoy favourable fees in return for meeting the market making obligations.\n• VIP 2 or above on fee schedule\n\n\n\nInterested parties can reach out to us using this form: https://okx.typeform.com/contact-sales\n\n\n\nMarket making obligations and trading fees will be shared to successful parties only.\n\nIf your business platform offers cryptocurrency services, you can apply to join the OKX Broker Program, become our partner broker, enjoy exclusive broker services, and earn high rebates through trading fees generated by OKX users.\n\n The Broker Program includes, and is not limited to, integrated trading platforms, trading bots, copy trading platforms, trading bot providers, quantitative strategy institutions, asset management platforms etc.\n• If you have any questions, feel free to contact our customer support.\n\nRelevant information for specific Broker Program documentation and product services will be provided following successful applications.\n\n\n\nRetrieve a list of assets (with non-zero balance), remaining balance, and available amount in the trading account.\n• Regarding more parameter details, you can refer to product documentations below:\n\n Spot and futures mode: cross margin trading \n\n Multi-currency margin mode: cross margin trading \n\n Multi-currency margin mode vs. Portfolio margin mode \n\n\n\nDistribution of applicable fields under each account level are as follows:\n\nRetrieve information on your positions. When the account is in mode, positions will be displayed, and when the account is in mode, or positions will be displayed. Return in reverse chronological order using ctime.\n\nRetrieve the updated position data for the last 3 months. Return in reverse chronological order using utime. Getting positions history is supported under Portfolio margin mode since 04:00 AM (UTC) on November 11, 2024.\n\nRetrieve the bills of the account. The bill refers to all transaction records that result in changing the balance of an account. Pagination is supported, and the response is sorted with the most recent first. This endpoint can retrieve data from the last 7 days.\n\nRetrieve the account’s bills. The bill refers to all transaction records that result in changing the balance of an account. Pagination is supported, and the response is sorted with most recent first. This endpoint can retrieve data from the last 1 year since July 1, 2024.\n\nApply for bill data since 1 February, 2021 except for the current quarter.\n\nApply for bill data since 1 February, 2021 except for the current quarter.\n\nSpot and futures mode and Multi-currency mode: and support both mode and mode. In mode, users can only have positions in one direction; In mode, users can hold positions in long and short directions.\n\n Portfolio margin mode: and only support mode\n\n\n\nThere are 10 different scenarios for leverage setting: \n\n\n\n 1. Set leverage for instruments under trade mode at pairs level. \n\n 2. Set leverage for instruments under trade mode and Spot mode (enabled borrow) at currency level. \n\n 3. Set leverage for instruments under trade mode and Spot and futures mode account mode at pairs level. \n\n 4. Set leverage for instruments under trade mode and Multi-currency margin at currency level. \n\n 5. Set leverage for instruments under trade mode and Portfolio margin at currency level. \n\n 6. Set leverage for instruments under trade mode at underlying level. \n\n 7. Set leverage for instruments under trade mode and buy/sell position mode at contract level. \n\n 8. Set leverage for instruments under trade mode and long/short position mode at contract and position side level. \n\n 9. Set leverage for instruments under trade at contract level. \n\n 10. Set leverage for instruments under trade mode and buy/sell position mode at contract level. \n\n 11. Set leverage for instruments under trade mode and long/short position mode at contract and position side level. \n\n\n\n\n\nNote that the request parameter is only required when margin mode is isolated in long/short position mode for FUTURES/SWAP instruments (see scenario 8 and 11 above). \n\n Please refer to the request examples on the right for each case. \n\n\n\nThe maximum quantity to buy or sell. It corresponds to the \"sz\" from placement.\n\nAvailable balance for isolated margin positions and SPOT, available equity for cross margin positions.\n\nIncrease or decrease the margin of the isolated position. Margin reduction may result in the change of the actual leverage.\n\nGet the maximum loan of instrument\n\nGet interest accrued data. Only data within the last one year can be obtained.\n\nYou can set the currency margin and futures/perpetual Isolated margin trading mode\n\nRetrieve the maximum transferable amount from trading account to funding account. If no currency is specified, the transferable amount of all owned currencies will be returned.\n\nPlease note that this endpoint will be deprecated soon.\n\nGet borrow and repay history in Quick Margin Mode\n\nGet record in the past 3 months.\n\nCalculates portfolio margin information for virtual position/assets or current position of the user.\n\n You can add up to 200 virtual positions and 200 virtual assets in one request.\n\nSet risk offset amount. This does not represent the actual spot risk offset amount. Only applicable to Portfolio Margin Mode.\n\nRetrieve a greeks list of all assets in the account.\n\nOnly applicable to and\n\nPre-set the required information for account mode switching. When switching from back to / , and if there are existing cross-margin contract positions, it is mandatory to pre-set leverage.\n\nIf the user does not follow the required settings, they will receive an error message during the pre-check or when setting the account mode.\n\nYou need to set on the Web/App for the first set of every account mode. If users plan to switch account modes while holding positions, they should first call the preset endpoint to conduct necessary settings, then call the precheck endpoint to get unmatched information, margin check, and other related information, and finally call the account mode switch endpoint to switch account modes.\n\nYou can unfreeze by this endpoint once MMP is triggered.\n\n\n\nOnly applicable to Option in Portfolio Margin mode, and MMP privilege is required.\n\nThis endpoint is used to set MMP configure\n\n\n\nOnly applicable to Option in Portfolio Margin mode, and MMP privilege is required.\n\nThis endpoint is used to get MMP configure information\n\n\n\nOnly applicable to Option in Portfolio Margin mode, and MMP privilege is required.\n\nRetrieve account information. Data will be pushed when triggered by events such as placing order, canceling order, transaction execution, etc. It will also be pushed in regular interval according to subscription granularity.\n\n\n\nConcurrent connection to this channel will be restricted by the following rules: WebSocket connection count limit.\n\nRetrieve position information. Initial snapshot will be pushed according to subscription granularity. Data will be pushed when triggered by events such as placing/canceling order, and will also be pushed in regular interval according to subscription granularity.\n\n\n\nConcurrent connection to this channel will be restricted by the following rules: WebSocket connection count limit.\n\nRetrieve account balance and position information. Data will be pushed when triggered by events such as filled order, funding transfer.\n\n This channel applies to getting the account cash balance and the change of position asset ASAP. \n\n Concurrent connection to this channel will be restricted by the following rules: WebSocket connection count limit.\n\nThis push channel is only used as a risk warning, and is not recommended as a risk judgment for strategic trading \n\n In the case that the market is volatile, there may be the possibility that the position has been liquidated at the same time that this message is pushed.\n\n The warning is sent when a position is at risk of liquidation for isolated margin positions. The warning is sent when all the positions are at risk of liquidation for cross-margin positions.\n\n Concurrent connection to this channel will be restricted by the following rules: WebSocket connection count limit.\n\nRetrieve account greeks information. Data will be pushed when triggered by events such as increase/decrease positions or cash balance in account, and will also be pushed in regular interval according to subscription granularity.\n\n Concurrent connection to this channel will be restricted by the following rules: WebSocket connection count limit.\n\nYou can place an order only if you have sufficient funds.\n\nRate Limit of lead instruments for Copy Trading: 4 requests per 2 seconds\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nPlace orders in batches. Maximum 20 orders can be placed per request. \n\n Request parameters should be passed in the form of an array. Orders will be placed in turn\n\n\n\nRate Limit of lead instruments for Copy Trading: 4 orders per 2 seconds\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nCancel incomplete orders in batches. Maximum 20 orders can be canceled per request. Request parameters should be passed in the form of an array.\n\nRate Limit of lead instruments for Copy Trading: 4 requests per 2 seconds\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nAmend incomplete orders in batches. Maximum 20 orders can be amended per request. Request parameters should be passed in the form of an array.\n\nRate Limit of lead instruments for Copy Trading: 4 orders per 2 seconds\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nClose the position of an instrument via a market order.\n\nRetrieve all incomplete orders under the current account.\n\nGet completed orders which are placed in the last 7 days, including those placed 7 days ago but completed in the last 7 days. \n\n\n\nThe incomplete orders that have been canceled are only reserved for 2 hours.\n\nGet completed orders which are placed in the last 3 months, including those placed 3 months ago but completed in the last 3 months. \n\n\n\nRetrieve recently-filled transaction details in the last 3 day.\n\nRetrieve recently-filled transaction details in the last 3 months.\n\nGet list of small convertibles and mainstream currencies. Only applicable to the crypto balance less than $10.\n\nGet the history and status of easy convert trades in the past 7 days.\n\nGet list of debt currency data and repay currencies. Debt currencies include both cross and isolated debts. Only applicable to / .\n\nTrade one-click repay to repay cross debts. Isolated debts are not applicable. The maximum repayment amount is based on the remaining available balance of funding and trading accounts. Only applicable to / .\n\nGet the history and status of one-click repay trades in the past 7 days. Only applicable to / .\n\nGet list of debt currency data and repay currencies. Only applicable to .\n\nTrade one-click repay to repay debts. Only applicable to .\n\nGet the history and status of one-click repay trades in the past 7 days. Only applicable to .\n\nCancel all the MMP pending orders of an instrument family.\n\n\n\nOnly applicable to Option in Portfolio Margin mode, and MMP privilege is required.\n\nCancel all pending orders after the countdown timeout. Applicable to all trading symbols through order book (except Spread trading)\n\n\n\nOnly new order requests and amendment order requests will be counted towards this limit. For batch order requests consisting of multiple orders, each order will be counted individually. \n\n\n\nFor details, please refer to Fill ratio based sub-account rate limit\n\nThis endpoint is used to precheck the account information before and after placing the order. \n\n Only applicable to , and .\n\nRetrieve order information. Data will not be pushed when first subscribed. Data will only be pushed when there are order updates.\n\n\n\nConcurrent connection to this channel will be restricted by the following rules: WebSocket connection count limit.\n\nRetrieve transaction information. Data will not be pushed when first subscribed. Data will only be pushed when there are order book fill events, where tradeId > 0.\n\n\n\nThe channel is exclusively available to users with trading fee tier VIP5 or above. For other users, please use WS / Order channel.\n\nYou can place an order only if you have sufficient funds.\n\n\n\n\n\nRate Limit of lead instruments for Copy Trading: 4 requests per 2 seconds\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nPlace orders in a batch. Maximum 20 orders can be placed per request\n\n\n\n\n\nRate Limit of lead instruments for Copy Trading: 4 orders per 2 seconds\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nCancel incomplete orders in batches. Maximum 20 orders can be canceled per request.\n\nRate Limit of lead instruments for Copy Trading: 4 requests per 2 seconds\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nAmend incomplete orders in batches. Maximum 20 orders can be amended per request.\n\nRate Limit of lead instruments for Copy Trading: 4 orders per 2 seconds\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nCancel all the MMP pending orders of an instrument family.\n\n\n\nOnly applicable to Option in Portfolio Margin mode, and MMP privilege is required.\n\nRate Limit of lead instruments for Copy Trading: 1 request per 2 seconds\n\nlearn more about Take Profit / Stop Loss Order\n\nChase order It will place a Post Only order immediately and amend it continuously\n\n Chase order and corresponding Post Only order can't be amended.\n\nCancel unfilled algo orders. A maximum of 10 orders can be canceled per request. Request parameters should be passed in the form of an array.\n\nAmend unfilled algo orders (Support Stop order and Trigger order only, not including Move_order_stop order, Iceberg order, TWAP order, Trailing Stop order).\n\n\n\nThis endpoint will be offline soon, please use Cancel algo order \n\n\n\nCancel unfilled algo orders (including Iceberg order, TWAP order, Trailing Stop order). A maximum of 10 orders can be canceled per request. Request parameters should be passed in the form of an array.\n\nRetrieve a list of untriggered Algo orders under the current account.\n\nRetrieve a list of all algo orders under the current account in the last 3 months.\n\nRetrieve algo orders (includes order, order, order). Data will not be pushed when first subscribed. Data will only be pushed when there are order updates.\n\nRetrieve advance algo orders (including Iceberg order, TWAP order, Trailing order). Data will be pushed when first subscribed. Data will be pushed when triggered by events such as placing/canceling order.\n\nGrid trading works by the simple strategy of buy low and sell high. After you set the parameters, the system automatically places orders at incrementally increasing or decreasing prices. Overall, the grid bot seeks to capitalize on normal price volatility by placing buy and sell orders at certain regular intervals above and below a predefined base price.\n\n The API endpoints of require authentication.\n\nA maximum of 10 orders can be stopped per request.\n\nClose position when the contract grid stop type is 'keep position'.\n\nIt is used to add investment and only applicable to contract gird.\n\nAuthentication is not required for this public endpoint.\n\nAuthentication is not required for this public endpoint.\n\nAuthentication is not required for this public endpoint.\n\nAuthentication is not required for this public endpoint.\n\n\n\nMaximum grid quantity can be retrieved from this endpoint. Minimum grid quantity always is 2.\n\nRetrieve spot grid algo orders. Data will be pushed when triggered by events such as placing/canceling order. It will also be pushed in regular interval according to subscription granularity.\n\nRetrieve contract grid algo orders. Data will be pushed when triggered by events such as placing/canceling order. It will also be pushed in regular interval according to subscription granularity.\n\nRetrieve contract grid positions. Data will be pushed when triggered by events such as placing/canceling order.\n\n Please ignore the empty data.\n\nRetrieve grid sub orders. Data will be pushed when triggered by events such as placing order.\n\n Please ignore the empty data.\n\nCreate and customize your own signals while gaining access to a diverse selection of signals from top providers. Empower your trading strategies and stay ahead of the game with our comprehensive signal trading platform. Learn more\n\nA maximum of 10 orders can be stopped per request.\n\nRetrieve the updated position data for the last 3 months. Return in reverse chronological order using utime.\n\nClose the position of an instrument via a market order.\n\nYou can place an order only if you have sufficient funds.\n\n\n\n\n\nRecurring buy is a strategy for investing a fixed amount in crypto at fixed intervals. An appropriate recurring approach in volatile markets allows you to buy crypto at lower costs. Learn more\n\n The API endpoints of require authentication.\n\nA maximum of 10 orders can be stopped per request.\n\nRetrieve recurring buy orders. Data will be pushed when triggered by events. It will also be pushed in regular interval according to subscription granularity.\n• The procedure can refer to How to become a lead trader;\n• You can know whether you are a lead trader by checking whether or from Get account configuration is 1.\n• GET / Leading instruments can get instruments that are supported to have leading trades and the instruments that you enable leading trade. For instruments that are disenabled copy trading, you can still trade normally, but copy trading will not be triggered;\n• Amend leading instruments can amend your leading instruments. You need to set initial leading instruments while applying to become a leading trader. All non-leading contracts can't have position or pending orders for the current request when setting non-leading contracts as leading contracts.\n• You can open the position by placing order endpoints and channels including Place order endpoint, Place multiple orders endpoint, Place order channel, Place multiple orders channel, should be for lead trading.\n• For buy/sell mode, the orders must be in the same direction as your existing positions and open orders. You can select the direction you want if the instrument does not have position and pending orders.\n• For long/short mode, you can open long or open short as you want.\n• You can close the position with customized price or size by placing order endpoints and channels including Place order endpoint, Place multiple orders endpoint, Place order channel, Place multiple orders channel, or close the position by Close positions / Close lead position;\n• Close positions can close certain position under the current instrument(e.g. the long or short position under long/shor mode ), which can contain multiple leading positions;\n• Close lead position can only close a leading position once a time. It is required to pass subPosId which can get from Get existing leading positions.\n• TP/SL can be set by Place algo order or Place lead stop order;\n• Place algo order can set TP/SL for certain position under the current instrument(e.g. the long or short position under long/shor mode ), which can contain multiple leading positions;\n• Place lead stop order set set TP/SL for only a leading position once a time. It is required to pass subPosId which can get from Get existing leading positions.\n\nRetrieve lead positions that are not closed.\n\n\n\nRetrieve the completed lead position of the last 3 months.\n\n Returns reverse chronological order with .\n\nSet TP/SL for the current lead position that are not closed.\n\nYou can only close a lead position once a time. \n\n It is required to pass subPosId which can get from Get existing leading positions.\n\nRetrieve instruments that are supported to lead by the platform. Retrieve instruments that the lead trader has set.\n\nThe leading trader can amend current leading instruments, need to set initial leading instruments while applying to become a leading trader.\n\n All non-leading instruments can't have position or pending orders for the current request when setting non-leading instruments as leading instruments.\n\nThe leading trader gets profits shared details for the last 3 months.\n\nThe leading trader gets the total amount of profit shared since joining the platform.\n\nThe leading trader gets the profit sharing details that are expected to be shared in the next settlement cycle.\n\n The unrealized profit sharing details will update once there copy position is closed.\n\nThe leading trader gets the total unrealized amount of profit shared.\n\nIt is used to amend profit sharing ratio.\n\nThe first copy settings for the certain lead trader. You need to first copy settings after stopping copying.\n\nYou need to use this endpoint to amend copy settings\n\nYou need to use this endpoint to stop copy trading\n\nRetrieve the copy settings about certain lead trader.\n\nPublic endpoint. The most frequently traded crypto of this lead trader. Results are sorted by ratio from large to small.\n\nPublic endpoint. Retrieve the lead trader completed leading position of the last 3 months.\n\n Returns reverse chronological order with .\n\nPublic endpoint. Retrieve copy trader coming from certain lead trader. Return according to from high to low\n\nThe notification when failing to lead trade.\n\nThe API endpoints of do not require authentication.\n\n There are multiple services for market data, and each service has an independent cache. A random service will be requested for every request. So for two requests, it’s expected that the data obtained in the second request is earlier than the first request.\n\nRetrieve the latest price snapshot, best bid/ask price, and trading volume in the last 24 hours.\n\nRetrieve the latest price snapshot, best bid/ask price, and trading volume in the last 24 hours.\n\nRetrieve order book of the instrument. The data will be updated once a second.\n\nRetrieve the candlestick charts. This endpoint can retrieve the latest 1,440 data entries. Charts are returned in groups based on the requested bar.\n\nRetrieve history candlestick charts from recent years(It is last 3 months supported for 1s candlestick).\n\nRetrieve the recent transactions of an instrument.\n\nRetrieve the recent transactions of an instrument from the last 3 months with pagination.\n\nRetrieve the recent transactions of an instrument under same instFamily. The maximum is 100.\n\nThe 24-hour trading volume is calculated on a rolling basis.\n\nRetrieve the last traded price, bid price, ask price and 24-hour trading volume of instruments. \n\n The fastest rate is 1 update/100ms. There will be no update if the event is not triggered. The events which can trigger update: trade, the change on best ask/bid.\n\nRetrieve the candlesticks data of an instrument. the push frequency is the fastest interval 1 second push the data.\n\nRetrieve the recent trades data. Data will be pushed whenever there is a trade. Every update may aggregate multiple trades. \n\n\n\n\n\nThe message is sent only once per taker order, per filled price. The count field is used to represent the number of aggregated matches.\n\nRetrieve the recent trades data. Data will be pushed whenever there is a trade. Every update contain only one trade.\n\nUse for 400 depth levels, for 5 depth levels, tick-by-tick 1 depth level, tick-by-tick 50 depth levels, and for tick-by-tick 400 depth levels.\n• : 400 depth levels will be pushed in the initial full snapshot. Incremental data will be pushed every 100 ms for the changes in the order book during that period of time.\n• : 5 depth levels snapshot will be pushed in the initial push. Snapshot data will be pushed every 100 ms when there are changes in the 5 depth levels snapshot.\n• : 1 depth level snapshot will be pushed in the initial push. Snapshot data will be pushed every 10 ms when there are changes in the 1 depth level snapshot.\n• : 400 depth levels will be pushed in the initial full snapshot. Incremental data will be pushed every 10 ms for the changes in the order book during that period of time.\n• : 50 depth levels will be pushed in the initial full snapshot. Incremental data will be pushed every 10 ms for the changes in the order book during that period of time.\n• The push sequence for order book channels within the same connection and trading symbols is fixed as: bbo-tbt -> books-l2-tbt -> books50-l2-tbt -> books -> books5.\n• Users can not simultaneously subscribe to and channels for the same trading symbol.\n• For more details, please refer to the changelog 2024-07-17\n\nis the sequence ID of the market data published. The set of sequence ID received by users is the same if users are connecting to the same channel through multiple websocket connections. Each has an unique set of sequence ID. Users can use and to build the message sequencing for incremental order book updates. Generally the value of seqId is larger than prevSeqId. The in the new message matches with of the previous message. The smallest possible sequence ID value is 0, except in snapshot messages where the prevSeqId is always -1.\n\n\n\nExceptions:\n\n 1. If there are no updates to the depth for an extended period, OKX will send a message with to inform users that the connection is still active. is the same as the last sent message and equals to . 2. The sequence number may be reset due to maintenance, and in this case, users will receive an incremental message with smaller than . However, subsequent messages will follow the regular sequencing rule.\n\nThis mechanism can assist users in checking the accuracy of depth data.\n\nAfter subscribing to the incremental load push (such as 400 levels) of Order Book Channel, users first receive the initial full load of market depth. After the incremental load is subsequently received, update the local full load.\n• If there is the same price, compare the size. If the size is 0, delete this depth data. If the size changes, replace the original data.\n• If there is no same price, sort by price (bid in descending order, ask in ascending order), and insert the depth information into the full load.\n\nUse the first 25 bids and asks in the full load to form a string (where a colon connects the price and size in an ask or a bid), and then calculate the CRC32 value (32-bit signed integer).\n• When the bid and ask depth data exceeds 25 levels, each of them will intercept 25 levels of data, and the string to be checked is queued in a way that the bid and ask depth data are alternately arranged. \n\nSuch as: : : : ...\n• When the bid or ask depth data is less than 25 levels, the missing depth data will be ignored.\n\nSuch as: : : : ...\n\nRetrieve the recent trades data. Data will be pushed whenever there is a trade. Every update contain only one trade.\n\nA block trade is a large sized, privately negotiated transaction that allows traders to execute spot, perpetuals, futures, options and a combination of instruments (multi leg) which are traded outside the order book and at a mutually agreed price between the counter-parties. Once the transaction economics have been agreed upon, it will be submitted to OKX to be seamlessly margined, cleared and executed.\n• RFQs - Request for Quote sent by the Taker to Maker(s). It captures the quantity, instrument or multi instrument strategy that a Taker wants to trade.\n• Quotes - Quotes are created by the Maker in response to a requested RFQ.\n• Trades - Trades occur when the Taker successfully executes upon a makers quote to an RFQ.\n\nTo trade as either Taker or Maker, users need to deposit at least 100,000 USD into their trading account. In addition, to become a Maker, Please complete the form to access block trading.\n• Taker creates an RFQ and selects which counterparties to broadcast the RFQ to.\n• Multiple Maker(s) send a two way quote as a response to the RFQ.\n• Taker chooses to execute upon the best quote and the trade is sent to OKX for clearing & settlement.\n\nSelf-trade Prevention Users cannot send RFQ requests to themselves.\n• Taker creates an RFQ using . Taker can pull available instruments via and available counterparties from .\n• Taker can cancel an RFQ anytime until it becomes inactive with .\n• Maker, who is a requested counterparty to the RFQ, and is notified over the WebSocket channel, can provide a Quote to the RFQ.\n• Taker, who will be notified of quotes from the WebSocket channel, can execute upon the best Quote with .\n• Taker will receive confirmation of the trade's successful execution on the and WebSocket channel.\n• Taker will also receive confirmation of the trade being completed on the WebSocket channel as well as all other block trades on OKX.\n• Maker is notified about a new RFQ who they are a counterparty to, on the WebSocket channel.\n• Maker can create a one way or two way Quote using .\n• Maker can cancel an existing quote anytime until it becomes inactive with .\n• Taker chooses to execute upon an available Quote.\n• Maker will receive updates of their Quote from the WebSocket channel.\n• Maker will receive confirmation of the successful execution of their Quote from the and WebSocket channel.\n• Maker will receive confirmation of the trade being completed on the WebSocket channel as well as all other block trades on OKX.\n\nRetrieves the list of counterparties that the user is permitted to trade with.\n\nTo learn more, please visit Support center > FAQ > Trading > Liquid marketplace > Demo trading\n\nCancel an existing active RFQ that you have created previously.\n\nCancel one or multiple active RFQ(s) in a single batch. Maximum 100 RFQ orders can be canceled per request.\n\nExecutes a Quote. It is only used by the creator of the RFQ\n\nRetrieve the products which makers want to quote and receive RFQs for, and the corresponding price and size limit.\n\nCustomize the products which makers want to quote and receive RFQs for, and the corresponding price and size limit.\n\nReset the MMP status to be inactive.\n\nThis endpoint is used to set MMP configure and only applicable to block trading makers\n\n\n\nThis endpoint is used to get MMP configure information and only applicable to block trading market makers\n\n\n\nAllows the user to Quote an RFQ that they are a counterparty to. The user MUST quote the entire RFQ and not part of the legs or part of the quantity. Partial quoting is not allowed.\n\nCancels an existing active Quote you have created in response to an RFQ.\n\nCancel one or multiple active Quote(s) in a single batch. Maximum 100 quote orders can be canceled per request.\n\nCancel all quotes after the countdown timeout.\n\nRetrieves details of RFQs that the user is a counterparty to (either as the creator or the receiver of the RFQ).\n\nRetrieve all Quotes that the user is a counterparty to (either as the creator or the receiver).\n\nRetrieves the executed trades that the user is a counterparty to (either as the creator or the receiver).\n\nRetrieve the latest block trading volume in the last 24 hours.\n\nRetrieve the latest block trading volume in the last 24 hours.\n\nRetrieves the executed block trades. The data will be updated 15 minutes after the block trade execution.\n\nRetrieve the recent block trading transactions of an instrument. Descending order by tradeId. The data will be updated 15 minutes after the block trade execution.\n\nRetrieve the RFQs sent or received by the user. Data will be pushed whenever the user sends or receives an RFQ.\n\nRetrieve the Quotes sent or received by the user. Data will be pushed whenever the user sends or receives a Quote.\n\nRetrieve user's block trades data. All the legs in the same block trade are included in the same update. Data will be pushed whenever there is a block trade that the user is a counterparty for.\n\nRetrieve the recent block trades data in OKX. All the legs in the same block trade are included in the same update. The data will be pushed 15 minutes after the block trade execution.\n\nRetrieve the recent block trades data by individual legs. Each leg in a block trade is pushed in a separate update. The data will be pushed 15 minutes after the block trade execution.\n\nRetrieve the latest block trading volume in the last 24 hours.\n\nThe data will be pushed when triggered by transaction execution event. In addition, it will also be pushed in 5 minutes interval according to subscription granularity.\n• Spread - Entering a trade where the trader is long one instrument and short an offsetting quantity of a related instrument, forming a trade with two risk offsetting legs.\n• Order-book - A collection of offers to trade an instrument or basket. Each offer contains a defined instrument or group of instruments, relevant quantity, and the price at which the offerer is willing to transact. Takers can then immediately consume these offers up to the full amount of quantity listed at the offered price. The pending order limit of spread trading is 500 across all spreads.\n\nNitro Spreads is centered around the familiar concept of a Central Limit Order Book (CLOB).\n• Spreads consist of instruments sourced from OKX where they are cleared and settled.\n• Anyone can act as a \"Taker,\" who consumes an existing resting order, or a \"Maker,\" whose order is consumed.\n• Trades take place when orders are crossed. Trades are then sent for clearing and settlement on OKX.\n\nAt a high level, the Nitro Spreads workflow is as follows:\n• The crossed orders are sent for clearing and settlement.\n• The Taker and Maker receive confirmation of the success or rejection of the Trade.\n• All users are notified of successfully settled & cleared Trades, minus the counterparties or sides ( / ) involved.\n• The availability of trading Spreads is determined by OKX. Typically, these Spreads encompass all possible combinations of delta one derivatives (Expiry Futures and Perpetual Futures) and SPOT within a specific instrument family (e.g. \"BTC/USDT\" or \"ETH/USDC\").\n• Partial fills and multiple orders can be consumed as part of a single trade.\n• Counterparties are NOT selected. All Spread Order Books can be engaged by anyone, effectively trading against the broader market.\n• Anonymity is maintained throughout the process, with all orders and trades conducted on an anonymous basis.\n• Users have the flexibility to place multiple orders on both the bid and ask sides of the Order Book, allowing for a ladder-style configuration.\n\nA user assumes the role of a Maker when their Order is executed upon by another Order. A user becomes a Taker when they submit an Order that crosses an existing Order in the Order Book.\n\nTo retrieve all available Spreads for trading on OKX, make a request to the endpoint.\n\nTo retrieve orders on OKX, make a request to the endpoint.\n\nTo retrieve trades on OKX, make a request to the endpoint.\n\nTo submit an order to a Spread's Order Book, make a request to the endpoint.\n\nThere are three different states during a Spread's life cycle: , , and as detailed below:\n• : Spreads that are actively traded on Nitro Spreads\n• : Spreads in which at least one of the legs is suspended and the other one is active or suspended on the OKX orderbook exchange; or spreads in which the underlying instruments are still live on the OKX orderbook exchange, but removed from Nitro Spreads\n• : Spreads in which at least one of the underlying instruments is expired on the OKX orderbook exchange\n\nPlease refer to the following table for all possible scenarios given the state of the underlying instruments and the resulting state of the spread on Nitro Spreads (except for the case that the spread is delisted on Nitro Spreads):\n\nIn order for a trade to take place, two orders must be crossed within a Spread's Order Book.\n\nObtain information about the state of an Order and determine if it has reached its final state by monitoring the WebSocket channel. The key in the channel indicates the current state of the Order. If the state is or , it means that the Order still has available size ( ) that the creator or another user can take action on. On the other hand, if the state is or , the Order no longer has any available actions that the creator or any other user can take action on.\n\nIt is important to closely track the values of the following attributes: (size), (pending fill size), (canceled size), and (accumulated fill size). These attributes provide crucial information regarding the status and progression of the Order.\n\nTrack the state of an order by subscribing to the WebSocket channel.\n• Upon submitting an order, whether as a Maker or Taker, an order update message is sent via the orders WebSocket channel. The message will indicate the order's == .\n• Order matching and trade settlement are asynchronous processes. When the order is matched but not settled, system pushes > 0 and == \"\"\n• If the order is partially filled, an order update message is sent with == .\n• In the event that the order is completely filled, an order update message is sent with the == .\n• If the order is not fully filled but has reached its final state, an order update message is sent with the == .\n• If a certain part of an order is rejected, an order update message is sent with updated and , and and corresponding to the error.\n\nTrack the state of a trade by subscribing to the WebSocket channel.\n• After an executed trade undergoes clearing and settlement on OKX, it reaches finality.\n• For successfully cleared trades, a WebSocket message is sent with the denoted as .\n• In the case of an unsuccessful trade clearing, a trade update message is sent with the reflected as .\n• If the trade state is , the trade update message will also include the error and a corresponding error message ( ) that explains the reason for the rejection.\n\nAll users have the ability to receive updates on all trades that take place through the OKX Nitro Spreads product.\n\nIt's important to note that OKX Nitro Spreads does not disclose information about the counterparties involved in the trades or the individual ( or ) of the composite legs that were traded.\n• By subscribing to the WebSocket channel, WebSocket messages are sent exclusively for trades that have been successfully cleared and settled.\n\nRetrieve all incomplete orders under the current account.\n\nRetrieve the completed order data for the last 21 days, and the incomplete orders (filledSz =0 & state = canceled) that have been canceled are only reserved for 2 hours. Results are returned in counter chronological order of orders creation.\n\nRetrieve the completed order data for the last 3 months, including those placed 3 months ago but completed in the last 3 months. Results are returned in counter chronological order.\n\nRetrieve historical transaction details for the last 7 days. Results are returned in counter chronological order.\n\nRetrieve all available spreads based on the request parameters.\n\nRetrieve the order book of the spread.\n\nRetrieve the latest price snapshot, best bid/ask price and quantity.\n\nRetrieve the recent transactions of an instrument (at most 500 records per request). Results are returned in counter chronological order.\n\nRetrieve the candlestick charts. This endpoint can retrieve the latest 1,440 data entries. Charts are returned in groups based on the requested bar.\n\nCancel all pending orders after the countdown timeout. Only applicable to spread trading.\n\nYou can place an order only if you have sufficient funds.\n\n\n\nRetrieve order information from the Websocket channel. Data will not be pushed when first subscribed. Data will only be pushed when triggered by events such as placing/canceling order.\n\nAll updates relating to User's Trades are sent through the WebSocket Notifications channel.\n\nThis is a private channel and consumable solely by the authenticated user.\n\nUpdates received through the WebSocket Notification channel can include Trades being or .\n\nYou may receive multiple notifications if an Order of yours interacts with more than one other Order.\n• : 1 depth level snapshot will be pushed in the initial push. Snapshot data will be pushed every 10 ms when there are changes in the 1 depth level snapshot.\n• : 5 depth levels snapshot will be pushed in the initial push. Snapshot data will be pushed every 100 ms when there are changes in the 5 depth levels snapshot.\n• : 400 depth levels will be pushed in the initial full snapshot. Incremental data will be pushed every 10 ms for the changes in the order book during that period of time.\n• The push sequence for order book channels within the same connection and trading symbols is fixed as: sprd-bbo-tbt -> sprd-books-l2-tbt -> sprd-books5.\n\nis the sequence ID of the market data published. The set of sequence ID received by users is the same if users are connecting to the same channel through multiple websocket connections. Each has an unique set of sequence ID. Users can use and to build the message sequencing for incremental order book updates. Generally the value of seqId is larger than prevSeqId. The in the new message matches with of the previous message. The smallest possible sequence ID value is 0, except in snapshot messages where the prevSeqId is always -1.\n\n\n\nExceptions:\n\n 1. If there are no updates to the depth for an extended period, OKX will send a message with to inform users that the connection is still active. is the same as the last sent message and equals to . 2. The sequence number may be reset due to maintenance, and in this case, users will receive an incremental message with smaller than . However, subsequent messages will follow the regular sequencing rule.\n\nThis mechanism can assist users in checking the accuracy of depth data.\n\nAfter subscribing to the incremental load push (such as 400 levels) of Order Book Channel, users first receive the initial full load of market depth. After the incremental load is subsequently received, update the local full load.\n• If there is the same price, compare the size. If the size is 0, delete this depth data. If the size changes, replace the original data.\n• If there is no same price, sort by price (bid in descending order, ask in ascending order), and insert the depth information into the full load.\n\nUse the first 25 bids and asks in the full load to form a string (where a colon connects the price and size in an ask or a bid), and then calculate the CRC32 value (32-bit signed integer).\n\nRetrieve the recent trades data from . Data will be pushed whenever there is a trade. Every update contains only one trade.\n\nRetrieve the last traded price, bid price, ask price. The fastest rate is 1 update/100ms. There will be no update if the event is not triggered. The events which can trigger update: trade, the change on best ask/bid price\n\nRetrieve the candlesticks data of an instrument. The push frequency is the fastest interval 1 second push the data.\n\nThe API endpoints of do not require authentication.\n\nRetrieve a list of instruments with open contracts for OKX. Retrieve available instruments info of current account, please refer to Get instruments.\n\nRetrieve the estimated delivery price which will only have a return value one hour before the delivery/exercise.\n\nRetrieve delivery records of Futures and exercise records of Options in the last 3 months.\n\nRetrieve the estimated settlement price which will only have a return value one hour before the settlement.\n\nRetrieve settlement records of futures in the last 3 months.\n\nRetrieve funding rate history. This endpoint can retrieve data from the last 3 months.\n\nRetrieve the total open interest for contracts on OKX.\n\nRetrieve the highest buy limit and lowest sell limit of the instrument.\n\nWe set the mark price based on the SPOT index and at a reasonable basis to prevent individual users from manipulating the market and causing the contract price to fluctuate.\n\nRetrieve position tiers information, maximum leverage depends on your borrowings and margin ratio.\n\nConvert the crypto value to the number of contracts, or vice versa\n\nIt will return premium data in the past 6 months.\n\nRetrieve the candlestick charts of the index. This endpoint can retrieve the latest 1,440 data entries. Charts are returned in groups based on the requested bar.\n\nRetrieve the candlestick charts of the index from recent years.\n\nRetrieve the candlestick charts of mark price. This endpoint can retrieve the latest 1,440 data entries. Charts are returned in groups based on the requested bar.\n\nRetrieve the candlestick charts of mark price from recent years.\n\nThis interface provides the average exchange rate data for 2 weeks\n\nGet the index component information data on the market\n\nGet the macro-economic calendar data within 3 months. Historical data from 3 months ago is only available to users with trading fee tier VIP1 and above.\n\nThe instruments will be pushed if there is any change to the instrument’s state (such as delivery of FUTURES, exercise of OPTION, listing of new contracts / trading pairs, trading suspension, etc.).\n\n (The full instrument list is not pushed since December 28, 2022, you can click here to view details)\n\nRetrieve the open interest. Data will be pushed every 3 seconds when there are updates.\n\nRetrieve funding rate. Data will be pushed in 30s to 90s.\n\nRetrieve the maximum buy price and minimum sell price of instruments. Data will be pushed every 200ms when there are changes in limits, and will not be pushed when there is no changes on limit.\n\nRetrieve detailed pricing information of all OPTION contracts. Data will be pushed at once.\n\nRetrieve the estimated delivery/exercise/settlement price of and contracts.\n\nOnly the estimated price will be pushed in an hour before delivery/exercise/settlement, and will be pushed if there is any price change.\n\nRetrieve the mark price. Data will be pushed every 200 ms when the mark price changes, and will be pushed every 10 seconds when the mark price does not change.\n\nRetrieve index tickers data. Push data every 100ms if there are any changes, otherwise push once a minute.\n\nRetrieve the candlesticks data of the mark price. The push frequency is the fastest interval 1 second push the data.\n\nRetrieve the candlesticks data of the index. The push frequency is the fastest interval 1 second push the data. .\n\nRetrieve the recent liquidation orders. For futures and swaps, each contract will only show a maximum of one order per one-second period. This data doesn’t represent the total number of liquidations on OKX.\n\nIn the state, data will be pushed once every minute to display the balance of insurance fund and etc.\n\nIn the warning state or when there is ADL risk ( ), data will be pushed every second to display information such as the real-time decline rate of insurance fund.\n\nFor more ADL details, please refer to Introduction to Auto-deleveraging\n\nRetrieve the most up-to-date economic calendar data. This endpoint is only applicable to VIP 1 and above users in the trading fee tier.\n\nThe API endpoints of do not require authentication.\n\nRetrieve the currencies supported by the trading statistics endpoints.\n\nRetrieve the contract open interest statistics of futures and perp. This endpoint can retrieve the latest 1,440 data entries. \n\n\n\nFor period=1D, the data time range is up to January 1, 2024; for other periods, the data time range is up to early February 2024.\n\nThe data returned will be arranged in an array like this: [ts, oi, oiCcy, oiUsd].\n\nRetrieve the taker volume for both buyers and sellers.\n\nRetrieve the contract taker volume for both buyers and sellers. This endpoint can retrieve the latest 1,440 data entries. \n\n\n\nFor period=1D, the data time range is up to January 1, 2024; for other periods, the data time range is up to early February 2024.\n\nThe data returned will be arranged in an array like this: [ts, sellVol, buyVol].\n\nRetrieve the ratio of cumulative amount of quote currency to base currency.\n\nRetrieve the account net long/short ratio of a contract for top traders. Top traders refer to the top 5% of traders with the largest open position value. This endpoint can retrieve the latest 1,440 data entries. The data time range is up to March 22, 2024.\n\nThe data returned will be arranged in an array like this: [ts, longShortAcctRatio].\n\nRetrieve the position long/short ratio of a contract for top traders. Top traders refer to the top 5% of traders with the largest open position value. This endpoint can retrieve the latest 1,440 data entries. The data time range is up to March 22, 2024.\n\nThe data returned will be arranged in an array like this: [ts, longShortPosRatio].\n\nRetrieve the account long/short ratio of a contract. This endpoint can retrieve the latest 1,440 data entries. \n\n\n\nFor period=1D, the data time range is up to January 1, 2024; for other periods, the data time range is up to early February 2024.\n\nThe data returned will be arranged in an array like this: [ts, longAcctPosRatio].\n\nRetrieve the ratio of users with net long vs net short positions for Expiry Futures and Perpetual Futures.\n\nRetrieve the open interest and trading volume for Expiry Futures and Perpetual Futures.\n\nRetrieve the open interest and trading volume for options.\n\nRetrieve the open interest ratio and trading volume ratio of calls vs puts.\n\nRetrieve the open interest and trading volume of calls and puts for each upcoming expiration.\n\nRetrieve the taker volume for both buyers and sellers of calls and puts.\n\nThis shows the relative buy/sell volume for calls and puts. It shows whether traders are bullish or bearish on price and volatility.\n\nRetrieve a list of all currencies available which are related to the current account's KYC entity.\n\nRetrieve the funding account balances of all the assets and the amount that is available or on hold.\n\nOnly API keys with privilege can call this endpoint.\n\nThis endpoint supports the transfer of funds between your funding account and trading account, and from the master account to sub-accounts.\n\nSub-account can transfer out to master account by default. Need to call Set permission of transfer out to grant privilege first if you want sub-account transferring to another sub-account (sub-accounts need to belong to same master account.)\n\nRetrieve the transfer state data of the last 2 weeks.\n\nQuery the billing record in the past month.\n\nRetrieve the deposit records according to the currency, deposit status, and time range in reverse chronological order. The 100 most recent records are returned by default.\n\n Websocket API is also available, refer to Deposit info channel.\n\nOnly supported withdrawal of assets from funding account. Common sub-account does not support withdrawal.\n\nYou can cancel normal withdrawal requests, but you cannot cancel withdrawal requests on Lightning.\n\nRetrieve the withdrawal records according to the currency, withdrawal status, and time range in reverse chronological order. The 100 most recent records are returned by default.\n\n Websocket API is also available, refer to Withdrawal info channel.\n\nAuthentication is not required for this public endpoint.\n\nApply for monthly statement in the past year.\n\nYou should make estimate quote before convert trade.\n\nFor the same side (buy/sell), there's a trading limit of 1 request per 5 seconds.\n\nTo display all the available fiat deposit payment methods\n\nTo display all the available fiat withdrawal payment methods\n\nInitiate a fiat withdrawal request (Authenticated endpoint, Only for API keys with \"Withdrawal\" access)\n\n Only supported withdrawal of assets from funding account.\n\nCancel a pending fiat withdrawal order, currently only applicable to TRY\n\nA push notification is triggered when a deposit is initiated or the deposit status changes.\n\n Supports subscriptions for accounts\n• If it is a master account subscription, you can receive the push of the deposit info of both the master account and the sub-account.\n• If it is a sub-account subscription, only the push of sub-account deposit info you can receive.\n\nA push notification is triggered when a withdrawal is initiated or the withdrawal status changes.\n\n Supports subscriptions for accounts\n• If it is a master account subscription, you can receive the push of the withdrawal info of both the master account and the sub-account.\n• If it is a sub-account subscription, only the push of sub-account withdrawal info you can receive.\n\nApplies to master accounts only and master accounts API Key must be linked to IP addresses. Only API keys with privilege can call this endpoint.\n\nQuery detailed balance info of Trading Account of a sub-account via the master account (applies to master accounts only)\n\nQuery detailed balance info of Funding Account of a sub-account via the master account (applies to master accounts only)\n\nRetrieve the maximum withdrawal information of a sub-account via the master account (applies to master accounts only). If no currency is specified, the transferable amount of all owned currencies will be returned.\n\nOnly applicable to the trading team's master account to getting transfer records of managed sub accounts entrusted to oneself.\n\nOnly API keys with privilege can call this endpoint.\n\nSet permission of transfer out for sub-account (only applicable to master account API key). Sub-account can transfer out to master account by default.\n\nThe trading team uses this interface to view the list of sub-accounts currently under escrow\n\nOnly the assets in the funding account can be used for purchase. More details\n\nETH Staking, also known as Ethereum Staking, is the process of participating in the Ethereum blockchain's Proof-of-Stake (PoS) consensus mechanism.\n\n Stake to receive BETH for liquidity at 1:1 ratio and earn daily BETH rewards\n\n Learn more about ETH Staking\n\nStaking ETH for BETH\n\n Only the assets in the funding account can be used.\n\n\n\ncode = means your request has been successfully handled.\n\nOnly the assets in the funding account can be used. If your BETH is in your trading account, you can make funding transfer first.\n\n\n\ncode = means your request has been successfully handled.\n\nThe balance is a snapshot summarized all BETH assets (including assets in redeeming) in account.\n\nBy staking SOL tokens and delegating them to validators on the Solana network, you can receive equivalent OKSOL and earn extra OKSOL rewards.\n\n Stake SOL on Solana to receive OKSOL at a 1:1 ratio for liquidity\n\n Learn more about OKSOL Staking\n\nStaking SOL for OKSOL\n\n Only the assets in the funding account can be used.\n\n\n\ncode = means your request has been successfully handled.\n\nOnly the assets in the funding account can be used. If your OKSOL is in your trading account, you can make funding transfer first.\n\n\n\ncode = means your request has been successfully handled.\n\nThe balance is summarized all OKSOL assets (including assets in redeeming) in account.\n\nSimple earn flexible (saving) is earned by lending to leveraged trading users in the lending market. learn more\n\nOnly the assets in the funding account can be used for saving.\n\nAuthentication is not required for this public endpoint.\n\nAuthentication is not required for this public endpoint.\n\n Only returned records after December 14, 2021.\n\nOKX Flexible Loan is a high-end loan product that allows users to increase cash flow without selling off their crypto. More details\n\ncode = means your request has been accepted (It doesn't mean the request has been successfully handled.)\n\nThe Affiliate API offers affiliate users a flexible function to query the invitee information. Simply enter the UID of your direct invitee to access their relevant information, empowering your affiliate business growth and day-to-day business operation. If you have additional data requirements regarding the Affiliate API, please don't hesitate to contact your BD. We will reach out to you through your BD to provide more comprehensive API support.\n\nThis endpoint will be offline soon, please use Get the invitee's detail \n\n\n\nIt is used to get the user's affiliate rebate information for affiliate.\n\nPlanned system maintenance that may result in short interruption (lasting less than 5 seconds) or websocket disconnection (users can immediately reconnect) will not be announced. The maintenance will only be performed during times of low market volatility.\n\nGet the status of system maintenance and push when rescheduling and the system maintenance status and end time changes. First subscription: \"Push the latest change data\"; every time there is a state change, push the changed content.\n\nPlanned system maintenance that may result in short interruption (lasting less than 5 seconds) or websocket disconnection (users can immediately reconnect) will not be announced. The maintenance will only be performed during times of low market volatility.\n\nGet announcements, the response is sorted by with the most recent first. The sort will not be affected if the announcement is updated. Every page has 20 records\n\n\n\n\n\nAuthentication is optional for this endpoint.\n\n\n\nIt will be regarded as private endpoint and authentication is required if OK-ACCESS-KEY in HTTP header is delivered.\n\n It will be regarded as public endpoint and authentication isn't required if OK-ACCESS-KEY in HTTP header isn't delivered. \n\n\n\n\n\nThere are differences between public endpoint and private endpoint. \n\n For public endpoint, the response is restricted based on your request IP.\n\n For private endpoint, the response is restricted based on your country of residence.\n\nAuthentication is not required for this public endpoint.\n\n\n\nHere is the REST API Error Code\n\nREST API Error Code is from 50000 to 59999.\n\n| 59206 | 200 | The lead trader doesn't have any more vacancies for copy traders | | 59216 | 200 | The position doesn't exist. Please try again | | 59218 | 200 | Closing all positions at market price... | | 59256 | 200 | To switch to One-way mode, lower the number of traders you copy to 1 | | 59247 | 200 | High leverage causes current position to exceed the maximum position size limit under this leverage. Adjust the leverage. | | 59260 | 200 | You are not a spot lead trader yet. Complete the application on our website or app first. | | 59262 | 200 | You aren't a contract lead trader yet. Complete the application first. | | 59641 | 200 | Can't switch account mode as you have fixed loan borrowings. | | 59642 | 200 | Lead and copy traders can only use spot or spot and futures modes | | 59643 | 200 | Couldn’t switch account modes as you’re currently copying spot trades | | 59245 | 200 | As a lead trader, number of {param0} contract per order must be no greater than {param1} | | 59263 | 200 | Only traders on the allowlist can use copy trading. ND brokers can reach out to BD for help. | | 59264 | 200 | Spot copy trading isn't supported | | 59267 | 200 | Cancellation failed as you aren't copying this trader | | 59268 | 200 | You can't copy trades with instId that hasn't been selected by the lead trader | | 59269 | 200 | This contract lead trader doesn't exist | | 59270 | 200 | Maximum total amount (copyTotalAmt) can't be lower than amount per order (copyAmt) when using fixed amount | | 59273 | 200 | You aren't a contract copy trader yet. Start by coping a contract trader. | | 59275 | 200 | You can't copy trade as you're applying to become a lead trader | | 59276 | 200 | You can't copy this lead trader as they've applied to stop leading trades | | 59277 | 200 | You can't copy this lead trader as they don't have any copy trader vacancies | | 59278 | 200 | Your request to stop copy trading is being processed. Try again later. | | 59279 | 200 | You've already copied this trader | | 59280 | 200 | You can't modify copy trade settings as you aren't copying this trader | | 59282 | 200 | Only ND sub-accounts under ND brokers whose main accounts are on the allowlist support this endpoint. Reach out to BD for help. | | 59283 | 200 | Your account isn’t currently using spot and futures mode | | 59284 | 200 | You've reached the monthly limit of {param0} ratio edits | | 59286 | 200 | You can't become a futures lead trader when using spot mode | | 59287 | 200 | Profit sharing ratio should be between {param0} and {param1} | | 59288 | 200 | You're leading trades but your account is in portfolio margin mode. Switch to spot and futures mode or multiple-currency margin mode and try again | | 59130 | 200 | The highest take profit level is {num}%. Enter a smaller number and try again. |\n\n| 59258 | 200 | Action not supported for lead traders | | 59259 | 200 | Enter a multiplier value that's within the valid range | | 59285 | 200 | You haven't led or copied any trades yet |"
    },
    {
        "link": "https://pypi.org/project/python-okx",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://my.okx.com/docs-v5/en",
        "document": "Welcome to our V5 API documentation. OKX provides REST and WebSocket APIs to suit your trading needs.\n• Learn how to trade with V5 API: Best practice to OKX’s v5 API\n• Get access to our market maker python sample code Python market maker sample\n• Please take 1 minute to help us improve: V5 API Satisfaction Survey\n• If you have any questions, please consult online customer service\n\nPlease refer to my api page regarding V5 API Key creation.\n\nCreate an API Key on the website before signing any requests. After creating an APIKey, keep the following information safe:\n\nThe system returns randomly-generated APIKeys and SecretKeys. You will need to provide the Passphrase to access the API. We store the salted hash of your Passphrase for authentication. We cannot recover the Passphrase if you have lost it. You will need to create a new set of APIKey.\n\n\n\n\n\nThere are three permissions below that can be associated with an API key. One or more permission can be assigned to any key.\n• Read : Can request and view account info such as bills and order history which need read permission\n• Trade : Can place and cancel orders, funding transfer, make settings which need write permission\n\nAll private REST requests must contain the following headers:\n• None The Base64-encoded signature (see Signing Messages subsection for details).\n• None The UTC timestamp of your request .e.g : 2020-12-08T09:08:57.715Z\n• None The passphrase you specified when creating the APIKey.\n\nRequest bodies should have content type and be in valid JSON format.\n\nThe header is generated as follows:\n• Sign the prehash string with the SecretKey using the HMAC SHA256.\n• Encode the signature in the Base64 format.\n\nThe value is the same as the header with millisecond ISO format, e.g. .\n\nThe request method should be in UPPERCASE: e.g. and .\n\nThe is the path of requesting an endpoint.\n\nThe refers to the String of the request body. It can be omitted if there is no request body (frequently the case for requests).\n\nThe SecretKey is generated when you create an APIKey.\n\nWebSocket is a new HTML5 protocol that achieves full-duplex data transmission between the client and server, allowing data to be transferred effectively in both directions. A connection between the client and server can be established with just one handshake. The server will then be able to push data to the client according to preset rules. Its advantages include:\n• The WebSocket request header size for data transmission between client and server is only 2 bytes.\n• Either the client or server can initiate data transmission.\n• There's no need to repeatedly create and delete TCP connections, saving resources on bandwidth and server.\n\nConnection limit: 3 requests per second (based on IP)\n\nWhen subscribing to a public channel, use the address of the public service. When subscribing to a private channel, use the address of the private service\n\nThe total number of 'subscribe'/'unsubscribe'/'login' requests per connection is limited to 480 times per hour.\n\nThe limit will be set at 30 WebSocket connections per specific WebSocket channel per sub-account. Each WebSocket connection is identified by the unique .\n\nThe WebSocket channels subject to this limitation are as follows:\n\nIf users subscribe to the same channel through the same WebSocket connection through multiple arguments, for example, by using and , it will be counted once only. If users subscribe to the listed channels (such as orders and accounts) using either the same or different connections, it will not affect the counting, as these are considered as two different channels. The system calculates the number of WebSocket connections per channel.\n\nThe platform will send the number of active connections to clients through the event message to new channel subscriptions.\n\nWhen the limit is breached, generally the latest connection that sends the subscription request will be rejected. Client will receive the usual subscription acknowledgement followed by the from the connection that the subscription has been terminated. In exceptional circumstances the platform may unsubscribe existing connections.\n\nOrder operations through WebSocket, including place, amend and cancel orders, are not impacted through this change.\n\napiKey: Unique identification for invoking API. Requires user to apply one manually.\n\ntimestamp: the Unix Epoch time, the unit is seconds, e.g. 1704876947\n\nsign: signature string, the signature algorithm is as follows:\n\nFirst concatenate , , , strings, then use HMAC SHA256 method to encrypt the concatenated string with SecretKey, and then perform Base64 encoding.\n\nsecretKey: The security key generated when the user applies for APIKey, e.g. : 22582BD0CFF14C41EDBF1AB98506286D\n\nWebSocket channels are divided into two categories: and channels.\n\n-- No authentication is required, include tickers channel, K-Line channel, limit price channel, order book channel, and mark price channel etc.\n\n-- including account channel, order channel, and position channel, etc -- require log in.\n\nUsers can choose to subscribe to one or more channels, and the total length of multiple channels cannot exceed 64 KB.\n\nBelow is an example of subscription parameters. The requirement of subscription parameters for each channel is different. For details please refer to the specification of each channels.\n\nUnsubscribe from one or more channels.\n\nClient will receive the information in the following scenarios:\n\n30 seconds prior to the upgrade of the WebSocket service, the notification message will be sent to users indicating that the connection will soon be disconnected. Users are encouraged to establish a new connection to prevent any disruptions caused by disconnection.\n\n\n\n\n\n The feature is supported by WebSocket Public (/ws/v5/public) and Private (/ws/v5/private) for now.\n\nTo facilitate your trading experience, please set the appropriate account mode before starting trading.\n\nIn the trading account trading system, 4 account modes are supported: , , , and .\n\nYou need to set on the Web/App for the first set of every account mode.\n\nCurrently, the V5 API works for Demo Trading, but some functions are not supported, such as , , , etc.\n\nOKX account can be used for login on Demo Trading. If you already have an OKX account, you can log in directly.\n\nStart API Demo Trading by the following steps:\n\n Login OKX —> Trade —> Demo Trading —> Personal Center —> Demo Trading API -> Create Demo Trading V5 API Key —> Start your Demo Trading\n\nOrders may not be processed in time due to network delay or busy OKX servers. You can configure the expiry time of the request using if you want the order request to be discarded after a specific time.\n\nIf is specified in the requests for Place (multiple) orders or Amend (multiple) orders, the request will not be processed if the current system time of the server is after the .\n\nSet the following parameters in the request header\n\nThe following endpoints are supported:\n\nThe following parameters are set in the request\n\nThe following endpoints are supported:\n\nOur REST and WebSocket APIs use rate limits to protect our APIs against malicious usage so our trading platform can operate reliably and fairly.\n\n When a request is rejected by our system due to rate limits, the system returns error code 50011 (Rate limit reached. Please refer to API documentation and throttle requests accordingly).\n\n The rate limit is different for each endpoint. You can find the limit for each endpoint from the endpoint details. Rate limit definitions are detailed below:\n• None WebSocket login and subscription rate limits are based on connection.\n• None Private REST rate limits are based on User ID (sub-accounts have individual User IDs).\n• None WebSocket order management rate limits are based on User ID (sub-accounts have individual User IDs).\n\nFor Trading-related APIs (place order, cancel order, and amend order) the following conditions apply:\n• None Rate limits are shared across the REST and WebSocket channels.\n• None Rate limits for placing orders, amending orders, and cancelling orders are independent from each other.\n• None Rate limits are defined on the Instrument ID level (except Options)\n• None Rate limits for Options are defined based on the Instrument Family level. Refer to the Get instruments endpoint to view Instrument Family information.\n• None Rate limits for a multiple order endpoint and a single order endpoint are also independent, with the exception being when there is only one order sent to a multiple order endpoint, the order will be counted as a single order and adopt the single order rate limit.\n\nAt the sub-account level, we allow a maximum of 1000 order requests per 2 seconds. Only new order requests and amendment order requests will be counted towards this limit. The limit encompasses all requests from the endpoints below. For batch order requests consisting of multiple orders, each order will be counted individually. Error code 50061 is returned when the sub-account rate limit is exceeded. The existing rate limit rule per instrument ID remains unchanged and the existing rate limit and sub-account rate limit will operate in parallel. If clients require a higher rate limit, clients can trade via multiple sub-accounts.\n\nThis is only applicable to >= VIP5 customers. \n\n As an incentive for more efficient trading, the exchange will offer a higher sub-account rate limit to clients with a high trade fill ratio. \n\n\n\n The exchange calculates two ratios based on the transaction data from the past 7 days at 00:00 UTC.\n• Sub-account fill ratio: This ratio is determined by dividing (the trade volume in USDT of the sub-account) by (sum of (new and amendment request count per symbol * symbol multiplier) of the sub-account). Note that the master trading account itself is also considered as a sub-account in this context.\n• Master account aggregated fill ratio: This ratio is calculated by dividing (the trade volume in USDT on the master account level) by (the sum (new and amendment count per symbol * symbol multiplier] of all sub-accounts).\n\nThe symbol multiplier allows for fine-tuning the weight of each symbol. A smaller symbol multiplier (<1) is used for smaller pairs that require more updates per trading volume. All instruments have a default symbol multiplier, and some instruments will have overridden symbol multipliers.\n\nThe fill ratio computation excludes block trading, spread trading, MMP and fiat orders for order count; and excludes block trading, spread trading for trade volume. Only successful order requests (sCode=0) are considered.\n\nAt 08:00 UTC, the system will use the maximum value between the sub-account fill ratio and the master account aggregated fill ratio based on the data snapshot at 00:00 UTC to determine the sub-account rate limit based on the table below. For broker (non-disclosed) clients, the system considers the sub-account fill ratio only.\n\nIf there is an improvement in the fill ratio and rate limit to be uplifted, the uplift will take effect immediately at 08:00 UTC. However, if the fill ratio decreases and the rate limit needs to be lowered, a one-day grace period will be granted, and the lowered rate limit will only be implemented on T+1 at 08:00 UTC. On T+1, if the fill ratio improves, the higher rate limit will be applied accordingly. In the event of client demotion to VIP4, their rate limit will be downgraded to Tier 1, accompanied by a one-day grace period.\n\nIf the 7-day trading volume of a sub-account is less than 1,000,000 USDT, the fill ratio of the master account will be applied to it.\n\nFor newly created sub-accounts, the Tier 1 rate limit will be applied at creation until T+1 8am UTC, at which the normal rules will be applied.\n\nBlock trading, spread trading, MMP and spot/margin orders are exempted from the sub-account rate limit.\n\nThe exchange offers GET / Account rate limit endpoint that provides ratio and rate limit data, which will be updated daily at 8am UTC. It will return the sub-account fill ratio, the master account aggregated fill ratio, current sub-account rate limit and sub-account rate limit on T+1 (applicable if the rate limit is going to be demoted). \n\n\n\n The fill ratio and rate limit calculation example is shown below. Client has 3 accounts, symbol multiplier for BTC-USDT-SWAP = 1 and XRP-USDT = 0.1.\n\nIf you require a higher request rate than our rate limit, you can set up different sub-accounts to batch request rate limits. We recommend this method for throttling or spacing out requests in order to maximize each accounts' rate limit and avoid disconnections or rejections.\n\nRetrieve a list of assets (with non-zero balance), remaining balance, and available amount in the trading account.\n\nDistribution of applicable fields under each account level are as follows:\n\nRetrieve the bills of the account. The bill refers to all transaction records that result in changing the balance of an account. Pagination is supported, and the response is sorted with the most recent first. This endpoint can retrieve data from the last 7 days.\n\nRetrieve the account’s bills. The bill refers to all transaction records that result in changing the balance of an account. Pagination is supported, and the response is sorted with most recent first. This endpoint can retrieve data from the last 3 months.\n\nThe maximum quantity to buy or sell. It corresponds to the \"sz\" from placement.\n\nRetrieve account information. Data will be pushed when triggered by events such as placing order, canceling order, transaction execution, etc. It will also be pushed in regular interval according to subscription granularity.\n\n\n\nConcurrent connection to this channel will be restricted by the following rules: WebSocket connection count limit.\n\nYou can place an order only if you have sufficient funds.\n\nPlace orders in batches. Maximum 20 orders can be placed per request. \n\n Request parameters should be passed in the form of an array. Orders will be placed in turn\n\n\n\nCancel incomplete orders in batches. Maximum 20 orders can be canceled per request. Request parameters should be passed in the form of an array.\n\nAmend incomplete orders in batches. Maximum 20 orders can be amended per request. Request parameters should be passed in the form of an array.\n\nRate limit of this endpoint will also be affected by the rules Sub-account rate limit and Fill ratio based sub-account rate limit.\n\nRetrieve all incomplete orders under the current account.\n\nGet completed orders which are placed in the last 7 days, including those placed 7 days ago but completed in the last 7 days. \n\n\n\nThe incomplete orders that have been canceled are only reserved for 2 hours.\n\nRetrieve the completed order data of the last 3 months, and the incomplete orders that have been canceled are only reserved for 2 hours.\n\nRetrieve recently-filled transaction details in the last 3 day.\n\nRetrieve recently-filled transaction details in the last 3 months.\n\nCancel all pending orders after the countdown timeout. Applicable to all trading symbols through order book (except Spread trading)\n\n\n\nOnly new order requests and amendment order requests will be counted towards this limit. For batch order requests consisting of multiple orders, each order will be counted individually. \n\n\n\nFor details, please refer to Fill ratio based sub-account rate limit\n\nRetrieve order information. Data will not be pushed when first subscribed. Data will only be pushed when there are order updates.\n\n\n\nYou can place an order only if you have sufficient funds.\n\n\n\n\n\nPlace orders in a batch. Maximum 20 orders can be placed per request\n\n\n\n\n\nCancel incomplete orders in batches. Maximum 20 orders can be canceled per request.\n\nAmend incomplete orders in batches. Maximum 20 orders can be amended per request.\n\nlearn more about Take Profit / Stop Loss Order\n\nCancel unfilled algo orders. A maximum of 10 orders can be canceled per request. Request parameters should be passed in the form of an array.\n\nAmend unfilled algo orders (Support Stop order and Trigger order only, not including Move_order_stop order, Iceberg order, TWAP order, Trailing Stop order).\n\n\n\nRetrieve a list of untriggered Algo orders under the current account.\n\nRetrieve a list of all algo orders under the current account in the last 3 months.\n\nRetrieve algo orders (includes order, order, order). Data will not be pushed when first subscribed. Data will only be pushed when there are order updates.\n\nRetrieve advance algo orders (including Iceberg order, TWAP order, Trailing order). Data will be pushed when first subscribed. Data will be pushed when triggered by events such as placing/canceling order.\n\nThe API endpoints of do not require authentication.\n\n There are multiple services for market data, and each service has an independent cache. A random service will be requested for every request. So for two requests, it’s expected that the data obtained in the second request is earlier than the first request.\n\nRetrieve the latest price snapshot, best bid/ask price, and trading volume in the last 24 hours.\n\nRetrieve the latest price snapshot, best bid/ask price, and trading volume in the last 24 hours.\n\nRetrieve order book of the instrument. The data will be updated once a second.\n\nRetrieve the candlestick charts. This endpoint can retrieve the latest 1,440 data entries. Charts are returned in groups based on the requested bar.\n\nRetrieve history candlestick charts from recent years(It is last 3 months supported for 1s candlestick).\n\nRetrieve the recent transactions of an instrument.\n\nRetrieve the recent transactions of an instrument from the last 3 months with pagination.\n\nRetrieve the last traded price, bid price, ask price and 24-hour trading volume of instruments. \n\n The fastest rate is 1 update/100ms. There will be no update if the event is not triggered. The events which can trigger update: trade, the change on best ask/bid.\n\nRetrieve the candlesticks data of an instrument. the push frequency is the fastest interval 1 second push the data.\n\nRetrieve the recent trades data. Data will be pushed whenever there is a trade. Every update may aggregate multiple trades. \n\n\n\n\n\nThe message is sent only once per taker order, per filled price. The count field is used to represent the number of aggregated matches.\n\nRetrieve the recent trades data. Data will be pushed whenever there is a trade. Every update contain only one trade.\n\nUse for 400 depth levels, for 5 depth levels, tick-by-tick 1 depth level, tick-by-tick 50 depth levels, and for tick-by-tick 400 depth levels.\n• : 400 depth levels will be pushed in the initial full snapshot. Incremental data will be pushed every 100 ms for the changes in the order book during that period of time.\n• : 5 depth levels snapshot will be pushed in the initial push. Snapshot data will be pushed every 100 ms when there are changes in the 5 depth levels snapshot.\n• : 1 depth level snapshot will be pushed in the initial push. Snapshot data will be pushed every 10 ms when there are changes in the 1 depth level snapshot.\n• : 400 depth levels will be pushed in the initial full snapshot. Incremental data will be pushed every 10 ms for the changes in the order book during that period of time.\n• : 50 depth levels will be pushed in the initial full snapshot. Incremental data will be pushed every 10 ms for the changes in the order book during that period of time.\n• The push sequence for order book channels within the same connection and trading symbols is fixed as: bbo-tbt -> books-l2-tbt -> books50-l2-tbt -> books -> books5.\n• Users can not simultaneously subscribe to and channels for the same trading symbol.\n• For more details, please refer to the changelog 2024-07-17\n\nis the sequence ID of the market data published. The set of sequence ID received by users is the same if users are connecting to the same channel through multiple websocket connections. Each has an unique set of sequence ID. Users can use and to build the message sequencing for incremental order book updates. Generally the value of seqId is larger than prevSeqId. The in the new message matches with of the previous message. The smallest possible sequence ID value is 0, except in snapshot messages where the prevSeqId is always -1.\n\n\n\nExceptions:\n\n 1. If there are no updates to the depth for an extended period, OKX will send a message with to inform users that the connection is still active. is the same as the last sent message and equals to . 2. The sequence number may be reset due to maintenance, and in this case, users will receive an incremental message with smaller than . However, subsequent messages will follow the regular sequencing rule.\n\nThis mechanism can assist users in checking the accuracy of depth data.\n\nAfter subscribing to the incremental load push (such as 400 levels) of Order Book Channel, users first receive the initial full load of market depth. After the incremental load is subsequently received, update the local full load.\n• If there is the same price, compare the size. If the size is 0, delete this depth data. If the size changes, replace the original data.\n• If there is no same price, sort by price (bid in descending order, ask in ascending order), and insert the depth information into the full load.\n\nUse the first 25 bids and asks in the full load to form a string (where a colon connects the price and size in an ask or a bid), and then calculate the CRC32 value (32-bit signed integer).\n• When the bid and ask depth data exceeds 25 levels, each of them will intercept 25 levels of data, and the string to be checked is queued in a way that the bid and ask depth data are alternately arranged. \n\nSuch as: : : : ...\n• When the bid or ask depth data is less than 25 levels, the missing depth data will be ignored.\n\nSuch as: : : : ...\n\nThe API endpoints of do not require authentication.\n\nRetrieve a list of all currencies available which are related to the current account's KYC entity.\n\nRetrieve the funding account balances of all the assets and the amount that is available or on hold.\n\nOnly API keys with privilege can call this endpoint.\n\nThis endpoint supports the transfer of funds between your funding account and trading account, and from the master account to sub-accounts.\n\nSub-account can transfer out to master account by default. Need to call Set permission of transfer out to grant privilege first if you want sub-account transferring to another sub-account (sub-accounts need to belong to same master account.)\n\nRetrieve the transfer state data of the last 2 weeks.\n\nQuery the billing record in the past month.\n\nRetrieve the deposit records according to the currency, deposit status, and time range in reverse chronological order. The 100 most recent records are returned by default.\n\n Websocket API is also available, refer to Deposit info channel.\n\nOnly supported withdrawal of assets from funding account. Common sub-account does not support withdrawal.\n\nYou can cancel normal withdrawal requests, but you cannot cancel withdrawal requests on Lightning.\n\nRetrieve the withdrawal records according to the currency, withdrawal status, and time range in reverse chronological order. The 100 most recent records are returned by default.\n\n Websocket API is also available, refer to Withdrawal info channel.\n\nAuthentication is not required for this public endpoint.\n\nYou should make estimate quote before convert trade.\n\nFor the same side (buy/sell), there's a trading limit of 1 request per 5 seconds.\n\nA push notification is triggered when a deposit is initiated or the deposit status changes.\n\n Supports subscriptions for accounts\n• If it is a master account subscription, you can receive the push of the deposit info of both the master account and the sub-account.\n• If it is a sub-account subscription, only the push of sub-account deposit info you can receive.\n\nA push notification is triggered when a withdrawal is initiated or the withdrawal status changes.\n\n Supports subscriptions for accounts\n• If it is a master account subscription, you can receive the push of the withdrawal info of both the master account and the sub-account.\n• If it is a sub-account subscription, only the push of sub-account withdrawal info you can receive.\n\nApplies to master accounts only and master accounts API Key must be linked to IP addresses. Only API keys with privilege can call this endpoint.\n\nQuery detailed balance info of Trading Account of a sub-account via the master account (applies to master accounts only)\n\nQuery detailed balance info of Funding Account of a sub-account via the master account (applies to master accounts only)\n\nRetrieve the maximum withdrawal information of a sub-account via the master account (applies to master accounts only). If no currency is specified, the transferable amount of all owned currencies will be returned.\n\nOnly API keys with privilege can call this endpoint.\n\nSet permission of transfer out for sub-account (only applicable to master account API key). Sub-account can transfer out to master account by default.\n\nOnly the assets in the funding account can be used for purchase. More details\n\nPlanned system maintenance that may result in short interruption (lasting less than 5 seconds) or websocket disconnection (users can immediately reconnect) will not be announced. The maintenance will only be performed during times of low market volatility.\n\nGet the status of system maintenance and push when rescheduling and the system maintenance status and end time changes. First subscription: \"Push the latest change data\"; every time there is a state change, push the changed content.\n\nPlanned system maintenance that may result in short interruption (lasting less than 5 seconds) or websocket disconnection (users can immediately reconnect) will not be announced. The maintenance will only be performed during times of low market volatility.\n\nHere is the REST API Error Code\n\nREST API Error Code is from 50000 to 59999."
    },
    {
        "link": "https://github.com/okxapi/python-okx",
        "document": "This is an unofficial Python wrapper for the OKX exchange v5 API\n\nIf you came here looking to purchase cryptocurrencies from the OKX exchange, please go here.\n\nMake sure you update often and check the Changelog for new features and bug fixes.\n• Fill in API credentials in the corresponding examples\n• RestAPI\n• Tweak the value of the parameter (live trading: 0, demo trading: 1 ) to switch between live and demo trading environment\n• WebSocketAPI\n• Use different URLs for different environment\n• To learn more about OKX API, visit official OKX API documentation\n• If you face any questions when using ,you can consult the following links"
    },
    {
        "link": "https://pypi.org/project/okx-sdk",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://spinningup.openai.com/en/latest/algorithms/ddpg.html",
        "document": "Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.\n\nThis approach is closely connected to Q-learning, and is motivated the same way: if you know the optimal action-value function , then in any given state, the optimal action can be found by solving\n\nDDPG interleaves learning an approximator to with learning an approximator to , and it does so in a way which is specifically adapted for environments with continuous action spaces. But what does it mean that DDPG is adapted specifically for environments with continuous action spaces? It relates to how we compute the max over actions in .\n\nWhen there are a finite number of discrete actions, the max poses no problem, because we can just compute the Q-values for each action separately and directly compare them. (This also immediately gives us the action which maximizes the Q-value.) But when the action space is continuous, we can’t exhaustively evaluate the space, and solving the optimization problem is highly non-trivial. Using a normal optimization algorithm would make calculating a painfully expensive subroutine. And since it would need to be run every time the agent wants to take an action in the environment, this is unacceptable.\n\nBecause the action space is continuous, the function is presumed to be differentiable with respect to the action argument. This allows us to set up an efficient, gradient-based learning rule for a policy which exploits that fact. Then, instead of running an expensive optimization subroutine each time we wish to compute , we can approximate it with . See the Key Equations section details.\n\nHere, we’ll explain the math behind the two parts of DDPG: learning a Q function, and learning a policy. First, let’s recap the Bellman equation describing the optimal action-value function, . It’s given by where is shorthand for saying that the next state, , is sampled by the environment from a distribution . This Bellman equation is the starting point for learning an approximator to . Suppose the approximator is a neural network , with parameters , and that we have collected a set of transitions (where indicates whether state is terminal). We can set up a mean-squared Bellman error (MSBE) function, which tells us roughly how closely comes to satisfying the Bellman equation: Here, in evaluating , we’ve used a Python convention of evaluating to 1 and to zero. Thus, when —which is to say, when is a terminal state—the Q-function should show that the agent gets no additional rewards after the current state. (This choice of notation corresponds to what we later implement in code.) Q-learning algorithms for function approximators, such as DQN (and all its variants) and DDPG, are largely based on minimizing this MSBE loss function. There are two main tricks employed by all of them which are worth describing, and then a specific detail for DDPG. Trick One: Replay Buffers. All standard algorithms for training a deep neural network to approximate make use of an experience replay buffer. This is the set of previous experiences. In order for the algorithm to have stable behavior, the replay buffer should be large enough to contain a wide range of experiences, but it may not always be good to keep everything. If you only use the very-most recent data, you will overfit to that and things will break; if you use too much experience, you may slow down your learning. This may take some tuning to get right. We’ve mentioned that DDPG is an off-policy algorithm: this is as good a point as any to highlight why and how. Observe that the replay buffer should contain old experiences, even though they might have been obtained using an outdated policy. Why are we able to use these at all? The reason is that the Bellman equation doesn’t care which transition tuples are used, or how the actions were selected, or what happens after a given transition, because the optimal Q-function should satisfy the Bellman equation for all possible transitions. So any transitions that we’ve ever experienced are fair game when trying to fit a Q-function approximator via MSBE minimization. Trick Two: Target Networks. Q-learning algorithms make use of target networks. The term is called the target, because when we minimize the MSBE loss, we are trying to make the Q-function be more like this target. Problematically, the target depends on the same parameters we are trying to train: . This makes MSBE minimization unstable. The solution is to use a set of parameters which comes close to , but with a time delay—that is to say, a second network, called the target network, which lags the first. The parameters of the target network are denoted . In DQN-based algorithms, the target network is just copied over from the main network every some-fixed-number of steps. In DDPG-style algorithms, the target network is updated once per main network update by polyak averaging: where is a hyperparameter between 0 and 1 (usually close to 1). (This hyperparameter is called in our code). DDPG Detail: Calculating the Max Over Actions in the Target. As mentioned earlier: computing the maximum over actions in the target is a challenge in continuous action spaces. DDPG deals with this by using a target policy network to compute an action which approximately maximizes . The target policy network is found the same way as the target Q-function: by polyak averaging the policy parameters over the course of training. Putting it all together, Q-learning in DDPG is performed by minimizing the following MSBE loss with stochastic gradient descent: where is the target policy. Policy learning in DDPG is fairly simple. We want to learn a deterministic policy which gives the action that maximizes . Because the action space is continuous, and we assume the Q-function is differentiable with respect to action, we can just perform gradient ascent (with respect to policy parameters only) to solve Note that the Q-function parameters are treated as constants here.\n\nDDPG trains a deterministic policy in an off-policy way. Because the policy is deterministic, if the agent were to explore on-policy, in the beginning it would probably not try a wide enough variety of actions to find useful learning signals. To make DDPG policies explore better, we add noise to their actions at training time. The authors of the original DDPG paper recommended time-correlated OU noise, but more recent results suggest that uncorrelated, mean-zero Gaussian noise works perfectly well. Since the latter is simpler, it is preferred. To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training. (We do not do this in our implementation, and keep noise scale fixed throughout.) At test time, to see how well the policy exploits what it has learned, we do not add noise to the actions. Our DDPG implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DDPG exploration."
    },
    {
        "link": "https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html",
        "document": "Deep Reinforcement Learning has recently gained a lot of traction in the machine learning community due to the significant amount of progress that has been made in the past few years. Traditionally, reinforcement learning algorithms were constrained to tiny, discretized grid worlds, which seriously inhibited them from gaining credibility as being viable machine learning tools. Here’s a classic example from Richard Sutton’s book, which I will be referencing a lot.\n\nAfter Deep Q-Networks [3] became a hit, people realized that deep learning methods could be used to solve high-dimensional problems. One of the subsequent challenges that the reinforcement learning community faced was figuring out how to deal with continuous action spaces. This is a significant obstacle, since most interesting problems in robotic control, etc., fall into this category. Logically, if you discretize your continuous action space too finely, you end up with the same curse of dimensionality problem as before. On the other hand, a naive discretization of the action space throws away valuable information concerning the geometry of the action domain.\n\nGoogle DeepMind has devised a solid algorithm for tackling the continuous action space problem. Building off the prior work of [2] on Deterministic Policy Gradients, they have produced a policy-gradient actor-critic algorithm called Deep Deterministic Policy Gradients (DDPG) [4] that is off-policy and model-free, and that uses some of the deep learning tricks that were introduced along with Deep Q-Networks (hence the “deep”-ness of DDPG). In this blog post, we’re going to discuss how to implement this algorithm using Tensorflow and tflearn, and then evaluate it with OpenAI Gym on the pendulum environment. I’ll also discuss some of the theory behind it. Regrettably, I can’t start with introducing the basics of reinforcement learning since that would make this blog post much too long; however, Richard Sutton’s book (linked above), as well as David Silver’s course, are excellent resources to get going with RL.\n\n[Wait, I just want to skip to the Tensorflow part!]\n\nPolicy-Gradient (PG) algorithms optimize a policy end-to-end by computing noisy estimates of the gradient of the expected reward of the policy and then updating the policy in the gradient direction. Traditionally, PG methods have assumed a stochastic policy $\\mu (a | s)$, which gives a probability distribution over actions. Ideally, the algorithm sees lots of training examples of high rewards from good actions and negative rewards from bad actions. Then, it can increase the probability of the good actions. In practice, you tend to run into plenty of problems with vanilla-PG; for example, getting one reward signal at the end of a long episode of interaction with the environment makes it difficult to ascertain exactly which action was the good one. This is known as the credit assignment problem. For RL problems with continuous action spaces, vanilla-PG is all but useless. You can, however, get vanilla-PG to work with some RL domains that take in visual inputs and have discrete action spaces with a convolutional neural network representing your policy (talk about standing on the shoulders of giants!). There are extensions to the vanilla-PG algorithm such as REINFORCE and Natural Policy Gradients that make the algorithm much more viable. For a first look into stochastic policy gradients, you can find an overview of the Stochastic Policy Gradient theorem in [2], an in-depth blog post by Andrej Karpathy on here.\n\nThe Actor-Critic learning algorithm is used to represent the policy function independently of the value function. The policy function structure is known as the actor, and the value function structure is referred to as the critic. The actor produces an action given the current state of the environment, and the critic produces a TD (Temporal-Difference) error signal given the state and resultant reward. If the critic is estimating the action-value function $Q(s,a)$, it will also need the output of the actor. The output of the critic drives learning in both the actor and the critic. In Deep Reinforcement Learning, neural networks can be used to represent the actor and critic structures.\n\nReinforcement Learning algorithms which are characterized as off-policy generally employ a separate behavior policy that is independent of the policy being improved upon; the behavior policy is used to simulate trajectories. A key benefit of this separation is that the behavior policy can operate by sampling all actions, whereas the estimation policy can be deterministic (e.g., greedy) [1]. Q-learning is an off-policy algorithm, since it updates the Q values without making any assumptions about the actual policy being followed. Rather, the Q-learning algorithm simply states that the Q-value corresponding to state $s(t)$ and action $a(t)$ is updated using the Q-value of the next state $s(t + 1)$ and the action $a(t + 1)$ that maximizes the Q-value at state $s(t + 1)$.\n\nOn-policy algorithms directly use the policy that is being estimated to sample trajectories during training.\n\nModel-free RL algorithms are those that make no effort to learn the underlying dynamics that govern how an agent interacts with the environment. In the case where the environment has a discrete state space and the agent has a discrete number of actions to choose from, a model of the dynamics of the environment is the 1-step transition matrix: $T(s(t + 1)|s(t), a(t))$. This stochastic matrix gives all of the probabilities for arriving at a desired state given the current state and action. Clearly, for problems with high-dimensional state and action spaces, this matrix is incredibly expensive in space and time to compute. If your state space is the set of all possible 64 x 64 RGB images and your agent has 18 actions available to it, the transition matrix’s size is $|S \\times S \\times A| \\approx |(68.7 \\times 10^{9}) \\times (68.7 \\times 10^9) \\times 18|$, and at 32 bits per matrix element, thats around $3.4 \\times 10^{14}$ GB to store it in RAM!\n\nRather than dealing with all of that, model-free algorithms directly estimate the optimal policy or value function through algorithms such as policy iteration or value iteration. This is much more computationally efficient. I should note that, if possible, obtaining and using a good approximation of the underlying model of the environment can only be beneficial. Be wary- using a bad approximation of a model of the environment will only bring you misery. Just as well, model-free methods generally require a larger number of training examples.\n\nThe Meat and Potatoes of DDPG\n\nAt its core, DDPG is a policy gradient algorithm that uses a stochastic behavior policy for good exploration but estimates a deterministic target policy, which is much easier to learn. Policy gradient algorithms utilize a form of policy iteration: they evaluate the policy, and then follow the policy gradient to maximize performance. Since DDPG is off-policy and uses a deterministic target policy, this allows for the use of the Deterministic Policy Gradient theorem (which will be derived shortly). DDPG is an actor-critic algorithm as well; it primarily uses two neural networks, one for the actor and one for the critic. These networks compute action predictions for the current state and generate a temporal-difference (TD) error signal each time step. The input of the actor network is the current state, and the output is a single real value representing an action chosen from a continuous action space (whoa!). The critic’s output is simply the estimated Q-value of the current state and of the action given by the actor. The deterministic policy gradient theorem provides the update rule for the weights of the actor network. The critic network is updated from the gradients obtained from the TD error signal.\n\nSadly, it turns out that tossing neural networks at DPG results in an algorithm that behaves poorly, resisting all of your most valiant efforts to get it to converge. The following are most likely some of the key conspirators:\n• In general, training and evaluating your policy and/or value function with thousands of temporally-correlated simulated trajectories leads to the introduction of enormous amounts of variance in your approximation of the true Q-function (the critic). The TD error signal is excellent at compounding the variance introduced by your bad predictions over time. It is highly suggested to use a replay buffer to store the experiences of the agent during training, and then randomly sample experiences to use for learning in order to break up the temporal correlations within different training episodes. This technique is known as experience replay. DDPG uses this.\n• Directly updating your actor and critic neural network weights with the gradients obtained from the TD error signal that was computed from both your replay buffer and the output of the actor and critic networks causes your learning algorithm to diverge (or to not learn at all). It was recently discovered that using a set of target networks to generate the targets for your TD error computation regularizes your learning algorithm and increases stability. Accordingly, here are the equations for the TD target $y_i$ and the loss function for the critic network:\n\nHere, a minibatch of size $N$ has been sampled from the replay buffer, with the $i$ index referring to the i’th sample. The target for the TD error computation, $y_i$, is computed from the sum of the immediate reward and the outputs of the target actor and critic networks, having weights $\\theta^{\\mu’}$ and $\\theta^{Q’}$ respectively. Then, the critic loss can be computed w.r.t. the output $Q(s_i, a_i | \\theta^{Q})$ of the critic network for the i’th sample.\n\nSee [3] for more details on the use of target networks.\n\nNow, as mentioned above, the weights of the critic network can be updated with the gradients obtained from the loss function in Eq. 2. Also, remember that the actor network is updated with the Deterministic Policy Gradient. Here lies the crux of DDPG! Silver, et al., [2] proved that the stochastic policy gradient $\n\nabla_{\\theta} \\mu (a | s, \\theta)$, which is the gradient of the policy’s performance, is equivalent to the deterministic policy gradient, which is given by:\n\nNotice that the policy term in the expectation is not a distribution over actions. It turns out that all you need is the gradient of the output of the critic network w.r.t. the actions, multiplied by the gradient of the output of the actor network w.r.t. its parameters, averaged over a minibatch. Simple!\n\nI think the proof of the deterministic policy gradient theorem is quite illuminating, so I’d like to demonstrate it here before moving on to the code.\n\nFor a greedy stochastic policy $ \\mu (a | s, \\theta)$ over a continuous action space, a global maximization step is required at every time step. Rather, we employ a deterministic policy $ \\mu (s | \\theta) $ and update the policy parameters by moving them in the direction of the gradient of the action-value function. We take an expectation to average over the suggested directions of improvement from each state w.r.t. the state distribution under the target policy $\\mu'$, given by $\\rho^{\\mu'}(s)$. $$ \\begin{equation} \\theta^{\\mu}_{k + 1} = \\theta^{\\mu}_k + \\alpha \\mathbb{E}_{\\mu'^{k}} \\big [ \n\nabla_{\\theta} Q(s, \\mu (s|\\theta^{\\mu}_k)|\\theta^{Q}_k) \\big ]. \\end{equation} $$ By applying the chain rule, $$ \\begin{equation} \\theta^{\\mu}_{k + 1} = \\theta^{\\mu}_k + \\alpha \\mathbb{E}_{\\mu'^{k}} \\big [ \n\nabla_{a} Q(s, a|\\theta^{Q}_k)|_{a=\\mu(s|\\theta^{\\mu}_k)} \n\nabla_{\\theta} \\mu(s|\\theta^{\\mu}_k) \\big ]. \\end{equation} $$\n\nThe expectation in the right-hand side of Eq. 6 is exactly what we want. In [3], it is shown that the stochastic policy gradient converges to Eq. 4 in the limit as the variance of the stochastic policy gradient approaches 0. This is significant because it allows for all of the machinery for stochastic policy gradients to be applied to deterministic policy gradients.\n\nEnough With The Chit Chat, Let’s See Some Code!\n\nWe’re writing code to solve the Pendulum environment in OpenAI gym, which has a low-dimensional state space and a single continuous action within [-2, 2]. The goal is to swing up and balance the pendulum.\n\nThe first part is easy. Set up a data structure to represent your replay buffer. I recommend using a deque from python’s collections library. The replay buffer will return a randomly chosen batch of experiences when queried.\n\nOkay, lets define our actor and critic networks. We’re going to use tflearn to condense the boilerplate code.\n\nI would suggest placing these two functions in separate Actor and Critic classes, as shown above. The hyperparameter and layer details for the networks are in the appendix of the DDPG paper [4]. The networks for the low-dimensional state-space problems are pretty simple, though. For the actor network, the output is a tanh layer scaled to be between $[-b, +b], b \\in \\mathbb{R}$. This is useful when your action space is on the real line but is bounded and closed, as is the case for the pendulum task.\n\nNotice that the critic network takes both the state and the action as inputs; however, the action input skips the first layer. This is a design decision that has experimentally worked well. Accommodating this with tflearn was a bit tricky.\n\nMake sure to use Tensorflow placeholders, created by , for the inputs. Leaving the first dimension of the placeholders as allows you to train on batches of experiences.\n\nYou can simply call these creation methods twice, once to create the actor and critic networks that will be used for training, and again to create your target actor and critic networks.\n\nYou can create a Tensorflow Op to update the target network parameters like so:\n\nThis looks a bit convoluted, but it’s actually a great display of Tensorflow’s flexibility. You’re defining a Tensorflow Op, , that will copy the parameters of the online network with a mixing factor $\\tau$. Make sure you’re copying over the correct Tensorflow variables by checking what is being returned by . You’ll need to define this Op for both the actor and critic.\n\nLet’s define the gradient computation and optimization Tensorflow operations. We’ll use ADAM as our optimization method. It’s sort of replaced SGD as the de-facto standard, now that it’s implemented in so many plug-and-play deep-learning libraries such as tflearn and keras and tends to outperform it.\n\nNotice how the gradients are combined. makes it quite easy to implement the Deterministic Policy Gradient equation (Eq. 4). I negate the action-value gradient since we want the actor to follow the action-value gradients. Tensorflow will take the sum and average of the gradients of your minibatch.\n\nThen, for the critic network…\n\nThis is exactly Eq. 2. Make sure to grab the action-value gradients at the end there to pass to the policy network for gradient computation.\n\nI like to encapsulate calls to my Tensorflow session to keep things organized and readable in my training code. For brevity’s sake, I’ll just show the ones for the actor network.\n\nNow, lets show the main training loop and we’ll be done!\n\nYou’ll want to add book-keeping to the code and arrange things a little more neatly- you can find the full code here.\n\nI used Tensorboard to view the total reward per episode and average max Q. Tensorflow is a bit low-level, so it’s definitely recommended to use wrappers like tflearn, keras, Tensorboard, etc., on top of it. I am quite pleased with TF though; it’s no surprise that it got so popular so quickly.\n\nDespite the fact that I ran this code on the courageous CPU of my Macbook Air, it converged relatively quickly to a good solution. Some ways to potentially get better performance (besides running the code on a GPU lol):\n• Use a priority algorithm for sampling from the replay buffer instead of uniformly sampling. See my summary of Prioritized Experience Replay.\n• Experiment with different stochastic policies to improve exploration.\n• Use recurrent networks to capture temporal nuances within the environment.\n\nThe authors of DDPG also used convolutional neural networks to tackle control tasks of higher complexities. They were able to learn good policies with just pixel inputs, which is really cool."
    },
    {
        "link": "https://medium.com/data-science-in-your-pocket/deep-deterministic-policy-gradient-ddpg-explained-with-codes-in-reinforcement-learning-5825fbdc77b2",
        "document": "So far so good, we have covered a bunch of exciting things in reinforcement learning till now ranging from basics to MAB, to Temporal Difference learning and plenty of Deep Reinforcement Learning algorithms namely REINFORCE, A2C, DQN, etc.\n\nYou can check them out below in the Reinforcement Learning section\n\nSo the algorithms we discussed in my last few posts are majorly around continuous state space but discrete actions. That is actions like Left, Right, up, down, accelerate, deaccelerate, etc\n\nThe difference you can observe very easily is how in the 2nd type of action, we need to estimate some continuous quantity as well alongside the action. Steer the wheel? Ok, but how much? 35° or 40°.\n\nSo, this time we will deep dive into DDPG that can be used for training agents for environments with continuous action space.\n\nDDPGs also belong to the family of Actor-Critic methods (as A2C we discussed ) only where we have Actor (Policy network) and Critic (Value Network) learning together and finally, the actor is used for determining actions.\n\nA few things that we would be changing compared to A2C in DDPG are\n• Use of Target networks for both Actor & Critic for stabilized training.\n• Use of Experience Replay (that we used in DQNs).\n• An updated loss function for both Actor & Critic networks.\n\nAs already covered in my previous blogs, below is a video if you missed\n\nWhat are Target networks getting used in DDPGs?\n\nSo they are nothing but copy networks of actual Actor & Critic networks we are training which is updated with the actual network counterparts periodically (say after every 10 epochs). Maintaining a copy of the network for Actors & critics helps in stabilized agent training.\n\nAs we discussed the changes compared to A2C, let’s have a brief on the environment I tried this time\n\nThe environment we are trying to train today is Pendulum-v1 from OpenAI Gym which intakes continuous values between -2,2 as action i.e. torque applied. The state space is a tuple of 3 values\n\nThe reward function is a continuous value designed to encourage the agent to swing the pendulum up and balance it at the upright position while minimizing the energy (torque) used. It is calculated using the below formula\n• Both Actor and Target Actor have the same architecture\n• They are shallow Feed-Forward Networks with the current state as input\n• The last activation used is ‘tanh’ as the action is in the range -2,2\n• Critic and Target Critic have the same architecture\n• Critic intakes both state and action as input outputting the expected q-value\n• The concat variable in between the architecture helps us to club up the state and action inputs. Could this be done as the first step? Yes. Though I didn’t try\n• The last activation is linear as q-values are continuous values\n\nThis function is created to update the target networks from time to time. Though this update is not a straight copy-paste but a mix of the target’s old weights and new update networks using the trade-off variable ‘tau’.\n\n6. Noise function which we will use to alter the action predicted by the Actor to avoid overfitting. I myself copy pasted this code to add noise and doesn’t feel requires a deep dive for now.\n\n7. Some constants to be used\n\nThis requires some explanation. Let’s get started\n\nHow did we get to q-value for the next state?\n\nSo this is the calculation where we would be using the Target Networks. Nowhere else target networks will be used!\n\nThe next step is to calculate gradients to update our Actor and Critic Networks (not the Target Networks but the actual ones) using tf.GradientTape\n\n9. To visualize the results, use the below code snippet which I have already given an explanation for in my previous blogs and vlogs\n\nA major problem I faced while training the pendulum environment, and hence got poor results is the fact that DDPGs are considered the hardest to train amongst other known algorithms as very susceptible to hyperparameters hence hyperparameter tuning is a must for DDPGs. Also, as the exploration is in a continuous space, this is actually a tough problem to crack down on.\n\nDo try this code snippet, play around with the hyperparameters and train other OpenAI Gym environments like CarRacing which is comparatively more complex."
    },
    {
        "link": "https://towardsdatascience.com/deep-deterministic-and-twin-delayed-deep-deterministic-policy-gradient-with-tensorflow-2-x-43517b0e0185",
        "document": "In this article, we will be implementing Deep Deterministic Policy Gradient and Twin Delayed Deep Deterministic Policy Gradient methods with TensorFlow 2.x. We won’t be going deeper into theory and will cover only essential things. Before you proceed further, it is recommended to be familiar with DQN and Double DQN. For this article, I have converted the Td3 PyTorch code by Dr. Phil into TensorFlow, and for DDPG, I took his target network update method. Kindly refer to his youtube channel here(best youtube channel for coding RL).\n\nDDPG is used for environments having continuous action space. DDPG combines Ideas from both DQN and Actor-Critic methods. Let us try to understand with code.\n• Our critic network takes state and action as inputs and these inputs are concatenated together.\n• Critic network outputs a value for action in a particular state.\n• We are using a continuous environment that’s why we are using tanh activation (output values b/w -1 and 1) and output is the length of action i.e in \"LunarLanderContinuous-v2\" an action is represented as an array [-1 1], so here length is 2.\n• In DDPG, we have target networks for both actor and critic like in DQN we have a target network.\n• Note that we have compiled our target networks as we don’t want to get an error while copying weights from main networks to target networks.\n• We have also used a replay buffer to store experiences.\n• For action selection, first, we convert our state into a tensor and then pass it to the actor-network.\n• For training, we added noise in action and for testing, we will not add any noise.\n• We will clip our action b/w range of max and min action value.\n• Updating target networks in ddpg and td3 uses soft updates i.e we will update weights slightly every time.\n• Networks can be updated as below as explain in the research paper.\n• First, we sample experience from our replay buffer and convert them into tensors.\n• The target value for critic loss is calculated by predicting action for the next states using the actor’s target network and then using these actions we get the next state’s values using the critic’s target network.\n• Then, we apply the Bellman equation to calculate target values (target_values = rewards + self.gamma target_next_state_values done) . Note here done is stored as (1-done) while training.\n• Our predicted values are the output of the main critic network which takes states and actions from the buffer sample.\n• Critic loss is then calculated as MSE of target values and predicted values.\n• Actor loss is calculated as negative of critic main values with inputs as the main actor predicted actions.\n• Then, we update our target networks with a tau of 0.005.\n• Our training loop is simple i.e it interacts and stores experiences and learns at each action step.\n\nTD3 is inspired by double DQN and solves the issue of the overestimation of critic values and has the following changes over DDPG.\n• Using Two main critic networks apart from their target networks.\n\nWe will be covering only differences in code from DDPG.\n• In agent class, we have 2 main critic networks and their respective target networks.\n• In action selection, we stop adding noise to actions after some steps.\n• There are only 3 differences in the td3 train function from that of DDPG.\n• First, actions from the actor’s target network are regularized by adding noise and then clipping the action in a range of max and min action.\n• Second, the next state values and current state values are both target critic and both main critic networks. And a minimum of two networks is taken into consideration for both next state values and current state values.\n• Third, actor-network is trained after every 2 steps. So, this was all about differences in implementation. Now let us look at training results.\n\nThese two graph shows episodes taken by both algorithms to reach an average score of 200 in \"LunarLanderContinuous-v2\" over the last 100 episodes.\n\nAs you can see the TD3 algorithm got an average reward of 200 for the last 100 episodes in just 400 episodes. While DDPG got only 100 after 560 episodes.\n\nWhat to take care of while implementation:\n\nWhile coding RL, the following things would be kept in mind.\n• The number of Neurons, hidden layers, learning rates have a huge impact on learning.\n• The shape of the tensors and NumPy array should be correct. Many times implementation is correct and codes work but the agent does not learn anything only because the shape of tensors is not correct and when operations are applied over those tensors gives wrong results.\n\nYou can find the full code for this article [here](https://github.com/abhisheksuran/Reinforcement_Learning/blob/master/td3withtau.ipynb) and here. Stay tuned for upcoming articles where we will be implementing some more RL Algorithms and Deep Learning Algorithms in TensorFlow 2.\n\nSo, this concludes this article. Thank you for reading, hope you enjoyed and were able to understand what I wanted to explain. Hope you read my upcoming articles. Hari Om…🙏"
    },
    {
        "link": "https://keras.io/examples/rl/ddpg_pendulum",
        "document": "Author: amifunny\n\n Date created: 2020/06/04\n\n Last modified: 2024/03/23\n\n Description: Implementing DDPG algorithm on the Inverted Pendulum Problem.\n\nⓘ This example uses Keras 3\n\nIt combines ideas from DPG (Deterministic Policy Gradient) and DQN (Deep Q-Network). It uses Experience Replay and slow-learning target networks from DQN, and it is based on DPG, which can operate over continuous action spaces.\n\nThis tutorial closely follow this paper - Continuous control with deep reinforcement learning\n\nWe are trying to solve the classic Inverted Pendulum control problem. In this setting, we can take only two actions: swing left or swing right.\n\nWhat make this problem challenging for Q-Learning Algorithms is that actions are continuous instead of being discrete. That is, instead of using two discrete actions like or , we have to select from infinite actions ranging from to .\n\nJust like the Actor-Critic method, we have two networks:\n• Actor - It proposes an action given a state.\n• Critic - It predicts if the action is good (positive value) or bad (negative value) given a state and an action.\n\nDDPG uses two more techniques not present in the original DQN:\n\nFirst, it uses two Target networks.\n\nWhy? Because it add stability to training. In short, we are learning from estimated targets and Target networks are updated slowly, hence keeping our estimated targets stable.\n\nConceptually, this is like saying, \"I have an idea of how to play this well, I'm going to try it out for a bit until I find something better\", as opposed to saying \"I'm going to re-learn how to play this entire game after every move\". See this StackOverflow answer.\n\nSecond, it uses Experience Replay.\n\nWe store list of tuples , and instead of learning only from recent experience, we learn from sampling all of our experience accumulated so far.\n\nNow, let's see how is it implemented.\n\nWe use Gymnasium to create the environment. We will use the parameter to scale our actions later.\n\nTo implement better exploration by the Actor network, we use noisy perturbations, specifically an Ornstein-Uhlenbeck process for generating noise, as described in the paper. It samples noise from a correlated normal distribution.\n\nCritic loss - Mean Squared Error of where is the expected return as seen by the Target network, and is action value predicted by the Critic network. is a moving target that the critic model tries to achieve; we make this target stable by updating the Target model slowly.\n\nActor loss - This is computed using the mean of the value given by the Critic network for the actions taken by the Actor network. We seek to maximize this quantity.\n\nHence we update the Actor network so that it produces actions that get the maximum predicted value as seen by the Critic, for a given state.\n\n# Number of \"experiences\" to store at max # Num of tuples to train on. # Its tells us num of times record() was called. # Instead of list of tuples as the exp.replay concept go # We use different np.arrays for each tuple element # Set index to zero if buffer_capacity is exceeded, # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows # TensorFlow to build a static graph out of the logic and computations in our function. # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one. # Used `-value` as we want to maximize the value given # by the critic for our actions # We compute the loss and update parameters # Based on rate `tau`, which is much less than one.\n\nHere we define the Actor and Critic networks. These are basic Dense models with activation.\n\nNote: We need the initialization for last layer of the Actor to be between and as this prevents us from getting or output values in the initial stages, which would squash our gradients to zero, as we use the activation.\n\nreturns an action sampled from our Actor network plus some noise for exploration.\n\nNow we implement our main training loop, and iterate over episodes. We sample actions using and train with at each time step, along with updating the Target networks at a rate .\n\nIf training proceeds correctly, the average episodic reward will increase with time.\n\nFeel free to try different learning rates, values, and architectures for the Actor and Critic networks.\n\nThe Inverted Pendulum problem has low complexity, but DDPG work great on many other problems.\n\nAnother great environment to try this on is continuous, but it will take more episodes to obtain good results."
    },
    {
        "link": "https://gymlibrary.dev/content/environment_creation",
        "document": "This documentation overviews creating new environments and relevant useful wrappers, utilities and tests included in Gym designed for the creation of new environments. You can clone gym-examples to play with the code that are presented here. We recommend that you use a virtual environment:\n\nBefore learning how to create your own environment you should check out the documentation of Gym’s API. We will be concerned with a subset of gym-examples that looks like this: To illustrate the process of subclassing , we will implement a very simplistic game, called . We will write the code for our custom environment in . The environment consists of a 2-dimensional square grid of fixed size (specified via the parameter during construction). The agent can move vertically or horizontally between grid cells in each timestep. The goal of the agent is to navigate to a target on the grid that has been placed randomly at the beginning of the episode.\n• None Observations provide the location of the target and agent.\n• None There are 4 actions in our environment, corresponding to the movements “right”, “up”, “left”, and “down”.\n• None A done signal is issued as soon as the agent has navigated to the grid cell where the target is located.\n• None Rewards are binary and sparse, meaning that the immediate reward is always zero, unless the agent has reached the target, then it is 1. An episode in this environment (with ) might look like this: where the blue dot is the agent and the red square represents the target. Let us look at the source code of piece by piece: Our custom environment will inherit from the abstract class . You shouldn’t forget to add the attribute to your class. There, you should specify the render-modes that are supported by your environment (e.g. , , ) and the framerate at which your environment should be rendered. Every environment should support as render-mode; you don’t need to add it in the metadata. In , we will support the modes “rgb_array” and “human” and render at 4 FPS. The method of our environment will accept the integer , that determines the size of the square grid. We will set up some variables for rendering and define and . In our case, observations should provide information about the location of the agent and target on the 2-dimensional grid. We will choose to represent observations in the form of a dictionaries with keys and . An observation may look like . Since we have 4 actions in our environment (“right”, “up”, “left”, “down”), we will use as an action space. Here is the declaration of and the implementation of : # The size of the square grid # The size of the PyGame window # Observations are dictionaries with the agent's and the target's location. # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]). # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\" The following dictionary maps abstract actions from `self.action_space` to the direction we will walk in if that action is taken. I.e. 0 corresponds to \"right\", 1 to \"up\" etc. If human-rendering is used, `self.window` will be a reference to the window that we draw to. `self.clock` will be a clock that is used to ensure that the environment is rendered at the correct framerate in human-mode. They will remain `None` until human-mode is used for the Since we will need to compute observations both in and , it is often convenient to have a (private) method that translates the environment’s state into an observation. However, this is not mandatory and you may as well compute observations in and separately: We can also implement a similar method for the auxiliary information that is returned by and . In our case, we would like to provide the manhattan distance between the agent and the target: Oftentimes, info will also contain some data that is only available inside the method (e.g. individual reward terms). In that case, we would have to update the dictionary that is returned by in . The method will be called to initiate a new episode. You may assume that the method will not be called before has been called. Moreover, should be called whenever a done signal has been issued. Users may pass the keyword to to initialize any random number generator that is used by the environment to a deterministic state. It is recommended to use the random number generator that is provided by the environment’s base class, . If you only use this RNG, you do not need to worry much about seeding, but you need to remember to call to make sure that correctly seeds the RNG. Once this is done, we can randomly set the state of our environment. In our case, we randomly choose the agent’s location and the randomly sample target positions, until it does not coincide with the agent’s position. The method should return a tuple of the initial observation and some auxiliary information. We can use the methods and that we implemented earlier for that: # We need the following line to seed self.np_random # We will sample the target's location randomly until it does not coincide with the agent's location The method usually contains most of the logic of your environment. It accepts an , computes the state of the environment after applying that action and returns the 4-tuple . Once the new state of the environment has been computed, we can check whether it is a terminal state and we set accordingly. Since we are using sparse binary rewards in , computing is trivial once we know . To gather and , we can again make use of and : # Map the action (element of {0,1,2,3}) to the direction we walk in # We use `np.clip` to make sure we don't leave the grid # An episode is done iff the agent has reached the target Here, we are using PyGame for rendering. A similar approach to rendering is used in many environments that are included with Gym and you can use it as a skeleton for your own environments: # The size of a single grid square in pixels # First we draw the target # The following line copies our drawings from `canvas` to the visible window # We need to ensure that human-rendering occurs at the predefined framerate. # The following line will automatically add a delay to keep the framerate stable. The method should close any open resources that were used by the environment. In many cases, you don’t actually have to bother to implement this method. However, in our example may be and we might need to close the window that has been opened: In other environments might also close files that were opened or release other resources. You shouldn’t interact with the environment after having called .\n\nIn order for the custom environments to be detected by Gym, they must be registered as follows. We will choose to put this code in . The environment ID consists of three components, two of which are optional: an optional namespace (here: ), a mandatory name (here: ) and an optional but recommended version (here: v0). It might have also been registered as (the recommended approach), or , and the appropriate ID should then be used during environment creation. The keyword argument will ensure that GridWorld environments that are instantiated via will be wrapped in a wrapper (see the wrapper documentation for more information). A done signal will then be produced if the agent has reached the target or 300 steps have been executed in the current episode. To distinguish truncation and termination, you can check . Apart from and , you may pass the following additional keyword arguments to : The reward threshold before the task is considered solved Whether this environment is non-deterministic even after seeding The maximum number of steps that an episode can consist of. If not , a wrapper is added Whether to wrap the environment in an wrapper Whether to wrap the environment in an The default kwargs to pass to the environment class Most of these keywords (except for , and ) do not alter the behavior of environment instances but merely provide some extra information about your environment. After registration, our custom environment can be created with . If your environment is not registered, you may optionally pass a module to import, that would register your environment before creating it like this - , where contains the registration code. For the GridWorld env, the registration code is run by importing so if it were not possible to import gym_examples explicitly, you could register while making by . This is especially useful when you’re allowed to pass only the environment ID into a third-party codebase (eg. learning library). This lets you register your environment without needing to edit the library’s source code.\n\nOftentimes, we want to use different variants of a custom environment, or we want to modify the behavior of an environment that is provided by Gym or some other party. Wrappers allow us to do this without changing the environment implementation or adding any boilerplate code. Check out the wrapper documentation for details on how to use wrappers and instructions for implementing your own. In our example, observations cannot be used directly in learning code because they are dictionaries. However, we don’t actually need to touch our environment implementation to fix this! We can simply add a wrapper on top of environment instances to flatten observations into a single array: Wrappers have the big advantage that they make environments highly modular. For instance, instead of flattening the observations from GridWorld, you might only want to look at the relative position of the target and the agent. In the section on ObservationWrappers we have implemented a wrapper that does this job. This wrapper is also available in gym-examples:"
    },
    {
        "link": "https://medium.com/@paulswenson2/an-introduction-to-building-custom-reinforcement-learning-environment-using-openai-gym-d8a5e7cf07ea",
        "document": "Getting into reinforcement learning (RL), and making custom environments for your problems can be a daunting task. I know it was for me when I was getting started (and I am by no means an expert in RL). Most posts on this subject are either overly complicated with terminology or have examples so complex that it can be hard to understand. I wanted to give back to the community with what I have learned in the most simple terms possible, so hopefully this will be helpful to you!\n\nThis article will take you through the process of building a very simple custom environment from scratch using OpenAI Gym. If you want to skip all the background for RL and Gym and get right to the code, go to “The Game” section.\n\nWhat You Need to Know\n\nNeed to Know:\n\nWhat is OpenAI Gym and Why Use It?\n\nOpenAI Gym is an open source Python module which allows developers, researchers and data scientists to build reinforcement learning (RL) environments using a pre-defined framework. The primary motivation for using Gym instead of just base Python or some other programming language is designed to interact with other RL Python modules. One such module is stablebaselines3, which allows you to quickly train RL models on these environments without having to write all the algorithms yourself.\n\nI won’t get too in the weeds here. This is meant to be an introduction not a super technical article!\n\nThere are two main parts of reinforcement learning (RL). The environment, and the agent. The environment is the space that the agent can interact with. In many cases, environments are just games such as Atari games like Asteroids, but they can be configured to represent a whole lot of other different problems too. The agent is what interacts with the environment. If you were interacting with Asteroids for example, you would be the agent playing the ship.\n\nRL environments have a few defining characteristics; the state of the environment, the actions an agent can take, and the reward returned to the agent after any given state transition. The state represents all of the information within the environment at a given time. For asteroids this would be like taking a snapshot of the game. In that snapshot (the state) you have the position of the ship, the number of points the agent has earned, the number of lives, where all of the asteroids are, how fast the asteroids are moving, etc, etc.\n\nWhen actions are taken by an agent, the environment state will transition to the next step and return a reward to the agent. In Asteroids, this would be something like shooting a laser. Once the “shoot laser” action is taken the next environment state will update all of the asteroid positions, the ship position, and most importantly shoot a laser from the ship. Not all actions or steps in the environment have to return rewards, but in Asteroids, a reward might be returned once an asteroid is hit. Notably, rewards can also be bad! If the ship is destroyed, a negative reward could be returned.\n\nThe rewards returned by an environment are all defined within the environment. These rewards will define what an RL agent tries to do, so it is important to think hard about what you want to reward an agent for. Having a positive reward for a ship exploding on an asteroid might cause the agent to blow itself up as fast as possible!\n\nWe’re going to implement a very simple game so that the focus remains on how to develop a reinforcement learning (RL) environment in Gym.\n\nThe game is as follows:\n• The agent is the blue square\n• The agent can move up, down, left, or right\n• The game is fully contained within a 6x6 grid\n• All of the colored square positions are randomized at the start of the game and cannot overlap\n• The agent cannot move outside of the grid, i.e. if it tries to move left two times from the above position, it will move left once, then not move on the second action.\n• If the agent gets to the green square, it wins the game\n• If the agent gets to the red square, it loses the game\n• The game will continue until the green or red squares are landed on\n\nSince Gym requires all of the environment states to be represented numerically, we will represent the environment as seen above.\n• The agent is the number 1\n• The green square is the number 2\n• The red square is the number 3\n• The empty squares are the number 0\n\nGym also requires the environment state to be represented as a single row of values, not a grid of values. We can achieve this by taking the top row, adding the 2nd row to the end of it, adding the 3rd row to the end of those combined rows, etc. etc. In linear algebra, this is called flattening a matrix (the grid). The new state visually, now looks something like this:\n\nThe actions the agent can take must represented as number too. I have defined them as follows:\n\nWe will define a few rewards to be returned to the agent after it takes certain actions.\n• If the agent wins the game, it will be rewarded with 1 point\n• If the agent loses the game, it will be rewarded with -1 point\n• Every action that does not result in a win or a loss will give a reward of -0.01 points. This is to incentivize the agent to take the fewest actions to win the game.\n\nYou will need to download and install Python 3.5+\n\nAfter Python is installed, I usually install stablebaselines3 which will also install Gym.\n\nAll of the following code is available publicly on my github\n\nGym environments have 4 functions that need to be defined within the environment class:\n\nA good starting point for any custom environment would be to copy another existing environment like this one, or one from the OpenAI repo\n\nGlobal constants used in this environment for readability\n\nBelow is a basic shell of a Gym environment. BasicEnv is the name of our custom environment. In the BasicEnv class definition, it is passed Env as an argument. This is the abstract Gym class that our custom environment will implement. Inside of the class we have all of the functions mentioned above.\n\nThe init function is the function which sets up all of the variables for the class and defines the action and state spaces. Every environment requires a few variables to be setup in order for agents to interact with it:\n• self.state\n\nThis is the state of the game which we described above (remember that flattened grid we talked about?)\n• self.observation_space\n\nThis is Gym’s way of describing what values are valid for a given position within the state (aka an observation). In our case, the observations within the state must be an agent, an empty square, a green square, or a red square. As numbers like we mentioned earlier, these are 0, 1, 2, or 3.\n• self.action_space\n\nThese are the valid actions that an agent can take. In our case: Up, Down, Left, or Right. Or as numbers: 0, 1, 2, or 3.\n\nThe step function is the most involved function. This will define how an action will update the environment state as well as what rewards (if any) were earned by the agent during the action. For this game, this code will define what happens when the agent moves, whether the game is over, and how much reward was earned.\n\nThe step function takes an action as a parameter when it is called. The action must be within the action_space defined in the init function, or an exception will be thrown.\n\nThis function is required to return 4 values:\n• self.state\n\nThe updated state after the action is taken.\n• reward\n\nThe reward earned by the agent after taking an action.\n• done\n\nTrue/False depending on whether the game has ended after the action was taken.\n• info\n\nA Python dictionary which can be used to return information for debugging purposes.\n\nThis function will reset the environment to an initial state. Kinda like a reset button on a video game. The reset function is required to return self.state.\n\nMost of the code in this section will look just like the init function, but we do not have to redefine the action space or the observation space.\n\nThe render function defines how the game will be visualized. On Linux computers, you can create video-game like visualizations of the environment, but this is not required. For this tutorial, we will use a text-based visualization.\n\nI created two helper functions to assist in the visualization.\n\nWith these functions setup, the render function looks like this.\n\nYour custom environment can be run by initializing it, and taking actions against it. Here I made a separate python script which takes user inputs to interact with the environment.\n\nYou can test if the environment is compatible with the stablebaselines3 module with this very handy function check_env:\n\nAny errors in implementation will be caught by this function with some good details on what went wrong.\n\nI hope this was helpful to you. This code probably isn’t optimized or perfect, but if you see any major errors, feel free to shoot me a message. Be sure to look at plenty of other examples of Gym environments, it will probably take more than just this tutorial to get a feel for how they work.\n\nGood luck on your journey with reinforcement learning!\n\nThings that were helpful to me when learning:"
    },
    {
        "link": "https://gymlibrary.dev/index.html",
        "document": "Gym is a standard API for reinforcement learning, and a diverse collection of reference environments#\n\nThe Gym interface is simple, pythonic, and capable of representing general RL problems:"
    },
    {
        "link": "https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation",
        "document": "This documentation overviews creating new environments and relevant useful wrappers, utilities and tests included in Gymnasium designed for the creation of new environments.\n\nYou can check that has been correctly installed by running the following command, which should output a version number: Then you can just run the following command and replace the string by the path to the directory where you want to create your new project. Answer the questions, and when it’s finished you should get a project structure like the following:\n\nBefore learning how to create your own environment you should check out the documentation of Gymnasium’s API. To illustrate the process of subclassing , we will implement a very simplistic game, called . We will write the code for our custom environment in . The environment consists of a 2-dimensional square grid of fixed size (specified via the parameter during construction). The agent can move vertically or horizontally between grid cells in each timestep. The goal of the agent is to navigate to a target on the grid that has been placed randomly at the beginning of the episode.\n• None Observations provide the location of the target and agent.\n• None There are 4 actions in our environment, corresponding to the movements “right”, “up”, “left”, and “down”.\n• None A done signal is issued as soon as the agent has navigated to the grid cell where the target is located.\n• None Rewards are binary and sparse, meaning that the immediate reward is always zero, unless the agent has reached the target, then it is 1. An episode in this environment (with ) might look like this: where the blue dot is the agent and the red square represents the target. Let us look at the source code of piece by piece: Our custom environment will inherit from the abstract class . You shouldn’t forget to add the attribute to your class. There, you should specify the render-modes that are supported by your environment (e.g., , , ) and the framerate at which your environment should be rendered. Every environment should support as render-mode; you don’t need to add it in the metadata. In , we will support the modes “rgb_array” and “human” and render at 4 FPS. The method of our environment will accept the integer , that determines the size of the square grid. We will set up some variables for rendering and define and . In our case, observations should provide information about the location of the agent and target on the 2-dimensional grid. We will choose to represent observations in the form of dictionaries with keys and . An observation may look like . Since we have 4 actions in our environment (“right”, “up”, “left”, “down”), we will use as an action space. Here is the declaration of and the implementation of : # The size of the square grid # The size of the PyGame window # Observations are dictionaries with the agent's and the target's location. # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]). # We have 4 actions, corresponding to \"right\", \"up\", \"left\", \"down\" The following dictionary maps abstract actions from `self.action_space` to the direction we will walk in if that action is taken. i.e. 0 corresponds to \"right\", 1 to \"up\" etc. If human-rendering is used, `self.window` will be a reference to the window that we draw to. `self.clock` will be a clock that is used to ensure that the environment is rendered at the correct framerate in human-mode. They will remain `None` until human-mode is used for the Since we will need to compute observations both in and , it is often convenient to have a (private) method that translates the environment’s state into an observation. However, this is not mandatory and you may as well compute observations in and separately: We can also implement a similar method for the auxiliary information that is returned by and . In our case, we would like to provide the manhattan distance between the agent and the target: Oftentimes, info will also contain some data that is only available inside the method (e.g., individual reward terms). In that case, we would have to update the dictionary that is returned by in . The method will be called to initiate a new episode. You may assume that the method will not be called before has been called. Moreover, should be called whenever a done signal has been issued. Users may pass the keyword to to initialize any random number generator that is used by the environment to a deterministic state. It is recommended to use the random number generator that is provided by the environment’s base class, . If you only use this RNG, you do not need to worry much about seeding, but you need to remember to call ``super().reset(seed=seed)`` to make sure that correctly seeds the RNG. Once this is done, we can randomly set the state of our environment. In our case, we randomly choose the agent’s location and the random sample target positions, until it does not coincide with the agent’s position. The method should return a tuple of the initial observation and some auxiliary information. We can use the methods and that we implemented earlier for that: # We need the following line to seed self.np_random # We will sample the target's location randomly until it does not coincide with the agent's location The method usually contains most of the logic of your environment. It accepts an , computes the state of the environment after applying that action and returns the 5-tuple . See . Once the new state of the environment has been computed, we can check whether it is a terminal state and we set accordingly. Since we are using sparse binary rewards in , computing is trivial once we know .To gather and , we can again make use of and : # Map the action (element of {0,1,2,3}) to the direction we walk in # We use `np.clip` to make sure we don't leave the grid # An episode is done iff the agent has reached the target Here, we are using PyGame for rendering. A similar approach to rendering is used in many environments that are included with Gymnasium and you can use it as a skeleton for your own environments: # The size of a single grid square in pixels # First we draw the target # The following line copies our drawings from `canvas` to the visible window # We need to ensure that human-rendering occurs at the predefined framerate. # The following line will automatically add a delay to keep the framerate stable. The method should close any open resources that were used by the environment. In many cases, you don’t actually have to bother to implement this method. However, in our example may be and we might need to close the window that has been opened: In other environments might also close files that were opened or release other resources. You shouldn’t interact with the environment after having called .\n\nIn order for the custom environments to be detected by Gymnasium, they must be registered as follows. We will choose to put this code in . The environment ID consists of three components, two of which are optional: an optional namespace (here: ), a mandatory name (here: ) and an optional but recommended version (here: v0). It might have also been registered as (the recommended approach), or , and the appropriate ID should then be used during environment creation. The keyword argument will ensure that GridWorld environments that are instantiated via will be wrapped in a wrapper (see the wrapper documentation for more information). A done signal will then be produced if the agent has reached the target or 300 steps have been executed in the current episode. To distinguish truncation and termination, you can check . Apart from and , you may pass the following additional keyword arguments to : The reward threshold before the task is considered solved Whether this environment is non-deterministic even after seeding The maximum number of steps that an episode can consist of. If not , a wrapper is added Whether to wrap the environment in an wrapper The default kwargs to pass to the environment class Most of these keywords (except for , and ) do not alter the behavior of environment instances but merely provide some extra information about your environment. After registration, our custom environment can be created with . If your environment is not registered, you may optionally pass a module to import, that would register your environment before creating it like this - , where contains the registration code. For the GridWorld env, the registration code is run by importing so if it were not possible to import gymnasium_env explicitly, you could register while making by . This is especially useful when you’re allowed to pass only the environment ID into a third-party codebase (eg. learning library). This lets you register your environment without needing to edit the library’s source code."
    },
    {
        "link": "https://digitalocean.com/community/tutorials/creating-custom-environments-openai-gym",
        "document": "OpenAI Gym comes packed with a lot of awesome environments, ranging from environments featuring classic control tasks to ones that let you train your agents to play Atari games like Breakout, Pacman, and Seaquest. However, you may still have a task at hand that necessitates the creation of a custom environment that is not a part of the Gym package. Thankfully, Gym is flexible enough to allow you to do so and that’s precisely the topic of this post.\n\nIn this post, we will be designing a custom environment that will involve flying a Chopper (or a helicopter) while avoiding obstacles mid-air. Note that this is the second part of the Open AI Gym series, and knowledge of the concepts introduced in Part 1 is assumed as a prerequisite for this post. So if you haven’t read Part 1, here is the link.\n• Python: a machine with Python installed and beginners experience with Python coding is recommended for this tutorial\n• Open AI Gym: this package must be installed on the machine/droplet being used\n\nWe first begin with installing some important dependencies.\n\nWe also start with the necessary imports.\n\nThe environment that we are creating is basically a game that is heavily inspired by the Dino Run game, the one which you play in Google Chrome if you are disconnected from the Internet. There is a dinosaur, and you have to jump over cacti and avoid hitting birds. The distance you cover is representative of the reward you end up getting.\n\nIn our game, instead of a dinosaur, our agent is going to be a Chopper pilot.\n• The chopper has to cover as much distance as possible to get the maximum reward. There will be birds that the chopper has to avoid.\n• The episode terminates in case of a bird strike. The episode can also terminate if the Chopper runs out of fuel.\n• Just like birds, there are floating fuel tanks (yes, no points for being close to reality, I know!) which the Chopper can collect to refuel the chopper to its full capacity (which is fixed at 1000 L).\n\nNote that this is going to be just a proof of concept and not the most aesthetically-pleasing game. However, in case you want to improve on it, this post will leave you with enough knowledge to do so!\n\nThe very first consideration while designing an environment is to decide what sort of observation space and action space we will be using.\n• The observation space can be either continuous or discrete. An example of a discrete action space is that of a grid-world where the observation space is defined by cells, and the agent could be inside one of those cells. An example of a continuous action space is one where the position of the agent is described by real-valued coordinates.\n• The action space can be either continuous or discrete as well. An example of a discrete space is one where each action corresponds to the particular behavior of the agent, but that behavior cannot be quantified. An example of this is Mario Bros, where each action would lead to moving left, right, jumping, etc. Your actions can’t quantify the behavior being produced, i.e. you can jump but not jump high, higher, or lower. However, in a game like Angry Birds, you decide how much to stretch the slingshot (you quantify it).\n\nWe begin my implementing the function of our environment class, . In the function, we will define the observation and the action spaces. In addition to that, we will also implement a few other attributes:\n• : This defines the legitimate area of our screen where various elements of the screen, such as the Chopper and birds, can be placed. Other areas are reserved for displaying info such as fuel left, rewards, and padding.\n• : This stores the active elements stored in the screen at any given time (like chopper, bird, etc.)\n• : Maximum fuel that the chopper can hold.\n\nOnce we have determined the action space and the observation space, we need to finalize what would be the elements of our environment. In our game, we have three distinct elements: the Chopper, Flying Birds, and and Floating Fuel Stations. We will be implementing all of these as separate classes that inherit from a common base class called .\n\nThe class is used to define any arbitrary point on our observation image. We define this class with the following attributes:\n• : Position of the point on the image.\n• : Permissible coordinates for the point. If we try to set the position of the point outside these limits, the position values are clamped to these limits.\n• : Name of the point.\n\nWe define the following member functions for this class.\n• : Get the coordinates of the point.\n• : Set the coordinates of the point to a certain value.\n• : Move the points by certain value.\n\nNow we define the classes , and . These classes are derived from the class, and introduce a set of new attributes:\n• : Icon of the point that will display on the observation image when the game is rendered.\n\nRecall from Part 1 that any gym class has two important functions:\n• : Resets the environment to its initial state and returns the initial observation.\n• : Executes a step in the environment by applying an action. Returns the new observation, reward, completion status, and other info.\n\nIn this section, we will be implementing the and functions of our environment along with many other helper functions. We begin with function.\n\nWhen we reset our environment, we need to reset all the state-based variables in our environment. These include things like fuel consumed, episodic return, and the elements present inside the environment.\n\nIn our case, when we reset our environment, we have nothing but the Chopper in the initial state. We initialize our chopper randomly in an area in the top-left of our image. This area is 5-10 percent of the image width and 15-20 percent of the image height.\n\nWe also define a helper function called that basically places the icons of each of the elements present in the game at their respective positions in the observation image. If the position is beyond the permissible range, then the icons are placed on the range boundaries. We also print important information such as the remaining fuel.\n\nWe finally return the canvas on which the elements have been placed as the observation.\n\nBefore we proceed further, let us now see what our initial observation looks like.\n\nSince our observation is the same as the gameplay screen of the game, our render function shall return our observation too. We build functionality for two modes, one which would render the game in a pop-up window, while returns it as a pixel array.\n\nNow that we have the function out of the way, we begin work on implementing the function, which will contain the code to transition our environment from one state to the next given an action. In many ways, this section is the proverbial meat of our environment, and this is where most of the planning goes.\n\nWe first need to enlist things that need to happen in one transition step of the environment. This can be basically broken down into two parts:\n• Everything else that happens in the environments, such as behaviour of the non-RL actors (e.g. birds and floating gas stations).\n\nSo let’s first focus on (1). We provide actions to the game that will control what our chopper does. We basically have 5 actions, which are move right, left, down, up, or do nothing, denoted by 0, 1, 2, 3, and 4, respectively.\n\nWe define a member function called that will tell us what integer each action is mapped to for our reference.\n\nWe also validate whether the action being passed is a valid action or not by checking whether it’s present in the action space. If not, we raise an assertion.\n\nOnce that is done, we accordingly change the position of the chopper using the function we defined earlier. Each action results in movement by 5 coordinates in the respective directions.\n\nNow that we have taken care of applying the action to the chopper, we focus on the other elements of the environment:\n• Birds spawn randomly from the right edge of the screen with a probability of 1% (i.e. a bird is likely to appear on the right edge once every hundred frames). The bird moves 5 coordinate points every frame to the left. If they hit the Chopper the game ends. Otherwise, they disappear from the game once they reach the left edge.\n• Fuel tanks spawn randomly from the bottom edge of the screen with a probability of 1 % (i.e. a fuel tank is likely to appear on the bottom edge once every hundred frames). The bird moves 5 co-ordinates up every frame. If they hit the Chopper, the Chopper is fuelled to its full capacity. Otherwise, they disappear from the game once they reach the top edge.\n\nIn order to implement the features outlined above, we need to implement a helper function that helps us determine whether two objects (such as a Chopper/Bird, Chopper/Fuel Tank) have collided or not. How do we define a collision? We say that two points have collided when the distance between the coordinates of their centers is less than half of the sum of their dimensions. We call this function .\n\nApart from this, we have to do some book-keeping. The reward for each step is 1, therefore, the episodic return counter is updated by 1 every episode. If there is a collision, the reward is -10 and the episode terminates. The fuel counter is reduced by 1 at every step.\n\nFinally, we implement our function. I’ve wrote extensive comments to guide you through it.\n\nSeeing It in Action\n\nThis concludes the code for our environment. Now execute some steps in the environment using an agent that takes random actions!\n\nThat’s it for this part, folks. I hope this tutorial gave you some insight into some of the considerations and design decisions that go into designing a custom OpenAI environment. You can now try creating an environment of your choice, or if you’re so inclined, you can make several improvements to the one we just designed for practice. Some suggestions right off the bat are:\n• Instead of the episode terminating upon the first bird strike, you can implement multiple lives for the chopper.\n• Design an evil alien race of mutated birds that are also able to fire missiles at the chopper, and the chopper has to avoid them.\n• Do something about when a fuel tank and a bird collides!"
    }
]