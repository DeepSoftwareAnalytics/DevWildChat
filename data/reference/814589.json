[
    {
        "link": "https://docs.opencv.org/3.4/d8/d38/tutorial_bgsegm_bg_subtraction.html",
        "document": "\n• We will familiarize with the background subtraction methods available in OpenCV.\n\nBackground subtraction is a major preprocessing step in many vision-based applications. For example, consider the case of a visitor counter where a static camera takes the number of visitors entering or leaving the room, or a traffic camera extracting information about the vehicles etc. In all these cases, first you need to extract the person or vehicles alone. Technically, you need to extract the moving foreground from static background.\n\nIf you have an image of background alone, like an image of the room without visitors, image of the road without vehicles etc, it is an easy job. Just subtract the new image from the background. You get the foreground objects alone. But in most of the cases, you may not have such an image, so we need to extract the background from whatever images we have. It becomes more complicated when there are shadows of the vehicles. Since shadows also move, simple subtraction will mark that also as foreground. It complicates things.\n\nSeveral algorithms were introduced for this purpose. In the following, we will have a look at two algorithms from the module.\n\nIt is a Gaussian Mixture-based Background/Foreground Segmentation Algorithm. It was introduced in the paper \"An improved adaptive background mixture model for real-time tracking with shadow detection\" by P. KadewTraKuPong and R. Bowden in 2001. It uses a method to model each background pixel by a mixture of K Gaussian distributions (K = 3 to 5). The weights of the mixture represent the time proportions that those colours stay in the scene. The probable background colours are the ones which stay longer and more static.\n\nWhile coding, we need to create a background object using the function, cv.bgsegm.createBackgroundSubtractorMOG(). It has some optional parameters like length of history, number of gaussian mixtures, threshold etc. It is all set to some default values. Then inside the video loop, use backgroundsubtractor.apply() method to get the foreground mask.\n\nSee a simple example below:\n\n( All the results are shown at the end for comparison).\n\nThis algorithm combines statistical background image estimation and per-pixel Bayesian segmentation. It was introduced by Andrew B. Godbehere, Akihiro Matsukawa, and Ken Goldberg in their paper \"Visual Tracking of Human Visitors under Variable-Lighting Conditions for a Responsive Audio Art Installation\" in 2012. As per the paper, the system ran a successful interactive audio art installation called “Are We There Yet?” from March 31 - July 31 2011 at the Contemporary Jewish Museum in San Francisco, California.\n\nIt uses first few (120 by default) frames for background modelling. It employs probabilistic foreground segmentation algorithm that identifies possible foreground objects using Bayesian inference. The estimates are adaptive; newer observations are more heavily weighted than old observations to accommodate variable illumination. Several morphological filtering operations like closing and opening are done to remove unwanted noise. You will get a black window during first few frames.\n\nIt would be better to apply morphological opening to the result to remove the noises.\n\nBelow image shows the 200th frame of a video"
    },
    {
        "link": "https://docs.opencv.org/4.x/d1/dc5/tutorial_background_subtraction.html",
        "document": "\n• Background subtraction (BS) is a common and widely used technique for generating a foreground mask (namely, a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\n• As the name suggests, BS calculates the foreground mask performing a subtraction between the current frame and a background model, containing the static part of the scene or, more in general, everything that can be considered as background given the characteristics of the observed scene.\n• In the first step, an initial model of the background is computed, while in the second step that model is updated in order to adapt to possible changes in the scene.\n• In this tutorial we will learn how to perform BS by using OpenCV.\n\nIn this tutorial you will learn how to:\n• Read data from videos or image sequences by using cv::VideoCapture ;\n• Create and update the background model by using cv::BackgroundSubtractor class;\n• Get and show the foreground mask by using cv::imshow ;\n\nIn the following you can find the source code. We will let the user choose to process either a video file or a sequence of images.\n\nWe will use cv::BackgroundSubtractorMOG2 in this sample, to generate the foreground mask.\n\nThe results as well as the input data are shown on the screen.\n\nWe discuss the main parts of the code above:\n• A cv::BackgroundSubtractor object will be used to generate the foreground mask. In this example, default parameters are used, but it is also possible to declare specific parameters in the create function.\n• A cv::VideoCapture object is used to read the input video or input images sequence.\n• Every frame is used both for calculating the foreground mask and for updating the background. If you want to change the learning rate used for updating the background model, it is possible to set a specific learning rate by passing a parameter to the method.\n• The current frame number can be extracted from the cv::VideoCapture object and stamped in the top left corner of the current frame. A white rectangle is used to highlight the black colored frame number.\n• We are ready to show the current input frame and the results.\n• With the video, for the following frame:\n\nThe output of the program will look as the following for MOG2 method (gray areas are detected shadows):\n\nThe output of the program will look as the following for the KNN method (gray areas are detected shadows):"
    },
    {
        "link": "http://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_video/py_bg_subtraction/py_bg_subtraction.html",
        "document": "Background subtraction is a major preprocessing steps in many vision based applications. For example, consider the cases like visitor counter where a static camera takes the number of visitors entering or leaving the room, or a traffic camera extracting information about the vehicles etc. In all these cases, first you need to extract the person or vehicles alone. Technically, you need to extract the moving foreground from static background.\n\nIf you have an image of background alone, like image of the room without visitors, image of the road without vehicles etc, it is an easy job. Just subtract the new image from the background. You get the foreground objects alone. But in most of the cases, you may not have such an image, so we need to extract the background from whatever images we have. It become more complicated when there is shadow of the vehicles. Since shadow is also moving, simple subtraction will mark that also as foreground. It complicates things.\n\nSeveral algorithms were introduced for this purpose. OpenCV has implemented three such algorithms which is very easy to use. We will see them one-by-one.\n\nIt is a Gaussian Mixture-based Background/Foreground Segmentation Algorithm. It was introduced in the paper “An improved adaptive background mixture model for real-time tracking with shadow detection” by P. KadewTraKuPong and R. Bowden in 2001. It uses a method to model each background pixel by a mixture of K Gaussian distributions (K = 3 to 5). The weights of the mixture represent the time proportions that those colours stay in the scene. The probable background colours are the ones which stay longer and more static. While coding, we need to create a background object using the function, cv2.createBackgroundSubtractorMOG(). It has some optional parameters like length of history, number of gaussian mixtures, threshold etc. It is all set to some default values. Then inside the video loop, use method to get the foreground mask. See a simple example below: ( All the results are shown at the end for comparison).\n\nIt is also a Gaussian Mixture-based Background/Foreground Segmentation Algorithm. It is based on two papers by Z.Zivkovic, “Improved adaptive Gausian mixture model for background subtraction” in 2004 and “Efficient Adaptive Density Estimation per Image Pixel for the Task of Background Subtraction” in 2006. One important feature of this algorithm is that it selects the appropriate number of gaussian distribution for each pixel. (Remember, in last case, we took a K gaussian distributions throughout the algorithm). It provides better adaptibility to varying scenes due illumination changes etc. As in previous case, we have to create a background subtractor object. Here, you have an option of selecting whether shadow to be detected or not. If (which is so by default), it detects and marks shadows, but decreases the speed. Shadows will be marked in gray color.\n\nThis algorithm combines statistical background image estimation and per-pixel Bayesian segmentation. It was introduced by Andrew B. Godbehere, Akihiro Matsukawa, Ken Goldberg in their paper “Visual Tracking of Human Visitors under Variable-Lighting Conditions for a Responsive Audio Art Installation” in 2012. As per the paper, the system ran a successful interactive audio art installation called “Are We There Yet?” from March 31 - July 31 2011 at the Contemporary Jewish Museum in San Francisco, California. It uses first few (120 by default) frames for background modelling. It employs probabilistic foreground segmentation algorithm that identifies possible foreground objects using Bayesian inference. The estimates are adaptive; newer observations are more heavily weighted than old observations to accommodate variable illumination. Several morphological filtering operations like closing and opening are done to remove unwanted noise. You will get a black window during first few frames. It would be better to apply morphological opening to the result to remove the noises."
    },
    {
        "link": "https://elbruno.com/2022/06/09/opencv-background-subtraction-in-a-camera-feed-%F0%9F%8E%A6-using-mog2",
        "document": "Yesterday I wrote a post about how to remove the background from my camera feed. To remove the background, I used a python library “cvzone”.\n\nWhile talking with some friends, someone asked about using some of the out-of-the-box features included in OpenCV, like MOG2 or KNN. So I read a little about them and I get this sample up and running.\n\nAs you can see the output is completely different than before, and not even close to what I’m looking for. And it makes sense, the official OpenCV documentation explains how this method works here: [How to Use Background Subtraction Methods].\n\nBackground subtraction (BS) is a common and widely used technique for generating a foreground mask (namely, a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\n\nAs the name suggests, BS calculates the foreground mask performing a subtraction between the current frame and a background model, containing the static part of the scene or, more in general, everything that can be considered as background given the characteristics of the observed scene.\n\nIn the first step, an initial model of the background is computed, while in the second step that model is updated in order to adapt to possible changes in the scene."
    },
    {
        "link": "https://stackoverflow.com/questions/62775713/background-substractor-python-opencv-remove-granulation",
        "document": "Hello in using MOG2 to make a Background substrator from a base frame to a next frames. but its showing me to much ruid\n\nid like if there is another background substractor that can elimitate this ponts. Also i have another problem. When a car passes with flash lights on the flashlights is showed as white im mi image . i need to ignorate the reflexion of fleshlight in the ground.\n\nSome one knows dow to do that ?"
    },
    {
        "link": "https://gradio.app/docs/gradio/interface",
        "document": "Interface is Gradio's main high-level class, and allows you to create a web-based GUI / demo around a machine learning model (or any Python function) in a few lines of code. You must specify three parameters: (1) the function to create a GUI for (2) the desired input components and (3) the desired output components. Additional parameters can be used to control the appearance and behavior of the demo. \n\n\n\nthe function to wrap an interface around. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component. a single Gradio component, or list of Gradio components. Components can either be passed as instantiated objects, or referred to by their string shortcuts. The number of input components should match the number of parameters in fn. If set to None, then only the output components will be displayed. a single Gradio component, or list of Gradio components. Components can either be passed as instantiated objects, or referred to by their string shortcuts. The number of output components should match the number of values returned by fn. If set to None, then only the input components will be displayed. sample inputs for the function; if provided, appear below the UI components and can be clicked to populate the interface. Should be nested list, in which the outer list consists of samples and each inner list consists of an input corresponding to each input component. A string path to a directory of examples can also be provided, but it should be within the directory with the python file running the gradio app. If there are multiple input components and a directory is provided, a log.csv file must be present in the directory to link corresponding inputs. If True, caches examples in the server for fast runtime in examples. If \"lazy\", then examples are cached (for all users of the app) after their first use (by any user of the app). If None, will use the GRADIO_CACHE_EXAMPLES environment variable, which should be either \"true\" or \"false\". In HuggingFace Spaces, this parameter is True (as long as `fn` and `outputs` are also provided). The default option otherwise is False. if \"lazy\", examples are cached after their first use. If \"eager\", all examples are cached at app launch. If None, will use the GRADIO_CACHE_MODE environment variable if defined, or default to \"eager\". if examples are provided, how many to display per page. a list of labels for each example. If provided, the length of this list should be the same as the number of examples, and these labels will be used in the UI instead of rendering the example values. whether the interface should automatically rerun if any of the inputs change. a title for the interface; if provided, appears above the input and output components in large font. Also used as the tab title when opened in a browser window. a description for the interface; if provided, appears above the input and output components and beneath the title in regular font. Accepts Markdown and HTML content. an expanded article explaining the interface; if provided, appears below the input and output components in regular font. Accepts Markdown and HTML content. If it is an HTTP(S) link to a downloadable remote file, the content of this file is displayed. a Theme object or a string representing a theme. If a string, will look for a built-in theme with that name (e.g. \"soft\" or \"default\"), or will attempt to load a theme from the Hugging Face Hub (e.g. \"gradio/monochrome\"). If None, will use the Default theme. one of \"never\", \"auto\", or \"manual\". If \"never\" or \"auto\", users will not see a button to flag an input and output. If \"manual\", users will see a button to flag. If \"auto\", every input the user submits will be automatically flagged, along with the generated output. If \"manual\", both the input and outputs are flagged when the user clicks flag button. This parameter can be set with environmental variable GRADIO_FLAGGING_MODE; otherwise defaults to \"manual\". if provided, allows user to select from the list of options when flagging. Only applies if flagging_mode is \"manual\". Can either be a list of tuples of the form (label, value), where label is the string that will be displayed on the button and value is the string that will be stored in the flagging CSV; or it can be a list of strings [\"X\", \"Y\"], in which case the values will be the list of strings and the labels will [\"Flag as X\", \"Flag as Y\"], etc. path to the the directory where flagged data is stored. If the directory does not exist, it will be created. either None or an instance of a subclass of FlaggingCallback which will be called when a sample is flagged. If set to None, an instance of gradio.flagging.CSVLogger will be created and logs will be saved to a local CSV file in flagging_dir. Default to None. whether to allow basic telemetry. If None, will use GRADIO_ANALYTICS_ENABLED environment variable if defined, or default to True. if True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component. the maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True) defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None, the name of the prediction function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event. if True, then will show a 'Duplicate Spaces' button on Hugging Face Spaces. if set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `.queue()`, which itself is 1 by default). Custom css as a code string. This css will be included in the demo webpage. Custom css as a pathlib.Path to a css file or a list of such paths. This css files will be read, concatenated, and included in the demo webpage. If the `css` parameter is also set, the css from `css` will be included first. Custom js as a code string. The custom js should be in the form of a single js function. This function will automatically be executed when the page loads. For more flexibility, use the head parameter to insert js inside <script> tags. Custom html code to insert into the head of the demo webpage. This can be used to add custom meta tags, multiple scripts, stylesheets, etc. to the page. Custom html code as a pathlib.Path to a html file or a list of such paths. This html files will be read, concatenated, and included in the head of the demo webpage. If the `head` parameter is also set, the html from `head` will be included first. a single Gradio component, or list of Gradio components. Components can either be passed as instantiated objects, or referred to by their string shortcuts. These components will be rendered in an accordion below the main input components. By default, no additional input components will be displayed. if a string is provided, this is the label of the `gr.Accordion` to use to contain additional inputs. A `gr.Accordion` object can be provided as well to configure other properties of the container holding the additional inputs. Defaults to a `gr.Accordion(label=\"Additional Inputs\", open=False)`. This parameter is only used if `additional_inputs` is provided. the button to use for submitting inputs. Defaults to a `gr.Button(\"Submit\", variant=\"primary\")`. This parameter does not apply if the Interface is output-only, in which case the submit button always displays \"Generate\". Can be set to a string (which becomes the button label) or a `gr.Button` object (which allows for more customization). the button to use for stopping the interface. Defaults to a `gr.Button(\"Stop\", variant=\"stop\", visible=False)`. Can be set to a string (which becomes the button label) or a `gr.Button` object (which allows for more customization). the button to use for clearing the inputs. Defaults to a `gr.Button(\"Clear\", variant=\"secondary\")`. Can be set to a string (which becomes the button label) or a `gr.Button` object (which allows for more customization). Can be set to None, which hides the button. a tuple corresponding [frequency, age] both expressed in number of seconds. Every `frequency` seconds, the temporary files created by this Blocks instance will be deleted if more than `age` seconds have passed since the file was created. For example, setting this to (86400, 86400) will delete temporary files every day. The cache will be deleted entirely when the server restarts. If None, no cache deletion will occur. how to show the progress animation while event is running: \"full\" shows a spinner which covers the output component area as well as a runtime display in the upper right corner, \"minimal\" only shows the runtime display, \"hidden\" shows no progress animation at all whether to horizontally expand to fill container fully. If False, centers and constrains app to a maximum width. The time limit for the stream to run. Default is 30 seconds. Parameter only used for streaming images or audio if the interface is live and the input components are set to \"streaming=True\". The latency (in seconds) at which stream chunks are sent to the backend. Defaults to 0.5 seconds. Parameter only used for streaming images or audio if the interface is live and the input components are set to \"streaming=True\". a string or `gr.DeepLinkButton` object that creates a unique URL you can use to share your app and all components **as they currently are** with others. Automatically enabled on Hugging Face Spaces unless explicitly set to False.\n\nLaunches a simple web server that serves the demo. Can also be used to create a public link used by anyone to access the demo from their browser by setting share=True. whether to display in the gradio app inline in an iframe. Defaults to True in python notebooks; False otherwise. whether to automatically launch the gradio app in a new tab on the default browser. whether to create a publicly shareable link for the gradio app. Creates an SSH tunnel to make your UI accessible from anywhere. If not provided, it is set to False by default every time, except when running in Google Colab. When localhost is not accessible (e.g. Google Colab), setting share=False is not supported. Can be set by environment variable GRADIO_SHARE=True. if True, blocks the main thread from running. If running in Google Colab, this is needed to print the errors in the cell output. the maximum number of total threads that the Gradio app can generate in parallel. The default is inherited from the starlette library (currently 40). If provided, username and password (or list of username-password tuples) required to access app. Can also provide function that takes username and password and returns True if valid login. By default, the gradio app blocks the main thread while the server is running. If set to True, the gradio app will not block and the gradio server will terminate as soon as the script finishes. If True, any errors in the gradio app will be displayed in an alert modal and printed in the browser console log to make app accessible on local network, set this to \"0.0.0.0\". Can be set by environment variable GRADIO_SERVER_NAME. If None, will use \"127.0.0.1\". will start gradio app on this port (if available). Can be set by environment variable GRADIO_SERVER_PORT. If None, will search for an available port starting at 7860. The height in pixels of the iframe element containing the gradio app (used if inline=True) The width in pixels of the iframe element containing the gradio app (used if inline=True) If a path to a file (.png, .gif, or .ico) is provided, it will be used as the favicon for the web page. If a path to a file is provided, will use this as the private key file to create a local server running on https. If a path to a file is provided, will use this as the signed certificate for https. Needs to be provided if ssl_keyfile is provided. If a password is provided, will use this with the ssl certificate for https. If False, skips certificate validation which allows self-signed certificates to be used. If True, shows the api docs in the footer of the app. Default True. List of complete filepaths or parent directories that gradio is allowed to serve. Must be absolute paths. Warning: if you provide directories, any files in these directories or their subdirectories are accessible to all users of your app. Can be set by comma separated environment variable GRADIO_ALLOWED_PATHS. These files are generally assumed to be secure and will be displayed in the browser when possible. List of complete filepaths or parent directories that gradio is not allowed to serve (i.e. users of your app are not allowed to access). Must be absolute paths. Warning: takes precedence over `allowed_paths` and all other directories exposed by Gradio by default. Can be set by comma separated environment variable GRADIO_BLOCKED_PATHS. The root path (or \"mount point\") of the application, if it's not served from the root (\"/\") of the domain. Often used when the application is behind a reverse proxy that forwards requests to the application. For example, if the application is served at \"https://example.com/myapp\", the `root_path` should be set to \"/myapp\". A full URL beginning with http:// or https:// can be provided, which will be used as the root path in its entirety. Can be set by environment variable GRADIO_ROOT_PATH. Defaults to \"\". Additional keyword arguments to pass to the underlying FastAPI app as a dictionary of parameter keys and argument values. For example, `{\"docs_url\": \"/docs\"}` The maximum number of sessions whose information to store in memory. If the number of sessions exceeds this number, the oldest sessions will be removed. Reduce capacity to reduce memory usage when using gradio.State or returning updated components from functions. Defaults to 10000. Use this to specify a custom FRP server and port for sharing Gradio apps (only applies if share=True). If not provided, will use the default FRP server at https://gradio.live. See https://github.com/huggingface/frp for more information. Use this to specify the protocol to use for the share links. Defaults to \"https\", unless a custom share_server_address is provided, in which case it defaults to \"http\". If you are using a custom share_server_address and want to use https, you must set this to \"https\". The path to a TLS certificate file to use when connecting to a custom share server. This parameter is not used with the default FRP server at https://gradio.live. Otherwise, you must provide a valid TLS certificate file (e.g. a \"cert.pem\") relative to the current working directory, or the connection will not use TLS encryption, which is insecure. A function that takes a FastAPI request and returns a string user ID or None. If the function returns None for a specific request, that user is not authorized to access the app (they will see a 401 Unauthorized response). To be used with external authentication systems like OAuth. Cannot be used with `auth`. The maximum file size in bytes that can be uploaded. Can be a string of the form \"<value><unit>\", where value is any positive integer and unit is one of \"b\", \"kb\", \"mb\", \"gb\", \"tb\". If None, no limit is set. Enables traffic monitoring of the app through the /monitoring endpoint. By default is None, which enables this endpoint. If explicitly True, will also print the monitoring URL to the console. If False, will disable monitoring altogether. If True, prevents external domains from making requests to a Gradio server running on localhost. If False, allows requests to localhost that originate from localhost but also, crucially, from \"null\". This parameter should normally be True to prevent CSRF attacks but may need to be False when embedding a *locally-running Gradio app* using web components. If True, the Gradio app will be rendered using server-side rendering mode, which is typically more performant and provides better SEO, but this requires Node 20+ to be installed on the system. If False, the app will be rendered using client-side rendering mode. If None, will use GRADIO_SSR_MODE environment variable or default to False. If True, the Gradio app will be set up as an installable PWA (Progressive Web App). If set to None (default behavior), then the PWA feature will be enabled if this Gradio app is launched on Spaces, but not otherwise. This listener is triggered when the Interface initially loads in the browser. the function to call when this event is triggered. Often a machine learning model's prediction function. Each parameter of the function corresponds to one input component, and the function should return a single value or a tuple of values, with each element in the tuple corresponding to one output component. List of gradio.components to use as inputs. If the function takes no inputs, this should be an empty list. List of gradio.components to use as outputs. If the function returns no outputs, this should be an empty list. defines how the endpoint appears in the API docs. Can be a string, None, or False. If set to a string, the endpoint will be exposed in the API docs with the given name. If None (default), the name of the function will be used as the API endpoint. If False, the endpoint will not be exposed in the API docs and downstream apps (including those that `gr.load` this app) will not be able to use this event. If True, will scroll to output component on completion how to show the progress animation while event is running: \"full\" shows a spinner which covers the output component area as well as a runtime display in the upper right corner, \"minimal\" only shows the runtime display, \"hidden\" shows no progress animation at all Component or list of components to show the progress animation on. If None, will show the progress animation on all of the output components. If True, will place the request on the queue, if the queue has been enabled. If False, will not put this event on the queue, even if the queue has been enabled. If None, will use the queue setting of the gradio app. If True, then the function should process a batch of inputs, meaning that it should accept a list of input values for each parameter. The lists should be of equal length (and be up to length `max_batch_size`). The function is then *required* to return a tuple of lists (even if there is only 1 output component), with each list in the tuple corresponding to one output component. Maximum number of inputs to batch together if this is called from the queue (only relevant if batch=True) If False, will not run preprocessing of component data before running 'fn' (e.g. leaving it as a base64 string if this method is called with the `Image` component). If False, will not run postprocessing of component data before returning 'fn' output to the browser. A list of other events to cancel when this listener is triggered. For example, setting cancels=[click_event] will cancel the click_event, where click_event is the return value of another components .click method. Functions that have not yet run (or generators that are iterating) will be cancelled, but functions that are currently running will be allowed to finish. If \"once\" (default for all events except `.change()`) would not allow any submissions while an event is pending. If set to \"multiple\", unlimited submissions are allowed while pending, and \"always_last\" (default for `.change()` and `.key_up()` events) would allow a second submission after the pending event is complete. Optional frontend js method to run before running 'fn'. Input arguments for js method are values of 'inputs' and 'outputs', return should be a list of values for output components. If set, this is the maximum number of this event that can be running simultaneously. Can be set to None to mean no concurrency_limit (any number of this event can be running simultaneously). Set to \"default\" to use the default concurrency limit (defined by the `default_concurrency_limit` parameter in `Blocks.queue()`, which itself is 1 by default). If set, this is the id of the concurrency group. Events with the same concurrency_id will be limited by the lowest set concurrency_limit. whether to show this event in the \"view API\" page of the Gradio app, or in the \".view_api()\" method of the Gradio clients. Unlike setting api_name to False, setting show_api to False will still allow downstream apps as well as the Clients to use this event. If fn is None, show_api will automatically be set to False. Class method that constructs an Interface from a Hugging Face transformers.Pipeline or diffusers.DiffusionPipeline object. The input and output components are automatically determined from the pipeline. the pipeline object to use. A catch-all method for integrating with other libraries. This method should be run after launch() If a comet_ml Experiment object is provided, will integrate with the experiment and appear on Comet dashboard If the wandb module is provided, will integrate with it and appear on WandB dashboard If the mlflow module is provided, will integrate with the experiment and appear on ML Flow dashboard By enabling the queue you can control when users know their position in the queue, and set a limit on maximum number of events allowed. If True, the REST routes of the backend will be open, allowing requests made directly to those endpoints to skip the queue. The maximum number of events the queue will store at any given moment. If the queue is full, new events will not be added and a user will receive a message saying that the queue is full. If None, the queue size will be unlimited. The default value of `concurrency_limit` to use for event listeners that don't specify a value. Can be set by environment variable GRADIO_DEFAULT_CONCURRENCY_LIMIT. Defaults to 1 if not set otherwise."
    },
    {
        "link": "https://gradio.app/docs",
        "document": "Introducing FastRTC, a new way to build real-time AI apps"
    },
    {
        "link": "https://gradio.app/guides/quickstart",
        "document": "Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. No JavaScript, CSS, or web hosting experience needed!\n\nIt just takes a few lines of Python to create your own demo, so let's get started 💫\n\nWe recommend installing Gradio using , which is included by default in Python. Run this in your terminal or command prompt:\n\nTip: It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems are provided here.\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\nTip: We shorten the imported name from to . This is a widely adopted convention for better readability of code.\n\nNow, run your code. If you've written the Python code in a file named , then you would run from the terminal.\n\nThe demo below will open in a browser on http://localhost:7860 if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\nTip: When developing locally, you can run your Gradio app in hot reload mode, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in before the name of the file instead of . In the example above, you would type: `gradio app.py` in your terminal. Learn more in the Hot Reloading Guide.\n\nYou'll notice that in order to make your first demo, you created an instance of the class. The class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.\n\nThe class has three core arguments:\n• : the function to wrap a user interface (UI) around\n• : the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n• : the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe argument is very flexible -- you can pass any Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe and arguments take one or more Gradio components. As we'll see, Gradio includes more than 30 built-in components (such as the , , and components) that are designed for machine learning applications.\n\nTip: For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`\"textbox\"`) or an instance of the class (`gr.Textbox()`).\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to , with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to . This flexibility makes the class a very powerful way to create demos.\n\nWe'll dive deeper into the on our series on building Interfaces.\n\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set in , and a publicly accessible URL will be created for your demo. Let's revisit our example demo, but change the last line as follows:\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on sharing your Gradio application.\n\nSo far, we've been discussing the class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio include?\n\nGradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction — still all in Python.\n\nYou can build very custom and complex applications using . For example, the popular image generation Automatic1111 Web UI is built using Gradio Blocks. We dive deeper into the on our series on building with Blocks.\n\nGradio includes another high-level class, , which is specifically designed to create Chatbot UIs. Similar to , you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to our dedicated guide on .\n\nThat's the gist of the core Python library, but Gradio is actually so much more! It's an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n• Gradio-Lite ( ): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide.\n• Hugging Face Spaces: the most popular place to host Gradio applications — for free!\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: let's dive deeper into the Interface class.\n\nOr, if you already know the basics and are looking for something specific, you can search the more technical API documentation.\n\nYou can also build Gradio applications without writing any code. Simply type into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or use this hosted version of Gradio Sketch, running on Hugging Face Spaces."
    },
    {
        "link": "https://github.com/gradio-app/gradio",
        "document": "Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. No JavaScript, CSS, or web hosting experience needed!\n\nIt just takes a few lines of Python to create your own demo, so let's get started 💫\n\nWe recommend installing Gradio using , which is included by default in Python. Run this in your terminal or command prompt:\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\nNow, run your code. If you've written the Python code in a file named , then you would run from the terminal.\n\nThe demo below will open in a browser on http://localhost:7860 if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\nYou'll notice that in order to make your first demo, you created an instance of the class. The class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.\n\nThe class has three core arguments:\n• : the function to wrap a user interface (UI) around\n• : the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n• : the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe argument is very flexible -- you can pass any Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe and arguments take one or more Gradio components. As we'll see, Gradio includes more than 30 built-in components (such as the , , and components) that are designed for machine learning applications.\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to , with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to . This flexibility makes the class a very powerful way to create demos.\n\nWe'll dive deeper into the on our series on building Interfaces.\n\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set in , and a publicly accessible URL will be created for your demo. Let's revisit our example demo, but change the last line as follows:\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on sharing your Gradio application.\n\nSo far, we've been discussing the class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio include?\n\nGradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction — still all in Python.\n\nYou can build very custom and complex applications using . For example, the popular image generation Automatic1111 Web UI is built using Gradio Blocks. We dive deeper into the on our series on building with Blocks.\n\nGradio includes another high-level class, , which is specifically designed to create Chatbot UIs. Similar to , you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to our dedicated guide on .\n\nThat's the gist of the core Python library, but Gradio is actually so much more! It's an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n• Gradio-Lite ( ): write Gradio apps in Python that run entirely in the browser (no server needed!), thanks to Pyodide.\n• Hugging Face Spaces: the most popular place to host Gradio applications — for free!\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: let's dive deeper into the Interface class.\n\nOr, if you already know the basics and are looking for something specific, you can search the more technical API documentation.\n\nYou can also build Gradio applications without writing any code. Simply type into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or use this hosted version of Gradio Sketch, running on Hugging Face Spaces.\n\nIf you'd like to report a bug or have a feature request, please create an issue on GitHub. For general questions about usage, we are available on our Discord server and happy to help.\n\nIf you like Gradio, please leave us a ⭐ on GitHub!\n\nGradio is built on top of many wonderful open-source libraries!\n\nGradio is licensed under the Apache License 2.0 found in the LICENSE file in the root directory of this repository.\n\nAlso check out the paper Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild, ICML HILL 2019, and please cite it if you use Gradio in your work."
    },
    {
        "link": "https://gradio.app",
        "document": "Gradio can be installed with pip. Creating a Gradio interface only requires adding a couple lines of code to your project. Seamlessly use any python library on your computer. If you can write a python function, gradio can run it. Gradio can be embedded in Python notebooks or presented as a webpage. A Gradio interface can automatically generate a public link you can share with colleagues that lets them interact with the model on your computer remotely from their own devices. Once you've created an interface, you can permanently host it on Hugging Face. Hugging Face Spaces will host the interface on its servers and provide you with a link you can share."
    },
    {
        "link": "https://geeksforgeeks.org/python-convert-image-to-pdf-using-img2pdf-module",
        "document": "img2pdf is an open source Python package to convert images to pdf format. It includes another module Pillow which can also be used to enhance image (Brightness, contrast and other things) Use this command to install the packages\n\nBelow is the implementation: Image can be converted into pdf bytes using img2pdf.convert() functions provided by img2pdf module, then the pdf file opened in wb mode and is written with the bytes."
    },
    {
        "link": "https://stackoverflow.com/questions/71325155/convert-images-to-pdf-using-img2pdf-package-in-python",
        "document": "The error message is cryptic largely because bends over backwards to be so versatile about what you can pass to the function: a file path (as a string or path object), an open file object, raw image data, multiples of any of those, or a list of any of those. If given a string, it first treats it as a file path and tries to open it; if that fails, it then assumes the input must be raw image data (as bytes) and attempts to process it that way, in which case you get the that you see.\n\nThe reason it can't open the files correctly is that returns only the file names themselves, not their full path, and those image files aren't in your current working directory. That is, you're trying to open , not .\n\nYou can fix this by manually adding the folder name to each file name:\n\nYou could also use . It often makes dealing with paths easier (and more easily platform-independent) than messing about with . objects are smarter than plain strings, and automatically know about their enclosing folder in this case.\n\nAll of Python's builtins (like ) will accept a object anywhere they already accept a string representing a path, and by this point practically all third-party libraries do as well. This includes img2pdf, as of 0.5.0.\n\nHere's what a very literal translation would look like, except using :\n\nBut now that the method's doing the filtering, the list comprehension isn't actually doing anything. We can't actually pass it the result of , as that's a generator rather than a list, but we can unpack it to pass the individual results as multiple arguments."
    },
    {
        "link": "https://medium.com/@hunter-j-phillips/how-to-convert-an-image-to-a-pdf-in-python-f1f9cee3b996",
        "document": "Want to convert one or more images to a PDF document? Look no further than the img2pdf and PyPDF2 packages.\n\nTo start, all you need is a Python environment, preferably version 3.10 or higher. The code in this tutorial was executed in a Google Colaboratory environment with Python 3.10.12.\n\nThe first step is to ensure the following packages are installed in the Python environment:\n\nPip can be used to install these packages in Colab:\n\nThe first package, img2pdf, will be used to convert an image to a PDF file. Then, PyPDF2 can be used to merge multiple PDFs into a single PDF file. Pillow is an image processing library; it provides additional functions necessary for the conversion.\n\nThese packages, along with and , can now be imported.\n\nBefore writing any more code, it is important to know the file location of each image. To make this as easy as possible, a new folder can be created in the Colab environment:\n\nAll the images need to be uploaded simultaneously to this location using an uploader provided by . The files will be ordered based on their names, so they should be named something like, .\n\nWith the images stored in a known file location, their names can be stored in a list.\n\nIf there are more than 9 images, there will likely be issues with this approach, and list should be created with the files in the order they need to be in.\n\nA for-loop can then be used to iterate over each image, convert it to a PDF, and write it to a new folder called .\n\nWith the images converted to PDF files, they can either be used independently and downloaded with , or they can be merged together. To merge the files together, extract the list of PDF files and sort them by their page number.\n\nOnce again, if there are more than 9 images or PDFs, they should be stored in a list in their respective order.\n\nA object can be used to concatenate each PDF into a single file.\n\nThe final merged PDF will contain each image in the order of their respective names.\n\nThe entirety of the code can be found below. It is highly customizable to meet most use cases."
    },
    {
        "link": "https://freecodecamp.org/news/convert-multiple-images-into-a-single-pdf-file-with-python",
        "document": "Creating projects is the best way to learn a programming language. It is fun and it's a creative way to learn new things.\n\nWhenever I try to learn a new language or new technology, I try to create a project, whether it's a small byte-sized or big project.\n\nIn this article, I am going to show you a small but very cool project if you are a complete beginner learning Python.\n\nYou will create a project that will grab all of the image files from a particular directory and create a single PDF file that includes all of the images.\n\nThe interesting thing about Python is, you will need only 4 lines of code to achieve that! So, let's get started, shall we?\n\nThis is what my project directory looks like without Git.\n\nYou can see what my project looks like with Git in this repo. Don't forget to star the repository to show your love.\n\nTo get started, first create a new folder for the project. Make sure that you do not include any spaces in the folder's name.\n\nAdd some image files in that directory. For this project, I am going to use the image files. Therefore I would suggest you to do the same thing!\n\nYou can download the royalty free images from Unsplash or Pexels.\n\nKeep in mind that our project can't handle large image files. So try to download those image files which are smaller in file size. You can select small files when downloading them from Unsplash.\n\nYou can also find the images in the folder in the GitHub repository listed earlier on.\n\nNext, open Visual Studio Code. Visual Studio Code is free and a widely used code editor.\n\nIf you prefer using a different editor, you can go ahead and open the project using the editor of your choice.\n\nNow, create a Python file named . Here is where you will write down the code for this project.\n\nLastly, install the package/library named . This library is used for converting images to PDF via direct JPEG inclusion. You can check this website for more details.\n\nI am going to install it using . Open a terminal window and enter the command .\n\nWe can work with some pre-defined libraries. If we do that, then we do not necessarily need to write everything from scratch.\n\nPython already has a ton of libraries, and we can directly use their pre-defined functionality. But for that, we need to import those libraries before trying to work with them.\n\nFirst, you need to import the relevant 2 packages/modules/libraries named and . If you want to work with some pre-defined libraries/modules, then it is necessary to mention them earlier as the interpreter would find them before proceeding to work on those specific libraries.\n\nWe need the library. This module comes under Python's standard utility modules.\n\nThe OS module in Python provides functions for interacting with the operating system. OS comes under Python's standard utility modules. This module provides a portable way of using operating system-dependent functionality. As we will use the file directory from our local storage, it is necessary for our task.\n\nFor importing a library in Python, we simply use . In this case, we used to import the library. After importing a library we can use them anytime we want in that script or Python file.\n\nAs for the other library, , remember that we are going to use this library for converting our image files to PDF files.\n\nFor importing the library, we use the same import command, .\n\nAfter importing the necessary two libraries, we can use them in our script anytime we want, and we can also use all of the functionality of the two libraries. It makes our tasks easier and our code shorter. Before doing that, make sure that you have already installed the library using or .\n\nNow I need to specify exactly in which file format and file name I want to place my image files. I will create a specific PDF file where all of the images will be integrated. Therefore I will specify that using the file use command.\n\nThe command structure is something like .\n\nTherefore, our command would be:\n\nThis will create a PDF file named and integrate all of the image files there.\n\nIf you want to have a different file name, then you can change the name, but make sure to not keep any spaces in the file name. For example, do not use any file names like . Instead of using any space, use underscore ( ), like . But I prefer to use something like .\n\nAnd check that you have also included the file extension ( in this case, you are working with a PDF file so the file extension must need to be ).\n\nAs we will write in that file and we will work on the binary files, we have used the formatting as the . The indicates that the file is opened for writing in binary mode.\n\nAccording to a solution from StackOverflow:\n\nThen I need to specify what I want to do with the file.\n\nI want to write in that file and I want the conversion functionality of the library.\n\nIn my directory, there might be a lot of different files and that is natural. But as I only want to convert the image files which have a extension in them, I need to specify that explicitly.\n\nAlso, I definitely need to include the file directory where it will get all of the images.\n\nTherefore the last line of our script would be:\n\nLet me explain the code now.\n\n: This line uses the module to list all the files in the directory specified by the given path. In this case, it is the directory \"C:\\Users\\fahim\\Desktop\\ImageToPdf\".\n\n[i for i in os.listdir(\"C:\\\\Users\\\\fahim\\\\Desktop\\\\ImageToPdf\") if i.endswith(\".jpg\")] : This is a list comprehension that filters the files obtained from the directory listing. It iterates through each file name in the directory and only includes those that end with the extension \".jpg\". This step ensures that only JPEG images will be considered for the conversion to PDF.\n\n: The library provides the function, which takes a list of image file paths and converts them into a single PDF file. The code passed inside the parentheses is generating the list of image file paths (JPEG images ending with \".jpg\") using the list comprehension.\n\n: It seems that is a file object that was opened in write mode. The method is being used to write the PDF content to the file.\n\nTo use this code successfully, you need to make sure of the following:\n• None That the library is installed in your Python environment.\n• None Replace the directory path with the path to the directory containing the JPEG images you want to convert to PDF.\n• None That you have appropriate write permissions for the specified directory and file.\n\nIt's important to note that the code converts all the JPEG images in the specified directory into a single PDF file. If there are other file types or non-image files in that directory, they will be ignored during the conversion.\n\nIn a nutshell, the entire Python script is:\n\nMake sure to include an extra backspace in the file directory. We do this because we want to notify it that it is not an escape sequence, but is part of that directory path string.\n\nIf we want, then we can modify the code more. Another example of using the same code by breaking them down into individual segments can be like below:\n\nAgain, let me provide the explanation for all of the lines in detail:\n• None First, we import the necessary modules: to interact with the file system and for image-to-PDF conversion.\n• None The variable should be replaced with the path to the folder containing the JPEG images that need to be converted.\n• None Using list comprehension, we obtain a list of image files in the specified directory, filtering only those with the \".jpg\" extension. These are the images that will be included in the PDF.\n• None The function takes the list of image files and converts them into a single PDF file, storing the PDF content in the variable.\n• None We open a new file named \"output.pdf\" in binary write mode ( ) using a statement to ensure proper file handling and closure.\n• None Finally, we write the content to the \"output.pdf\" file, effectively creating the PDF with the converted images.\n\nNote: Before running the code, ensure that the library is installed in your Python environment. You can install it using . Also, make sure you have the necessary write permissions for the specified directory and file.\n\nIf you have the Code Runner extension installed on your VS Code, then you can run the file using that extension.\n\nBut if you like to run the code from your terminal then the command would be for Windows and for Mac or Linux.\n\nAs my filename is and I am using my Windows machine, my command would be .\n\nInstantly you will receive the PDF file which contains all of the image files (where the image files have a file extension).\n\nI know that many of you like to watch a video instead of following a complete article. Fear not! I have also created a complete video tutorial for you:\n\nI hope you have enjoyed this short article. You should now be able to convert your images into a single PDF file in your own projects. 😊\n\nIf you have any questions then please let me know by reaching out on Twitter or LinkedIn.\n\nYou can also follow me on:\n\n🎁GitHub: FahimFBA\n\n🎁YouTube: @FahimAmin\n\nIf you are interested then you can also check my website: https://fahimbinamin.com/"
    },
    {
        "link": "https://nigelayen.com/convert-images-to-pdf-using-python-and-the-img2pdf-library",
        "document": "I recently worked on an ETL project where I had to refer to screenshots from the legacy application quite often. I had a folder full of screenshot images but it was quite inconvenient to keep clicking through images to find the one I wanted. A better solution would be to have all of the images in a single PDF file where I could just scroll through. One option would be to paste the images to a Word document and then export that to PDF. There must be a better way. Python to the rescue with a learning opportunity.\n\nIn today’s digital age, working with images and documents is a common task in various fields. Sometimes, you might need to consolidate a collection of image files into a single PDF document. In this blog post, I will walk you through the process of creating a Python program to achieve just that, using the Python img2pdf library. This library simplifies the process of converting a list of image files into a PDF file, making it an excellent tool for automating PDF document creation.\n\nImg2pdf is a Python library that provides a straightforward way to convert image files into PDF documents. It supports various image formats, including PNG, JPEG, and more, allowing you to create PDF files from a variety of source images. The library is lightweight, easy to use, and can be a valuable addition to your Python toolkit when dealing with image-to-PDF conversion tasks.\n• Import required modules:\n\nTo begin, we import the necessary Python modules, ‘os‘ and ‘img2pdf,’ which will be used in our program.\n• Define the working directory:\n\nWe specify the path to the directory containing the image files you want to convert into a PDF. The new PDF file will also be saved to this folder. Make sure to replace with the actual path to your image directory.\n• Generate a list of image files:\n\nWe create an empty list called ‘images’ to store the file paths of image files with the “.png” extension found in the specified directory. Make sure to change the image file extension if not working with PNG images.\n• Create the PDF file:\n\nWith our list of image file paths appended to the ‘images‘ list, we can proceed to create the PDF document. We determine the name of the output PDF file by using the base name of the source directory using the ‘basename‘ method from the ‘os.path‘ library.\n• Write the PDF file:\n\nWe open a PDF file for writing in binary mode (‘wb’) and use img2pdf’s ‘convert‘ method to convert the list of image files into a single PDF. The resulting PDF data is written to the file.\n\nThe completed code should look like this:\n\nIn this blog post, we’ve explored the process of using the img2pdf library to create a Python program that consolidates a collection of image files into a single PDF document. The program is concise, efficient, and an excellent example of how Python can simplify complex tasks. We’ve learned about importing Python modules, working with file paths, and using the img2pdf library for image-to-PDF conversion. By mastering these concepts, you can streamline various document-related processes in your Python projects and automate your workflow."
    }
]