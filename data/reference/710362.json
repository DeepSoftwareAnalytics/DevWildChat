[
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html",
        "document": "Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified. The optional scale argument can only be specified as a keyword argument.\n\nThis function always applies dropout according to the specified argument. To disable dropout during evaluation, be sure to pass a value of when the module that makes the function call is not in training mode.\n\nThere are currently three supported implementations of scaled dot product attention:\n• None FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n• None A PyTorch implementation defined in C++ matching the above formulation The function may call optimized kernels for improved performance when using the CUDA backend. For all other backends, the PyTorch implementation will be used. All implementations are enabled by default. Scaled dot product attention attempts to automatically select the most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation is used, the following functions are provided for enabling and disabling implementations. The context manager is the preferred mechanism:\n• None : A context manager used to enable or disable any of the implementations.\n• None : Globally enables or disables the PyTorch C++ implementation. Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation, disable the PyTorch C++ implementation using . In the event that a fused implementation is not available, a warning will be raised with the reasons why the fused implementation cannot run. Due to the nature of fusing floating point operations, the output of this function may be different depending on what backend kernel is chosen. The c++ implementation supports torch.float64 and can be used when higher precision is required. For math backend, all intermediates are kept in torch.float if inputs are in torch.half or torch.bfloat16.\n\nFor more information please see Numerical accuracy\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting . See Reproducibility for more information.\n• None attn_mask (optional Tensor) – Attention mask; shape must be broadcastable to the shape of attention weights, which is (N,...,L,S). Two types of masks are supported. A boolean mask where a value of True indicates that the element should take part in attention. A float mask of the same type as query, key, value that is added to the attention score.\n• None dropout_p (float) – Dropout probability; if greater than 0.0, dropout is applied\n• None is_causal (bool) – If set to true, the attention masking is a lower triangular matrix when the mask is a square matrix. The attention masking has the form of the upper left causal bias due to the alignment (see ) when the mask is a non-square matrix. An error is thrown if both attn_mask and is_causal are set.\n• None scale (optional python:float, keyword-only) – Scaling factor applied prior to softmax. If None, the default value is set to E ​1​.\n• None enable_gqa (bool) – If set to True, Grouped Query Attention (GQA) is enabled, by default it is set to False."
    },
    {
        "link": "https://medium.com/@vmirly/tutorial-on-scaled-dot-product-attention-with-pytorch-implementation-from-scratch-66ed898bf817",
        "document": "Now, let’s see how the query, key, and value vectors are computed. We assume that prior to the attention layer, the input sequence has been fed through some layers such as an embedding layer and feature vectors x₁ to x₁₁ have already been obtained. These feature vectors x₁ to x₁₁ form a sequence, and each of them has size d, i.e., a row-vector of dimensionality 1 by d. Concatenating these features x₁ to x₁₁ will form a matrix X with 11 rows and d columns.\n\nOn each of these feature vectors xᵢ, we perform three matrix multiplications using three learnable weight matrices to obtain qᵢ, kᵢ and vᵢ. The size of query vector q and key vector k must match so we can denote them by dₖ, but the value vector could have different size, e.g., dᵥ.\n\nIn practice, it’s easier to make the query, key, and value vectors have the same dimensionality to simplify tracking their dimensionalities. Therefore, we use the same dimensionality d=dₖ=dᵥ for all three vectors.\n\nThis ensures that the dot product between the query and key vectors results in a matrix with appropriate dimensions to compute the attention weights, which are then used to compute the weighted sum of the value vectors.\n\nThe three weight matrices are learnable with a size of d×d, and they are shared among all elements of the input sequence X. We can compute qᵢ, kᵢ, and vᵢ sequentially by going through each feature xᵢ from i=1 to i=11. Alternatively, we can perform these operations more efficiently using the entire feature sequence X multiplied by the weight matrices.\n\nA simplified implementation from scratch\n\nIn the following code-block, I will start with a very simple example, and implement the self-attention in PyTorch step-by-step. For the initial setup, we can assume we have got our token IDs (e.g. by applying a tokenizer to sentence), so we startwith these :\n\nAnd then, we build an Embedding layer to compute features from the :\n\nSo we have made our initial setup. Next, the attention layer start from here. So, we need to compute the Q, K, and V matrices:\n\nIn the next section, we will explain how the query, key, and value vectors are obtained and how they are used in the Scaled Dot-Product Attention mechanism.\n\nStep 1: Dot-product between query and key matrices\n\nThe first step of the Scaled Dot-Product Attention layer involves taking vector qᵢ and vector kⱼ and performing a dot product between them. However, as explained in my previous video, for a dot product, the first vector must be a row-vector and the second vector must be a column-vector. Therefore, we must transpose vector kⱼ to get a column vector with dimensionality d×1. Performing this dot product results in a scalar output, as shown on the right.\n\nWe can repeat this process for all values of i and j from 1 to T, and at the end, we obtain a similarity matrix or compatibility matrix, as the authors call it. Alternatively, we can perform this using a matrix multiplication to compute the compatibility matrix at once:\n\nThe similarity matrix captures the similarities between the query and key vectors, providing a measure of how much attention the mechanism should assign to each value vector in the input sequence. Implementing this step can be done using the function in PyTorch as shown below:\n\nNote that in the code above, will transpose matrix K to make the correct dimensionalities for the dot-product.\n\nIn the next section, we will explain how the similarity matrix is scaled and normalized using the Softmax function to obtain the attention weights.\n\nStep 2: Scaling the compatibility matrix\n\nIn the second step, we scale the dot-product of the query and key vectors by a factor of\n\nsince we assumed that d = dₖ = dᵥ.\n\nIt is important to note that in the literature, there are two main types of attention: multiplicative attention, such as this dot-product attention, and additive attention. While we are not covering additive attention in this video, it’s worth mentioning that dot-product attention is more efficient than additive attention, and they both perform fairly similarly as long as the dimension d is small. However, when the dimension d is very large, dot-product attention without this scaling factor performs poorly compared to additive attention.\n\nThe authors of the paper “Attention Is All You Need” attribute this poor performance to having large variance. If we start with q and k that have a normal distribution with zero mean and unit variance, then the resulting dot product of q and k will have variance d. If d is large, the variance of q times k is also large, and this will push the output of the Softmax function to regions with very small gradients, similar to the vanishing gradients problem.\n\nTo show this effect, in the following code-block, we will scale the compatibility matrix from the previous step, and plot the heatmapts for before and after the scaling:\n\nAs you can see, the variance of Q and K are both around ≈34 and ≈26, but after the dot-product, the variance increases to ≈25801. So scaling is done to bring the variance down to remedy problems associated with Softmax which we will apply in the next step.\n\nStep 3: Obtaining Attention Weights with Softmax Normalization\n\nMoving to step 3, we apply the Softmax function to our scaled compatibility matrix. For a brief recap of the Softmax function, it takes a vector z and produces an output vector [σ(z₁), …, σ(zₙ)] where σ(zᵢ) given by the equation:\n\nThe Softmax function has two important properties:\n• Each output is between zero and one:\n\n2. The sum of all the outputs of softmax is 1:\n\nSo step 3 is as simple as applying the Softmax to each row of the scaled compatibility matrix, resulting in a matrix of attention weights where each row sums up to 1:\n\nStep 4: Computing the Context Vector as final output\n\nIn the last step of the scaled dot-product attention mechanism, we take the attention matrix from the previous step, denoted as A with dimensionality T×T, and the value matrix V with dimensionality T×d. We perform the matrix multiplication A×V to obtain a new matrix of size T×d.\n\nUnlike in step 1, we don’t need to transpose anything because the dimensionality of the left and right matrices are already compatible. The resulting matrix is the context matrix, which represents the final output of our attention layer. The context matrix contains d-dimensional vectors, where each vector represents the context for the corresponding token in the input sequence."
    },
    {
        "link": "https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html",
        "document": "Click here to download the full example code\n\nDepending on what machine you ran the above cell on and what hardware is available, your results might be different. - If you don’t have a GPU and are running on CPU then with FP32 the context manager will have no effect and all three runs should return similar timings. - Depending on what compute capability your graphics card supports flash attention or memory efficient might have failed.\n\nWith the release of PyTorch 2.0, a new feature called has been introduced, which can provide significant performance improvements over eager mode. Scaled dot product attention is fully composable with . To demonstrate this, let’s compile the module using and observe the resulting performance improvements. \"The non compiled module runs in The non compiled module runs in 416.026 microseconds The compiled module runs in 517.141 microseconds The exact execution time is dependent on machine, however the results for mine: The non compiled module runs in 166.616 microseconds The compiled module runs in 166.726 microseconds That is not what we were expecting. Let’s dig a little deeper. PyTorch comes with an amazing built-in profiler that you can use to inspect the performance characteristics of your code. # For even more insights, you can export the trace and use ``chrome://tracing`` to view the results ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Non-Compilied Causal Attention 0.00% 0.000us 0.00% 0.000us 0.000us 10.537ms 101.58% 10.537ms 10.537ms 1 Non-Compilied Causal Attention 20.52% 2.265ms 77.21% 8.521ms 8.521ms 0.000us 0.00% 10.373ms 10.373ms 1 aten::linear 1.17% 129.613us 28.65% 3.162ms 63.236us 0.000us 0.00% 7.767ms 155.333us 50 aten::matmul 2.43% 268.403us 24.54% 2.708ms 54.153us 0.000us 0.00% 7.767ms 155.333us 50 aten::mm 15.35% 1.694ms 19.77% 2.182ms 43.639us 7.767ms 74.87% 7.767ms 155.333us 50 ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn 0.00% 0.000us 0.00% 0.000us 0.000us 5.566ms 53.65% 5.566ms 222.628us 25 aten::scaled_dot_product_attention 2.01% 221.551us 18.30% 2.020ms 80.800us 0.000us 0.00% 2.607ms 104.261us 25 aten::_scaled_dot_product_flash_attention 3.08% 340.394us 16.30% 1.798ms 71.938us 0.000us 0.00% 2.607ms 104.261us 25 aten::_flash_attention_forward 3.62% 399.344us 11.44% 1.262ms 50.496us 2.607ms 25.13% 2.607ms 104.261us 25 void pytorch_flash::flash_fwd_kernel<pytorch_flash::... 0.00% 0.000us 0.00% 0.000us 0.000us 2.607ms 25.13% 2.607ms 104.261us 25 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 11.036ms Self CUDA time total: 10.373ms ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Name Self CPU % Self CPU CPU total % CPU total CPU time avg Self CUDA Self CUDA % CUDA total CUDA time avg # of Calls ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Compiled Causal Attention 0.00% 0.000us 0.00% 0.000us 0.000us 10.496ms 101.10% 10.496ms 10.496ms 1 Compiled Causal Attention 9.52% 1.061ms 78.15% 8.706ms 8.706ms 0.000us 0.00% 10.382ms 10.382ms 1 Torch-Compiled Region: 2/0 8.70% 969.588us 66.49% 7.407ms 296.283us 0.000us 0.00% 10.382ms 415.288us 25 CompiledFunction 27.45% 3.058ms 57.79% 6.437ms 257.499us 0.000us 0.00% 10.382ms 415.288us 25 aten::mm 9.95% 1.108ms 15.08% 1.680ms 33.596us 7.766ms 74.80% 7.766ms 155.315us 50 ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn 0.00% 0.000us 0.00% 0.000us 0.000us 5.563ms 53.58% 5.563ms 222.509us 25 aten::_scaled_dot_product_flash_attention 2.29% 254.914us 15.26% 1.700ms 67.991us 0.000us 0.00% 2.616ms 104.657us 25 aten::_flash_attention_forward 3.69% 411.523us 11.15% 1.242ms 49.669us 2.616ms 25.20% 2.616ms 104.657us 25 void pytorch_flash::flash_fwd_kernel<pytorch_flash::... 0.00% 0.000us 0.00% 0.000us 0.000us 2.616ms 25.20% 2.616ms 104.657us 25 ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_3... 0.00% 0.000us 0.00% 0.000us 0.000us 2.203ms 21.22% 2.203ms 88.122us 25 ------------------------------------------------------- ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ ------------ Self CPU time total: 11.140ms Self CUDA time total: 10.382ms The previous code snippet generates a report of the top 10 PyTorch functions that consumed the most GPU execution time, for both the compiled and non-compiled module. The analysis reveals that the majority of time spent on the GPU is concentrated on the same set of functions for both modules. The reason for this here is that is very good at removing the framework overhead associated with PyTorch. If your model is launching large, efficient CUDA kernels, which in this case is, then the overhead of PyTorch can be hidden. In reality, your module does not normally consist of a singular block. When experimenting with Andrej Karpathy NanoGPT repository, compiling the module took the time per train step from: to ! This was done on commit: of NanoGPT training on the Shakespeare dataset.\n\nIn this tutorial, we have demonstrated the basic usage of . We have shown how the context manager can be used to assert a certain implementation is used on GPU. As well, we built a simple module that works with and is torch compilable. In the process we have shown how to the profiling tools can be used to explore the performance characteristics of a user defined module."
    },
    {
        "link": "https://medium.com/@heyamit10/guide-to-scaled-dot-product-attention-with-pytorch-56b9c603836f",
        "document": "I understand that learning data science can be really challenging… …especially when you are just starting out. But it doesn’t have to be this way. That’s why I spent weeks creating a 46-week Data Science Roadmap with projects and study resources for getting your first data science job. If that’s not enough, I’ve also added: 4. A Discord community to help our data scientist buddies get access to study resources, projects, and job referrals. Click here to access everything! Now, let’s get back to the blog:\n\n“Attention is all you need.” This phrase might sound like it belongs in a self-help book, but in the world of transformers and NLP, it’s almost literal. Scaled Dot-Product Attention is the foundation of the self-attention mechanism at the heart of transformer models like BERT and GPT. It’s what gives these models the ability to capture intricate dependencies across words, phrases, and contexts — far better than previous approaches like RNNs or CNNs could achieve. Brief Motivation:\n\nSo, why does Scaled Dot-Product Attention matter? Well, this mechanism revolutionized NLP by making it possible for models to “focus” on different parts of the input dynamically, adjusting based on the context. This adaptability is crucial because it lets models understand relationships across words or tokens, which in turn allows them to interpret sentences in ways that mimic human understanding. But here’s the deal: this focus needs to be mathematically stable, especially when dealing with long sequences or stacked attention layers. That’s where scaling comes in — by adjusting for the dimensionality of input vectors, we prevent our attention scores from exploding, ensuring reliable training in deep architectures. In this guide, we’ll go beyond simply “using” Scaled Dot-Product Attention. You’ll not only learn how to implement it from scratch in PyTorch, but also gain insights into the nuances that can help you customize and optimize it for your own models.\n\nLet’s set you up with everything you need. For an implementation-heavy guide like this, you’ll want to make sure you’re working with compatible libraries and a stable environment.\n• We’ll need a recent version of PyTorch. Here’s a quick setup to get you started: # requirements.txt\n\ntorch==1.13.0 # Or the version you're currently using Ensure you have a good GPU setup, as attention mechanisms can be computation-heavy, especially when experimenting with larger tensors. Knowledge Assumptions: I’m going to assume you already have a good understanding of PyTorch basics — think tensor manipulations, broadcasting, and matrix operations. You’re also familiar with attention mechanisms at a high level. While I won’t cover foundational theory, I’ll provide some key tips along the way to make sure you’re catching any potential issues or optimizations, especially if you’re adapting this code to real-world applications.\n\nAlright, here’s where the real work begins. The key elements in Scaled Dot-Product Attention are the Query, Key, and Value matrices. These matrices are derived from your input data and form the basis of the attention mechanism. Let’s start by defining our Q, K, and V matrices. Typically, in a transformer, these matrices are generated by applying learned weight matrices to your input data. For this guide, we’ll keep it simple and initialize random tensors to represent these matrices for demonstration purposes. First, let’s set up reproducibility to keep our random results consistent. This is critical in attention mechanisms, especially if you’re troubleshooting or comparing outputs. import torch\n\nimport math\n\n\n\n# Setting a random seed for reproducibility\n\ntorch.manual_seed(42)\n\n\n\n# Dimensions for demonstration\n\nbatch_size = 2 # Number of samples\n\nseq_len = 5 # Sequence length (number of tokens)\n\nd_k = 64 # Dimension of key (and also query)\n\n\n\n# Initialize random Q, K, V matrices\n\nQ = torch.randn(batch_size, seq_len, d_k)\n\nK = torch.randn(batch_size, seq_len, d_k)\n\nV = torch.randn(batch_size, seq_len, d_k)\n\n\n\nprint(\"Query Matrix Q:\", Q)\n\nprint(\"Key Matrix K:\", K)\n\nprint(\"Value Matrix V:\", V) Here, , , and are random tensors with identical dimensions. In a typical transformer, they would be projections of the input embedding, with each one carrying a different “perspective” on the data. Now, let’s get into the core scaling step. When computing attention scores, the dot product between and (the transpose of ) can lead to large values. To prevent these from causing gradients to explode, we scale by the square root of the dimensionality, . This keeps our values within a stable range, ensuring that softmax has a well-behaved distribution. # Compute the raw attention scores by performing the dot product of Q and Kᵀ\n\nattention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n\n\n\nprint(\"Scaled Attention Scores:\", attention_scores) In this code, we use to compute the dot product. Since has dimensions , we transpose it to to align dimensions with . This way, can compute the dot product efficiently in batches. Here’s a trick: scaling by may seem like a small adjustment, but it’s essential in deep networks. Without it, you’d find your gradients might oscillate or become unstable in complex models, especially as sequence lengths grow.\n\nThe essence of Scaled Dot-Product Attention lies in calculating how “similar” each token in a sequence is to every other token. We achieve this similarity through a dot product between and the transpose of . But here’s something you might not have thought of: in PyTorch, choosing the right matrix multiplication function— or —can significantly impact performance and memory usage, especially when working in batches. When you’re working with batches, (batch matrix-matrix product) is your go-to for multiplying 3D tensors. Let’s break down how it works in the code below.\n• We use and (transposed ) to get our attention scores, scaled by .\n• multiplies with across batches efficiently, producing an output of shape . This might surprise you: in attention-heavy models, memory can be a bottleneck. Using conserves memory compared to higher-level operations, making it optimal for handling larger batches. Code Optimizations: For even larger models, is another option, allowing flexible manipulation of dimensions. It can offer speed advantages for very specific use cases but requires tuning. You could rewrite the above as: Experiment with both approaches to see which one best suits your memory and performance constraints, as can be particularly advantageous in larger models with complex tensor manipulations. Once you have the raw attention scores, the next step is to convert them into a probability distribution. This is where softmax comes in, turning the scores into values between 0 and 1 that sum to 1 across each sequence. But here’s the deal: apply over the last axis, , to ensure normalization across each row. You might be wondering why softmax scaling matters. In fact, if you need to tweak the sharpness of attention, you can scale the softmax with a temperature parameter: temperature = 0.5 # Try experimenting with different values for sharper focus\n\nattention_weights = torch.softmax(attention_scores / temperature, dim=-1) If you’re encountering values, it’s often due to overflow in softmax. A quick fix: subtract the max value in before applying softmax. This re-centers your values, preventing large exponentials.\n\nWhen you’re working with scaled dot-product attention in large models, optimizations become crucial. Here are some advanced techniques to ensure your code remains efficient: To minimize memory overhead, always use batch operations. Additionally, tensor reshaping with operations like or can reduce data movement across devices, which is often a hidden cost in attention mechanisms. Here’s a code snippet showing how to use for a potentially faster attention operation, depending on the architecture you’re working with: Experimenting with may yield faster results on GPUs, especially for specific tensor shapes and configurations. If you’re implementing self-attention or working with padded sequences, you’ll need masked attention to ensure padding tokens don’t interfere. In practice, this involves adding a large negative value (often ) to the positions you want masked before applying softmax. # Create a mask for padded positions (assuming 0 indicates padding)\n\nmask = (torch.randn(batch_size, seq_len) > 0).unsqueeze(1).expand(-1, seq_len, -1)\n\n\n\n# Apply the mask to the attention scores\n\nattention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n\nattention_weights = torch.softmax(attention_scores, dim=-1) By applying a mask before softmax, you zero out padded tokens’ influence without adding much computational cost. PyTorch’s operation is efficient here and ensures your attention weights stay valid.\n\nNow that we’ve gone through each part, it’s time to bring them all together into a single, modular class that will let you implement Scaled Dot-Product Attention with ease. This function will be flexible, allowing you to adjust parameters if you’re looking to extend it to multi-head attention or integrate it into a larger architecture. import torch\n\nimport math\n\n\n\nclass ScaledDotProductAttention:\n\n def __init__(self, d_k):\n\n \"\"\"\n\n Initialize with the dimension of the key (d_k) for scaling.\n\n :param d_k: Dimension of key, used for scaling factor.\n\n \"\"\"\n\n self.d_k = d_k\n\n\n\n def forward(self, Q, K, V, mask=None):\n\n \"\"\"\n\n Compute the scaled dot-product attention.\n\n \n\n :param Q: Query matrix of shape (batch_size, seq_len, d_k)\n\n :param K: Key matrix of shape (batch_size, seq_len, d_k)\n\n :param V: Value matrix of shape (batch_size, seq_len, d_k)\n\n :param mask: Optional mask to exclude padded tokens (shape should match Q-K similarity shape)\n\n :return: Context matrix after applying attention\n\n \"\"\"\n\n # Step 1: Compute raw attention scores\n\n attention_scores = torch.bmm(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n\n \n\n # Step 2: Apply mask (if provided) to exclude padded tokens from attention calculation\n\n if mask is not None:\n\n attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n\n \n\n # Step 3: Normalize scores with softmax\n\n attention_weights = torch.softmax(attention_scores, dim=-1)\n\n \n\n # Step 4: Weighted sum of Value matrix based on attention weights\n\n context = torch.bmm(attention_weights, V)\n\n \n\n return context, attention_weights\n• Initialization: We initialize the class with , which is used for scaling the dot product.\n• Method: The forward method takes in , , , and an optional mask. We perform all steps from computing the dot product to scaling, masking, normalizing, and obtaining the context matrix. The method returns both the context and attention weights, which is helpful for debugging or visualization. This setup makes the code reusable in any architecture requiring Scaled Dot-Product Attention. You can easily adapt this by adding custom scaling factors or modifying it for multi-head attention if required."
    },
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html",
        "document": "Allows the model to jointly attend to information from different representation subspaces.\n\nMethod described in the paper: Attention Is All You Need.\n\nwill use the optimized implementations of when possible.\n\nIn addition to support for the new function, for speeding up Inference, MHA will use fastpath inference with support for Nested Tensors, iff:\n• None self attention is being computed (i.e., , , and are the same tensor).\n• None Either autograd is disabled (using or ) or no tensor argument\n• None and are equal to\n• None if a NestedTensor is passed, neither nor is passed\n\nIf the optimized inference fastpath implementation is in use, a NestedTensor can be passed for / / to represent padding more efficiently than using a padding mask. In this case, a NestedTensor will be returned, and an additional speedup proportional to the fraction of the input that is padding can be expected.\n\nCompute attention outputs using query, key, and value embeddings.\n• None query (Tensor) – Query embeddings of shape (L,Eq​) for unbatched input, (L,N,Eq​) when or (N,L,Eq​) when , where L is the target sequence length, N is the batch size, and Eq​ is the query embedding dimension . Queries are compared against key-value pairs to produce the output. See “Attention Is All You Need” for more details.\n• None key (Tensor) – Key embeddings of shape (S,Ek​) for unbatched input, (S,N,Ek​) when or (N,S,Ek​) when , where S is the source sequence length, N is the batch size, and Ek​ is the key embedding dimension . See “Attention Is All You Need” for more details.\n• None value (Tensor) – Value embeddings of shape (S,Ev​) for unbatched input, (S,N,Ev​) when or (N,S,Ev​) when , where S is the source sequence length, N is the batch size, and Ev​ is the value embedding dimension . See “Attention Is All You Need” for more details.\n• None key_padding_mask (Optional[Tensor]) – If specified, a mask of shape (N,S) indicating which elements within to ignore for the purpose of attention (i.e. treat as “padding”). For unbatched , shape should be (S). Binary and float masks are supported. For a binary mask, a value indicates that the corresponding value will be ignored for the purpose of attention. For a float mask, it will be directly added to the corresponding value.\n• None need_weights (bool) – If specified, returns in addition to . Set to use the optimized and achieve the best performance for MHA. Default: .\n• None attn_mask (Optional[Tensor]) – If specified, a 2D or 3D mask preventing attention to certain positions. Must be of shape (L,S) or (N⋅num_heads,L,S), where N is the batch size, L is the target sequence length, and S is the source sequence length. A 2D mask will be broadcasted across the batch while a 3D mask allows for a different mask for each entry in the batch. Binary and float masks are supported. For a binary mask, a value indicates that the corresponding position is not allowed to attend. For a float mask, the mask values will be added to the attention weight. If both attn_mask and key_padding_mask are supplied, their types should match.\n• None average_attn_weights (bool) – If true, indicates that the returned should be averaged across heads. Otherwise, are provided separately per head. Note that this flag only has an effect when . Default: (i.e. average weights across heads)\n• None is_causal (bool) – If specified, applies a causal mask as attention mask. Default: . Warning: provides a hint that is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.\n• None attn_output - Attention outputs of shape (L,E) when input is unbatched, (L,N,E) when or (N,L,E) when , where L is the target sequence length, N is the batch size, and E is the embedding dimension .\n• None attn_output_weights - Only returned when . If , returns attention weights averaged across heads of shape (L,S) when input is unbatched or (N,L,S), where N is the batch size, L is the target sequence length, and S is the source sequence length. If , returns attention weights per head of shape (num_heads,L,S) when input is unbatched or (N,num_heads,L,S). argument is ignored for unbatched inputs."
    },
    {
        "link": "https://datacamp.com/tutorial/building-a-transformer-with-py-torch",
        "document": "First introduced in the paper Attention is All You Need by Vaswani et al., Transformers have since become a cornerstone of many NLP tasks due to their unique design and effectiveness.\n\nAt the heart of Transformers is the attention mechanism, specifically the concept of 'self-attention,' which allows the model to weigh and prioritize different parts of the input data. This mechanism is what enables Transformers to manage long-range dependencies in data. It is fundamentally a weighting scheme that allows a model to focus on different parts of the input when producing an output.\n\nThis mechanism allows the model to consider different words or features in the input sequence, assigning each one a 'weight' that signifies its importance for producing a given output.\n\nFor instance, in a sentence translation task, while translating a particular word, the model might assign higher attention weights to words that are grammatically or semantically related to the target word. This process allows the Transformer to capture dependencies between words or features, regardless of their distance from each other in the sequence.\n\nTransformers' impact in the field of NLP cannot be overstated. They have outperformed traditional models in many tasks, demonstrating superior capacity to comprehend and generate human language in a more nuanced way.\n\nFor a deeper understanding of NLP, DataCamp's Introduction to Natural Language Processing in Python course is a recommended resource.\n\nBefore diving into building a Transformer, it is essential to set up the working environment correctly. First and foremost, PyTorch needs to be installed. PyTorch (current stable version - 2.0.1) can be easily installed through pip or conda package managers.\n\nFor pip, use the command:\n\nFor conda, use the command:\n\nFor using pytorch with a cpu kindly visit the pytorch documentation.\n\nAdditionally, it is beneficial to have a basic understanding of deep learning concepts, as these will be fundamental to understanding the operation of Transformers. For those who need a refresher, the DataCamp course Deep Learning in Python is a valuable resource that covers key concepts in deep learning.\n\nTo build the Transformer model the following steps are necessary:\n• Combining the Encoder and Decoder layers to create the complete Transformer network\n\n1. Importing the necessary libraries and modules\n\nWe’ll start with importing the PyTorch library for core functionality, the neural network module for creating neural networks, the optimization module for training networks, and the data utility functions for handling data. Additionally, we’ll import the standard Python math module for mathematical operations and the copy module for creating copies of complex objects.\n\nThese tools set the foundation for defining the model's architecture, managing data, and establishing the training process.\n\nThe Multi-Head Attention mechanism computes the attention between each pair of positions in a sequence. It consists of multiple “attention heads” that capture different aspects of the input sequence.\n\nTo know more about Multi-Head Attention, check out this Attention mechanisms section of the Large Language Models (LLMs) Concepts course.\n\nThe class is defined as a subclass of PyTorch's nn.Module.\n• num_heads: The number of attention heads to split the input into.\n\nThe initialization checks if d_model is divisible by num_heads, and then defines the transformation weights for query, key, value, and output.\n• Calculating Attention Scores: attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k). Here, the attention scores are calculated by taking the dot product of queries (Q) and keys (K), and then scaling by the square root of the key dimension (d_k).\n• Applying Mask: If a mask is provided, it is applied to the attention scores to mask out specific values.\n• Calculating Attention Weights: The attention scores are passed through a softmax function to convert them into probabilities that sum to 1.\n• Calculating Output: The final output of the attention is calculated by multiplying the attention weights by the values (V).\n\nThis method reshapes the input x into the shape (batch_size, num_heads, seq_length, d_k). It enables the model to process multiple attention heads concurrently, allowing for parallel computation.\n\nAfter applying attention to each head separately, this method combines the results back into a single tensor of shape (batch_size, seq_length, d_model). This prepares the result for further processing.\n\nThe forward method is where the actual computation happens:\n• Apply Linear Transformations: The queries (Q), keys (K), and values (V) are first passed through linear transformations using the weights defined in the initialization.\n• Split Heads: The transformed Q, K, V are split into multiple heads using the split_heads method.\n• Apply Scaled Dot-Product Attention: The scaled_dot_product_attention method is called on the split heads.\n• Combine Heads: The results from each head are combined back into a single tensor using the combine_heads method.\n• Apply Output Transformation: Finally, the combined tensor is passed through an output linear transformation.\n\nIn summary, the MultiHeadAttention class encapsulates the multi-head attention mechanism commonly used in transformer models. It takes care of splitting the input into multiple attention heads, applying attention to each head, and then combining the results. By doing so, the model can capture various relationships in the input data at different scales, improving the expressive ability of the model.\n\nThe class is a subclass of PyTorch's nn.Module, which means it will inherit all functionalities required to work with neural network layers.\n• d_model: Dimensionality of the model's input and output.\n• d_ff: Dimensionality of the inner layer in the feed-forward network.\n• self.fc1 and self.fc2: Two fully connected (linear) layers with input and output dimensions as defined by d_model and d_ff.\n• self.relu: ReLU (Rectified Linear Unit) activation function, which introduces non-linearity between the two linear layers.\n• x: The input to the feed-forward network.\n• self.fc1(x): The input is first passed through the first linear layer (fc1).\n• self.relu(...): The output of fc1 is then passed through a ReLU activation function. ReLU replaces all negative values with zeros, introducing non-linearity into the model.\n• self.fc2(...): The activated output is then passed through the second linear layer (fc2), producing the final output.\n\nIn summary, the PositionWiseFeedForward class defines a position-wise feed-forward neural network that consists of two linear layers with a ReLU activation function in between. In the context of transformer models, this feed-forward network is applied to each position separately and identically. It helps in transforming the features learned by the attention mechanisms within the transformer, acting as an additional processing step for the attention outputs.\n\nPositional Encoding is used to inject the position information of each token in the input sequence. It uses sine and cosine functions of different frequencies to generate the positional encoding.\n\nThe class is defined as a subclass of PyTorch's nn.Module, allowing it to be used as a standard PyTorch layer.\n• d_model: The dimension of the model's input.\n• max_seq_length: The maximum length of the sequence for which positional encodings are pre-computed.\n• pe: A tensor filled with zeros, which will be populated with positional encodings.\n• position: A tensor containing the position indices for each position in the sequence.\n• div_term: A term used to scale the position indices in a specific way.\n• The sine function is applied to the even indices and the cosine function to the odd indices of pe.\n• Finally, pe is registered as a buffer, which means it will be part of the module's state but will not be considered a trainable parameter.\n\nThe forward method simply adds the positional encodings to the input x.\n\nIt uses the first x.size(1) elements of pe to ensure that the positional encodings match the actual sequence length of x.\n\nThe PositionalEncoding class adds information about the position of tokens within the sequence. Since the transformer model lacks inherent knowledge of the order of tokens (due to its self-attention mechanism), this class helps the model to consider the position of tokens in the sequence. The sinusoidal functions used are chosen to allow the model to easily learn to attend to relative positions, as they produce a unique and smooth encoding for each position in the sequence.\n\nFigure 2. The Encoder part of the transformer network (Source: image from the original paper)\n\nThe class is defined as a subclass of PyTorch's nn.Module, which means it can be used as a building block for neural networks in PyTorch.\n• d_model: The dimensionality of the input.\n• num_heads: The number of attention heads in the multi-head attention.\n• d_ff: The dimensionality of the inner layer in the position-wise feed-forward network.\n• dropout: The dropout rate used for regularization.\n• self.norm1 and self.norm2: Layer normalization, applied to smooth the layer's input.\n• self.dropout: Dropout layer, used to prevent overfitting by randomly setting some activations to zero during training.\n• x: The input to the encoder layer.\n• mask: Optional mask to ignore certain parts of the input.\n• Self-Attention: The input x is passed through the multi-head self-attention mechanism.\n• Add & Normalize (after Attention): The attention output is added to the original input (residual connection), followed by dropout and normalization using norm1.\n• Feed-Forward Network: The output from the previous step is passed through the position-wise feed-forward network.\n• Add & Normalize (after Feed-Forward): Similar to step 2, the feed-forward output is added to the input of this stage (residual connection), followed by dropout and normalization using norm2.\n• Output: The processed tensor is returned as the output of the encoder layer.\n\nThe EncoderLayer class defines a single layer of the transformer's encoder. It encapsulates a multi-head self-attention mechanism followed by position-wise feed-forward neural network, with residual connections, layer normalization, and dropout applied as appropriate. These components together allow the encoder to capture complex relationships in the input data and transform them into a useful representation for downstream tasks. Typically, multiple such encoder layers are stacked to form the complete encoder part of a transformer model.\n• d_model: The dimensionality of the input.\n• num_heads: The number of attention heads in the multi-head attention.\n• d_ff: The dimensionality of the inner layer in the feed-forward network.\n• self.cross_attn: Multi-head attention mechanism that attends to the encoder's output.\n• x: The input to the decoder layer.\n• enc_output: The output from the corresponding encoder (used in the cross-attention step).\n• src_mask: Source mask to ignore certain parts of the encoder's output.\n• tgt_mask: Target mask to ignore certain parts of the decoder's input.\n• Self-Attention on Target Sequence: The input x is processed through a self-attention mechanism.\n• Add & Normalize (after Self-Attention): The output from self-attention is added to the original x, followed by dropout and normalization using norm1.\n• Cross-Attention with Encoder Output: The normalized output from the previous step is processed through a cross-attention mechanism that attends to the encoder's output enc_output.\n• Add & Normalize (after Cross-Attention): The output from cross-attention is added to the input of this stage, followed by dropout and normalization using norm2.\n• Feed-Forward Network: The output from the previous step is passed through the feed-forward network.\n• Add & Normalize (after Feed-Forward): The feed-forward output is added to the input of this stage, followed by dropout and normalization using norm3.\n• Output: The processed tensor is returned as the output of the decoder layer.\n\nThe DecoderLayer class defines a single layer of the transformer's decoder. It consists of a multi-head self-attention mechanism, a multi-head cross-attention mechanism (that attends to the encoder's output), a position-wise feed-forward neural network, and the corresponding residual connections, layer normalization, and dropout layers. This combination enables the decoder to generate meaningful outputs based on the encoder's representations, taking into account both the target sequence and the source sequence. As with the encoder, multiple decoder layers are typically stacked to form the complete decoder part of a transformer model.\n\nNext, the Encoder and Decoder blocks are brought together to construct the comprehensive Transformer model.\n\n5. Combining the Encoder and Decoder layers to create the complete Transformer network\n\nFigure 4. The Transformer Network (Source: Image from the original paper)\n\nThe constructor takes the following parameters:\n• d_model: The dimensionality of the model's embeddings.\n• num_heads: Number of attention heads in the multi-head attention mechanism.\n• num_layers: Number of layers for both the encoder and the decoder.\n• d_ff: Dimensionality of the inner layer in the feed-forward network.\n\nAnd it defines the following components:\n\nThis method is used to create masks for the source and target sequences, ensuring that padding tokens are ignored and that future tokens are not visible during training for the target sequence.\n\nThis method defines the forward pass for the Transformer, taking source and target sequences and producing the output predictions.\n• Input Embedding and Positional Encoding: The source and target sequences are first embedded using their respective embedding layers and then added to their positional encodings.\n• Encoder Layers: The source sequence is passed through the encoder layers, with the final encoder output representing the processed source sequence.\n• Decoder Layers: The target sequence and the encoder's output are passed through the decoder layers, resulting in the decoder's output.\n• Final Linear Layer: The decoder's output is mapped to the target vocabulary size using a fully connected (linear) layer.\n\nThe final output is a tensor representing the model's predictions for the target sequence.\n\nThe Transformer class brings together the various components of a Transformer model, including the embeddings, positional encoding, encoder layers, and decoder layers. It provides a convenient interface for training and inference, encapsulating the complexities of multi-head attention, feed-forward networks, and layer normalization.\n\nThis implementation follows the standard Transformer architecture, making it suitable for sequence-to-sequence tasks like machine translation, text summarization, etc. The inclusion of masking ensures that the model adheres to the causal dependencies within sequences, ignoring padding tokens and preventing information leakage from future tokens.\n\nThese sequential steps empower the Transformer model to efficiently process input sequences and produce corresponding output sequences.\n\nFor illustrative purposes, a dummy dataset will be crafted in this example. However, in a practical scenario, a more substantial dataset would be employed, and the process would involve text preprocessing along with the creation of vocabulary mappings for both the source and target languages.\n\nThese values define the architecture and behavior of the transformer model:\n• src_vocab_size, tgt_vocab_size: Vocabulary sizes for source and target sequences, both set to 5000.\n• d_model: Dimensionality of the model's embeddings, set to 512.\n• num_heads: Number of attention heads in the multi-head attention mechanism, set to 8.\n• num_layers: Number of layers for both the encoder and the decoder, set to 6.\n• d_ff: Dimensionality of the inner layer in the feed-forward network, set to 2048.\n\nThis line creates an instance of the Transformer class, initializing it with the given hyperparameters. The instance will have the architecture and behavior defined by these hyperparameters.\n\nThe following lines generate random source and target sequences:\n• src_data: Random integers between 1 and src_vocab_size, representing a batch of source sequences with shape (64, max_seq_length).\n• tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of target sequences with shape (64, max_seq_length).\n• These random sequences can be used as inputs to the transformer model, simulating a batch of data with 64 examples and sequences of length 100.\n\nThe code snippet demonstrates how to initialize a transformer model and generate random source and target sequences that can be fed into the model. The chosen hyperparameters determine the specific structure and properties of the transformer. This setup could be part of a larger script where the model is trained and evaluated on actual sequence-to-sequence tasks, such as machine translation or text summarization.\n\nNext, the model will be trained utilizing the aforementioned sample data. However, in a real-world scenario, a significantly larger dataset would be employed, which would typically be partitioned into distinct sets for training and validation purposes.\n• criterion = nn.CrossEntropyLoss(ignore_index=0): Defines the loss function as cross-entropy loss. The ignore_index argument is set to 0, meaning the loss will not consider targets with an index of 0 (typically reserved for padding tokens).\n• optimizer = optim.Adam(...): Defines the optimizer as Adam with a learning rate of 0.0001 and specific beta values.\n• transformer.train(): Sets the transformer model to training mode, enabling behaviors like dropout that only apply during training.\n\nThe code snippet trains the model for 100 epochs using a typical training loop:\n• for epoch in range(100): Iterates over 100 training epochs.\n• optimizer.zero_grad(): Clears the gradients from the previous iteration.\n• output = transformer(src_data, tgt_data[:, :-1]): Passes the source data and the target data (excluding the last token in each sequence) through the transformer. This is common in sequence-to-sequence tasks where the target is shifted by one token.\n• loss = criterion(...): Computes the loss between the model's predictions and the target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the cross-entropy loss function.\n• loss.backward(): Computes the gradients of the loss with respect to the model's parameters.\n• optimizer.step(): Updates the model's parameters using the computed gradients.\n• print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\"): Prints the current epoch number and the loss value for that epoch.\n\nThis code snippet trains the transformer model on randomly generated source and target sequences for 100 epochs. It uses the Adam optimizer and the cross-entropy loss function. The loss is printed for each epoch, allowing you to monitor the training progress. In a real-world scenario, you would replace the random source and target sequences with actual data from your task, such as machine translation.\n\nAfter training the model, its performance can be evaluated on a validation dataset or test dataset. The following is an example of how this could be done:\n• transformer.eval(): Puts the transformer model in evaluation mode. This is important because it turns off certain behaviors like dropout that are only used during training.\n• val_src_data: Random integers between 1 and src_vocab_size, representing a batch of validation source sequences with shape (64, max_seq_length).\n• val_tgt_data: Random integers between 1 and tgt_vocab_size, representing a batch of validation target sequences with shape (64, max_seq_length).\n• with torch.no_grad(): Disables gradient computation, as we don't need to compute gradients during validation. This can reduce memory consumption and speed up computations.\n• val_output = transformer(val_src_data, val_tgt_data[:, :-1]): Passes the validation source data and the validation target data (excluding the last token in each sequence) through the transformer.\n• val_loss = criterion(...): Computes the loss between the model's predictions and the validation target data (excluding the first token in each sequence). The loss is calculated by reshaping the data into one-dimensional tensors and using the previously defined cross-entropy loss function.\n\nThis code snippet evaluates the transformer model on a randomly generated validation dataset, computes the validation loss, and prints it. In a real-world scenario, the random validation data should be replaced with actual validation data from the task you are working on. The validation loss can give you an indication of how well your model is performing on unseen data, which is a critical measure of the model's generalization ability.\n\nFor further details about Transformers and Hugging Face, our tutorial, An Introduction to Using Transformers and Hugging Face, is useful.\n\nIn conclusion, this tutorial demonstrated how to construct a Transformer model using PyTorch, one of the most versatile tools for deep learning. With their capacity for parallelization and the ability to capture long-term dependencies in data, Transformers have immense potential in various fields, especially NLP tasks like translation, summarization, and sentiment analysis.\n\nFor those eager to deepen their understanding of advanced deep learning concepts and techniques, consider exploring the course Advanced Deep Learning with Keras on DataCamp. You can also read about building a simple neural network with PyTorch in a separate tutorial."
    },
    {
        "link": "https://medium.com/@vishal93ranjan/understanding-transformers-implementing-self-attention-in-pytorch-4256f680f0b3",
        "document": "In the rapidly advancing field of deep learning, self-attention mechanisms have revolutionized the way models process sequential data. This article delves into the concept of self-attention and provides a practical implementation using PyTorch.\n\nIn this tutorial, we will just be focusing on implementing self-attention in pytorch, if you wish to understand it in detail you can refer to the infamous blog here. We will also refer to each section from the blog as we implement them. Also, you can submit this solution to the transformer problem series here (just note that there seems to be one bug with the submission, if key is initialised before query, answers dont seem to match)\n\nSelf-attention, also known as intra-attention, is a mechanism that allows a model to weigh the importance of different words or elements in a sequence when making predictions. Unlike traditional attention mechanisms, which focus on external information, self-attention dynamically calculates the relevance of each part of the sequence with respect to itself.\n• Query (Q), Key (K), and Value (V): In self-attention, each element in the sequence is transformed into three vectors:\n• Key (K): Represents the elements to be compared against.\n• Value (V): The actual content to be aggregated based on attention scores.\n\n2. Attention Scores: The relevance of each element is determined by calculating the dot product of the query with all keys, followed by a softmax operation to obtain normalized weights.\n\n3. Weighted Sum: The final output is a weighted sum of the values, with weights determined by the attention scores.\n\nBelow is a PyTorch implementation of a single-head self-attention mechanism. Seems simple enough, now lets go through the code.\n\nRepresenting this in the image form from the blog, this is what it looks like:\n\nIn below sections, we will go through each section of code and try to explain every line of the code.\n• Linear Transformations: We define three linear layers to transform the input embeddings into queries ( ), keys ( ), and values ( ). These transformations project the input embeddings into a lower-dimensional space defined by .\n\nForward pass can be broken down into following sub points. Lets go through each one of them:\n• Compute Q, K, V: The input embeddings are transformed into queries, keys, and values using the respective linear layers.\n• Attention Scores: The attention scores are computed by taking the dot product of the queries and the transposed keys, scaled by the square root of the key dimension to maintain stable gradients.\n• Masking and Softmax: An upper triangular mask is applied to prevent attending to future positions in the sequence (useful in autoregressive models). The scores are then normalized using softmax.\n• Weighted Sum: Finally, the attention scores are used to compute a weighted sum of the values.\n\nSelf-attention is a powerful mechanism that enables models to dynamically focus on different parts of a sequence. This implementation in PyTorch showcases the core principles of self-attention, providing a foundation for building more complex models like Transformers. As deep learning continues to evolve, self-attention remains a cornerstone technique for advancing model performance across various tasks."
    },
    {
        "link": "https://deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch",
        "document": "This course clearly explains the ideas behind the attention mechanism. It walks through the algorithm itself and how to code it in Pytorch. Attention in Transformers: Concepts and Code in PyTorch, was built in collaboration with StatQuest, and taught by its Founder and CEO, Josh Starmer.\n\nThe attention mechanism was a breakthrough that led to transformers, the architecture powering large language models like ChatGPT. Transformers, introduced in the 2017 paper “Attention is All You Need” by Ashish Vaswani and others, revolutionized AI with their scalable design.\n\nLearn how this foundational architecture works to improve your intuition about building reliable, functional, and scalable AI applications.\n• None Understand the evolution of the attention mechanism, a key breakthrough that led to transformers.\n• None Learn the relationships between word embeddings, positional embeddings, and attention.\n• None Learn about the Query, Key, and Value matrices, how to produce them, and how to use them in attention.\n• None Go through the math required to calculate self-attention and masked self-attention to learn how and why the equation works the way it does.\n• None Understand the difference between self-attention and masked self-attention, and how one is used in the encoder to build context-aware embeddings and the other is used in the decoder for generative outputs.\n• None Learn the details of the encoder-decoder architecture, cross-attention, and multi-head attention, and how they are incorporated into a transformer.\n• None Use PyTorch to code a class that implements self-attention, masked self-attention, and multi-head attention."
    },
    {
        "link": "https://medium.com/@ashishbisht0307/understanding-transformers-architecture-with-pytorch-code-c422c5fb1cd2",
        "document": "\n• The Transformer architecture can be utilized as a Seq2Seq model, in translating sentences between languages.\n• At the heart of the transformer lies the attention mechanism.\n• Before we begin to understand the attention mechanism, let’s look at an overview of a SeqToSeq(sequence-to-sequence) model.\n• At a specific instance of Seq2Seq translation, most architectures perform the encoding and decoding in this way:\n• When predicting the word ‘cadeau,’ the Decoder incorporates information about the encoded sentence (present in a latent state) and the words decoded up to the preceding instance in the target language, all within a shared latent space.\n• For the Decoder to produce the next token(word), ‘cadeau’ in this case, it has to have a mechanism wherein it looks at the previous token ‘un’, which should, in turn, have info and context with respect to the previous words upto the point (‘un’, ‘homme’, ‘ouvre’, ‘un’). And also be able to store info and context with respect to the tokens encoded by the Encoder.\n• The first info and context are achieved by the self-attention mechanism, and the second info and context are achieved by the attention mechanism between Encode and Decoder.\n• Let’s begin by understanding what the attention mechanism is.\n• Consider the following sentence: ‘a man is opening a present and posing with it for a picture’. Here what does ‘it’ refer to? We as humans know that ‘it’ means ‘present’, but how do you make the model learn it?\n• To achieve this, we employ a scoring mechanism whereby we calculate the score of each word(Key) in the sentence with respect to the Query word(‘it’ in our case)\n• We can sum up all the Query*Keys scores for each word to create the context-aware Value of that word. This process is performed for every word in the sentence.\n• This is what we refer to as the attention mechanism, and the score mentioned above is calculated through the dot product interaction of the Queries with the Keys.\n• With respect to the Encoder and the source sentence, we have the Encoder self-attention mechanism in play.\n• With respect to the Decoder and the target sentence (words predicted up to the previous iteration), we have the Decoder self-attention mechanism.\n• Now that the attention mechanism and the self-attention mechanism for the Encoder and Decoder are clear, what about the Decoder-Encoder attention mechanism?\n• It functions similarly, but in the Decoder-Encoder attention mechanism, the Queries consist of words from the Decoder self-attention aware context, while the Keys encompass words from the Encoder self-attention aware context.\n• When attempting to predict the next token ‘cadeau,’ it examines the preceding word ‘un,’ which serves as the Query. Scores are then calculated against the Keys, which are the encoded self-attention-aware tokens.\n• The decoder predicts ‘cadeau’ by examining the preceding word ‘un.’ This prediction is made by utilizing the Value of the previous word (obtained by summing up the product of the attention score, Query*Keys, with the word’s inherent value).\n• Now let’s get an overview of the transformer architecture: The Encoder in the Transformer architecture starts with an input sequence, such as a sentence, and uses “self-attention” to pick up on complex word associations. ‘Positional encoding’ is applied to retain word order information and the Encoder produces transformed representations for each word. After that, the Decoder makes use of these representations to produce an output sequence, using “self-attention” as well as “encoder-decoder attention” for comprehensive understanding. The output word order is maintained by “positional encoding.” Diverse patterns are cooperatively captured by Multi-Head Attention(operates by employing multiple independent self-attention mechanisms, allowing the model to capture relationships), and information is processed for predictions by the Feedforward Neural Network. Residual Connections and Layer Normalization ensure stable training, collectively forming a powerful framework for sequence transduction.\n• Let’s go through the code walkthrough for the transformer model architecture.\n\nclass MultiHeadedAttention(nn.Module):\n\n def __init__(self, embed_size, heads):\n\n super(MultiHeadedAttention, self).__init__()\n\n self.embed_size = embed_size\n\n self.heads = heads\n\n self.head_dim = embed_size // heads\n\n self.values = nn.Linear(embed_size, embed_size)\n\n self.keys = nn.Linear(embed_size, embed_size)\n\n self.queries = nn.Linear(embed_size, embed_size)\n\n self.fc = nn.Linear(embed_size, embed_size)\n\n\n\n def forward(self, query, keys, values, mask):\n\n # Get number of training examples\n\n N = query.shape[0]\n\n value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n\n\n\n # Linear transformations for values, keys, and queries\n\n values = self.values(values) # (N, value_len, embed_size)\n\n keys = self.keys(keys) # (N, key_len, embed_size)\n\n queries = self.queries(query) # (N, query_len, embed_size)\n\n\n\n # Split the embedding into different heads pieces\n\n values = values.reshape(N, value_len, self.heads, self.head_dim) # (N, value_len, heads, embed_size)\n\n keys = keys.reshape(N, key_len, self.heads, self.head_dim) # (N, key_len, heads, embed_size)\n\n queries = queries.reshape(N, query_len, self.heads, self.head_dim) # (N, query_len, heads, embed_size)\n\n\n\n #N*H,Q,D\n\n queries_reshaped = queries.permute(0, 2, 1, 3).contiguous().view(-1, queries.size(1), queries.size(-1))\n\n #N*H,K,D\n\n keys_reshaped = keys.permute(0, 2, 1, 3).contiguous().view(-1, keys.size(1), keys.size(-1))\n\n\n\n # Compute energy (query-key interaction)\n\n #N*H,Q,K\n\n energy = torch.matmul(queries_reshaped, keys_reshaped.transpose(1, 2))\n\n #N,H,Q,K\n\n energy = energy.view(queries.size(0), queries.size(2), queries.size(1), keys.size(1))\n\n if mask is not None:\n\n energy = energy.masked_fill(mask == False, float(\"-inf\"))\n\n\n\n # Apply softmax to obtain attention weights\n\n attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n\n\n\n #N,H,V,D\n\n values_reshaped = values.permute(0, 2, 1, 3).contiguous()\n\n\n\n # Compute the weighted sum using attention weights\n\n #N,Q,H,D\n\n out = torch.matmul(attention, values_reshaped).permute(0,2,1,3).contiguous().reshape(N, query_len,self.heads * self.head_dim)\n\n\n\n #N,Q,embed_size\n\n out = self.fc(out)\n\n\n\n return out\n• The class represents a multi-headed attention mechanism used in the Transformer model for capturing dependencies between different parts of the input sequence.\n\nclass TransformerBlock(nn.Module):\n\n def __init__(self, embed_size, heads, dropout, expansion):\n\n super(TransformerBlock, self).__init__()\n\n self.attention = MultiHeadedAttention(embed_size, heads)\n\n self.norm1 = nn.LayerNorm(embed_size)\n\n self.norm2 = nn.LayerNorm(embed_size)\n\n # Feedforward network with expansion for introducing non-linearity\n\n self.feed_forward = nn.Sequential(\n\n nn.Linear(embed_size, expansion * embed_size),\n\n nn.ReLU(),\n\n nn.Linear(expansion * embed_size, embed_size),\n\n )\n\n self.dropout = nn.Dropout(dropout)\n\n\n\n def forward(self, query, key, value, mask):\n\n # Attention mechanism using MultiHeadedAttention\n\n attention = self.attention(query, key, value, mask)\n\n # Residual connection and normalization for the first stage\n\n x = self.dropout(self.norm1(attention + query))\n\n # Feedforward network\n\n forward = self.feed_forward(x)\n\n # Residual connection and normalization for the second stage\n\n out = self.dropout(self.norm2(forward + x))\n\n return out\n• This TransformerBlock represents a single block within the Transformer model, containing an attention mechanism, residual network, normalization, a feedforward network, and dropout for regularization.\n\nclass Encoder(nn.Module):\n\n def __init__( self, src_vocab_size, embed_size, num_layers,\n\n heads, forward_expansion, dropout, max_length):\n\n\n\n super(Encoder, self).__init__()\n\n self.embed_size = embed_size\n\n self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n\n self.position_embedding = nn.Embedding(max_length, embed_size)\n\n\n\n # Stacked Transformer blocks for encoding the input sequence\n\n self.layers = nn.ModuleList([\n\n TransformerBlock(embed_size, heads, dropout, forward_expansion)\n\n for _ in range(num_layers)])\n\n\n\n self.dropout = nn.Dropout(dropout)\n\n\n\n def forward(self, x, mask):\n\n\n\n N, seq_length = x.shape\n\n # Generate positional indices for the input sequence\n\n positions = torch.arange(0, seq_length).expand(N, seq_length).to(DEVICE)\n\n # Apply dropout to the sum of word and positional embeddings\n\n x = self.dropout(\n\n (self.word_embedding(x) + self.position_embedding(positions)))\n\n # Pass the input through stacked Transformer blocks\n\n for layer in self.layers:\n\n x = layer(x, x, x, mask)\n\n\n\n return x\n• This Encoder class represents the encoder component of a Transformer model, incorporating word embeddings, positional embeddings, and multiple layers of Transformer blocks for encoding the input sequence.\n\nclass DecoderBlock(nn.Module):\n\n def __init__(self, embed_size, heads, forward_expansion, dropout):\n\n super(DecoderBlock, self).__init__()\n\n self.norm = nn.LayerNorm(embed_size)\n\n self.attention = MultiHeadedAttention(embed_size, heads=heads)\n\n self.transformer_block = TransformerBlock(\n\n embed_size, heads, dropout, forward_expansion\n\n )\n\n self.dropout = nn.Dropout(dropout)\n\n\n\n def forward(self, query, value, key, src_mask, trg_mask):\n\n # Self-attention mechanism for decoding\n\n attention = self.attention(query, query, query, trg_mask)\n\n # Residual connection and normalization for the self-attention output\n\n query = self.dropout(self.norm(attention + query))\n\n # Pass through the transformer block for further processing\n\n out = self.transformer_block(query, key, value, src_mask)\n\n\n\n return out\n\nclass Decoder(nn.Module):\n\n def __init__(self, trg_vocab_size, embed_size, num_layers,\n\n heads, forward_expansion, dropout, max_length):\n\n\n\n super(Decoder, self).__init__()\n\n self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n\n self.position_embedding = nn.Embedding(max_length, embed_size)\n\n # Stacked DecoderBlocks for decoding the target sequence\n\n self.layers = nn.ModuleList(\n\n [DecoderBlock(embed_size, heads, forward_expansion, dropout)\n\n for _ in range(num_layers)])\n\n\n\n self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n\n self.dropout = nn.Dropout(dropout)\n\n\n\n def forward(self, x, enc_out, src_mask, trg_mask):\n\n\n\n N, seq_length = x.shape\n\n # Generate positional indices for the input sequenc\n\n positions = torch.arange(0, seq_length).expand(N, seq_length).to(DEVICE)\n\n x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n\n # Pass through stacked DecoderBlocks for decoding\n\n for layer in self.layers:\n\n x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n\n # Output through fully connected layer\n\n out = self.fc_out(x)\n\n return out\n• The DecoderBlock represents a block within the decoder, and the Decoder class represents the decoder component of a Transformer model.\n\nclass Transformer(nn.Module):\n\n def __init__( self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx,\n\n embed_size=512, num_layers=6, forward_expansion=4, heads=8, dropout=0.1, max_length=100):\n\n\n\n super(Transformer, self).__init__()\n\n # Instantiate the Encoder and Decoder\n\n self.encoder = Encoder( src_vocab_size, embed_size, num_layers, heads,\n\n forward_expansion, dropout, max_length)\n\n\n\n self.decoder = Decoder( trg_vocab_size, embed_size, num_layers, heads,\n\n forward_expansion, dropout, max_length)\n\n\n\n self.src_pad_idx = src_pad_idx\n\n self.trg_pad_idx = trg_pad_idx\n\n\n\n def make_src_mask(self, src):\n\n # Create a mask to ignore padding tokens in the source sequence\n\n src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n\n return src_mask.to(DEVICE)\n\n\n\n def make_trg_mask(self, trg):\n\n # Create a mask to ignore padding tokens and future tokens in the target sequence\n\n trg_mask1 = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n\n N, trg_len = trg.shape\n\n trg_mask2 = torch.tril(torch.ones((trg_len, trg_len), dtype=bool)\n\n ).expand(N, 1, trg_len,trg_len).to(DEVICE)\n\n trg_mask = trg_mask1 & trg_mask2\n\n\n\n return trg_mask\n\n\n\n\n\n def forward(self, src, trg):\n\n # Generate source and target masks\n\n src_mask = self.make_src_mask(src)\n\n trg_mask = self.make_trg_mask(trg)\n\n # Pass the source sequence through the encoder\n\n enc_src = self.encoder(src, src_mask)\n\n # Pass the target sequence and encoded source sequence through the decoder\n\n out = self.decoder(trg, enc_src, src_mask, trg_mask)\n\n return out\n\n\n\nclass ModelBuilder:\n\n def __init__(self, model_config):\n\n self.model_config = model_config\n\n def make_model(self):\n\n model = Transformer(**self.model_config).to(DEVICE)\n\n # Initialize model parameters using Xavier uniform initialization\n\n for p in model.parameters():\n\n if p.dim() > 1:\n\n nn.init.xavier_uniform_(p)\n\n return model\n• The Transformer class represents the overall Transformer model, and the ModelBuilder class is a utility class for building models with specific configurations.\n\nFor End-to-end example of a translation task from English to French you can refer the following notebook ."
    },
    {
        "link": "https://lightning.ai/docs/pytorch/stable//notebooks/course_UvA-DL/05-transformers-and-MH-attention.html",
        "document": "In this tutorial, we will discuss one of the most impactful architectures of the last 2 years: the Transformer model. Since the paper Attention Is All You Need by Vaswani et al. had been published in 2017, the Transformer architecture has continued to beat benchmarks in many domains, most importantly in Natural Language Processing. Transformers with an incredible amount of parameters can generate long, convincing essays, and opened up new application fields of AI. As the hype of the Transformer architecture seems not to come to an end in the next years, it is important to understand how it works, and have implemented it yourself, which we will do in this notebook. This notebook is part of a lecture series on Deep Learning at the University of Amsterdam. The full list of tutorials can be found at https://uvadlc-notebooks.rtfd.io.\n\nGive us a ⭐ on Github | Check out the documentation | Join us on Discord\n\nThis notebook requires some packages besides pytorch-lightning. WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. Despite the huge success of Transformers in NLP, we will not include the NLP domain in our notebook here. There are many courses at the University of Amsterdam that focus on Natural Language Processing and take a closer look at the application of the Transformer architecture in NLP (NLP2, Advanced Topics in Computational Semantics). Furthermore, and most importantly, there is so much more to the Transformer architecture. NLP is the domain the Transformer architecture has been originally proposed for and had the greatest impact on, but it also accelerated research in other domains, recently even Computer Vision. Thus, we focus here on what makes the Transformer and self-attention so powerful in general. In a second notebook, we will look at Vision Transformers, i.e. Transformers for image classification (link to notebook). Below, we import our standard libraries. inline # Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10) # Path to the folder where the pretrained models are saved # Ensure that all operations are deterministic on GPU (if used) for reproducibility Two pre-trained models are downloaded below. Make sure to have adjusted your before running this code if not already done. # Github URL where saved models are stored for this tutorial # Create checkpoint path if it doesn't exist yet # For each file, check whether it already exists. If not, try downloading it. \"Something went wrong. Please try to download the file manually,\" \" or contact the author with the full output including the following error:\n\nIn the first part of this notebook, we will implement the Transformer architecture by hand. As the architecture is so popular, there already exists a Pytorch module (documentation) and a tutorial on how to use it for next token prediction. However, we will implement it here ourselves, to get through to the smallest details. There are of course many more tutorials out there about attention and Transformers. Below, we list a few that are worth exploring if you are interested in the topic and might want yet another perspective on the topic after this one:\n• None Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n• None The Illustrated Transformer (Jay Alammar, 2018) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\n• None Illustrated: Self-Attention (Raimi Karim, 2019) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\n• None The Transformer family (Lilian Weng, 2020) - A very detailed blog post reviewing more variants of Transformers besides the original one. The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of “attention” in the literature, but the one we will use here is the following: the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys. So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to “attend” more than others. In particular, an attention mechanism has usually four parts we need to specify:\n• None Query: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n• None Keys: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is “offering”, or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n• None Values: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n• None Score function: To rate which elements we want to pay attention to, we need to specify a score function \\(f_{attn}\\). The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP. The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write: Visually, we can show the attention over a sequence of words as follows: For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights. Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used. The attention applied inside the Transformer architecture is called self-attention. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements’ keys, and returned a different, averaged value vector for each element. We will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which is in the Transformer case the scaled dot product attention. The core concept behind self-attention is the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries \\(Q\\in\\mathbb{R}^{T\\times d_k}\\), keys \\(K\\in\\mathbb{R}^{T\\times d_k}\\) and values \\(V\\in\\mathbb{R}^{T\\times d_v}\\) where \\(T\\) is the sequence length, and \\(d_k\\) and \\(d_v\\) are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element \\(i\\) to \\(j\\) is based on its similarity of the query \\(Q_i\\) and key \\(K_j\\), using the dot product as the similarity metric. In math, we calculate the dot product attention as follows: The matrix multiplication \\(QK^T\\) performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape \\(T\\times T\\). Each row represents the attention logits for a specific element \\(i\\) to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - Vaswani et al., 2017). One aspect we haven’t discussed yet is the scaling factor of \\(1/\\sqrt{d_k}\\). This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we initialize our layers with the intention of having equal variance throughout the model, and hence, \\(Q\\) and \\(K\\) might also have a variance close to \\(1\\). However, performing a dot product over two vectors with a variance \\(\\sigma\\) results in a scalar having \\(d_k\\)-times higher variance: If we do not scale down the variance back to \\(\\sigma\\), the softmax over the logits will already saturate to \\(1\\) for one random element and \\(0\\) for all others. The gradients through the softmax will be close to zero so that we can’t learn the parameters appropriately. The block in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value. After we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values: Note that our code above supports any additional dimensionality in front of the sequence length so that we can also use it for batches. However, for a better understanding, let’s generate a few random queries, keys, and value vectors, and calculate the attention outputs: Before continuing, make sure you can follow the calculation of the specific values here, and also check it by hand. It is important to fully understand how the scaled dot product attention is calculated. The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into \\(h\\) sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as: We refer to this as Multi-Head Attention layer with the learnable parameters \\(W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}\\), \\(W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}\\), \\(W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}\\), and \\(W^{O}\\in\\mathbb{R}^{h\\cdot d_k\\times d_{out}}\\) (\\(D\\) being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - Vaswani et al., 2017). How are we applying a Multi-Head Attention layer in a neural network, where we don’t have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, \\(X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}\\), as \\(Q\\), \\(K\\) and \\(V\\) (\\(B\\) being the batch size, \\(T\\) the sequence length, \\(d_{\\text{model}}\\) the hidden dimensionality of \\(X\\)). The consecutive weight matrices \\(W^{Q}\\), \\(W^{K}\\), and \\(W^{V}\\) can transform \\(X\\) to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below. \"Embedding dimension must be 0 modulo number of heads.\" # Stack all weight matrices 1...h together for efficiency # Note that in many implementations you see \"bias=False\" which is optional One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. \\(X_1\\leftrightarrow X_2\\) (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable! But what if the order of the input is actually important for solving the task, like language modeling? The answer is to encode the position in the input features, which we will take a closer look at later (topic Positional encodings below). Before moving on to creating the Transformer architecture, we can compare the self-attention operation with our other common layer competitors for sequence data: convolutions and recurrent neural networks. Below you can find a table by Vaswani et al. (2017) on the complexity per layer, the number of sequential operations, and maximum path length. The complexity is measured by the upper bound of the number of operations to perform, while the maximum path length represents the maximum number of steps a forward or backward signal has to traverse to reach any other position. The lower this length, the better gradient signals can backpropagate for long-range dependencies. Let’s take a look at the table below: \\(n\\) is the sequence length, \\(d\\) is the representation dimension and \\(k\\) is the kernel size of convolutions. In contrast to recurrent networks, the self-attention layer can parallelize all its operations making it much faster to execute for smaller sequence lengths. However, when the sequence length exceeds the hidden dimensionality, self-attention becomes more expensive than RNNs. One way of reducing the computational cost for long sequences is by restricting the self-attention to a neighborhood of inputs to attend over, denoted by \\(r\\). Nevertheless, there has been recently a lot of work on more efficient Transformer architectures that still allow long dependencies, of which you can find an overview in the paper by Tay et al. (2020) if interested. Next, we will look at how to apply the multi-head attention blog inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. While this structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding, we will focus here on the encoder part. Many advances in NLP have been made using pure encoder-based Transformer models (if interested, models include the BERT-family, the Vision Transformer, and more), and in our tutorial, we will also mainly focus on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as follows (figure credit - Vaswani et al., 2017). : The encoder consists of \\(N\\) identical blocks that are applied in sequence. Taking as input \\(x\\), it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates \\(\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))\\) (\\(x\\) being \\(Q\\), \\(K\\) and \\(V\\) input to the attention layer). The residual connection is crucial in the Transformer architecture for two reasons:\n• None Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\n• None Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position \\(i\\) has no relation to its original input. All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow. The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. We are not using Batch Normalization because it depends on the batch size which is often small with Transformers (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language as the features of words tend to have a much higher variance (there are many, very rare words which need to be considered for a good distribution estimate). Additionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear\\(\\to\\)ReLU\\(\\to\\)Linear MLP. The full transformation including the residual connection can be expressed as: This MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to “post-process” the new information added by the previous Multi-Head Attention, and prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8\\(\\times\\) larger than \\(d_{\\text{model}}\\), i.e. the dimensionality of the original input \\(x\\). The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution. Finally, after looking at all parts of the encoder architecture, we can start implementing it below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization. num_heads: Number of heads to use in the attention block dim_feedforward: Dimensionality of the hidden layer in the MLP dropout: Dropout probability to use in the dropout layers # Layers to apply in between the main layers Based on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called . The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with a grain of salt as it does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including Attention is not Explanation and Attention is not not Explanation). We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. In tasks like language understanding, however, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows: \\(PE_{(pos,i)}\\) represents the position encoding at position \\(pos\\) in the sequence, and hidden dimensionality \\(i\\). These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see “Positional encoding”), and constitute the position information. We distinguish between even (\\(i \\text{ mod } 2=0\\)) and uneven (\\(i \\text{ mod } 2=1\\)) hidden dimensionalities where we apply a sine/cosine respectively. The intuition behind this encoding is that you can represent \\(PE_{(pos+k,:)}\\) as a linear function of \\(PE_{(pos,:)}\\), which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from \\(2\\pi\\) to \\(10000\\cdot 2\\pi\\). The positional encoding is implemented below. The code is taken from the PyTorch tutorial about Transformers on NLP and adjusted for our purposes. # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs # register_buffer => Tensor which is not a parameter, but should be part of the modules state. # Used for tensors that need to be on the same device as the module. # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) To understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let’s do it below. You can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions \\(1\\), \\(2\\), \\(3\\) and \\(4\\). As we can see, the patterns between the hidden dimension \\(1\\) and \\(2\\) only differ in the starting angle. The wavelength is \\(2\\pi\\), hence the repetition after position \\(6\\). The hidden dimensions \\(2\\) and \\(3\\) have about twice the wavelength. One commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations. Thus, we slowly start learning instead of taking very large steps from the beginning. In fact, training a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing. Take for instance the following plot by Liu et al. (2019) comparing Adam-vanilla (i.e. Adam without warm-up) vs Adam with a warm-up: Clearly, the warm-up is a crucial hyperparameter in the Transformer architecture. Why is it so important? There are currently two common explanations. Firstly, Adam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like RAdam have been shown to overcome this issue, not requiring warm-up for training Transformers. Secondly, the iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using Pre-Layer Normalization (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques (Adaptive Normalization, Power Normalization). Nevertheless, many applications and papers still use the original Transformer architecture with Adam, because warm-up is a simple, yet effective way of solving the gradient problem in the first iterations. There are many different schedulers we could use. For instance, the original Transformer paper used an exponential decay scheduler with a warm-up. However, the currently most popular scheduler is the cosine warm-up scheduler, which combines warm-up with a cosine-shaped learning rate decay. We can implement it below, and visualize the learning rate factor over epochs. In the first 100 iterations, we increase the learning rate factor from 0 to 1, whereas for all later iterations, we decay it using the cosine wave. Pre-implementations of this scheduler can be found in the popular NLP Transformer library huggingface. Finally, we can embed the Transformer architecture into a PyTorch lightning module. From Tutorial 5, you know that PyTorch Lightning simplifies our training and test code, as well as structures the code nicely in separate functions. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional token to the sequence, representing the classifier token. However, here we focus on tasks where we have an output per element. Additionally to the Transformer architecture, we add a small input network (maps input dimensions to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). We also add the learning rate scheduler, which takes a step each iteration instead of once per epoch. This is needed for the warmup and the smooth cosine decay. The training, validation, and test step is left empty for now and will be filled for our task-specific models. model_dim: Hidden dimensionality to use inside the Transformer num_classes: Number of classes to predict per sequence element num_heads: Number of heads to use in the Multi-Head Attention blocks num_layers: Number of encoder blocks to use. warmup: Number of warmup steps. Usually between 50 and 500 max_iters: Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler input_dropout: Dropout to apply on the input features mask: Mask to apply on the attention outputs (optional) add_positional_encoding: If True, we add the positional encoding to the input. Might not be desired for some tasks. \"\"\"Function for extracting the attention matrices of the whole Transformer for a single batch. Input arguments same as the forward pass. # We don't return the lr scheduler because we need to apply it per iteration, not per epoch\n\nAfter having finished the implementation of the Transformer architecture, we can start experimenting and apply it to various tasks. In this notebook, we will focus on two tasks: parallel Sequence-to-Sequence, and set anomaly detection. The two tasks focus on different properties of the Transformer architecture, and we go through them below. A Sequence-to-Sequence task represents a task where the input and the output is a sequence, not necessarily of the same length. Popular tasks in this domain include machine translation and summarization. For this, we usually have a Transformer encoder for interpreting the input sequence, and a decoder for generating the output in an autoregressive manner. Here, however, we will go back to a much simpler example task and use only the encoder. Given a sequence of \\(N\\) numbers between \\(0\\) and \\(M\\), the task is to reverse the input sequence. In Numpy notation, if our input is \\(x\\), the output should be \\(x\\)[::-1]. Although this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies. Transformers are built to support such, and hence, we expect it to perform very well. We create an arbitrary number of random sequences of numbers between 0 and . The label is simply the tensor flipped over the sequence dimension. We can create the corresponding data loaders below. Let’s look at an arbitrary sample of the dataset: During training, we pass the input sequence through the Transformer encoder and predict the output for each input token. We use the standard Cross-Entropy loss to perform this. Every number is represented as a one-hot vector. Remember that representing the categories as single scalars decreases the expressiveness of the model extremely as \\(0\\) and \\(1\\) are not closer related than \\(0\\) and \\(9\\) in our example. An alternative to a one-hot vector is using a learned embedding vector as it is provided by the PyTorch module . However, using a one-hot vector with an additional linear layer as in our case has the same effect as an embedding layer ( maps one-hot vector to a dense vector, where each row of the weight matrix represents the embedding for a specific category). To implement the training dynamic, we create a new class inheriting from and overwriting the training, validation and test step functions. Finally, we can create a training function similar to the one we have seen in Tutorial 5 for PyTorch Lightning. We create a object, running for \\(N\\) epochs, logging in TensorBoard, and saving our best model based on the validation. Afterward, we test our models on the test set. An additional parameter we pass to the trainer here is . This clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like DeepAI glossary). For Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward. In plain PyTorch, you can apply gradient clipping via (see documentation). The clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients. After having explained this, let’s implement the training function: # Optional logging argument that we don't need # Check whether pretrained model exists. If yes, load it and skip training # Test best model on validation and test set Finally, we can train the model. In this setup, we will use a single encoder block and a single head in the Multi-Head Attention. This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted as an “explanation” of the predictions (compared to the other papers above dealing with deep Transformers). GPU available: True (cuda), used: True TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default /usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint saved_models/Transformers/ReverseTask.ckpt` You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision Missing logger folder: saved_models/Transformers/ReverseTask/lightning_logs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance. The warning of PyTorch Lightning regarding the number of workers can be ignored for now. As the data set is so simple and the finishes a neglectable time, we don’t need subprocesses to provide us the data (in fact, more workers can slow down the training as we have communication overhead among processes/threads). First, let’s print the results: As we would have expected, the Transformer can correctly solve the task. However, how does the attention in the Multi-Head Attention block looks like for an arbitrary input? Let’s try to visualize it below. The object is a list of length \\(N\\) where \\(N\\) is the number of layers. Each element is a tensor of shape [Batch, Heads, SeqLen, SeqLen], which we can verify below. Next, we will write a plotting function that takes as input the sequences, attention maps, and an index indicating for which batch element we want to visualize the attention map. We will create a plot where over rows, we have different layers, while over columns, we show the different heads. Remember that the softmax has been applied for each row separately. Finally, we can plot the attention map of our trained Transformer on the reverse task: The model has learned to attend to the token that is on the flipped index of itself. Hence, it actually does what we intended it to do. We see that it however also pays some attention to values close to the flipped index. This is because the model doesn’t need the perfect, hard attention to solve this problem, but is fine with this approximate, noisy attention map. The close-by indices are caused by the similarity of the positional encoding, which we also intended with the positional encoding. Besides sequences, sets are another data structure that is relevant for many applications. In contrast to sequences, elements are unordered in a set. RNNs can only be applied on sets by assuming an order in the data, which however biases the model towards a non-existing order in the data. Vinyals et al. (2015) and other papers have shown that the assumed order can have a significant impact on the model’s performance, and hence, we should try to not use RNNs on sets. Ideally, our model should be permutation-equivariant/invariant such that the output is the same no matter how we sort the elements in a set. Transformers offer the perfect architecture for this as the Multi-Head Attention is permutation-equivariant, and thus, outputs the same values no matter in what order we enter the inputs (inputs and outputs are permuted equally). The task we are looking at for sets is Set Anomaly Detection which means that we try to find the element(s) in a set that does not fit the others. In the research community, the common application of anomaly detection is performed on a set of images, where \\(N-1\\) images belong to the same category/have the same high-level features while one belongs to another category. Note that category does not necessarily have to relate to a class in a standard classification problem, but could be the combination of multiple features. For instance, on a face dataset, this could be people with glasses, male, beard, etc. An example of distinguishing different animals can be seen below. The first four images show foxes, while the last represents a different animal. We want to recognize that the last image shows a different animal, but it is not relevant which class of animal it is. In this tutorial, we will use the CIFAR100 dataset. CIFAR100 has 600 images for 100 classes each with a resolution of 32x32, similar to CIFAR10. The larger amount of classes requires the model to attend to specific features in the images instead of coarse features as in CIFAR10, therefore making the task harder. We will show the model a set of 9 images of one class, and 1 image from another class. The task is to find the image that is from a different class than the other images. Using the raw images directly as input to the Transformer is not a good idea, because it is not translation invariant as a CNN, and would need to learn to detect image features from high-dimensional input first of all. Instead, we will use a pre-trained ResNet34 model from the torchvision package to obtain high-level, low-dimensional features of the images. The ResNet model has been pre-trained on the ImageNet dataset which contains 1 million images of 1k classes and varying resolutions. However, during training and testing, the images are usually scaled to a resolution of 224x224, and hence we rescale our CIFAR images to this resolution as well. Below, we will load the dataset, and prepare the data for being processed by the ResNet model. # As torch tensors for later preprocessing # Resize to 224x224, and normalize to ImageNet statistic Extracting /__w/13/s/.datasets/cifar-100-python.tar.gz to /__w/13/s/.datasets Files already downloaded and verified Next, we want to run the pre-trained ResNet model on the images, and extract the features before the classification layer. These are the most high-level features, and should sufficiently describe the images. CIFAR100 has some similarity to ImageNet, and thus we are not retraining the ResNet model in any form. However, if you would want to get the best performance and have a very large dataset, it would be better to add the ResNet to the computation graph during training and finetune its parameters as well. As we don’t have a large enough dataset and want to train our model efficiently, we will extract the features beforehand. Let’s load and prepare the model below. # In some models, it is called \"fc\", others have \"classifier\" # Setting both to an empty sequential represents an identity map of the final features. /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead. warnings.warn( /usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to saved_models/Transformers/hub/checkpoints/resnet34-b627a593.pth 100%|██████████| 83.3M/83.3M [00:01<00:00, 48.1MB/s] We will now write a extraction function for the features below. This cell requires access to a GPU, as the model is rather deep and the images relatively large. The GPUs on GoogleColab are sufficient, but running this cell can take 2-3 minutes. Once it is run, the features are exported on disk so they don’t have to be recalculated every time you run the notebook. However, this requires >150MB free disk space. So it is recommended to run this only on a local computer if you have enough free disk and a GPU (GoogleColab is fine for this). If you do not have a GPU, you can download the features from the GoogleDrive folder. Let’s verify the feature shapes below. The training should have 50k elements, and the test 10k images. The feature dimension is 512 for the ResNet34. If you experiment with other models, you likely see a different feature dimension. As usual, we want to create a validation set to detect when we should stop training. In this case, we will split the training set into 90% training, 10% validation. However, the difficulty is here that we need to ensure that the validation set has the same number of images for all 100 labels. Otherwise, we have a class imbalance which is not good for creating the image sets. Hence, we take 10% of the images for each class, and move them into the validation set. The code below does exactly this. # Get indices of images per class # Get image indices for validation and training Now we can prepare a dataset class for the set anomaly task. We define an epoch to be the sequence in which each image has been exactly once as an “anomaly”. Hence, the length of the dataset is the number of images in it. For the training set, each time we access an item with , we sample a random, different class than the image at the corresponding index has. In a second step, we sample \\(N-1\\) images of this sampled class. The set of 10 images is finally returned. The randomness in the allows us to see a slightly different set during each iteration. However, we can’t use the same strategy for the test set as we want the test dataset to be the same every time we iterate over it. Hence, we sample the sets in the method, and return those in . The code below implements exactly this dynamic. labels: Tensor of shape [num_imgs], containing the class labels for the images set_size: Number of elements in a set. N-1 are sampled from one class, and one from another one. train: If True, a new set will be sampled every time __getitem__ is called. # The set size is here the size of correct images # Tensors with indices of the images per class # Pre-generates the sets for each image for the test set \"\"\"Samples a new set of images, given the label of the anomaly. The sampled images come from a different class than anomaly_label # Sample class from 0,...,num_classes-1 while skipping anomaly_label as class # Sample images from the class determined above # If test => use pre-generated ones # Concatenate images. The anomaly is always the last image for simplicity # We return the indices of the images for visualization purpose. \"Label\" is the index of the anomaly Next, we can setup our datasets and data loaders below. Here, we will use a set size of 10, i.e. 9 images from one category + 1 anomaly. Feel free to change it if you want to experiment with the sizes. To understand the dataset a little better, we can plot below a few sets from the test dataset. Each row shows a different input set, where the first 9 are from the same class. We can already see that for some sets the task might be easier than for others. Difficulties can especially arise if the anomaly is in a different, but yet visually similar class (e.g. train vs bus, flour vs worm, etc. ). After having prepared the data, we can look closer at the model. Here, we have a classification of the whole set. For the prediction to be permutation-equivariant, we will output one logit for each image. Over these logits, we apply a softmax and train the anomaly image to have the highest score/probability. This is a bit different than a standard classification layer as the softmax is applied over images, not over output classes in the classical sense. However, if we swap two images in their position, we effectively swap their position in the output softmax. Hence, the prediction is equivariant with respect to the input. We implement this idea below in the subclass of the Transformer Lightning module. # No positional encodings as it is a set, not a sequence! Finally, we write our train function below. It has the exact same structure as the reverse task one, hence not much of an explanation is needed here. # Optional logging argument that we don't need # Check whether pretrained model exists. If yes, load it and skip training # Test best model on validation and test set Let’s finally train our model. We will use 4 layers with 4 attention heads each. The hidden dimensionality of the model is 256, and we use a dropout of 0.1 throughout the model for good regularization. Note that we also apply the dropout on the input features, as this makes the model more robust against image noise and generalizes better. Again, we use warmup to slowly start our model training. GPU available: True (cuda), used: True TPU available: False, using: 0 TPU cores HPU available: False, using: 0 HPUs /usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature. Lightning automatically upgraded your loaded checkpoint from v1.0.2 to v2.3.3. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint saved_models/Transformers/SetAnomalyTask.ckpt` Missing logger folder: saved_models/Transformers/SetAnomalyTask/lightning_logs LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders. We can print the achieved accuracy below. With ~94% validation and test accuracy, the model generalizes quite well. It should be noted that you might see slightly different scores depending on what computer/device you are running this notebook. This is because despite setting the seed before generating the test dataset, it is not the same across platforms and numpy versions. Nevertheless, we can conclude that the model performs quite well and can solve the task for most sets. Before trying to interpret the model, let’s verify that our model is permutation-equivariant, and assigns the same predictions for different permutations of the input set. For this, we sample a batch from the test set and run it through the model to obtain the probabilities. You can see that the predictions are almost exactly the same, and only differ because of slight numerical differences inside the network operation. To interpret the model a little more, we can plot the attention maps inside the model. This will give us an idea of what information the model is sharing/communicating between images, and what each head might represent. First, we need to extract the attention maps for the test batch above, and determine the discrete predictions for simplicity. Below we write a plot function which plots the images in the input set, the prediction of the model, and the attention maps of the different heads on layers of the transformer. Feel free to explore the attention maps for different input examples as well. Depending on the random seed, you might see a slightly different input set. For the version on the website, we compare 9 tree images with a volcano. We see that multiple heads, for instance, Layer 2 Head 1, Layer 2 Head 3, and Layer 3 Head 1 focus on the last image. Additionally, the heads in Layer 4 all seem to ignore the last image and assign a very low attention probability to it. This shows that the model has indeed recognized that the image doesn’t fit the setting, and hence predicted it to be the anomaly. Layer 3 Head 2-4 seems to take a slightly weighted average of all images. That might indicate that the model extracts the “average” information of all images, to compare it to the image features itself. Let’s try to find where the model actually makes a mistake. We can do this by identifying the sets where the model predicts something else than 9, as in the dataset, we ensured that the anomaly is always at the last position in the set. As our model achieves ~94% accuracy, we only have very little number of mistakes in a batch of 64 sets. Still, let’s visualize one of them, for example the last one: In this example, the model confuses a palm tree with a building, giving a probability of ~90% to image 2, and 8% to the actual anomaly. However, the difficulty here is that the picture of the building has been taken at a similar angle as the palms. Meanwhile, image 2 shows a rather unusual palm with a different color palette, which is why the model fails here. Nevertheless, in general, the model performs quite well."
    }
]