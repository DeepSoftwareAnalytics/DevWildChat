[
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-writing-shaders-9",
        "document": "When in operation, a programmable vertex shader replaces the vertex processing done by the Microsoft Direct3D graphics pipeline. While using a vertex shader, state information regarding transformation and lighting operations is ignored by the fixed function pipeline. When the vertex shader is disabled and fixed function processing is returned, all current state settings apply.\n\nTessellation of high-order primitives should be done before the vertex shader executes. Implementations that perform surface tessellation after the shader processing must do so in a way that is not apparent to the application and shader code.\n\nAs a minimum, a vertex shader must output vertex position in homogeneous clip space. Optionally, the vertex shader can output texture coordinates, vertex color, vertex lighting, fog factors, and so on.\n\nPixel processing is performed by pixel shaders on individual pixels. Pixel shaders work in concert with vertex shaders; the output of a vertex shader provides the inputs for a pixel shader. Other pixel operations (fog blending, stencil operations, and render-target blending) occur after execution of the shader.\n\nA pixel shader completely replaces the pixel-blending functionality specified by the multi-texture blender including operations previously defined by the texture stage states. Texture sampling and filtering operations which were controlled by the standard texture stage states for minification, magnification, mip filtering, and the wrap addressing modes, can be initialized in shaders. The application is free to change these states without requiring the regeneration of the currently bound shader. Setting state can be made even easier if your shaders are designed within an effect.\n\nFor pixel shader versions ps_1_1 - ps_2_0, diffuse and specular colors are saturated (clamped) in the range 0 to 1 before use by the shader.\n\nColor values input to the pixel shader are assumed to be perspective correct, but this is not guaranteed (for all hardware). Colors sampled from texture coordinates are iterated in a perspective correct manner, and are clamped to the 0 to 1 range during iteration.\n\nFor pixel shader versions ps_1_1 - ps_1_4, the result emitted by the pixel shader is the contents of register r0. Whatever it contains when the shader completes processing is sent to the fog stage and render-target blender.\n\nFor pixel shader versions ps_2_0 and above, output color is emitted from oC0 - oC4.\n\nThe simplest variable declaration includes a type and a variable name, such as this floating-point declaration:\n\nYou can initialize a variable in the same statement.\n\nAn array of variables can be declared,\n\nor declared and initialized in the same statement.\n\nHere are a few declarations that demonstrate many of the characteristics of high-level shader language (HLSL) variables:\n\nData declarations can use any valid type including:\n\nA shader can have top-level variables, arguments, and functions.\n\nTop-level variables are declared outside of all functions. Top-level arguments are parameters to a top-level function. A top-level function is any function called by the application (as opposed to a function that is called by another function).\n\nVertex and pixel shaders accept two kinds of input data: varying and uniform. The varying input is the data that is unique to each execution of the shader. For a vertex shader, the varying data (for example: position, normal, etc.) comes from the vertex streams. The uniform data (for example: material color, world transform, etc.) is constant for multiple executions of a shader. For those familiar with the assembly shader models, uniform data is specified by constant registers and varying data by the v and t registers.\n\nUniform data can be specified by two methods. The most common method is to declare global variables and use them within a shader. Any use of global variables within a shader will result in adding that variable to the list of uniform variables required by that shader. The second method is to mark an input parameter of the top-level shader function as uniform. This marking specifies that the given variable should be added to the list of uniform variables.\n\nUniform variables used by a shader are communicated back to the application via the constant table. The constant table is the name for the symbol table that defines how the uniform variables used by a shader fit into the constant registers. The uniform function parameters appear in the constant table prepended with a dollar sign ($), unlike the global variables. The dollar sign is required to avoid name collisions between local uniform inputs and global variables of the same name.\n\nThe constant table contains the constant register locations of all uniform variables used by the shader. The table also includes the type information and the default value, if specified.\n\nVarying input parameters (of a top-level shader function) must be marked either with a semantic or uniform keyword indicating the value is constant for the execution of the shader. If a top-level shader input is not marked with a semantic or uniform keyword, then the shader will fail to compile.\n\nThe input semantic is a name used to link the given input to an output of the previous part of the graphics pipeline. For example, the input semantic POSITION0 is used by the vertex shaders to specify where the position data from the vertex buffer should be linked.\n\nPixel and vertex shaders have different sets of input semantics due to the different parts of the graphics pipeline that feed into each shader unit. Vertex shader input semantics describe the per-vertex information (for example: position, normal, texture coordinates, color, tangent, binormal, etc.) to be loaded from a vertex buffer into a form that can be consumed by the vertex shader. The input semantics directly map to the vertex declaration usage and the usage index.\n\nPixel shader input semantics describe the information that is provided per pixel by the rasterization unit. The data is generated by interpolating between outputs of the vertex shader for each vertex of the current primitive. The basic pixel shader input semantics link the output color and texture coordinate information to input parameters.\n\nInput semantics can be assigned to shader input by two methods:\n• Appending a colon and the semantic name to the parameter declaration.\n• Defining an input structure with input semantics assigned to each structure member.\n\nVertex and pixel shaders provide output data to the subsequent graphics pipeline stage. Output semantics are used to specify how data generated by the shader should be linked to the inputs of the next stage. For example, the output semantics for a vertex shader are used to link the outputs of the interpolators in the rasterizer to generate the input data for the pixel shader. The pixel shader outputs are the values provided to the alpha blending unit for each of the render targets or the depth value written to the depth buffer.\n\nVertex shader output semantics are used to link the shader both to the pixel shader and to the rasterizer stage. A vertex shader that is consumed by the rasterizer and not exposed to the pixel shader must generate position data as a minimum. Vertex shaders that generate texture coordinate and color data provide that data to a pixel shader after interpolation is done.\n\nPixel shader output semantics bind the output colors of a pixel shader with the correct render target. The pixel shader output color is linked to the alpha blend stage, which determines how the destination render targets are modified. The pixel shader depth output can be used to change the destination depth values at the current raster location. The depth output and multiple render targets are only supported with some shader models.\n\nThe syntax for output semantics is identical to the syntax for specifying input semantics. The semantics can be either specified directly on parameters declared as \"out\" parameters or assigned during the definition of a structure that either returned as an \"out\" parameter or the return value of a function.\n\nSemantics identify where data comes from. Semantics are optional identifiers that identify shader inputs and outputs. Semantics appear in one of three places:\n• After an argument in a function's input argument list.\n\nThis example uses a structure to provide one or more vertex shader inputs, and another structure to provide one or more vertex shader outputs. Each of the structure members uses a semantic.\n\nThe input structure identifies the data from the vertex buffer that will provide the shader inputs. This shader maps the data from the position, normal, and blendweight elements of the vertex buffer into vertex shader registers. The input data type does not have to exactly match the vertex declaration data type. If it doesn't exactly match, the vertex data will automatically be converted into the HLSL's data type when it is written into the shader registers. For instance, if the normal data were defined to be of type UINT by the application, it would be converted into a float3 when read by the shader.\n\nIf the data in the vertex stream contains fewer components than the corresponding shader data type, the missing components will be initialized to 0 (except for w, which is initialized to 1).\n\nInput semantics are similar to the values in the D3DDECLUSAGE.\n\nThe output structure identifies the vertex shader output parameters of position and color. These outputs will be used by the pipeline for triangle rasterization (in primitive processing). The output marked as position data denotes the position of a vertex in homogeneous space. As a minimum, a vertex shader must generate position data. The screen space position is computed after the vertex shader completes by dividing the (x, y, z) coordinate by w. In screen space, -1 and 1 are the minimum and maximum x and y values of the boundaries of the viewport, while z is used for z-buffer testing.\n\nOutput semantics are also similar to the values in D3DDECLUSAGE. In general, an output structure for a vertex shader can also be used as the input structure for a pixel shader, provided the pixel shader does not read from any variable marked with the position, point size, or fog semantics. These semantics are associated with per-vertex scalar values that are not used by a pixel shader. If these values are needed for the pixel shader, they can be copied into another output variable that uses a pixel shader semantic.\n\nGlobal variables are assigned to registers automatically by the compiler. Global variables are also called uniform parameters because the contents of the variable is the same for all pixels processed each time the shader is called. The registers are contained in the constant table, which can be read using the ID3DXConstantTable interface.\n\nInput semantics for pixel shaders map values into specific hardware registers for transport between vertex shaders and pixel shaders. Each register type has specific properties. Because there are currently only two semantics for color and texture coordinates, it is common for most data to be marked as a texture coordinate even when it is not.\n\nNotice that the vertex shader output structure used an input with position data, which is not used by the pixel shader. HLSL allows valid output data of a vertex shader that is not valid input data for a pixel shader, provided that it is not referenced in the pixel shader.\n\nInput arguments can also be arrays. Semantics are automatically incremented by the compiler for each element of the array. For instance, consider the following explicit declaration:\n\nThe explicit declaration given above is equivalent to the following declaration that will have semantics automatically incremented by the compiler:\n\nJust like input semantics, output semantics identify data usage for pixel shader output data. Many pixel shaders write to only one output color. Pixel shaders can also write out a depth value into one or more multiple render targets at the same time (up to four). Like vertex shaders, pixel shaders use a structure to return more than one output. This shader writes 0 to the color components, as well as to the depth component.\n\nPixel shader output colors must be of type float4. When writing multiple colors, all output colors must be used contiguously. In other words, COLOR1 cannot be an output unless COLOR0 has already been written. Pixel shader depth output must be of type float1.\n\nA sampler contains sampler state. Sampler state specifies the texture to be sampled, and controls the filtering that is done during sampling. Three things are required to sample a texture:\n\nSamplers can be initialized with textures and sampler state as shown here:\n\nHere's an example of the code to sample a 2D texture:\n\nThe texture is declared with a texture variable tex0.\n\nIn this example, a sampler variable named s_2D is declared. The sampler contains the sampler state inside of curly braces. This includes the texture that will be sampled and, optionally, the filter state (that is, wrap modes, filter modes, etc.). If the sampler state is omitted, a default sampler state is applied specifying linear filtering and a wrap mode for the texture coordinates. The sampler function takes a two-component floating-point texture coordinate, and returns a two-component color. This is represented with the float2 return type and represents data in the red and green components.\n\nFour types of samplers are defined (see Keywords) and texture lookups are performed by the intrinsic functions: tex1D(s, t) (DirectX HLSL), tex2D(s, t) (DirectX HLSL), tex3D(s, t) (DirectX HLSL), texCUBE(s, t) (DirectX HLSL). Here is an example of 3D sampling:\n\nThis sampler declaration uses default sampler state for the filter settings and address mode.\n\nHere is the corresponding cube sampling example:\n\nAnd finally, here is the 1D sampling example:\n\nBecause the runtime does not support 1D textures, the compiler will use a 2D texture with the knowledge that the y-coordinate is unimportant. Since tex1D(s, t) (DirectX HLSL) is implemented as a 2D texture lookup, the compiler is free to choose the y-component in an efficient manner. In some rare scenarios, the compiler cannot choose an efficient y-component, in which case it will issue a warning.\n\nThis particular example is inefficient because the compiler must move the input coordinate into another register (because a 1D lookup is implemented as a 2D lookup and the texture coordinate is declared as a float1). If the code is rewritten using a float2 input instead of a float1, the compiler can use the input texture coordinate because it knows that y is initialized to something.\n\nAll texture lookups can be appended with \"bias\" or \"proj\" (that is, tex2Dbias (DirectX HLSL), texCUBEproj (DirectX HLSL)). With the \"proj\" suffix, the texture coordinate is divided by the w-component. With \"bias,\" the mip level is shifted by the w-component. Thus, all texture lookups with a suffix always take a float4 input. tex1D(s, t) (DirectX HLSL) and tex2D(s, t) (DirectX HLSL) ignore the yz- and z-components respectively.\n\nSamplers may also be used in array, although no back end currently supports dynamic array access of samplers. Therefore, the following is valid because it can be resolved at compile time:\n\nHowever, this example is not valid.\n\nDynamic access of samplers is primarily useful for writing programs with literal loops. The following code illustrates sampler array accessing:\n\nFunctions break large tasks into smaller ones. Small tasks are easier to debug and can be reused, once proven. Functions can be used to hide details of other functions, which makes a program composed of functions easier to follow.\n\nHLSL functions are similar to C functions in several ways: They both contain a definition and a function body and they both declare return types and argument lists. Like C functions, HLSL validation does type checking on the arguments, argument types, and the return value during shader compilation.\n\nUnlike C functions, HLSL entry point functions use semantics to bind function arguments to shader inputs and outputs (HLSL functions called internally ignore semantics). This makes it easier to bind buffer data to a shader, and bind shader outputs to shader inputs.\n\nA function contains a declaration and a body, and the declaration must precede the body.\n\nThe function declaration includes everything in front of the curly braces:\n\nThe return type can be any of the HLSL basic data types such as a float4:\n\nThe return type can be a structure that has already been defined:\n\nIf the function does not return a value, void can be used as the return type.\n\nThe return type always appears first in a function declaration.\n\nAn argument list declares the input arguments to a function. It may also declare values that will be returned. Some arguments are both input and output arguments. Here is an example of a shader that takes four input arguments.\n\nThis function returns a final color, that is a blend of a texture sample and the light color. The function takes four inputs. Two inputs have semantics: LightDir has the TEXCOORD1 semantic, and texcrd has the TEXCOORD0 semantic. The semantics mean that the data for these variables will come from the vertex buffer. Even though the LightDir variable has a TEXCOORD1 semantic, the parameter is probably not a texture coordinate. The TEXCOORDn semantic type is often used to supply a semantic for a type that is not predefined (there is no vertex shader input semantic for a light direction).\n\nThe other two inputs LightColor and samp are labeled with the uniform keyword. These are uniform constants that will not change between draw calls. The values for these parameters come from shader global variables.\n\nArguments can be labeled as inputs with the in keyword, and output arguments with the out keyword. Arguments cannot be passed by reference; however, an argument can be both an input and an output if it is declared with the inout keyword. Arguments passed to a function that are marked with the inout keyword are considered copies of the original until the function returns, and they are copied back. Here's an example using inout:\n\nThis function increments the values in A and B and returns them.\n\nThe function body is all of the code after the function declaration.\n\nThe body consists of statements which are surrounded by curly braces. The function body implements all of the functionality using variables, literals, expressions, and statements.\n\nThe shader body does two things: it performs a matrix multiply and returns a float4 result. The matrix multiply is accomplished with the mul (DirectX HLSL) function, which performs a 4x4 matrix multiply. mul (DirectX HLSL) is called an intrinsic function because it is already built into the HLSL library of functions. Intrinsic functions will be covered in more detail in the next section.\n\nThe matrix multiply combines an input vector Pos and a composite matrix WorldViewProj. The result is position data transformed into screen space. This is the minimum vertex shader processing we can do. If we were using the fixed function pipeline instead of a vertex shader, the vertex data could be drawn after doing this transform.\n\nThe last statement in a function body is a return statement. Just like C, this statement returns control from the function to the statement that called the function.\n\nFunction return types can be any of the simple data types defined in HLSL, including bool, int half, float, and double. Return types can be one of the complex data types such as vectors and matrices. HLSL types that refer to objects cannot be used as return types. This includes pixelshader, vertexshader, texture, and sampler.\n\nHere is an example of a function that uses a structure for a return type.\n\nThe float4 return type has been replaced with the structure VS_OUTPUT, which now contains a single float4 member.\n\nA return statement signals the end of a function. This is the simplest return statement. It returns control from the function to the calling program. It returns no value.\n\nA return statement can return one or more values. This example returns a literal value:\n\nThis example returns the scalar result of an expression:\n\nThis example returns a float4 constructed from a local variable and a literal:\n\nThis example returns a float4 that is constructed from the result returned from an intrinsic function, and a few literal values:\n\nThis example returns a structure that contains one or more members:\n\nMost current vertex and pixel shader hardware is designed to run a shader line by line, executing each instruction once. HLSL supports flow control, which includes static branching, predicated instructions, static looping, dynamic branching, and dynamic looping.\n\nPreviously, using an if statement resulted in assembly-language shader code that implements both the if side and the else side of the code flow. Here is an example of the in HLSL code that was compiled for vs_1_1:\n\nAnd here is the resulting assembly code:\n\nSome hardware allows for either static or dynamic looping, but most require linear execution. On the models that do not support looping, all loops must be unrolled. An example is the DepthOfField Sample sample that uses unrolled loops even for ps_1_1 shaders.\n\nHLSL now includes support for each of these types of flow control:\n\nStatic branching allows blocks of shader code to be switched on or off based on a Boolean shader constant. This is a convenient method for enabling or disabling code paths based on the type of object currently being rendered. Between draw calls, you can decide which features you want to support with the current shader and then set the Boolean flags required to get that behavior. Any statements that are disabled by a Boolean constant are skipped during shader execution.\n\nThe most familiar branching support is dynamic branching. With dynamic branching, the comparison condition resides in a variable, which means that the comparison is done for each vertex or each pixel at run time (as opposed to the comparison occurring at compile time, or between two draw calls). The performance hit is the cost of the branch plus the cost of the instructions on the side of the branch taken. Dynamic branching is implemented in shader model 3 or higher. Optimizing shaders that work with these models is similar to optimizing code that runs on a CPU."
    },
    {
        "link": "https://stackoverflow.com/questions/14427627/how-to-manually-or-automatically-optimize-hlsl-pixel-shader-code",
        "document": "What are successful strategies to optimize HLSL shader code in terms of computational complexity (meaning: minimizing runtime of the shader)?\n\nI guess one way would be to minimize the number of arithmetic operations that result from compiling the shader.\n\nHow could this be done a) manually and b) using automated tools (if existing) ?\n• Avoid branching (But how to do that best?)\n• Whenever possible: precompute outside shader and pass as argument.\n\nAn example code would be:"
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dgetstarted/work-with-shaders-and-shader-resources",
        "document": "It's time to learn how to work with shaders and shader resources in developing your Microsoft DirectX game for Windows 8. We've seen how to set up the graphics device and resources, and perhaps you've even started modifying its pipeline. So now let's look at pixel and vertex shaders.\n\nIf you aren't familiar with shader languages, a quick discussion is in order. Shaders are small, low-level programs that are compiled and run at specific stages in the graphics pipeline. Their specialty is very fast floating-point mathematical operations. The most common shader programs are:\n• Vertex shader—Executed for each vertex in a scene. This shader operates on vertex buffer elements provided to it by the calling app, and minimally results in a 4-component position vector that will be rasterized into a pixel position.\n• Pixel shader—Executed for each pixel in a render target. This shader receives rasterized coordinates from previous shader stages (in the simplest pipelines, this would be the vertex shader) and returns a color (or other 4-component value) for that pixel position, which is then written into a render target.\n\nThis example includes very basic vertex and pixel shaders that only draw geometry, and more complex shaders that add basic lighting calculations.\n\nShader programs are written in Microsoft High Level Shader Language (HLSL). HLSL syntax looks a lot like C, but without the pointers. Shader programs must be very compact and efficient. If your shader compiles to too many instructions, it cannot be run and an error is returned. (Note that the exact number of instructions allowed is part of the Direct3D feature level.)\n\nIn Direct3D, shaders are not compiled at run time; they are compiled when the rest of the program is compiled. When you compile your app with Microsoft Visual Studio 2013, the HLSL files are compiled to CSO (.cso) files that your app must load and place in GPU memory prior to drawing. Make sure you include these CSO files with your app when you package it; they are assets just like meshes and textures.\n\nIt's important to take a moment to discuss HLSL semantics before we continue, because they are often a point of confusion for new Direct3D developers. HLSL semantics are strings that identify a value passed between the app and a shader program. Although they can be any of a variety of possible strings, the best practice is to use a string like or that indicates the usage. You assign these semantics when you are constructing a constant buffer or input layout. You can also append a number between 0 and 7 to the semantic so that you use separate registers for similar values. For example: COLOR0, COLOR1, COLOR2...\n\nSemantics that are prefixed with \"SV_\" are system value semantics that are written to by your shader program; your game itself (running on the CPU) cannot modify them. Typically, these semantics contain values that are inputs or outputs from another shader stage in the graphics pipeline, or that are generated entirely by the GPU.\n\nAdditionally, semantics have different behaviors when they are used to specify input to or output from a shader stage. For example, (output) contains the vertex data transformed during the vertex shader stage, and (input) contains the pixel position values that were interpolated by the GPU during the rasterization stage.\n\nHere are a few common HLSL semantics:\n• (n) for vertex buffer data. provides a pixel position to the pixel shader and cannot be written by your game.\n• (n) for normal data provided by the vertex buffer.\n• (n) for RGBA color data supplied to a shader. Note that it is treated identically to coordinate data, including interpolating the value during rasterization; the semantic simply helps you identify that it is color data.\n• [n] for writing from a pixel shader to a target texture or other pixel buffer.\n\nWe'll see some examples of HLSL semantics as we review the example.\n\nAny shader can read from a constant buffer if that buffer is attached to its stage as a resource. In this example, only the vertex shader is assigned a constant buffer.\n\nThe constant buffer is declared in two places: in the C++ code, and in the corresponding HLSL files that will access it.\n\nHere's how the constant buffer struct is declared in the C++ code.\n\nWhen declaring the structure for the constant buffer in your C++ code, ensure that all of the data is correctly aligned along 16-byte boundaries. The easiest way to do this is to use DirectXMath types, like XMFLOAT4 or XMFLOAT4X4, as seen in the example code. You can also guard against misaligned buffers by declaring a static assert:\n\nThis line of code will cause an error at compile time if ConstantBufferStruct is not 16-byte aligned. For more information about constant buffer alignment and packing, see Packing Rules for Constant Variables.\n\nNow, here's how the constant buffer is declared in the vertex shader HLSL.\n\nAll buffers—constant, texture, sampler, or other—must have a register defined so the GPU can access them. Each shader stage allows up to 15 constant buffers, and each buffer can hold up to 4,096 constant variables. The register-usage declaration syntax is as follows:\n• s*#*: A register for a sampler. (A sampler defines the lookup behavior for texels in the texture resource.)\n\nFor example, the HLSL for a pixel shader might take a texture and a sampler as input with a declaration like this.\n\nIt's up to you to assign constant buffers to registers—when you set up the pipeline, you attach a constant buffer to the same slot you assigned it to in the HLSL file. For example, in the previous topic the call to VSSetConstantBuffers indicates '0' for the first parameter. That tells Direct3D to attach the constant buffer resource to register 0, which matches the buffer's assignment to register(b0) in the HLSL file.\n\nThe vertex buffer supplies the triangle data for the scene objects to the vertex shader(s). As with the constant buffer, the vertex buffer struct is declared in the C++ code, using similar packing rules.\n\nThere is no standard format for vertex data in Direct3D 11. Instead, we define our own vertex data layout using a descriptor; the data fields are defined using an array of D3D11_INPUT_ELEMENT_DESC structures. Here, we show a simple input layout that describes the same vertex format as the preceding struct:\n\nIf you add data to the vertex format when modifying the example code, be sure to update the input layout as well, or the shader will not be able to interpret it. You might modify the vertex layout like this:\n\nIn that case, you'd modify the input-layout definition as follows.\n\nEach of the input-layout element definitions is prefixed with a string, like \"POSITION\" or \"NORMAL\"—that is the semantic we discussed earlier in this topic. It's like a handle that helps the GPU identify that element when processing the vertex. Choose common, meaningful names for your vertex elements.\n\nJust as with the constant buffer, the vertex shader has a corresponding buffer definition for incoming vertex elements. (That's why we provided a reference to the vertex shader resource when creating the input layout - Direct3D validates the per-vertex data layout with the shader's input struct.) Note how the semantics match between the input layout definition and this HLSL buffer declaration. However, has a \"0\" appended to it. It isn't necessary to add the 0 if you have only one element declared in the layout, but it's a good practice to append it in case you choose to add more color elements in the future.\n\nShaders take input types and return output types from their main functions upon execution. For the vertex shader defined in the previous section, the input type was the VS_INPUT structure, and we defined a matching input layout and C++ struct. An array of this struct is used to create a vertex buffer in the CreateCube method.\n\nThe vertex shader returns a PS_INPUT structure, which must minimally contain the 4-component (float4) final vertex position. This position value must have the system value semantic, , declared for it so the GPU has the data it needs to perform the next drawing step. Notice that there is not a 1:1 correspondence between vertex shader output and pixel shader input; the vertex shader returns one structure for each vertex it is given, but the pixel shader runs once for each pixel. That's because the per-vertex data first passes through the rasterization stage. This stage decides which pixels \"cover\" the geometry you're drawing, computes interpolated per-vertex data for each pixel, and then calls the pixel shader once for each of those pixels. Interpolation is the default behavior when rasterizing output values, and is essential in particular for the correct processing of output vector data (light vectors, per-vertex normals and tangents, and others).\n\nThe example vertex shader is very simple: take in a vertex (position and color), transform the position from model coordinates into perspective projected coordinates, and return it (along with the color) to the rasterizer. Notice that the color value is interpolated right along with the position data, providing a different value for each pixel even though the vertex shader didn't perform any calculations on the color value.\n\nA more complex vertex shader, such as one that sets up an object's vertices for Phong shading, might look more like this. In this case, we're taking advantage of the fact that the vectors and normals are interpolated to approximate a smooth-looking surface.\n\nThis pixel shader in this example is quite possibly the absolute minimum amount of code you can have in a pixel shader. It takes the interpolated pixel color data generated during rasterization and returns it as output, where it will be written to a render target. How boring!\n\nThe important part is the system-value semantic on the return value. It indicates that the output is to be written to the primary render target, which is the texture buffer supplied to the swap chain for display. This is required for pixel shaders - without the color data from the pixel shader, Direct3D wouldn't have anything to display!\n\nAn example of a more complex pixel shader to perform Phong shading might look like this. Since the vectors and normals were interpolated, we don't have to compute them on a per-pixel basis. However, we do have to re-normalize them because of how interpolation works; conceptually, we need to gradually \"spin\" the vector from direction at vertex A to direction at vertex B, maintaining its length—wheras interpolation instead cuts across a straight line between the two vector endpoints.\n\nIn another example, the pixel shader takes its own constant buffers that contain light and material information. The input layout in the vertex shader would be expanded to include normal data, and the output from that vertex shader is expected to include transformed vectors for the vertex, the light, and the vertex normal in the view coordinate system.\n\nIf you have texture buffers and samplers with assigned registers (t and s, respectively), you can access them in the pixel shader also.\n\nShaders are very powerful tools that can be used to generate procedural resources like shadow maps or noise textures. In fact, advanced techniques require that you think of textures more abstractly, not as visual elements but as buffers. They hold data like height information, or other data that can be sampled in the final pixel shader pass or in that particular frame as part of a multi-stage effects pass. Multi-sampling is a powerful tool and the backbone of many modern visual effects.\n\nHopefully, you're comfortable with DirectX 11at this point and are ready to start working on your project. Here are some links to help answer other questions you may have about development with DirectX and C++:"
    },
    {
        "link": "https://stackoverflow.com/questions/10804783/hlsl-coordinates-system-orientation",
        "document": "In the vertex shader you can work with several coordinate systems or spaces...\n\nusually your vertex data arrive at world coordinates...\n\nthen vertex data is transformed by WordViewProjection matrix, that tranform from world coordinates to homogeneus projected coordinates, this is the value returned in the vertex shader...\n\nIf the vertex is inside the screen, if you divide the x and y coordinates by the w component, you will get a point in range(-1..1,-1..1) ... I'm not sure but I think (-1,-1) is upper left... you only have to test it\n\nyou can send to the graphics card two triangles representing a quad with the screen corners in this space (-1,-1,0), (1,-1,0), (1,1,0), (-1,1,0), this way you don't need to transform vertex data by any matrix.\n\nYou have not explained what you want to do.. but I suppose you want to do a post process...\n\nIf is the case... you can work in the pixel shader with texture coordinates... this way the upper left point will be (0,0) and the right bottom point will be (1,1)\n\nHere you can find code and an extended explanation: http://ploobs.com.br/?p=1092"
    },
    {
        "link": "http://code4k.blogspot.com/2011/11/advanced-hlsl-using-closures-and.html",
        "document": "A simple problem of abstraction and code reuse in HLSL\n• An interface cannot inherit from another interface (that would be really interesting)\n• An interface can only have method members.\n• A class can inherit from another class and from several interfaces.\n• Unlike in C/C++, we cannot pre-declare an interface, but we can use a declaration being declared (See the example of the method INoise::Next, returning a INoise).\n• The compiler has a limitation against the reuse of an implementation in a call chain and will complain about a recursive call (even if there is no recursive call at all): For example, It is not possible to reuse twice the sample type of class closure in a call chain, meaning that it is not possible to make a call chain like this one: Marble => FBM => Marble => Abs => Perlin. The fxc compiler would complain about the second \"Marble\" as It would see it as a kind of recursive call. In order to reuse a function, we need to duplicate it, that's probably the only point that is annoying here.\n• Generated compiled asm output from closures are exactly the same as using standard inlining methods.\n• Before going to local class-closure, I have tried several techniques that were sometimes crashing fxc compiler.\n• Thus, as it is a way of hacking the usage HLSL, It is not guarantee that this will be supported in the future. But at least, if it is working for SM5.0, SM4.0 and 3.0, we can expect that we are safe for a while!\n• Also, the compilation time under vs_3_0/ps_3_0 profile seems to take more time, not sure if its the language construction or a regular behavior of 3.0 profiles.\n\nShader languages like HLSL, Cg or GLSL are nowadays driving the most powerful processors in the world, but if you are developing with them, you may have been already a little bit frustrated by one of their expressiveness limitations: the common problem of abstraction and code reuse. In order to overcome this problem, solutions so far were mostly using a glue combination of #define/#include preprocessors directives in order to generate combinations of code, permutation of shaders, so called UberShaders. Recently, this problem has been addressed, for HLSL (new in Direct3D11), by providing the concept of Dynamic Linking , and for GLSL, the concept of SubRoutines , For Direct3D11, the new mechanism has been only available for Shader Model 5.0, meaning that even if this could greatly simplified the problem of abstraction, It is unfortunately only available for Direct3D11 class graphics card, which is of course a huge limitation...But, here is the good news: While the classic usage of dynamic linking is not really possible from earlier version (like SM4.0 or SM3.0), I have found(!). This solution doesn't involve any kind of preprocessing directive and is able to, so It might be interesting for folks like me that like to abstract and reuse the code as often as possible! But let's see how It can be achieved...I have been working recently at my work on a GPU implementation of a versatile perlin/simplex/fbm/turbulence noise in HLSL. While some of the individual algorithm are pretty simples, it is often common to use several permutations of those functions in order to produce some nice noise and turbulences functions (like the worm-lava texture I did for Ergon 4k intro ). Thus, they are an ideal candidate to demonstrate the use of closures and functions pointers. I won't explain here the basic principle of perlin and fbm noise generation to focus on the problem of code reuse in HLSL.Here is a simplified version of a Turbulence Noise implemented in a Pixel Shader:The problem with the previous code is that if we want to change the code behindcalled from FBMNoise (for example, apply cos/sin on the coordinates, or use of a simplex noise instead of the old Perlin Noise),. Of course, we could use the preprocessor to inline the code, but It would end up in something less readable, less debuggable, error prone...etc.Another example: Ken Perlin introduced some really cool functions to modify the noise, like the famous marble effect But wait! The MarbleNoise function could even be used in place of the AbsNoise function, in order to get another noise effect. So we could have a marble function calling a FBM... but we could also have a marble function called by a FBM... or both... ugh... so as we can see, It is possible to permute those functions to generate interesting patterns, but unfortunately, the shading language doesn't provide us a way to make those functions pluggable!... Almost! In fact, there is a small breach in the HLSL language and we are going to use it!So as I said in the introduction, Direct3D11 has introduced the concept of dynamic linking. I suggest the reader to go to an explanation on msdn \" Interfaces and classes \". Basically, the main feature introduced in the HLSL language is a bit of Object Oriented Programming (OOP) in order to address the problem of abstraction: Now HLSL has theandkeyword. But they were mainly introduced for dynamic linking of a shader, and as I said, dynamic linking is only available with SM5.0 profile.To be able to use this shader, we need to setup the abstractLight variable from the C++/C# code, through the usage of ID3D11Device::CreateClassLinkage and in the instatiation of a Pixel Shader ID3D11Device::CreatePixelShader As we can see, we need to declare the interface and classes variable globally, so that they can be accessed by the C++ program. This is the standard way to use dynamic linking in HLSL... but what If we want to use this differently?The principle is very simple:. The way to use it is then straightforward:The previous example could be compiled flawlessly with ps_4_0 (Shader Model 4) or ps_3_0 (with some minor changes for the pixel shader), and It would compile just fine! So basically,, that has two implementations available through the ClassicCalculator and ComplexCalculator classes. MyFunctionUsingICalculator doesn't have to change its signature to adapt to the underlying function, so as we can see, we have a suitable solution for developing function pointers in HLSL.Now, lets try to see if we could use this model to build our flexible noise functions. Replace ICalculator by a INoise interface. We are seeing that an implementation would have to call another INoise interface. In fact, ideally, we would like to code something like this:Unfortunately,. This limitation was quite annoying, as It excludes a whole range of combination, like aggregation, composition... making these function pointers useful only for a very limited set of cases...I have tried to overcome this problem using abstract class instead of interface, as classes can be declared as variable members of classes... but, again, there is a huge limitation: The class variable is in fact acting a a final or const variable that cannot be changed, thus making its usage almost useless...But I knew that HLSL permits lots of unusual constructions, and this is where closures are going to resolve this.So we know that interfaces can be used as function pointers, but their usage is limited as we cannot use anykind of composition. An interesting fact is that we can declare local variables in methods as being class or interfaces... The trick is to use a quite uncommon feature of HLSL:Therefore, It is possible to use a kind of. Let's rewrite our noise functions using this new closure technique:1.2.that is implementing the methods. If we had the keyword abstract in hlsl we wouldn't have to implement methods of this class.3.. If you look at AbsNoise, FbmNoise or MarbleNoise, they are using the INoise::Next() method to get an instance of the INoise interface they rely on. This is where functions pointers are extremely useful here.4.. We are declaring local classes that will override INoise::Next() method in order to chain INoise function pointers together.Et voila! As you can see, we are able to declare local classes from a pixel shader that are acting as closures. It is for example even possible to declare local classes that have a specific code in their Compute() methods.Behind the scene, when chaining the INoise::Next() methods, the fxc HLSL compiler is seeing all thoses classes as \"INoise*\".It is then possible to perform a fbm(marble(abs(perlin_noise()))) as well as a marble(fbm(abs(perlin_noise()))).In the end,From the previous example, we can extend the concept by1. Addingto each Noise function :2. And then we can rewrite the Pixel shader functions toThis way, It allows a syntax that is even more concise and modular!This is a very exciting technique that could open lots of abstraction opportunities while developing in HLSL. Though, in order to use this technique, there are a couple of advantages and things to take into account:Let me know if you are able to use this technique and If you are finding other interesting constructions or problems. That would be very interesting to dig a little more into the opportunities it opens. Lastly, I have done a small google search about this kind of technique, but didn't found anything... but It could have been used already by someone else, thus this whole technique is a new hypothetical discovery, but I enjoyed a lot to discover it!"
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-tex2d",
        "document": "The value of the texture data.\n\nThis function is supported in the following shader models."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-tex2d-s-t-ddx-ddy",
        "document": "Samples a 2D texture using a gradient to select the mip level.\n\nThe value of the texture data.\n\nThis function is supported in the following shader models.\n• Significant code reordering is done to move gradient computations outside of flow control.\n• If the D3DPSHADERCAPS2_0 cap is set with D3DD3DPSHADERCAPS2_0_GRADIENTINSTRUCTIONS, the compiler maps this function to texldd.\n\nWhen flow control is present in a shader, the result of a gradient calculation requested inside a given branch path is ambiguous when adjacent pixels may go down separate flow control paths. Therefore, it is deemed illegal to use any pixel shader operation that requests a gradient calculation to occur at a location that is inside a flow control construct which could vary across pixels for a given primitive being rasterized. If either side of an if statement with the branch attribute uses a gradient function a compiler error may be generated. See if Statement (DirectX HLSL)."
    },
    {
        "link": "https://microsoft.github.io/DirectX-Specs/d3d/archive/D3D11_3_FunctionalSpec.htm",
        "document": "Full Table of Contents at end of document.\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 1.1 Purpose\n\n 1.2 Audience\n\n 1.3 Topics Covered\n\n 1.4 Topics Not Covered\n\n 1.5 Not Optimized for Smooth Reading\n\n 1.6 How D3D11.3 Fits into this Unified Spec\n\n \n\n\n\nIt is assumed that the reader is familiar with real-time graphics, modern Graphics Processing Unit (GPU) design issues and the general architecture of Microsoft Windows Operating Systems, as well their planned release roadmap.\n\nThe target audience for this spec are the implementers, testers and documenters of hardware or software components that would be considered part of a D3D11.3-compliant system. In addition, software developers who are vested in the details about medium-term GPU hardware direction will find interesting information.\n\nTopics covered in this spec center on definition of the hardware architecture being targeted by the D3D11.1 Graphics Pipeline, in a form that attempts to be agnostic to any single vendor's hardware implementation. Included will be some references to how the Graphics Pipeline is controlled through a Device Driver Interface (DDI), and occasionally depictions of API usage as needed to illustrate points.\n\nThe exact relationship and interactions between topics covered in the Graphics Pipeline with other Operating System components is not covered.\n\nGPU resource management, GPU process scheduling, and low-level Operating System driver/kernel architecture are not covered.\n\nHigh-level GPU programming concepts (such as high level shading languages) are not covered.\n\nLittle to no theory or derivation of graphics concepts, techniques or history is provided. Equally rare for this spec is any attempt to characterize what sorts of things applications software developers might do using the functionality provided by D3D11.3. There are exceptions, but do not expect to gain much more than an understanding of the \"facts\" about D3D11.3 from this spec.\n\nBeware, there is little flow to the content in this spec, although there are plenty of links from place to place.\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 2.1 Input Assembler (IA) Overview\n\n 2.2 Vertex Shader (VS) Overview\n\n 2.3 Hull Shader (HS) Overview\n\n 2.4 Tessellator (TS) Overview\n\n 2.5 Domain Shader (DS) Overview\n\n 2.6 Geometry Shader (GS) Overview\n\n 2.7 Stream Output (SO) Overview\n\n 2.8 Rasterizer Overview\n\n 2.9 Pixel Shader (PS) Overview\n\n 2.10 Output Merger (OM) Overview\n\n 2.11 Compute Shader (CS) Overview\n\n \n\n\n\nD3D11.1 hardware, like previous generations, can be designed with shared programmable cores. A farm of Shader cores exist on the GPU, able to be scheduled across the functional blocks comprising the D3D11.1 Pipeline, depicted below.\n\nThe Input Assembler (IA) introduces triangles, lines, points or Control Points (for Patches) into the graphics Pipeline, by pulling source geometry data out of 1D Buffers(5.3.4).\n\nVertex data can come from multiple Buffers, accessed in an \"Array-of-Structures\" fashion from each Buffer. The Buffers are each bound to an individual input slot and given a structure stride. The layout of data across all the Buffers is specified by an Input Declaration, in which each entry defines an \"Element\" with: an input slot, a structure offset, a data type, and a target register (for the first active Shader in the Pipeline).\n\nA given sequence of vertices is constructed out of data fetched from Buffers, in a traversal directed by a combination of fixed-function state and various Draw*() API/DDI calls. Various primitive topologies are available to make the sequence of vertex data represent a sequence of primitives. Example topologies are: point-list, line-list, triangle-list, triangle-strip, 8 control-point patch-list.\n\nVertex data can be produced in one of two ways. The first is \"Non-Indexed\" rendering, which is the sequential traversal of Buffer(s) containing vertex data, originating at a start offset at each Buffer binding. The second method for producing vertex data is \"Indexed\" rendering, which is sequential traversal of a single Buffer containing scalar integer indices, originating at a start offset into the Buffer. Each index indicates where to fetch data out of Buffer(s) containing vertex data. The index values are independent of the characteristics of the Buffers they are referring to; Buffers are described by a declaration as mentioned earlier. So the task accomplished by \"Non-Indexed\" and \"Indexed\" rendering, each in their own way, is producing addresses from which to fetch vertex data in memory, and subsequently assemble the results into vertices and primitives.\n\nInstanced geometry rendering is enabled by allowing the sequential traversal, in either Non-indexed or Indexed rendering, to loop over a range within each Vertex Buffer (Non-Indexed case) or Index Buffer (Indexed case). Buffer-bindings can be identified \"Instance Data\" or \"Vertex Data\", indicating how to use the bound Buffer while performing instanced rendering. The address generated by \"Non-Indexed\" or \"Indexed\" rendering is used to fetch \"Vertex Data\", accounting also for looping when doing Instanced rendering. \"Instance Data\", on the other hand, is always sequentially traversed starting from a per-Buffer offset, at a frequency equal to one step per instance (e.g. one step forward after the number of vertices in an instance are traversed). The step rate for \"Instance Data\" can also be chosen to be a subharmonic of the instance frequency (i.e. one step forward every other instance, every third instance etc.).\n\nAnother use of the Input Assembler is that it can read Buffers that were written to from the Stream Output(2.7) stage. Such a scenario necessitates a particular type of Draw, DrawAuto(8.9). DrawAuto enables the Input Assembler to know how much data was dynamically written to a Stream Output Buffer without CPU involvement.\n\nIn addition to producing vertex data from Buffers, the IA can auto-generate scalar counter values such as: VertexID(8.16), PrimitiveID(8.17) and InstanceID(8.18), for input to shader stages in the graphics pipeline.\n\nIn \"Indexed\" rendering of strip topologies, such as triangle strips, a mechanism is provided for drawing multiple strips with a single Draw*() call (i.e. 'cut'ting strips).\n\nSpecific operational details of the IA are provided here(8).\n\nThe Vertex Shader stage processes vertices, performing operations such as transformations, skinning, and lighting. Vertex Shaders always operate on a single input vertex and produce a single output vertex. This stage must always be active.\n\nThe Hull Shader operates once per Patch (can only be used with Patces from the IA). It can transform input Control Points that make up a Patch into Output Control Points, and it can perform other setup for the fixed-function Tessellator stage (outputting TessFactors, which are numbers that indicate how much to tessellate).\n\nSpecific operational details of the Hull Shader are provided here(10).\n\nThe Tessellator is a fixed function unit whose operation is defined by declarations in the Hull Shader. It operates once per Patch output by the Hull Shader. The Hull shader outputs TessFactors which are numbers that tell the Tessellator how much to tessellate (generate geometry and connectivity) over the domain of the Patch.\n\nThe Domain Shader is invoked once per vertex generated by the Tessellator. Each invocation is identified by its coordinate on a generic domain, and the role of the Domain Shader is to turn that coordinate into something tangible (such as a point in 3D space) for use downstream. Each Domain Shader invocation for a Patch also sees shared input of all the Hull Shader output (such as output Control Points).\n\nSpecific operational details of the Domain Shader are provided here(12).\n\nThe Geometry Shader runs application-specified Shader code with vertices as input and the ability to generate vertices on output. The Geometry Shader's inputs are the vertices for a full primitive (two vertices for lines, three vertices for triangles, a single vertex for point, or all Control Points for a Patch if it reaches the GS with Tessellation disabled). Some types of primitives can also include the vertices of edge-adjacent primitive (an additional two vertices for a line, an additional three for a triangle).\n\nAnother input is a PrimitiveID auto-generated by the IA. This allows per-face data to be fetched or computed if desired.\n\nThe Geometry Shader stage is capable of outputting multiple vertices forming a single selected topology (GS output topologies available are: tristrip, linestrip, pointlist). The number of primitives emitted can vary freely within any invocation of the Geometry Shader, though the maximum number of vertices that could be emitted must be declared statically. Strip lengths emitted from a GS invocation can be arbitrary (there is a 'cut'(22.8.1) command).\n\nOutput may be fed to rasterizer and/or out to vertex Buffers in memory. Output fed to memory is expanded to individual point/line/triangle lists (the same way they would get passed to the rasterizer).\n\nSpecific operational details of the Geometry Shader are provided here(13).\n\nVertices may be streamed out to memory just before arriving at the Rasterizer. This is like a \"tap\" in the Pipeline, which can be turned on even as data continues to flow down to the Rasterizer. Data sent out via Stream Output is concatenated to Buffer(s). These Buffers may on subsequent passes be recirculated as Pipeline inputs.\n\nOne constraint about Stream Output is that it is tied to the Geometry Shader, in that both must be created together (though either can be \"NULL\"/\"off\"). The particular memory Buffer(s) being Streamed out are not tied to this GS/SO pair though. Only the description of which parts of vertex data to feed to Stream Output are tied to the GS.\n\nSince the amount of output written through Stream Output can be unpredictably dynamic, a special type of Draw command, DrawAuto(8.9), is necessary. DrawAuto enables the Input Assembler to know how much data was dynamically written to a Stream Output Buffer without CPU involvement. In addition, Queries are necessary to mitigate Stream Output overflow(20.4.10), as well as retrieve how much data was written(20.4.9) to the Stream Output Buffers.\n\nSpecific operational details of the Stream Output are provided here(14).\n\nThe rasterizer is responsible for clipping, primitive setup, and determining how to invoke Pixel Shaders. D3D . does not view this as a \"stage\" in the Pipeline, but rather an interface between Pipeline stages which happens to perform a significant set of fixed function operations, many of which can be adjusted by software developers.\n\nThe rasterizer always assumes input positions are provided in clip-space, performs clipping, perspective divide and applies viewport scale/offset.\n\nSpecific operational details of the Rasterizer are provided here(15).\n\nInput data available to the Pixel Shader includes vertex attributes that can be chosen, on a per-Element basis, to be interpolated with or without perspective correction, or be treated as constant per-primitive.\n\nThe Pixel Shader can also be chosen to be invoked either once per pixel or once per covered sample within the pixel.\n\nOutputs are one or more 4-vectors of output data for the current pixel or sample, or no color (if pixel is discarded).\n\nThe Pixel Shader has some other inputs and outputs available as well, similar to the kind of inputs and outputs the Compute Shader can use, allowing, for instance, the ability to write to scattered locations.\n\nThe final step in the logical Pipeline is visibility determination, through stencil or depth, and writing or blending of output(s) to RenderTarget(s), which may be one of many Resource Types(5).\n\nSpecific operational details of the Output Merger are provided here(17).\n\nThe Compute Shader allows the GPU to be viewed as a generic grid of data-parallel processors, without any graphics baggage from the graphics pipeline. The Compute Shader has explicit access to fast shared memory to facilitate communication between groups of shader invocations, and the ability to perform scattered reads and writes to memory. The availablility of atomic operations enables unique access to shared memory addresses. The Compute Shader is not part of the Graphics Pipeline (all the previously discussed shader stages). The Compute Shader exists on its own, albeit on the same device as all the other Shader Stages. To invoke this shader, Dispatch*() APIs are called instead of Draw*().\n\nD3D11 supports several different floating point representations for storage. However, all floating point computations in D3D11, whether in Shader programs written by application developers or in fixed function operations such as texture filtering or RenderTarget blending, are required to operate under a defined subset of the IEEE 754 32-bit single precision floating point behavior.\n\nOne ULP is the smallest representable delta from one value in a numeric representation to an adjacent value. The absolute magnitude of this delta varies with the magnitude of the number in the case of a floating point number. If, hypothetically, the result of an arithmetic operation were allowed to have a tolerance 1 ULP from the infinitely precise result, this would allow an implementation that always truncated its result (without rounding), resulting in an error of at most one unit in the last (least significant) place in the number representation. On the other hand, it would be much more desirable to require 0.5 ULP tolerance on arithmetic results, since that requires the result be the closest possible representation to the infinitely precise result, using round to nearest-even.\n\nHere is a summary of expected 32-bit floating point behaviors for D3D11. Some of these points choose a single option in cases where IEEE-754 offers choices. This is followed by a listing of deviations or additions to IEEE-754 (some of which are significant). Refer to IEEE-754 for topics not mentioned.\n• IEEE-754 requires floating point operations to produce a result that is the nearest representable value to an infinitely precise result, known as \"round to nearest even\". D3D11 has the same requirement: 32-bit floating point operations must produce a result that is within 0.5 ULP(3.1.2) of the infinitely preceise result. This applies only to addition, subtraction and multiplication.\n• Divide by 0 produces +/- INF, except 0/0 which results in NaN.\n• log of (+/-) 0 produces -INF. log of a negative value (other than -0) produces NaN.\n• rsq or sqrt of a negative number produces NaN. The exception is -0; sqrt(-0) produces -0, and rsq(-0) produces -INF.\n• The comparisons EQ, GT, GE, LT, and LE, when either or both operands is NaN returns FALSE.\n• Comparisons ignore the sign of 0 (so +0 equals -0).\n• The comparison NE, when either or both operands is NaN returns TRUE.\n• Comparisons of any non-NaN value against +/- INF return the correct result.\n• min(x,NaN) == min(NaN,x) == x (same for max). This used to be a deviation from IEEE 754 but now aligns with IEEE-754-2019's minimumNumber and maximumNumber operations.\n\n3.1.3.2 Complete Listing of Deviations or Additional Requirements vs. IEEE-754\n• There is no support for floating point exceptions, status bits or traps.\n• Denorms MUST be flushed to sign-preserved zero on input and output of any floating point mathematical operation.\n• An exception to the above point about flushing denorms is that any I/O or data movement operation that does not manipulate the data, such as using the ld(22.4.6) instruction to access Resource data, or executing mov instruction or conditional move/swap instruction (excluding min or max instructions), must not alter data at all (so a denorm remains denorm). Note that doing something that amounts to just moving data, but isn't explicitly doing so, such as multiplying a number by 1.0f is not detected as just a \"mov\", and in this case a denorm flush would happen.\n• Floating point values provided to hardware through states, such as Viewport MinDepth/MaxDepth, BorderColor values etc., may be provided as denorm values and may or may not be flushed before use by the hardware.\n• min or max operations must flush denorms for comparison, but the result may or may not be denorm flushed.\n• NaN input to an operation obviously always produces NaN on output, however the exact bit pattern of the NaN is not required to stay the same (unless the operation is a raw \"mov\" instruction which does not alter data at all.)\n• The arithmetic rules in D3D10+ do not make any distinctions between \"quiet\" and \"signaling\" NaN values (QNaN vs SNaN). All \"NaN\" values are handled the same way.\n• If both inputs to min() or max() are NaN, any NaN is returned.\n• A IEEE 754R rule is that min(-0,+0) == min(+0,-0) == -0, and max(-0,+0) == max(+0,-0) == +0, which honor the sign, in contrast to the comparison rules for signed zero (stated above). D3D11 recommends the IEEE 754R behavior here, but it will not be enforced; it is permissible for the result of comparing zeros to be dependent on the order of parameters, using a comparison that ignores the signs.\n• x*1.0f must always result in x (except denorm flushed).\n• x/1.0f must always result in x (except denorm flushed).\n• x +/- 0.0f must always result in x (except denorm flushed). But -0 + 0 = +0.\n• Fused operations (such as mad, dp3) must produce results that are no less accurate than the worst possible serial ordering of evaluation of the unfused expansion of the operation. Note that the definition of the worst possible ordering, for the purpose of tolerance, is not a fixed definition for a given fused operation; it depends on the particular values of the inputs. The individual steps in the unfused expansion must adhere to 0.5 ULP tolerance (except in cases where instructions in D3D11 are called out with a relaxed tolerance, the relaxed tolerance is allowed).\n• A corollary of the above tolerance for fused operations that leeway is provided for intermediate operations overflowing or underflowing. Implementations may either saturate to the appropriate extreme representable value for the output format during intermediate operations that go outside the input/output number format range, or implementations may maintain extra intermediate precision (possibly arriving at an output result that falls back into correct, representable form).\n• Fused operatons must adhere to the NaN rules defined for non-fused operations, with behavior that matches that of executing any one of the possible combinations of multiple non-fused operations that constitute the fused operation.\n• sqrt and rcp have 1 ULP tolerance. The shader reciprocal and reciprocal square-root instructions, rcp(22.10.18) and rsw(22.10.19), have their own separate relaxed precision requirement.\n• Divide operations may be implemented as x*(1.0f/y), although implementing divide directly as x/y, is permitted. If the two-step method, x*(1.0f/y), is chosen by the implementation, the multiply and the divide must each independently operate at the D3D11 32-bit floating point precision level (accuracy to 0.5 ULP for multiply, 1.0 ULP for reciprocal). If x/y is implemented directly, results must be of greater or equal accuracy than a two-step method.\n• See the Floating Point Conversion(3.2.2) section below for rules on converting to/from float representations.\n\nDouble-precision floating-point support is optional, however all double-precision floating point instructions listed in this spec (here (arithmetic)(22.14), here (conditional)(22.15), here (move)(22.16) and here (type conversion)(22.17) ) must be implemented if double support is enabled.\n\nDouble-precision floating-point usage is indicated at compile time by declaring shadel model 5_a. Support for Shader Model 5.0a will be reportable by drivers and discoverable by users via an API.\n\nWhen supported, double-precision instructions match IEEE 754R behavior requirements (with the exception of double precision reciprocal(22.14.5) which is permitted 1.0 ULP tolerance and the exact result if representable).\n\nAn exception to the 4-vector register convention exists for double-precision floating-point instructions, which operate on pairs of doubles. Double-precision floating-point values are in IEEE 754R format. One double is stored in .xy with the least significant 32 bits in x, and the most significant 32 bits in y. Similarly the second double is stored in .zw with the least significant 32 bits in z, and the most significant 32 bits in w.\n\nThe permissible swizzles for double operations are .xyzw, .xyxy, .zwxy, .zwzw. The permissible write masks for double operations are .xy, .zw, and .xyzw.\n\nSupport for generation of denormalized values is required for double-precision data (no flush-to-zero behavior). Likewise, instructions do not read denormalized data as a signed zero - they honor the denorm value.\n\nSeveral resource formats in D3D11 contain 16-bit representations of floating point numbers. This section describes the float16 representation.\n• 10 bits of fraction, (f), with an additional hidden bit\n\nA float16 value, v, made from the format above takes the following meaning:\n• (a) if e == 31 and f != 0, then v is NaN regardless of s\n\n32-bit floating point rules also hold for 16-bit floating point numbers, adjusted for the bit layout described above.\n• Precision: Unfused operations on 16-bit floating point numbers must produce a result that is the nearest representable value to an infinitely precise result (round to nearest even, per IEEE-754, applied to 16-bit values). Anywhere in the 32-bit floating point rules(3.1.3) where 1 ULP tolerance is stated, the same rules apply for unfused float16 arithmetic, but with the 1 ULP tightened to 0.5 ULP. For fused float16 operations, the rules are the same as fused operations are described in the 32-bit floating point rules(3.1.3), but with 1 ULP replaced with ULP.\n\nA single resource format in D3D11 contains 11-bit and 10-bit representations of floating point numbers. This section describes the float11 and float10 representations.\n• 6 bits of fraction for an 11-bit format, 5 bits of fraction for a 10-bit format, with an additional hidden bit. (f)\n\nA float11/float10 value, v, made from the format above takes the following meaning:\n• (a) if e == 31 and f != 0, then v is NaN\n• (b) if e == 31 and f == 0, then v = +infinity\n\n32-bit floating point rules also hold for 11-bit and 10-bit floating point numbers, adjusted for the bit layout described above.\n• Precision: Operations on 10/11-bit floating point numbers must produce a result that is the nearest representable value to an infinitely precise result (round to nearest even, per IEEE-754, applied to 10/11-bit values). Anywhere in the 32-bit floating point rules(3.1.3) where 1 ULP tolerance is stated, the same rules apply for 10/11-bit arithmetic, but with the 1 ULP tightened to 0.5 ULP.\n• Sign: Since there is no sign bit, any operation that would result in a number less than zero clamps to zero before being stored in 11-bit or 10-bit float.\n\nThis section describes the rules for various data conversions in D3D11. Other relevant information regarding data conversion is in the Data Invertability(19.1.2) section.\n\nWhenever a floating point conversion between different representations occurs, including to/from non-floating point representations, the following rules apply.\n\nThese are rules for converting from a higher range representation to a lower range representation:\n• Round-to-zero must be used during conversion to another float format. If the target is an integer or fixed point format, round-to-nearest-even must be used, unless the conversion is explicitly documented in the spec using another rounding behavior, such as round-to-nearest for FLOAT->SNORM(3.2.3.4), FLOAT->UNORM(3.2.3.6), FLOAT->SRGB(3.2.3.8). Other exceptions are the ftoi(22.13.3) and ftou(22.13.4) shader instructions, which use round-to-zero. Finally, the float-to-fixed(3.2.4.1) conversions used by the texture sampler and rasterizer have a specified tolerance measured in ULP(3.1.2) from an infinitely precise ideal.\n• For source values greater than the dynamic range of a lower range target format (eg. a large 32-bit float value is written into a 16-bit float RenderTarget), the maximum representable (appropriately signed) value results, NOT including signed infinity (due to the round to zero described above).\n• NaN in a higher range format must get converted to NaN representation in the lower range format if the NaN representation exists in the lower range format. If the lower format does not have a NaN representation, the result must be 0.\n• INF in a higher range format must get converted to INF in the lower range format if available. If the lower format does not have an INF representation, it must be converted to the maximum value representable. The sign must be preserved if available in the target format.\n• Denorm in a higher range format must get converted to the Denorm representation in the lower range format if available in the lower range format and the conversion is possible, otherwise the result is 0. The sign bit must be preserved if available in the target format.\n\nThese are rules for converting from a lower precision/range representation to a higher precision/range representation:\n• NaN in a lower range format must get converted to the NaN representation in the higher range format if available in the higher range format. If the higher range format does not have a NaN representation, it must be converted to 0.\n• INF in a lower range format must get converted to the INF representation in the higher range format if available in the higher range format. If the higher format does not have an INF representation, it must be converted to the maximum value representable (MAX_FLOAT in that format). The sign must be preserved if available in the target format.\n• Denorm in a lower range format must get converted to a normalized representation in the higher range format if possible, or else to a Denorm representation in the higher range format if the Denorm representation exists. Failing those, if the higher range format does not have a Denorm representation, it must be converted to 0. The sign must be preserved if available in the target format. Note that 32-bit float numbers count as a format without a Denorm representation (as D3D11 requires Denorms in operations on 32-bit floats to flush to sign preserved 0).\n\nThe following set of terms are subsequently used to characterize various integer format conversions.\n\nNote that the terms above are also used as Format Name Modifiers(19.1.3.2), where they describe both how data is layed out in memory and what conversion to perform in the transport path (potentially including filtering) from memory to/from a Pipeline unit such as a Shader. See the Formats(19.1) section to see exactly how these names are used in the context of resource formats.\n\nWhat follows are descriptions of conversions from various representations described above to other representations. Not all permutations are shown, but at least all the ones that show up in D3D11 somewhere are shown.\n\nUnless otherwise specified for specific cases, all conversions to/from integer representations to float representations described below must be done exactly. Where float arithmetic is involved, FULL IEEE-754 precision is required (1/2 ULP(3.1.2) of the infinitely precise result), which is stricter than the general D3D11 Floating Point Rules(3.1).\n\nGiven an n-bit integer value representing the signed range [-1.0f to 1.0f], conversion to floating-point is as follows:\n• The most-negative value maps to -1.0f. e.g. the 5-bit value 10000 maps to -1.0f.\n• Every other value is converted to a float (call it c), and then result = c * (1.0f / (2(n-1)-1)). e.g. the 5-bit value 10001 is converted to -15.0f, and then divided by 15.0f, yielding -1.0f.\n\nGiven a floating-point number, conversion to an n-bit integer value representing the signed range [-1.0f to 1.0f] is as follows:\n• Suppose the starting value is c\n• If c is NaN, the result is 0\n• If c > 1.0f, including INF, it is clamped to 1.0f. If c < -1.0f, including -INF, it is clamped to -1.0f.\n• Convert from float scale to integer scale: c = c * (2n-1-1).\n• Convert to integer:\n• If c >= 0, c = c + 0.5f, else, c = c - 0.5f.\n• Drop the decimal fraction, and the remaining floating point (integral) value is converted directly to an integer.\n\nThis conversion is permitted tolerance of ULP(3.1.2) (on the integer side). This means that after converting from float to integer scale, any value within ULP(3.1.2) of a representable target format value is permitted to map to that value. The additional Data Invertability(19.1.2) requirement ensures that the conversion is nondecreasing across the range and all output values are attainable.\n• The starting n-bit value is converted to float (0.0f, 1.0f, 2.0f, etc.) and then divided by (2n-1).\n• Suppose the starting value is c\n• If c is NaN, the result is 0\n• If c > 1.0f, including INF, it is clamped to 1.0f. If c < 0.0f, including -INF, it is clamped to 0.0f.\n• Convert from float scale to integer scale: c = c * (2n-1).\n• Convert to integer:\n• Drop the decimal fraction, and the remaining floating point (integral) value is converted directly to an integer.\n\nThis conversion is permitted tolerance of ULP(3.1.2) (on the integer side). This means that after converting from float to integer scale, any value within ULP(3.1.2) of a representable target format value is permitted to map to that value. The additional Data Invertability(19.1.2) requirement ensures that the conversion is nondecreasing across the range and all output values are attainable.\n\nThe following is the ideal SRGB to FLOAT conversion.\n• Take the starting n-bit value, convert it a float (0.0f, 1.0f, 2.0f, etc.); call this c.\n• If (c < = ) then: result = c / , else: result = ((c + )/ )\n\nThis conversion will be permitted a tolerance of ULP(3.1.2) (on the SRGB side). The procedure for measuring this tolerance, given that it is relative to the SRGB side even though the result is a FLOAT, is to convert the result back into SRGB space using the ideal FLOAT -> SRGB conversion specified below, but WITHOUT the rounding to integer, and taking the floating point difference versus the original SRGB value to yield the error. There are a couple of exceptions to this tolerance, where exact conversion is required: 0.0f and 1.0f (the ends) must be exactly achievable.\n\nThe following is the ideal FLOAT -> SRGB conversion.\n• Suppose the starting value is c\n• If c is NaN, the result is 0\n• If c > 1.0f, including INF, is clamped to 1.0f. If c < 0.0f, including -INF, it is clamped to 0.0f.\n• If (c <= ) then: c = * c, else: c = * c( / ) -\n• Convert from float scale to integer scale: c = c * (2n-1).\n• Convert to integer:\n• Drop the decimal fraction, and the remaining floating point (integral) value is converted directly to an integer.\n\nThis conversion is permitted tolerance of ULP(3.1.2) (on the integer side). This means that after converting from float to integer scale, any value within ULP(3.1.2) of a representable target format value is permitted to map to that value. The additional Data Invertability(19.1.2) requirement ensures that the conversion is nondecreasing across the range and all output values are attainable.\n\nTo convert from SINT to an SINT with more bits, the MSB bit of the starting number is \"sign-extended\" to the additional bits available in the target format.\n\nTo convert from UINT to an SINT with more bits, the number is copied to the target format's LSBs and additional MSB's are padded with 0.\n\nTo convert from SINT to UINT with more bits: If negative, the value is clamped to 0. Otherwise the number is copied to the target format's LSBs and additional MSB's are padded with 0.\n\nTo convert from UINT to UINT with more bits the number is copied to the target format's LSBs and additional MSB's are padded with 0.\n\n3.2.3.13 SINT or UINT -> SINT or UINT (With Fewer or Equal Bits)\n\nTo convert from a SINT or UINT to SINT or UINT with fewer or equal bits (and/or change in signedness), the starting value is simply clamped to the range of the target format.\n\nFixed point integers are simply integers of some bit size that have an implicit decimal point at a fixed location. The ubiquitous \"integer\" data type is a special case of a fixed point integer with the decimal at the end of the number. Fixed point number representations are characterized as: i.f, where i is the number of integer bits and f is the number of fractional bits. e.g. 16.8 means 16 bits integer followed by 8 bits of fraction. The integer part is stored in 2's complement, at least as defined here (though it can be defined equally for unsigned integers as well). The fractional part is stored in unsigned form. The fractional part always represents the positive fraction between the two nearest integral values, starting from the most negative. Exact details of fixed point representation, and mechanics of conversion from floating point numbers are provided below.\n\nAddition and subtraction operations on fixed point numbers are performed simply using standard integer arithmetic, without any consideration for where the implied decimal lies. Adding 1 to a 16.8 fixed point number just means adding 256, since the decimal is 8 places in from the least significant end of the number. Other operations such as multiplication, can be performed as well simply using integer arithmetic, provided the effect on the fixed decimal is accounted for. For example, multiplying two 16.8 integers using an integer multiply produces a 32.16 result.\n\nFixed point integer representations are used in a couple of places in D3D11:\n• Post-clipped vertex positions in the rasterizer are snapped to fixed point, to uniformly distribute precision across the RenderTarget area. Many rasterizer operations, including face culling as one example, occur on fixed point snapped positions, while other operations, such as attribute interpolator setup, use positions that have been converted back to floating point from the fixed point snapped positions.\n• Texture coordinates for sampling operations are snapped to fixed point (after being scaled by texture size), to uniformly distribute precision across texture space, in choosing filter tap locations/weights. Weight values are converted back to floating point before actual filtering arithmetic is performed.\n\nThe following is the general procedure for converting a floating point number n to a fixed point integer i.f, where i is the number of (signed) integer bits and f is the number of fractional bits:\n• If n is a NaN, result = 0; if n is +Inf, result = FixedMax*2^f; if n is -Inf, result = FixedMin*2^f\n• Else compute n*2^f and convert to integer.\n\nNote: Sign of zero is preserved.\n\nFor D3D11 implementations are permitted ULP(3.1.2) tolerance in the integer result vs. the infinitely precise value n*2^f after the last step above.\n\nThe diagram below depicts the ideal/reference float to fixed conversion (including round-to-nearest-even), yielding 1/2 ULP accuracy to an infinitely precise result, which is more accurate than required by the tolerance defined above. Future D3D versions will require exact conversion like this reference.\n\nSpecific choices of bit allocations for fixed point integers are listed in the places in the D3D11 spec where they are used.\n\nAssume that the specific fixed point representation being converted to float does not contain more than a total of 24 bits of information, no more than 23 bits of which is in the fractional component. Suppose a given fixed point number, fxp, is in i.f form (i bits integer, f bits fraction). The conversion to float is akin to the following pseudocode:\n\nAlthough the situation rarely, if ever arises, consider that a number that originates as fixed point, gets converted to float32, and then gets converted back to fixed point will remain identical to its original value. This holds provided that bit representation for the fixed point number does not contain more information than can be represented in a float32. This lossless conversion property does not hold when making the opposite round-trip, starting from float32, moving to fixed-point, and back; indeed lossy conversion is in fact the \"point\" of converting from float32 to fixed-point in the first place.\n\nOne final note on round-trip conversion. Observe that when the float32 number -2.75 is converted to fixed-point, it becomes -3 +0.25, that is, the integer part is negative but the fixed point part, considered by itself, is positive. When that is converted back to float32, it becomes -2.75 again, since floating point stores negative numbers in sign-magnitude form, instead of in two's complement form.\n\nThe Pixel Coordinate System defines the origin as the upper-left corner of the RenderTarget. Pixel centers are therefore offset by ( , ) from integer locations on the RenderTarget. This choice of origin makes rendering screen-aligned textures trivial, as the pixel coordinate system is aligned with the texel coordinate system.\n\nThe texel coordinate system has its origin at the top-left corner of the texture. See the \"Texel Coordinate System\" diagram below. This is consistent with the Pixel Coordinate System.\n\nThe memory load instructions like sample(22.4.15) or ld(22.4.6) have a couple of ways texture coordinates are interpreted (normalized float, or scaled integer respectively). The \"Texture Coordinate Interpretation\" diagram below describes how these interpretations get mapped to specific texel(s), for point and linear sampling. The diagram does not illustrate address wrapping, which occurs after the shown equations for computing texel locations. The addressing math shown in this diagram is only a general guideline, and exact definition of texel selection arithmetic is provided in the Texture Sampling(7.18) section, including the role of Fixed Point(3.2.4.1) snapping of precision in the addressing process.\n\nConsider a set of vertices going through the Rasterizer, after having gone through clipping, perspective divide and viewport scale. Suppose that any further primitive expansion has been done (e.g. rectangular lines can be drawn by implementations as 2 triangles, described later). After the final primitives to be rasterized have been obtained, the x and y positions of the vertices are snapped to exactly n. fixed point integers. Any front/back culling is applied (if applicable) after vertices have been snapped. Interpolation of pixel attributes is set up based on the snapped vertex positions of primitives being rasterized.\n\nAny pixel sample locations which fall inside the triangle are drawn. An example with a single sample per pixel (at the center) is shown below. If a sample location falls exactly on the edge of the triangle, the Top-Left Rule applies, to ensure that adjacent triangles do not overdraw. The Top-Left rule is described below.\n\nTop edge: If an edge is exactly horizontal, and it is above the other edges of the triangle in pixel space, then it is a \"top\" edge.\n\nLeft edge: If an edge is not exactly horizontal, and it is on the left side of the triangle in pixel space, then it is a \"left\" edge. A triangle can have one or two left edges.\n\nTop-Left Rule: If a sample location falls exactly on the edge of a triangle, the sample is inside the triangle if the edge is a \"top\" edge or a \"left\" edge. If two edges from the same triangle touch the pixel center, then if both edges are \"top\" or \"left\" then the sample is inside the triangle.\n\nRasterization rules for infinitely-thin lines, with no antialiasing, are described below.\n\nOne futher implication of these line rasterization rules is that lines that are geometrically clipped to the viewport extent may set one less pixel than lines that are rendered to a larger 2D extent with the pixels outside the viewport discarded. (This is due to the handling of the line endpoints.)\n\nSince geometric clip to the viewport is neither required nor disallowed, aliased line rendering is allowed to differ in viewport-edge pixels due to geometric clipping.\n\nThe alpha-based antialiased rasterization of a line (defined by two end vertices) is implemented as the visualization of a rectangle, with the line's two vertices centered on two opposite \"ends\" of the rectangle, and the other two edges separated by a width (in D3D11 width is only 1.0f). No accounting for connected line segments is done. The region of intersection of this rectangle with the RenderTarget is estimated by some algorithm, producing \"Coverage\" values [0.0f..1.0f] for each pixel in a region around the line. The Coverage values are multiplied into the Pixel Shader output o0.a value before the Output Merger Stage. Undefined results are produced if the PS does not output o0.a. D3D11 exposes no controls for this line mode.\n\nIt is deemed that there is no single \"best\" way to perform alpha-based antialiased line rendering. D3D11 adopts as a guideline the method shown in the diagram below. This method was derived empirically, exhibiting a number of visual properties deemed desirable. Hardware need not exactly match this algorithm; tests against this reference shall have \"reasonable\" tolerances, guided by some of the principles listed further below, permitting various hardware implementations and filter kernel sizes. None of this flexibility permitted in hardware implementation, however, can be communicated up through D3D11 to applications, beyond simply drawing lines and observing/measuring how they look.\n\nThe following is a listing of the \"nice\" properties that fall out of the above algorithm, which in general will be expected of hardware implementations (admittedly many of which are likely difficult to test):\n• Lines are \"smooth\", with minimal jagged edges or braiding.\n• There is virtually no variation in intensity along a line, including during animation.\n• There is virtually no moire patterning with close lines, including during animation.\n• There is virtually no variation in intensity at junctions between line segments placed end-to-end.\n• The total numerical contribution of a line to an image is virtually equal to the visible area of the line, regardless of line orientation, except appropriately accounting for color variation along the line from shading. This stipulation is based on using the Coverage multiplied into Pixel Shader o0.a (srcAlpha) in the following blend formula at the Output Merger: srcColor * srcAlpha + destColor * (1-srcAlpha).\n\nQuadrilateral lines take 2 endpoints and turn them into a simple rectangle with width , drawn with triangles. The attributes at each end of the line are duplicated for the 2 vertices at each end of the rectangle.\n\nThis mode is not supported with center sample patterns (D3D11_CENTER_MULTISAMPLE_PATTERN) where there is more than one sample overlapping the center of the pixel, in which case results of drawing this style of line are undefined. See here(19.2.4.1).\n\nFor the purpose of rasterization, a point is represented as a square of width 1 oriented to the RenderTarget. Actual implementation may vary, but output behavior should be identical to what is described here. The coordinate for a point indentifies where the center of the square is located. Pixel coverage for points follows Triangle Rasterization Rules, interpreted as though a point is composed of 2 triangles in a Z pattern, with attributes duplicated at the 4 vertices. Cull modes do not apply to points.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 3.5.1 Overview\n\n 3.5.2 Warning about the MultisampleEnable State\n\n 3.5.3 Multisample Sample Locations And Reconstruction\n\n 3.5.4 Effects of Sample Count > 1\n\n\n\nMultisample Antialiasing seeks to fight geometry aliasing, without necessarily dealing with surface aliasing (leaving that as a shading problem, e.g. texture filterng). This is accomplished by performing pixel coverage tests and depth/stencil tests at multiple sample locations per pixel, backed by storage for each sample, while only performing pixel shading calculations once for covered pixels (broadcasting Pixel Shader output across covered samples). It is also possible to request Pixel Shader invocations to occur at sample-frequency rather than at pixel-frequency.\n\nThe MultisampleEnable Rasterizer State remains as an awkward leftover from D3D9. It no longer does what the name implies; it no longer has any bearing on multisampling; it only controls line rendering behavior now. The state should have been renamed/refactored, but the opportunity was missed in D3D11. For a detailed discussion about what this state actually does now, see State Interaction With Point/Line/Triangle Rasterization Behavior(15.14).\n\nSpecifics about sample locations and reconstruction functions for multisample antialiasing are dependent on the chosen Multisample mode, which is outside the scope of this section. See Multisample Format Support(19.2), and Specification of Sample Positions(19.2.4).\n\nRasterization behavior when sample count is greater than 1 is simply that primitive coverage tests are done for each sample location within a pixel. If one or more sample locations in a pixel are covered, the Pixel Shader is run once for the pixel in Pixel-Frequency mode, or in Sample-Frequency mode once for each covered sample that is also in the Rasterizer SampleMask. Pixel-frequency execution produces a single set of Pixel Shader output data that is replicated to all covered samples that pass their individual depth/stencil tests and blended to the RenderTarget per-sample. Sample-frequency execution produces a unique set of Pixel Shader output data per covered sample (and in SampleMask), each output getting blended 1:1 to the corresponding RenderTarget sample if its depth/stencil test passes.\n\nNote that points(3.4.6) and quadrilateral lines(3.4.5) are functionally equivalent to drawing their area with triangles. So Sample-Frequency execution is easily defined for all of these primitives. For points, the samples covered by the point area (and in the RasterizerState's SampleMask) each get Pixel Shader invocations with attributes replicated from its single vertex (except one parameter is available that is varying - an ID identifying each sample from the total set of samples in the pixel). For quadrilateral lines, the two end vertices define how attributes interpolate along the length, staying constant across the perpendicular. Again, the samples covered by the area of the primitive (and in the SampleMask) each get a Pixel Shader invocations in Sample-Frequency execution mode, with unique input attributes per sample, including an ID identifying which sample it is.\n\nAlpha-Antialiased Lines(3.4.4) and Aliased Lines(3.4.3) are algorithms that inherently do not deal with discrete sample locations within a pixel's area, and thus it is illegal/undefined to request Sample-Frequency execution for these primitives, unless the sample count is 1, which is identical to Pixel-Frequency execution.\n\nConsider a Pixel Shader that operates only on pixel-frequency inputs (e.g. all attributes have one of the following interpolation modes(16.4): constant, linear, linear_centroid, linear_noperspective or linear_noperspective_centroid). Implementations need only execute the shader once per pixel and replicate the results to all samples in the pixel. Now suppose code is added to the shader that generates new outputs based on reading sample-frequency inputs. The existing pixel-frequency part of the shader behaves identically to before. Even though the shader will now execute at sample-frequency (so the new outputs can vary per-sample), each invocation produces the same result for the original outputs.\n\nThough this example happens to separate out the different interpolation frequencies to highlight their invariance, of course it is perfectly valid in general for shader code to mix together inputs with any different interpolation modes.\n\nWhen a sample-frequency interpolation mode(16.4) is not needed on an attribute, pixel-frequency interpolation-modes such as linear evaluate at the pixel center. However with sample count > 1 on the RenderTarget, attributes could be interpolated at the pixel center even though the center of the pixel may not be covered by the primitive, in which case interpolation becomes \"extrapolation\". This \"extrapolation\" can be undesirale in some cases, so short of going to sample-frequency interpolation, a compromise is the centroid interpolation mode.\n\nCentroid behaves exactly as follows:\n• (1) If all samples in the primitive are covered, the attribute is evaluated at the pixel center (even if the sample pattern does not happen to have a sample location there).\n• (2) Else the attribute is evaluated at the first covered sample, in increasing order of sample index, where sample coverage is after ANDing the coverage with the SampleMask Rasterizer State.\n• (3) If no samples are covered, such as on helper pixels executed off the bounds of a primitive to fill out 2x2 pixel stamps, the attribute is evaluated as follows: If the SampleMask Rasterizer state is a subset of the samples in the pixel, then the first sample covered by the SampleMask Rasterizer State is the evaluation point. Otherwise (full SampleMask), the pixel center is the evaluation point.\n\nOverriding the Rasterizer sample count means defining the multisample pattern at the Rasterizer independent of what RenderTargetViews(5.2) (or UnorderedAccessView(5.3.9)s) may be bound at the Output Merger (and their associated sample count / Quality Level).\n\nThe ForcedSampleCount state setting is located in the Rasterizer State(15.1) object.\n\nDevices must support all the standard sample patterns up to and including 16 for the ForcedSampleCount. This is even if the device does not support that many samples in RenderTarget / DepthStencil resources.\n\nWith a forced sample count/pattern selected at the rasterizer (ForcedSampleCount > 0), pixels are candidates for shader invocation based on the selected sample pattern, independent of the RTV (\"output\") sample count. The burden is then on shader code to make sense of the possible mismatch between rasterizer and output storage sample count, given the defined semantics.\n\nHere are the behaviors with ForcedSampleCount > 0.\n• The ForcedSampleCount identifies one of the D3D standard sample patterns, 1, 4, 8 or 16 samples (0 means the feature is off and rendering is 'normal').\n• If ForcedSampleCount is greater than 1, any RTVs that are bound while rendering must only have a single sample, otherwise rendering behavior is undefined.\n• If ForcedSampleCount is 1, RTVs that are bound can be any sample count.\n• If a rendered primitive covers any samples in the ForcedSampleCount sample pattern, the pixel is a candidate for Pixel Shader invocation.\n• Output sample locations have no bearing on whether a pixel is a candidate for Pixel Shader invocation.\n• Depth/Stencil Views must not be bound, depth testing must be disabled, and the shader must not output depth while rendering with ForcedSampleCount 1 or greater, otherwise rendering behavior is undefined.\n• Shader invocation happens for a candidate pixel if any of the output sample locations is in the SampleMask Rasterizer State.\n• In pixel-frequency shader invocation, one invocation occurs if any output sample is in the SampleMask.\n• Sample-frequency shader invocation cannot be requested, otherwise rendering results are undefined.\n• Pixel Shader input coverage to the shader sees the primitive's coverage of the ForcedSampleCount sample pattern for the pixel.\n• Pull Model attribute interpolation is based on the ForcedSampleCount pattern when selecting sample location by sample index.\n• The centroid interpolation algorithm doesn't take into account the SampleMask Rasterizer State, since the SampleMask is relevant to the output sample pattern only, not the ForcedSampleCount pattern.\n• See D3D11_QUERY_OCCLUSION(20.4.6) for description of interaction of ForcedSampleCount, SampleMask and the occlusion query sample count.\n\nThe above functionality is required for Feature Level 11_1 hardware.\n\nD3D10.0 - D3D11.0 hardware (and Feature Level 10_0 - 11_0) supports ForcedSampleCount set to 1 (and any sample count for RTV) along with the described limitations (e.g. no depth/stencil).\n\nFor 10_0, 10_1, and 11_0 hardware, when ForcedSampleCount is set to 1, line rendering cannot be configured to 2-triangle (quadrilateral) based mode (i.e. the MultisampleEnable state cannot be set to true). This limitation isn't present for 11_1 hardware. Note the naming of the 'MultisampleEnable' state is misleading since it no longer has anything to do with enabling multisampling; instead it is now one of the controls along with AntialiasedLineEnable for selecting line rendering mode.\n\nUAV(5.3.9)-only rendering with multisampling at the rasterizer is possible by keying off the ForcedSampleCount state described earlier, with the sample patterns limited to 0, 1, 4, 8 and 16. (The UAVs themselves are not multisampled in terms of allocation.) A setting of 0 is equivalent to the setting 1 - single sample rasterization.\n\nShaders can request pixel-frequency invocation with UAV-only rendering, but requesting sample-frequency invocation is invalid (produces undefined shading results).\n\nThe SampleMask Rasterizer State does not affect rasterization behavior at all here.\n\nOn D3D11.0 hardware, ForcedSampleCount can be 0, 1, 4 and 8 with UAV only Rasterization. D3D11.1 hardware additionally supports 16.\n\nAttempting to render with unsupported ForcedSampleCount produces undefined rendering results - though if a ForcedSampleCount is chosen that could never be valid for TIR or UAV-only rendering the runtime will fail the Rasterizer State object creation immediately.\n\nPixel Shaders always run in minimum 2x2 quanta to be able to support derivative calculations, regardless of the RenderTarget sample count. These Pixel Shader derivative calculations, used in texture filtering operations, but also available directly in shaders, are calculated by taking deltas of data in adjacent pixels. This requires data in each pixel has been sampled with unit spacing horizontally or vertically.\n\nRenderTarget sample counts > 1 do not affect derivative calculation methods. If derivatives are requested on an attribute that has been Centroid sampled, the hardware calculation is not adjusted, and therefore incorrect derivatives will often result. What the Shader expects to be a derivative wrt a unit distance in the x or y direction in RenderTarget space will actually be the rate of change with respect to some other direction vector, which also probably isn't unit length.\n\nUnder sample-frequency execution, a 2x2 quad of Pixel Shaders executes for each sample index where that sample is covered in at least one of the pixels participating in the 2x2 quad. This allows derivatives to be calculated in the usual way since any given sample is located one unit apart horizonally or vertically from the corresponding sample in the neighboring pixels.\n\nFurther important discussion of Pixel Shader derivatives is under Interaction of Varying Flow Control With Screen Derivatives(16.8).\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 4.1 Minimal Pipeline Configurations\n\n 4.2 Fixed Order of Pipeline Results\n\n 4.3 Shader Programs\n\n 4.4 The Element\n\n \n\n\n\nThe rendering Pipeline encapsulates all state related to the rendering of a primitive. This includes a sequence of pipeline stages as well as various state objects.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 4.1.1 Overview\n\n 4.1.2 No Buffers at Input Assembler\n\n 4.1.3 IA + VS (+optionally GS) + No PS + Writes to Depth/Stencil Enabled\n\n 4.1.4 IA + VS (+optionally GS) + PS (incl. Rasterizer, Output Merger)\n\n 4.1.5 IA + VS + SO\n\n 4.1.6 No RenderTarget(s) and/or Depth/Stencil and/or Stream Output\n\n 4.1.7 IA + VS + HS + Tessellation + DS + ...\n\n 4.1.8 Compute alone\n\n 4.1.9 Minimal Shaders\n\n\n\nNot all Pipeline Stages must be active. This section clarifies this concept by illustrating some minimal configurations that can produce useful results. The Graphics pipeline is accessed by Draw* calls from the API. The alternative pipeline, Compute, is accessed by issuing Dispatch* calls from the API.\n\nFor the Graphics pipepine, the Input Assembler is always active, as it produces pipeline work items. In addition, the Vertex Shader is always active. Relying on the presence of the Vertex Shader at all times simplifies data flow permutations very significantly, versus allowing the Input Assembler with its limited programming flexibility to feed any pipeline stage.\n\nA minimal use of the Input Assembler is to not have any input Buffers bound (vertex or index data). The Input Assembler can generate counters such as VertexID(8.16), InstanceID(8.18) and PrimitiveID(8.17), which can identicy vertices/primitives generated in the pipeline by Draw*(), or DrawIndexed*() (if at least an Index Buffer is bound). Thus Shaders can minimally drive all their processing based on the IDs if desired, including fetching appropriate data from Buffers or Textures.\n\n4.1.3 IA + VS (+optionally GS) + No PS + Writes to Depth/Stencil Enabled\n\nIf the shader stage before the rasterizer outputs position, and Depth/Stencil writes are enabled, the rasterizer will simply perform the fixed-function depth/stencil tests and updates to the Depth/Stencil buffer, even if there is no Pixel Shader active. No Pixel Shader means no updates to RenderTargets other than Depth/Stencil.\n\nThe Input Assembler + Vertex Shader (required) can drive the Pixel Shader directly (GS does not have to be used, but can be). If an application seeks to write data to RenderTarget(s), not including Depth/Stencil which were explained earlier, the Pixel Shader must be active. This implicitly Output Merger as well, though as described further below, there's no requirement that RenderTargets need to be bound just because rasterization is occuring.\n\nThe Input Assembler (+required VS) can feed Stream Output directly with no other stages active. Note that as described in the Stream Output Stage(14) section, Stream Output is tied to the Geometry Shader, however a \"NULL\" Geometry Shader can be specified, allowing the outputs of the Vertex Shader to be sent to Stream Output with no other stages active.\n\nWhether or not the Pixel Shader is active, it is always legal to NOT have any output targets bound (and/or have output masks defined so that no output targets are written). Likewise for Stream Output. This might be interesting for performance tests which don't include output memory bandwidth (and which might examine feedback statistics such as shader invocation counts, which is itself a form of pipeline output anyway).\n\nThe Input Assembler (+required VS) can feed Stream Output directly with no other stages active. Note that as described in the Stream Output Stage(14) section, Stream Output is tied to the Geometry Shader, however a \"NULL\" Geometry Shader can be specified, allowing the outputs of the Vertex Shader to be sent to Stream Output with no other stages active.\n\nTake any of the configurations above, and HS + Tessellator + DS can be inserted after the VS. The presence of the DS is what implises the presence of the Tessellator before it.\n\nWhen the Compute Shader runs, it runs by itself. The state for both the Graphics pipeline shaders and Compute Shader can be simultaneously bound. The selection of which pipeline to use is Draw* invokes Graphics and Dispatch* invokes Compute.\n\nAll vertex shaders must have a minimum of one input and one output, which can be as little as one scalar value. Note that System Generated Values such as VertexID(8.16) and InstanceID(8.18) count as input.\n\nThe rendering Pipeline is designed to allow hardware to execute tasks at various stages in parallel. However observable rendering results must match results produced by serial processing of tasks. Whenever a task in the Pipeline could be performed either serially or in parallel, the results produced by the Pipeline must match serial operation. That is, the order that tasks enter the Pipeline is the order that tasks are observed to be propagated all the way through to completion. If a task moving through the Pipeline generates additional sub-tasks, those sub-tasks are completed as part of completing the spawning task, before any subsequent tasks are completed. Note that this does not prevent hardware from executing tasks out of order or in parallel if desirable, just as long as results are buffered appropriately such that externally visible results reflect serial execution.\n\nOne exception to this fixed ordering is with Tessellation. With the fixed function Tessellation stage, implementations are free to generate points and topology in any order as long as that order is consistent given the same input on the same device. Vertices can even be generated multiple times in the course of tessellating a patch, as long as the Tessellator output topology is not point (in which case only the unique points in the patch must be generated). This tessellator exception is discussed here(11.7.9).\n\nAnother exception to the fixed ordering of pipeline results is any access to an Unordered Transaction View of a Resource (for example via the Compute Shader or Pixel Shader). These types of Views explicitly allow unordered results, leaving the burden to applications to make careful choices of atomic instructions to access Unordered Transaction Views if deterministic and implementation invariant output is desired.\n\nA Shader object encapsulates a Shader program for any type of Shader unit. All shaders have a common binary format and basically have the following typical layout. A helpful reference for this is the source code accompanying the Reference Rasterizer, which includes facilities for parsing the shader binary.\n\nThe Tessellation related shaders have a significantly different structure, particularly the Hull Shader, which appears as multiple phases of shaders concatenated together (not depicted here).\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 4.4.1 Overview\n\n 4.4.2 Elements in the Pipeline\n\n 4.4.3 Passing Elements Through Pipeline Interfaces\n\n\n\nFrom the perspective of individual D3D . Pipeline stages accessing and interpreting memory, all memory layouts (e.g. Buffer, Texture1D/2D/3D/Cube) are viewed as being composed of \"Elements\". An individual Element represents a vector of anywhere from 1 to values. An Element could be an R8G8B8A8 packing of data, a single 8-bit integer value, 4 float32 values, etc. In particular, an Element is any one of the DXGI_FORMAT_* formats(19.1), e.g. DXGI_FORMAT_R8G8B8A8 (DXGI stands for \"DirectX Graphics Infrastructure\", a software component outside the scope of this specification which happens to own the list of DirectX formats going forward). Filtering may be involved in the process of fetching an Element from a texture, and this simply involves looking at multiple values for a given Element in memory and blending them in some fashion to produce an Element that is returned to the Shader.\n\nBuffers in memory can be made up of structures of Elements (as opposed to being a collection of a single Element). For example a Buffer could represent an array of vertices, each vertex containing several elements, such as: position, normal and texture coordinates. See the Resources(5) section for full detail.\n\nThe concept of \"Elements\" does not only apply to resources. Elements also characterize data passing from one Pipeline stage to the next. For example the outputs of a Vertex Shader (Elements making up a vertex) are typically read into a subsequent Pipeline stage as input data, for instance into a Geometry Shader. In this scenario, the Vertex Shader writes values to output registers, each of which represents an individual Element. The subsequent Shader (Geometry Shader in this example) would see a set of input registers each initialized with an Element out of the set of input data.\n\nThere are various types of data interfaces in the hardware Pipeline through which Elements pass. This section describes the interfaces in generic terms, and characterizes how Elements of data pass through them. Specific descriptions for each of the actual interfaces in the Pipeline are provided throughout the spec, in a manner consistent with the principles outlined here. The overall theme here is that data mappings through all interfaces are always direct, without any linkage resolving required.\n\nThe first type of interface is Memory-to-Stage, where an Element from a Resource (Texture/Buffer) is being fetched into the some part of the Pipeline, possibly the \"top\" of the Pipeline (Input Assembler(8)), or the \"side\", meaning a fetch driven from within a Shader Stage. At the point of binding of memory Resources to these interfaces, a number is given to each Element that is bound, representing which input (v#) or texture (t#) \"register\" at the particular interface refers to the Element. Note that there is no linkage resolving done on behalf of the application; the Shader assumes which \"registers\" will refer to particular Elements in memory, and so when memory is bound to the interface, it must be bound (or declared, in cases where multiple Elements come from the same Resource in memory) at the \"register\" expected by the Shader.\n\nFor Memory-to-Stage interfaces, Elements always provide to the Shader components of data, with defaults provided for Elements in memory containing fewer than components (though this can be masked to be any subset of the components in the Shader if desired).\n\nFor interfaces on the \"side\", where memory Resources are bound to Shader Stages so they can be fetched from via Shader code, the set of binding points (t# registers in the Shader) cannot be dynamically indexed within the Shader program without using flow control.\n\nOn the other hand, the interface at the \"top\" of the Pipeline (the input v# registers of the first active Shader Stage) can be dynamically indexed as an array from Shader code. The Elements in v# registers being indexed must have a declaration(22.3.30) specifying each range that is to be indexed, where each range specifies a contiguous set of Elements/v# registers, ranges do not overlap, and the components declared for each Element in a given range are identical across the range.\n\nThe second type of interface is Stage-to-Stage, where one Pipeline Stage outputs a set of component Elements (written to output o# registers) to the subsequent active Pipeline Stage, which receives Elements in its input v# registers. The mapping of output registers in one Stage to input registers in the next Stage is always direct; so a value written to o3 always goes to v3 in the subsequent Stage. Any subset of the components of any Element can be declared rather than the whole thing.\n\nIf more Elements or components within Elements are output than are expected/declared for input by the subsequent Stage, the extra data gets discarded / becomes undefined. If fewer Elements or components within Elements are output than are expected/declared for input by the subsequent Stage, the missing data is undefined.\n\nSimilar to the Memory-to-Stage interface at the \"top\" of the Pipeline, which feeds the input v# registers of the first active Pipeline Stage, at a Stage-to-Stage interface, writes to output Elements (o#) and at the subsequent Stage, reads from input elements (v#) can each be dynamically indexed as arrays from code at the respective Shaders. The Elements in o# registers being indexed must have a declaration(22.3.30) for each range, specifying a contiguous set of Elements/o# registers, without overlapping, and with the same component masks declared for each Element in a given range. The same applies to input v# registers at the subsequent stage (the array declarations for the input v# registers in the Shader are independent/orthogonal to the array declarations for o# in the previous Shader).\n\nThere is a detail which is mostly orthogonal to the the Stage-to-Stage interface discussion above: the frequency of operation at subsequent Stages varies, in addition to different amounts of data different Stages can input. For example the Geometry Shader(13) inputs all the vertices for a primitive. The Pixel Shader(16) can choose to have its inputs inperpolated from vertices, or take the data from one. The point of the above discussion is only to describe the mechanism for Element transport through the interfaces independently of these varying frequencies of operation between Stages.\n\nThe final type of interface is Stage-to-Memory, where a Pipeline Stage outputs a set of component Elements (written to output o# registers) on a path out to memory. These interfaces (e.g. to RenderTargets or Stream Output) are somewhat the converse of the Memory-to-Stage Interface. Each memory Resource representing one or more Elements of output identifies each Element by a number #, corresponding directly to an output o# register. There is no linkage resolving done on behalf of the application; the application must associate target memory for Element output directly with each o# register that will provide it. Details on specifying these associations are unique for the different Stage-to-Memory interfaces (RenderTargets, Stream Output).\n\nIf a Stage-to-Memory interface outputs more Elements or components within Elements than there are destination memory bindings to accommodate, the extra data is discarded. If a Stage-to-Memory interface outputs fewer Elements or components within Elements than there are destination memory bindings expecting to be written, undefined data will be output (i.e. no defaults). At RenderTarget output, there are various means to mask what data gets output, most interesting of which is depth testing, but that is outside the scope of this discussion.\n\nAt the RenderTarget output interface (which is Pixel Shader(16) output), dynamic indexing of the o# registers is not supported. For the other Stage-to-Memory interface, Stream Output, indexing of outputs is permissible. Stream Output shares the output o# registers used for Stage-to-Stage output in the Geometry Shader(13) Stage, where indexing is permitted as defined for the Stage-to-Stage interface.\n\nThere are various hardware generated values which can each be made available when for input to certain Shader Stages by declaring them for input to a component of an input register. A listing of each System Generated Value in D3D . can be found in the System Generated Value Reference(23), but in addition, here are links to descriptions of some (not all) of the System Generated Values: VertexID(8.16), InstanceID(8.18), PrimitiveID(8.17), IsFrontFace(15.12).\n\nIn the Hull Shader(10), Domain Shader(12) and Geometry Shader(13), PrimitiveID is a special case that has its own input register, but for all other cases of inputting hardware generated values into Shaders, (including the PrimitiveID into the Pixel Shader(16)), the Shader must declare a scalar component of one of its input v# registers as one of the System Generated Values to receive each input value. If that v# register also has some components provided by a the previous Stage or Input Assembler(8), the hardware generated value can only be placed in one of the components after the rest of the data. For example if the Input Assembler provides v0.xz, then VertexID might be declared for v0.w (since w is after z), but not v0.y. There cannot be overlap between the target for generated values and the target for values arriving from an upstream Stage or the Input Assembler.\n\nHardware generated values that are input into the generic v# registers can only be input into the first active Pipeline Stage in a given Pipeline configuration that understands the particular value; from that point on it is the responsibility of the Shader to manually pass the values down if desired through output o# registers. If multiple Stages in the pipeline request a hardware generated value, only the first stage receives it, and at the subsequent stages, the declaration is ignored (though a prudent Shader programmer would pass down the value manually to correspond with the naming).\n\nSince VertexID(8.16), InstanceID(8.18) are both meaningful at a vertex level, and IDs generated by hardware can only be fed into the the first stage that understands them, these ID values can only be fed into the Vertex Shader. PrimitiveID(8.17) generated by hardware can only be fed into the Hull Shader, Domain Shader, as well as whichever of the follwing is the first remaining active stage: Geometry Shader or Pixel Shader.\n\nIt is not legal to declare a range of input registers as indexable(22.3.30) if any of the registers in the range contains a System Generated Value.\n\nIn many cases, hardware must be informed of the meaning of some of the application-provided or computed data moving through the D3D . Pipeline, so the hardware may perform a fixed function operation using the data. The most obvious example is \"position\", which is interpreted by the Rasterizer (just before the Pixel Shader). Data flowing through the D3D . Pipeline must be identified as a System Interpreted Value at the output interface between Stages where the hardware is expected to make use of the data. For the case where the Input Assembler(8) is the only Stage present in a Pipeline configuration before the place where the hardware is expected to interpret some data, the Input Assembler(8) has a mechanism for identifying System Interpreted Values to the relevant (components of) Elements it declares.\n\nA listing of each System Interpreted Value in D3D . can be found in the System Interpreted Values Reference(24). Each System Interpreted Value has typically one place in the Pipeline where it is meaningful to the hardware. Also, there may be constraints on how many components in an Element need to be present (such as .xyzw for \"position\" going to the Rasterizer).\n\nIf data produced by the Input Assembler or by the output o# registers of any Stage is identified as a System Interpreted Value at a point in the pipeline where the hardware has no use for interpreting the data, the label is silently ignored (and the data simply flows to the next active Stage uninterpreted). For example if the Input Assembler labels the xyzw components of one of the Elements it is producing as \"position\", but the first active Pipeline Stage is the Vertex Shader, the hardware ignores the label, since there is nothing for hardware to do with a \"position\" going into the Vertex Shader.\n\nJust because data is tagged as a System Interpreted Value, telling hardware what to do with it, does not mean the hardware necessarily \"consumes\" the data. Any data flowing through the Pipeline (System Interpreted Value or not) can typically be input into the next Pipeline Stage's Shader regardless of whether the hardware did something with the data in between. In other words, output data identified as a System Interpreted Value is available to the subsequent Shader Stage if it chooses to input the data, no differently from non-System Interpreted Values. If there are exceptions, they would be described in the System Interpreted Value Reference(24). One catch is that if a given Pipeline Stage, or the Input Assembler, identifies a System Interpreted Value (e.g. \"clipDistance\"), and the next Shader Stage declares it wants to input that value, it must not only declare as input the appropriate register # and component(s), but also identify the input as the same System Interpreted Value (e.g. \"clipDistance\"). Mismatching declarations results in undefined behavior. e.g. Identifying an output o3.x as \"clipDistance\", but not naming a declared input at the next stage v3.x as \"clipDistance\" is bad. Of course, in this example it would be legal for the subsequent Shader to not declare v3.x for input at all.\n\nIt is not legal to declare a range of input or output registers as indexable(22.3.30) if any of the registers in the range contains a System Interpreted Value, with the exception of System Interpeted Values for the Tessellator, which have their own indexing rules - see the Hull Shader(10) specification.\n\nIn many cases in D3D . , an offset for an Element is required, a stride for a structure (e.g. vertex) is required, or an initial offset for a Buffer is required. All of these types of values have the following alignment restrictions:\n• The alignment of an Element in a structure in memory must be, in bytes, the nearest power of 2 greater or equal to the width of the Element's format, or , whichever is less. Thus alignment is always at 1 byte, 2 bytes or 4 byte granularity. This alignment is measured from the starting address of the structure. The starting address of the structure must also maintain alignment of all its members.\n• The magnitude of any initial offset into a 1D buffer, and/or the structure stride, must result in the Elements in the Buffer remaining aligned as per the previous rule.\n\nExample byte alignments for some of the formats(19.1) which can be used in structures (e.g. vertex buffers) or as elements in index buffers:\n\nHowever, these alignment rules do not apply to Buffer offsets when creating Views on Buffers. These Buffer offsets have more stringent requirements, detailed in the View section(5.2).\n\nThere is also some similar discussion, focused on memory accesses common to UAVs(5.3.9), SRVs and Thread Group Shared Memory in the Memory Addressing and Alignment Issues(7.13) section.\n\nNone of these rules are validated (except in debug mode) and violations will result in undefined behavior.\n\nSeveral different Resource Types (arrangements of memory storage) are available for input or output by various Pipeline stages. The available Resource Types are: Buffer(5.3.4) (Typically a Structured(5.1.3) or \"Unstructured(5.1.2) region of memory), Texture1D(5.3.5) (Homogeneous array of 1D Textures), Texture2D(5.3.6) (Homogeneous array of 2D Textures), Texture3D(5.3.7) (Volume Texture), and TextureCube(5.3.8) (3D enclosure). The Resource Type, in general, determines many characteristics, like whether the memory is Structured(5.1.3), where the Resource may be bound to in the graphics pipeline, how many mip levels there are, what the sampling behavior is, and other possible restrictions/properties on the Resource. Resources are built up of one or more Subresources, which each are a generalized 3D quantity of data which degenerates to store 2D and 1D quantities of data. The arrangement of Subresources to build up a Resource is tied to the Resource Type and dimensions.\n\nThere are also distinctions in how a Resource is bound to the graphics pipeline. The binding location can also be thought of as accepting either Buffers directly or accepting Views of Resources. Each binding location which accepts Views requires a unique View type for that location - e.g. Render Target View or Shader Resource View.\n\nThe size for mipmap slice subresources 1..n are computed sequentially from the size of the largest subresource (subresource 0, where for each mipped dimension:\n\nThe following diagram depicts Resources, their Subresource arrangement, and how they are sampled from within shaders. While the following diagram depicts deep mip mapping, it is valid to create Resources less than the maximum amount of mip levels.\n\nWhen a Resource is allocated, it's memory structure can generally be classified either as Unstructured, Prestructured+Typeless, or Prestructured+Typed.\n\nOnly the Buffer Resource(5.3.4) construction may be created as \"Unstructured\". Unstructured identifies the Resource as a single contiguous block of memory with no mipmaps, nor array slices. Unstructured Resources generally must have the memory structure defined when the Resource is bound to the graphics pipeline (providing types and offsets for the Element(s) in the Resource, as well as an overall stride). This memory structure can change freely, since it is late-bound to the Resource at the graphics pipeline binding location.\n\nThe same Unstructured Resource may be bound to multiple slots in the graphics Pipeline with different memory interpretations at each location, as long as the Resource is only being read from at each binding. The same Unstructured Resource may not be bound to read and write stages of the pipeline simultaneously for a single Draw/Dispatch operation.\n\nUnstructured Resources do not have mipmaps nor array slices. See the Resource Binding Table(5.3.1) for descriptions of where Buffers (the only Resources that can be Unstructured) can be bound in the Pipeline.\n\nOnly the Buffer Resource(5.3.4) construction may be created as \"Structured\". Structured identifies the Resource as a single contiguous block of memory with no mipmaps, nor array slices, but it does have a structure size (stride), so that it represents an array of structures. Implementations can take advantage of knowing there is a fixed structure size in they way they lay out the memory physically (hidden from the application).\n\nA number of application scenarios require the ability to write a structure of data out to an index in an array. E.g. Generating an unordered collection of output data in an Append buffer(5.3.10). Hardware may be optimized for smaller reads and writes than the stride of a data. Consider a group of 16 shader threads where each thread wants to write out the first 4 bytes of a structure. If the structure is only 4 bytes, the 16 threads will collectively write out 16 consecutive 32-bit locations, which tends to be fast. But if the structure is larger – say 64 bytes, then the 16 threads will each issue a write that is spaced 64 bytes apart. Then when reading the data back in a later pass, the same problem will be reoccur. Reads will be issued with a spacing equal to the stride of the structure, with larger structures likely to have more of a performance issue.\n\nDue to the reads and the writes having similar access patterns it would be better to have the data layout in memory match the access pattern that occurs. Since the actual access pattern is hardware specific as well as the performance characteristics of reads spaced by stride boundaries, the design pattern of textures is followed to allow for better performance by hiding the physical layout of the memory.\n\nThe same Structured Resource may be bound to multiple slots in the graphics Pipeline, as long as the Resource is only being read from at each binding. The same Structured Resource may not be bound to read and write stages of the pipeline simultaneously for a single Draw/Dispatch operation.\n\nStructured Resources do not have mipmaps nor array slices. See the Resource Binding Table(5.3.1) for descriptions of where Buffers (the only Resources that can be Structured) can be bound in the Pipeline.\n\nSometimes a convenient way to access the contents of a Buffer is to treat it simply as a huge bag of bits. The Raw view comes close to this, by allowing access to a Buffer in the form of 32-bit aligned addressing and accessing of data in chunks of 1-4 32-bit values, with no type.\n\nRaw access to a Buffer is indicated when creating either a Shader Resource View(5.2) (SRV) or Unordered Access View(5.3.9) (UAV), with the flag D3D11_BUFFER_SRV_FLAG_RAW (SRV) or D3D11_BUFFER_UAV_FLAG_RAW (UAV).\n\nTo be able to create a RAW View, the underlying resource had to have been created with D3D11_RESOURCE_MISC_ALLOW_RAW_VIEWS.\n\nThis flag cannot be combined with D3D11_RESOURCE_MISC_STRUCTURED_BUFFER. Also, a Buffer created with D3D11_BIND_CONSTANT_BUFFER cannot also specify D3D11_RESOURCE_MISC_ALLOW_RAW_VIEWS. This is not a limitation, since Constant Buffers already have a constraint that they cannot be accessed with any other View in the first place.\n\nOther than those invalid cases, specifying D3D11_RESOURCE_MISC_ALLOW_RAW_VIEWS when creating a Buffer does not limit any functionality versus not having it – e.g. the Buffer can be used for non-RAW access in any number of ways possible with D3D. Specifying the D3D11_RESOURCE_MISC_ALLOW_RAW_VIEWS flag only increases available functionality – it is just giving the system an early indication that the Buffer may participate in RAW style access in addition to other uses.\n\nAny Resource type may be created as \"Prestructured+Typeless\". A structure size is provided, plus bit widths of components (but not the types of those components), and also dimensions (in units of structures) appropriate for the Resource type. This is unlike a Structured Buffer, which only specifies a structure size/stride and no definition of the contents of the structure. Before the Resource is bound to the pipeline, Resource Views must be created which will fully qualify the component's types. These Resource Views also allow the Resource to be decomposed into smaller compatible subgroupings of the Subresources. For example, a fully mipped DXGI_FORMAT_R32G32B32A32_TYPELESS Texture3D with a width of four, a height of three, and a depth of five, would have three mip levels. To use this texture, a Resource View would have to fully qualify the format of the Resource, possible to DXGI_FORMAT_R32G32B32A32_UINT. In addition, the Resource View could also regroup only the two least detailed mip levels or select only a particular mip level. This allows the original Resource to be manipulated as if it were a Resource made up of only a few Subresources within the original Resource. The full details of Resource Views(5.2) is described later.\n\nThe benefit of Prestructured+Typeless Resources is that memory may be used as weakly typed storage, enabling limited reuse or reinterpretation of the memory, as long as the component bit counts remain the same. The same Prestructured+Typeless Resource may be bound to multiple slots in the graphics pipeline with Views of different fully qualified formats at each location. This forces bit representations of formats to be well-defined with respect to each other.\n\nFor example, a Resource created with the format R32G32B32A32_TYPELESS may be used as R32G32B32A32_FLOAT and R32G32B32A32_UINT at different locations in the pipeline simultaneously.\n\nAny Resource type may be created as \"Prestructured+Typed\", also known as creating the Resource with a fully-qualified type or format. In general, this may allow Resource optimizations, especially when the Resource is created with flags indicating that the Resource cannot be Mapped/ Locked by the application.\n\nSpecial resource formats, such as Block Compression Formats(19.5), have the characteristic that in order to read an individual Element in the resource, there is not a unique location in the resource that corresponds to the Element. Some sort of decompression or decoding of data from locations in the resource that are not unique to a particular Element is required during the read process in order to resolve what an individual Element is (even when no filtering is being applied). Complex formats like this must be created as part of a \"Prestructured+Typed\" resource.\n\n\"Prestructured+Typed\" and \"Prestructured+Typeless\" resources support mipmapping, as the combination of Resource type, dimensions and structure size provided during resource creation supply enough information to allocate all memory in the layout required. Additionally, Resource Views created against Prestructured+Typed Resources must have indentical Resource Formats as the Prestructured+Typed Resource.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 5.2.1 Overview\n\n 5.2.2 Shader Resource View Support for Raw and Structured Buffers\n\n 5.2.3 Clearing Views\n\n\n\nIn order to indirectly bind a Resource to certain stages of the graphics pipeline, Resource Views must be used. In addition, since some Resources may be created as \"Prestructured+Typeless\", the View provides the final opportunity to fully qualify the Resource component's types. The Resource Views also allow the Resource to be decomposed into smaller compatible subgroupings of the Mip Slices, Array Slices, and Subresources. This means that the effective dimensions and array sizes of the Views will, naturally, always be less than or equal to the original Resource. Each stage of the pipeline requires a unique type of View, and each type of View may have it's own custom set of state parameters that are needed to complete the process of binding a particular Resource to the graphics pipeline stage. All necessary restrictions to the basic Resource have already been done through the Pipeline Bind Flags during Resource creation. These Resource Views are directly bound to the pipeline, instead of the Resource objects, themselves.\n\nA resource view is distinct from the underlying resource from which the view was created, so where views are used, the view properties (number of mipmaps, number of array elements, type, etc.) are always used in place of the properties of the original resource. Thus, for example, a render target array index of zero always indicates the first array element in the view, even if the first array element in the view is not the first array element in the underlying resource. Out of range behaviors are also always with respect to the view properties where views are used.\n\nEach unique View type has certain restrictions associated with the bind location of the graphics pipeline stage. For example, Render Target Views of Buffers may have a maximum width of . This maximum is smaller than the maximum size of a Buffer (min(max( , * (Amount of Dedicated VRAM)), ) MB), so only a subsection of large Buffers may be bound as a Render Target at a time. In addition, Render Target Views of Texture3D may have a maximum array size of . This fortunately matches the maximum W dimension size of a Texture3D ( ).\n\nWhen Views are created of Buffers, restrictions are placed on the View's starting offset in the Buffer. If represented as a byte offset, the offset must be a multiple of the View Element Size. Another way to comply with this restriction is by specifying the Buffer offset in an integral number of View Elements. In addition, there exists another restriction on Buffer View creation. Views of the R32G32B32 element type cannot be created on a Buffer which had the Pipeline Bind flag of IAVERTEXINPUT, IAINDEXINPUT, CONSTANTBUFFER, or STREAMOUTPUT set. This prevents an R32G32B32 element from being used simultaneously as vertex and texture data.\n\nTo characterize the kind of decomposition that Shader Resource Views are capable of, here's a complete listing of the number of Views that are possible with a Texture2D Resource that was created fully mipped with the most detailed LOD: width = 4, height = 4, arraysize = 3.\n\nThe Views bound at the Render Target, Depth Stencil and Unordered Access binding locations in the pipeline have futher restrictions, in that they can only choose a Mip Slice, aka. select only one mip level. Here's a listing of the possible decomposition that can occur with Render Target, Depth Stencil and Unordered Access Views of the same Resource used in the previous example:\n\nThe following DDIs indicate the way Shader Resource Views (SRVs) are created, allowing read-only access to Raw and Structured Buffers in any shader stage.\n\nMaking an SRV of a Raw buffer allows it to be declared for read in any shader stage by the ld_raw instruction. This is accomplished by specifying a flag on creation of the Buffer View requesting Raw access (D3D11_DDI_BUFFEREX_SRV_FLAG_RAW) shown below.\n\nIn contrast, if the underlying Buffer was created as a Structured Buffer, then any SRV of the Buffer inherits the Structured semantics. In this case all shader stages can declare the resource for read by the ld_structured instruction. Note that unlike _RAW views (where the View decides that the Buffer will be \"viewed\" as RAW), nothing about the creation of a View of a Structured Buffer needs to indicate that it is structured, because once the Structured property is assigned to a Buffer on creation of the resource (including a structure stride), all Views on the Buffer are automatically Structured.\n\nClearing is an optimized operation to enable filling Render Target, Depth Stencil and Unordered Access Views with certain clear values.\n\nThe floating point values passed in through the DDI must be converted to the fully qualified format type of the View desired to be cleared. The standard type conversion rules(3.2) indicate how to convert to most values; but these conversion rules do not explicitly handle the case where the destination fixed point format contains more integer bits than the floating point format mantissa. When converting these floating point values to a format such as DXGI_FORMAT_R32G32B32A32_UINT or _SINT, the closest value is chosen. When the original floating point absolute value is larger than 2^24, the least significant bits of the destination are to be filled with 0's for _UINT and positive _SINT; or 1's for negative _SINT values.\n\nThe full extent of the resource view is always cleared. Viewport and scissor are not applied.\n\nDepth clear values outside of the range specified in viewport range(15.6.1) will not be passed to the DDI.\n\nFor UnorderedAccessViews(5.3.9), there are a couple of ways to Clear the View.\n\nClearUnorderedAccessViewUint(...) clears a UAV with bit-precise values, copying the lower ni bits from each array element i to the corresponding channel, where ni is the number of bits in the ith channel of the resource Format (for example, R8G8B8_FLOAT has 8 bits for the first 3 channels). This works on any UAV with no format conversion. For RAW Buffer and Structured Buffer Views, only the first array element’s value is used.\n\nClearUnorderedAccessViewFloat(...) clears a UAV with a float value. It only works on FLOAT, UNORM, and SNORM UAVs, with format conversion from FLOAT to *NORM where appropriate. On other UAVs, the operation is invalid and the call will not reach the driver.\n\nView clearing command, implemented however the driver sees is the most efficient way. The primary distinction here versus the other Clears described above in D3D11 is that this takes a list of rects (an empty list clears the entire surface). This method only works on RTV, UAV, or any Video View of a Texture2D surface (runtime drops invalid calls). All array slices in the view get the same clear applied (any rects apply to each array slice).\n\nThe driver or hardware is responsible for clamping rects to the surface extents.\n\nColor values are converted/clamped to the destination format as appropriate per D3D conversion rules. E.g. if the format of the view is R8G8B8A8_UNORM, inputs are clamped to 0.0f to 1.0f (NaN to 0).\n\nIf the format is integer, such as R8G8B8A8_UINT, inputs are taken as integral floats, so 235.0f maps to 235 (fractions rounded to zero, out of range/INF values clamped to target range, NaN to 0).\n\nFor Video Views with YUV or YCbBr formats, no color space conversion happens – and in cases where the format name doesn’t indicate _UNORM vs. _UINT etc., _UINT is assumed (so input 235.0f maps to 235 as described above).\n\nThis feature is required to be supported for all D3D10+ hardware in D3D11.1 drivers and for D3D9 drivers maps to the already existing functionality there. The D3D9 equivalent honored the scissor rect, so emulation of ClearView on the D3D9 DDI will unset scissor / clear / reset scissor to achieve the intended behavior of ClearView (e.g. this scissor manipulation isn't needed on the new D3D11.1 ClearView DDI which ignores scissor/viewports by definition.).\n\nFor RTVs and UAVS: The space the ClearView rects apply on is that of the view format (as opposed to the surface format, which for video surfaces can be different sizes). This is consistent with how Viewports and rendering work on those views. e.g. for a 64x64 YUYV surface, an RTV with the format R8G8B8A8_UINT appears in shaders (and to RSSetViewports()) as having dimensions 32x64 RGBA values. ClearView’s rects apply to the same space. The “color” coming into ClearView is just maps to the channels in the view (RGBA) ignoring the video layout. So a single clear color could really mean “stripes” of color if interpreted in video space. That’s not interesting to do, but it just falls out and isn’t worth bothering to validate out – the user who makes D3D views of video surfaces has to know they are operating on the raw memory via D3D – be it shaders or APIs like ClearView.\n\nBy contrast, ClearView on Video Views (the views that are used with the video pipeline and not D3D Rasterization) operate on logical surface dimensions. So a 64x64 YUYV surface appears as though it is that size, and so rects passed into ClearView are in that full 64x64 space (not 32x64). It is undefined to request clearing non-aligned rects (covering only half of the pixel pairs). The color passed into ClearView is just a single YUV value that is appropriately replicated for subsampled pixels by the driver. Video Views hide the memory layout from the API user, so they do not have to worry about what type of subsampling is going on (an exception is the alignment of the rect bounds).\n\nAll Resources must be qualified with a set of Pipeline Bind flags at creation time to indicate where in the graphics pipeline the Resource may be bound. Binding a Resource at a certain pipeline location imposes certain restrictions on the Resource for it's entire lifetime. Naturally, Resources may be bound at more than one location in the pipeline (even simultaneously within certain restrictions), but the Resource must satisfy all the restrictions that each Pipeline Bind flag imposes. Certain pipeline locations only accept Resource Views(5.2) to be bound to them. In such a case, the presence of the Pipeline Bind flag indicates that Resource Views can be created against the Resource in order to bind the Resource to such a pipeline location. Sometimes Pipeline Bind flags impose restrictions which conflict with each other, so such Pipeline Usage flags are naturally mutually exclusive. Otherwise, explicit mention is given when one Pipeline Bind flag prevents the usage of other Pipeline Bind flags.\n\nThe following table indicates which Resource Types may be bound to which available graphics Pipeline locations. A single entire Resource may not be able to have itself bound entirely to both an input and output Pipeline stage during a Draw operation. However, it is possible to refer to discrete components of the Resource, with Resource Views(5.2), allowing the same Resource to be bound as an input and output simultaneously, as long as the different Views do not share the same Subresources. For example: A two-dimensional mipped Resource created with the appropriate Pipeline Bind flags may have Subresources bound as Shader Resource Inputs, and a mutually exclusive Subresource from the same Resource bound as a RenderTarget Output, by using different Views.\n• U = The entire Resource may be bound (in its entirety) at each slot at this stage, as this slot accepts Unstructured Buffers.\n• V = A View of the Resource may be bound at each slot at this stage, allowing usage of the entire Resource or groupings of the Subresources.\n\nAny Resource that is used as an output for the graphics pipeline cannot be mapped/ locked. This is not meant to block an application from viewing the contents of such a Resource. It is expected that to read the contents of such Resources in a performant manner, the contents must be copied to a Resource which is able to be mapped/ locked for CPU read access. Typically, the Resource which is able to be mapped/ locked will not be marked with any Pipeline Bind flags, and as such is expected to be a driver allocated system memory Resource which is allocated in such a fashion to be compatible with the hardware DMA engine. The Resource is also expected to be allocated for performant CPU reads. This enables an asynchronous performant read back for the CPU.\n\nThe Performant Readback(5.3.2) scenario highlights the need that for any device-dependent memory arrangement, used to optimize GPU Resources which cannot be mapped/ locked, there is always a performant ability to convert the memory arrangement into the device-independent memory arrangement that will be used to satisfy the map/ lock. This principle also relates to input Resources that cannot be mapped/ locked. Since non-mappable/ non-lockable input Resources may use a device-dependent memory arrangement and still be updated with UpdateSubresourceUP(5.6.8), CopyResource(5.6.3), and CopySubresourceRegion(5.6.2). Therefore, there is a need for a performant ability to convert the device-indepenedent memory arrangement into any device-dependent memory arrangement.\n\nThe Buffer is the only Resource which can be created as Unstructured(5.1.2). When the Buffer is bound to the graphics Pipeline, it's memory interpretation generally must also be bound to the graphics Pipeline along with it (providing types and offsets for the Element(s) in the Resource, as well as an overall stride). Sometimes this information is bound or described separately.\n\nA Buffer has neither multiple mip levels nor multiple array slices, so a Buffer is made up of only a single Subresource. Buffers can be bound at multiple places in the pipeline simulatenously during a Draw call as long the Buffer is only read from at each location. If the Buffer is being written to, then the Buffer may only be bound to one location in the pipeline during a Draw call.\n\nWhen a Buffer has been created with the Pipeline Bind flag indicating that it may be used as an Input Assembler Vertex Input, the Buffer may be contain multiple types of data per vertex. This data type, offset, and stride binding is done when the Resource is bound to the Pipeline.\n\nWhen a Buffer has been created with the Pipeline Bind flag indicating that it may be used as an Input Assembler Index Input, and the Buffer is bound as an Index Input, at the time of binding, the format must be specified as one of: R16_UINT, or R32_UINT.\n\nWhen a Buffer has been created with the Pipeline Bind flag indicating that it may be used as a Shader Constant Input, the format of the Buffer is assumed to be R32G32B32A32_TYPELESS when bound as a Shader Constant Input. The Buffer size viewable from a shader is restricted to hold a maximum of elements. The overall buffer size can be larger - see Offsetting Constant Buffer Bindings(5.3.4.3.2). The usage of Constant Buffers within the shaders is expected to make Shader execution more efficient than using ld(22.4.6) or sample(22.4.15) with a Shader Resource within the Shader. Constant Input is read into a Shader given an integer array index to fetch a single Element. This is similar to point sampling of a texture; as there is no filtering. Constant Input is only needed to store Shader constants which could change between Draw() calls, as opposed to Immediate Constants or an Immediate Constant Buffer, which is are embedded into a Shader.\n\nA Shader Constant Resource is expected to be optimized for moving constant data from the CPU to the graphics adapter, and as such, may not be able to be mapped/ locked, allowing the CPU to read the contents of the Buffer directly. Therefore, the Resource may only be CPUWRITE (write-only) or not mappable/ lockable. In addition, if the Resource is mappable/ lockable, Map/ Lock must be called with DISCARDRESOURCE. NOOVERWRITE is not valid on Shader Constant Resources either. The Resource may still be used with CopyResource(5.6.3) and CopySubresourceRegion(5.6.2). All other Pipeline Bind flags are prevented from being used, disallowing constant buffers to be vertex buffers, streamed out to or rendered to, etc.\n\nMap() allows NO_OVERWRITE for Constant Buffers. This was disallowed before D3D11.1.\n\nSimilarly, UpdateSubresource1() adds the ability to perform partial Constant Buffer updates. So the pDstBox parameter does not have to be null NULL when updating Constant Buffers via UpdateSubresource1(). Either NO_OVERWRITE or DISCARD flags must be specified for a partial update, and the extents of the pDstBox parameter must be aligned to 16 byte (full constant) boundaries or the call is dropped.\n\nBefore the first call with NO_OVERWRITE on a deferred context, a DISCARD must be done on the same context (via Copy*()/Update*()/Map() API flag or Discard*() API). This is not required on immediate contexts if the application knows the GPU is finished with the resource (though discard can be used if not).\n\nThis feature is required to be supported for all D3D10+ hardware with D3D11.1 drivers.\n\nConstant Buffers are allowed to be created larger than the maximum Constant Buffer size that an individual shader can reference, which is at most 16-byte elements - 65kB. Each \"element\" is one 4-component Shader Constant.\n\nThe Constant Buffer Resource size is limited only by the size of memory allocation the system is capable of handling (limits defined elsewhere, and more than large enough for the purpose of the discussion here).\n\nWhen a Constant Buffer larger than elements in size is bound to the pipeline via *SetShaderConstants() APIs [e.g. VSSetShaderConstants()], it appears to the shader as if it is only elements in size.\n\nVariants of the *SetShaderConstants() APIs, *SetShaderConstants1() allow a \"FirstConstant\" and \"NumConstants\" to be specified along with the binding. When the shader accesses a Constant Buffer bound this way it will appear as if it starts at the specified \"FirstConstant\" offset (where 1 means 16 bytes) and has a size defined by NumConstants (number of 16 byte Constants). This is basically a lightweight \"View\" of a region of a larger Constant Buffer.\n\nFirstConstant must be a multiple of 16 constants.\n\nNumConstants must be a multiple of 16 constants, in the range [0..4096].\n\nIf any part of the range defined by FirstConstant and ConstantCount falls off the underlying resource, accesses to those addresses count as out of bounds reads from the shader, which is defined to return 0 for all components.\n\nThis feature is required to be supported for all D3D10+ hardware in D3D11.1 drivers and is emulated by the runtime on Feature Level 9_x running on D3D9 drivers.\n\nWhen a Buffer has been created with the Pipeline Bind flag indicating that it may be used as a Shader Resource Input and it is a typed Buffer (the view specifies a format type), it may be read from within shaders with the load(22.4.6). See the description of this instruction for detail. To use a typed Buffer as a Shader Resource Input, it must be bound at one of the available slots for input Resources, by first creating the appropriate View for this particular stage of the graphics pipeline. It is fine for the same Buffer to be bound to multiple slots simultaneously, possibly even with different Element formats or inital offsets. However at each binding, only a single Element type is permitted, and the data stride is implied to be equal the Element size. In other words, \"Array-of-structure\" style layouts cannot be described for typed Buffers bound at Shader Resource Input. Structured Buffers allow array-of-structures access, though without any automatic format conversion for elements.\n\nJust like Typed Buffers, Raw and Structured Buffers can be bound to the pipeline via Shader Resource Views for reading into shaders via ld_raw(22.4.10) and ld_structured(22.4.12) instructions, respectively.\n\nDetails of the usage of such a Resource are described in the Streaming Output section(14). There are two types of bindings available for Stream Output Buffers, one that treats a single output Buffer as a Multiple-Element Buffer (array-of-structures), while the other permits multiple output Buffers each treated as Single-Element Buffers (structure-of-arrays). Single-Element Buffer output is expected to be used typically for recirculation (subsequently) as a Shader Resource Input, but this can also be used as Input Assembler Vertex Input. Multiple-Element Buffer output is only intended to be used for recirculating data (subsequently) back as Input Assembler Vertex Input (since Multiple-Element Buffer access is not currently available in Shaders).\n\nIf the Resource has the Input Assembler Vertex Input Pipeline Bind flag specified, the Resource may also be used with DrawAuto(8.9).\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nWhen a Buffer has been created with the Pipeline Bind flag indicating that it may be used as a RenderTarget Output, this Pipeline Bind flag indicates that Render Target Views may be created with this Resource.\n\nConstraints when a Buffer is used as RenderTarget output: it cannot be paired with any Depth/Stencil Output (i.e. no depth buffering); it can only have a single Element defined, with a data stride implied to be equal to the Element width; the View is limited to a maximum width of (multiple Views with different offsets would be needed to leverage the entire Buffer). In all other regards, a Buffer render target output is identical to the Texture1D case.\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nWhen the Unordered Access Pipeline Bind has been indicated, Unordered Access Views may be created for use at the Compute Shader or Pixel Shader.\n\nA Texture1D is a homogeneous array of 1D Textures. The array is homogeneous in the sense that each Texture has the same data format and dimensions (including miplevels). The entire array of Textures are created atomically. The memory for the entire Resource need not be contiguous. A Texture1D may not be created as Unstructured(5.1.2), but may be created as Prestructured+Typeless Memory(5.1.5) or as Prestructured+Typed Memory(5.1.6). As illustrated by the diagram(5) and binding configurations(5.3.1), a Texture1D may be decomposed into sub-groups of Mip Slices, Array Slices, and Subresources in order to refer to discrete components of the Resource to accomplish certain operations. The decomposition for graphics Pipeline Binding is achieved through the usage of Views for each stage of the pipeline.\n\nLike other Resources, a Texture1D must be qualified with a set of flags at creation indicating where in the graphics pipeline the Resource may be bound. Naturally, the Resource may be bound at more than one location in the pipeline, but the Resource must've been created with the restrictions that each Pipeline Usage flag indicates. Sometimes Pipeline Bind flags have restrictions which conflict with each other, so such Pipeline Bind flags are mutually exclusive.\n\nWhen the Texture1D has been created with the Pipeline Bind flag indicating that it may be used as a Shader Resource Input, the Texture1D Resource may be read from within shaders with the ld(22.4.6) or sample(22.4.15) instructions, after they are bound to the pipeline through the usage of Views. See the descriptions of these instructions for details. Each Element from a Texture1D to be read into a Shader counts towards a limit on the total number of elements addressable from Resources ( ). Texture1D Resources are addressed from the Shader with a 1D coordinate plus a 2nd coordinate specifying which Array Slice in the Texture1D to fetch from. The 2nd coordinate, if provided as floating point data, is rounded (nearest even), producing an integral array index. Typical 1D filtering occurs on the Array Slice chosen by the 2nd coordinate.\n\nWhen a Texture1D Mip Slice is bound as a RenderTarget Output, through the usage of Views, it is allowable to use either an accompanying Texture1D Depth/ Stencil of the same dimensions. For example, if the most detailed Mip Slice View of a Texture1D (width=6, arraysize=8) is bound as a RenderTarget Output; an effective Texture1D View of (width=6, arraysize=8) may be used as a Depth/ Stencil. Also, the particular Array Slice in the Texture1D to render is chosen, from the Geometry Shader stage, by declaring a scalar component output data as the System Interpreted Value \"renderTargetArrayIndex\". If such a value is not present in primitive data reaching the rasterizer, the default is to render to Array Slice .\n\nRasterization to Texture1D resources is identical to rasterizing to a Texture2D resource with a y dimension of 1, thus both x and y coordinates are honored and only rendering that covers the Nx1 area of these resources will update them.\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nWhen the Texture1D has been created with the Pipeline Bind flag indicating that it may be used as a Depth/ Stencil Output, the Texture1D Resource may only be one of a few Resource Formats (essentially only those which have a 'D' component or those TYPELESS formats which can be converted to a format with a 'D' component), such as D32_FLOAT or R32_TYPELESS, etc.\n\nResources created with this Pipeline Bind flag cannot also be used as a RenderTarget (the two flags are mutually exclusive).\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. In addition, Depth/ Stencil Resources cannot be a destination for CopyResource(5.6.3), CopySubresourceRegion(5.6.2), nor UpdateSubresourceUP(5.6.8) operations. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nA Texture2D is a homogeneous array of 2D Textures. The array is homogeneous in the sense that each Texture has the same data format and dimensions (including miplevels). The entire array of Textures are created atomically. The memory for the entire Resource need not be contiguous. A Texture2D may not be created as Unstructured(5.1.2), but may be created as Prestructured+Typeless Memory(5.1.5) or as Prestructured+Typed Memory(5.1.6). As illustrated by the diagram(5) and binding configurations(5.3.1), a Texture2D may be decomposed into sub-groups of Mip Slices, Array Slices, and Subresources in order to refer to discrete components of the Resource to accomplish certain operations. The decomposition for graphics Pipeline Binding is achieved through the usage of Views for each stage of the pipeline.\n\nLike other Resources, a Texture2D must be qualified with a set of flags at creation indicating where in the graphics Pipeline the Resource may be bound. Naturally, the Resource may be bound at more than one location in the Pipeline, but the Resource must've been created with the restrictions that each Pipeline Bind flag indicates. Sometimes Pipeline Bind flags have restrictions which conflict with each other, so such Pipeline Bind flags are mutually exclusive.\n\nWhen the Texture2D has been created with the Pipeline Bind flag indicating that it may be used as a Shader Resource Input, the Texture2D Resource may be read from within shaders with the ld(22.4.6) or sample(22.4.15) instructions, after they are bound to the pipeline through the usage of Views. See the descriptions of these instructions for details. Each Element from a Texture2D to be read into a Shader counts towards a limit on the total number of elements addressable from Resources ( ). Texture2D Resources are addressed from the Shader with a 2D coordinate plus a 3rd coordinate specifying which Array Slice in the Texture2D to fetch from. The 3rd coordinate, if provided as floating point data, is rounded (nearest even), producing an integral array index. Typical 2D filtering occurs on the Array Slice chosen by the 3rd coordinate.\n\nWhen a Texture2D Mip Slice View is bound as a RenderTarget Output, through the usage of Views, it is allowable to use either an accompanying effective Texture2D Depth/ Stencil View of the same dimensions. For example, if the most detailed Mip Slice View of a Texture2D (width=6, height=4, arraysize=8) is bound as a RenderTarget Output; an effective Texture2D View of (width=6, height=4, arraysize=8) may be used as a Depth/ Stencil. Also, the particular Array Slice in the Texture2D to render is chosen, from the Geometry Shader stage, by declaring a scalar component of output data as the System Interpreted Value \"renderTargetArrayIndex\". If such a value is not present in primitive data reaching the rasterizer, the default is to render to Array Slice .\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nWhen the Texture2D has been created with the Pipeline Bind flag indicating that it may be used as a Depth/ Stencil Output, the Texture2D Resource may only be one of a few Resource Formats (essentially only those which have a 'D' component or those TYPELESS formats which can be converted to a format with a 'D' component), such as D32_FLOAT or R32_TYPELESS, etc.\n\nResources created with this Pipeline Bind flag cannot also be used as a RenderTarget (the two flags are mutually exclusive).\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. In addition, Depth/ Stencil Resources cannot be a destination for CopyResource(5.6.3), CopySubresourceRegion(5.6.2), nor UpdateSubresourceUP(5.6.8) operations. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nA Texture3D is a 3D grid data layout, supporting mipmaps; and is also known as a Volume Texture. The entire Resource is created atomically. The memory for the entire Resource need not be contiguous. A Texture3D may not be created as Unstructured(5.1.2), but may be created as Prestructured+Typeless Memory(5.1.5) or as Prestructured+Typed Memory(5.1.6). As illustrated by the diagram(5) and binding configurations(5.3.1), a Texture3D may be decomposed into sub-groups of Mip Slices, Array Slices, and Subresources in order to refer to discrete components of the Resource to accomplish certain operations. The decomposition for graphics Pipeline Binding is achieved through the usage of Views for each stage of the pipeline.\n\nWhen the Texture3D has been created with the Pipeline Bind flag indicating that it may be used as a Shader Resource Input, the Texture3D Resource may be read from within shaders with the ld(22.4.6) or sample(22.4.15) instructions, after they are bound to the pipeline through the usage of Views. See the descriptions of these instructions for details. Each Element from a Texture3D to be read into a Shader counts towards a limit on the total number of elements addressable from Resources ( ). Texture3D Resources are addressed from the Shader with a 3D coordinate. Typical 3D filtering occurs with this coordinate.\n\nWhen a Texture3D Mip Slice is bound as a RenderTarget Output, through the usage of Views, the Texture3D behaves identically to a Texture2D with n Array Slices where n is the depth (3rd dimension) of the Texture3D. The particular z slice in the Texture3D to render is chosen, from the Geometry Shader stage stage, by declaring a scalar component of output data as the System Interpreted Value \"renderTargetArrayIndex\". If such a value is not present in primitive data reaching the rasterizer, the default is to render\\ to z= .\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nA TextureCube has 6 faces, each of which is like a square Texture2D, including mipmaps. The entire Resource is created atomically. The memory for the entire Resource need not be contiguous. A Texture3D may not be created as Unstructured(5.1.2), but may be created as Prestructured+Typeless Memory(5.1.5) or as Prestructured+Typed Memory(5.1.6). As illustrated by the diagram(5) and binding configurations(5.3.1), a TextureCube may be decomposed into sub-groups of Mip Slices, Array Slices (each representing a face), and Subresources in order to refer to discrete components of the Resource to accomplish certain operations. The decomposition for graphics Pipeline Binding is achieved through the usage of Views for each stage of the pipeline.\n\nTextureCubes can also represent an array of cubes, which means a multiple of 6 faces. Used as a Cube Array, the \"array\" dimension selects which Cube to use. However, the same resource can also be viewed as a 2D Array, in which case each face of each Cube appears as a single location along the \"array\" dimension.\n\nWhen the TextureCube has been created with the Pipeline Bind flag indicating that it may be used as a Shader Resource Input, the TextureCube{Array} Resource may be read from within shaders after they are bound to the pipeline through the usage of Views. The View can expose the TextureCube{Array} as an array of TextureCubes starting from any face (from the perspective of a sequence of 2D faces), then spanning a multiple of 6 faces, such that each 6 faces appears as a location on the array axis. Alternatively, the TextureCube can be viewed as a 2D Array spanning any contiguous set of faces in the resource where each face is a slice, hiding the \"Cube-ness\" of the resource. Each Element from a TextureCube resource to be read into a Shader counts towards a limit on the total number of elements addressable from Resources ( ). TextureCube Resources viewed as a Cube are addressed from the Shader with a 3D vector pointing out from the center of the TextureCube, and as a Cube Array, an additional coordinate provides the Array Slice. If the Array Slice is provided as a floating point number, is is rounded to nearest even.\n\nWhen a TextureCube{Array} Mip Slice is bound as a RenderTarget Output, the TextureCube behaves identically to a Texture2DArray, such that any contiguous subset of the faces in the array participate in the View. The particular Array slice in the View to render to is chosen from the Geometry Shader stage, by declaring a scalar component of output data as the System Interpreted Value \"renderTargetArrayIndex\". If such a value is not present in primitive data reaching the rasterizer, the default is to render to Array Slice .\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nWhen the TextureCube{Array} has been created with the Pipeline Bind flag indicating that it may be used as a Depth/ Stencil Output, the Resource may only be one of a few Resource Formats (essentially only those which have a 'D' component or those TYPELESS formats which can be converted to a format with a 'D' component), such as D32_FLOAT or R32_TYPELESS, etc. In addition, when rendering using such a Depth/ Stencil TextureCube (viewed as a Texture2DArray Depth Stencil View), only equally sized RenderTarget Views are compatable for use as a RenderTarget Output.\n\nResources created with this Pipeline Bind flag cannot also be used as a RenderTarget (the two flags are mutually exclusive).\n\nSince this is an output stage, Resources with this Pipeline Bind flag are not able to be mapped/ locked for CPU access ever. In addition, Depth/ Stencil Resources cannot be a destination for CopyResource(5.6.3), CopySubresourceRegion(5.6.2), nor UpdateSubresourceUP(5.6.8) operations. This doesn't prevent Resources completely from being viewed by the CPU, as there are performant(5.3.2) methods for viewing the contents of the Resource.\n\nUnordered Access Views (UAVs) can be bound at the Output Merger(17) (available to all graphics shader stages from there) and Compute Shader(18) stage.\n\nAt the Output Merger, there is the constraint that the total of the number of o# slots (Render Target Views - RTVs) and u# slots (UAVs) that may be bound simultaneously is at most , where no more than can be RTVs. The way this is enforced, for simplicity, is that all o# (RTV) slots that are declared must have a slot # that is less than the minimum # of the u# (UAV) slots that are declared. So it is valid for a Pixel Shader to declare o0, o1, u4 and u63, but it is not valid for a Pixel Shader to declare o0, u3, and o4.\n\nSeparating o# from u# this way minimizes future dependence on the fact that they happen to live in the same bind space in D3D11, if that turns out not to be desirable.\n\nThe UAVs bound at the Output Merger are visible to all graphics stages (a shared set of UAV bindings). So multiple graphics shader stages can access the same UAVs simultaneously.\n\nCertain shader stages, like the Vertex Shader or Domain Shader (with Tessellation), are implemented by hardware using shader result caches. So if nearby primitives share the same vertex, the results of the corresponding shader invocation for that vertex may be retrieved from a result cache rather than re-executing the shader. The presence of these result caches and their behavior is hardware specific. Previously, without the ability for the unique shader invocations to have side-effects, the user had no way of knowing or depending on any caching taking place, beyond observing some performance wins if the caching worked well. With UAVs available to all shaders (enabling shaders to write arbitrarily to the UAV memory), any hardware-specific shader result caching will be visible, and the burden is left to the application developer to avoid depending on any given hardware's behavior. In particular, the behavior of such caching would not take into account any UAV accesses that take place; the hash key for shader result caching is simply the inputs for a given shader invocation independent of what may be read from UAVs during the shader invocation (which may not occur at all if there is a cache hit).\n\nThere is no guarantee that UAV accesses issued from within or across shader stages executing within a given Draw*(), or issued from the Compute Shader within Dispatch*(), finish in the order issued. All UAV accesses are finished at the end of the Draw*()/Dispatch*() though.\n\nThe Compute Shader has its own separate set of slots where only UAVs may be bound, independent of the set of RTV+UAV bindpoints for the graphics stages.\n\nDynamic indexing of UAV registers (i.e. dynamically indexing # in u#) is not permitted.\n\nShader Instructions (defined elsewhere) which are accessing UAVs simply take a u# as a parameter, much like instructions that are sampling from textures take a t# as a parameter.\n\nThe D3D11 Resource types that can have a UAV on them are Texture1D{Array}, Texture2D{Array}, Texture3D and Buffer. When the Resource is created at the API/DDI, the bind flag D3D11_{DDI_}BIND_UNORDERED_ACCESS must be specified in order for subsequent creation of UAVs on the resource to be valid.\n\nThe D3D11_BIND_UNORDERED_ACCESS flag may be combined with any of the following bind flags:\n\nThe D3D11_BIND_UNORDERED_ACCESS flag may NOT be combined with any of the following bind flags:\n• D3D11_BIND_STREAM_OUTPUT // Unordered Access Buffers imply some hidden storage for counters, as do Stream Output Buffers – so to simplify matters, both usages are not allowed to be mixed.\n\nThe constraints combining D3D11_BIND_UNORDERED_ACCESS with other flags on Resource Creation, such as Usage (dynamic, staging etc) are the same as existing constraints present specified for D3D11_BIND_RENDER_TARGET.\n\nThe Sample Count on the resource must be 1, and the Sample Quality must be 0.\n\nNote in the DDI, the names above become D3D11_DDI_BIND_*.\n\n5.3.9.2 Creating an Unordered Access View (UAV) at the DDI\n\nThe Format parameter must be compatible with the format the Resource was created with, and can be any format that supports being bound at the RenderTarget except for SRGB formats. Additional restrictions on the Format for Buffer views are discussed shortly below.\n\nThe D3D11DDIARG_*_UNORDEREDACESSVIEW parameters, describing the view parameters based on resource dimension, are as follows:\n\nThe _RAW_FLAG allows the shader to access the buffer simply as a 1D array of untyped 32-bit data. The Format must be specified as R32_TYPELESS when this flag is used. The underlying Buffer must have been created with D3D11_DDI_MISC_FLAG_ALLOW_RAW_VIEWS (D3D11_MISC_FLAG_ALLOW_RAW_VIEWS at the API).\n\nThe _STRUCTURED flag (mutually exclusive to _RAW) requires that the Buffer was created as a Structured Buffer. The Format for a structured buffer must be specified as DXGI_FMT_UNKNOWN. The type information for the structured buffer will be inherited from the buffer resource.\n\nThe absence of _RAW and _STRUCTURED flags means the Buffer View is Typed, so the Format of the view can be specified as freely as any with other UAV dimension (1D, 2D, 3D).\n\nWhen a UAV or SRV is Raw, the FirstElement parameter (defining the start of the view) must result in a 128bit aligned offset, otherwise the creation of the View will fail. Knowing the base address of a view is conveniently aligned enables various optimizations/assumptions in hardware given accesses from a shader that are offsets from the base of the view (where the offsets are often literals in the shader).\n\n5.3.9.3 Binding an Unordered Access View at the DDI\n\nThe D3D11 OMSetRenderTargets API/DDI accepts both RenderTargetViews, DepthStencilView, and UnorderedAccessViews at the same time. This affects the Graphics side of the pipeline, not the Compute side. Here is the DDI:\n\nThere is a separate CSSetUnorderedAccessViews API/DDI that accepts UnorderedAccessViews to be bound for the Compute side of the device. It is similar to the above, except doesn’t include RenderTargets.\n\nThe last two parameters, UAVRangeStart and UAVRangeSize exist at the DDI level and not at the OMSetRenderTargets API level. The Direct3D 11 runtime tracks the set of bound UAVs which have changed (which may be different from the set of bound UAVs overall) whereby the driver may use this information for optimization purposes.\n\nUAVs have the same precedence in Hazard Tracking as RTVs and SO Targets:\n• If the same subresource is being set to multiple bind points in the set of all RTVs, UAVs and the DSV being set, the entire call is ignored (similarly on the Compute Shader side, where only UAVs are bound).\n• If a subresource is being bound as an RTV/UAV/SO Target and it is currently bound as another output, the currently bound output is unset and the new binding is recognized\n• If a subresource is ever asked to be bound as an RTV/UAV/DSV/SO Target while also bound as an input (either one first), the input view is unset\n\nIf a subresource is ever bound as an output (RTV/UAV/SO Target), subsequently unbound, and then bound as a shader input, a ReadAfterWriteHazard DDI is called. Drivers can use this as a hint as to when a rendering flush may be required. There are additional situations where Read After Write hazards are reported given the two pipelines – Graphics and Compute, in particular resources moving from output binding on one side to input binding on the other side, as well Compute outputs moving to Compute input. Note UAVs are considered as \"output\", since if an application only needs to read a resource, it should be bound as an input instead.\n\nThere is a significant and unfortunate limitation in many hardware designs that had to be built into D3D. While Typed UAVs support many formats – essentially any format that can be a RenderTarget - the majority of these formats only support being written as a UAV, but not read at the same time.\n\nShader Resource Views are of course always available in any shader stage when only read-only access from arbitrary locations in a Typed resource is needed. Conversely, it is useful that if write-only access to arbitrary locations in a Typed resource is needed, UAVs support that scenario.\n\nHowever, simultaneous reading and writing to a UAV within a single Draw* or Dispatch* operation is only supported if the UAV’s Type is R32_UINT/_SINT/_FLOAT. In particular, the ld_uav_typed IL instruction for reading from a typed UAV is limited to R32_UINT/_SINT/_FLOAT formats. E.g. a UAV with a type such as R8G8B8A8_UNORM_SRGB cannot be read from (but it can be written).\n\nD3D has a partial workaround for this inability to simultaneously read+write from Typed UAVs. The purpose is to make tasks such as editing an image in-place simpler, given the circumstances.\n\nD3D allows Texture1D/2D/3D resources created with any of the following small set of 32-bit per element formats to have UAVs created from them with R32_UINT/_SINT/_FLOAT as the type:\n\nOnce an R32_* UAV is created, it allows arbitrary reading and writing to the UAV’s memory in-place. The catch is there is no type conversion since the format is R32_*, meaning reads and writes simply move raw data unaltered between a shader and memory. Since the desire of the application is that the memory is really interpreted as some format like DXGI_FORMAT_R8G8B8A8_UNORM_SRGB, the application is responsible for manually performing type conversion in the shader code upon reads and writes to the R32_* UAV.\n\nThe upside is that because the original resource was created with one of the _TYPELESS formats listed above, it allows other views such as Shader Resource Views or Render Target Views to be created using the actual format that the application intended – such as DXGI_FORMAT_R8G8B8A8_UNORM_SRGB. These properly typed views can then benefit from the fixed-function hardware type conversion upon reading and writing to the format during texture filtering on read or blending on writes, even though these were not available to the UAV, where manual type conversion code had to be done in the shader.\n\nThe formats supporting this casting to R32_* are limited those for which the hardware really makes no difference in memory layout versus R32_*, but excluding a few that have complex encoding cost such as DXGI_FORMAT_R11G11B10_FLOAT. If this ability to cast to R32_* UAVs was not included in D3D, applications would have to perform a copy rendering pass to move data from an R32_* resource where the image editing occurred to a separate resource that has the desired type (e.g. R10G10B10A2_UNORM), which is a waste of memory.\n\nUnordered Append Buffers enable a usage pattern whereby Pixel Shader and Compute Shaders can write structures of data to memory in variable quantity, in an unordered way. Hardware can take advantage of knowing this type of operation is going on, producing optimized performance.\n\nFor Structured Buffers that have been created with the Bind flag: D3D11_DDI_BIND_UNORDERED_ACCESS, Unordered Access Views can be created with one of the optional flags D3D11_DDI_BUFFER_UAV_FLAG_COUNTER or D3D11_DDI_BUFFER_UAV_FLAG_APPEND. The latter flag gives up some flexibility for (possibly) performance – described later.\n\nCreating a Structured Buffer UAV with UAV_FLAG_COUNTER causes the driver to allocate storage for a single hidden 32-bit unsigned integer counter associated with the UAV (as opposed to being associated with the underlying resource), initialized to 0. Multiple UAVs created on the same Buffer with this flag will thus have multiple independent counters.\n\nShaders can atomically increment or decrement this count (but not do both in one shader) and use the returned index to indicate which structure index in the UAV to access. If the _COUNTER flag is used, count values (representing struct index) returned to the shader may be saved for use later after the shader has completed, for example for linked lists.\n\nIf the _APPEND flag is used when creating the UAV, a counter is created like with the _COUNTER flag, except the counter values returned to a shader invocation when incrementing or decrementing the count are only valid for the lifetime of the shader invocation. So the shader can use the index during the shader invocation to access the corresponding struct index in the UAV, but the hardware is permitted to reorder the struct layout from the point of view of anything outside the shader invocation, or after the shader invocation is complete. This is for cases where an application is simply generating struct records and it does not care that the order of the records is maintained. However if the application goes out of its way to examine the buffer (such as copying from it or using some other type of View) the hardware will have to pack the records into the range of struct locations corresponding to the number of times shader invocations incremented the counter on a given UAV. Even though the data will appear packed, the structs may be reordered. Some hardware will take advantage of not having to maintain the order to provide better access performance.\n\nWhen Pixel Shaders and Compute Shaders bind UAVs that have _COUNT or _APPEND usage specified, an initial value for the View’s hidden counter must be provided as part of the bind call. Specifying -1 means maintain the current counter value already in the Buffer. Any other value sets the counter value.\n\nWhen an Append UAV is bound to the pipeline, the instructions that can access it are restricted to the following:\n• atomic increment hidden counter in a Count/Append UAV and return original value – see details in instruction definition. For Append UAVs, the returned count value is only valid as a reference to a particular struct in the UAV for the lifetime of the shader invocation.\n• write 32-bit value(s) to a UAV – see details in instruction definition\n• this instruction is also available on any UAVs (and other view types), not just Count/Append UAVs.\n• atomic decrement hidden counter in a Count/Append UAV and return new counter value – see details in instruction definition. For Append UAVs, the returned count value is only valid as a reference to a particular struct in the UAV for the lifetime of the shader invocation.\n• read 32-bit value(s) from a UAV – see details in instruction definitions\n• this instruction is also available on any UAVs (and other view types), not just Count/Append UAVs.\n\nFor an Append UAV, the HLSL compiler can use imm_atomic_alloc to obtain an \"address\" and then use a sequence of store_* commands to write out data a unique location in the unordered output to the UAV.\n\nConversely, the HLSL compiler can use imm_atomic_consume to obtain an \"address\" that already has data and then use a sequence of ld_* commands to read back data from a unique location in the UAV.\n\nFor Append UAVs, the count values returned by imm_atomic_alloc and imm_atomic_consume are hidden from the shader by the HLSL compiler, which exposes simply the ability to Append() structs or Consume() structs (not both in the same shader).\n\nFor Count UAVs, where the returned count value may be stored, any instructions capable of accessing Structured Buffers are permitted from the shader, in addition to all of the instructions listed above. Unlike Append UAVs, the HLSL compiler exposes the count values returned by imm_atomic_alloc and imm_atomic_consume for access in the shader – allowing the value to be saved.\n\nThe counter behind imm_atomic_alloc and imm_atomic_consume has no overflow or underflow clamping, and there is no feedback given to the shader as to whether overflow/underflow happened (wrapping of the counter). The only thing the counter really accomplishes is a way of generating unique addresses that is conveniently bundled with the UAV.\n\nIt is invalid for a single shader, or multiple shaders in flight on a GPU, to have the presence of both imm_atomic_alloc and imm_atomic_consume instructions operating on the same UAV. For a single shader, compilation fails if these operations (however they appear in HLSL) are mixed. The GPU must guarantee that Shader invocations from separate Draw*/Dispatch operations do not run out of sequence when there is a possibility that an alloc/consume hazard could exist.\n\nThe counter associated with a Count/Append UAV is somewhat like the counters that are associated with Stream Output buffers (note a Buffer cannot be both a Stream Output and Count/Append Buffer), although those counters have slightly different semantics. There is an API/DDI CopyStructureCount which allows the hidden count in a Count/Append UAV to be copied to another Buffer. This can serve as the vertex count parameter to Draw*InstancedIndirect, allowing data that has been written to an Append Buffer to be recirculated back into the GPU without CPU knowledge of the exact quantity involved.\n\nWhen Append/Count UAVs are bound to the pipeline the application can specify what the initial counter value should be, or choose to maintain the existing count value.\n\nFor an Append UAV, since the storage is unordered, when binding the UAV to the pipeline as a UAV or any other tpe of view (e.g. SRV), the contents of any struct entries in the UAV beyond the count value become undefined, and any contents within the count value are maintained, but may be reordered. It is fine for multiple different types of UAVs to overlap, but the application has to beware of the effect that the unordered nature of Append UAVs may have (when bound/used) on other overlapping views of the same memory. It is safest for an application not to mix usage of overlapping UAVs with expectations of data order being maintained in between.\n\nCount UAVs do not create any such ordering issues, since by definition applications are allowed to save count values as references to specific locations in the UAV.\n\nFor some implementations, Append UAVs will behave identically to Count UAVs (e.g. no reordering). Still, if the application does not care about the ordering of records being maintained in the UAV, it does not hurt (and can only help on some implementations) to make use of the constrained Append semantics for generating and subsequently consuming unordered collections of items.\n\nAs of the D3D11.1 API/DDI, Video Resources can have SRV/RTV/UAVs created so that D3D shaders can process them. The way the underlying Video Resource shows up in D3D as an ID3D11Resource* is described in separate D3D11 Video specs. This section covers how given an ID3D11Resource* to a Video Resource, SRV/RTV/UAVs can be created in D3D.\n\nThese Video Resources will be either Texture2D or Texture2DArray, so the ViewDimension in the VIEW_DESC structure must match. Additionally, the format of the underlying Video Resource restricts the formats that the View can use.\n\nThe following table describes all the combinations of Video Resource and View(s) that can be made from them. Note that multiple views of different parts of the same surface can be created, and depending on the format they may have different sizes from each other. A few video formats do not support D3D SRV/UAV/RTVs at all: DXGI_FORMAT_420_OPAQUE, _AI44, _IA44, _P8 and _A8P8. Further details on all the video formats is provided in the D3D11 Video DDI spec.\n\nRuntime read+write conflict prevention logic (which stops a resource from being bound as an SRV and RTV/UAV at the same time) treats Views of different parts of the same Video surface as conflicting for simplicity. It doesn’t seem interesting to allow the case of reading from luma while simultaneously rendering to chroma in the same surface, for example, even though it may be possible in hardware.\n\nResources have the following properties in common, specified at Resource creation:\n• Type: What the Resource Type(5) is (buffer, texture 1D, texture 2D, texture 3D, and texture cube).\n• Pipeline and Resource Usage: Pipeline Bind flags indicate where the Resource may be bound to in the graphics Pipeline. The unique locations that a Resource may be bound to are: Input Assembler Vertex Input, Input Assembler Index Input, Shader Resource (aka. Tex) Input, Shader Constant Input, Stream Output, RenderTarget Output, Unordered Access, and Depth/ Stencil Output. The application provides a combination of these flags at creation time in order for the Resource to possibly be optimized. These flags will be expected to be strictly honored by the application, and therefore are a failure case if not. There are many restrictions that may be placed on the Resource when one of these flags is used, meaning it is very possible that a few flags cannot be used together. See Resource Types(5) for details. Separately, Resource Usage refers to which functionality can be leveraged along with other optimization hints for the Resource:\n• SHAREDRESOURCE: Indicates a Resource must be allocated in such a way as to enable cross process usage. In general, the driver must expect that the resource may be referenced or destroyed from a process other than the one that created the resource. Therefore, the driver must not use any process-specific user memory allocations to support the resource, since the destroying process will, if different, not have access to that memory to free it.\n• DISCARDONPRESENT: Indicates that the contents of a Resource can be discarded or uninitialized after Present is invoked.\n• CPUWRITE: Indicates that the Resource should be created in such a way to satisfy requests to use the CPU to write to the Resource, through mapping/ locking. Note that Resources may still be read and written with the GPU, through the rendering Pipeline, with CopyResource(5.6.3), or with CopySubresourceRegion(5.6.2). Using UpdateSubresourceUP(5.6.8) is mutually exclusive with the ability to map/ lock. This flag is incompatible with multisampled Resources, as they are not able to be mapped/ locked.\n• CPUREAD: Indicates that the Resource should be created in such a way to satisfy requests to use the CPU to read from the Resource, through mapping/ locking. Note that Resources may still be read and written with the GPU, through the rendering Pipeline, with CopyResource(5.6.3), or with CopySubresourceRegion(5.6.2). Using UpdateSubresourceUP(5.6.8) is mutually exclusive with the ability to map/ lock. This flag is incompatible with multisampled Resources, as they are not able to be mapped/ locked.\n• DYNAMIC: Indicates the frequency of mapping/ locking or accessing the Resource with the CPU is typically once or more per frame. It is invalid to request a DYNAMIC Resource without indicating CPU read or write access is necessary.\n• HINTSTATIC: Indicates the frequency of mapping/ locking or accessing the Resource with the CPU is typically less than once per frame.\n• Format: The format of the data (e.g. DXGI_FORMAT_*). DXGI_FORMAT_UNKNOWN generally means Unstructured(5.1.2); but see Memory Structure(5.1) for details. (DXGI stands for \"DirectX Graphics Infrastructure\", a software component outside the scope of this specification which happens to own the list of DirectX formats going forward).\n• SampleDesc: Multisample parameters for the surface (Sample Count and Quality Level). For buffers, Texture1D, Texture1DArray, and Texture3D resources, the only Sample Count allowed is 1 and the only allowed Quality Level is zero.\n\nResources are made up of one of more Subresources. These Subresources share a common lifespan with each other and the Resource. In other words, the Resource and Subresources are atomically allocated and destroyed. However, some operations occur at the Subresource level, versus the Resource level. Subresources are three dimensional entities (with height, width, depth, pitch, and slice pitch), but degenerate into two and one dimensional entities for a certain Resource. For ex. a fully mipped Texture2D Resource creation with a width of two, a height of two, and an array size of two will have four Subresources that can be individually referenced for certain operations. Two Subresources have a width of two, height of two, and depth of one. These two Subresources are the most detailed mip level. The additional two Subresources have a width of one, height of one, and depth of one. Each Subresource is allowed to have it's own address, so the Resource may have somewhere between one and four disjoint allocations to satisfy the previous example. Each Subresource inherits the properties of the Resource, and Subresources may not be part of multiple Resources.\n\nA structured buffer(5.1.3) is created by specifying both a new misc flag and the stride of the structure.\n\nThe only D3D11 Resource type that can have a structure defined is the Buffer type. When the Resource is created at the API, the misc flag D3D11_RESOURCE_MISC_STRUCTURED_BUFFER and a structure stride in bytes must be specified.\n\nThe StructureByteStride can be at most bytes.\n\nThe D3D11_RESOURCE_MISC_STRUCTURED_BUFFER flag cannot be combined with D3D11_RESOURCE_MISC_ALLOW_RAW_VIEWS (described elsewhere).\n\nThe D3D11_RESOURCE_MISC_STRUCTURED_BUFFER flag may be combined with any of the following bind flags:\n\nThe D3D11_RESOURCE_MISC_STRUCTURED_BUFFER flag may NOT be combined with any of the following bind flags:\n\nBuffers that define a structure cannot be used with the InputAssembler, either for vertex or index data. Structured buffers also cannot be bound as a stream output target or render target.\n\nIf the D3D11_RESOURCE_MISC_STRUCTURED_BUFFER is not set, then StructureByteStride parameter to the Buffer creation must be 0. If not, the runtime will fail the creation call.\n\nIf the D3D11_RESOURCE_MISC_STRUCTURED_BUFFER is set, then StrideInBytes must be non-zero and ByteWidth must be evenly divisible by StructureByteStride . If either condition is not true when creating a structured buffer, the create call will be failed by the runtime.\n\nResource size dimensions (Width, Height, Depth) are always specified in pixel units. Size dimensions are restricted only for subsampled and block compressed formats (see Formats(19.1) section), and are otherwise restricted only to positive integers. Furthermore, the size dimensions of a Resource have no bearing on what functionality is available for the resource (such as filtering support).\n\nResource pitches are always expressed in bytes, and indicate the memory delta between the start of pixel rows or array slices, with the only exception being block compressed formats, where the pitch is defined as between between 'block' rows instead of pixel rows. Pitch values are restricted only to non-negative integers, intentionally including zero for which the first row will be replicated to all rows.\n\nSize dimensions for lower level mipmapped resources are computed by the Direct3D runtime based on the size of the level zero map. These computed dimensions are adjusted upward as necessary to adhere to physical size dimension restrictions for subsampled and block compressed formats - refer to the discusson of physical and virtual dimensions in Block Compressed Formats(19.5) and Sub-Sampled Formats(19.4).\n\nMapping/ locking is done at the Subresource level, instead of the Resource level. Mapping means granting CPU access to the Subresource's storage or contents. Typically, the user mode driver must invoke the Lock callback to achieve this operation. The application subsequently relinquishes direct access to mapped Subresources by unmapping them. Only one Map for a given Subresource is allowed (even for non-overlapping regions) and no accelerator operations on a Subresource may be ongoing while a Map is outstanding on that Subresource. However, multiple Subresources of the same Resource may be Mapped at the same time. Each Map method returns a structure that contains a pointer to the storage backing the Resource, and pitch values representing the distances between rows or planes of data, depending on the Subresource dimensionality. The returned pointer always points to the top-left byte (U = 0, V = 0, W = 0) to the mapped Subresource. The layout is similar to that of a multidimensional 'C' array, where the Subresource can be considered to be the following 'C' declaration:\n\nwith the additional characteristic that the driver is allowed to specify the byte pitch between each row (or block-row for BC formats) and each depth slice.\n\nWhen returning a pointer to the mapped resource, the pointer must be 16-byte aligned. This restriction allows applications to perform SSE-optimized operations on the data natively, without realignment or copy (example usages include CPU geometry and texture processing).\n• CPUREAD: Indicates this Subresource's Resource must've been created to allow read access, and that the application will read from the Subresource with the CPU.\n• CPUWRITE: Indicates this Subresource's Resource must've been created to allow write access, and that the application will write to the Subresource with the CPU.\n• DISCARDRESOURCE: Indicates this Subresource's Resource must've been created with the DYNAMIC flag, and that the entire Resource being mapped need not be preserved, expecting the contents of the entire Resource to eventually be overwritten. It is still valid to pass a region during a map with the DISCARDRESOURCE flag. This flag behaves consistently across all Resource types.\n• NO_OVERWRITE: Indicates this Subresource's Resource must be a Buffer (but not a Constant Shader Resource), must only be used as an Input in the graphics pipeline, and must have been created with the flags DYNAMIC and only the CPUWRITE (write-only). The NO_OVERWRITE map flag must be used in conjunction with only the CPUWRITE map flag (write-only). Use of this flag indicates the application will not modify any data referred to by a previous Draw or Resource update, etc.\n\n5.6.1.2 Map() NO_OVERWRITE on Dynamic Buffers used as Shader Resource Views\n\nMap() allows NO_OVERWRITE for Buffers with DYNAMIC usage and the SHADER_RESOURCE (shader input) bind flag. Before D3D11.1 this was disallowed (though DISCARD was allowed).\n\nBefore the first call with NO_OVERWRITE on a deferred context, a DISCARD must be done on the same context (via Copy*()/Update*()/Map() API flag or Discard*() API). This is not required on immediate contexts if the application knows the GPU is finished with the resource (though discard can be used if not).\n\nThis feature is required to be supported for all D3D10+ hardware with D3D11.1 drivers.\n\n5.6.1.3 Map() on DEFAULT Buffers used as SRVs or UAVs\n\nMap() can be called on Buffers with DEFAULT usage and SHADER_RESOURCE and/or UNORDERED_ACCESS bind flags.\n\nThe Buffer can have MiscFlags BUFFER_ALLOW_RAW_VIEWS, BUFFER_STRUCTURED or nothing.\n\nBefore D3D11.2 this was disallowed. As of D3D11.2, this feature is required to be supported for Feature Level 11.0+ devices with WDDM1.3+ drivers.\n\nThis function allows sub-region copying of data from one Subresource to another. No stretch, color key, blend, nor format conversion. However, format types of each Subresource need not be exactly equal to each other, as the Resource may be Prestructured+Typeless Memory(5.1.5), which is also supported. For example, a R32_FLOAT Texture can be copied to an R32_UINT Texture, as both of these formats are in the same R32_TYPELESS group. Conceptually, the interpreted value of texels changes during this type of copy; but the raw value of memory happens to be equal. This function also works when both Subresources are Unstructured Memory(5.1.2) also, except that the regions to copy will be in raw bytes, versus pixel or Element units.\n\nIn addition, the Subresources need not be of equal size; but the source and destination regions must fit entirely within the Subresources. The source and destination Subresources must not be the same Subresources.\n\nResources which can be used as Depth/ Stencil cannot partipate in this operation as a destination; but they can as a source. Multisampled Resources cannot partcipate in Copy operations.\n\n5.6.2.1 CopySubresourceRegion with Same Source and Dest\n\nCopySubresourceRegion*() allow the source and dest to be the same resource, with D3D11.1 drivers. The driver must handle overlapping copies.\n\nThis feature is required to be supported for all D3D10+ hardware with D3D11.1 runtime+drivers. When the application uses feature level 9.x all drivers support this with the D3D11.1 runtime.\n\nCopySubresourceRegion*() allows a new TILEABLE flag when the source is a currently bound RenderTarget (flag ignored otherwise). This is intended for tile / deferred rendering GPUs (no impact on the copy for non-tiled rendering GPUs). The flag indicates that if the GPU happens to be processing only given tile of a RenderTarget at a time (where the RenderTarget is the source in the copy), the GPU can break the copy call to occur per-tile along with the surrounding rendering calls batched for the scene, without having to flush the scene for all tiles.\n\nThe application is guaranteeing that future access to the destination of the copy will only be used for 1:1 cycling of that data back into the same pixel location of the affected RenderTarget (which remains bound). Said another way, the application is guaranteeing that when a tiling GPU replays batched rendering commands to produce any given tile, there will be no visible effect (e.g. to commands earlier in the batch) of the copy having already occured for previously processed tiles.\n\nThe source and dest don't have to be the same size resource; this flag is relevant to just the region being copied.\n\nWhen the application is finished using the target of the TILEABLE copy for recirculating back to the original surface, DiscardResource() should be called if the contents are no longer needed (but this is not strictly required). For some implementations, knowing the end of life of the data in the scratch surface could allow the entire copy to be optimized away into leaving the data in fast tile memory and never having to write it out to GPU memory.\n\nIf an application violates the 1:1 property when using the TILEABLE flag on CopySubresourceRegion, such as reading into a different pixel, or into a shader stage other than the Pixel Shader in the second pass, the the data being read is undefined (it will have been generated by an unknown rendering pass by the application or uninitilized).\n\nIf the RenderTarget gets unbound, any copies from it that happened with the TILEABLE flag while bound lose the TILEABLE property after the RenderTarget unbinding.\n\nThis feature is available for all D3D9+ hardware with D3D11.1 drivers (D3D9 portion of the DDI for D3D9 hardware and both D3D9 and D3D11.1 portions of the DDI for D3D10+ hardware).\n\nThis feature will be exposed only to customers of Direct3D within the Windows OS, at least initially, given the narrowly focused application.\n\nThis function allows copying of an entire Resource, assuming the Resources are identical types and dimensions. No stretch, color key, blend, nor format conversion. However, format types of each Subresource need not be exactly equal to each other, as the Resource may be Prestructured+Typeless Memory(5.1.5), which is also supported. For example, a R32_FLOAT Texture can be copied to an R32_UINT Texture, as both of these formats are in the same R32_TYPELESS group. Conceptually, the interpreted value of texels changes during this type of copy; but the raw value of memory happens to be equal. This function also works when both Resources are Unstructured Memory(5.1.2).\n\nResources which can be used as Depth/ Stencil cannot partipate in this operation as a destination; but they can as a source. Multisampled Resources cannot partcipate in Copy operations. This operation also impacts heavily on performant readback and upload scenarios.(5.3.2)\n\nOn the ARM CPU, cache coherency isn’t provided when the GPU writes to system memory, so a GPU driver would normally be tempted to put a staging (D3D CPU memory) surface in uncached memory (which is slow for CPU access) to avoid incorrect values being read from the cache. However, the Win8 Video Memory Manager will manually flush the CPU cache on ARM when data has been copied from the GPU to a staging surface – so GPU drivers can safely use cacheable memory for STAGING surfaces (yielding good performance on CPU reads). VidMM will also flush CPU caches for the opposite case as well - before the GPU reads from a STAGING surface.\n\nAt the D3D11.1 DDI, when a STAGING surface is created, the CPU_ACCESS flags (READ and/or WRITE) are mapped directly down through the DDI, so there it is obvious to drivers when the cacheable memory choice should be made (when WRITE is not set). For the D3D9 DDI (which all drivers for all hardware feature levels must implement), the mapping from D3D11's CPU_ACCESS flags to the D3D9 DDI’s is described in the separate API/DDI spec - see PFND3DDDI_CREATERESOURCE - the situation is SYSTEMMEMORY surfaces that don't have the WriteOnly flag set at the D3D9 DDI.\n\nA note for User Mode drivers: The driver must not cache Map on surfaces that rely on the software enforced coherency described above (i.e. surface is cacheable but mapped into an aperture segment which doesn’t support CacheCoherency). The driver must explicitly call LockCb and UnlockCb at every Map for such surfaces to give an opportunity to VidMm to apply the proper memory barrier. Failing to do so will result in the surface getting corrupted over time.\n\nCopyResource and CopySubresourceRegion allow either or both the source and destination to be structured buffers. It is possible to copy from linear to structured, structured to linear, and structured to structured. If copying between structured buffers, the strides must be the same or the runtime will fail the copy operation. If the region to copy is not specified as complete structures, then the runtime will fail the copy operation.\n\nWhen the either the source or destination is linear and the other is structured, it is up to the driver to do rearrange the layout if necessary. If structured buffers are stored linearly, then the copy operation is a straightforward copy. If not stored linearly, then any tiling or other reorganization must occur as part of the copy operation.\n\nOnly multisample render targets are able to be resolved to a single-sampled resource. Naturally, the source must be a multisampled render target, while the destination must be a single-sampled resource restricted such that it resides in video memory. For example, the destination cannot be a dynamic or system-memory friendly Resource. Thus the destination Resource must be USAGE_DEFAULT. The algorithm to resolve multiple samples to one pixel is implementation dependent. Resolve shares some of the restrictions of Copy, such as both Resources must be the same type (ie. Texture2D), and no strecting. Only a whole Subresource can be resolved, so both Subresources must be the same dimensions. Format conversion is not desired for ResolveSubresource either. However, due to typeless Resources, there is an interesting interaction with either Resource Format. If each Resource is prestructured+typed, then both Resources must have the same Format; and that must match the passed in ResolveFormat (ie. all R32_FLOAT). If one Resource is prestructured+typeless, then the prestructured+typed Resource's format must be compatable with the typeless format; and the ResolveFormat must match the prestructured+typed format (ie. Src: R32_TYPELESS, Dst & ResolveFormat: R32_FLOAT). If both Resource are prestructured+typeless, then they must be equal formats, and the ResolveFormat may be any format compatable with the typeless format and supporting resolve. (ie. Src & Dst: R32_TYPELESS -> ResolveFormat must be R32_FLOAT).\n\nFurther discussion on format interpretations and Multisample Resolve can be found in the Multisample Format Support(19.2) section.\n\nMultisample resolve is performed in linear space, so conversion to linear for sRGB formats is performed prior to any arithmetic operations on the resource data, similar to the requirement for conversion to linear prior to filtering and blending arithmetic operations.\n\nThis operation identifies a Read-after-Write Hazard on a Resource granularity throughout the usage of a Device Context. This operation will be sent to the driver immediately before the Resource is used as an input in the graphics pipeline, as this is when the hazard is detected. For example, as a Render Target/ Texture transitions from a Render Target to a Texture, FlushResource will identify this transition immediately before the Resource is set as a Texture. FlushResource will identify the Resource, as a whole, and not the individual Subresources involved. It is expected that this operation detects when GPU caches need to be flushed.\n\nWhen the pipeline is configured to read from non-overlapping Subresources that are being written to, at the same time non-overlapping Subresources are being read from, FlushResource operations will not be sent for such a Resource. So, the driver should not rely on notifications for this type of condition, as it doesn't appear there is really a Read-after-Write Hazard.\n\nAdditionally, FlushResource should not be expected to be used for to identify any hazards related to shared Resources: same-process cross-Device Context Resources nor cross-process Resources. Whenever a Device Context is swapped for another Device Context, GPU caches should be flushed, as needed, to maintain correct behavior. The only hazards FlushResource exposes are within the same device context.\n\nIf a Subresource was created with flags preventing the CPU to map/ lock and write to the Resource, the Subresource may still be able to be modified with UpdateSubresourceUP, as these concepts are mutually exclusive.\n\nUpdateSubresourceUP may not be used when the Resource was created with flags allowing the CPU to map/ lock the Resource. It also may not be used with Resources that can be used as Depth/ Stencil, nor for multisampled Resources.\n\nPartial updates of ConstantBuffers are disallowed, so when modifying ConstantBuffers with UpdateSubresourceUP, the update box will always be NULL.\n\nUpdateSubresource works with structured buffers as a destination. The source data is interpreted as an array of structures of the destination’s stride. If necessary, any conversion of the data to a different layout must happen during the update process. It is only valid to update ranges of complete structures. If the bounds of the region being updated are not a range of complete structures, the runtime will fail the update operation.\n\n5.6.9 UpdateSubresource and CopySubresourceRegion with NO_OVERWRITE or DISCARD\n\nThis is a new variant of the UpdateSubresource() and CopySubresourceRegions APIs (which both update a portion of a GPU surface) for D3D1.1. The addition is a Flags field where NO_OVERWRITE or DISCARD can be specified. A separate new feature that also affects UpdateSubresource is that it now allows overlapping copies.\n\nSpecifying NO_OVERWRITE means that the system can assume that existing references to the surface that may be in flight on the GPU will not be affected by the update, so the copy can proceed immediately (avoiding either a batch flush or the system maintaining multiple copies of the resource behind the scenes).\n\nDISCARD means that the system may discard the entire contents of the destination memory outside the region being updated.\n\nBefore the first call with NO_OVERWRITE on a deferred context, a DISCARD must be done on the same context (via Copy*()/Update*()/Map() API flag or Discard*() API). This is not required on immediate contexts if the application knows the GPU is finished with the resource (though discard can be used if not).\n\nThe implementation of system to video blts is critical for good performance in Direct2D text rendering. Drivers that expose the cap bit indicating that they are a tile-based renderer will see encounter the following situation during Direct2D text rendering:\n• A system to video blt (PFND3DDDI_TEXBLT1 on the D3D9 DDI, PFND3D11_1DDI_RESOURCECOPYREGION on the D3D11.1 DDI)\n• The destination of the blt has DYNAMIC usage\n• Either the NoOverwrite or the Discard flags are specified in the blt\n\nWhen drivers encounter this scenario, they should implement the copy with the CPU synchronously. The NoOverWrite or Discard flag specified in the blt call can be used by the driver to map the destination surface for CPU access. These flags also enable drivers to implement this blt without a mid-scene flush. Drivers that implement this blt asynchronously (with either the CPU or the GPU) will see slowdowns when Direct2D attempts to map the system memory surface in the future.\n\nDrivers on immediate-mode GPUs are free to implement system to video blts asynchronously.\n\nDiscardResource() and DiscardView() API/DDIs (the latter allowing rects to be specified) allow applications to specify the contents of a resource (or the subset of it that is in a View) may be discarded. This is be reflected in both the D3D11.1 and D3D9 DDIs. The D3D9 DDI does not have Views, but does support limited subsetting of resources, so that is reflected in the new D3D9 Discard DDI (documented elsewhere).\n\nD3D11 includes a way for applications to prevent some of the mipmaps in a resource from being accessible via the 3D pipeline (by clamping the mipmaps). This mechanism operates per-resource, as opposed to per-sampler(7.18.2) or per-ShaderResourceView, allowing applications a convenient way to globally control the GPU memory footprint that is referenced at any point. Drivers can easily take advantage of these per-resource clamps since they know that clamped off miplevels do not have to be resident in GPU memory.\n\nEach resource (such as a texture2D) that an application creates will have a method on its interface that queues a D3D command setting a float32 scalar global MinLOD clamp for all Shader Resource Views of that resource. The fact that the command is queued means it does not affect the behavior of anything ahead of it in the queue.\n\nRecall that lower LOD values define the more detailed mipmaps in a mipmap chain, so applying a MinLOD clamp has the effect of clamping off the most detailed miplevel(s).\n\nThe per-resource global MinLOD clamp applies to any reference to the resource from a shader via a Shader Resource View, such as using sample* or ld*instructions. Note that Sampler(7.18.2) objects already contain a fixed MinLOD and MaxLOD clamp, honored by instructions that take a Sampler as an operand such as sample*. The per-resource MinLOD clamp has the same effect as the Sampler MinLOD clamp (both clamps are applied), except each has a different number space for identifying mipmaps.\n\nThe per-resource MinLOD clamp considers the most detailed mipmap on the resource as LOD 0, so specifying a MinLOD clamp of 1 causes miplevel 0 on the resource to be ignored. On the other hand, the Sampler’s MinLOD clamp defines most detailed mipmap in the current Shader Resource View as LOD 0. So on a Shader Resource View that, for example, limits a mipmap chain to exclude the most detailed 3 mips from a resource, setting the Sampler MinLOD to 1 causes miplevel [3] (the fourth mip) in the resource to be ignored.\n\nThe per-resource MinLOD clamp can be fractional (like the Sampler(7.18.2) MinLOD clamp) – this is useful with linear mipmap filtering. For example suppose the per-resource MinLOD clamp is 1.1, and the current Shader Resource View is the entire mipchain. Texture filters would behave as if the most detailed mipmap available is a blend of 90% of mipmap [1] and 10% of mipmap [2]. Both mipmap [1] and [2] would have to be resident on the GPU. A way to make use of the fractions is to start with a high MinLOD clamp (limiting the memory footprint enough to prevent stalling on texture upload to the GPU), and gradually lowing the MinLOD clamp on the resource over time, allowing the driver/hardware more time to make all of the resource resident. Visually there would be no popping, as the influence of more detailed mipmaps is blended in.\n\nA fractional per-resource MinLOD clamp basically requires the floor of the MinLOD miplevel and the less detailed miplevels to be resident. In the example above with a per-resource MinLOD clamp of 1.1, if a ld instruction requests data from miplevel [1], it will be resident.\n\nAs another example, consider the same Shader Resource View with a full mipchain, but a MinLOD clamp of 0.1. The gather4(22.4.2) instruction is defined to operate on mip 0 in the view only (otherwise an out of bounds result is returned). But since the clamp of 0.1 requires mip 0 to be present, gather4 will fetch from mip 0.\n\nSuppose a ShaderResourceView on a resource is defined which limits the miplevels visible in the resource. Now suppose a per-resource MinLOD clamp is set such that the intersection of the remaining active miplevels after the clamp, with the miplevels used in a ShaderResourceView, is empty. e.g using a ShaderResourceView of mipmaps 0..3 on a resource along with a resource MinLOD clamp of 5. The result of fetching from the ShaderResourceView with such an empty intersection with the per-resource clamp is the defined out-of-bounds access result. That is, 0 is returned for all non-missing components of the format of the resource, and the default is provided for missing components. The lod(22.5.6) instruction returns 0 for the clamped LOD in this empty-set case.\n\nIf a texture has 6 mip levels (0..5) and the MinLOD clamp is set to any value past the least detailed mip in the view (e.g. 5.1), the out of bounds behavior applies. This is an exception to the rule that the floor of the MinLOD clamp is required to be present.\n\nShader ld*(22.4.6) instructions, which do not perform filtering, and which access miplevels directly, also honor the per-resource MinLOD clamp. This is unlike the MinLOD clamp in Sampler state, since ld* instructions do not use samplers. The previous section has an example illustrating how ld behaves with a fractional clamp.\n\nIf sample*(22.4.15) instructions that explicitly provide a miplevel to fetch from, such as sample_l(22.4.18), request a miplevel that is clamped off by a per-resource MinLOD clamp (where the per-resource clamp still falls within the View), the result of the fetch is the same as what happens with sampler clamping; that is the most detailed available clamped mip (after both sampler and MinLOD clamp) is used.\n\nWhen sampling using a Sampler(7.18.2) configured to use BorderColor, accessing the border region of a mipmap that has been clamped off due to MinLOD clamp, the result is the out of bounds behavior (as opposed to returning the border color).\n• From the Pixel Shader a sample instruction using the above SRV and Sampler results in a pre-clamp LOD calculation of -2.\n• The Sampler MinLOD/MaxLOD clamp of [1.2…4] brings the LOD to 1.2 in the SRV mip number space.\n• The Per-Resource MinLOD clamp brings the LOD to 2.5 in the SRV mip number space (since the clamp is 3.5 in the Resource space).\n• Since the post-clamped LOD is > 0, the minfilter is used (linear).\n• So the sample instruction fetches from View mips 2 and 3, applies LINEAR filtering to both mips (since that is the MIN filter), and blends them 50% each, due to the .5 in the LOD with LINEAR as the MIP filter.\n• The getLOD instruction would return -2 as the unclamped LOD and 2.5 as the clamped LOD.\n• sample_l with -2 as the LOD would fetch from LOD 2.5 with MIN filtering the same as the sample did above.\n• A ld instruction (note this doesn’t use a sampler) that specifies an unsigned integer mipLevel of 2 results in data being fetched from miplevel 2 in View space (3 in Resource space), since the per-Resource clamp is 3.5 (in Resource space), which forces mip 2 (3 in Resource space) to be available.\n• A ld instruction that specifies an unsigned integer miplevel of 1 results in out-of-bounds ld behavior since mip 1 in View space (2 in Resource space) has been clamped off.\n• gather4_* instructions, which can only operate on view mip 0, would return out of bounds result. For gather4_*_c instructions (which do a comparison), the out of bounds result is used as the comparison value against the reference provided from the shader, and the comparison results are returned.\n• Suppose in the sample example above, the pre-clamp LOD calculation was 2.\n• The Sampler MinLOD/MaxLOD clamp of [1.2…4] leaves the LOD at 2 in the SRV mip number space.\n• The Per-Resource MinLOD clamp brings the LOD to 2.5 in the SRV mip number space (since the clamp is 3.5 in the Resource space).\n• Since the post-clamped LOD is > 0, the minfilter is used (linear).\n• So the sample instruction fetches from View mips 2 and 3, applies LINEAR filtering to both mips (since that is the MIN filter), and blends them 50% each, due to the .5 in the LOD with LINEAR as the MIP filter.\n• The LOD instruction would return 2 as the unclamped LOD and 2.5 as the clamped LOD.\n• sample_l with 2 as the LOD would fetch from LOD 2.5 with MIN filtering the same as the sample did above.\n\n5.8.6.2 Case 2: Per-Resource Clamp falls within SRV, but outside Sampler clamp\n• From the Pixel Shader a sample instruction using the above SRV and Sampler results in a pre-clamp LOD calculation of -2.\n• The Sampler MinLOD/MaxLOD clamp of [1.2…4] brings the LOD to 1.2 in the SRV mip number space.\n• The Per-Resource MinLOD clamp brings the LOD to 4.5 in the SRV mip number space (since the clamp is 5.5 in the Resource space).\n• Since the post-clamped LOD is > 0, the minfilter is used (linear).\n• So the sample instruction fetches from View mips 4 and 5, applies LINEAR filtering to both mips (since that is the MIN filter), and blends them 50% each, due to the .5 in the LOD with LINEAR as the MIP filter.\n• The LOD instruction would return -2 as the unclamped LOD and 4.5 as the clamped LOD.\n• sample_l with -2 as the LOD would fetch from LOD 4.5 with MIN filtering the same as the sample did above.\n• A ld instruction (note this doesn’t use a sampler) that specifies an unsigned integer mipLevel of 4 results in data being fetched from miplevel 4 in View space (5 in Resource space), since the per-Resource clamp is 5.5 (in Resource space), which forces mip 4 (5 in Resource space) to be available.\n• A ld instruction that specifies an unsigned integer miplevel of 3 results in out-of-bounds ld behavior since mip 3 in View space (4 in Resource space) has been clamped off.\n• gather4* instructions, which can only operate on view mip 0, would return out of bounds result. For gather4_c* instructions (which do a comparison), the out of bounds result is used as the comparison value against the reference provided from the shader, and the comparison results are returned.\n• Suppose in the sample example above, the pre-clamp LOD calculation was 2.\n• The Sampler MinLOD/MaxLOD clamp of [1.2…4] causes no change to the LOD of 2.\n• The Per-Resource MinLOD clamp brings the LOD to 4.5 in the SRV mip number space (since the clamp is 5.5 in the Resource space).\n• Since the post-clamped LOD is > 0, the minfilter is used (linear).\n• So the sample instruction fetches from View mips 4 and 5, applies LINEAR filtering to both mips (since that is the MIN filter), and blends them 50% each, due to the .5 in the LOD with LINEAR as the MIP filter.\n• The LOD instruction would return 2 as the unclamped LOD and 4.5 as the clamped LOD.\n• sample_l with 2 as the LOD would fetch from LOD 4.5 with MIN filtering the same as the sample did above.\n• From the Pixel Shader a sample instruction using the above SRV and Sampler results in a pre-clamp LOD calculation of -2.\n• Since the Per-Resource MinLOD clamp is outside the View, the sample returns out of bounds behavior -> 0 for all defined components, and defaults for missing components. The result is identical no matter what the pre-clamp calculated LOD is.\n• The LOD instruction would return -2 as the unclamped LOD and 0 as the clamped LOD.\n• sample_l would return the same out of bounds behavior as above, regardless of what mip is requested.\n• A ld instruction (note this doesn’t use a sampler) would return out of bounds behavior regardless of what mip is requested.\n• gather4* instructions, which can only operate on view mip 0, would return out of bounds result. For gather4_c* instructions (which do a comparison), the out of bounds result is used as the comparison value against the reference provided from the shader, and the comparison results are returned.\n• Suppose in the sample example above, the pre-clamp LOD calculation was 2.\n• Since the Per-Resource MinLOD clamp is outside the View, the sample returns out of bounds behavior -> 0 for all defined components, and defaults for missing components. The result is identical no matter what the pre-clamp calculated LOD is.\n• The LOD instruction would return 2 as the unclamped LOD and 0 as the clamped LOD.\n\nPer-resource MinLOD clamps only affect the behavior of ShaderResourceView accesses from shader code – such as sample* and ld*instructions discussed so far.\n\nOther operations on the resource are unaffected by per-resource MinLOD clamps, including reading and/or writing via RenderTargetViews, DepthStencilViews, or resource manipulation APIs such as CopySubresourceRegion, UpdateResource or GenerateMips. Any such reference to the contents of a resource, i.e. NOT through a ShaderResourceView, requires the system to make appropriate memory resident for the requested operation to proceed as expected, unaffected by per-resource MinLOD clamping.\n\nThe behavior of the resinfo instruction wrt. Per-resource MinLOD clamp is defined within the instruction's definition(22.4.14).\n\nThis spec is for \"Tiled Resources\" in D3D. Other terms that have been used for the same concept are \"Sparse Textures\" and \"Partially Resident Textures\"\n\nThis document outlines what might be expected of D3D implementations if this hypothetical feature was included in a future version of D3D.\n\nRecall that all D3D memory allocations are managed at subresource granularity (in a system without Tiled Resource support). For a Buffer, the entire Buffer is the subresource. For a Texture, each mip level is a subresource (at a given array slice if it is a Texture Array). The graphics system (OS, driver, hardware) only expose the ability to manage the mapping of allocations at this subresource granularity. \"Mapping\", in the context of Tiled Resources in this spec, refer to making data visible to the GPU.\n\nSuppose an application knows that a particular rendering operation only needs to access a small portion of an image mipmap chain (perhaps not even the full area of a given mipmap). Ideally the system could be told about this and only bother to ensure that the needed memory is mapped on the GPU without paging in too much. In reality, the system can only be informed about what memory needs to be mapped on the GPU at subresource granularity (i.e. a range of full mipmap levels that could be accessed). There is no demand faulting in the graphics system either, so potentially a lot of excess GPU memory needs to be used make full subresources mapped before a rendering command that references any part of the memory is executed. This is just one issue that makes the use of large memory allocations difficult in D3D.\n\nD3D11 supports Texture2D surfaces with up to 16384 pixels on a given side. An image that is 16384 wide by 16384 tall and 4 bytes per pixel would consume 1GB of video memory (and adding mipmaps would double that). In practice it is unlikely/rare that all 1GB would need to be referenced in a single rendering operation.\n\nSome game developers are now modeling terrain surfaces as large as 128K by 128K. The way they get this to work on existing GPUs is to break the surface into tiles that are small enough for hardware to handle. The application must figure out which tiles might be needed and load them into a cache of textures on the GPU - a software paging system. A significant downside to this approach comes from the hardware not knowing anything about the paging that is going on: When a part of an image needs to be shown on screen that straddles tiles, the hardware does not know how to perform fixed function (i.e. efficient) filtering across tiles. This means the application managing its own software tiling must resort to manual texture filtering in shader code (which becomes very expensive if a good quality anisotropic filter is desired) and/or waste memory authoring gutters around tiles that contain data from neighboring tiles so that fixed function hardware filtering can continue to provide some assistance.\n\nIf a Tiled representation of surface allocations could be a 1st class feature in the graphics system, the application could tell the hardware which tiles to make available. So (a) less GPU memory is wasted storing regions of surfaces that the application knows will not be accessed, and (b) the hardware can understand how to filter across adjacent tiles, alleviating some of the pain experienced by developers doing software tiling today.\n\nBut to provide a complete solution, something must be done to deal with the fact that, independent of whether tiling within a surface is supported, the maximum surface dimension is currently 16384 - nowhere near the 128K+ that applications already want. Just requiring the hardware to support larger texture sizes is one approach, however there are significant costs and/or tradeoffs to going this route. D3D11's texture filter path and rendering path are already saturated in terms of precision in supporting 16K textures with the other requirements, such as supporting viewport extents falling off the surface during rendering, or supporting texture wrapping off the surface edge during filtering. A possibility is to define a tradeoff such that as the texture size increases beyond 16K, functionality/precision is given up in some manner. Even with this concession however, additional hardware costs may be required in terms of addressing capability thoughout the hardware system to go to larger texture sizes.\n\nOne issue that comes into play as textures get very large is that single precision floating point texture coordinates (and the associated interpolators to support rasterization) run out of precision to specify locations on the surface accurately. Jittery texture filtering would ensue. One expensive option would be to require double precision interpolator support, though that could be overkill given a reasonable alternative - discussed later.\n\nRegardless of whether the supported texture size may be increased above 16K, if there is some limit that is arrived at that is not magnitudes larger, the question would still remain: What if the application wants a surface even larger than whatever limit is in place? A reasonable approach could be to \"Quilt\" these large textures manually, independent of the Tiling within each texture. This document covers an approach along these lines. This might also mitigate a lack of double precision attribute interpolation.\n\nThe reason for one of the alternate names for this is \"Sparse Texture\" is that \"Sparse\" conveys both the Tiled nature of the resources as well as the perhaps the primary reason for Tiling them - that not all of them are expected to be mapped at once. In fact, it is conceivable that an application could author a Sparse/Tiled Resource in which no data is authored for all regions+mips of the resource, intentionally. So the content itself could be sparse, and the mapping of the content in GPU memory at a given time would be a subset of that (even more sparse).\n\nAnother scenario that could be served by Tiled Resources is enabling multiple Resources of different dimensions/formats to share the same memory. Sometimes applications have exclusive sets of resources that are known not to be used at the same time, or resources that are created only for very brief use and then destroyed, followed by creation of other resources. A form of generality that can fall out of \"Tiled Resources\" is that it is possible to allow the user to point multiple different resources at the same (overlapping) memory. In other words, the creation and destruction of \"resources\" (which define a dimension/format etc.) can be decoupled from the management of the memory underlying the resources from the application's point of view.\n\nThe rest of this section dives into the details required to define \"Tiled Resources\" in the context of D3D.\n\nTo create a Tiled Resource, the flag D3D11_RESOURCE_MISC_TILED has to be specified as a MiscFlag on the Create* call. Restrictions on when this flag can be used are described later.\n\nWhereas a non-Tiled Resource's storage is allocated in the system when the resource is created (e.g. CreateTexture2D API call), for a Tiled Resource, the storage for the Resource contents is not allocated. Instead, when a Tiled Resource is created at the API, the system makes an address space reservation for the tiled surface's area only, and then allows the mapping of the tiles to be controlled by the application. The \"mapping\" of a tile is simply the physical location in memory that a logical tile in a resource points to (or NULL for an unmapped tile). This is not to be confused with the notion of mapping a D3D resource for CPU access, which despite using the same name is completely independent. The developer will be able to define and change the mapping of each tile individually as needed, knowing that all tiles for a surface don't need to be mapped at a time, thereby making effective use of the amount of memory available.\n\nWhen the flag D3D11_RESOURCE_MISC_TILED is specified on a resource, the tiles that make up the resource come from pointing at locations in a Tile Pool. A Tile Pool is a pool of memory (backed by one or more allocations behind the scenes - unseen by the application) that simple to manage by the operating system / driver and whose memory footprint is easily understood by an application. Tiled Resources map 64KB regions by pointing to locations in a Tile Pool. One fallout of this setup is it allows multiple Resources to share/reuse the same tiles, and also for the same tiles to be reused at different locations within a Resource if desired.\n\nThe cost for the flexibility of populating the tiles for a Resource out of a Tile Pool is that the Resource has to do the work of defining and maintaining the mapping of which tiles in the Tile Pool represent the tiles needed for the Resource. Tile mappings can be changed. Also, not all tiles in a Resource need to be mapped at a time; it is a feature to be able to have NULL mappings - that is the definition of a tile not being available from the point of view of the Resource accessing it.\n\nMultiple Tile Pools can be created, and any number of Tiled Resources can map into any given Tile Pool at the same time. Tile Pools can also be grown or shunk (see Resizing Tile Pools(5.9.2.2.2) for details). One constraint, existing merely to simplify driver and runtime implementation, is that a given Tiled Resource may only have mappings into at most one Tile Pool at a time (as opposed to having simultaneous mapping to multiple Tile Pools).\n\nThe amount of storage associated with a Tiled Resource itself (independent Tile Pool memory) should be roughly proportional to the number of tiles actually mapped to the pool at any given time. In hardware this boils down to scaling the memory footprint for page table storage roughly with the amount of tiles that are mapped (e.g. using a multilevel page table scheme as appropriate).\n\nThe initial contents of the page table are NULL for all entries. Applications also can't pass initial data for the memory contents of the surface since it starts off with no memory backing.\n\nApplications can create one or more Tile Pools per D3D device. The total size of a given Tile Pool is be restricted to D3D11's resource size limit, which is roughly 1/4 of GPU ram.\n\nA Tile Pool is made of 64KB tiles, but the operating system (driver) manages the entire pool as one or more allocations behind the scenes - the breakdown is not visible to applications. Tiled Resources define content by pointing at tiles within a Tile Pool. Unmapping a tile from a Tiled Resource is done simply by pointing it to NULL. Such unmapped tiles have rules about the behavior of reads or writes (defined later).\n\nA Tile Pool is created via the CreateBuffer API using a flag to indicate it is a tile pool.\n\nA ResizeTilePool()(5.9.3.4) API allows a Tile Pool to be grown if the application needs more working set for the Tiled Resource(s) mapping into it, or shunk if less space is needed. Another options for applications is to allocate additional Tile Pools for new Tiled Resources, however if any singe Tiled Resource needs more space than initially available in its Tile Pool, growing the Tile Pool is a good option. A Tiled Resource can't have mappings into multiple Tile Pools at once.\n\nWhen a Tile Pool is grown, additional Tiles are added to the end via one or more new allocations by the driver (breakdown into allocations not visible to the application). Existing memory in the Tile Pool is left untouched and existing Tiled Resource mappings into that memory remain intact.\n\nWhen a Tile Pool is shrunk, tiles are removed from the end (this is allowed even below the initial allocation size, down to 0), meaning new mappings cannot be made past the new size. Existing mappings past the end of the new size, however, remain intact and useable, and Drivers will keep the memory around as long as mappings to any part of the allocation(s) the driver uses for the Tile Pool memory remains. If after shrinking, some memory has been kept alive because Tile Mappings are pointing to it and the Tile Pool is regrown, again (by any amount), the existing memory is reused first before any additional allocations occur to service the size of the grow operation.\n\nTo be able to save memory, an application has to not only shrink a Tile Pool but also remove/remap existing mappings past the end of the new smaller Tile Pool size.\n\nThe act of shrinking (and removing mappings) doesn't necessarily produce immediate memory savings. Freeing of memory depends on how granular the driver's underlying allocations for the Tile Pool are - when shrinking happens to be enough to make a driver allocation unused, the driver can free it. If a Tile Pool was grown, it is most likely that shrinking to previous sizes (and removing/remapping tile mappings correspondingly) will yield memory savings, though not guaranteed in the case that the sizes don't exactly align with the underlying allocation sizes chosen by the driver.\n\nFor non-Tiled Resources, D3D is able to prevent certain hazard conditions during rendering. For example, the D3D runtime does not allow any given SubResource to be bound as an input (such as a ShaderResourceView) and as an output (such as a RenderTargetView) at the same time. If such a case is encountered, the runtime unbinds the input. This tracking overhead in the runtime is cheap and is done at the SubResource level. One of the benefits of this is to minimize the chances of applications accidentally depending on hardware shader execution order - something that could vary if not on a given GPU, certainly would vary across different GPUs.\n\nIt may, however, be too expensive to do similar work on a per-tile level that may be necessary for Tiled Resources, since tracking would be at a tile level. New issues arise such as possibly validating away attempts to render to an RTV with one tile mapped to multiple areas in the surface simultaneously. If it turns out this per-tile hazard tracking is too expensive for the D3D runtime, ideally this would at least be an option in the Debug Layer.\n\nApplications are required to inform the driver when it has issued a write or read to a tiled resource that refrences tile pool memory that will also be referenced by separate tiled resources in upcoming read or write operations and is expecting the first operations to complete before the second can begin. See the TiledResourceBarrier()(5.9.3.5) command.\n\nThere are some constraints on the type of D3D resources allowed to be created with the D3D11_RESOURCE_MISC_TILED flag. The valid parameters are:\n\nSupported Resource Type: Texture2D[Array] (incl. TextureCube[Array], which is a variant of Texture2D[Array]), Buffer (not Texture1D[Array] or Texture3D - Texture3D expected for future).\n\nSupported Bind Flags: D3D11_BIND_SHADER_RESOURCE, _RENDER_TARGET, _DEPTH_STENCIL, _UNORDERED_ACCESS (not _CONSTANT_BUFFER, _VERTEX_BUFFER [note that binding a tiled Buffer as an SRV/UAV/RTV is still ok], _INDEX_BUFFER, _STREAM_OUTPUT, _BIND_DECODER, _BIND_VIDEO_ENCODER)\n\nSupported Formats: All formats that would be available for the given configuration regardless of it being tiled, with some exceptions detailed elsewhere.\n\nSupported SampleDesc (Multisample count, quality): Whatever would be supported for the given configuration regardless of it being tiled, with some exceptions detailed elsewhere.\n\nSupported Width/Height/MipLevels/ArraySize:Full extents supported by D3D11. Tiled Resources do not have the restriction on total memory size imposed on non-Tiled Resources - they are only constrained by overall Virtual Address Space limits(5.9.2.3.1).\n\nThe initial contents of Tile Pool memory are undefined.\n\nOn 64 bit OSs, at least 40 bits of virtual address space (1 Terabyte) is available.\n\nFor 32 bit OSs, the address space is 32 bit. For 32 bit ARM systems, individual Tiled Resource creation can fail if the allocation would use more than 27 bits of address space (128 MB). This includes any hidden padding in the address space the hardware may use for mipmaps, packed tile padding, and possibly padding surface dimensions to powers of 2.\n\nOn systems with a separate page table for the GPU, most of this address space will be available to GPU resources made by the application, though GPU allocations made by the driver fit in the same space.\n\nOn future systems with a page table shared between the CPU and GPU, the available address space is shared between all CPU and GPU allocations in a process.\n\nTile Pools are defined by the following application specified properties (via the CreateBuffer API):\n\nSize: Allocation size, as a multiple of 64KB (0 is valid since there is a Resize operation available).\n\nTile Pools can be shared with other processes just like traditional resources. Tiled Resources (which reference Tile Pools) cannot be shared across devices/processes. However separate processes can create their own Tiled Resources that map to Tile Pool(s) shared between them.\n\nFormats containing stencil are not supported with Tiled Resources.\n\nThis includes DXGI_FORMAT_D24_UNORM_S8_UINT (and related formats in the R24G8 family) and DXGI_FORMAT_D32_FLOAT_S8X24_UINT (and related formats in the R32G8X24 family).\n• void UpdateTileMappings() / CopyTileMappings()(5.9.3.1) - Point tile locations in a Tiled Resource to locations in Tile Pool(s) (and/or to NULL). Able to update a disjoint subset of the tile pointers.\n• Copy*() / Update*() - All the APIs that can copy data to/from a DEFAULT pool surface work for Tiled Resources. Reading from unmapped tiles produces 0 and writes to unmapped tiles are dropped.\n• In addition, custom APIs exist for copying tiles(5.9.3.3) at 64KB granularity to/from any Tiled Resource and a Buffer Resource in a canonical memory layout (described in the Copying Tiles(5.9.3.3) section). The driver/hardware performs any memory \"swizzling\" necessary for the Tiled Resource.\n• D3D pipeline bindings and View creations / bindings that would work on non Tiled Resources work on Tiled Resources as well.\n\nTile controls are available on immediate or deferred contexts (just like updates to normal Resources) and upon execution impact subsequent accesess to the tiles (not previously submitted operations).\n• The lifetime of Tile Pools works like any other D3D Resource, backed by reference counting, including in this case tracking of mappings from Tiled Resources. When the application no longer references a Tile Pool and any tile mappings to the memory are gone and GPU accesses completed, a Tile Pool will be deallocated.\n• APIs related to surface sharing and synchronization work for Tile Pools (but not directly on Tiled Resources). Similar to the behavior for offered Tile Pools, D3D commands that access Tiled Resources pointing to a Tile Pool are dropped if the Tile Pool has been shared and is currently acquired by another device/process.\n• Offer()/Reclaim() - These existing APIs for yielding memory temporarily to the system operate on the entire Tile Pool (and are not available for individual Tiled Resources). If a Tiled Resource points to a tile in an offered Tile Pool, the Tiled Resource behaves as if it is offered (e.g. D3D runtime drops commands that reference it).\n\nData cannot be copied to/from Tile Pool memory directly. Accesses to the memory are always done through Tiled Resources.\n\nWhen a Tiled Resource is created, the dimensions, format element size and number of mipmaps and/or array slices (if applicable) determine the number of tiles that would be required to back the entire surface area. The pixel/byte layout within tiles is implementation-chosen (until such time as a standard layout is defined for future hardware). The number of pixels that fit in a tile, depending on the format element size, is fixed and identical whether using a (future) standard swizzle or not.\n\nThis means that the number of tiles that will be used by a given surface size and format element width is well defined/predictable based on the following tables. For Resources that contain mipmaps, or cases where surface dimensions don't fill a tile, however, there are some constraints, discussed later(5.9.2.8.5).\n\nDifferent Tiled Resources can point to the same memory with different formats as long as applications don't rely on the results of writing to the memory with one format and reading with another, unless the formats are in the same format family (have the same typeless parent format) - e.g. R8G8B8A8_UNORM and R8G8B8A8_UINT are compatible with each other but not with R16G16_UNORM. There is one exception where bleeding data from one format aliasing to another is well defined: If a tile completely contains 0 for all its bits can be used with any format that interprets those memory contents as 0 (regardless of memory layout). So a tile could be cleared to 0x00 with the format R8_UNORM and then used with a format like R32G32_FLOAT and it would appear the contents are still (0.0f,0.0f).\n\nThe layout of data within a tile does not depend on where the tile is mapped in a resource overall. So, for example, a tile can be reused in different locations of a surface at once with consistent behavior in all locations.\n\nTexture1D[Array] Tiled Resource support was designed as follows but not exposed for lack of utility.\n\nOther format bit counts not supported with Tiled Resources: 96bpp formats, video formats, R1_UNORM, R8G8_B8G8_UNORM, G8R8_G8B8_UNORM.\n\nOther format bit counts not supported with Tiled Resources: 96bpp formats, video formats, R1_UNORM, R8G8_B8G8_UNORM, R8R8_G8B8_UNORM.\n\nOnly sample counts 1 and 4 are required (and allowed) to be supported with Tiled Resources. 2, 8, and 16 are shown for future consideration.\n\nImplementations may choose to support 2, 8, and/or 16 sample MSAA for NON-Tiled Resources even though tiled resource don't support them.\n\nTiled Resources with sample counts larger than 1 cannot use 128bpp formats).\n\nThis takes the Texture2D tiling divides the x/y dimensions by 4 each and adds 16 layers of depth. All the tiles for the first plane (2D plane of tiles defining the first 16 layers of depth) appear before the subsequent planes.:\n\nTexture3D support in Tiled Resources is not exposed in the initial implementation of Tiled Resource, but the desired tile shapes are listed here for consideration in a future release.\n\nOther format bit counts not supported with Tiled Resources: 96bpp formats, video formats, R1_UNORM, R8G8_B8G8_UNORM, R8R8_G8B8_UNORM.\n\nA Buffer Resource is trivially divided into 64KB tiles, with some empty space in the last tile if the size is not a multiple of 64KB.\n\nStructured Buffers must have no constraint on the Stride to be Tiled, however possible performance optimizations in hardware for using Structured Buffers may be sacrificed by making them Tiled in the first place.\n\nDepending on the Tier(5.9.7) of Tiled Resources support, mipmaps with certain dimensions do not follow the standard tile shapes and are considered to all be packed together with one another in a manner that is opaque to the application. Higher Tiers of support have broader guarantees about what types of surface dimensions fit in the standard tile shapes (and can therefore be individually mapped by applications).\n\nWhat can vary between implementations is that - given a Tiled Resource's dimensions, format, number of mipmaps and array slices - some number M of mips (per array alice) may be packed into some number N tiles. The GetResourceTiling()(5.9.3.2) API exists to allow the driver to report to the application what M and N are (among other details about the surface that this API reports that are standard and do not vary by IHV). The set of tiles for the packed mips are still 64KB and can be individually mapped into disparate locations in a Tile Pool, however the pixel shape of the tiles and how the mipmaps fit across the set of tiles is IHV specific and too complex to expose. So applications are required to either map all of the tiles that are designated as packed, or none of them, at a time. Otherwise the behavior for accessing the Tiled Resource is undefined.\n\nFor arrayed surfaces, the set of packed mips and the number of packed tiles storing those mips (M and N described above) applies individually for each array slice.\n\nDedicated APIs for CopyingTiles(5.9.3.3) cannot access packed mips. Applications that wish to copy data to/from packed mips can do so using all the non-Tiled Resource specific APIs for copying and rendering to surfaces.\n\nFor the purposes of populating the contents of mipmapped Tiled Resources for mips that are non packed (use the standard tile shapes) from CPU memory (e.g. Staging memory or user data pointers), there is a well defined CPU-side layout for the tiling of all mipmaps independent of implementation (described in the Copying Tiles(5.9.3.3) section). Implementations can hide any differences in tile breakdown of mipmaps on the GPU side during Copy operations.\n\nThe following APIs allow manipulation and querying of tile mappings. Update calls only affect the tiles identified in the call, and others are left as defined previously.\n\nAny given tile from a Tile Pool can be mapped to multiple locations in a Resource and even multiple Resources. This includes tiles in a Resource that have an implementation chosen layout, described earlier, where multiple mipmaps are packed together into a single tile. The catch is that if data is written to the tile via one mapping, but read via a differently configured mapping, the results are undefined. Careful use of this flexibility can still be useful for an application though, like sharing a tile between resources that will not be used simultaneously, where the contents of the tile are always initialized through the same Resource mapping as they will be subsequently read from. Similarly a tile mapped to hold the packed mipmaps of multiple different Resources with the same surface dimensions will work fine - the data will appear the same in both mappings.\n\nChanges to tile assignments for a Resource can be made at any time in an immediate or deferred context.\n\nAs mentioned, existing methods in D3D for moving data around work with Tiled Resources just as if they are not Tiled, except that writes to unmapped areas are dropped and reads from unmapped areas produce 0. If a copy involves writing to the same memory location multiple times because multiple locations in the destination resource are mapped to the same tile memory, the resulting writes to multi-mapped tiles are nondeterministic/nonrepeatable - accesses happen in whatever order the hardware happens to execute the copy.\n\nThis section describes methods for the following additional methods of copying:\n\n (a) between tiles in a Tiled Resource (at 64KB tile granularity) and (to/from) a Buffer in GPU memory (or staging resource) - CopyTiles()\n\n (b) from application provided memory to tiles in a Tiled Resource - UpdateTiles()\n\n These methods swizzle/deswizzle as needed, and allow a D3D11_TILE_COPY_NO_OVERWRITE flag when the caller promises the destination memory is not referenced by GPU work that is in flight.\n\nThe tiles involved in the copy cannot include tiles containing packed mipmaps or results are undefined. To transfer data to/from mipmaps that the hardware packs into one tile, the standard (non-tile specific) Copy/Update APIs (or GenerateMips for the whole mip chain) must be used.\n\nUsing GenerateMips() on a resource with partially mapped tiles will produce results that simply follow the rules for reading and writing NULL applied to whatever algorithm the hardware/driver happens to use to GenerateMips(). So it is not particularly useful for an application to bother doing this unless somehow the areas with NULL mappings (and their effect on other mips during the generation phase) will have no consequence on the parts of the surface the application does care about.\n\nCopying tile data from a staging surface or from application memory would be the way to upload tiles that may have been streamed off disk, for example. A variation when streaming off disk is uploading some sort of compressed data to GPU memory and then decoding on the GPU. The decode target could be a buffer resource in GPU memory, from which CopyTiles() then copies to the actual Tiled Resource. This copy step allows the GPU to swizzle when the swizzle pattern is not known. Swizzling is not needed if the Tiled Resource itself is a Buffer resource (e.g. as opposed to a Texture).\n\nThe memory layout of the tiles in the non-tiled Buffer resource side of the copy is simply linear in memory within 64KB tiles, which the hardware/driver would swizzle/deswizzle per tile as appropriate when transferring to/from a Tiled Resource. For MSAA surfaces, each pixel's samples are traversed in sample-index order before moving to the next pixel. For tiles that are partially filled on the right side (for a surface that has a width not a multiple of tile width in pixels), the pitch/stride to move down a row is the full size in bytes of the number pixels that would fit across the tile if the tile was full. So there can be a gap between each row of pixels in memory. For specification simplicity, mipmaps smaller than a tile are not packed together in the linear layout. This seems to be a waste of memory space, but as mentioned copying to mips that the hardware packs together is not allowed via CopyTiles() or UpdateTiles(). The application can just use generic UpdateSubresource*() or CopySubresource*() APIs to copy small mips individually, though in the case of CopySubresource*() that means the linear memory has to be the same dimension as the Tiled Resource - CopySubresource*() can't copy from a Buffer resource to a Texture2D for instance.\n\nIf a hardware standard swizzle is defined, flags could be added indicate that the data in the Buffer is to be interpreted in that format (no swizzle necessary on transfer), though alternative approaches to uploading data may also make sense in that case such as allowing allowing applications direct access to Tile Pool memory.\n\nCopying operations can be done on an immediate or deferred context.\n\nTiled Resources can be used in Shader Resource Views, Render Target Views, Depth Stencil Views and Unordered Access Views, as well as some bindpoints where Views aren't used, such as Vertex Buffer bindings. See the list of supported bindings earlier. Copy* operations also work on Tiled Resources.\n\nIf multiple tile coordinates in one or more views is bound to the same memory location, reads and writes from different paths to the same memory will occur in a nondeterministic/nonrepeatable order of memory accesses.\n\nIf all tiles behind a memory access footprint from a shader are mapped to unique tiles, behavior is identical on all implementations to the surface having the same memory contents in a non-tiled fashion.\n\nBehavior for SRV reads that involve non-mapped tiles depends on the level of hardware support - see read behavior in Tiled Resources Feature Tiers(5.9.7) for a breakdown of requirements. The following summarizes the ideal behavior (which Tier 2 requires.\n\nConsider a texture filter operation that reads from a set of texels in an SRV. Texels that fall on non-mapped tiles contribute 0 in all non-missing components of the format, and the default for missing components(19.1.3.3), into the overall filter operation alongside contributions from mapped texels. The texels are all weighted and combined together undependent of whether the data came from mapped or non-mapped tiles.\n\nSome first generation Tier 2 level hardware does not meet this spec requirement and returns the 0 with defaults described above as the overall filter result if ANY texels (with nonzero weight) fall on non-mapped tiles. No other hardware will be allowed to miss the requirement to include all (nonzero weight) texels in the filter.\n\nBehavior of UAV reads and writes depends on the level of hardware support. See overall read and write behavior for Tiled Resources Feature Tiers(5.9.7) for a breakdown of requirements.\n\nShader operations that read from a non-mapped tile in a UAV return 0 in all non-missing components of the format, and the default for missing components(19.1.3.3).\n\nShader operations that attempt to write to a non-mapped tile cause nothing to be written to the non-mapped area (while writes to mapped area proceed). This ideal definition for write handling is not requried by Tier 2 - writes to non-mapped tiles may end up in a cache that subsequent reads could pick up.\n\nBehavior of DSV reads and writes depends on the level of hardware support. See overall read and write behavior for Tiled Resources Feature Tiers(5.9.7) for a breakdown of requirements.\n\nIf a tile is not mapped in the DepthStencilView, the return value from reading depth is 0, which is then fed into whatever operation(s) are configured for the depth read value. Write to the missing depth tile are dropped. This ideal definition for write handling is not requried by Tier 2 - writes to non-mapped tiles may end up in a cache that subsequent reads could pick up.\n\nBehavior of RTV reads and writes depends on the level of hardware support. See overall read and write behavior for Tiled Resources Feature Tiers(5.9.7) for a breakdown of requirements.\n\nOn all implementations it is valid for different RTVs (and DSV) bound simultaneously have different areas mapped vs non-mapped and have different sized surface formats (which means different tile shapes).\n\nReads from RenderTargetViews return 0 in all non-missing components of the format, and the default for missing components(19.1.3.3). Writes to RenderTargetViews are dropped. This ideal definition for write handling is not requried - writes to non-mapped tiles may end up in a cache that subsequent reads could pick up.\n\nIf tiles in the source and dest area of a Copy* operation have duplicated mappings in the copy area that would have overlapped even if both resources were not Tiled Resources and the Copy* call supports overlapping copies, this will behave fine (as if the source is copied to a temp before going to the dest). However if the overlap is not obvious (like the source and dest resources are different but share mappings, or mappings are duplicated over a given surface), then results of the copy operation on the tiles that are shared are undefined.\n\n5.9.4.4.2 Copying To Tiled Resource with Duplicated Tiles in Dest Area\n\nCopying to a Tiled Resource with duplicated tiles in the destination area produces undefined results in these tiles unless the data itself is identical - different tiles may write the tiles in different orders.\n\nSuppose an Unordered Access View on a Tiled Resource has duplicate tile mappings in its area or with other resources bound to the pipeline. Ordering of accesses to these duplicated tiles is undefined if performed by different threads, just as any ordering of memory access to UAVs in general is unordered.\n\n5.9.4.4.4 Rendering After Tile Mapping Changes Or Content Updates from Outside Mappings\n\nIf a Tiled Resource's Tile Mappings have changed or content in mapped Tiled Pool tiles have changed via another Tiled Resource's mappings, and the Tiled Resource is going to be rendered via RenderTargetView or DepthStencilView, the application must Clear (using the fixed function Clear APIs) or fully copy over using Copy*/Update* APIs the tiles that have changed within the area being rendered (mapped or not). Failure of an application to clear/copy in these cases results in hardware optimization structures for the given RenderTargetView or DepthStencilView being stale and will result in garbage rendering results on some hardware and inconsistency across different hardware. These hidden optimization data structures used by hardware may be local to individual mappings, not visible to other mappings to the same memory.\n\nThe ClearView API/DDI supports clearing RenderTargetViews with rects, and for hardware that supports Tiled Resources, ClearView must also support clearing of DepthStencilViews with rects, for depth only surfaces (without stencil). This allows applications to Clear only the necessary area of a surface.\n\nIf an application needs to preserve existing memory contents of areas in a Tiled Resources where mappings have changed it has to work around the Clear requirement, unfortunately. This can be accomplished by the application by first saving the contents where Tile mappings have changed (by copying them to a temporary surface, for example using CopyTiles()), issuing the required Clear and then copying the contents back. While this would accomplish the task of preserving surface contents for incremental Rendering, the downside is that is that subsequent rendering performance on the surface may suffer because rendering optimizations may be lost.\n\nIf a tile is mapped into multiple Tiled Resources at the same time and tile contents are manipulated by any means (render, copy etc.) via one of the Tiled Resoruces then if the same tile is to be rendered via any other Tiled Resource, the tile must be Cleared first as above.\n\nSuppose an area in a Tiled Resource is being rendered to and the Tile Pool tiles referenced by the render area are also mapped to from outside the render area (including via other Tiled Resources, at the same time or not). Data rendered to these tiles is not guaranteed to appear correctly when viewed through the other mappings, even though the underlying memory layout is compatible. This is due to optimization data structures some hardware uses that can be local to individual mappings for renderable surfaces, not visible to other mappings to the same memory location. This restriction can be worked around by copying from the rendered mapping to all the other mappings to the same memory that might be accessed (or clearing that memory or copying other data to it if the old contents are no longer needed). While this seems redundant, it makes all other mappings to the same memory correctly understand how to access its contents, and at least the memory savings of having only a single physical memory backing remains intact. Also, note that when switching between using different Tiled Resources that share mappings (unless only reading), the TiledResourceBarrier API must be called in between.\n\nIf an area in a Tiled Resources is being rendered to and within the render area multiple tiles are mapped to the same Tile Pool locaition, rendering results are undefined on those tiles.\n\nSuppose multiple Tiled Resources have mappings to the same Tile Pool locations and each resource is used to access the same data. This is only valid if the other rules about avoiding problems with hardware optimization structures are avoided, appropriate calls to TiledResourceBarrier made and the Tiled Resources are compatible with each other. The latter is described here (in terms of what it means for Tiled Resources sharing tiles to be incompatible). The conditions incompatibility accessing the same data across duplicate tile mappings are the use of different surface dimensions, format, or differences the presence of RenderTarget or DepthStencil BindFlags on the Resources. Writing to the memory with one type of mapping produces undefined results if subsequently reading or rendering via a mapping from an incompatible Resource. If the other Resource sharing mappings will be first initialized with new data (recycling the memory for a different purpose), that is fine since data is not bleeding across incompatible interpretations, however the TiledResourceBarrier API must be called when switching between accessing incompatible mappings like this.\n\nIf the RenderTarget or DepthStencil BindFlag is not set on any of the resources sharing mappings with each other, there are far fewer restrictions: As long as the format and surface types (e.g Texture2D) are the same, tiles can be shared. Some cases of different format are compatible such as BC* surfaces and the equivalent sized uncompressed 32 bit or 16 bit per component format, like BC6H and R32G32B32A32. Many 32 bit per element formats can be aliased with R32_* as well (R10G10B10A2_*, R8G8B8A8_*, B8G8R8A8_*,B8G8R8X8_*,R16G16_*) - this has always been allowed for non Tiled Resources.\n\nSharing between packed and non-packed tiles is fine if the formats are compatible and the tiles are filled with solid color.\n\nFinally, if nothing is common about the Resources sharing tile mappings except that none have RenderTarget/DepthStencil BindFlags, then only memory filled with 0 can be shared safely - it will appear as whatver 0 decodes to for the definition of the given Resource format (typically just 0).\n\nThe texture sampling features described here require Tier(5.9.7) 2 level of Tiled Resources support.\n\nAny instruction that reads and/or writes to a Tiled Resource causes status information to be recorded. This is exposed as an optional extra return value on every resource access instruction that goes into a 32-bit temp register. The contents of the return value are opaque - direct reading by the shader program is disallowed. However dedicated instruction(s) (initially only one) allow status information to be extracted.\n\nThe check_access_mapped(22.4.26) instruction interprets the status return from a memory access and indicates whether all data being accessed was mapped in the resource - true (0xFFFFFFFF) or false (0x00000000).\n\nDuring filter operations, sometimes the weight of a given texel ends up being 0.0. An example is a linear sample with texture coordinates that fall directly on a texel center: 3 other texels (which ones they are can vary by hardware) contribute to the filter - but with 0 weight. These 0 weight texels do not contribute to the filter result at all, so if they happen to fall on NULL tiles they don't count as an unmapped access. Note the same guarantee applies for texture filters that include multiple mip levels - if the texels on one of the mipmaps is not mapped but the weight on those texels is 0, those texels don't count as an unmapped access.\n\nWhen sampling from a format that has fewer than 4 components (such as DXGI_FORMAT_R8_UNORM), any texels that fall on NULL tiles result in the a NULL mapped access being reported regardless of which component(s) the shader actually looks at in the result. For example reading from R8_UNORM and masking the read result in the shader with .gba/.yzw wouldn't appear to need to read the texture at all, but if the texel address is a NULL mapped tile it still counts as a NULL map access.\n\nTo help shaders avoid areas in mipmapped Tiled Resources that are known to be non-mapped, most shader instructions that involve using a Sampler (filtering) have a new mode that allows the shader to pass an additional float32 MinLOD clamp parameter to the texture sample. This value is in the View's mipmap number space, as opposed to the underlying resource.\n\nThe hardware performs max(fShaderMinLODClamp,fComputedLOD) in the same place in the LOD calculation where the per-Resource MinLOD clamp occurs (which is also a max()).\n\nIf the result of applying the Per-sample LOD clamp and any other LOD clamps defined in the sampler is an empty set, the result is the same out of bounds access result as the per-Resource minLOD clamp: 0 for components in the surface format and defaults for missing components.\n\nThe lod instruction (which predates the per-sample minLOD clamp described here) returns both a clamped and unclamped LOD. The clamped LOD return from this lod instruction reflects all clamping including the per-resource clamp, but not a per-sample clamp. Per-sample clamp is controlled/known by the shader anyway, so the shader author can manually apply that clamp to the lod instruction's return value if desired.\n\nThe following shader instructions include combinations of feedback and/or clamp in addition to their basic operation, followed by instructions that examine the feedback return. If the clamp is used, it is an additional scaler float32 register or immediate operand. If feedback is requested, it comes out in an additional 32 bit scalar register operand that needs to be fed into instruction(s) that interpret feedback.\n\nThese instructions can be used on Tiled or non-Tiled Resources for all applicable resource dimensions (Buffer, Texture1D/2D/3D). Non-Tiled Resources always appear to be fully mapped.\n\nThe suffix _s indicates mapping status, and _cl indicates LOD clamp.\n\nThe following instructions have a mapping status return option [_s] (but no clamp option):\n\nThe following instructions have both mapping status [_s] and clamp [_cl] options:\n\nThe following instruction examines the status return from any of the above instructions:\n\nMin/Max Reduction filtering is a mode on Samplers that fetches the same set of texels that a normal texture filter would fetch, but instead of blending the values to produce an answer, it returns the min() or max() of the texels fetched, on a per-component basis (e.g. the min of all the R values, separately from the min of all the G values etc.)\n\nThe min/max operations follow D3D arithmetic precision rules. The order of comparisons does not matter.\n\nDuring filter operations that are not min/max, sometimes the weight of a given texel ends up being 0.0. An example is a linear sample with texture coordinates that fall directly on a texel center - 3 other texels (which ones they are may vary by hardware) contribute to the filter but with 0 weight. For any of these texels that would be 0 weight on a non-min/max filter, if the filter is min/max these texels still do not contribute to the result (and the weights do not otherwise affect the min/max filter operation).\n\nThe full list of filter modes is shown in the D3D11_FILTER enum in the Sampler State(7.18.3) section - note the modes with MINIMUM and MAXIMUM in the name.\n\nSupport for this feature depends on Tier(5.9.7) 2 support for Tiled Resources.\n\nNew HLSL syntax is required to support tiled resources in Shader Model 5.0 (allowed only on devices with Tiled Resources support). Each relevant HLSL intrinsic method for tiled resources (see the table below) accepts either one (feedback) or two (clamp and feedback in this order) additional optional parameters. For example, the Sample method is:\n\nThe offset, clamp and feedback parameters are optional. Programmers have to specify all optional parameters up to the one they need, which is consistent with the C++ rules for default function arguments. For example, if the feedback status is needed, both offset and clamp parameters need to be explicitly supplied to Sample, even though they may not be logically needed.\n\nThe clamp parameter is a scalar float value. The literal value of clamp=0.0f indicates that clamp operation is not performed.\n\nThe feedback parameter is a uint variable that can be supplied to memory-access querying intrinsic: CheckAccessFullyMapped. Programmers must not modify or interpret the value of the feedback parameter; however, the compiler does not provide any advanced analysis and diagnostics to detect this.\n\nThere is one HLSL intrinsic to query the feedback status:\n\nCheckAccessFullyMapped interprets the value of FeedbackVar and returns true if all data being accessed was mapped in the resource; otherwise, CheckAccessFullyMapped returns false.\n\nIf either clamp or feedback parameter is present, the compiler emits a variant of the basic instruction. For example, Sample of a tiled resource generates sample_cl_s instruction. If neither clamp nor feedback is specified, the compiler emits the basic instruction, so that there is no change from the current behavior. The clamp value of 0.0f indicates that no clamp is performed; thus, the driver compiler can further tailor the instruction to the target hardware. If feedback is a NULL register in an instruction, the feedback is unused; thus, the driver compiler can further tailor the instruction to the target architecture.\n\nIf the HLSL compiler infers that clamp is 0.0f and feedback is unused, the compiler emits the corresponding basic instruction (e.g., sample rather than sample_cl_s).\n\nIf a tiled resource access consists of several constituent byte code instructions, e.g., for structured resources, the compiler aggregates individual feedback values via the OR operation to produce the final feedback value. Therefore, programmers see a single feedback value for such a complex access.\n\nThis is the summary table of HLSL intrinsic methods changed to support feedback and/or clamp. These all work on tiled and non-tiled resources of all dimensions. Non-tiled resources always appear to be fully mapped.\n\nThis existing DDI includes new options on the MiscFlags parameter:\n\nThis existing enum for filter types has new entries for min/max filtering.\n\nThis section is not part of the requirements for the initial implementation of Tiled Resources - it is for future consideration only.\n\nTexture filtering shader instructions can view Texture2DArray Resources as if all the array slices are arranged in a \"quilt\"/grid that appears as one surface rather than an array of them.\n\nAny Texture2DArray Resource that is not Multisampled can have a Quilted Shader Resource View created on it. Starting with a Texture2DArray Resource, the following parameters describe how to define a Quilt:\n\nShaders have to declare the dimension (e.g. Texture2D) of any SRV they access. This applies to Quilted Texture2D SRVs as well (the Quilt property will be part of the dimensionality naming).\n\nAny Shader instruction that involves the texture filtering hardware (e.g. instructions that take a Sampler as a parameter) sees the Quilting on a Quilted Texture2D, but addresses the surface using the same coordinates as if it is a Texture2DArray. That means that the texture coordinates include an integer array slice in addition to the U/V normalized coordinates. The U/V normalized coordinates are relative to the selected array slice. So coordinates in the range [0..1] span the selected array slice, just like a normal Texture2DArray. However U/V coordinates outside [0..1] refer to the appropriate neighboring array slice in the Quilt layout. e.g. a U coordinate of 1.5 indicates the middle of the array slice to the right in the quilt. The texture filtering hardware knows how to navigate the quilt in this fashion for each individual texel that is fetched.\n\nHardware derivative calculations do not understand anything about Quilting; they are not able to remap coordinates from different array slices into the same number space.\n\nThe ability of the filtering hardware to traverse over the Quilt applies to the mipmaps as well.\n\nWhen falling off an edge of the entire Quilt, the coordinate wraps to the other side of the entire Quilt. The Sampler addressing configuration (wrap/mirror/border etc.) is ignored for Quilts.\n\nThe resinfo instruction (which reports texture dimensions to the shader) reports the dimensions of a Quilted Texture2D not in terms of the underlying Texture2DArray but rather as if it is a large non-array texture whose width/height span the quilt. The number of mipmaps is of course the same for every array slice as for the entire quilt.\n\nWindows Blue exposes Tiled Resources support in two tiers using caps. In future releases, a new tier may be added including the recommendations listed below.\n• No 2, 8 or 16 sample MSAA support. Only 4x is required, except no 128bpp formats.\n• No standard swizzle pattern (layout within 64KB tiles and tail mip packing is up to the IHV).\n• Limitations on how tiles can be accessed when there are duplicate mappings, described here(5.9.4.4).\n• Tiled Resources may have NULL mappings but reading from them or writing to them produces undefined results, including Device Removed. Applications can get around this by mapping a single dummy page to all the empty areas. Care must be taken if writing/rendering to a page mapped to multiple rendertarget locations, however, as the order of writes will be undefined.\n• Shader instructions for clamping LOD and mapped status feedback are not available.\n• Alignment constraints for standard tile shapes: It is only guaranteed that mips (starting from the finest) whose dimensions are all multiples of the standard tile size support the standard tile shapes and can have individual tiles arbitrarily mapped/unmapped. The first mipmap in a Tiled Resource that has any dimension not a multiple of standard tile size, along with all coarser mipmaps, may have an non-standard tiling shape, fitting into N 64KB tiles for this set of mips at once (N reported to the application). These N tiles are considered packed as one unit which must be either fully mapped or fully unmapped by the application at any given time, though the mappings of each of the N tiles can be at arbitrarily disjoint locations in a Tile Pool.\n• Tiled Resources with any mipmaps not a multiple of standard tile size in all dimensions are not allowed to have an array size larger than 1.\n• In order to switch between referencing tiles in a tile pool via a Buffer resource to referencing the same tiles via a Texture resource, or vice-versa, all mappings for the original resource (i.e. Buffer when going from Buffer to Texture and Texture when going from Texture to Buffer) must be set to NULL by the application before new mappings for the new resource type are defined. Otherwise behavior is undefined including the chance of device reset. So for example calling UpdateTileMappings() to define tile mappings for a Buffer, then UpdateTileMappings() to the same tiles in the Tile Pool via a Texture2D resource, then accessing the tiles via the Buffer is invalid.\n• All features of the previous tier (without Tier 1 specific limitaitons) plus the following additions:\n• Shader instructions for clamping LOD and mapped status feedback are available.\n• Reads from non-mapped tiles return 0 in all non-missing components of the format, and the default for missing components(19.1.3.3).\n• Writes to non-mapped tiles are stopped from going to memory but may end up in caches that subsequent reads to the same address may or may not pick up.\n• Texture filtering with a footprint that straddles NULL and non-NULL tiles contributes 0 (with defaults for missing format components) for texels on NULL tiles into the overall filter operation. Some early hardware does not meet this requirement and returns 0 (with defaults for missing format components) for the full filter result if ANY texels (with nonzero weight) fall on a NULL tile. No other hardware will be allowed to miss the requirement to include all (nonzero weighted) texels in the filter operation.\n• NULL texel accesses cause the CheckAccessFullyMapped operation on the status feedback for a texture read to return false. This is regardless of how the texture access result may get write masked in the shader and how many components are in the texture format (the combination of which may make it appear that the texture does not need to be accessed).\n• Alignment constraints for standard tile shapes: Mipmaps that fill at least one standard tile in all dimensions are guaranteed to use the standard tiling, with the remainder considered packed as a unit into N tiles (N reported to the application). The application can map the N tiles into arbitrarily disjoint locations in a Tile Pool, but must either map all or none of the packed tiles. The mip packing is a unique set of packed tiles per array slice.\n• Tiled Resources with any mipmaps less than standard tile size in any dimension are not allowed to have an array size larger than 1.\n• Limitations on how tiles can be accessed when there are duplicate mappings, described here(5.9.4.4), continue to apply.\n• 2, 8 and 16 sample MSAA support, except perhaps no 128bpp formats.\n• Writes to non-mapped tiles are dropped, without altering cache contents, so subsequent reads always return 0.\n• Removed the constraint that Tiled Resources with mipmaps less than standard tile size in any dimension are not allowed to have an array slice larger than 1.\n• Standard swizzle (to be defined) and no more alignment constraints on mip dimensions that cause mips with particular dimensions to have an opaque layout.\n\nThe CheckFeatureSupport DDI has a query for Tiled Resources support:\n\nThis query reports support via flags bitfield to allows for some amount of future expansion of the caps reporting at the DDI needed. The Tier flags are cumulative (if the runtime sees Tier 2 support it assumes Tier 1 support regardless of the flag).\n\nAt the API, the Tiers are exposed via CheckFeatureSupport using an enum for the Tiers. Support for Min/Max Filtering is called out as a separate cap since the feature is distinct from Tiled Resources, however the runtime simply sets this capability true for hardware that supports Tier 2 and false for any lower level.\n\nThe CheckMultisampleQualityLevels1 API and corresponding CheckMultisampleQualityLevels DDI now has a flags field to allow the driver to be queried for their level of support for Multisampling on Tiled Resources (which can be different from the level of support for non-tiled resources - the number of Quality Levels for example).\n\nThe objectives of the features described in this section are to enable efficient distribution of rendering workload/ overhead in the application, runtime, and driver across multiple CPU cores in D3D11. These architectural changes are designed to allow multithreaded rendering applications to be written without overbearing restrictions, and gain close to the expected efficiency advantages when doing so.\n• Command Lists, (a.k.a. Display Lists) which can be created asynchronously in separate threads.\n\nA separate D3D11 API/DDI spec contains more concrete implementation details about the topics discussed here.\n\nApplications would like to create all object types (most particularly resources and shaders) on different threads simultaneously and in parallel with other rendering threads, especially to enable background or bulk loading/ compiling. D3D11 will continue to rely on shared resources to achieve fully parallel GPU usage or multi-GPU usage, which effectively means only limited resource sharing is available for such scenarios. Lastly, the ability to generate Command Lists also fits in well when trying to leverage multi-core CPUs, as long as each Command List can be built on separate CPU threads. However, Command Lists are still required to be executed by the one thread that is, generally, dedicated as the render thread.\n\nIt is important to note that although Command Lists are reusable across frames, the design point for this feature is use-once. Command List creation overhead in the runtime and driver should be low enough that single-use for the sole purpose of distribution of work across threads provides a significant performance win. Likewise, the overhead of submitting the Command List in the main rendering thread (immediate context) should be minimized – the design should diminish any need to patch or recompile Command Lists. If multi-use optimizations become interesting, implementations are encourages to promote such optimizations once a use-threshold has been reached. While the use of a single-use hint flag has been considered, detecting multi-use seems best to avoid application abuse/ mis-use of hints.\n\nOverview (the names here were chosen to align with kernel concepts to promote quicker understanding, and do not represent the final API or DDI):\n\nThe main aspects to notice are: the separation of IDevice from IContext (as IContext is expected to be implemented by two types of Contexts), the concept of a single Immediate Context per Device, the possibility of multiple Deferred Contexts, the Command List object types, and all the new methods that deal with these new objects. It is not expected that Map, Unmap, and GetData will work on a Deferred Context, while Finalize will not work on the Immediate Context. Further details and options are provided later.\n\nD3D11 allows creation routines to be thread re-entrant, as highlighted in the diagram by grouping such methods on the IDevice interface. This is not accomplished with coarse-grained critical sections. Fine-grained critical-sections are required internally, when necessary. Ideally, no internal synchronization needs to occur; but that is probably not realistic. Not only can one thread be rendering (i.e. calling Draw) while another thread is calling CreateShader; but two threads can be calling CreateShader, while a third thread calls CreateResource, and a fourth is rendering, etc. Due to symmetry, destruction of objects will also be re-entrant. However, the typical destruction of an object goes through multiple stages to keep destruction performant. See Deferred Destruction(6.4.3) for details.\n\nIn the D3D10 timeframe, the majority of drivers treated Initial Data passed to the Create functions equivalent to using UpdateSubresource, which is technically a rendering command that naturally presents obstacles for separating creation and rendering. In addition, the UpdateSubresource path would typically force the resource to be faulted into video memory. With changes to the OS kernel, the driver can use the Map/ Unmap path for Initial Data; but this path is unavailable for both Vista and Windows 7. Unfortunately, drivers are required to significantly change their current implementation surrounding this feature, in order to concurrently upload initial data without significantly perturbing the render thread/ frame rate. This is viewed as short-term pain, until the desired kernel changes are available, with an unknown duration for short-term.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 6.3.1 Overview\n\n 6.3.2 Fire and Forget Model, No Feedback\n\n 6.3.3 No Context State Inheritance\n\n 6.3.4 No Context State Aftermath\n\n 6.3.5 Object State Inheritance & Aftermath\n\n 6.3.6 Query Interactions\n\n 6.3.7 Nested Command Lists\n\n 6.3.8 Allow Map Write on Resources with Restriction\n\n 6.3.9 Application Immutable, but Patching is Still Required\n\n\n\nThe concept of a Command List has been around in other graphics APIs, and partially supported by features in previous versions of Direct3D. Instead of immediately executing graphics commands (or giving the impression of such a model), the graphics commands are recorded for execution later. In the overview, the Deferred Context represents the facility to generate Command Lists. Command Lists work well when supporting multi-core CPUs. Command Lists can be generated by separate threads, although they must be manually executed via the render thread using the Immediate Context. The threading model is that a Context (either Immediate or Deferred) cannot be manipulated by more than one CPU thread simultaneously. Two Contexts, however, can be manipulated simultaneously, in parallel with each other, etc. After generation, a Command List can be used multiple times; but cannot be altered by the application explicitly. The interface for a Deferred Context is generally the same as the Immediate Context, with some exceptions. After work has been built up with a Deferred Context, the Command List must be generated by invoking Finalize. By default, Finalize will leave the Deferred Context in a zombie state, waiting for the Deferred Context to be destroyed. However, there will be an option to reset the Deferred Context and allow a new sequence of commands to be recorded, effectively re-creating the Deferred Context. If specialized IContext methods designed for the Immediate Context are invoked off a Deferred Context, they fail; and vice versa.\n\nSince a Deferred Context is building up a deferred timeline for the GPU, the CPU must restrict itself to only sending data to the GPU in a fire-and-forget manor. Deferred Contexts cannot get any feedback from the GPU. Therefore, Resources cannot be Mapped, allowing read access. Query data cannot be retrieved, etc. Such operations can only be done by the rendering thread manipulating the Immediate Context, as the GPU is actually able to make forward progress and resolve the dependencies on data that the CPU requires.\n\nState Inheritance refers to the ability of the Command List to inherit the current state of the Immediate Context when executed. No Immediate Context state (such as bound render targets nor shaders) can be inherited by the Command List. The state of the Deferred Context always starts out in the default Context state (i.e. equivalent to giving the new Deferred Context ClearState, as its first command or equivalent to the Immediate Context state immediately upon creation).\n\nWhen a Command List is actually scheduled/ executed on either the Immediate or Deferred Context, the state of the Context (such as bound render targets and shaders) will altered afterward. The state of the Context will revert to the default Context state (ie. equivalent to executing ClearState implicitly immediate after Command List execution).\n\nWhile Command Lists and the Immediate Context state are effectively sheltered from each other, there is a form of Inheritance and Aftermath that needs to occur to make Command Lists useful: Resources and Query contents, etc. When a Command List executes on the Immediate Context, it inherits and can change the global state of objects, such as texture data, constant buffer data, and query data. Therefore it is possible to generate Command Lists that conditionally do different things, with creative use of Predicates and Resource data.\n\nQuery data can be generated by Deferred Contexts, just as Render Target data is generated; and Queries can be wrapped around Command List execution. However, there are some problematic cases that need to be handled, assuming the Query syntax remains unchanged.\n\nFirst, for Queries that have a Beginning and an End, like Predicates, such bracketing must stay local to a particular Context (i.e. Begin & End must occur within same command timeline). It is not possible for a Begin to happen on one Context to be matched with an End on another Context or Command List. For example, problematic cases are exposed when a bracketing is begun in the Immediate Context and ended by a Command List, and vice versa. This is not allowed, and is enforced. If a Command List manipulates a Query (where the corresponding Deferred Context called Begin or End on the Query), the Command List execution will not be allowed on a Context where the same Query has only been Begun. In addition, any Queries that have been Begun in the Deferred Contexts but not Ended, are implicitly Ended by the invocation to Finalize.\n\nSecond, when the Command List was being generated, was it assumed that the Command List execution could’ve been wrapped by any of the available Queries? This can be particularly troubling if a Query has hardware bugs related to it and needs some form of emulation. For example, if Blts are being emulated by the 3d pipeline, such operations are specified not to affect certain Queries. To satisfy the specification, the driver could poll any actively monitored counters and subtract off the Blt contribution from Query results. Such driver workarounds are hard to adapt to the Blts that may occur in a Command List. This does have implications on Software Command List implementations (i.e. it may not be known until Command List execution whether a software fallback will be leveraged, meaning the Deferred Context may need to build multiple types of Command Lists).\n\nCommand Lists can call Command Lists, i.e. Execute can be called on a Deferred Context. Once Command List usage becomes popular, preventing nested Command Lists presents an obstacle to quickly offload code from the Immediate Context to a Deferred Context. Reducing the disparity between Deferred Context authoring and Immediate Context authoring, when possible, removes obstacles to Deferred Context usage. Infinite recursion is prevented naturally due to the separation of Command List and Deferred Context (i.e. in order to execute a Command List, the Deferred Context must be Finalized). This also means that nested Command Lists are finalized before they can be called by other Command Lists. There is no limit on the level of Command List indirection; but a practial limit on how deep can be realistically tested.\n\nExecuting a Command List from a Deferred Context has the same State Aftermath as executing it on the Immediate Context: an implicit ClearState occurs. The Query restrictions that exist between Immediate Context and Deferred Context also exist for nested Command Lists.\n\n6.3.8 Allow Map Write on Resources with Restriction\n\nThe restriction that Deferred Contexts cannot Map any Resource presents an obstacle to quickly offload code from the Immediate Context to a Deferred Context. Efficiently written software and middleware inevitably use dynamic resources for quick upload to the GPU. Such software would have separate code-paths in order to be Context-agnostic (i.e. run against an Immediate Context or a Deferred Context) if Map is completely disallowed. However, if the first invocation to Map for a Deferred Context was a discard, and all Map were Write-Only, these resource operations can be captured without conceptual complications. The entire operation can be converted to be analogous to the UpdateSubresource scenario on the same Deferred Context. Reducing the disparity between Deferred Context authoring and Immediate Context authoring, when possible, removes obstacles to Deferred Context usage.\n\n6.3.9 Application Immutable, but Patching is Still Required\n\nFor all practical purposes, the application interprets the Command Lists as immutable, (i.e. constant after creation). However, there are some cases that could require modification of the Command List to some degree behind the scenes. These are forms of Resource renaming, though they are accomplished via different means.\n\nEven if Map were not allowed on the Deferred Context, there are still interactions between Command Lists and discarding Map that requires special attention. Imagine this code sequence:\n\nThe contents of the staging Buffer must be 2, not 1.\n\nThe following case is similar to Dynamic Buffers. Even though Present is not allowed on the Deferred Context, there are still interactions between Command Lists and Present that requires special attention. Present rotates the identities of the back buffers, which naturally must affect any Command List that contains references to the Back Buffers.\n\nResource read-after-write-hazards and other similar issues still need attention. One Command List could be executed which read from a Resource after another Display List that was executed which wrote to the same Resource. It may be feasible to do full pipeline flushes between the Command Lists which are used to achieve multi-CPU thread parallelism. A dual core probably only will execute one of these Command Lists per frame. But, Command Lists which are re-used will have a tendency to be smaller and used many times per frame. Full pipeline flushes may not be acceptable for such Command Lists.\n\nThe need to make certain DDI entry points thread re-entrant implies an increased awareness of threading at the DDI, and naturally, a myriad of changes to keep things efficient and reduce the propensity for bugs. With the increased usage of critical sections come the increased chances for deadlocks. For example, in D3D10, there was a well-defined ordering that critical sections must be acquired and released in, to prevent such deadlocks when holding critical sections simultaneously. If the following type of semantics (i.e. can one component hold a critical section during the invocation into another component) do not fall out of the general design of runtime and DDI, then there is increased burden of documentation and testing. If the API and callbacks could be designed such that the user mode driver needs no synchronization, internally, ensuring no deadlocks occur should be much easier.\n\nWith multiple threads in the user mode driver at one time, the DDI callbacks must be thread-safe. The DDI callbacks are generally thin wrappers around the thunks provided by DXGI. They isolate the driver from kernel handles and kernel function signatures. The kernel function signatures may change from OS release to OS release. D3D11 DDI callbacks have identical function signatures and functionality as D3D10 DDI callbacks. However, in contrast to D3D10 DDI callbacks, D3D11 DDI callbacks are designed to be free-threaded when used with a driver that support thread-safe creation. Callbacks used to satisfy creations will need to be thread re-entrant or provide thread re-entrant counterparts. Ideally D3D11 DDI callbacks would be completely free-threaded, but there are few restrictions that still remain. One restriction is that only a single thread can be working against a HCONTEXT at a time. Callbacks that use a HCONTEXT are pfnPresentCb, pfnRenderCb, pfnEscapeCb, pfnDestroyContextCb, pfnWaitForSynchronizationObjectCb, and pfnSignalSynchronizationObjectCb. Thus, if more than one thread is calling these callbacks using the same HCONTEXT, they are required to be synchronized. This is quite natural since these are callbacks that are likely to be called only from the thread that is manipulating the immediate context. Another restriction is that callbacks below are required to be invoked during DDI function calls using the same thread that called the DDI:\n• pfnAllocateCb: Invoke on the same thread which D3D10DDI_DEVICEFUNCS::pfnCreateResource was called when creating shared resources. Regular non-shared allocations with the device are fully free-threaded.\n• pfnRenderCb: Invoke on the same thread that invoked D3D10DDI_DEVICEFUNCS::pfnFlush. This is quite natural due to the HCONTEXT restrictions.\n\npfnDeallocateCb deserves special mention, as it is not required to be called before the driver returns from D3D10DDI_DEVICEFUNCS::pfnDestroyResource for the majority of resource types. Since pfnDestroyResource is a free-threaded function, the driver must defer destruction of the object until it can be efficiently ensured that no existing immediate context reference remains (i.e. that pfnRenderCb is called before calling pfnDeallocateCb). This applies even to shared resources, or any other invocation using HRESOURCE to complement HRESOURCE usage with pfnAllocateCb; but does not apply to primaries.\n\nOne of the basic tasks of the API is lifetime management of objects and handles. To stay efficient, the API prefers that object and handle destruction is deferred and amortized by default. Typically, deferment means until the GPU is no longer using the object. However, here, the term is meant to represent that the CPU is no longer using an object. The API will not delete an object whose ref count drops to 0. Instead, every flush of a command buffer gives the API an amortized opportunity to check to find those objects whose ref count is 0 and are no longer bound to the Immediate Context. This list of handles to delete can be provided to the driver to assist with an efficient flush. There may be additional mechanisms to destroy handles to suit all the needs of the API; but the guarantee will still exist that destroyed handles will not be currently bound to any context.\n\nThe user mode driver has to manipulate data local to each object/ handle involved, in order to interact with the driver models. For example, allocation lists have to be built up to accompany command buffer submissions. Because all objects are now becoming nearly process-global, modifying data directly associated with these objects would require synchronization. It is more efficient to have an area of memory strongly associated with each object, but also local to a context, allowing CPU thread modification of memory without synchronization. The user mode driver can provide the size required for such memory, to gain efficiency with anything the runtime needs to allocate also.\n\nThe runtime provides a default implementation of the Deferred Context that will emulate Command List support. Even if all the API features can be supported directly in hardware, this does help bootstrap a driver faster. In addition, it can possibly be leveraged for debugging.\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 7.1 Instruction Counts\n\n 7.2 Common instruction set\n\n 7.3 Temporary Storage\n\n 7.4 Immediate Constants\n\n 7.5 Constant Buffers\n\n 7.6 Shader Output Type Interpretation\n\n 7.7 Shader Input/Output\n\n 7.8 Integer Instructions\n\n 7.9 Floating Point Instructions\n\n 7.10 Vector vs Scalar Instruction Set\n\n 7.11 Uniform Indexing of Resources and Samplers\n\n 7.12 Limitations on Flow Control and Subroutine Nesting\n\n 7.13 Memory Addressing and Alignment Issues\n\n 7.14 Shader Memory Consistency Model\n\n 7.15 Shader-Internal Cycle Counter (Debug Only)\n\n 7.16 Textures and Resource Loading\n\n 7.17 Texture Load\n\n 7.18 Texture Sampling\n\n 7.19 Subroutines / Interfaces\n\n 7.20 Low Precision Shader Support in D3D\n\n \n\n\n\nFull details of the Shader models for each shader stage are provided in dedicated sections elsewhere in the spec. What follows is a discussion of a few general items (not an exhaustive list) that are common to all of the Shader models.\n\nThere are no limits on total shader program length or execution time (accounting for loops and subroutines), aside from any limitations in what may be expressed in the shader token format. Clearly longer programs will degrade in performance, but D3D . currently does not specify how steeply performance will degrade relative to program length or execution time given that there are so many variables that might affect performance.\n\nAside from a few exceptions, the instruction set for all the shader stages are identical. The exceptions are confined to instructions that only make sense in a given Shader unit. For example the sample instruction computes LOD based on derivatives, so sample and sample_b (sample with LOD bias) are only relevant in the Pixel Shader where derivatives are present, while sample_l (sample at selected LOD) and sample_d (sample with application-provided derivatives) is available in all stages.\n\nTemporary storage is composed of a single Element type, which is a -tuple of untyped -bit quantities. Temporary storage consists of two classes of storage: registers, which are non-indexed single elements; and arrays, which are indexable 1D arrays of elements. Temporary storage is read/write, and is uninitialized at the start of a Shader execution instance. Reads of temporary storage that has not been previously written within a Shader execution instance return undefined values, but cannot return data outside of the address space of the device context.\n\nTemporary registers are declared(22.3.35) r#, and can be used as a temporary operand in D3D . instructions.\n\nTemporary arrays are declared(22.3.36) as x#[n], where “n” is the array length (indexed with 0..n-1). Temporary arrays must be indexed by an r# scalar, statically indexed x# scalar, and/or and optional immediate constant (literal), and can have only one level of index nesting (e.g. x0[x1[r0.x+1].x+1] is not legal, but x0[x1[1].x+1] is legal). A temporary array reference, x#[?], can be used as a temporary operand in D3D . instructions (i.e. anywhere an r# can be used). Out of bounds access to x#[?] is undefined, except that data outside the GPU process context is never visible.\n\nThe total quantity of temporary storage per Shader execution instance is elements, which can be utilized in any combination of registers and arrays. i.e. the total number of r# and x# declared must be <= .\n\nNote that the namespace for r# and x# (the #) are independent. e.g. Suppose r2 and x2[5] are declared. They are independent, but together both count as 6 units of storage against the limit of temporary registers.\n\nTo provide a run-time stack, a program allocates a temporary array of a fixed size. The program should provide its own stack bounds checking, e.g., skip calls if the stack push would exceed the array bounds.\n\nThere is no limit on the total number of times a temp registers (the same one or different ones) that can appear in a single instruction or in a shader.\n\nFor any instruction source argument that is capable of taking a temporary register, it is also permitted to supply -bit immediate scalar or -bit immediate -vector in the Shader code. Only at most one source operand per instruction may be specified using an immediate value (having up to components). Immediate scalar values used in indexing of registers can only be used once per indexed operand in an instruction, and but these immediate values do not count against the limit of one immediate as a raw source operand. e.g. \"add r0, v[1 + r0.x], float4(1.0f,2.0f,3.0f,4.0f)\" is valid, since there is only one immediate source operand present (the float4), with the value 1 in the indexing of v[] not counting against the limit.\n\nIf a source operand is a Constant Buffer reference (see Constant Buffers below), the reference to a Constant Buffer DOES count against the same limit as immediate values. This allows implementations to provide immediate values through the same hardware path as Constant Buffers if desired. e.g. \"add r0, cb0[r1.x], float4(1.0f,2.0f,3.0f,4.0f)\" is invalid, since both an immediate value is used as well as a Constant Buffer read in the same instruction.\n\nThere is no limit on the total number of times immediate constants can appear in a single instruction or in a shader.\n\nThere are slots for ConstantBuffers that can be active per Pipeline stage. Indexing across ConstantBuffers is not permitted. A given ConstantBuffer is accessed as an operand to any Shader operation as if it is an indexable read-only register in the Shader. Unlike other Buffer binding locations in the pipeline, Constant Buffers do not allow Buffer offsets nor custom strides. The stride of the Buffer is assumed to be the Element width of R32G32B32A32_TYPELESS; and the first Element in the Buffer (at Buffer offset zero) is assumed to constant #[ 0 ], when referenced from the Shader.\n\nIn Shader code, just as a t# register is a placeholder for a Texture, a cb# register is a placeholder for a ConstantBuffer at \"slot\" #. A ConstantBuffer is accessed in a Shader using: cb#[index] as an operand to Shader instructions, where 'index' can be either an r# or statically indexed x# containing a -bit unsigned integer, an immediate -bit unsigned integer constant, or a combination of the two, added together. e.g. \"mov r0, cb3[x3[0].x+6]\" represents moving Element 7 from the ConstantBuffer assigned to slot 3 into r0, assuming x3[0].x contains 1.\n\nThere is no limit on the total number of times constant buffer reads (from any buffer and location in the buffer) that can appear in a single instruction or in a shader.\n\nThe declaration of a ConstantBuffer (cb# register) in a Shader includes the following information:\n• The size of the ConstantBuffer can be declared (a special flag will allow for unknown-length).\n• The Shader must indicate whether the ConstantBuffer will be accessed via Shader-computed offset values or only by literal offsets.\n• The order that the declaration of a cb# appears in a Shader, relative to other cb# declarations, defines the priority of that ConstantBuffer, starting at highest priority.\n\nOut of bounds access to ConstantBuffers returns in all components. Out of bounds behavior is always with respect to the size of the buffer bound at that slot.\n\nIf the constant buffer bound to a slot is larger than the size declared in the shader for that slot, implementations are allowed to return incorrect data (not necessarily ) for indices that are larger than the declared size but smaller than the buffer size.\n\nFetching from a ConstantBuffer slot with no Buffer present always returns in all components for all indices.\n\nIn addition to the aforementioned slots for Constant Buffers, every shader program can declare(22.3.4) a single Immediate Constant Buffer with up to -vector values. The data is tied to the shader program permanently, but otherwise behaves (gets accessed) by the shader exactly the same way as Constant Buffers.\n\nThere is no limit on the total number of times immediate constant buffer reads (from any location the buffer) can appear in a single instruction or in a shader.\n\nThe application is given control over the data type interpretation for Shader outputs (i.e. writing raw integer values vs. writing normalized float values) by simply choosing an appropriate format to interpret the output resource's contents as. See the Formats(19.1) section for detail.\n\nDetails on Shader input/output registers (indeed all registers) are provided in the sections dedicated to each Shader unit elsewhere in the spec.\n\nOne thing in common about input/output registers for all shaders is that if they are declared(22.3.30) to be dynamically indexable from the shader, and the shader indexes them out of the declared range, results are undefined, although no data from outside the GPU process context is never visible.\n\nThere is a collection of instructions available to Shaders which are dedicated to performing integer arithmetic and bitwise operations. Operands and output registers for integer instructions can be any of the register classes available to the floating point instructions. There is no data type associated with registers; Shader instructions determine how the data stored in registers is interpreted. Integer instructions simply assume that the data being read from operands and written to the destination are all -bit values (unsigned or signed 2's complement, depending on the instruction).\n\nShader register storage is made up of -bit* -component quantities, and integer arithmetic on these registers is required to be performed at full bit in all cases.\n\nThe bitwise instructions are listed in the Bitwise Instructions(22.11) sub-section of the full instruction listing.\n\nSee the Integer Arithmetic Instructions(22.12) sub-section of the full instruction listing.\n\nThere is no implicit conversion between floating-point and integer values. Contents of registers are interpreted as float or ints by the particular instruction being executed. Two instructions exist that allow explicit conversions to be performed, listed in the Type Conversion Instructions(22.13) sub-section of the full instruction listing.\n\nInteger offsets for reads from register banks are available. These offsets must be scalar values (i.e. a select swizzle must be used to select one component of any vector-valued register used as an index) and are considered to be unsigned bit values.\n\nThis indexing mechanism applied to indexable x# registers allows compilers to generate stack-like behavior for Shader subroutines.\n\nAn example syntax for indexing is:\n\nThis instruction assumes that an unsigned -bit integer value exists in r2.x, and uses that value to offset into ConstantBuffer 7, starting from location 3 in the ConstantBuffer. Thus, if r2.x contains integer value 2, entry 5 of ConstantBuffer 7 would be referenced.\n\nA listing of all floating point instructions can be found here(22.10).\n\nInstructions are provided for rounding floating point values to integral floating point values:\n\nThe D3D intermediate language (IL) and register model are 4-vec oriented. Since this does not constrain hardware implementation (vector vs scalar) too much, this convention will carry forward until a good reason to switch paradigms surfaces. It is known that many implementations actually happen to operate on scalars or combinations of layouts even now.\n\nOne area where the vector assumption seems to materially impact data organization is the indexing of registers such as inputs or outputs – the indexing happens across registers. If it is important to be able to express cleanly how to index through an array of scalars, this could be an example of an argument for switching the IL to be completely scalar.\n\nShaders have bindpoint arrays for various classes of read-only input resources: Constant Buffers (cb), Texture/Buffers (t), Samplers (s).\n\nD3D11 allows all of these to be dynamically but uniformly indexed from a shader, whereas previously none of them were indexable.\n\nAs with indexing of other types, such as indexable temps (x#), the dynamic index can be either an r# or statically indexed x# containing a 32-bit unsigned integer, an immediate 32-bit unsigned integer constant, or the combination of the two, added together.\n\nThe constraint on the indexing of resources or samplers is that the index must be uniform. That is, the computed index must be the same at that point in the lockstep execution of the program for all invocations of the shader within the Draw*() call. If due to flow control, some of the lockstep shader invocations are inactive, the computed index in those shaders is ignored and therefore cannot cause a violation of the uniform indexing constraint on all the active invocations.\n\nThe HLSL compiler will enforce this behavior and driver compilers must not break it either. Violations of the uniform indexing constraint would be a result of an HLSL compiler bug or a driver compiler bug only, and in such cases the indexing results are undefined.\n\nOut of bounds resource indexing produces the same result as if accessing a slot with no resource bound.\n\nIn particular note that with Constant Buffers, there are API-visible Constant Buffer slots (a couple of other slots are reserved for various purposes). The valid indexing range for Constant Buffers is therefore [0..13], and accesses out of that range behave as if accessing a slot with no Constant Buffer bound.\n\nOut of bounds indexing of the Samplers (s#) results in undefined behavior.\n\nSuppose x3[0].x contains 4 and x4[2].y contains 5. The following mov instruction:\n\nis therefore equivalent to:\n\nwhich means read a 32-bit * 4-vector from location [14] in the ConstantBuffer, at ConstantBuffer bind point [10] (0-based counting).\n\nThe uniform dynamic indexing of which Constant Buffer to read from is what was not supported previously. Dynamic indexing within the Constant Buffer itself has always been supported.\n\nSuppose x3[0].x contains 4. The following ld instruction:\n\nNote the \"texture2D\" at the end is also a new requirement, whereby all ld/sample instructions will indicate which Shader Resource View type is to be sampled.\n\nSuppose x3[0].x contains 4 and x4[2].y contains 5. The following sample instruction:\n\nShader declarations from Shader Model 4.x for individual resources, constant buffers and samplers remain the same in Shader Model 5.0. These are particularly informative for parts of shader code that reference these objects directly, just as before.\n\nHowever, all instructions that reference texture objects (t#) now specify the view dimension (e.g. textureCubeArray) as a literal parameter. This is redundant when indexing is not used, since the up-front declaration of each t# has a view dimension, but useful when indexing is used.\n\nA flow control block is defined as an if(22.7.1) block, loop(22.7.4) block, or switch(22.7.18) block. Flow control blocks can nest up to deep per subroutine (and main). Behavior of flow control instructions beyond this nesting limit is undefined.\n\nSubroutines can nest up to deep. If there are already entries on the return address stack and a \"call\" is issued, the call is skipped over.\n\nFor Typed memory views, the number of components in an address when accessed by a shader instruction is determined by the number of components in the resource dimension. Each address component is an unsigned 32-bit integer element index.\n\nFor Raw memory views, the address is a single component unsigned 32-bit integer byte offset from the beginning of the view. The addresses must be 32-bit aligned. If an unaligned address is specified for an operation involving a write, the entire contents of the UAV(5.3.9) being written, or all of Thread Group Shared Memory (in the Compute Shader(18)) - whichever is being accessed - becomes undefined. If an unaligned address is specified for an operation involving a read, an undefined result is returned to the shader. It is invalid for implementations to perform the access as if there were no 32-bit alignment constraints.\n\nFor Structured memory views, the address is two unsigned 32-bit integer values. The first value is the struct index, and the second value is a byte offset into the struct. The byte offset must be aligned to 32-bits, otherwise the same behavior described for misaligned raw memory access above applies.\n\nEach memory access instruction defines its behavior for out of bounds accesses, with distinctions for the memory location being accessed (UAV vs SRV vs Thread Group Shared Memory), and the layout (raw vs structured vs typed). See the documentation of individual instructions for details. The behaviors are similar for similar classes of instructions – e.g. all atomics have the same out of bounds behavior, all immediate atomics (which return a value to a shader) have their own consistent out of bounds access behavior, etc.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 7.14.1 Intro\n\n 7.14.2 Atomicity\n\n 7.14.3 Sync\n\n 7.14.4 Global vs Group/Local Coherency on Non-Atomic UAV Reads\n\n\n\nThe types of memory accesses included in the scope of this chapter are: to Unordered Access Views(5.3.9) (UAVs, u#), available to the Compute Shader(18) and Pixel Shader(16), as well as Thread Group Shared Memory (g#), available to the Compute Shader.\n\nThe D3D11 Shader Memory Consistency Model is weak/relaxed, as generally understood in existing architectures and literature. Loosely, this means the program author and/or compiler are responsible for identifying all memory and thread synchronization points via some appropriately expressive labeling.\n\nThis section outlines how this weak/relaxed Memory Consistency Model appears to function from the point of view of D3D software.\n\nAn atomic operation may involve both reading from and then writing to a memory location. Atomic operations apply only to either u# (Unordered Access Views) or g# (Thread Group Shared Memory).\n\nIt is guaranteed that when a thread issues an atomic operation on a memory address, no write to the same address from outside the current atomic operation by any thread can occur between the atomic read and write.\n\nIf multiple atomic operations from different threads target the same address, the operations are serialized in an undefined order.\n\nAtomic operations do not imply a memory or thread fence. Fence operations (dubbed \"sync\") are introduced below. If the program author/compiler does not make appropriate use of fences, it is not guaranteed that all threads see the result of any given memory operation at the same time, or in any particular order with respect to updates to other memory addresses.\n\nAtomicity is implemented at 32-bit granularity. If a load or store operation spans more than 32-bits, the individual 32-bit operations are atomic, but not the whole.\n\nLimitation: Atomic operations on Thread Group Shared Memory are atomic with respect to other atomic operations, as well as operations that only perform reads (\"load\"s). However atomic operations on Thread Group Shared Memory are NOT atomic with respect to operations that perform only writes (\"store\"s) to memory. Mixing of atomics and stores on the same Thread Group Shared Memory address without thread synchronization and memory fencing between them produces undefined results at the address involved. This limitation arises because some implementations of loads and stores do not honor the locking semantics for implementing atomics. It turns out this has no impact on loads, since they are guaranteed to retrieve a value either before or after an atomic (they will not retrieve partially updated values, given they are all defined at 32-bit quanta). However store operations could find their way into the middle of an atomic operation and thus have their effect possibly lost.\n\nNote that there is no such limitation on atomics to UAV memory; atomic operations on UAV memory is atomic both with respect to other atomic operations as well as loads and stores.\n\nA sync(22.17.7) instruction is included in the Shader IL for Pixel Shader and the Compute Shader.\n\nThis provides memory fence semantics at various scopes, and optional thread group synchronization semantics (the latter only applies to the Compute Shader). For details, including some discussion of the implications see the description of the sync(22.17.7) instruction.\n\nTypical implementations will have a cache hierarchy to improve read access performance on UAV(5.3.9) accesses. A constraint that some implementations have with the first stage in this cache hierarchy is that, in addition to operating at per-thread-group scope only, the cache does not have an efficient way of being synchronized with writes or atomics that have happened by other thread groups. Such behavior only surfaces as an issue for applications when cross-thread-group communication needs to be performed involving data loads. In this case, the hardware basically needs to know that it must bypass the first stage of caches on loads, reaching out to a more global memory so that the cross thread-group communication can function. D3D allows applications specify this cross-thread-group communication intent as follows.\n\nIf a Compute Shader(18) thread in a given thread group needs to perform loads of data that was written by atomics or stores in another thread group, the UAV slot where the data resides must be tagged upon declaration in the shader as \"globally coherent\", so the implementation can ignore the local cache. Otherwise, this form of cross-thread group data sharing will produce undefined results.\n\nAtomic read-modify-write operations do not have this constraint (even though a part of the operation is a read/load), because a byproduct of the hardware honoring atomicity is that the entire system sees the operation, whereas simple loads on some implementations may only go to a local cache that has no knowledge of external updates.\n\nIf a UAV is not declared as \"globally coherent\", it is only \"group coherent\", which means loads can only see data written by stores and atomics in other threads in the same thread group. The affected hardware knows it can make use of its thread-group specific caching for loads, since writes to the memory only came from the current thread group. A UAV tagged as \"globally coherent\" is also inherently obviously \"group coherent\", although the affected hardware would not use its local cache. As such, the \"globally coherent\" flag should only be specified when necessary.\n\nAs a reminder though, to guarantee coherency on UAV accesses on all implementations, not only must shaders make the global vs group scope distinction discussed here upon UAV declaration, but they must also make appropriate use of memory and/or thread barriers (\"sync_*\" in the IL) as needed within in the shader to enforce proper ordering of operations by individual threads as seen by others. In addition, the \"sync\" operation has options for memory barriers that also distinguish between global vs group scope, but that control is separate from the topic of this section, and may not be exposed until a later time, as discussed in the sync instruction definition.\n\nBack to issue of global vs group coherency on non-atomic UAV reads. Importantly, for many scenarios where cross thread-group communication or reduction (such as histograms) can be accomplished using only atomic operations (no cross thread-group loads involved), there is no problem since atomic operations are implemented by all hardware in a globally coherent way, regardless of whether the UAV has been tagged as \"globally coherent\" or not.\n\nIn the Pixel Shader(16), if a UAV is not declared as \"globally coherent\", it is only \"locally coherent\". \"Local coherency\" is the Pixel Shader’s equivalent of the Compute Shader’s \"group coherency\", except having scope limited only to a single Pixel Shader invocation. This indicates that the Pixel Shader is not doing any cross-PS-invocation communication involving simple load operations. Note, however, that in the Pixel Shader just like in the Compute Shader, atomic read-modify-write operations are always globally coherent. Indeed it is likely to be rare for a Pixel Shader or perhaps even the Compute Shader to need to declare a UAV as \"globally coherent\", given that atomic operations, which are always globally coherent, might provide the most practical mechanism for cross-PS-invocation or cross-group operations.\n\nTo assist comparisons of algorithms running on GPUs during application development, a cycle counter can be read into shaders. The cycle counter is a 64-bit unsigned integer.\n\nThe cycle counter appears as an additional 2*32-bit (64 bit total) input register type that can declared in any version 5.0+ shader. There are currently no native 64-bit integer arithmetic operations in shaders, although it is simple enough to emulate this. It may be fine for shaders to just look at the low 32-bits of the counter – this can be requested in the shader. Applications may also export the measurements using standard shader outputs for later analysis such as on the CPU.\n\nThe counter is an implementation-dependent measure of cycles in the GPU engine, requiring care to interpret it usefully.\n\nFor this discussion, consider a shader \"invocation\" to be a single execution of one shader program from beginning to end. For the Compute Shader however, an \"invocation\" is a single thread-group’s execution – e.g. the lifespan of the contents of thread-group shared memory.\n\nThe initial value of the counter is undefined.\n\nA single reading of the cycle counter is meaningless. But any shader invocation can poll the counter value any number of times.\n\nComputing a delta from cycle counter readings within a shader invocation is meaningful.\n\nComputing a delta from cycle counter readings across separate shader invocations is not meaningful on all hardware. Developers must obtain information directly from IHVs about whether this is meaningful.\n\nThe only IHV agnostic approach to interpreting the counters is to limit calculation of deltas to within a given shader invocation, and only make comparisons of deltas within or between shader invocations.\n\nThere are plenty of reasons why test runs will execute differently. The obvious one is that execution of a shader can be interrupted by thread switching, so delta measurements will be arbitrarily larger than the number of cycles spent executing instructions in a given thread.\n\nThere is no supported way to find out the frequency of the counter. There is no way to correlate this shader internal counter with external timers such as asynchronous time queries. The counter measurements cannot be correlated with measurements on different hardware by other hardware vendors or even necessarily the same vendor.\n\nIf a GPU’s speed changes, such as for power saving, there is no way to know this happened, or its effect on cycle measurements.\n\nBeyond these hints about the care needed to interpret the counter, the onus is on developers to research the properties of new hardware designs that may affect measurements.\n\nThe HLSL shader compiler and driver compilers must treat reads of the cycle counter as barriers. Instructions can’t be moved across a counter read, and counter reads can’t be merged.\n\nThe runtime enforces that shaders using this feature can only be created on a system with debug layer enabled. The debug layer is not allowed to be redistributed to end-user machines. The point is that shaders that use this counter are not intended to be shipped.\n\nThis feature will not be tested on hardware by WHQL, except perhaps simply checking that drivers do not crash. Microsoft will test that the HLSL compiler output is correct.\n\nA new input register, vCycleCounter(22.3.29), can be declared in any version 5_0 (and beyond) shader:\n\nReading x yields the 32 LSBs of the 64-bit count, and reading y yields the 32 MSBs.\n\nThis register can only be used as the source to a mov instruction, e.g. mov r0.w, vCycleCounter.x.\n\nUp to Resources (e.g. Buffer, Texture1D/2D/3D/Cube) can be active per Pipeline stage. A Resource binding is a representation of a Resource's base pointer (and other data such as size and pixel layout) and is independent of the samplers.\n\nA texture out of a set of bound textures cannot be selected via Shader indexing, however Texture1D/2D/3D resources with an Array dimension > 1, or TextureCube (which has an Array dimension of 6), allow indexing along the array axis from within Shader code.\n\nTextures can only have a single Element format. Likewise, Buffers used as input to Shaders can also only have a single Element format, and have an implied data stride equal to the Element size. A single Buffer (or Texture) could be set to multiple input slots simultaneously, with different Element formats and/or offsets, however because Buffers bound as Shader inputs have their data stride implied by the Element format, it is not possible to describe \"Array-of-Structures\" style layouts in Buffers bound at Shader input. This unlike the Input Assembler Stage, where multiple element Buffers are permitted, and Element offsets and strides can be defined Buffers freely.\n\nData from textures is accessed in shaders via the load (ld) and sample instructions. The ld instruction provides a simple read and (optional) float32 conversion of texture data using integral addresses, while the sample instructions use normalized floating point addressing and perform filtering in addition to the format conversion.\n\nThe load operation performs a non-filtered read of resource data. See the ld(22.4.6) instruction definition for details.\n\nMultisample resources can be set as shader inputs, which allows individual samples to be read by the shader. Support for multisample shader reads has the following restrictions:\n• Pixel Shader only (not supported for other shader stages)\n• load instruction only (no use of sample instructions)\n• number of samples in bound resource must be declared in shader\n• sample index for load instruction must be a literal\n\nSee ld(22.4.6) and dcl_resource(22.3.12) definitions for details.\n\nThis section describes the mechanics of sampling Texture1D/2D/3D/Cube resources using filtering. The simplest form of sampling a texture is point sampling, supported for all data formats, however more complex filtering operations are only available to some formats, indicated in the format list in the Formats(19.1) section.\n\nThe behaviors described here are obtained via the the various sample* instructions, such as sample(22.4.15). See the specs for those instructions for further details that complement this section.\n\nUnless otherwise noted, all texture sampling address operations are performed according to the arithmetic processing rules described in the Basics(3) section.\n\nSamplers identify filtering modes and other sampler state, described below. Samplers are not indexable from within shaders. There are samplers \"slots\" per Pipeline stage, to which \"Sampler Objects\" can be arbitrarily assigned/reassigned.\n\nThe state for a sampler is encapsulated in a \"sampler object\", up to of which can be created through the API. At the time a sampler object is created, all of its state must be chosen permanently, and can never be changed. These sampler objects can be arbitrarily assigned to any of the \"sampler slots\" at each of the Shader stages (a single sampler object is allowed to be assigned to multiple sampler slots, even on multiple pipelines stages simultaneously, if desired.\n\nSee the Sampler Declaration Statement(22.3.34) in the shader instruction reference for a description of which sampler states are honored depending on the choice of Filter setting, and a description of which sampler* instructions in the shader are permitted to reference samplers configured various ways.\n\nThe magnitude of normalized-space texture coordinates (allowing for texture tiling) has no effect on the maximum supportable texture dimensions that can be sampled. The only catch is that as the absolute magnitude of a normalized-space texture coordinate gets larger (e.g. large amounts of tiling), floating point dictates that less precision will be available to resolve individual texels in a given tiling of the texture being sampled. Large amounts of tiling of large dimension textures will yield sampling artifacts where float32 precision becomes inadequate. But separate from this tradoff, in order to otherwise achieve decoupling of the magnitude of normalized-space texture coordinates from having any effect on maximum texture dimension that can be sampled given float32 normalized-space addressing, a range reduction to about [ ... ], depending on the scenario, is applied on the texture coordinates.\n\nUsing range reduction to decouple texture coordinate magnitude from supportable texture size has the following implication: The maximum texture dimension possible to be sampled in D3D . is 2^ . This limit is derived starting with 24 bits of float32 fractional precision for the original texture coordinate, subtracting required subtexel precision ( bits), and subtracting 1 more bit due to the factor of 2 scaling in the reduced range. Of course, the minimum upper limit for filterable texture dimension required to be exposed by all D3D . implementations is far smaller, at only (see System Limits(21)).\n\nThis section describes in general how to convert a normalized texture coordinate to a texture address. The description is based on sampling a Texture1D, but applies equally to Texture2D and Texture3D (and not TextureCubes).\n\nA normalized texture coordinate (U) maps the range [0, 1] to the range [0, numTexelsU], where numTexelsU is the size of a 1D texture in texels. The process of computing a texture address is as follows:\n• Reducing the normalized texture coordinate range based on the texture address mode\n• Performing point sample or linear sample addressing (scaling the normalized texture coordinate by the texture size and snapping the value to a fixed point number with bits fraction).\n\nTo limit the number of bits needed to store the texture coordinate in fixed point after conversion from floating point, the range of the normalized texture coordinate is reduced to be within [ , ], depending on the Address mode. This removes the magnitude of texture tiling from the texture coordinate, while not affecting the behavior of texture address wrap modes. The same address mode handling can be applied to the range reduced texture coordinate as the original, producing the same result. The benefit is that the magnitude of texture tiling is not stored in the coordinate at the same time that texture size scaling is performed on the coordinate. This enables far larger texture coordinate range to be handled cleanly than would otherwise be possible without reduction.\n\nThe following logic describes how normalized texture coordinate range reduction is performed. (This is different form final Texture Address Processing(7.18.9), which happens a couple of steps later, on scaled coordinates that identify texels.)\n\nSetting aside how sampler state is configured and how mipmap LOD is chosen, consider simply the task of point sampling an Element from a particular miplevel of a Texture1D, given a scalar floating point texture coordinate in normalized space. In the Texture Coordinate Interpretation(3.3.3) section, there is a diagram illustrating generally how a 1D texture coordinates maps to a texel (not accounting for wrapping). Note from the \"Texture Coordinate System\" diagram shown that texel corners have integral coordinates in texel-space, and so texel centers are at half-units away from the corners. Point sampling selects the \"nearest\" texel based on the proximity of texel centers to the texture coordinate (keeping in mind that texel centers are at half-units):\n• Given a 1D texture coordinate in normalized space U, assumed to be any float32 value.\n• U is scaled by the Texture1D size. Call this scaledU\n• scaledU is converted to at least . Fixed Point(3.2.4.1). Call this fxpScaledU.\n• The integer part of fxpScaledU is the chosen texel. Call this t. Note that the conversion to Fixed Point(3.2.4.1) basically accomplished: t = floor(scaledU).\n• If t is outside [0...numTexels-1] range, D3D11_SAMPLER_STATE's AddressU mode is applied(7.18.9).\n\nFor Texture2D and Texture3D Resources, the same rules apply independently on the other dimensions.\n\nFor TextureCube Resources, the following occurs:\n• Choose the largest magnitude component of the input vector. Call this magnitude of this value AxisMajor. In the case of a tie, the following precedence should occur: Z, Y, X.\n• Select and mirror the minor axes as defined by the TextureCube(5.3.8) coordinate space. Call this new 2d coordinate Position.\n• Project the coordinate onto the cube by dividing the components Position by AxisMajor.\n• Transform to 2d Texture space as follows: Position = Position * 0.5f + 0.5f;\n• Convert the coordinate to fixed point as for a Texture2D.\n\nSimilar to the previous section, set aside how sampler state is configured and how mipmap LOD is chosen for now, and consider simply the task of linear sampling an Element from a particular miplevel of a Texture1D, given a scalar floating point texture coordinate in normalized space. Linear sampling in 1D selects the nearest two texels to the sample location and weights the texels based on the proximity of the sample location to them.\n• Given a 1D texture coordinate in normalized space U, assumed to be any float32 value.\n• U is scaled by the Texture1D size, and 0.5f is subtracted. Call this scaledU.\n• scaledU is converted to at least . Fixed Point(3.2.4.1). Call this fxpScaledU.\n• The integer part of fxpScaledU is the chosen left texel. Call this tFloorU. Note that the conversion to Fixed Point(3.2.4.1) basically accomplished: tFloorU = floor(scaledU).\n• The right texel, tCeilU is simply tFloorU + 1.\n• The weight value wCeilU is assigned the fractional part of fxpScaledU, converted to float(3.2.4.2) (although using less than full float32 precision for computing and processing wCeilU and wFloorU is permitted).\n• The weight value wFloorU is 1.0f - wCeilU.\n• If tFloorU or tCeilU are out of range of the texture, D3D11_SAMPLER_STATE's AddressU mode is applied(7.18.9) to each individually.\n• Since more than one texel is chosen, the single sample result is computed as:\n\nThe procedure described above applies to linear sampling of a given miplevel of a Texture2D as well:\n• Peform the texel selection to both U and V directions independently, producing 2 U texel locations and 2 V texel locations. Combined, these select 4 texels: (tFloorU,tFloorV), (tFloorU,tCeilV), (tCeilU,tFloorV), (tCeilU,tCeilV).\n• There are also 4 weight values produced: wFloorU, wCeilU, wFloorV, wCeilV.\n\nPerforming linear sampling of a miplevel of a Texture3D Resource extends the concepts described above to fetching of 8 texels.\n\nIn the case of a TextureCube, see the section regarding TextureCube Edge and Corner Handling(7.18.12)\n\nThe sample* instructions provide texture coordinates in normalized floating point form, such that values in [0..1] range span a given dimension of a texture, and values outside this range fall off the borders of the texture. Later in the filtering process, when individual texels are fetched, if the address is outside the extents of the texture, either the address gets mapped back into range by the texture address mode for each component, or the border-color is used. The texture address mode is defined by the AddressU, AddressV, and AddressW members of D3D11_SAMPLER_STATE.\n\nConsider the moment in the process of sampling of a Texture1D just after picking a particular integer address scaledU to fetch a texel from (details on choosing sample locations described elsewhere for various filter modes). Suppose the texel address scaledU falls off the Texture1D, meaning either (scaledU < 0), or (scaledU > numTexelsU - 1), where numTexelsU is the count of texels in the U dimension of the Texture1D. The following pseudocode describes how the setting on D3D11_SAMPLER_STATE member AddressU gets applied on scaledU:\n\nFor Texture2D and Texture3D, all of the above modes apply to the V and W dimensions independently, based on AddressV and AddressW. If any single dimension selects Border Color, then the Border Color(7.18.9.1) is applied.\n\nBorder Color values are defined in the DDI via 4 floating point values (RGBA), in linear space. The Border Color used in filtering is snapped to the precision the hardware performs filtering at for the format.\n\nNote that the only components of the BorderColor used by filtering hardware are the ones present in the resource format description.\n\nFor example, suppose the resource format is DXGI_FORMAT_R8_SNORM, and BorderColor is needed during a sample operation. In this case only the RED component of BorderColor is used, along with the appropriate format-specific defaults for the other components. The BorderColor (the red part in this case) is taken as floating-point data and clamped into the range of the format before filtering. In this case, the red part of the BorderColor is clamped to [-1.0f,1.0f] range before being used by the filtering hardware. From this point (entering the filtering hardware) onward, the fact that BorderColor is being used has no more behavioral effect.\n\nSuppose the task at hand is to choose a mipmap level from a Resource, given a floating point LOD value. The choice of mipmap level is based on the particular choice of filter mode in the Sampler State(7.18.3); in which the possible choices are POINT and LINEAR. Anisotropic texture filtering uses LINEAR mipmap selection.\n• If the Sampler defines a Filter for which MIP is set to POINT (otherwise known as 'nearest'), the LOD is first converted to at least . fixed point (if not already in fixed point form), 0.5 is added, and then the integer part of the LOD is taken as the mipmap level (clamped to available miplevels or any settings for clamping miplevels). This selects the \"nearest\" miplevel.\n• If the Sampler defines a Filter for which MIP is set to LINEAR:\n• The two nearest mipmaps are selected as follows.\n• First, the LOD is converted to at least . fixed point (if not already in fixed point form). Call this fxpLOD.\n• The integer part of the fxpLOD is the first miplevel. Call this mipFloor.\n• The second miplevel, call it mipCeil, is miplFloor+1.\n• The selected miplevels are clamped to the range of mipmaps available, plus any other settings for clamping miplevels.\n• The weight for mipCeil, call it wMipCeil, is the fractional component of fxpLOD, converted to float.\n• The weight for mipFloor, call it wMipFloor, is 1.0f - wMipCeil. . In the past multiple IHVs have cheated here (weight selection) with tactics such as snapping LOD values loosely \"around\" a given mipmap level to that level in order to avoid performing fetches from multiple mipmap levels. Such practices were always in violation of spec, and will continue to be violations in D3D\n• Finally, the texture filtering operation receives the pair of chosen miplevels and weights. The filter can perform some sampling operation at each miplevel combines them using the weights: sampleAt(mipCeil) * wMipCeil + sampleAt(mipFloor) * wMipFloor, where the particular sample operation performed depends on the filtering mode (and multiple such operations involving LINEAR mipmap selection could be involved in a complicated filtering process, e.g. in anisotropic filtering).\n\nThis section describes how LOD is computed as part of sample* instructions involving filtering.\n• The following determines whether LOD will be computed by a sample instruction, either in an isotropic formulation or in anisotropic formulation: bool ComputeAnisotropicLOD = (SamplerState.Filter == D3D11_FILTER_ANISOTROPIC) && IsTexture2D // Includes. 2D array. // Note: Implementations may choose to perform anisotropic texture // filtering for TextureCubes as well, however D3D . does not require(7.18.13) // filtering of TextureCubes to behave any better than tri-linear filtering. bool ComputeIsotropicLOD = !ComputeAnisotropicLOD bool Magnifying = (clampedLOD <= 0)\n• Given a texture coordinate vector (1D, 2D or 3D), let it be referred to here as:\n• If the Shader is a Pixel Shader, compute the partial derivative vectors in the RenderTarget x and y directions for TC.uvw. Let the derivatives be referred to here as:\n• See the deriv_rtx_coarse(22.5.2) and deriv_rty_coarse(22.5.3) instructions for details on how to compute these quantities.\n• A couple of variants of the sampling instructions allow the Shader to provide derivatives directly or specify LOD directly (and are available in all Shader stages, not just the Pixel Shader). The sample_d(22.4.17) instruction provides derivatives directly, and the sample_l(22.4.18) instruction allows the LOD to be provided directly. When anisotropic filtering, the ratio of anisotropy with sample_l(22.4.18) is 1 (isotropic).\n• If the current texture is a TextureCube, transform the partial derivative vectors into the space of the primary TextureCube face as follows:\n• Using TC, determine which component is of the largest magnitude, as when calculating the texel location(7.18.7). If any of the components are equivalent, precedence is as follows: Z, Y, X. The absolute value of this will be referred to as AxisMajor.\n• select and mirror the minor axes of TC as defined by the TextureCube coordinate space to generate TC'.uv\n• select and mirror the minor axes of the partial derivative vectors as defined by the TextureCube coordinate space, generating 2 new partial derivative vectors dX'.uv & dY'.uv.\n• Suppose DerivativeMajorX and DerivativeMajorY are the major axis component of the original partial derivative vectors.\n• Calculate 2 new dX and dY vectors for future calculations as follows:\n• Scale the derivatives by the texture size at largest mipmap: if (IsTextureCube) { // multiplying by 0.5f to adjust for TextureCube coordinate system dX.uvw = 0.5f * dX.uvw * [NumTexelsAlongCubeSide,NumTexelsAlongCubeSide,0]; dY.uvw = 0.5f * dY.uvw * [NumTexelsAlongCubeSide,NumTexelsAlongCubeSide,0]; } else { dX.uvw = dX.uvw * [NumTexelsInUDimension,NumTexelsInVDimension,NumTexelsInWDimension]; dY.uvw = dY.uvw * [NumTexelsInUDimension,NumTexelsInVDimension,NumTexelsInWDimension]; }\n• Given a pair of partial derivative vectors representing an elliptical transform, it is important to calculate LOD using a proper orthogonal Jacobian matrix, as described by [Heckbert 89]. When performing anisotropic filtering, it is also important to use these modified vectors to calculate the proper filtering footprint. D3D . will allow approximations to this effect. The following describes the ideal transformation, given 2 dimensional vectors: Defining the following variables: The new vectors may be then calculated as: new_dX.u = sqrt(F * (t+p) / ( t * (q+t))) new_dX.v = sqrt(F * (t-p) / ( t * (q+t)))*sgn(B) // The paper says sgn(B*p), which appears to be incorrect. new_dY.u = sqrt(F * (t-p) / ( t * (q-t)))*-sgn(B) new_dY.v = sqrt(F * (t+p) / ( t * (q-t))) If w is nonzero, as when calculating LOD for a volume map, an orthogonal transformation must be used to calculate a pair of 2 dimensional vectors with the same lengths and inner angle prior to computing the correct Jacobian matrix. The following is the transformation implemented by the reference rasterizer: The following caveats also apply:\n• if either of dX or dY are of zero length, an implementation should skip these transformations.\n• if dX and dY are parallel, an implementation should skip these transformations.\n• if dX and dY are perpendicular, an implementation should skip these transformations.\n• if any component of dX or dY is inf or NaN, an implementation should skip these transformations.\n• if components of dX and dY are large or small enough to cause NaNs in these calculations, an implementation should skip these transformations.\n• if(ComputeAnisotropicLOD), the LOD calculation is: // Compute outputs: // (1) float ratioOfAnisotropy // (2) float anisoLineDirection // (3) float LOD // (For 1D Textures, dX.v and dY.v are 0, so all the // math below can be simplified) float squaredLengthX = dX.u*dX.u + dX.v*dX.v float squaredLengthY = dY.u*dY.u + dY.v*dY.v float determinant = abs(dX.u*dY.v - dX.v*dY.u) bool isMajorX = squaredLengthX > squaredLengthY float squaredLengthMajor = isMajorX ? squaredLengthX : squaredLengthY float lengthMajor = sqrt(squaredLengthMajor) float normMajor = 1.f/lengthMajor output.anisoLineDirection.u = (isMajorX ? dX.u : dY.u) * normMajor output.anisoLineDirection.v = (isMajorX ? dX.v : dY.v) * normMajor output.ratioOfAnisotropy = squaredLengthMajor/determinant // clamp ratio and compute LOD float lengthMinor if ( output.ratioOfAnisotropy > input.maxAniso ) // maxAniso comes from a Sampler state. { // ratio is clamped - LOD is based on ratio (preserves area) output.ratioOfAnisotropy = input.maxAniso lengthMinor = lengthMajor/output.ratioOfAnisotropy } else { // ratio not clamped - LOD is based on area lengthMinor = determinant/lengthMajor } // clamp to top LOD if (lengthMinor < 1.0) { output.ratioOfAnisotropy = MAX( 1.0, output.ratioOfAnisotropy*lengthMinor ) // lengthMinor = 1.0 // This line is no longer recommended for future hardware // // The commented out line above was part of the D3D10 spec until 8/17/2009, // when it was finally noticed that it was undesirable. // // Consider the case when the LOD is negative (lengthMinor less than 1), // but a positive LOD bias will be applied later on due to // sampler / instruction settings. // // With the clamp of lengthMinor above, the log2() below would make a // negative LOD become 0, after which any LOD biasing would apply later. // That means with biasing, LOD values less than the bias amount are // unavailable. This would look blurrier than isotropic filtering, // which is obviously incorrect. The output of this routine must allow // negative LOD values, so that LOD bias (if used) can still result in // hitting the most detailed mip levels. // // Because this issue was only noticed years after the D3D10 spec was originally // authored, many implementations will include a clamp such as commented out // above. WHQL must therefore allow implementations that support either // behavior - clamping or not. It is recommended that future hardware // does not do the clamp to 1.0 (thus allowing negative LOD). // The same applies for D3D11 hardware as well, since even the D3D11 specs // had already been locked down for a long time before this issue was uncovered. } output.LOD = log2(lengthMinor);\n• Given an LOD specified either from the shader or calculated from derivatives, MipLODBias, srcLODBias (sample_b(22.4.16) only), and MinLOD and MaxLOD clamps are applied to it: biasedLOD = output.LOD + MipLODBias; biasedLOD = biasedLOD + srcLODBias; // for sample_b only; must be per done pixel clampedLOD = max(MinLOD,(min(MaxLOD, biasedLOD))); The ordering of min/max guarantees that if MinLOD > MaxLOD, then MinLOD takes precedence. These min and max operations follow the Floating Point Rules(3.1), so NaN never gets propagated. A sampler state that specifies NaN for MinLOD or MaxLOD is invalid. Note that the naming for MinLOD and MaxLOD is different/opposing from the D3DSAMP_MAXMIPLEVEL sampler state present in Direct3D9. \n\n The selection of minification vs magnification occurs after LOD clamping. Prior to feature level 11.0, it was undefined whether magnification selection occurred before or after LOD clamping. Also note the independent Per-Resource Mipmap Clamping(5.8) feature, which is an optional additional clamp on the LOD like MinLOD above but specified at a resource level as opposed to a sample+shader-resource view level. In some future D3D version, a better definition of magnification should be considered. For one, filtering should take into account the available mipmaps after clamping. Further, perhaps whenever the most detailed available mipmap is read, it should receive magnification filtering, while minification filtering would always be applied to any less detailed mips read in a given filter operation. Thus a given trilinear filter operation could be applying both magnification on one of the mips referenced simultaneously with minification filtering on the other before blending the mips together. This distinction becomes interesting if more compelling magnification filter types are ever introduced, particularly in avoiding discontinuities transitioning between minification and magnification. Regarding MipLODBias: The valid range for MipLODBias in the sampler and srcLODBias in the sample_b(22.4.16) instruction are ( ... ). An implementation must support sufficient range for the LOD value before the application-defined MinLOD/MaxLOD/MipLODBias/srcLODBias equation above, such that if the calculated LOD before this equation is outside of the internally supported range and gets clamped (prior to applying application-defined MinLOD/MaxLOD), then the MipLODBias part of the equation (given any valid MipLODBias and srcLODBias value) must not cause the LOD to come back into the range that affects mip selection.\n\nTextureCube filtering near Cube edges, where 2x2 (bilinear) filter taps would fall off a face are required to spill over by one texel row/column to the appropriate adjacent map.\n\nAt TextureCube corners, a linear combination of the three relevant samples is required. The ideal (reference) linear combination of the three samples in the corner case is as follows: Imagine flattening out the Cube faces at the corner, yielding 3 texels and a missing one. Apply bilinear weights on this virtual grid of 4 texels, and then divide the weight for the missing texel evenly amongst the 3 other texels. It is alternatively permissible for an implementation to, instead of dividing the weight evenly amongst the 3 other texels, just split the weight of the missing texel across the 2 adjacent texels. However in future versions of D3D, only the reference behavior will be permitted.\n\nAnisotropic texture filtering on a TextureCube does not have specified/required behavior except that it must at least behave no \"worse\" than tri-linear filtering would.\n\nThe application is given control over the return type of texture load instructions (i.e. reading raw integer values vs. reading normalized float values) by simply choosing an appropriate format to interpret the resource's contents as. See the Formats(19.1) section for detail.\n\nFor details on comparison filtering, see the sample_c(22.4.19) and sample_c_lz(22.4.20) instructions.\n\nWhen Feature Level 9.x is used at the D3D11.1+ API (meaning the D3D9 DDI is used) on a Win8+ driver, regardless of hardware feature level, applications can do the following:\n• Create Texture2D surfaces with the format DXGI_FORMAT_R16_TYPELESS or DXGI_FORMAT_R24G8_TYPELESS and set BindFlags to both D3D11_BIND_SHADER_RESOURCE and D3D11_BIND_DEPTH_STENCIL together.\n• Use BorderColor addressing if desired on these samplers, even though border color is otherwise not normally allowed on Feature Levels 9.1 and 9.2. This is useful to allow applications to choose what happens when sampling off the bounds of Depth Buffer. A typical choice would be using a depth value (placed in the R component of the border color) that would result in the depth comparison always passing or always failing.\n• The Mag and Min filter settings in the comparison filter choose between linear or point filtering (using different choices for Mag/Min filter is undefined). Anisotropic filtering is not allowed. The Mip filter choice is meaningless since Feature Level 9.x does not allow mipmapped depth buffers.\n• Create a DepthStencil View of the typeless Texture2D resource with format DXGI_FORMAT_D16_UNORM / DXGI_FORMAT_D24_UNORM_S8_UINT and render depth to it.\n• Create a Shader Resource View of the typeless Texture2D resource with format DXGI_FORMAT_R16_UNORM / DXGI_FORMAT_R24_UNORM_X8_TYPELESS and bind it with the comparison sampler described above.\n• Use the SampleCmp/SampleCmpLevelZero texture2D methods in ps_4_0_level_9_* shaders to sample from the Shader Resource View above.\n• Note these methods already exist for ps_4_0+ (sample_c/sample_c_lz in the bytecode). They are simply now also available for ps_4_0_level_9_*, where the D3D9 texld operation is repurposed for comparision filtering, described further below.\n• sample_c/sample_c_lz (the latter forcing mip level 0) behave identically since depth textures cannot be mipmapped on Feature Level 9.x.\n• Passing these shaders to CreatePixelShader (or using any of these features) on an old runtime will fail.\n• If any state is configured incorrectly by the application, either the runtime will fail state creation, else if the mismatch is only visible at Draw-time the Draw call gets dropped by the runtime. Basically the runtime drops the Draw call if a texture is bound and it is depth but the sampler is not a comparision sampler or the texture is not depth and the sampler is comparison. This validation does not check whether the current shader even uses the texture at all, so in that sense it is stricter than necessary (for simplicity of implementation).\n\n7.18.15.1.1 Mapping the Shadow Buffer Scenario to the D3D9 DDI\n\nThe D3D11.1 runtime maps this shadow scenario to the D3D9 DDI (regardless of hardware feature level) as follows.\n• Surfaces can be created with both depth and texture flags as long as the format is either D3DDDIFMT_S824 or D3DDIFMT_D16.\n• If a depth texture is bound as a texure input to the Pixel Shader, comparison filtering with less-equal comparison is always assumed. (There is no DDI in D3D9 for explicitly turning on or off comparison filtering.)\n• The Mag and Min filter settings in the comparison filter choose between linear or point filtering (using different choices for Mag/Min filter is undefined). Anisotropic filtering is not allowed. The Mip filter choice is meaningless since Feature Level 9.x does not allow mipmapped depth buffers.\n• BorderColor addressing is allowed to be requested by the application when a depth buffer is set as a texture. For all other cases border addressing is not allowed on Feature Level 9.1 and 9.2.\n• In the Pixel Shader, the 3rd component of the texture cooordinate input to the texld instruction specifies the reference z value to use during comparision filtering. For a description of comparision filtering, refer to D3D10+ sample_c shader instruction. The difference for the (repurposed) D3D9 texld instruction is that the z value is packed with the texture coordinate rather than a separate argument.\n\nThe D3D11 CheckFeatureSupport() API has a new capability that can be checked: D3D11_FEATURE_D3D9_SHADOW_SUPPORT. This is set to true if the driver is Win8+ (no need to ask the driver anything else).\n\nOn the other hand if the D3D11 CheckFeatureSupport() / CheckFormatSupport() APIs are used to query format support on the individual DXGI_FORMAT_* names described here, the runtime will NOT report support for any capabilities specific to the shadow buffer scenario. For example support for using DXGI_FORMAT_R16_UNORM as a texture is not reported on Feature Level 9.1/9.2 (though it is supported on 9.3, independent of the shadow scenario).\n\nDuring Texture Sampling(7.18), the amount of range required for selecting texels (after scaling normalized texture coordinates by texture size) is at least 2 . This range is centered around 0.\n\nThe amount of subtexel precision required (after scaling texture coordinates by texture size) is at least -bits of fractional precision (2 subdivisions).\n\nIn mipmap selection, after conversion from float, at least -bits must represent the integer component of the LOD, and at least -bits must represent the fractional component of an LOD (2 subdivisions).\n\nAll of the texture filtering operations in D3D . , when being performed on floating point formats (regardless of format width), are required to follow the D3D . Floating Point Rules(3.1), with one exception: When a filter weight of 0.0 is encountered, NaN's or signed zeros may or may not be propagated from the source texture.\n\nTexture filtering operations performed on fixed point formats must be done with at least as much precision as the format.\n\nHere are some general observations about things that can be expected of texture filtering operations.\n• On a filter that samples from multiple texels, the output must fall between the min and the max values of the texels accessed. Consequently, filtering a constant-color texture always yields that color. The one exception to this is that as stated here(22.4.15), point sampling of a denormalized -bit float value, the result may or may not be flushed.\n• In a bilinear filter operation, colors must increase or decrease monotonically as a function of the U or V filter weights.\n\nSampling from a slot with no texture bound returns in all components.\n\nTo satisfy the performance requirements of inner loop code, the overhead of calling conventions and lost optimizations needs to be addressed. Our method avoids the overhead by using a subroutine model that virtually \"inlines\" the functions that can be called. This is done by compiling code normally up to a call site, and then compiling all possible callees with the current state of the caller. The functions called would then be optimized for the current register state by mapping inputs and outputs to their current register locations. While this approach increases overall program size, it avoids the cost of both parameter passing and stack save/restore, thereby avoiding the overhead of traditional function calls while preserving runtime flexibility.\n\nThe IL ASM has code blocks that act and look like subroutines; there are defined in/out parameters and registers are all local (in/out/temp/scratch). Some global references remain: textures, constant buffers, and sampler. The main difference from normal subroutines is that each location that can call a subroutine has a declaration describing the call destinations that are possible.\n\nThe set of functions to call when executing a given shader program can be changed between draw calls when calling SetShader. When binding the shader program to the pipeline, the list of functions to use is specified. Selecting the set of functions to use between draw calls allows the driver to recalculate the hardware requirements for a specified set of functions. Calculating the true number of registers required for a given \"specialization\" of a shader provides the combined flexibility of choice at runtime and the performance of a specialized shader.\n\nThe primary difference of this approach from \"real\" subroutines is that at runtime no calling convention is used. Each time a function could be called, a version of the function is emitted to match the caller’s register and other state. Since a new version of the callee is emitted for each location in the caller code that the function is called from, all optimizations used when inlining apply, except that callee code must remain functionally separate from caller code.\n\nTake an example: The main function has an fcall(22.7.19) instruction and that fcall instruction has two function implementations that could be called. When generating the microcode for the program to execute, the code is generated up to the fcall routine and the current state of the registers and other shader state is stored off in \"StateBeforeCall\". Then code is generated for the first function that can be called starting with the current state of register allocation, scratch registers, etc. Next the current state is restored to StateBeforeCall and the code for the second function is generated. Finally the current state is restored to StateBeforeCall again and the impacts of the outputs of the fcall are applied to the current state, and code generation continues after the fcall.\n\nLimitations are present in the IL that allow for the calling destination to have a version of a function’s microcode emitted using the current register knowledge of the caller to allocate the callee’s local registers after the caller’s registers so that no saving/restoring of data is required when crossing the function boundary.\n\nThe downside from \"real\" subroutines is that the amount of code to represent the program can become quite large. No code sharing is done between multiple call sites. If code is larger than the code cache, and the miss latency is not hidden by some other mechanism, then \"real\" subroutines are very useful. Assuming that the code bloat size is minimal (i.e. each function is only ever called from one location), then performance will be better with the new method – no parameter passing overhead, inlining optimizations, etc.\n\nAnother problem with the new method is that all destinations must be known at compile time. Due to validation that is currently done, all calls will be need to be known. As that requirement is relaxed, \"real\" subroutines are a better way of handling late binding destinations.\n\nHLSL requires that all texture and sampler parameters be rooted in some well-known global object so that the compiler can determine which texture or sampler index to use for a particular texture or sampler variable throughout the entire program. As fcalls constitute a late-binding boundary the compiler cannot easily track parameter identity and thus texture and sampler arguments to fcalls are not allowed. Note that when only concrete classes are used this isn’t a problem. Additionally, texture and sampler members of classes should be allowed, this limitation only applies to parameters to interface methods that are used with full fcall dispatch.\n\nAlso see the related topics Uniform Indexing of Resources and Samplers(7.11) as well as the this[](22.7.20) register.\n• Not intended for reuse of microcode for standard libraries\n• Changes to functions called occurs between draw calls – relatively low frequency\n\nThe programming model for subroutines is an interface driven model. The interface provides the definition of the function tables that can be switched between efficiently. A level of data abstraction is also present to allow for swapping of both data and function pointers during SetShader calls. At SetShader time, an array of class instantiations is specified that correspond to the interfaces that are used by the shader. The shader reflection system specifies information for each entry in the required interface array. A runtime reflection API is required to be able to specify the class instance in a way that can be efficiently mapped by the runtime to function pointers for the driver calls to consume. The runtime API does not need to be complex, just a method of providing handles to class instances.\n\nThe runtime API has only one goal: Provide a handle to SetShader that can be efficiently used to specify to the driver what functions should be executed for a given shader bind. To achieve this goal, a collection of class information is required if the class instance handles are to be shared across multiple shaders i.e. between all shaders within an effect. When a shader is created, a ID3D11ClassLinkage is a new parameter that specifies where to add the class metadata to. If the same class library is specified to two shaders, then the same class instance handles are used when binding either shader. The collection of class metadata could be global to a given device, but that could become cumbersome when mixing large collection of shaders (i. e. keeping a middleware solution separate from another middleware solution).\n\nThis adds support for 10bit (2.8 fixed point) and 16bit precision float and in some cases limited integer arithmetic to shader model 2.0+.\n\nShader<->memory I/O operations are unchanged for simplicity, e.g. shader constants continue to be defined as 32-bit per component.\n\nImplementations are allowed to execute low precision operations at higher precision. So 10-bit arithmetic could be done at 10-bits or more (say 32-bit) precision.\n• Enable D3D applications to take advantage of hardware that implements low precision shader arithmetic\n• Shaders authored for low precision work unmodified on hardware that operates at higher precision\n• Application does not have to author multiple versions of a shader, but has to be careful that the shader will operate at variable precision as low as the minimum precision it chooses\n• Shaders authored for low precision can trivially be cleaned up by the runtime to be in a format that old drivers understand\n• Minimal driver work to either support low precision processing or not support it\n• E.g. Drivers can compile shaders once when they are initially submitted\n• Ideally, Constant Buffers also don’t require any special processing by drivers to account for the contents being referenced at various precisions (IHV can choose to build downconverting hardware for this)\n• Drivers that don’t support the feature can simply ignore the precision hints.\n• To understand the precision level a given shader instruction in the bytecode can operate (including converting precisions on operands if necessary), drivers will not have to do any complex far reaching analysis – just looking at the current instruction should be informative enough, possibly with the help of shader declarations.\n• Application codebase does not need to change at all to use low precision shaders\n• Shaders can be dropped in with no other codebase change\n• Low precision support is added to all interesting shader models (2.x-5.0) as opposed to limiting it to the bottom end or adding a new shader model.\n• Applications don’t have to make a choice between choosing low precision vs using other features if the hardware supports it all.\n• Similarly hardware vendors implementing any shader level can choose to exploit low precision (indepdendent decisions).\n• Data format for the various low precisions is well defined, though it is not directly visible to applications\n• During shader execution, implementations can use equal or any amount of additional precision.\n\nThe new 10 and 16 bit precision levels for shaders are inspired by their existence in some real hardware and their presence in OpenGL ES. (8 bit was considered but cut due to its limitations versus the value it seemed to provide at the time).\n\nThis is a 2.8 fixed point value, though the fixed point semantics may not be identical to the general fixed point semantics defined in the D3D10+ specs. Following the D3D10+ fixed point semantics is recommended for future hardware that may choose to implement the 10-bit precision level.\n\n8-bit UNORM data is invertable when passed through 10-bit min-precision storage. For example: Suppose UNORM 8-bit data that is point sampled from the texture format DXGI_FORMAT_R8G8B8A8_UNORM gets read into a shader and is stored and passed around in the 10-bit representation. If that data s subsequently written unchanged out to a UNORM 8-bit output (such as a DXGI_FORMAT_R8G8B8A8_UNORM rendertarget) the output UNORM value matches the input UNORM value. This guarantee does not (cannot) apply for other formats passing through 10-bit, such as 8-bit UNORM_SRGB or higher precision UNORM values like 16-bit UNORM.\n\nFrom the shader point of view the 10-bit min-precision level this appears as a float value with at minimum [-2,2) range.\n\nHardware that supports 10-bit precision must also support 16-bit precision.\n\nFor float values, this is float 16 as defined in the D3D10+ specs. The exception is that for Shader Models 2, the max. exponent encoding (normally defining NaN/INF) are unused (undefined).\n\nConversion from float32 (e.g. from shader constants) to float16 may or may not flush float16 denorm to 0, and round to zero is used, per D3D spec for high to low precision float. Float16 arithmetic operations within the shader may or may not flush float16 denorm to 0, and may either round to nearest even or truncate to a representable number. Out of range values in conversion from float32 or arithmetic may produce +/-MAX_FLOAT16 or +/- INF.\n\n16-bit integer min-precision is available as well in HLSL. For Shader Models 2, this is constrained to be representable as integral floats (1.0f, 2.0f, etc.) in a float16 encoding. In the shader bytecode these appear simply as float16, so native integer operations are not available. (it may not be worth bothering to expose this constrained form of int16 for SM 2/3)\n\nFor shader model 4+, native integer ops can be used on 16-bit min-precision values, however applications must beware that the device could choose to simply use larger-than-16-bit (e.g. 32 bit) integer ops without any clamping to maintain the illusion that there are not more than 16 bits present.\n\nShader Constants feeding 16-bit shader arithmetic are always fp32 encoded for Shader Model 2. For Shader Models 4+, Shader Constants feeding 16-bit in the shader are specified as float32 or UINT32/INT32 as appropriate (i.e. unchanged from the way constants feed into float32 arithmetic).\n\nA new MIN_PRECISION enum is added to the source and dest parameter token, definition below. This specifies the minimum precision level for the entire operation – implementations can use equal or greater precision. This new enum co-exists with the PARTIALPRECISION flag that is already in the same dest parameter token – see the comment below.\n\nThe src/dest token for instructions in PS/VS 2.x can use the MIN_PRECISION enum in the following circumstances:\n• Any shader instruction with an output (e.g. arithmetic, texture fetch instructions )\n• \n• Does not apply to PS 2.x input color (v#) declarations, as these were already by definition called out to be as low as 8 bit)\n• See here(7.20.3.4); this is common across D3D9 and D3D10+.\n\nA new MIN_PRECISION enum is added to the dest parameter token, definition below. This specifies the minimum precision level for the entire operation – implementations can use equal or greater precision.\n\nThe encoding distinguishes type (e.g. float vs. sint vs. uint), in addition to precision level, to disambiguate instructions like “mov” that don’t already imply a type. This makes a difference when there is a size change involved in the instruction. E.g. moving a 32 bit float to a min. 16 bit float is a different task for hardware than moving a 32 bit uint to a min. 16 bit uint. This type distinction is not needed for the D3D9 shader bytecode because all arithmetic is “float” there.\n\nThe dest and source operand tokens in SM 4.0+ can use the MIN_PRECISION enum in the following circumstances:\n• Any instruction that returns values to the shader\n• Those involving doubles, e.g. ftod or dtof only allow precision lowering on the float32 side of the operation.\n• Other conversions, such as f32tof16, allow precision lowering on either side of the operation.\n• load/store to non-Typed Unordered Access Views (Typed UAVs ok, since that involves format conv.)\n• At VS input, the input data types continue to be defined externally (Input Layout), but MIN_PRECISION can still be part of the shader input declaration, indicating how the shader will expect to see the data after it has been read in (post format conversion).\n\n7.20.3.4 Interpreting Precision (same for D3D9 and D3D10+)\n• Source operands are incoming stored at the (minimum) precision indicated on the operand. If no minimum precision is specified (default) the operand precision is 32-bit.\n• The precision specified on the output operand determines the minimum storage needed for the output as well as the minimum precision for the operation.\n• Mixing precisions across operands and the instruction is valid, but should be rare. Drivers may need to expand format changes into separate individual type conversions to the instruction’s precision unless the conversion is supported natively.\n• Precisions on the index value in dynamic indexing scenarios or other addressing (such as texture coordinates for a texture fetch) just follow the precision indicated on the value, unaffected by the instruction precision. The same applies for condition parameters in conditional instructions (like movc).\n• See below(7.20.3.5) for a discussion about shader constants.\n\nShader constants are defined at full 32-bit per component. New hardware implementing low precision is encouraged to design efficient downconversion support upon constant access, otherwise some driver work or extra conversion instructions will need to be added by the driver into shaders that read 32-bit per component constants into lower precision shader operations.\n\nAlternative approaches were considered where low precision constants are exposed all the way to the application (freeing driver/hardware from having to convert constants), but the added complexity in the programming model vs the benefit didn’t hold up at least at this time.\n\nWhen referencing a shader constant from a low precision instruction, if the constant value is out of the range of the instruction’s precision level, the value read is undefined. For constant values within range of a low precision instruction reference, the precision of the value may still get quantized down from full 32 bits.\n\nShader constants referenced in shader source operands will be marked at the precision they are to be referenced at, even though they come down the API/DDI at 32-bit per component.\n• The constant buffer precision indicated on reference may be different than the precision of a given instruction, since multiple instructions in the shader at different precisions may read the same constant.\n• The HLSL compiler guarantees that all accesses of a given constant are marked with the same precision, indicating how much storage is needed for them regardless of what precision operations that reference them are using.\n• Implementations that may need to downconvert constants ahead of shader invocation (likely not ideal) can easily determine the required precision/storage for constants within a shader just by observing how they are tagged on first reference in the shader.\n• In cases of dynamic indexing of constants, there is no way to know which parts of a constant buffer will be referenced at what precision ahead of time. Adding declarations that indicate this information was not deemed worth it at this time.\n\nLow precision data is referenced by component in masks and swizzles – xyzw - just like default precision data. It is as though the registers do have a smaller number of bits (for hardware that supports lower precision). This is unlike the way double precision is mapped, where xy contains one double and zw contains another. Low precision doesn’t yield sub-fields within .x for example.\n\nThe HLSL compiler will not generate code that mixes precisions in different components of any xyzw register (mostly for simplicity, even though this may not matter for hardware).\n\nThe use of min / low precision specifiers never increases the maximum amount of resources available to a shader (such as limits on inputs, outputs or temp storage), since the shader must always be able to function on hardware that does not operate at low precision.\n\nIn the D3D system, HLSL shaders are compiled independent of any given device – e.g. they should typically be compiled offline. This compilation step produces device-agnostic bytecode, apart from the choice of shader target, e.g. vs_4_0.\n\nThe minimum precision facility described above can be optionally used within any 4_0+ shader, including 4_0_level_9_1 to 4_0_level9_3. These shader targets are all available through the D3D11 runtime, exposing D3D9+ hardware via Shader Model 2_x+. The D3D9 runtime will not expose the low precision modes – updating that runtime is out of scope.\n\nThere is a mechanism at the API to discover the precision levels supported by the current device. Note that in Windows 8 the OS did not allow drivers to expose only 10 bit without also exposing 16 bit, but subsequent operating systems relax that requirement (so an implementation may expose 10 bit min precision but not 16 bit min precision).\n\nEven though the hardware’s precision support is visible to applications, applications do not have to adjust their shaders for the hardware’s precision level given that by definition operations defined with a min precision run at higher precision on hardware that doesn’t support the min precision.\n\nIt is fine for hardware to not support low precision processing at all – by simply reporting “DEFAULT” as its precision support. The reason it is called “DEFAULT” rather than some numerical precision is depending on the shader model, there may not be standard value to express. E.g. the default precision in SM 2.x is fp24 (or greater) within the shader, even though there is no API visible fp24 format. If the device reports “DEFAULT” precision, all min-precision specifiers in shaders are ignored.\n\nD3D9 devices are permitted to report a min-precision level that is lower for the Pixel Shader than for the Vertex Shader (all reported via the Windows Next D3D9 DDI). D3D10+ devices can only report a single min-precision level that applies to all shader stages (reported via the Windows Next D3D11.1 DDI) – since it does not seem to make sense to single out the VS any more. Note that if the application uses Feature Level 9_x on D3D10+ hardware, the D3D9 DDIs are still used, so the min-precision levels can be reported differently there between VS and PS, as mentioned for D3D9, even though via the D3D11.1 DDI only a single precision can be reported.\n\nRegardless of the min precision level supported by a given device, it is always valid to use a shader that was compiled using any combination of the low precision levels on it. For example if a device’s min precision level is 32-bit, it is fine to use a shader compiled with some variables that have a min precision of 10 bit. The device is free to implement the low precision operations at any equal or higher precision level (including precision levels not available at the API).\n\nFor old drivers (pre-D3D11.1 DDI) that are not aware of the low precision feature, the D3D runtime will patch the shader bytecode on shader creation to remove it. This preserves the intent of the shader, since it is valid for the device to execute operations tagged with a min precision level at a higher precision.\n\nAn API for reporting device precision support, no other D3D11 API surface area changes apply.\n\nAs far as other DDI additions, there is device precision reporting, the shader bytecode additions detailed earlier, and finally a variant of the existing shader stage I/O signature DDI:\n\nThe I/O signature DDI includes MinPrecision in the signature entry. This shows up as D3D11_SB_INSTRUCTION_MIN_PRECISION_DEFAULT if the shader didn’t specify a min-precision:\n\nMotivation: Recall that this DDI exists to complement the shader creation DDIs by providing a more complete picture of the shader stage<->stage I/O layout than may be visible just from an individual shader’s bytecode. For example sometimes an upstream stage provides data not consumed by a downstream shader, but it should be possible for a driver to compile a shader on its own without having to wait and see what other shaders it gets used with. MinPrecision is added in case that affects how the driver shader compiler would want to pack the inter-stage I/O data.\n\nOut of scope for this spec.\n\nAn overview of the IA is at the beginning(2.1) of the document. This section provides implementation details more like they are viewed from the DDI perspective (exact parameter names may not match). The API view is different, in that instead of hardcoding shader register numbers in the state declaration, names are used, and when creating Input Assembler State objects, the runtime figures out which registers the names correspond based on a shader input signature definition.\n\nAn illustrated example of the IA being used is at the end(8.21) of this section.\n\nThe states defining the Input Assembler's operation are described here. Draw*() commands on the Device, described below(8.2), use the currently active IA state to define most of their behavior.\n\nThe following enumeration lists the various Primitive Topologies(8.10) available to the IA.\n\nThe current primitive topology for the IA is defined by the following method:\n\nThe following enumerations are used to build declarations of 1D Buffer structure layout. Structure fields are defined with format and offset, plus a target register. Multiple elements (from one or more structures) can not feed a single register.\n\nThe following command creates an input layout.\n\nThe following methods bind input vertex buffer(s) to the IA. A set of up to Buffers can be bound at once. The layout of verrtex or instance data in all of the Buffers is defined by an Input Layout object. There is also a method for binding an Index Buffer to the IA (having a single Element format describing its data layout).\n\nThe following rendering commands on a device, Draw()(8.3), DrawInstanced()(8.4), DrawIndexed()(8.5), DrawIndexedInstanced()(8.6), DrawInstancedIndirect()(8.7), and DrawIndexedInstancedIndirect()(8.8) introduce primitives into the D3D . Pipeline.\n\n8.3.1 Pseudocode for Draw() Vertex Address Calculations and VertexID/PrimitiveID/InstanceID Generation in Hardware\n\nSee the pseudocode for DrawInstanced(), below. Draw() behaves the same as DrawInstanced(), with InstanceCount = 1 and StartInstanceLocation = 0. If \"Instance\" data has been bound, it will be used. But the intent is for this method to be used without instancing.\n\n8.5.1 Pseudocode for DrawIndexed() Vertex Address and VertexID/PrimitiveID/InstanceID Calculations in Hardware\n\nSee the pseudocode for DrawIndexedInstanced(), below. DrawIndexed() behaves the same as DrawIndexedInstanced(), with InstanceCount = 1 and StartInstanceLocation = 0. If \"Instance\" data has been bound, it will be used. But the intent is for this method to be used without instancing.\n\nIf the address range in the Buffer where DrawInstancedIndirect’s parameters will be fetched from would go out of bounds of the Buffer, behavior is undefined.\n\nHere(18.6.5.1) is a discussion about ways to initialize the arguments for DrawInstancedIndirect.\n\nIf the address range in the Buffer where DrawIndexedInstancedIndirect’s parameters will be fetched from would go out of bounds of the Buffer, behavior is undefined.\n\nHere(18.6.5.1) is a discussion about ways to initialize the arguments for DrawIndexedInstancedIndirect.\n\nDrawAuto is used with StreamOutput(14) in order to use a Stream Output Buffer as an Input Assembler Vertex Input Buffer without requiring the BufferFilledSize to get back to the CPU. The Buffer bound to slot zero must have both the Stream Output andInput Assembler Vertex Input Bind Flags set. When invoked, DrawAuto will draw from the Buffer offset associate with slot zero to the BufferFilledSize(14.4) associated with the Buffer. If the BufferFilledSize is less then or equal to the specified buffer offset, then nothing is drawn. The primitive type for DrawAuto is the current primitive topology set via IASetPrimitiveTopology(8.1.2), regardless of the geometry shader output topology used while the buffer is filled.\n\nBuffers may be bound to other IA input slots above zero for DrawAuto (only the IA bind flag is required on these slots), and these can be part of the Vertex Declaration as well. Reading out of bounds on any Buffer above slot zero in DrawAuto invokes the default behavior for reading out of bounds (as with any other Draw* call).\n\nThe diagram below defines the vertex ordering for all of the primitive topologies that the IA can produce. The enumeration of primitive topologies is here(8.1.2).\n\nAs an example, suppose the IA is asked to draw triangle lists with adjacency, and it is invoked with a vertex cont of 36 by a Draw() call. From the diagram it should be apparent that a 36-vertex triangle list with adjacency will result in 6 completed primitives.\n\nNot shown in the previous diagram (but part of the same list) are 32 additional topologies which represent 1... control point patches, respectively. These Patch topologies can be used with Tessellation(11). Also, when Tessellation is disabled(11.8) (meaning no Hull Shader and no Domain Shader bound), they can be fed to the Geometry Shader and/or Stream Output, allowing patch data to be saved to memory, and allowing non-traditional primitive types to be fed to the GS (such as simulating cubes using 8 control point patches to represent 8 vertices).\n\nIn Indexed rendering of strip topologies, the maximum representable index value in the index format (i.e. for 32-bit indices) means the strip defined up to the previous index is to be completed, and the next index is a new strip. This special \"cut\" value is not required to be used, in which case a DrawIndexed*() command will simply draw one strip. In IndexedInstanced rendering, there is an automatic \"cut\" after every instance. Regardless of Instanced rendering or not, it is optional whether to make the last index the cut value, or omit the value; both result in the same behavior, except that the IndexCount[PerInstance] parameter to DrawIndexed[Instanced]() is different by 1.\n\nEven if the current Primitive Topology is not a strip, then the cut index value still takes effect, potentially resulting in an incomplete primitive (see next section). Thus, handling of the cut is kept orthogonal to primitive topology, even though it is not useful for some of them.\n\nEach Draw*() call starts a new Primitive Topology; there is no persistence of any topology produced by a previous Draw() call. Triangle strips don't continue across Draw() call boundaries.\n\nIf a Draw*() call produces incomplete primitives (not enough vertices), either at the end of the Draw*() call, or anywhere in the middle (possible with the \"cut\" index), any incomplete primitives are silently discarded. For example, suppose a Draw*() call is made with triangle list as the topology, and an vertex count of 5. This case would result in a single triangle, and the last 2 vertices being silently discarded. For another example showing handling of an incomplete primitive, see the diagram under the Geometry Shader Stage here(13.10), depicting which primitives are instantiated given a triangle strip with adjacency that has a dangling vertex.\n\nFor the purpose of assigning constant vertex attributes to primitives, there must be a way to map a vertex to a primitive. Let us identify the vertex in a primitive which supplies its per-primitive constant data as the \"leading vertex\". A primitive topology can have multiple leading vertices, one for each primitive in the topology. The leading vertex for an individual primitive in a topology is the first non-adjacent vertex in the primitive. For the triangle strip with adjacency above, the leading vertices are 0, 2, 4, 6, etc. For the line strip with adjacency, the leading vertices are 1, 2, 3 etc.\n\nNote that adjacent primitives have no leading vertex. This means that there is no primitive data associated with adjacent primitives. With the strip topologies, a given interior primitive has some adjacent primitives which are also interior to the topology, and so actually can have primitive data. However, as far as the Geometry Shader is concerned (it sees a single primitive and its neigboring primitives in an invocation), only the single interior primitive defining the Geometry Shader invocation can have Primitive Data, and adjacent primitives, whether they are interior to the strip or adjacent primitives on the strip, never come with Primitive Data.\n\nThe only place in the Pipeline where adjacency information is visible to the application is in the Geometry Shader. Each invocation of the Geometry Shader sees a single primitive: a point, line, or triangle, and some of these might include adjacent vertices.\n\nThe inputs to the Geometry Shader are like a single primitive of any of the \"list\" primitive topologies (with or without adjacency) in the diagram above. When adjacency is available, the Geometry Shader will see the appropriate adjacent vertices along with the primitive's vertices. So for example if the Geometry Shader is invoked with a triangle including adjacency (the source could have been a strip with adjacency), this would mean that data for 6 vertices would be available as input in the Geometry Shader: 3 vertices for the triangle, and 3 for the adjacency.\n\nThe data layout for adjacent vertices is identical to the standard vertices they accompany. Note that Vertex Shaders are always run on all vertices, including adjacent vertices. Note that adjacent vertices are typically also surface vertices some other primitive that will get drawn, so the Vertex Shader result cache can take advantage of this.\n\nWhen the IA is instructed to produce a primitive topology with adjacency for its output, all adjacent vertices must be specified. There is no concept of handling edges with no adjacent primitive. The application must deal with this on their own, perhaps by providing a dummy vertex (possibly forming a degenerate triangle), or perhaps by flagging in one of the vertex attributes whether the vertex \"exists\" or not. The application's Geometry Shader code will have to detect this situation, if desired, and deal with it manually. Implied in this is that there must be no culling of degenerate primitives until rasterizer setup, so that the Geometry Shader is guaranteed to see all geometry.\n\nNote that when Tessellation is enabled, topologies with adjacency cannot be used. The Tessellator operates a patch at a time without hardware knowledge about adjacency (although shader code is free to encode it on its own). The Tessellator's outputs are independent primitives, with no adjacency information.\n\nVertexID is a -bit unsigned integer scalar counter value coming out of Draw*() calls identifying to Shaders each vertex. This value can be declared(22.3.11) for input by the Vertex Shader only.\n\nFor Draw() and DrawInstanced(), VertexID starts at 0, and it increments for every vertex. Across instances in DrawInstanced(), the count resets back to the start value. Should the -bit VertexID calculation overflow, it simply wraps.\n\nFor DrawIndexed() and DrawIndexedInstanced(), VertexID represents the index value.\n\nThe mere presence of VertexID in a Vertex Shaders' input declarations activates the feature (there is no other control outside the shader). If the application wishes to pass this data to later Pipeline stages, the application can do so by simply writing the value to a Shader output register like any other data.\n\nFor Primitive Topologies with adjacency, such as a triangle strip w/adjacency, the \"adjacent\" vertices participate have a VertexID associated with them just like the \"non-adjacent\" vertices do, all generated uniformly (i.e. without regards to which vertices are adjacent and which are not in the topology).\n\nFor more information, see the general discussion of System Generated Values here(4.4.4), the reference for VertexID here(23.1), and the System Interpreted/Generated Value input(22.3.11) declaration for Shaders.\n\nPrimitiveID is a -bit unsigned integer scalar counter value coming out of Draw*() calls identifying to Shaders each primitive. This value can be declared(22.3.11) for input by either the Hull Shader, Domain Shader, Geometry Shader or Pixel Shader Stage. For the GS and PS, if the GS is active the hardware PrimitiveID goes there and shader computed PrimitiveIDs go to the PS.\n\nPrimitiveID starts at 0 for the first primitive generated by a Draw*() call, and increments for each subsequent primitive. When Draw*Instanced() is used, the PrimitiveID resets to its starting value whenever a new instance begins in the set of instances produced by the call. Should the -bit PrimitiveID calculation overflow, it simply wraps.\n\nThe mere presence of PrimitiveID in a compatible Shader Stage's input declarations activates the feature (there is no other control outside the shader). In the Geometry Shader this is declared as the special register vPrim (to decouple the value from the other per-vertex inputs). If the application wishes to pass PrimitiveID to a later Pipeline stage, the application can do so by simply writing the value to a Shader output register like any other data. The Pixel Shader does not have a separate input for PrimitiveID; it just goes into a component of a normal input register, with the requirement that the interpolation mode on the entire input register (which may contain other data as well in other components, is chosen as \"constant\".\n\nFor Primitive Topologies(8.10) with adjacency, such as a triangle strip w/adjacency, the PrimitiveID is only maintained for the interior primitives in the topology (the non-adjacent primitives), just like the set of primitives in a triangle strip without adjacency. No point in the Pipeline has a way of asking for an auto-generated PrimitiveID for adjacent primitives.\n\nFor more information, see the general discussion of System Generated Values here(4.4.4), the reference for PrimitiveID here(23.2), and the System Interpreted/Generated Value input(22.3.11) and output(22.3.33) declarations for Shaders.\n\nInstanceID is a -bit unsigned integer scalar counter value coming out of Draw*() calls identifying to Shaders which instance is being drawn. This value can be declared(22.3.11) for input by the by the Vertex Shader only.\n\nInstanceID starts at 0 for the first instance of vertices generated by a Draw*() call. If the Draw is a Draw*Instanced() call, after each instance of vertices, the InstanceID increments by one. If the Draw is not a Draw*Instanced() call, then InstanceID never changes. Should the -bit InstanceID calculation overflow, it simply wraps.\n\nThe mere presence of InstanceID in the Vertex Shader's input declarations activates the feature (there is no other control outside the shader). If the application wishes to pass this data to later Pipeline stages, the application can do so by simply writing the value to a Shader output register like any other data.\n\nFor more information, see the general discussion of System Generated Values here(4.4.4), the reference for InstanceID here(23.3), and the System Interpreted/Generated Value input(22.3.11) declaration for Shaders.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 8.19.1 Input Assembler Arithmetic Precision\n\n 8.19.2 Addressing Bounds\n\n 8.19.3 Buffer and Structure Offsets and Strides\n\n 8.19.4 Reusing Input Resources\n\n 8.19.5 Fetching Data in the IA vs. Fetching Later (i.e. Multiple Ways to Do the Same Thing)\n\n\n\nThe Input Assembler performs -bit unsigned integer arithmetic, conforming to the IA addressing pseudocode shown in this spec. In other words, should any calculation overflow -bits, it would wrap - and should that result happen to fall back into a valid range for the scenario, so be it. Wherever input parameters are listed as signed integers (such as BaseVertexLocation in DrawIndexed()(8.5)) they are interpreted, unaltered, as unsigned -bit numbers, used in unsigned -bit addressing arithmetic, producing unsigned -bit results.\n\nAn individual Draw*() call is limited to producing a finite number of vertices. This limit includes any instancing that is occurring within the Draw*() call. Independent of such a limit, there are also limits on how big various source data buffers can be. All of these (large) numbers can be found within the table(21) in the Limits On Various System Resource section. These numbers are expected to be reasonable for the foreseeable lifetime of D3D . .\n\nAny calculated address that would fall out of bounds for a Buffer being accessed results in out-of-bounds behavior being invoked, where the return is in all non-missing components of the format (defined in the Input Layout), and the default for missing components (see Defaults for Missing Components(19.1.3.3)). This out-of-bounds behavior applies, for example, when an index refers to a vertex number that is outside of the bound vertex buffer.\n\nThe minimum extent for the bounds is any initial offset applied on the Buffer (so \"negative\" indexing isn't a feature).\n\nIt is perfectly legal to read any given memory Buffer in multiple places in the Pipeline, including the IA, simultaneously, even applying different interpretations to the data in the Buffer. A single Buffer can even be set as input at multiple slots at a single stage such as the IA.\n\nFor example, suppose an application has a Vertex Shader that requires 2 different sets of input texture coordinates. One scenario could be to use 2 different input Buffers to provide the different texture coordinates to be fetched by the IA (or both texture coordinates could be interleaved in one Buffer). But an alternate, equally valid scenario is to reuse the same source data to supply both texture coordinates to what the Vertex Shader expects as two different sets. This is simply a matter of binding the same input Buffer to two different input slots.\n\nAnother way to achieve the same effect of reusing a single set of data is to bind the source texture coordinate Buffer to a single slot and provide a data declaration where the definition of 2 different texture coordinates overlaps (same structure offset). Partial-overlapping of types in a data declaration is even permitted (even though it isn't interesting); the point is that D3D . doesn't care or bother to check.\n\nSimilarly, the structure stride in a vertex declaration can be any non-negative value (up to a maximum of Bytes, and conforming to alignment(4.4.6) rules), without regards to whether it is large enough to support the fields defined for the structure. Again, the point is that D3D . doesn't care or bother to check. Debug tools can be provided to optionally enforce well-ordered, logical data layouts, however the arithmetic that underlying hardware uses to actually address data simply blindly follows the intent shown by the pseudocode for address-calculations for the Draw*()(8.2) routines.\n\nIt is legal to have a single Buffer containing both vertex data and index data, and thus bind the Buffer at both a Vertex Buffer input slot and as an Index Buffer simultaneously. One might store indices at the beginning of the Buffer and the vertex data being referred to elsewhere in the same Buffer. D3D . doesn't care.\n\nAs yet another, final (contrived) example, to drive the point home: Suppose a Vertex Buffer is set as input to the IA to provide data for vertices going to the Vertex Shader (as usual). Simultaneously, the same Vertex Buffer may be accessed directly by the Vertex Shader, if for some reason the Shader occasionally wanted to look at some of the input data for vertices other than itself.\n\n8.19.5 Fetching Data in the IA vs. Fetching Later (i.e. Multiple Ways to Do the Same Thing)\n\nThe highly flexible and programmable nature of the D3D . Pipeline leads to many situations where there are multiple ways to accomplish a single task. A particular example relevant to this section is that the fetching of vertex data performed by the IA can be identically performed by doing memory fetches from the Vertex Shader only given a VertexID as input. There are nice properties from this, such as the fact that even though the amount of data the IA can pre-fetch for a single vertex is limited in size, memory fetches from shaders can allow much more unbounded amounts of vertex data to be fetched if necessary. Memory fetches from shaders can also use much more complex addressing arithmetic than the common-case dedicated fixed-function arithmetic used by the IA.\n\nNo guarantees or requirements are made by D3D . , however, as to the performance characteristics of using alternative mechanisms to perform a task that can be performed by an explicit feature intended for that task in the Pipeline. As a general rule, whenever there is an explicit mechanism to perform a task in D3D . , IHVs and ISVs should assume that as much as possible, the dedicated functionality is the preferred route, at least when all of or most of the other parts of the graphics Pipeline are simultaneously active.\n\nWhen the Input Assembler reads Elements of data from Buffers, it gets converted to the appropriate -bit data type for the Format(19.1) interpretation specified. The conversion uses the the Data Conversion(3.2) rules. If the source data contains -bit per-component float, UINT or SINT data, it is read without modifying the bits at all (no conversion).\n\nIf a Vertex Buffer or Index Buffer is read by the Input Assembler, but the slot being read has no Buffer bound, the result of the read is for all expected components. Even though there is format information available via the input layout, defaults are not applied to missing channels for this case.\n\nThe following example shows DrawIndexedInstanced()(8.6) being used to draw 3 instances of an indexed mesh.\n\nThe example does not attempt to draw anything particularly interesting, but it does show most of the functionality of the IA being used at once, in complete detail. Included is a depiction of the resulting workload for the rest of the Graphics Pipeline.\n\nAs input, one Vertex Buffer supplies Vertex Data, another Vertex Buffer supplies Instance Data, and there is an Index Buffer. The data layouts and configuration of all of these buffers is illustrated. VertexID(8.16), PrimitiveID(8.17) and InstanceID(8.18) are all shown as well, assuming Shaders in the pipeline requested them. The Primitive Topology(8.10) being rendered is triangle strip with adjacency. The Index Buffer has a Cut(8.12) in it, so multiple strips are produced (per instance).\n\nVarious states shown in boxes represent the API settings for Buffers and for the IA states described earlier in this IA spec.\n\nFor every vertex generated by the IA, Vertex Shader is invoked, provided that there is a miss on the hardware's Vertex Shader result cache. Adjacent vertices are treated equivalently to interior vertices in a topology, so the Vertex Shader is executed for all vertices.\n\nThe primary inputs to a Vertex Shader invocation are -bit* -component registers (v#) comprising the elements of the input vertex (not all have to be used). ConstantBuffers (cb#) and textures (t#) provide random access input to Vertex Shaders.\n\nThe output of a Vertex Shader is up to -bit* component registers (o#). The o# registers to be written by the Shader must be declared (i.e. \"dcl_output o[3].xyz\").\n\nThe following registers are available in the vs_ _ model:\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 10.1 Hull Shader Instruction Set\n\n 10.2 Hull Shader Invocation\n\n 10.3 HS State Declarations\n\n 10.4 HS Control Point Phase\n\n 10.5 HS Patch Constant Phases\n\n 10.6 Hull Shader Structure Summary\n\n 10.7 Hull Shader Control Point Phase Contents\n\n 10.8 Hull Shader Fork Phase Contents\n\n 10.9 Hull Shader Join Phase Contents\n\n 10.10 Hull Shader Tessellation Factor Output\n\n 10.11 Restrictions on Patch Constant Data\n\n 10.12 Shader IL \"Ret\" Instruction Behavior in Hull Shader\n\n 10.13 Hull Shader MaxTessFactor Declaration\n\n \n\n\n\nFor a Tessellation overview, see the Tessellator(11) section.\n\nThe Hull Shader operates once per patch, transforming Control Points, computing Patch Constant data and defining Tessellation Factors.\n\nThe Hull Shader has four phases, all defined together as one program. That is, from the API/DDI point of view, the Hull Shader is a single atomic shader, and its phases are an implementation detail within the Hull Shader program. Implementations can choose to exploit independent work within a Patch by executing work within a single patch in parallel.\n\nThe phases appear in the Intermediate Language as standalone shaders, each with individual input and output declarations tailored to what each independent program is doing. However the inputs and outputs across all of the shaders come out of a fixed pool of Hull Shader-wide input data and output storage, described later in great detail.\n\nThe Hull Shader phase structure is depicted in the following picture:\n\nThis section of the Hull Shader has no executable code. It simply declares some overall characteristics of Hull Shader operation, such as how many control points the HS inputs and outputs (an independent number). The operation of the fixed function Tessellator is also defined here – such as choosing the patch domain, partitioning etc. A tessellation pattern overview is given here(11.7).\n\nNote that declarations that are typical in shaders, such as input and output register declarations and declarations of input Resources, Constant Buffers, Samplers etc. are part of each individual shader phase below, not part of this HS State declaration section.\n\nIn the Hull Shader’s Control Point phase, a thread is invoked once per patch output control point. An input value vOutputControlPointID(23.7) identifies to each thread which output control point it represents. Each of the threads see shared input of all the input control points for the patch. The output of each thread is one of the output control points for the patch.\n\nThe Patch Constant phases compute constant data such as Tessellation Factors(10.10) (how much the fixed function Tessellator should tessellate), as well as any other Patch Constant data, beyond the patch Control Points, that the application may need in the Domain Shader(12) (the shader that runs once per Tessellator output point).\n\nThe Patch Constant phases occur after the Control Point phase is complete, and has read-only access to all of the input and output Control Points. So for example, Control Points could be examined to help calculate Tessellation Factors(10.10) for each patch edge.\n\nThere are two Patch Constant phases:\n\nThe Patch Constant Fork Phase is a collection of an arbitrary number of independent programs. For the discussion in this section let us call these independent programs mini-shaders.\n\nEach mini-shader produces independent (non-overlapping) parts of the total output Patch Constant data (such as all the different TessFactors(10.10)).\n\nAn implementation could choose to execute each mini-shader in parallel, since they are independent. Or, in the opposite extreme an implementation could choose to trivially concatenate all the mini-shaders together and run them serially. Such transformations of the mini-shaders are trivial to perform (in a driver’s compiler) given they all share the same inputs and perform non-overlapping writes to a unified output space.\n\nAn implementation could even choose to hoist any amount of the code from the Fork Phase phase up into the Control Point Phase if that happened to be most efficient. This is allowable because all the parts of a Hull Shader are specified together as if it is one program – how its contents are executed does not matter as long as the output is deterministic.\n\nThe shared inputs to each mini-shader are all of the Control Point Phase’s Input and Output Control Points.\n\nThe output of each mini-shader is a non overlapping subset of the output Patch Constant Data.\n\nThere is no communication of data between mini-shaders, other than the fact that they share Control Point input.\n\nTo further enable parallelism within a single mini-shader, any mini-shader can be declared to run in an instanced fashion, given a fixed instance count per patch. During execution, each instance of an instance mini-shader is identified by a ForkInstanceID(23.8) and is responsible for producing a unique output, typically by indexing an array of outputs. So for example, a single mini-shader instanced 4 times could output edge TessFactors for each edge of a quad patch.\n\nThe final Hull Shader phase is the Patch Constant Join Phase. This phase behaves the same way as the Fork Phase, in that there can be multiple Join programs that are independent of each other. All of them execute after all the Fork Phase programs. An example use for this phase is to derive TessFactors(10.10) for the inside of a patch given the edge TessFactors computed in the previous phase.\n\nThe input to each Patch Constant Join Phase shader are all the Control Point Phase’s Input and Output Control Points as well as all the Patch Constant Fork Phase’s output.\n\nThe output of each Patch Constant Join Phase shaders is a subset of the output Patch Constant data that does not overlap any of the outputs of the shaders from the Patch Constant Fork Phase or other Join Phase shaders.\n\nSimilar to the fork phase, to enable parallelism within a join phase mini-shader, any mini-shader can be declared to run in an instanced fashion, given a fixed instance count per patch. During execution, each instance of an instance mini-shader is identified by a JoinInstanceID(23.9) and is responsible for producing a unique output, typically by indexing an array of outputs. So for example, a single mini-shader instanced 2 times could output inside TessFactors for each inside direction of a quad patch.\n\nThe various phases of the Hull Shader are described in the Intermediate Language as separate shader models. A single Hull Shader program consists of a collection of the following shaders appearing in the order listed here:\n• 1 of this section must appear in an HS program\n• 0 or 1 Control Point Phase program can be present\n• If there is no Control Point Phase program:\n• If the declared input control point count matches the declared output control point count, this is like passing through all of the control points\n• If the declared output control point count is 0, the HS does not output any control points, however the fork and join phases in the HS can always read the input control points\n• It is invalid for the output control point count to be more than 0 but not equal to the input control point count.\n• 0 or more Fork Phase programs can be present\n• 0 or more Join Phase program can be present\n\nFrom the point of view of the HLSL code author and API user, the name for the Hull Shader compiler target is simply hs_5_0\n\nhs_control_point_phase(22.3.21) is a shader program with the following register model. Note the footnotes which provide a detailed discussion of output storage size calculations.\n\n(1) Each Hull Shader Control Point Phase output register is up to a -vector, of which up to registers can be declared. There are also from 1 to output control points declared, which scales amount of storage required. Let us refer to the maximum allowable aggregate number of scalars across all Hull Shader Control Point Phase output as #cp_output_max.\n\nThis limit happens to be based on a design point for certain hardware of 4096* -bit storage here. The amount for Control Point output is =4096-128, which is (control points)* (component)* (elements) - (component)* (elements). The subtraction reserves 128 scalars (one control point) worth of space dedicated to the HS Phase 2 and 3, discussed below. The choice of reserving 128 scalars for Patch Constants (as opposed to allowing the amount to be simply whatever of the 4096 scalars of storage is unused by output Control Points) accommodates the limits of another particular hardware design. Note the Control Point Phase can declare output control points, but they just can’t be fully elements with components each, since the total storage would be too high.\n\nInstanceID(8.18) and VertexID(8.16) can be input as long as the previous Vertex Shader stage outputs them.\n\nPrimitiveID(8.17) is also available as a scalar -bit integer input for each Control Point. PrimitiveID indicates the current patch in the Draw*() call, starting with 0. This PrimitiveID is the same value that the Geometry Shader would see for every patch if it input PrimitiveID - that is every point/line/triangle produced by the tessellator for a given patch has a single PrimitiveID for the entire Patch.\n\nOutputControlPointID(23.7) is a scalar -bit integer input for each Control Point identifying which one it is [0..n-1] given n declared output Control Points.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 10.8.1 HS Fork Phase Programs\n\n 10.8.2 HS Fork Phase Registers\n\n 10.8.3 HS Fork Phase Declarations\n\n 10.8.4 Instancing of an HS Fork Phase Program\n\n 10.8.5 System Generated Values in the HS Fork Phase\n\n\n\nThere can be 0 or more Fork Phase programs present in a Hull Shader. Each of them declares its own inputs, but they come from the same pool of input data – the Control Points. Each Fork Phase program declares its own outputs as well, but out of the same output register space as all Fork Phase and Join Phase programs, and the outputs can never overlap.\n\nThe following registers are visible in the hs_fork_phase(22.3.23) model.\n\nThe input resources (t#), samplers (s#), constant buffers (cb#) and immediate constant buffer (icb) below are all shared state with all other HS Phases. That is, from the API/DDI point of view, the Hull Shader has a single set of input resource state for all phases. This goes with the fact that from the API/DDI point of view, the Hull Shader is a single atomic shader; the phases within it are implementation details.\n\nNote the footnotes which provide a detailed discussion of output storage size calculations.\n\n(1) The HS Fork Phase’s Input Control Point register (vicp) declarations must be any subset, along the [element] axis, of the HS Control Point input (pre-Control Point phase). Similarly the declarations for inputting the Output Control Points (vocp) must be any subset, along the [element] axis, of the HS Output Control Points (post-Control Point Phase).\n\nAlong the [vertex] axis, the number of control points to be read for each of the vicp and vocp must similarly be a subset of the HS Input Control Point count and HS Output Control Point count, respectively. For example, if the vertex axis of the vocp registers are declared with n vertices, that makes the Control Point Phase’s Output Control Points [0..n-1] available as read only input to the Fork Phase.\n\n(2) The HS Fork and Join phase outputs are a shared set of -vector registers. The outputs of each Fork/Join phase program cannot overlap with each other. System Interpreted values such as TessFactors(10.10) come out of this space.\n\nThe declarations for inputs, outputs, temp registers, resource etc. in an HS Fork Phase program are like any standalone shader. A given HS Fork Phase program need only declare what it needs to read and write. Further, if it does not need to see all Input or Output Control Points, it can declare a subset of the counts for each, by declaring a smaller number on the [vertex] array axis than the corresponding number of Control Points actually available.\n\nThere is not a way to declare that a sparse set of the Control Points is read. E.g. a shader that needs read Input Control Points [0],[3], [11] and [15] would just declare the Input Control Point (vicp) register’s [vertex] axis size as 16. Note that if references to the Control Points from shader code use static indexing, it will be obvious to drivers exactly what subset of Control Points is actually needed by the program anyway.\n\nAny individual HS Fork Phase program can be declared to execute instanced, with a declaration identifying a fixed instance count from 1 to ( is the maximum number of scalar Patch Constant outputs). The HS Fork Phase program executes the declared number of times per patch, with each instance identified by its 32-bit UINT input register vForkInstanceID(23.8).\n\nNote that if the role of an instanced Fork Phase program is for each instance to produce a System Interpreted Value(4.4.5), say one of the edge TessFactors(10.10) for a quad patch per instance, the declarations for each of those outputs would identify the System Interpreted Value being produced, just like any other shader.\n\nThe HS Fork Phase can input PrimitiveID(8.17) in its own register just like the HS Control Point Phase. The value in this register is the same as what the HS Control Point Phase sees. The other special input register in the HS Fork Phase is vForkInstanceID(23.8), described previously.\n\nThe system doesn’t go out of its way to automatically provide other System Generated Values(4.4.4) (VertexID(8.16), InstanceID(8.18)) to the HS Fork Phase. Values like these are part of the Input Control Points (if they were declared to be there) already, so the HS Fork phase can read VertexID/InstanceID by reading them out of the Input Control Points.\n\nThe treatment of InstanceID(8.18) does seem strange, in that InstanceID would be the same for all Control Points in a Patch (indeed, unchanging across multiple patches), yet it shows up per-Input Control Point. However, this is consistent with the behavior elsewhere in the pipeline, where the first active stage that can input a System Generated Value (for InstanceID, that is the Vertex Shader) is responsible for passing the value down to the next stage via shader output (rather than the hardware feeding the value down to subsequent stages separately). For the Geometry Shader to see InstanceID, it also shows up in each input vertex there, just like it shows up in each Input Control Point in the Hull Shader.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 10.9.1 HS Join Phase Program\n\n 10.9.2 HS Join Phase Registers\n\n 10.9.3 HS Join Phase Declarations\n\n 10.9.4 Instancing of an HS Join Phase Program\n\n 10.9.5 System Generated Values in the HS Join Phase\n\n\n\nThere can be 0 or more Join Phase programs present in a Hull Shader. Each of them declares its own inputs, but they come from the same pool of input data – the Control Points as well as the Patch Constant outputs of the Fork Phase programs. Each Join Phase program declares its own outputs as well, but out of the same output register space as all Fork Phase and Join Phase programs, and the outputs can never overlap.\n\nThe following registers are visible in the hs_join_phase(22.3.26) model. Note there are three sets of input registers: vicp (Control Point Phase Input Control Points), vocp (Control Point Phase Output Control Points), and vpc (Patch Constants). vpc are the aggregate output of all the HS Fork Phase programs(s). The HS Join Phase output o# registers are in the same register space as the HS Fork Phase outputs.\n\nThe input resources (t#), samplers (s#), constant buffers (cb#) and immediate constant buffer (icb) below are all shared state with all other HS Phases. That is, from the API/DDI point of view, the Hull Shader has a single set of input resource state for all phases. This goes with the fact that from the API/DDI point of view, the Hull Shader is a single atomic shader; the phases within it are implementation details.\n\nNote the footnotes which provide a detailed discussion of output storage size calculations.\n\n(1) The HS Join Phase’s Input Control Point register (vicp) declarations must be any subset, along the [element] axis, of the HS Control Point input (pre-Control Point phase). Similarly the declarations for inputting the Output Control Points (vocp) must be any subset, along the [element] axis, of the HS Output Control Points (post-Control Point Phase).\n\nAlong the [vertex] axis, the number of control points to be read for each of the vicp and vocp must similarly be a subset of the HS Input Control Point count and HS Output Control Point count, respectively. For example, if the vertex axis of the vocp registers are declared with n vertices, that makes the Control Point Phase’s Output Control Points [0..n-1] available as read only input to the Join Phase.\n\n(2) The HS Fork and Join phase outputs are a shared set of -vector registers. The outputs of each Fork/Join phase program cannot overlap with each other. System Interpreted values such as TessFactors(10.10) come out of this space.\n\n(3) In addition to Control Point input, the HS Join phase also sees as input the Patch Constant data computed by the HS Fork Phase program(s). This shows up at the HS Fork phase as the vpc# registers. The HS Join Phase’s input vpc# registers share the same register space as the HS Fork Phase output o# registers. The declarations of the o# registers must not overlap with any HS Fork phase program o# output declaration; the HS Join Phase is adding to the aggregate Patch Constant data output for the Hull Shader.\n\nThe declarations for inputs, outputs, temp registers, resource etc. in an HS Join Phase program function the same was as HS Fork Phase declarations(10.8.3).\n\nAny individual HS Join Phase program can be declared to execute instanced, with a declaration identifying a fixed instance count from 1 to ( is the maximum number of scalar Patch Constant outputs). The HS Join Phase program executes the declared number of times per patch, with each instance identified by its 32-bit UINT input register vJoinInstanceID(23.9).\n\nNote that if the role of an instanced Join Phase program is for each instance to produce a System Interpreted Value(4.4.5), say one of the inside TessFactors(10.10) for a quad patch per instance, the declarations for each of those outputs would identify the System Interpreted Value being produced, just like any other shader.\n\nSystem Generated Values are dealt with the same(10.8.5) way in the HS Join Phase as the HS Fork Phase. Instead of vForkInstanceID(23.8), in the Join Phase the same thing is called vJoinInstanceID(23.9). PrimitiveID(8.17) is available a standalone input register.\n\nHull Shader(10) Fork and Join Phase code can declare up to 6 of their output scalars as System Interpreted Values that identify various Tessellation Factors, driving how much tessellation the fixed function Tessellator should perform. For example, on a Quad there are 4 TessFactors for the edges, as well as 2 for the inside. HLSL exposes alternative (helper) ways to generate the inside tessfactors automatically from the edge TessFactors, e.g. deriving them by min/max/avg on the edge values, and possibly scaling based on user-provided scale values. The hardware does not understand anything about this helper processing (it just appears as shader code)\n\nThe optional (from the HLSL author point of view) tessellation factor processing results in HLSL compiler autogenerated shader code in either or both of the Fork and Join Phases. This standard processing can involve cleaning up of values, handling of special low TessFactor cases to prevent popping, and rounding of the values depending on the tessellation mode.\n\nThe final Tessellation Factors after this processing go to the fixed function Tessellator hardware – TessFactors for each edge and explicit TessFactors for the patch inside (as opposed to TessFactorScale the user specifies).\n\nDownstream, Domain Shader(12) code may be interested in seeing all of the intermediate values generated during any optional TessFactor processing. For example, to be able to perform blending during Pow2 Partitioning tessellation, one might want to see the ratio between unrounded and rounded TessFactor values. To enable that, the auto-generated code in the Fork and/or Join Phases will output not only final TessFactor values for the tessellator, but also the intermediate values, so the Domain Shader can access them. There are at most 12 such additional values (in the case of a Quad Patch). Again, the hardware does not understand anything about these \"helper\" values, and they are not discussed in detail here.\n\nThe next sections describe just the TessFactors relevant to the hardware without discussing the various optional helper routines that HLSL provides to derive them.\n\nFurther information about how Tessellation Factors are interpreted is here(11.7.10).\n\nThe first component provides the TessFactor for the U==0 edge of the patch.\n\nThe second component provides the TessFactor for the V==0 edge of the patch.\n\nThe third component provides the TessFactor for the W==0 edge of the patch.\n\nThe above hardware/system interpreted values must be declared in the same component of 3 consecutive registers (since indexing is on that axis).\n\nThis determines how much to tessellate the inside of the tri patch.\n\nThe first component provides the TessFactor for the U==0 edge of the patch.\n\nThe second component provides the TessFactor for the V==0 edge of the patch.\n\nThe third component provides the TessFactor for the U==1 edge of the patch.\n\nThe fourth component provides the TessFactor for the V==1 edge of the patch.\n\nThe ordering of the edges is clockwise, starting from the U==0 edge (visualized as the \"left\" edge of the patch).\n\nThe above hardware/system interpreted values must be declared in the same component of 4 consecutive registers (since indexing is on that axis).\n\nThe first component determines how much to tessellate along the U direction of the inside of the patch.\n\nThe second component determines how much to tessellate along the V direction of the inside of the patch.\n\nThe first component destermines the line density (how many tessellated parallel lines to generate in the V direction over the patch area).\n\nThe second component determines the line detail (how finely tessellated each of the parallel lines is, in the U direction over the patch area).\n\nThe above hardware/system interpreted values must be declared in the same component of 2 consecutive registers (since indexing is on that axis).\n\nThe Hull Shader output Patch Constant data appears as 32 vec4 elements. The placement of the Final TessFactors are constrained as described in the previous sections – each grouping of TessFactors must appear in a specific order in the same component of consecutive registers/elements in the Patch Constant Data. E.g. For Quad Patches, the four Final Edge TessFactors in a fixed order make up one grouping, and the two Final Inside TessFactors in a fixed order make up another separate grouping.\n\nShader indexing of the Patch Constant data across the vec elements is restricted, due to the limitations of a particular hardware implementation, as follows:\n• Indexing ranges (declared via dcl_indexRange regMin, regMax) on Patch Constant registers cannot cross over the start or end of any group of hardware TessFactors.\n\nSince the Hull Shader has multiple phases, each of which can be instanced (e.g. multiple Control Points in the Control Point phase, or instanced Fork or Join Phases), the \"ret*\" (return(22.7.16) or conditional return(22.7.17)) shader instruction is defined to end only the current instance of the current phase. So a \"ret*\" in the Control Point Phase would only finish the current Control Point invocation without affecting the others or other phases. Or a \"ret*\" in a Fork or Join Phase program would only end that instance of that program without affecting other instances (if it is instanced) or other Fork/Join programs.\n\nThe HS State Declaration Phase can optionally include a fixed float32 MaxTessFactor(22.3.20) in the range { .0... .0}.\n\nThis MaxTessFactor declaration(22.3.20) is useful when application knows the maximum amount of tessellation it could possibly ask for through the TessFactor values will output from the Hull Shader. Communicating this knowledge to the device allows it to optionally take advantage and perform better scheduling of resources on the GPU.\n\nIf a MaxTessFactor is declared, it is enforced by HLSL autogenerated TessFactor clamping code as the last step in the calculation of all of the following hardware System Interpreted Values (whose meanings were described earlier):\n\nFor simplicity only a single MaxTessFactor value can be declared, and when it is present, it is applied to all the TessFactors listed above.\n\nThe device sees the MaxTessFactor declaration as a part of the Hull Shader. The knowledge of this declaration is what hardware can optionally take advantage of to optimize Tessellation performance for content going through that Hull Shader, versus an otherwise identical Hull Shader without the declaration.\n\nIf HLSL fails to enforce the MaxTessFactor when it is declared (by clamping the HS output TessFactors), and a TessFactor larger than MaxTessFactor arrives at the Tessellator, the Tessellator’s behavior is undefined. Hitting this undefined situation is a Microsoft HLSL compiler (or driver compiler) bug, not the fault of the shader author or hardware.\n\nNote that independent of this optional application-defined MaxTessFactor, the Tessellator always performs some additional basic clamping and rounding of Final TessFactors as appropriate for the situation, described later (5.5). Those manipulations guarantee the hardware behavior by limiting the range of inputs possible. The only exception to that well defined hardware interface is this MaxTessFactor declaration which must rely on HLSL to generate code to enforce it. The reason it is the responsibility of HLSL to enforce consistency in this one case is it was too late in the spec process to arrive at any consistent hardware definition here, either by defining what the hardware behavior is if MaxTessFactor was not enforced but then exceeded at runtime, or getting all hardware vendors enforce the same MaxTessFactor clamping in hardware.\n\nThe tessellation model processes a patch at a time, either a quad, tri or \"isoline\" domain, and does not embody any specific surface representation. It strictly generates domain locations that are fed to a programmable shader (Domain Shader(12)) that is responsible for generating positions and any ancillary shading information (texture coordinates, tangent frames, normals, etc.). The domain locations are water tight across a boundary if identical level of detail is used, otherwise the hardware plays no role in ensuring crack free surfaces. This specification does not cover any specific surface representation, or how to map representations to the given pipeline.\n\nSee the D3D pipeline(2) diagram to see how Tessellation (Hull Shader(10), Tessellator(11) and Domain Shader(12)) fits in.\n\nThe Input Assembler(8) has a new primitive topology called \"patch list\", which is accompanied by a vertex count per patch: [1.. ]. This is also described under Patch Topologies(8.11).\n\nAll existing IA behaviors work orthogonally with patches. i.e. indexing, instancing, DrawAuto etc.\n\nIncomplete patches are discarded – for example if the vertex count is 32 per patch, and a Draw call specifies 63 vertices, one 32 vertex patch will be produced, and the remaining 31 vertices will be discarded.\n\nHere are pointers to the stages involved in Tessellation, in the order of data flow:\n\nThis fixed function stage takes floating point TessFactor values as input and generates a tessellation of the domain. The domain can be tri, quad or isoLine (see next section for a definition of isoLines).\n• (1) A set of domain points - UV for isoLine or quad domains and UVW (barycentric) for a tri domain. Each of these domain points is input to its own Domain Shader invocation. Each Domain Shader invocation also all sees shared input of all the Hull Shader(10) output data.\n• (2) Topology connectivity - fed downstream past the Domain Shader(12). For tri and quad domains, the valid output topologies are points or triangles. For isoLines, the valid output topologies are points or lines.\n\nNote the domains are defined such that for isoLines and quads, the V direction is clockwise from the U direction. For tri domain, UVW are clockwise, in that order.\n\nAdjacency(8.15) information is not available when using the tessellator - only independent points, lines or triangles are generated. The order that points/lines/triangles and their vertices are produced must be invariant between similar tessellator invocations on the same device, but no explicit order is prescribed.\n\nThe isoLine domain is a specialized form of the quad domain. It is the only domain that can produce tessellated lines. For isoLines, the U direction over a quad domain is the direction tessellated lines are drawn (lines of constant V). There are two TessFactor(10.10.4) values:\n\nThe first is the line density, which is always rounded to integer and determines how many U-parallel tessellated line segments to generate across the V direction. The spacing of these line segments across V is uniform, starting at V=0. So if the line density is 1, a single tessellated line is generated from (U=0,V=0) to (U=1,V=0). If the line density is 2, the first tessellated line is generated from (0,0) to (1,0) and the second tessellated line is generated from (0,0.5)-(1,0.5). Notice that no line is ever generated at V=1.\n\nThe second TessFactor is the line detail, determining how much to tessellate each line of constant V.\n\nFor more concrete info on the tessellation pattern for isolnes see IsoLine Pattern Details(11.7.8).\n\nDetails of the point placement and connectivity described in words in this section.\n\nA more concrete description can be found in the reference fixed function tessellator code, entirely encapsulated in the following C++ files:\n\nThe inside of a triangle/quad patch is a tessellated triangle/square based on a specified InsideTessFactor(s). For a triangle, there is a single TessFactor(10.10.2) for the inside region of the patch. For a quadrilateral, there are 2 inside TessFactors(10.10.3).\n\nHLSL exposes helpers that can optionally derive inside TessFactors from the edge TessFactors (these amount to shader code, so the hardware doesn't need to know about them). For example in the case of a quad patch, the helpers have a couple of options for deriving inside TessFactors – 1-axis and 2-axis. In the 1-axis mode, the inside TessFactor reduction is applied on all 4 edges producing a single inside TessFactor. In the 2-axis mode, the reduction from 4 edge TessFactors is divided into two separate parts. The V==0 and V==1 edge TessFactors are reduced to a single TessFactor for the V direction of the interior. Similarly the U==0 and U==1 TessFactors are reduced to a single TessFactor for the U direction on the interior.\n\nThe boundaries of the patch transition between the inside TessFactor(s) and each per-edge TessFactor.\n\nThere are two basic flavors of fractional tessellation: either using an even number of segments (intervals) on an edge or an odd number. When using an even number of segments the coarsest an edge can be refined is to have two segments an edge, so it is impossible to model a level of detail with a single segment.\n\nFor integer partitioning, TessFactors are rounded to integer. The parity (even/odd) of each edge and inside TessFactor after rounding determines how that area is tessellated: an odd integer TessFactor matches odd fractional tessellation at the same TessFactor. Similarly, an even integer TessFactor matches even fractional tessellation at the same TessFactor.\n\nFor pow2 partitioning, TessFactors are rounded to a power of 2, and tessellation of pow2 TessFactors matches even fractional tessellation at the same TessFactor, but in addition the power of 2 mode can go down to 1 segment on any side (1 is a power of 2). From the hardware point of view there is no distinction between pow2 and integer - the hardware doesn't do the rounding of the TessFactors to pow2. That rounding is the responsibility of the HLSL compiler, given the shader being authored using the appropriate helper intrinsics in shader code (not discussed here).\n• Since tessellation is symmetric about edges, geometry is always introduced by splitting 2 existing mirrored points across an axis into 4\n• Number of segments along an axis always increments by 2 at a time, thus:\n• Starting at a minimum TessFactor of either 1 or 2 produces two flavors of fractional partitioning: ODD and EVEN...\n• Minimum TessFactor is ; for all TFs on a patch is like no tessellation (nice)\n• Next TFs that have uniform segment widths are 3, 5, 7, 9...\n• Between odd valued TFs, geometry is in transit.\n• When TF bumps above an odd integer, the number of segments is the next odd (3.1->5)\n• Next TFs that have uniform segment widths are 4, 6, 8...\n• For even or odd partitioning, vertex splits always at mirrored pair of inside vertices\n• Exception: fractional odd partitioning at Tessellation Factor 1 only has 2 vertices per side – the corners\n• No choice but to split the edges\n• Since vertices are symmetric about center of an edge, only need to consider one half\n• Let us define an \"epoch\" in the TessFactor number space as: Any TessFactor with a power of 2 number of vertices on one half of the domain (excluding corners)\n• Starting at an epoch, split each vertex at that epoch one at a time from outside to inside as the TF increases, until the next epoch arrives\n• This is dubbed the \"Ruler Function\"\n• Tri and Quad Domains are tiled with quads, each made of 2 triangles\n• Which way do the quad diagonals go?\n• Quads closer to one of the patch corners than the others have diagonal pointing towards that corner\n• If quad is between 2 corners, diagonal is the direction a counter-clockwise spiral radiating out from center of patch would cross the quad\n• Center of a quad patch is a Z: The diagonal of the Z is oriented such that the top edge of the patch is V==0 and the left edge of the patch is U==0.\n• This is a rotationally symmetric as possible\n• How can each edge and the inside of a patch all have different Tessellation Factors from each other?\n• Each side of the frame is a trapezoid\n• Using two inside TessFactors, one for each of U and V axes\n• Can also just set these inside TessFactors equally to get uniform interior tessellation\n• Each of the \"frame\" sides is a row of triangles stitching the TessFactor on the outside edge to the inside TF\n• A tri patch is just a picture+frame with only 3 sides\n• where one segment on each end of the inside is not needed since the neighboring picture frame is there - thus trapezoid shape\n• Each vertex on the edge with the larger TessFactor connects to its \"parent\" vertex on the edge with the smaller TF, given the \"ruler function\" split history\n• Plus a triangle at each end, making a trapezoid\n• The quads are triangulated using previous diagonal rules\n• Application can, for example, determine the inside TF based on a min/max/avg reduction similar to fractional tessellation\n• HW then rounds each edge TF and inside TF up to next integer\n• Rest of tessellation behavior same as fractional\n• Each edge and inside TF can independently be even or odd\n• Thus smaller jumps in vert/tri count vs fractional\n• Same as integer partitioning, except instead of rounding to integer, round to next power of 2\n• Application (or HLSL compiler) is responsible for rounding to pow2, not hardware. Hardware just treats pow2 mode exactl like integer mode.\n• Pow2 isn’t just a subset of the integer mode, when the inside TessFactor reduction is \"average\"\n• Handy to call this mode out on its own anyway\n• As TessFactors increase, once a point shows up on the domain, it stays there permanently\n• Determines how to finely tessellate a line, with same controls (fractional, integer etc) as an edge has in tri or quad domains\n• Determines how many parallel tessellated lines to draw\n• Always rounded up to next integer\n• Tessellated lines of constant V are drawn over a UV quad domain\n• Lines density TF == 2 means: Draw 2 tessellated lines, one having V==0, and the other having V== 0.5\n• A tessellated line is never drawn at V==1\n\nThe order that geometry is generated for a patch must be repeatable on a device, however no particular ordering of the geometry within a patch is prescribed. A strict requirement is that all geometry for a given patch flows down the pipeline before any geometry for subsequent patches.\n\nSuppose the rasterizer is the next active stage in the pipeline after tessellation, and there are vertex attributes that are declared in the Pixel Shader with constant interpolation. The leading vertex, used to provide the constant attribute for any individual line or triangle, can be any of the vertices in the line or triangle (albeit repeatable for a given patch and tessellator configuration on a device).\n\nWhen a patch topology is used, PrimitiveID(8.17) identifies which patch in the Draw*() call is being processed, starting from the Hull Shader onward. Even though tessellation may produce multiple points/lines/triangles, for a given patch, all of the primitives generated for it have the same PrimitiveID. As such, the freedom of point/line/triangle ordering within a patch is not visible to shader code. When a patch topology is used, the true \"primitive\" is the patch itself.\n\nThe TessFactor number space roughly corresponds to how many line segments there are on the corresponding edge. This isn’t a precise definition of the number of segments because different tessellation modes snap to different numbers of segments (i.e. integer versus fractional_even versus fractional_odd).\n\nFor integer partitioning, TessFactor range is [ ... ] (fractions rounded up).\n\nFor pow2 partitioning, TessFactor range is [1,2,4,8,16,32,64]. Anything outside or in between values in this set is rounded to the next entry in the set by HLSL code... so from the hardware point of view, pow2 partitioning technically isn't different from integer partitioning.\n\nFor fractional odd partitioning, TessFactor range is [ ... ]. Odd TessFactors produce uniform partitioning of the space. Other TessFactors in the range produce a segment count that is the next odd TessFactor higher, transitioning the point locations based on the distance between the nearest lower odd TessFactor and nearest greater odd TessFactor.\n\nFor fractional even tessellation, TessFactor range is [ ... ]. Even TessFactors produce uniform partitioning of the space. Other TessFactors in the range produce a segment count that is the next even TessFactor higher, transitioning the point locations based on the distance between the nearest lower even TessFactor and nearest greater even TessFactor.\n\nFor the IsoLine domain, the line detail TessFactor honors all the above modes. However the line density TessFactor always behaves as integer – [ ... ] (fractions rounded to next).\n\nThis particular clamp on TessFactors is discussed here(10.13), and is independent of the hardware clamps defined in the rest of this section.\n\nThe following describes the float32 patch edge TessFactor range that the hardware Tessellator must accept from the Hull Shader.\n\nFirst of all, if any edge TessFactor is <= 0 or NaN, the patch is culled.\n\nOtherwise, hardware must clamp each edge input TessFactor to the range specified below.\n\nFor IsoLines, the LineDensity Tessfactor (which is how many constant V iso-lines to draw) is clamped by the hardware to [ ... ] and rounded to the next integer.\n\nIn addition to patch edge TessFactors, hardware will be given inside TessFactors from the Hull Shader. There are two inside TessFactors for quad patches (U and V axes), and one inside TessFactor for tri patches.\n\nThese HS outputs may have been derived (optinally) from the edge TessFactors via some operation such as max or avg in Hull Shader code autogenerated by HLSL. This derivation may involve low TessFactor fixups to prevent popping as TessFactors transition through extreme cases. Such processing is just shader code, irrellevant to the hardware.\n\nFor the final inside TessFactors coming out of the Hull Shader, the following is pseudocode for the hardware validation hardware must do, effectively creating safe bounds on the complexity of cases the hardware tessellation algorithm has to handle.\n\nIf any of the edge TessFactors from the HS for a patch are <= 0 or NaN, the patch is culled. No Domain Shader invocations or anything later in the pipeline are produced for that patch.\n\nA discussion elsewhere about enabling and disabling(11.8) of tessellation discusses how patch culling interacts with tessellation disabled, but patches being streamed out to memory.\n\nA shared edge has to generate identical domain locations for crack free tessellation to be possible. Domain Shader authors are responsible for achieving this, given some guarantees from the hardware. First, hardware tessellation on any given edge must always produce a distribution of domain points symmetric about the edge based on the TessFactor for that edge alone. Second, the parameterization of each domain point (U/V for quad or U/V/W for tri) must produce “clean” values in the space [0.0 ... 1.0]. “Clean” means that given a domain point on one side of the edge, with the parameter for that edge (say it is U) in [0 ... 0.5], the mirrored domain point produced on the other side, call it U' in [0.5 ... 1.0] will have a complement satisfying (1-U') == U exactly.\n\nEven if a neighboring patch sharing an edge happens to produce a complementary parameterization (U moving in the other direction, and/or U/V swapped), both side’s parameterization for each shared edge domain point will be equivalent because they are clean.\n\nHaving clean parameterization means that DS authors can write domain point evaluation algorithms with a carefully constructed order of operations that is guaranteed to produce the same result even if the control points for the patch are traversed in reverse order and/or with the parameter space complemented.\n\nTessellator input float32 TessFactor values are immediately converted to fixed point. Note this is after float processing of TessFactors, such as Inside TessFactor derivation has been done by HLSL generated shader code in HS Patch Constant Fork or Join Phases. Once the final TessFactors have been converted to fixed point, all remaining tessellator arithmetic (computing domain locations), is performed using fixed point arithmetic with 16 bits of fraction. The last step in domain point coordinate calculation is to convert the coordinates back to float32 for input to the Domain Shader.\n\nThe fact that output U/V/W domain coordinates(23.10) have been quantized to 16 bit fixed point means there is a uniform spacing of representable values across the [0...1] range. This uniform spacing facilitates the symmetry and watertightness issues discussed above.\n\nDue to the fixed point arithmetic involved, it is possible for the tessellator to produce degenerate lines or triangles, where each vertex has identical domain coordinates. This will not be visible if the primitives are sent to the rasterizer, because they will be culled. However, if the Geometry Shader and/or Stream Output are enabled, the degenerate primitives will appear, and it is the application’s responsibility to be robust to this. For example, Geometry Shader code could check for and discard degenerates if that turns out to be the only way to avoid the algorithm being used from falling over on the degenerate input.\n\nIf the Tessellator’s output primitive is points (as opposed to triangles or lines), this scenario requires only unique points within a patch to be generated. The one exception is points that are on the threshold of merging, if TessFactors were to incrementally decrease, may appear in the system as duplicated points (with the same U/V coords) in an implementation dependent way.\n• Output Primitive (Topology)(22.3.15): {point | line | triangle_cw | triangle_ccw}\n\n // Point can be used with any domain (IsoLine,Tri,Quad)\n\n // Line is only valid with the IsoLine domain\n\n // Triangle (CW or CCW) are only valid with Tri or Quad domains.\n• MaxTessFactor(22.3.20): {1.. } // Clamp placed on all TessFactors coming out of the Hull Shader.\n\nThe presence of both a Hull Shader and Domain Shader enables tessellation. When a Hull Shader and Domain Shader are bound, the Input Assembler topology is required to be a patch type (otherwise behavior is undefined). If a Hull Shader is bound and no Domain Shader is bound, or vice versa, the behavior is undefined.\n\nPatches can be used at the Input Assembler without tessellation (no Hull Shader or Domain Shader), as long as the Geometry Shader and/or Stream Output are being used.\n\nWhen tessellation is disabled (no Hull Shader and no Domain Shader bound), patches arriving at the Geometry Shader cause the GS to be invoked once per patch. Each GS invocation sees all the Control Points of the patch as an array of input vertices.\n\nAllowing the GS to be invoked with patches allows it to effectively input non-traditional topologies (beyond points, lines, triangles). E.g. to invoke the GS with a cube as its input primitive, one could send 8 Control Point patches.\n\nThe GS does not support output of patches. The output of the GS remains one of: point list, line strips or triangle strips.\n\nSending un-tessellated patches to NULL GS + Stream Output is valid. This enables, for example, Control Points that have gone through the Vertex Shader to be streamed out for multi-pass or reuse scenarios. Note, however, it is not possible for Hull Shader outputs to be streamed out (or go into the GS) - the presence of the Hull Shader requires a simultaneous Domain Shader and enables Tessellation – both of which consumes Hull Shader output entirely.\n\nWhen un-tessellated patches arrive at Stream Output, each Control Point in the patch appears as a single vertex for Stream Output. This definition is similar to the way NULL GS + Stream Output behaves with traditional primitive topologies such as triangle lists. As with other primitive types, only complete patches get written out; if there is not enough room to store a complete patch, it is discarded.\n\nIt could have been defined that Control Points arriving at the rasterizer are interpreted as points and rasterized as such, but that would have required a RenderTarget-space projected \"position\" to be present in the control points, and the application would have to have wanted to draw them as points. This is an extremely unlikely scenario, not worth targeting. Therefore, if an un-Tessellated patch arrives at the Rasterizer, behavior is undefined and the debug runtime will call this out as an error.\n\nFor a Tessellation overview, see the Tessellator(11) section.\n\nInputs for this stage are the 2D or 3D domain location(23.10) generated by the tessellator(11) and all of the data generated by the Hull Shader(10). This latter data is visible to all domain points in a patch. In all other ways this shader is effectively analogous to a Vertex Shader(9).\n\nThe Domain Shader can see all the data output by both phases of the Hull Shader, as well as the domain location of a particular point. The Domain Shader is invoked for every domain location generated by the Tessellator.\n\nThe following registers are available in the ds_5_0 model.\n\n(1) The domain shader sees the Hull Shader outputs in 2 separate sets of registers. The vcp registers can see all of the Hull Shader’s output Control Points. The vpc registers can see all of the Hull Shader’s Patch Constant output data.\n\nSince code for Hull Shader Patch Constant Fork or Join Phases output TessFactors using names such as SV_TessFactor, the DS must match those declarations on the equivalent vpc input if it wishes to see those values.\n\nInstanceID(8.18) and VertexID(8.16) can be input as long as the Hull Shader output these values (per-Control Point).\n\nThe domain location is another System Generated Value, appearing in its own input register (vDomain(23.10)).\n\nThe final set of System Values are the various TessFactors produced by the Hull Shader, discussed elsewhere(10.10). These can be declared as input out of part of the input Patch Constant (vpc) registers.\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 13.1 Geometry Shader Instruction Set\n\n 13.2 Geometry Shader Invocation and Inputs\n\n 13.3 Geometry Shader Output\n\n 13.4 Geometry Shader Output Data\n\n 13.5 Geometry Shader Output Streams\n\n 13.6 Geometry Shader Output Limitations\n\n 13.7 Partially Completed Primitives\n\n 13.8 Maintaining Order of Operations Geometry Shader Code\n\n 13.9 Registers\n\n 13.10 Geometry Shader Input Register Layout\n\n \n\n\n\nWhen a Geometry Shader is active, it is invoked once for every primitive passed down or generated earlier in the Pipeline. Each invocation of the Geometry Shader sees as input the data for the invoking primitive, whether that is a single point, a single line, a single triangle, or the Control Points for a Patch (if a Patch arrives with Tessellation disabled). A triangle strip from earlier in the Pipeline would result in an invocation of the Geometry Shader for each individual triangle in the strip (as if the strip were expanded out into a triangle list). All the input data for each vertex in the individual primitive is available (i.e. 3 verts for triangle), plus adjacent vertex data if applicable/available. All vertex inputs/Element-layout/adjacency to be read must be declared, and this declaration must be compatible with the data being produced above in the Pipeline. Other inputs include textures, and also Primitive ID as a -bit scalar integer input .\n\nAn alternate method of invoking the Geometry Shader is via instancing. A GS Instancing declaration(22.3.7) specifies a (fixed) number of times for the GS to be invoked for each primitive. Each instance that executes is identified by a GS instance ID value [0...n-1], and the outputs of each GS instance are appended to the end of the outputs of the previous invocation (with an implicit cut of the topology between instances - see the description of cutting further below). The maximum instance count that can be declared is , but for a full explanation of constraints of GS instancing, see the description of the GS instancing declaration(22.3.7)\n\nThe GSInvocations Pipeline Statistics counter(20.4.7) reports the number of primitives input to the GS multiplied by the instance count per primitive. That is, each \"instance\" counts as a GSInvocation.\n\nThe Geometry Shader outputs data one vertex at a time using the \"emit\"(22.8.3) command. The topology of these vertices is determined by a fixed declaration(22.3.8), choosing one of: pointlist, linestrip, or trianglestrip as the output for the GS. Strips can be restarted by using the \"cut\"(22.8.1) command, which ends the current strip at the last emitted vertex, so that the next emitted vertex begins a new strip. The \"emitThenCut\"(22.8.5) instruction both emits a vertex, and stops the current strip on this vertex, so that the next vertex that is emitted begins a new strip. For pointlist output, \"cut\" has no effect (including the \"cut\" part of \"emitThenCut\").\n\nThe outputs of a given invocation of the Geometry Shader are independent of other invocations (though ordering(4.2) is respected). A Geometry Shader emitting triangle strips will start a new strip on every invocation. In addition, as mentioned above, an invocation of the Geometry Shader can produce multiple separate strips using \"cut\"s.\n\nThe Geometry Shader must declare the maximum number of vertices an invocation of the Shader will output. The total amount of data that a Geometry Shader invocation can produce is -bit values. The calculation of the Stream Output record with one or more streams is as follows: Given that each stream declares its outputs in its own clean slate view of the full output register set, the total output record size is the number of scalars in the union of all the stream declarations. This size multiplied by the max output vertex count must not exceed . When Geometry Shader instancing is used, the Stream Output record size restriction applies to each instance individually\n\nThe limit on Geometry Shader output is based on how many \"emit\" calls the Shader makes. The limit on Geometry Shader output is not affected in any way by the size of the output buffer(s) that are present or whether or not they have even been bound. Even if no output Buffers happen to be bound to a Stream and a vertex is output (and therefore dropped), it still counts against the limit.\n\nHardware must enforce the limit above by stopping writes if the Geometry Shader program continues after emitting the declared maximum number of vertices.\n\nSee the documentation of the GS maximum output vertex count declaration(22.3.5), as well as the GS Instancing declaration(22.3.7) for more details.\n\nThe o# registers to be written by the Geometry Shader must be declared (e.g. \"dcl_output o[3].xyz\"). The set of these declarations defines which registers are read when an \"emit\"(22.8.3) command is issued, defining a vertex. Therefore, all vertices emitted by the Geometry Shader have the same data layout.\n\nWhen a Geometry Shader output is identified as a System Interpreted Value(4.4.5) (e.g. \"renderTargetArrayIndex\" or \"position\"), hardware looks at this data and performs some behavior dependent on the value, in addition to being able to pass the data itself to the next Shader stage for input. When such data output from the Geometry Shader has meaning to the hardware on a per-primitive basis (such as \"renderTargetArrayIndex\" or \"ViewportArrayIndex\"), rather than on a per-vertex basis (such as \"clipDistance\" or \"position\"), the per-primitive data is taken from the Leading Vertex(8.14) emitted for the primitive.\n\nEach time an \"emit\"(22.8.3) or \"emitThenCut\"(22.8.5) is issued the contents of the declared Geometry Shader output registers are read to produce a vertex, and in addition the Geometry Shader outputs immediately become uninitialized. In other words, if any output data needs to be repeated for consecutive vertices, the Geometry Shader program must write the data over again to the output registers for each vertex.\n\nThe Geometry Shader outputs have a close relationship to the Stream Output Stage/functionality, described here(14.3).\n\nSTREAM: For the discussion here, let us define a stream as a sequence of writes of a structure of data out of a shader. A Geometry Shader can output up to streams, each at different rates, with independent data going to each stream. The utility of this is in conjunction with Stream Output(14).\n\nBUFFER: For the discussion in this section, in the context of Stream Output(14), a Buffer is a resource in memory that can receive any subset of the data from one stream. A stream can have its data split out (not replicated) across multiple buffers, and this mapping is defined by a Stream Output declaration (which is not visible in the Geometry Shader code). A Buffer cannot receive data from multiple streams at once.\n\nstreams can be declared(22.3.9) by the GS. Without the GS present, all vertex data is a single stream.\n\nWhen the GS defines multiple streams, variants of the \"emit\"(22.8.3), cut(22.8.1) or \"emitThenCut\"(22.8.5) instructions which take an immediate stream # [0.. -1] parameter must be used by the GS to indicate which stream is being output. These instructions are \"emit_stream\"(22.8.4), cut_stream(22.8.2) and \"emitThenCut_stream\"(22.8.6), respectively.\n\nFrom the point of view of the Geometry Shader, all the declarations of its output registers appear multiple times indepdendently, once per stream. A statement appears in the bytecode setting the current output stream being declared, and subsequent declarations of output registers define what data gets latched when vertex data is emitted to each stream. The set of output registers available to the GS program during execution is the union of all output registers declared for each stream (individual streams can use the same output registers). When a vertex is emitted to a given stream, only the output registers declared for that stream feed the output to the stream, however ALL declared output registers for all streams become uninitialized.\n\nIf output register indexing is declared(22.3.30), specifying a range of output registers that can be dynamically indexed, the register space that can be declared for indexing is the union of all stream output register declarations.\n\nWhen outputting to multiple streams, the GS output topology declaration(22.3.8) must appear for each stream, and must bet set to \"point\". In other words, multiple streams means that non-point output is unavaliable.\n\nWhen outputting to only a single stream, the output from the GS can be a point list, line strip or triangle strip (strips are expanded to lists when streamed to memory), or a patch list. Output of a patch list from the GS is only valid for Stream Output, not for rasterization (undefined behavior).\n\nWhen outputting to multiple streams, one of them can be sent to the rasterizer (independently of whether it is also streaming to memory). The Stream Output declaration specifies this (outside the shader code, but appearing to the driver side by side). Interpolation modes, System Interpreted Values and System Generated Values can be declared on any combination of Streams in the Shader, but the only ones that have any meaning are the ones corresponding to the Stream (if any) declared (outside the shader) as going to the rasterizer (if any). For Streams that are not going to the Rasterizer, the names are ignored. Notice that the same shader could be created with different Stream Output declarations, each time selecting a different Stream to go to Rasterization.\n\nIf a GS with streams is passed to CreateGeometryShader at the API/DDI (meaning there is no Stream Output declaration or rasterizer stream selection), the active stream defaults to 0. So stream 0 goes to rasterization if rasterization is enabled, and the absence of a Stream Output declaration means nothing is streamed out to memory. If the stream selected to go to rasterization isn’t declared in the GS or doesn’t include a position and rasterization is enabled, behavior is undefined, just as with any shader that feeds the rasterizer without a position.\n\nInterpolation modes declared for the outputs on one Stream don’t have to match those on another Stream. Note that when the Geometry Shader is created, a choice of which stream (if any) is going to rasterization is made, so the driver shader compiler only needs to pay attention to interpolation modes and System Interpreted Values (such as \"position\") only on at most a single Stream’s declarations\n\nWhen the application knows that some GS outputs will be treated as per-primitive constant at the subsequent Pixel Shader, the Geometry Shader need only initialize such output registers when they represent the Leading Vertex(8.14) for a primitive. For example, on the last 2 vertices in a triangle strip, outputs that (on Leading vertices) would have be treated as constant by the Pixel Shader need not be written. If Stream Output is being used, which has no knowledge of what data is per-primitive constant or not, in the expansion of GS output strips to lists, Stream Output simply dumps out all the declared outputs for each vertex for each primitive. If the GS chooses to not write out what it knows is non-Leading-Vertex data for Elements that will be used to drive per-primitive constants in a later pass, uninitialized data gets written to these unwritten Elements in Stream Output. This is fine as long as the application never attempts to later read such uninitialized Stream Output data. If the application later recirculates the Streamed Out data in a way that correctly interprets only per-primitive constant data at Leading Vertices and never interprets the uninitialized data at non-Leading-Vertices (even though it does get read back into the pipeline), no undefined behavior results.\n\nThere is a mechanism to retrieve the number of output primitives in the output buffer. Further details regarding writing to memory from the Geometry Shader are described elsewhere in the spec.(14)\n\nPartially completed primitives could be generated by the the Geometry Shader if the Geometry Shader ends and the primitive is incomplete. Incomplete primitives are silently discarded and no counters are incremented. This is similar to the way the IA treats Partially Completed Primitives(8.13).\n\nTo ensure consistent order of operations on an edge and primitive level for primitives that show up in multiple invocations of the Geometry Shader (as an adjacent primitive in some invocations, or the root primitive for one invocation), it is up to the application to write Shader code that traverses vertices in a consistent manner. This ordering can be obtained by a variety of methods, including simply sorting of vertices based on position in Shader code. A more robust ordering can be achieved by providing a vertex \"coloring\" (a number) as vertex attribute, such that for any primitive, the coloring is guaranteed to be unique for each vertex in the primitive. This method has the benefit that the sorting operation in the Geometry Shader is more efficient (and robust) than sorting xyz vertex positions. Colorings can be generated offline by an authoring tool.\n\nThe following registers are available in the gs_ _ model:\n\nThe Geometry Shader must declare which type of primitive it expects as input, out of the set of choices: {point,line,triangle,line_adj,triangle_adj,1-32 control point patch list}. The input primitive type specifies the number of vertices that are present, and the vertices are always fully indexed (there is no declaration for vertex indexing range). Even if strips are being used earlier in the Pipeline, individual primitives cause Geometry Shader Invocations. See the GS Input Primitive Declaration Statement(22.3.6) in the instruction reference.\n\nThe following diagrams depict the layout of Geometry Shader Input Primitives into the input v# registers:\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 14.1 Mapping Streams to Buffers\n\n 14.2 Stream Output Buffer Declarations/Bindings\n\n 14.3 Stream Output Declaration Details\n\n 14.4 Current Stream Output Location\n\n 14.5 Tracking Amount of Data Streamed Out\n\n 14.6 Stream Output Buffer Bind Rules\n\n 14.7 Stream Output Is Orthogonal to Rasterization\n\n \n\n\n\nThe Pipeline can stream vertices out to memory just before clipping and rasterization (even if rasterization is still enabled). Vertices are always written out as complete primitives (e.g. 3 vertices at a time for triangles); incomplete primitives are never written out.\n\nJust before Streaming Out, all topologies are always expanded to lists (i.e. if the topology is a triangle strip, it is expanded to a triangle list, having 3 vertices per primitive).\n\nIf the Geometry Shader is active, it is capable of producing outputs with up to Elements per-vertex (each Element up to components) for the Rasterizer, any subset of which can be routed to Stream Output. The presence of the GS allows multiple streams to be generated as well, as described here(13.5).\n\nIf the Geometry Shader is not active, whatever data arrives at the point in the pipeline where Stream Output appears (just before clipping and rasterization) can be Streamed Out (after expansion to a list topology as described above). Topologies with adjacency discard the \"adjacent\" vertices and only Stream Out the \"interior\" vertices. Patch topologies arriving at Stream Output can only go to Stream Output; the rasterizer must be disabled (undefined behavior otherwise).\n\nA description of the distinction between a Stream and Buffer is given here(13.5). Up to Streams can be present when the GS is used, otherwise there is a single Stream, Stream 0.\n\nStream Output can send data from any Stream to up to Buffers simultaneously. The total number of output Buffers across all Streams is also constrained to . Data from multiple Streams cannot go to a single Buffer, but each Stream can send its output to multiple Buffers. Stream data cannot be replicated across multiple buffers.\n\nUp to scalar components of data per-vertex can be streamed out across the output Buffers, as long as the total window of data being output per-vertex to any one Buffer is bytes or less. Vertex stride to a given Buffer can be up to bytes.\n\nThe mapping of data from Streams to where they are written in output Buffers appears in a declaration outlined further below.\n\nIn all cases, the only supported output data formats at Stream Output are -bit per component integer and floating point formats, with 1 to 4 components. This is not as general as the other Resource input/output paths in the D3D . Pipeline. See the \"Stream Output\" column in the formats(19.1) table to see which formats can be used for Stream Output (all of which can of course be used at other parts of the D3D . Pipeline for input). When any given -bit component of data in the Pipeline goes out the Stream Output path and gets written to memory, the hardware must simply dump out the bits (per component) of data out unaltered, which is consistent with the sorts of formats supported for Stream Output described here.\n\nThe selection of which Elements to send to the Stream Output is tied to the Geometry Shader. When a Geometry Shader program is \"Created\" on the D3D . Device, additional parameters can be passed into the \"Create\" call alongside the Geometry Shader code, describing both (a) what subset of data from the GS output to send to Stream Output for each of 1 to Streams, (b) where to write the data to memory, (c) selection of 0 or 1 of the output Streams as going to the Rasterizer (indepdendent of it is going to Stream Output as well). If the Geometry Shader is not needed, but Stream Output functionality is desired, a \"NULL\" GS program can be specified, along with a Stream Output declaration for Stream 0 only, in which case whatever geometry reaches the GS stage of the pipeline gets Streamed out\n\nThe vertices in one Stream reaching the point in the pipeline just before the Rasterizer/clipping can be sent both to the Rasterizer (if the Pixel Shader is active) as well as to Stream Output if it is active, simultaneously. The Pixel Shader can consume any subset of the data reaching it, while Stream Output can simultaneously select any other (possibly overlapping) subset of the data.\n\nThe CreateGeometryShaderWithStreamOutput() DDI is defined roughly as follows (exact details will vary; IHVs should defer to the reference codebase). The API differs in a few ways from this DDI, such as hiding the concept of \"registers\" and \"masks\" appearing below, instead using string names for elements in a shader output signature, and component counts / offets to identify data within elements.\n\nIn order to use Stream Output, the application must:\n• Create a GeometryShader+StreamOutput \"object\" per above (this can be done ahead of drawing). Then, at Draw-time:\n• Set the GeometryShader+StreamOutput object onto the GS stage of the pipeline.\n• Bind 1 to Buffers for Stream Output. These correspond to each \"outputSlot\" parameter in the Stream Output declaration entries above. If the developer wishes to mix and match combinations of Buffers being assigned or not, this requires separate GS+SO objects to be created, with appropriate declarations. The same Buffer cannot be bound at muliple output slots simultaneously.\n\nBuffers used for Stream Output need to have a way to keep track of how full they are, in order to support the append ability and potentially to be able to invoke DrawAuto(8.9) without the CPU knowing how full the Buffer is at that time. See the Stream Output Pipeline Bind Flag for Buffers(5.3.4). This value is referred to as the BufferFilledSize. When the Buffer is newly created, the BufferFilledSize must equal 0.\n\nIn addition to structure definition (or type declaration for single Element Buffer) there is a mechanism for defining the starting offset into the Buffers where Shader outputs will start to be written. This offset is equivalent/equal to the BufferFilledSize associated with each Stream Output Buffer, since defining the starting offset also redefines the BufferFilledSize value. The next Draw() calls will begin streaming output data to the Buffer, starting at the offset, effectively appending data to the Buffer and accumulating the BufferFilledSize value associated with the Buffer. Subsequent Draw() calls continue to append to the location after the previous Draw() call finished. This is as if the starting offset were implicitly moved forward at the end of each Draw() call. The starting offset can also simply be reset to any location in the Buffer, overriding the implicit advancement after Draw() calls, and redefining the BufferFilledSize. When setting the Stream Output Buffer and starting Buffer offset, a reserved value for the starting Buffer offser (Ex. -1) is used to indicate to use the BufferFilledSize of the Buffer as the starting Buffer offset. This will allow a Stream Output Buffer to be appended to even if the Buffer is unbound from the Pipeline and bound back again later. So, these two call patterns would be identical:\n\nIn order to monitor how much data the Pipeline has streamed out, there are a some asynchronous queries: SO_STATISTICS(20.4.9) and SO_OVERFLOW_PREDICATE(20.4.10)s. In short, SO_STATISTICS provides a mechanism to retrieve values from two hardware counters for each Stream:\n\n (a) UINT64 NumPrimitivesWritten = the number of primitives written to a Stream\n\n (b) UINT64 PrimitiveStorageNeeded = the total number of primitives that would have been written given sufficient storage for the Buffer(s) in a Stream.\n\n Since the raw values of hardware counters are typically never useful, the popular usage of these counters is that they will be sampled twice and then subtracted from each other. The NumPrimitivesWritten difference and PrimitiveStorageNeeded difference will not be equal if the Draw() call(s), which were invoked between the two hardware counter sample points, generate more primitives than there is space left in the smallest of the currently bound Buffer(s) to store them. Note there is only one NumPrimitivesWritten counter per Stream even though it is possible to have multiple simultaneous Buffers bound for writing by a Stream. Stream Output is defined to stop all writes to a Stream if one of the Buffers being written by that stream does not have room for another complete primitive.\n\nThe hardware always writes as many complete primitives (e.g. 3 vertices for a triangle) as possible to the Buffer(s) for a Stream; a given primitive is written only if there is enough space for its entire contents (e.g. 3 times the vertex stride for triangles must be available in the Buffer) in all the output Buffers for the Stream. If any Buffer for a Stream becomes full before the Draw() call has completed (i.e. no more space for a complete primitive to be appended), Shader execution continues, along with sustained incrementing of the PrimitiveStorageNeeded counter for that Stream, but not the NumPrimitivesWritten counter for that Stream. In addition, the Shader's outputs are no longer written to any of the output Buffers for that Stream. Output to other Streams functions independently.\n\nAn application can detect the overflow condition with the SO_OVERFLOW_PREDICATE(20.4.10). In particular, there are + 1 predicates, one for each Stream, and an additional predicate that indicates if any of the Streams has overflowed. These predicates can be used to mask future graphics commands to, for example, prevent a corrupted frame from being shown to the application. This could be useful when streaming unpredictable mounts of data out from the Geometry Shader.\n\nIf multiple Buffers are being written by a given Stream, as soon as one of the Buffers can no longer hold any more complete primitives, writes to ALL Buffers for that Stream are stopped, while as mentioned above, Shader execution continues, and the PrimitiveStorageNeeded counter continues to tally for that Stream. Other Streams operate independently.\n\nIf an output buffer slot (0..3) has data streamed out to it (as indicated by the stream output declaration), but no buffer is attached, then that output buffer slot is treated as if a full buffer is attached, resulting in the overflow behavior described here(14.5).\n\nIf an output buffer slot does not have data being streamed out to it, and a buffer is attached, then that buffer is fully ignored, including having no impact on overflow and output tracking.\n\nThe path through Rasterizer output is always available, even if Stream Output is active. When the Stream Output declaration is provided (created), the application must have indicated one of the output Streams as being enabled for Rasterization. This is covered in the DDI here(14.3).\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 15.1 Rasterizer State\n\n 15.2 Disabling Rasterization\n\n 15.3 Always Active: Clipping, Perspective Divide, Viewport Scale\n\n 15.4 Clipping\n\n 15.5 Perspective divide\n\n 15.6 Viewport\n\n 15.7 Scissor Test\n\n 15.8 Viewport and Scissor Controls\n\n 15.9 Viewport/Scissor State\n\n 15.10 Depth Bias\n\n 15.11 Cull State\n\n 15.12 IsFrontFace\n\n 15.13 Fill Modes\n\n 15.14 State Interaction With Point/Line/Triangle Rasterization Behavior\n\n 15.15 Per-Primitive RenderTarget Array Slice Selection\n\n 15.16 Rasterizer Precision\n\n 15.17 Conservative Rasterization\n\n 15.18 Axis-Aligned Quad Rasterization\n\n \n\n\n\nAn Rasterizer overview is here(2.8). Many fundamental basics of Rasterizer operation are also provided in the Basics(3) section.\n\nVertices (x,y,z,w), coming to the Rasterizer, are assumed to be in homogenous clip-space. In this coordinate space the X axis points right, Y points up and Z points away from camera.\n\nThe meanings of the states are either self explanatory, or described further below.\n\nRasterizer state is encapsulated in a object, which once created can not be edited. Up to such objects can be created on a given device context.\n\nRasterization is disabled when the following are all true:\n\nThere is NO facility in D3D11 for disabling clipping of X and Y coordinates, the viewport scale, or the perspective divide if the rasterizer is enabled. Clipping of the Z coordinates can be disabled by setting the DepthClipEnable Rasterizer State(15.1) to FALSE.\n\nIn clip space primitives are clipped to the following volume:\n\n0 < w\n\n -w <= x <= w (or arbitrarily wider range if implementation uses a guard band to reduce clipping burden)\n\n -w <= y <= w (or arbitrarily wider range if implementation uses a guard band to reduce clipping burden)\n\n 0 <= z <= w\n\n\n\nBy default primitives are clipped to a volume that includes a 0 <= z <= w depth range clip. Clipping of the Z coordinates can be disabled by setting the DepthClipEnable Rasterizer State(15.1) to FALSE. Primitives that fall outside of the depth range are thus still rendered, but are given the value of the nearest limit of the viewport depth range. Even when Z clipping is disabled, primitives must be clipped such that only w > 0 vertices result. Coordinates coming in to clipping with infinities at x,y,z may or may not result in a discarded primitive. Coordinates with NaN at x,y,z or w coming out of clipping are discarded.\n\nThere are no restrictions to the range of input vertex coordinates to clipping. Clipping operations are performed using at least float32 precision, and accordingly NaNs and infinities are processed using the floating point rules.\n\nTwo additional mechanisms for slicing geometry against application defined planes are provided, similar to each other in programming method but different in behavior:\n\n(b) A method for culling primitives if all vertices are on the \"out\" side of of a plane.\n\nThese mechanisms, dubbed \"Clip Distances\" and \"Cull Distances\" respectively, are described below.\n\nTo enable primitive setup / rasterizer to perform clipping against arbitrary planes defined by the application, vertex component(s) can be identified as the System Interpreted Value(4.4.5) \"clipDistance\". When component(s) of vertex Element(s) are identified this way, these values are each assumed to be a float32 signed distance to a plane. Primitive setup only invokes rasterization on pixels for which the interpolated plane distance(s) are >= 0.\n\nMultiple clip planes can be implemented simultaneously, by declaring multiple component(s) of one or more vertex elements as the System Interpreted Value \"clipDistance\".\n\nWhen multisampling, implementations MUST clip against clip distances at subsample resolution.\n\nIf a vertex has a clip distance of NaN, the primitives containing that vertex are discarded.\n\nFor further information about \"clipDistance\", see its listing(24.1) in the System Interpreted Values reference.\n\nTo enable rough primitive-level culling against arbitrary planes defined by the application, vertex component(s) can be identified as System Interpreted Value(4.4.5) \"cullDistance\". When component(s) of vertex Element(s) are given this label, these values are each assumed to be a float32 signed distance to a plane. Primitives will be completely discarded if the plane distance(s) for all of the vertices in the primitive are are < 0. Said another way, if any of the plane distance(s) (data labeled as the System Interpreted Value \"cullDistance\") in a primitive is >= 0, the primitive is not culled (though other culling such as backface culling could still occur and is orthogonal).\n\nMultiple cull planes can be used simultaneously, by declaring multiple component(s) of one or more vertex elements as the System Interpreted Value \"cullDistance\".\n\nIf a vertex has a cull distance of NaN, that vertex counts as \"out\" (as if it is < 0).\n\nFor further information about \"cullDistance\", see its listing(24.2) in the System Interpreted Values reference.\n\nAt most components in at most vertex elements may be defined as System Interpreted Values \"clipDistance\" or \"cullDistance\".\n\nFor a given primitive with one or multiple components labeled as System Interpreted Value \"cullDistance\", the rejection test (primitive rejected if all distances < 0) is applied using all vertices for each cullDistance component, and if the primitive is rejected by any one or more of the tests it is discarded.\n\nAfter cullDistance processing is complete, for remaining primitives going into rasterization setup, if there are one or multiple components labeled as System Interpreted Value \"clipDistance\", any region(s) of a primitive that result in one or more of the clipDistances being < 0 after interpolation are not rasterized.\n\nInside the Pixel Shader it is valid to declare input Element(s) labeled as System Interpreted Values \"clipDistance\" and \"cullDistance\", in which case the appropriately interpolated clip distances or cull distances show up, as expected.\n\nThe interpolation mode declared(22.3.10) by the Pixel Shader on any input v# register labeled as System Interpreted Value \"clipDistance\" must be D3DINTERPOLATION_LINEAR. No such limitation exists for input v# registers labeled as System Interpreted Value \"cullDistance\"; these can be interpolated any way into the Pixel Shader.\n\nNote that clip/cull distances have no effect on GS stream output if it is active. The clip/cull can be thought of as appearing after the stream output in the Pipeline.\n\nAfter clipping, position X,Y,Z coordinates and non-constant vertex attributes with interpolation mode linear (meaning with perspective), are divided by the position W value.\n\nViewports map clip-space vertex positions into RenderTarget space. In the RenderTarget space Y axes points down, so the Y coordinates are flipped during the viewport scale. Multiple Viewports can be made available simultaneously, so that primitives can choose their one (see Viewport Index(15.8.1)), however the basic case is to simply use a single Viewport for all rendering in a particular scene. Only one Viewport can ever apply to an individual primitive being rasterized.\n\nViewport extents are specified as int32 values (except Z extents which are float32). Operations using all of the extents are done with float32 arithmetic (int32 extents converted to float32).\n\nThere is always an implicit scissoring by the Viewport x/y extents, orthogonal to other Scissor(15.7) state. In other words, regardless of whether or not an implementation has a guard band in its clipper or not, rendering will never touch any area outside the Viewport's x/y extents (except a small nondeterministic region that appears if the viewport left and top extents have fractional coordinates, discussed in the Viewport Range(15.6.1) section).\n\nIf a Viewport has not been set, then the default is a Viewport with all extents 0: { , , , , , }. When RenderTargets change, there is no automatic update of the Viewport.\n\nViewport scale is performed using float32 arithmetic according to the following formulas:\n\nAn additional effect of the Viewport is that in the Output Merger, just before the final rounding of z to depth-buffer format before depth compare, the z value is always clamped: z = min(Viewport.MaxDepth,max(Viewport.MinDepth,z)), in compliance with D3D11 Floating Point Rules(3.1) for min and max. This clamping occurs regardless of where z came from: out of interpolation, or from z output by the Pixel Shader (replacing the interpolated value). Z input to the Pixel Shader is not clamped (since the clamp described here occurs after the Pixel Shader).\n\nD3D11 may need to expose a 'cap' bit indicating whether an implementation clamps shader z input or not.\n\nViewport MinDepth and MaxDepth must both be in the range [ ... ], and MinDepth must be less-than or equal-to MaxDepth.\n\nThe Rasterizer must support(15.16) fixed-point x,y positions after Viewport scale with . precision (approximately [ … ] range). As such D3D11 defines the following constraints on the float Viewport Width, Height, TopLeftX and TopLeftY parameters:\n\nViewport parameters are validated in the runtime such that values outside these ranges will never be passed to the DDI.\n\nIn D3D10/D3D10.1, the Viewport extents at the API were integer, but they were changed to floating point to enable fractional scrolling of viewports and to enable emulating the D3D9 coordinate system easily by using 0.5 offsets on the viewport extents.\n\nThe runtime validates the parameters to be in valid range, skipping the call if there is an error (the DDI will never see invalid parameters).\n\nThe behavior of the implicit scissor to the viewport with fractional viewport extents is described in the Scissor(15.7) section (basically rounding X and Y to negative infinity to get integers).\n\nObserve that when the viewport location is fractional, which results in rounding to determine the implicit scissor, there is effectively a non-deterministic zone of up to 1/2 pixel wide along the left and top edges within the scissor area, not covered by the viewport. Because it is optional for implementations to perform guard-band clipping to viewport extents, and even if they do, implementations of it could vary, this means that rendering results in the non-deterministic zone will be some undefined combination of background values and primitives that may or may not have been clipped off the zone.\n\nScissor cuts out a rectangle in RenderTarget space where pixels are permitted to appear. Any pixel outside these extents is discarded. Multiple Scissor rectangles can be active simultaneously, from which individual primitives can choose one (see Selecting Viewport/Scissor(15.8.1) below). Only one scissor rectangle can ever apply to an individual primitive being rasterized, though this does not count the implied scissoring that is always applied to the Viewport(15.6)'s x/y extents.\n\nScissor extents are specified in unsigned integer, with no limits on the magnitudes of the extents. If the Scissor rectangle falls off the currently set RenderTargets, then simply nothing will get drawn. If the Scissor rectangle is larger than the currently set RenderTarget(s) or straddles an edge, then the only pixels that can be drawn are the ones in the covered area of the RenderTarget(s). The Scissor can be enabled or disabled (all Scissors together) using the Rasterizer State(15.1) ScissorEnable. If disabled, any pixel on the RenderTarget(s) can be drawn to. The default Scissor Rectangle is an empty Scissor Rectangle: { , , , }.\n\nThe implicit scissor to the viewport (mentioned in the Viewport(15.6) section) rounds the viewport X and Y extents to negative infinity. This way the scissor extents are always integers. The rounding to derive scissor extents applies to the locations where the fractional left/right/top/bottom edges would be after the float viewport transform. E.g. the viewport width and height cannot be rounded; they must be added to unrounded TopLeftX and TopLeftY to determine the right and bottom extents, which then get rounded to determine the scissor extents.\n\nThere is a set of Viewports and Scissor rects that can be set active via the API/DDI. By default, the -th Viewport and Scissor settings are used during rasterization setup. But Viewports can be selected on a per-primitive basis from the Geometry Shader by naming a component of GS output vertex data \"ViewportArrayIndex\"(24.5). \"ViewportArrayIndex\", taken from the Leading Vertex(8.14) for a primitive, is interpreted as a -bit unsigned integer value, with meaningful values in the range [0 and n-1] (where n is the maximum number of viewports allowed). Values outside [0..n-1] are treated as for indexing viewports. Should the Pixel Shader input \"ViewportArrayIndex\", whatever value \"ViewportArrayIndex\" was given shows up unmodified/unclamped in the Shader (even if out of [0..n-1] range).\n\nIf the Geometry Shader is not used, the default -th Viewport and Scissor settings are used.\n\nBiasing is constant for a given primitive, with the same value added to the z for each vertex before interpolator setup.\n\nThe biasing formulas are performed with float32 arithmetic.\n\nDepth Bias is not applied to any point or line primitives, except for lines drawn in wireframe mode as described in the Fill Modes(15.13) section.\n\nDepth Bias is disabled by setting both DepthBias and SlopeScaledDepthBias to zero, in which case the depth value is unmodified. Note that this disables propagation of IEEE specials that may be generated if the operation is performed even with zero DepthBias and SlopeScaledDepthBias values.\n\nThe Rasterizer State(15.1) FrontCounterClockwise governs whether clockwise primitives are considered front- or back- facing, and the Rasterizer State(15.1) CullMode chooses which primitives to cull, front, back or none. Culling of primitives is done after they are snapped(15.16) to fixed point during rasterization.\n\nThe rasterizer can generate a scalar value that is constant per-primitive which represents the whether the primitive being rasterized is front or back facing. The Rasterizer State(15.1) FrontCounterClockwise governs whether clockwise primitives are considered front- or back- facing. For front-facing primitives, IsFrontFace has the ( -bit unsigned integer) value , and for backfacing primitives, IsFrontFace has the value . For lines and points, IsFrontFace has the value . The exception is lines drawn out of triangles (wireframe mode(15.13)), which sets IsFrontFace the same way as rasterizing the triangle in solid mode.\n\nIsFrontFace can be input by the Pixel Shader by declaring a scalar component of one of its inputs as the System Generated Value(4.4.4) \"IsFrontFace\".\n\nThe mere presence of IsFrontFace in the Pixel Shader's input declarations activates the feature (there is no other control outside the shader).\n\nSee the general discussion of System Generated Values(4.4.4) for more information, the reference for IsFrontFace here(23.5), and the System Interpreted/Generated Value input(22.3.11) declaration for Shaders.\n\nTriangles can be rasterized in one of two modes selected by the Rasterizer State(15.1) FillMode from the following:\n\nIn solid mode, triangles are rasterized using the triangle rasterization rules in the D3D11 spec.\n\nIn wireframe mode, triangles are drawn using a line for each clipped original triangle edge reaching the rasterizer, but drawing nothing for new edges introduced by the clipper. If Depth Bias(15.10) is being performed, it is calculated once for each post-clip triangle (as in SOLID mode), added to each vertex to be drawn as a line for the surviving clipped edges of the original triangle. The lines are drawn using line rasterization rules for whatever line mode is currently set, be it aliased lines, antialiased lines, or multisample antialiased lines. Wireframe rendering of triangle strips is no different than drawing each triangle independently in wireframe mode.\n\nThe IsFrontFace input to the Pixel Shader is set the same way for triangles drawn in wireframe mode as it is for triangles drawn in solid mode (unlike normal lines, which set IsFrontFace to ). This is also discussed in the IsFrontFace(15.12) section.\n\nOnly triangles reaching the rasterizer are affected by fill mode; line and point primitives reaching the rasterizer are not affected at all.\n\nThe discussion in this section highlights some minor changes about the point/line/triangle rasterization behavior from D3D10.0\n\nThe key change to rasterization behavior is that the MultisampleEnable Rasterizer State(3.5.2) now only affects how line rasterization behaves. Points or triangles are always rasterized as if MultisampleEnable is true. The name MultisampleEnable is now misleading since it only affects lines, but the name remains unchanged. (Not changing the name in D3D10.1 was to minimize API churn, but again not fixing it in D3D11 was just an oversight). Because a dedicated enum for choosing the line mode was not added, it means the MultisampleEnable state is still needed to help choose amongst various line algorithms (same behavior as in D3D10.0), but other than that, it no longer has any of the other meanings it had in D3D10.0.\n\nThere are some existing multisample rasterization behaviors that were cut to support this change in D3D10.1, details discussed below. Cutting features like this without an easy emulation path is certainly an unusual event for DirectX, but the hope is these ones are rarely used, particularly given they are corner cases within a historically optional feature (Multisampling). Unfortunately any D3D9 and D3D10.0 applications that do depend on the behaviors cut from D3D10.1+ will not be able to be trivially ported.\n\nThe effect of the MultisampleEnable and AntialiasedLineEnable renderstates on choice of line algorithm is unchanged from D3D10. What is different is that in D3D10.1+ these states are now only used for this purpose, nothing else.\n\nIn particular, lines have 3 different rasterization methods available, as shown below:\n\nRegardless of what the MSAA sample count is, when the MultisampleEnable state is false, the Pixel Shader executes based on non-MSAA rasterization rules for aliased or alpha-based AA lines. This means that when the line covers a pixel, given these sample-pattern-agnostic line algorithms, all of the MSAA samples in the pixel are hit. Furthermore, for alpha-based AA lines all samples receive an identical coverage alpha value. If, however, the Pixel Shader requests Sample-Frequency(3.5.4.1) operation when MultisampleEnable is false, line rasterization behavior is defined only in the trivial case when sample count is 1, and left undefined for sample count > 1.\n\nOn the other hand with MultisampleEnable true, a shader requesting Sample-Frequency(3.5.4.1) execution will encounter well defined line rasterization behavior for any sample count. With MultisampleEnable true, the coverage rules for lines are equivalent to 2 triangles making a rectangle. Also, the way attribute evaluation works for MSAA lines is that attributes can vary along the length, but are constant across the perpendicular. So for example given MultisampleEnable is true, if a line with sample-frequency interpolated attributes covers multiple samples in a pixel, each Pixel Shader invocation within the pixel sees independently evaluated attributes.\n\nThe point rendering behavior from D3D10.0 is changed - so now, the MultisampleEnable state from the API/DDI is ignored and the hardware assumes it is true.\n\nNote that in D3D10.0 when MultisampleEnable is true, the coverage rules for a point are like drawing a unit area square out of 2 triangles, and attributes are all constant over the area. For D3D10.1+, this behavior holds regardless of what the API/DDI MultisampleEnable state is. Furthermore, these rasterization and attribute evaluation behaviors continue to apply during sample-frequency evaluation, except that each shader invocation is uniquely aware of its sample position (and sample index) if the shader requests it.\n\nThe rendering behavior for triangles is changed from D3D10.0 - so now, the MultisampleEnable state from the API/DDI is ignored and the hardware assumes it is true.\n\nWhen a Texture1D/2D Array, Texture3D, or TextureCube is set as the RenderTarget in the Pipeline (or multiple of these via MRT rendering), it is possible to select which array slice is being rendered to on a per-primitive basis from the Geometry Shader. If the Leading Vertex(8.14) for a primitive reaching the rasterizer from the Geometry Shader has a scalar component of its data labeled as System Interpreted Value \"renderTargetArrayIndex\"(24.4), then the rasterizer will use this -bit unsigned integer to select which surface to render to from the Pixel Shader for that primitive. This is useful with a RenderTarget that is a Texture(1D/2D/3D) with an Array size > 1, or a TextureCube (Array size of 6).\n\nIf the System Interpreted Value \"renderTargetArrayIndex\" is not used, the default array index rendered to is . If the Geometry Shader is not active, \"renderTargetArrayIndex\" cannot be changed from .\n\nThe range supported for renderTargetArrayIndex must be enough to accommodate the maximum resource array size(21). If the value written to \"renderTargetArrayIndex\" is out of range of the particular resource array that is set as a RenderTarget, the -th RenderTarget is used. If the renderTargetArrayIndex value is input to the Pixel Shader, it arrives unmodified, not incorporating any clamping that occurred in selecting which of the available Array slices as the RenderTarget.\n\nFor further information about \"renderTargetArrayIndex\", see its listing(24.4) in the System Interpreted Values reference.\n\nAfter Viewport(15.6) scale has been applied (but before Scissor Test), positions are converted to fixed-point, to evenly distribute precision across the RenderTarget range and to enable face culling. The Rasterizer(15) must support . (integer.fraction) fixed point precision for x and y. Particularly for the fractional part, the requirement is EXACTLY bits. This conversion is also subject to the rules specified in float-to-fixed(3.2.4.1), including round-to-nearest.\n\nAfter the Scissor Test(15.7) has been applied, the number of pixels along a given RenderTarget axis (x or y) that must be addressable starting from a base location is at least 2 .\n\nThe number of slices along the Array axis of a RenderTarget that must be addressable starting from a base is at least 2\n\nDuring Texture filtering, a sample location in the filter must be able to resolve sub-texels with at least -bits of fractional precision (2 subdivisions). This includes the precision along the LOD axis in mipmap selection.\n\nAfter Clipping, Perspective Divide and Viewport Scale have occured, if the float32 x, y or z has the value NaN, the primitive is discarded. No validation of w is done.\n\nIf x,y,z and w components of vertex position going into the Clip/Perspective Divide/Viewport Scale are all within the range [- , ], which is [-D3D11_FLOAT32_MAX/16,D3D11_FLOAT32_MAX/16], then Clip/Divide/Scale must never generate NaN or +-INF in position components, though +-INF can be handled by the rasterizer cleanly (x/y clamped to the furthest representable position extent in the hardware).\n\nAttribute Interpolators follow the Floating Point Rules(3.1), including propagation of NaN and handling of +/-INF. Interpolator setup is done based on vertex positions that have already been converted (snapped) to whatever fixed-point(15.16) representation is supported by the Rasterizer (this is also stated in the Coordinate Snapping(3.4.1) section). This does mean that attributes are slightly moved, but avoids extrapolating attributes off the intended \"gamut\" of the primitive that would happen if interpolators were set up before snapping positions for rasterization. Other than that, the input Z must exactly match the fixed function interpolated Z (they are one and the same).\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 16.1 Pixel Shader Instruction Set\n\n 16.2 Pixel Shader Invocation\n\n 16.3 Pixel Shader Inputs\n\n 16.4 Rasterizer / Pixel Shader Attribute Interpolation Modes\n\n 16.5 Pull Model Attribute Evaluation\n\n 16.6 Pixel Shader Output\n\n 16.7 Registers\n\n 16.8 Interaction of Varying Flow Control With Screen Derivatives\n\n 16.9 Output Writes\n\n 16.10 Pixel Shader Unordered Accesses\n\n 16.11 UAV Only Rendering\n\n 16.12 Pixel Shader Execution Control: Force Early/Late Depth/Stencil Test\n\n 16.13 Pixel Shader Discarded Pixels and Helper Pixels\n\n \n\n\n\nFor each primitive entering the rasterizer, the Pixel Shader is invoked once for each pixel covered by the primitive (pixel-frequency), or once per sample (sample-frequency). Sample-frequency execution is chosen if the Pixel Shader declares any input as needing sample-frequency evaluation (described in more detail later).\n\nIn either pixel- or sample-frequency execution, note the minimum atom size for shader execution is actually 2x2 blocks of shaders, to support derivative calculations via x/y deltas between shader invocations. This means there may be dummy invocations off the edge of a primitive to fill out the minimum 2x2 size.\n\nIn pixel-frequency operation, even though the Pixel Shader is invoked once per covered pixel, the depth/stencil tests occur for each covered sample, and samples that pass the tests are each blended to RenderTargets with the replicated Pixel Shader output color(s). In contrast, for sample-frequency execution, since the Pixel Shader is run once for each covered sample, there is a unique set of Pixel Shader outputs to go with the unique depth/stencil operation for each sample - this is pure \"supersampling\".\n\nIn either execution frequency, early depth/stencil culling may be performed by hardware, preventing the need to run the Pixel Shader in cases where the outputs would be guaranteed to be discarded anyway.\n\nThe Pixel Shader inputs -bit* -component vectors (v# registers), each of which is interpolated from the vertex attributes of the primitive being rasterized, based on the interpolation mode(16.4) declared in the Pixel Shader (subject to some restrictions on the mode described in the next paragraph). If the primitive gets clipped before rasterization, the interpolation mode is honored during the clipping process as well.\n\nA per-primitive value that can be declared(22.3.11) for hardware to initialize in an input register component is the IsFrontFace(15.12) value, generated by the Rasterizer.\n\nA per-sample value that can be declared(22.3.11) for hardware to initialize in an input register component is the sampleIndex(23.6), generated by the Rasterizer. Requesting this input is one of thw ways to force the Pixel Shader into sample-frequency execution.\n\nA per-pixel value that can be declared(22.3.11) for hardware to initialize in an input register component is Input Coverage(16.3.2), which indicates which samples in the pixel are covered by the primitive.\n\nOne of the input v# registers to the Pixel Shader can be declared with the name position(24.3), which means it will be initialized with the pixel's float32 xyzw position. Note that w is the reciprocal of the linearly interpolated 1/w value. The position location can be chosen by appropriate choice of interpolation mode(16.4): LINEAR_NOPERSPECTIVE yields the pixel center, in which the xy components will have a fraction of . LINEAR_NOPERSPECTIVE_CENTROID yields the pixel centroid(3.5.5) location. LINEAR_NOPERSPECTIVE_SAMPLE yields the sample location (and forces sample-frequency execution). Note: Separately the sampleopos(22.4.22) instruction can also be used to query the location of any given sample (including the current) within the pixel in terms of a delta from the pixel center, where the absolute location can be obtained by adding the delta to the pixel center position.\n\nFor all the interpolation modes listed above that are valid for position input, the z and w values of position input are interpolated at the corresponding xy coordinates.\n\nPixel Shader Input Z is not snapped to the precision of any depth buffer -> z and w input to the Pixel Shader are just interpolated floating point values. In other words, the contents of the input position register are properties of the current pixel in the primitive being rendered, and have nothing to do with what is in RenderTarget(s)/depth/stencil buffers.\n\nPixel Shader Input Z is required to NOT be clamped to [Viewport.MinDepth..Viewport.MaxDepth] range (also mentioned here(15.6)), and required to not reflect any quantization to depth format that is done before depth testing. Otherwise, Pixel Shader Input Z must exactly match the way fixed function Z interpolation is performed.\n\nHere is an example of the implications of this requirement: Suppose we have single sample RenderTarget(s), or multi sample RenderTargets under sample-frequency Pixel Shader execution. In this case, if a Pixel Shader inputs Z and writes it out unmodified, the resulting per-sample depth test and any update to the depth buffer must be identical to what would have happened if the shader did not input and output Z.\n\nThis does not mean that if a Pixel Shader reads a depth buffer generated with an identical rendering in a previous pass as an input Shader Resource View (SRV), the PS input Z will match the value read from the SRV given the same primitive and location. The reason is that the values in the depth SRV reflect quantization/clamping which has not been performed on the PS input Z. However, if the SRV format is float32, then it will exactly match the PS input Z except for clamping to [Viewport.MinDepth–Viewport.MaxDepth].\n\nThe Pixel Shader has a new input -bit scalar integer System Generated Value available: InputCoverage(23.4). This is a bitfield, where bit i from the LSB indicates (with 1) if the current primitive covers sample i in the current pixel on the RenderTarget.\n\nRegardless of whether the Pixel Shader is configured to be invoked at pixel frequency or sample frequency, the first n bits in InputCoverage from the LSB are used to indicate primitive coverage, given an n sample per pixel RenderTarget and/or Depth/Stencil buffer is bound at the Output Merger. The rest of the bits are 0.\n\nTo access InputCoverage, it must be declared as a single component out of one of the Pixel Shader input registers. The interpolation mode on the declaration must be constant (interpolation does not apply).\n\nThe InputCoverage bitfield is not affected by depth/stencil tests, but it is ANDed with the SampleMask Rasterizer state.\n\nIf no samples are covered, such as on helper pixels executed off the bounds of a primitive to fill out 2x2 pixel stamps, InputCoverage is 0.\n\nThese modes are selected via Pixel Shader input register declaration(22.3.10), on a per-Element basis. Should multiple declarations be present in the Pixel Shader for the different components of a given input register (perhaps for identifying System Interpreted Values(4.4.5) or System Generated Values(4.4.4) for some of the components, the interpolation modes for all components of the given register are required to be the same.\n\nNote that when an interpolation mode with no perspective correction is used, the clipper must account for this appropriately (different than how attributes that are to be interpolated with perspective correction would be handled). Also, attributes set with interpolation mode constant must pass through clipping and interpolation in the rasterizer completely unchanged from the value in the leading vertex (e.g. the bits in the attribute are untouched, with no type interpretation).\n\nInterpolation modes with sample in the name cause sample-frequency execution of the Pixel Shader.\n• Constant : data from the leading vertex(8.14) is provided to all pixels in the primitive.\n• LinearCentroid : same as linear, but centroid(3.5.5) clamped.\n• LinearNoPerspectiveCentroid: same as LinearNoPerspective, but centroid(3.5.5) clamped when multisampling.\n• LinearSample: same as linear, but evaluated at each covered sample.\n• LinearNoPerspectiveSample: same as LinearNoPerspective, but evaluated at each covered sample.\n\nAttributes evaluated without use of the intrinsics defined below will be evaluated according to the specification in the previous section.\n\nPull model attribute evaluation enables programmable interpolation of inputs in pixel shaders. This functionality allows the programmer to choose how an input is interpolated at runtime, to use multiple interpolation modes on the same input, and to change where the input is evaluated.\n\nThe programmer declares input attributes along with their interpolation mode (similar to earlier shader models). What is unique to pull model is that in the shader body, the programmer can call intrinsics to evaluate an input attribute at programmable locations.\n\nWhen using programmable locations for evaluation, the only aspect of the interpolation mode declaration that is honored is choice of constant/linear/linearNoPespective. On the other hand, location based modifiers on the attribute declaration, centroid or sample, are ignored during pull-model evaluation. Such modifiers have to do with where evaluation happens spatially, and in pull-model, spatial positioning comes from the instruction.\n\nIf attributes are referenced directly from a shader, all properties of the attribute declaration are honored – the type (constant/linear/linearNoPerspective) and any location modifiers – centroid or sample. This is the same as previous shader models.\n\nDue to a limitation in some hardware, position is the one attribute that cannot be \"pulled\". The intention is that this limitation will go away in future APIs.\n\nThe following new intrinsics are being added:\n\nThe index range declaration (dcl_indexRange(22.3.30)) that allows input registers to be indexed when referenced within shader code also applies to references to input registers by pull-model eval* operations.\n\nAll restrictions on the dcl_indexRange declaration are unaffected by pull model usage. One restriction in particular is that the interpolation mode on all elements in the range being declared is identical.\n\nFor index based addressing, if the sample index is out of the range of the number of samples per pixel in the RenderTarget, results for the pull model evaluation are undefined.\n\nFor offset based addressing, by definition no out of bounds index can be produced.\n\nConsider the mode where the address comes in as an offset. This mode allows full access to the grid (256 available sample locations), as opposed to the sample index mode, which only chooses from among the renderTarget sample locations.\n\nIn the offset mode, the offset is an integer tuple (U,V). This maps to grid coordinates in each axis span the integer range [-8–7], where 0 is the center. The left and top edges of a pixel are included, but the bottom and right edges are not.\n\nThe least significant 4 bits of each int pixelOffset coordinate are interpreted as fixed point numbers. The conversion from 4 bit fixed point to float is as follows (MSB–LSB), where the MSB is both a part of the fraction and determines the sign:\n\nAll other bits in the 32-bit integer U and V offset values are ignored.\n\nAs an example, an implementation can take this shader provided offset and obtain a full 32-bit fixed point value (28.4) spanning the valid range by performing:\n\nIf an implementation needed to map this to a floating point offset, that would simply be:\n\nIn practice, implementers will find shortcuts to the desired effect for their situation.\n\nThe Pixel Shader is capable of outputting up to -bit* -component elements of data, in addition to an optional -bit float scalar depth value for the depth test.\n\nThe following registers are available in the ps_ _ model:\n\nThe Pixel Shader instruction set includes several instructions that produce or use derivatives of quantities wrt screen space x and y. The most common use for derivatives is to compute LOD calculations for texture sampling, and in the case of anisotropic filtering, selecting samples along the axis of anisotropy. Implementations run the Pixel Shader on multiple pixels (in particular at least a 2x2 grid) simultaneously, so that derivatives of quantities computed in the Shader can be reasonably approximated as deltas of the values at the same point of execution in adjacent pixels.\n\nWhen flow control is present in a Shader and it is possible for different Shader invocations to take different paths, the result of a derivative calculation on registers inside a branch is ambiguous if these registers are updated in any of the branches.\n\nThe following restriction is in place to help applications avoid producing such ambiguous cases in shader code. Actually, the restriction is even tighter than minimally necessary to stop the scenario described above. The restriction is conservatively defined to permit some implementation flexibility for hardware.\n\nThe high level shading language compiler will attempt to emit warnings (but will not fail) when these restrictions are violated. Not all cases can be caught depending on how programs get compiled.\n\nA varying quantity in a Pixel Shader is a register which could have different values across different Pixel Shader invocations on a single primitive, at a common point in execution of the Pixel Shader.\n\nSpecifically, varying quantities are input registers which are interpolated (not defined as constant), or temporary registers (non-indexable r# or indexable x#[] arrays) whose contents are dependent directly or indirectly on interpolated input registers. Any instruction inside varying flow control (defined below) also produces a varying result.\n\nIn contrast, NON-varying quantity is an input register defined as constant, a literal/immediate value in the shader, or any quantity derived directly or indirectly from only other non-varying quantities in the shader. In general, any instruction not inside varying flow control, whose inputs are entirely non-varying produces a non-varying result. Examples: The results of constant/texture fetches with non-varying address are considered non-varying. If all writes to an x#[] (indexable temp register array) were non-varying, the x#[] (indexable temp array) is considered non-varying. If the index into a fetch from a non-varying x#[] is non-varying, the result is non-varying.\n\nIf a varying quantity is present as any condition(s) for a flow control construct, the entire contents of the flow control construct are considered to be within varying flow control.\n\nIf a varying flow control construct is nested inside another flow control construct, the fact that the nested construct is varying has no effect on whether or not the outer flow control construct is considered varying. The exception would be if the nested construct contains an instruction that could jump across scopes, as described next.\n\nThe presence of a retc(22.7.17) using a varying quantity as the condition or ret(22.7.16) inside a varying flow control construct means the rest of the code from the retc/ret to the end of the current scope (current subroutine or main program) is deemed to be withing varying flow control.\n\nThe presence of a break(22.7.8), breakc(22.7.9), ret(22.7.16), retc(22.7.17), continue(22.7.6), or continuec(22.7.7) instruction inside a loop(22.7.4) means the entire contents of loop is deemed to be within varying flow control.\n\nIn contrast, the presence of a discard(22.5.1) instruction anywhere in a program has NO effect on whether code following it is considered varying or not.\n\nA shader-computed temporary is any value that has been written to a register in a shader invocation that can be read again in the same invocation (i.e. r# or x#[] registers). Shader input or output registers are not included.\n\n(a) The following uses of sample or derivative instructions are not permitted inside varying flow control (though the HLSL compiler only attempts to warn about it):\n• sample(22.4.15), sample_b(22.4.16), or sample_c(22.4.19), or lod(22.5.6) when the texture address is a shader-computed temporary.\n• deriv_rtx_coarse(22.5.2), deriv_rty_coarse(22.5.3), deriv_rtx_fine(22.5.4), deriv_rty_fine(22.5.5), when the input is a shader-computed temporary.\n\n(b) Other uses of sample or derivative instructions have no restrictions with flow control. Examples are:\n• sample(22.4.15), sample_b(22.4.16), or sample_c(22.4.19), when the texture address is a shader input (regardless of interpolation mode) or statically indexed constant.\n• deriv_rtx_fine(22.5.4), deriv_rty_fine(22.5.5), deriv_rtx_coarse(22.5.2) and deriv_rty_coarse(22.5.3) when the input operand is a shader input (regardless of interpolation mode) or statically indexed constant, though the latter is not useful.\n• sample_l(22.4.18): here the application provides LOD as an operand, so no derivative calculation is required, and there is no issue with flow control.\n• sample_d(22.4.17): here the application provides derivatives as input operands, so there is no issue with flow control.\n• sample_c_lz(22.4.20), gather4(22.4.2), gather4_c(22.4.3), gather4_po(22.4.4), gather4_po_c(22.4.5): here the LOD is fixed, so no derivative calculation is required, and there is no issue with flow control.\n\nRegardless of the restriction above, shader authors still must ensure that before computing any derivative (or performing a texture sample that implicitly computes a derivative) where permitted, the register containing the source data must have been initialized for all execution paths beforehand. Initialization of temporary registers is not validated or enforced in general.\n\nThe component(s) of any output o# registers that a Shader intends to write must be declared(22.3.31) (statically) in each Pixel Shader, down to the component level. A distinct mask for each o# is permitted.\n\nIf a given o# register has no components declared for output then the RenderTarget at that output slot is not modified regardless of any other settings (such as write masks or blend modes).\n\nIf a given o# register IS declared for output, then all the declared components are assumed to be output from the Shader, however separate write-enable masks(17.15), per-RenderTarget, per-component, can be set outside the Shader at the Output Merger which ultimately decides which components get written to the RenderTarget (through the Output Merger blend if applicable). Therefore, hardware never needs to track during Shader execution which output registers/components are written, and can assume all declared ones are written, while relying on the masks defined outside the Shader to determine which portions of the RenderTarget(s) get updated.\n\nPartial writes to a given o# output register (writing a nonempty proper subset of the declared components) will produce undefined results in the unwritten component(s) that were declared for output. i.e. Declaring o0.rga but only writing o0.r means the RenderTarget location for o0.ga will be written with undefined values. However the application can take advantage of the write-enable masks to prevent undefined values from being written out and thus vary outputs with flow control in a Shader, as long as the condition doesn't vary within a given Draw*() (since the write-enable masks can only be updated between Draw*() calls).\n\nNote that o# registers may be written multiple times in a Shader; the defined output of the Shader is the contents of the declared o# register components at the end of Shader execution, only for o# registers that were actually written at all. Of course, if the Shader was \"discard\"(22.5.1)ed, that would mean there are no outputs.\n\nIt is permissible for a pixel shader to have no declared outputs - this case is not treated as a NULL pixel shader, especially because of the interaction with 'discard'. Only a NULL pixel shader prevents PSInvocations statistics from being incremented. If the pixel shader is NULL and DepthEnable and StencilEnable are both FALSE, rasterization is disabled and rasterizer-related counters, CInvocations and CPrimitives, will not update.\n\nIf a Shader intends to write to oDepth, it must be declared(22.3.37) statically in the Shader, just as o# registers. The Shader is then assumed to always write oDepth (replacing the interpolated depth value), and the oDepth value is always used in the depth comparison (if depth compare is enabled). Failure to write oDepth when declared results in undefined behavior (which may or may not include discard of the pixel/sample). This is consistent with the undefined behavior when not writing to declared o#. In pixel-frequency execution, the single oDepth output is replicated to all samples for their unique depth tests. In sample-frequency execution, each sample gets a Pixel Shader invocation, so oDepth can provide unique values per-sample.\n\nAny float32 value including +/-INF and NaN can be written to oDepth.\n\nConservative oDepth provides knowledge of the correspondence beween oDepth and the rasterizer generated depth in a pixel shader. This enables early depth culling and depth modification to be used together.\n\nIf a Shader intends to use conservative depth writes, it must be declared(22.3.38) statically in the Shader with parameters SV_DepthGreaterEqual(24.6) or SV_DepthLessEqual(24.7). If the shader chooses SV_DepthGreaterEqual or SV_DepthLessEqual, then a guarantee is made that the shader never writes smaller or larger values (respectively) than the rasterizer depth value by inserting instructions that either max or min the desired output depth value with the rasterizer depth. If the desired output value would be in violation of the defined conservative depth type, then the rasterizer depth is used.\n\nThe valid range is indentical to that for standard oDepth.\n\nIf the shader declares the depth output as SV_DepthGreaterEqual, then an extra max instruction is added to the end of the shader program.\n\nThis instruction enforces the guarantee that the output depth value of the pixel shader is greater than or equal to the rasterizer depth value. Now that the value is known to be equal to or behind the depth values defined by the primitive, then early depth cull can be enabled when the depth comparison mode is \"less\" or \"less or equal\".\n\nIf the shader declares the depth output as SV_DepthLessEqual, then an extra min instruction is added to the end of the shader program.\n\nThis instruction enforces the guarantee that the output depth value of the pixel shader is less than or equal to the rasterizer depth value. Now that the value is known to be equal to or in front of the depth values defined by the primitive, then early depth cull can be enabled when the depth comparison mode is \"greater\" or \"greater or equal\".\n\nUsing SV_DepthGreaterEqual and SV_DepthLessEqual is valid with any depth mode, but the early depth cull will be disabled if the knowledge of is GreaterEqual/LessEqual is not compatible with the early depth cull optimization. The min/max test against the rasterizer depth always occurs, but the benefits of the guarantee are only useful with the correct depth test mode.\n\n16.9.3.2 Rasterizer Depth Value Used in Clamp\n\nFor either clamp described above, RasterizerDepthValue is the centroid depth value if the shader is executing at pixel-frequency. It is enforced by the HLSL compiler that if the shader inputs depth and outputs one of the above clamped depth values, the input depth must be interpolated as linear_noperspective_centroid in pixel-frequency execution (if position is input at all). If the shader does not input position, for pixel-frequency execution the centroid depth is used for conservative depth clamping, and for sample-frequency execution the per-sample depth is used for per-sample conservative depth clamping.\n\nThe purpose for requiring centroid in pixel-frequency execution is that it guarantees the clamp is done against a safe depth value within the gamut of the covered samples, thus not violating any traditional depth optimizations. More ideal would have been to pick the min or max covered sample, depending on which conservative depth mode is chosen, but that would have been too costly to require hardware to compute for the benefit. It was deemed adequate to use an existing interpolation mode – centroid.\n\nThe shader can also ask for position to be interpolated with linear_noperspective_sample, but that makes the shader run at sample-frequency, so the situation is simpler given there is a depth per sample and thus a clamp per sample. Similary, if the shader is running at sample frequency for some other reason (such as inputting sample index), input depth can be interpolated in any valid way, unaffected by whether or not the shader is outputting conservative depth.\n\nThe Pixel Shader output register oMask receives from he shader an output coverage mask, behaves like the SampleMask Rasterizer state. The final coverage values are the result of ANDing the sample mask with the coverage mask, followed by the output coverage mask if one is written. Alpha to Coverage is disabled if this register is written in a shader.\n\nWhen the Pixel Shader runs at sample-frequency, the coverage mask is also ANDed with a mask that selects the sample currently being processed. As a result, sample N is always masked by bit N of oMask. This allows a shader to run at either sample-frequency or pixel-frequency with identical oMask behavior. The same rule applies to Alpha To Coverage when the shader runs at sample-frequency.\n\nIf a Shader intends to write to oMask, it must be declared(22.3.39) statically in the Shader, just as o# registers. The Shader is then assumed to always write oMask. Failure to write oMask means its contents are undefined as with any other output register (which may or may not cause random samples to disappear).\n\nIt is valid for a Pixel Shader to not have any outputs other than oMask, such as for a z-prepass. This is similar outputting nothing but using discard(22.5.1), except with per-sample control.\n\nD3D11 Pixel Shaders support all the memory read/write instructions that are available to the Compute Shader(18). That is, Pixel Shader invocations will be able to perform atomic read/write operations on random access memory via Unordered Access Views(5.3.9).\n\nThe same hardware that is designed for running Compute Shaders can execute shaders in the Graphics Pipeline (Vertex Shader, Pixel Shader etc). So features in the Compute Shader can be considered for the Graphics Pipeline.\n\nIn order not to break the clean and specialized semantics of the Graphics Pipeline, many features in the Compute Shader are NOT exposed (at least for this generation). Examples of features not considered for Graphics are the Compute Shader’s ability to share scratch memory between threads, and the ability for a thread to control the synchronization of a thread group.\n\nIn fact, only one feature from the Compute Shader is deemed interesting to expose in Graphics for now, and that is the ability to perform random Unordered Access (UA) on memory, both input and output, including atomic operations such as atomic compare and exchange or atomic increment. Note this is different from the Pixel Shader’s Output Merger (\"Blender\") which is able to perform atomic operations, but does not allow variable addressing from a given Shader thread. The word \"Unordered\" denotes the fact that with multiple Shader threads in flight free to perform random accesses to memory, no ordering is enforced, and if the program running wants to achieve determinism, it must make use of atomic operations as appropriate, or be careful to compute unique addresses for memory writes for each thread.\n\nIt happens that the number of output memory Buffers that can participate in UA from a Compute Shader is . This number is exactly the number of RenderTargets in the Graphics Pipeline, by design (common resource in the hardware). Given that the Pixel Shader is the place in the Graphics Pipeline where RenderTargets are already accessed via shaders, it is in the Pixel Shader that Compute Shader’s UA ability is being exposed.\n\nIf no DSV or RTVs are bound, only UAVs, the rasterizer needs a way of knowing what width/height and sample pattern to execute at. The size cannot come from the dimensions of the UAV, since in general, UAVs of different sizes and types (Buffer vs Texture2D) can be simultaneously bound.\n\nThe Viewport dimensions (rounded down to integers) determine the width/height that the rasterizer operates at and the Scissor determines which range of \"Pixels\" are available to cause Shader invocations. If Scissor is not enabled, the full Viewport is used. In addition to these bounds, however, there is always an implicit scissor to [0...D3D11_REQ_TEXTURE2D_U_OR_V_DIMENSION] in x and y. This limits the rendering span expected of the rasterizer to be the same as the RTV/DSV rendering scenario.\n\nThe rasterizer sampling pattern is single sample at pixel centers.\n\nTo improve the ability to achieve deterministic output for shaders performing write operations to Unordered Access Views(5.3.9) (UAVs), it is important for an application to be able to have predictable control over how many Pixel Shader invocations are invoked which are permitted to write to UAVs.\n\nWhen depth/stencil testing is being used, some hardware is able to pull the depth/stencil test before PS invocation when it knows the Pixel Shader is not going to affect the result of the depth/stencil test. This saves executing the depth/stencil test unnecessarily, without affecting functional behavior.\n\nIf a Pixel Shader has any UAVs declared for access, the decision about whether to run the PS or not based on depth/stencil must be under the control of the application.\n\nAs such, there are 2 modes the Pixel Shader can be declared to run in. One of the modes is selectable by passing a flag to the dcl_globalFlags(22.3.2) declaration in the shader bytecode. The other mode is implied by the absence of the flag. The following two sections describe each mode. Note that selection between these modes is available to Pixel Shaders independent of the use of UAVs by the shader.\n\nSpecifying the FORCE_EARLY_DEPTH_STENCIL flag in the dcl_globalFlags declaration for a Pixel Shader indicates that the implementation must perform Depth/Stencil tests and depth/stencil writes before executing the Pixel Shader.\n\nIf the tests do not pass, the Pixel Shader is not invoked unless it is a helper (see further below). If the Pixel Shader is in Sample-Frequency mode, the same applies based on per-sample Depth/Stencil tests.\n\nIf the tests pass, the Pixel Shader is invoked, and it may perform operations with external effects such as accessing UAVs (Unordered Access Views), outputting to RenderTargets, output Coverage etc. Attempts to write Depth and/or Stencil (the latter isn’t yet a feature) from the PS are simply ignored with no effect, since Depth/Stencil processing has already happened.\n\nThe D3D Occlusion Query(20.4.6) counts the number of MultiSamples which passed Depth and Stencil. In the FORCE_EARLY_DEPTH_STENCIL mode, a sample is counted for the query if it passes the Depth/Stencil tests that happen before the Pixel Shader invocation, and nothing downstream further impacts the count.\n\nThe absence of the FORCE_EARLY_DEPTH_STENCIL flag indicates Depth/Stencil testing must occur based on the final state of the Depth/Stencil values, appearing as if the tests occur after the Pixel Shader runs.\n\nImplementations may perform optimizations that maintain this behavior but which do not execute \"unnecessary\" Pixel Shader invocations. However, if, for instance, a Pixel Shader declares that it might output Depth, or it might access a UAV(5.3.9) (Unordered Access View), any optimizations the hardware may be capable of which seek to cull the Pixel Early by performing an early Depth/Stencil test must be disabled. This enables applications to rely on a deterministic set of Pixel Shader invocations which can perform actions that have external side effects, such as manipulating UAV memory.\n\nAs in the previous section, helper pixels/samples, which only exist to fill out 2x2 quanta for derivatives, have their access to UAVs ignored, and immediate atomics that return a value return 0.\n\nThe D3D Occlusion Query(20.4.6) counts the number of MultiSamples which passed Depth and Stencil and also were not masked in any other ways such as SampleMask, Pixel Shader Output Coverage, or discarding of the Pixel.\n\nThe Pixel Shader can discard(22.5.1) itself, which means RenderTarget updates will not happen, however any access to UAVs(5.3.9) from the shader before the discard is issued are \"in the past\" and proceed to completion. After the discard is issued, further operations on UAVs do not change the UAV memory, and if they return a value to the shader, the value returned is undefined.\n\nRegardless of whether executing in Sample-Frequency mode or not, sometimes helper Pixel Shader invocations need to exist to support derivatives in 2x2 stamps. If a Pixel Shader invocation only exists as a helper, and not because it passed Depth/Stencil, then any output from that shader invocation such as writes to RenderTargets, output Coverage Mask, atomic memory updates etc. are valid but ignored. Atomic operations on a UAV (Unordered Access View) that return a result to a helper shader invocation (\"immediate\" atomics) return and undefined value without changing the UAV memory. This matches the execution behavior after a Pixel has been discarded, described above.\n\nMemory fence operations need not be honored in helper and discarded pixels. Fences are further discussed in the definition of the sync(22.17.7) instruction, along with the more general discussion of the shader memory consistency model(7.14). A discard instruction itself, however, acts implicitly as a memory fence that prevents operations from being reordered before or after the discard.\n\nIt is invalid for any result dependent on an access to UAV memory to contribute to a derivative calculation in a Pixel Shader. This will be enforced to the extent possible by the HLSL compiler. This is a conservative restriction (until perhaps a better proposal comes along), but it is a safe and simple way to mitigate pollution of active pixel shader invocations through derivatives with undefined results returned when helper or discarded pixels access UAVs.\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 17.1 Blend State\n\n 17.2 D3D11_BLEND values valid for source and destination alpha\n\n 17.3 Interaction of Blend with Multiple RenderTargets\n\n 17.4 Gamma Correction\n\n 17.5 Blending Precision\n\n 17.6 Dual Source Color Blending\n\n 17.7 Logic Ops\n\n 17.8 Depth/Stencil State\n\n 17.9 DepthEnable and StencilEnable\n\n 17.10 Depth Clamp\n\n 17.11 Depth Comparison\n\n 17.12 Stencil\n\n 17.13 Read-Only Depth/Stencil\n\n 17.14 Multiple RenderTargets\n\n 17.15 Output Write Masks\n\n 17.16 Interaction of Depth/Stencil with MRT and TextureArrays\n\n 17.17 SampleMask\n\n 17.18 Alpha-to-Coverage\n\n \n\n\n\nAn introduction to this final stage in the D3D11 Pipeline is here(2.10).\n\nThe states governing Output Merger, listed in this section, are grouped into two cagegories, Blend State, and Depth/Stencil State. Most of the state within each of these categories is defined atomically on creation of a state object (with a couple of exceptions for states that are separated out because they are likely to change frequently). For Blend State, at most Blend Objects can be created per context. For Depth/Stencil State, at most objects can be created per context. Once created, a Blend State Object or Depth/Stencil State Object cannot be edited. When a Blend State Object and Depth/Stencil State Object are set active on the device (along with the other states that are separated out of the blend objects, shown below), the Output Merger on the hardware is then controlled by these objects when rendering.\n\nFixed-function blend can be enabled and configured independently for each RenderTarget.\n\nThe blender must be able to write in accordance with sRGB rules for formats which include _SRGB in the name, for example: R8G8B8A8_UNORM_SRGB.\n\nAll Output Merger math/blending operations with floating point RenderTarget(s) (regardless of format size) must honor the D3D11 Floating Point Rules(3.1), although operations are considered to be \"fused\", and reordering is permitted, outside of application control.\n\nNaN's and signed zeros must be propagated by blending hardware for all cases (including 0.0 blend weights).\n\nValues entering Blending Hardware, including the BlendFactor value, are always clamped to the range of the RenderTarget before being used in the Blend. Components not present in the format must be clamped to the minimum range of all the components that are present. e.g. with the format R8G8_UNORM, the components B,A entering the blending hardware get clamped to the same range as R,G, which would be [0..1].\n\nNote that this clamping must be done on a per-rendertarget basis, so if one render target is a float type and another is UNORM type, the shader values and blend factor must be float range for the float render target Blend, and clamped to 0..1 for the UNORM render target Blend.\n\nAn exception is float16, float11, float10 or R9G9B9E5 RenderTargets, where it is permitted for implementations to not clamp data going into the blend. So it is required that blend operations on these formats to be be done with at least equal precision/range as the output format but an implementation can choose to perform blending with precision/range (up to float32).\n\nWhen a RenderTarget is has a fixed point format, as stated above, implementations are required to clamp data going into Blending to the RenderTarget format range, however blending operations may be performed at equal or more (e.g up to float32) precision/range than the output format. For SNORM data, intermediate operations such as (1-x) are performed without clamping [-1..1], though input to and output from blending are still clamped.\n\nFor fixed point formats with components having fewer than 8 bits (e.g. DXGI_FORMAT_B5G6R5_UNORM introduced in D3D11.1), the allowance above that blending operations may be peformed at equal or more precision than the output format applies even if blending is disabled. That is, the hardware may or may not upconvert to some intermediate precision level, say 8 bit, even if blending is off, before converting down to the final output format precision (say 5 bit).\n\nFor all formats, there is a clamp to the RenderTarget range after blend, before writing values out to memory.\n\nThis feature enables Output Merger to use both the Pixel Shader outputs o0 and o1 simultaneously as input sources to a blending operation with the single RenderTarget at slot 0.\n\nAdditional options are available for the SrcBlend, DestBlend, SrcBlendAlpha or DestBlendAlpha terms in the Blend equation. The presence of any of the following choices in the Blend equation means that Dual Source Color Blending is enabled:\n\nWhen Dual Source Color Blending is enabled, the Pixel Shader must have only a single RenderTarget bound, at slot 0, and must output both o0 and o1. Writing to other outputs (o2, o3 etc.) produces undefined results for the corresponding RenderTargets, if bound illegally. Writing oDepth is valid when performing Dual Source Color Blending.\n\nThe only valid blend ops with Dual Source Color Blending are: add, subtract and revsubtract. Others are undefined.\n\nThe configured blend equation and the Output Write Mask(17.15) at slot 0 imply exactly which components from Pixel Shader outputs o0 and o1 must be present. If expected output components are not present, results are undefined. If extra components in o0 or o1 are output, they are ignored.\n\nThis feature enables bitwise logic operations between Pixel Shader output and RenderTarget contents.\n\n17.7.1 Where it is supported\n\nThis feature is required to be supported for Feature Level 11.1 hardware, and is optional for Feature Levels 10.0, 10.1 and 11.0 (exposed by drivers via the D3D11.1 DDI).\n\nLogic ops are supported only on renderable UINT formats. Implementations that expose Logic Ops support must support them for all renderable UINT formats.\n\nIdeally, the number of bits per component in the output format indicates how many bits from the corresponding Pixel Shader output component are used, starting from the LSB of the PS output (e.g. 8 bits per component from LSB used for R8G8B8A8). This will be required in a future D3D Feature Level.\n\nFor now, it is allowed for hardware to clamp the shader output as a UINT to the number of bits in the format, e.g. for an 8-bit output format component, the value 0x100 coming out of the shader turns into 0xff going into the Logic Op. Ideal hardware would just take the bottom 8 bits, 0x00.\n\nSo applications using Logic Op must zero out bits above the number of bits in the output format to guarantee consistent behavior across all hardware.\n\n17.7.2 How it is exposed\n\nLogic ops are configured by the LogicOpEnable and LogicOp members of D3D11_RENDER_TARGET_BLEND_DESC1 (see here)(17.1).\n\nFloat blending (i.e. not logic op) supports independent blend configuration per RenderTarget. At the API, logic ops will appear to be exposed in a way that has similar orthogonality, including the ability to use logic ops on some RTs and float blend on others.\n\nHowever, the hardware does not have this full flexibility.\n\nConfiguration of logic op is constrained in the following way:\n• (a) for logic ops to be used, IndependentBlendEnable must be set to false, so the logic op that has meaning comes from the first RT blend desc and applies to all RTs.\n• (b) when logic blending all RenderTargets bound must have a UINT format (undefined rendering otherwise).\n\nDepthEnable and StencilEnable are overall enable/disable controls for the depth and stencil processing portions of the output merger. When DepthEnable is false, depth test and depth buffer write are not performed, regardless of any other settings. When StencilEnable is false, the stencil test and stencil buffer write are not performed, regardless of any other settings. When DepthEnable is false but StencilEnable is true, the depth test is always pass when incorporated into the stencil operation.\n\nNote that DepthEnable is limited in scope to the output merger area - in particular it does not affect functionality such as clipping, depth bias, or clamping that occurs to depth prior to input to the pixel shader.\n\nDepth values that reach the Output Merger, whether coming from interpolation or from Pixel Shader output (replacing the interpolated z), are always clamped: z = min(Viewport.MaxDepth,max(Viewport.MinDepth,z)) following the D3D11 Floating Point Rules(3.1) for min/max. The MinDepth and MaxDepth values are defined by the Viewport(15.6).\n\nWhen the DepthEnable state is true and a Depth Buffer is bound at the Output Merger, the clamped(17.10) z value gets converted to the format/precision of the Depth Buffer (nop if the Depth Buffer format is float32), and is then compared using DepthFunc against the corresponding Depth Buffer value. The conversion of z to Depth Buffer precision uses round to nearest (+0.5 and truncate). If no Depth Buffer is bound, the depth test always passes.\n\nWhen the StencilEnable state is true and a Depth Bufer having Stencil bits is bound at the Output Merger, a long list of states are used to drive stencil testing (the ones with 'Stencil' in the name in D3D11_DEPTH_STENCIL_STATE). If there is no stencil component in the Depth Buffer format, or no DepthBbuffer bound, then the stencil test always passes. Other than that, functionality here is unchanged from the past, and doesn't need further documentation here.\n\nApplications can indicate to the system that a depth and/or stencil buffer bound at the Output Merger (OM) is read-only, via flags in the DepthStencilView (DSV).\n\nThe D3D11 Depth/Stencil View (DSV) description structure has a flags field, where the flags can be:\n\nIndependent of the DSV, there is the Depth/Stencil State(17.8) object that gets bound to the Output Merger.\n\nTo determine whether depth writing is enabled, D3D11 hardware must AND together the following two pieces of information (where a result of 0 means writes to depth must be forced off):\n\n(2) The D3D11_DSV_FLAG_READ_ONLY_DEPTH flag must NOT be set in the currently bound DSV.\n\nSimilarly, to determine if stencil writing is enabled, D3D11 hardware must AND together the following two pieces of information (where a result of 0 means writes to stencil must be forced off):\n\n(1) The Depth Stencil State Object has stencil writes enabled via any of the state shown above.\n\n(2) The D3D11_DSV_FLAG_READ_ONLY_STENCIL flag must NOT be set in the currently bound DSV.\n\nThis behavior allows hazard tracking on Shader Resource Views (SRVs) to only have to check the flags in the current DSV at bind-time for any DSV or SRV. There is a hazard if there are simultaneously bound SRV + DSV without the appropriate read-only flag, in which case the SRV needs to be unbound. Note that Depth Stencil State Objects have no impact hazard tracking at all.\n\nIt is required that the Pixel Shader be able to simultaneously render to at least separate RenderTargets. All of these RenderTargets must be the same type of resource: Buffer, Texture1D[Array], Texture2D[Array], Texture3D, or TextureCube. All RenderTargets must have the same size in all dimensions (width and height, and depth for 3D or array size for *Array types). If Multisample Antialiasing is being used, all bound RenderTargets and Depth Buffer must be the same form of Multisample Resources (i.e. the sample counts must be the same). Each RenderTarget may have a different data format; there are no requirements that the formats have identical bit-per-Element counts.\n\nAny combination of the slots for RenderTargets can have a RenderTarget set or not set.\n\nThe same resource view cannot be bound to multiple simultaneous RenderTarget slots simultaneously. Note that setting multiple non-overlapping resource views of a single resource as simultaneous multiple rendertargets is supported.\n\nThe Output Write Masks control on a per-RenderTarget, per-component level what data gets written to the RenderTarget(s) (assuming all other conditions passed, such as depth/stencil, and the pixel wasn't discarded). Failure to provide sufficient data to the Output Merger for all of the RenderTarget(s)/component(s) enabled with the write masks results in undefined values being written out. See the discussion of Output Writes(16.9) for further detail on the interaction between Pixel Shader outputs and the Output Write masks.\n\nNote that the Output Write Masks do not affect what data may get read from the RenderTarget(s) in the process of performing Blend operations specified in the Output Merger, depending on the operation specified. The masks simply limit writes.\n\n17.16 Interaction of Depth/Stencil with MRT and TextureArrays\n\nThere can only be one Depth/Stencil buffer active, regardless of how many RenderTargets are active. Should Resource Views of TextureArray(s) be set as RenderTarget(s), the Resource View of Depth/Stencil (if bound) must also be the same dimensions and array size. Note that this does not mean that the Resources, themselves, need to be of the same dimensions (including array size). Only that the Views that are used together must be of the same effective dimensions. See Resource Views(5.2) for a description of the View's effective dimensions and array size. Of course if Depth/Stencil is not being used, a Depth/Stencil buffer need not be bound.\n\nSampleMask is a -bit coverage mask applied to the Multisample coverage for a primitive to determine which samples get updated in all the active Rendertargets. There is only one coverage shared for all RenderTargets in Multisampling. SampleMask is always applied, regardless of whether Multisample rendertargets are bound or not. For n-sample rendering, the first n bits of MultisapleMask from the LSB are used to mask the coverage. n can be from 1 to , depending on the multisample mode used (out of the selection of modes offered by the individual hardware implementation). The mapping of bits in SampleMask to samples in a multisample RenderTarget is up to the individual implmentation to decide (as long as it is some 1:1 mapping). There is no direct mechanism for applications to query the mapping order (let alone for querying the spatial location of samples).\n\nThe Blend State(17.1) bool AlphaToCoverageEnable toggles whether the .a component of output register o0 from the Pixel Shader is converted to an n-step coverage mask (given an n-sample RenderTarget). This mask is ANDed with the usual sample coverage for the pixel in the primitive (in addition to SampleMask) to determine which samples get updated in all the active RenderTarget(s).\n\nIf the Pixel Shader outputs oMask (output coverage)(16.9.4), Alpha-to-Coverage is disabled.\n\nNote that there is only one coverage shared for all RenderTargets in Multisampling. The fact that .a from output o0 is read and converted to coverage when AlphaToCoverageEnable is true does not change the .a value going to the Blender at RenderTarget 0 (if a RenderTarget happens to be set there). In general, enabling Alpha-to-Coverage is completely orthogonal to how all color outputs from Pixel Shaders interact with RenderTarget(s) through Output Merger Stage, EXCEPT the addition that the coverage mask is ANDed with the Alpha-to-Coverage mask. Alpha-to-coverage works ortohogonally to whether the RenderTarget is blendable or not (or whether blending is being used on it).\n\nThere is no precise specification of exactly how Pixel Shader o0.a (alpha) gets converted to a coverage mask by the hardware, except that alpha of 0 (or less) must map to no coverage and alpha of 1 (or greater) must map to full coverage (before ANDing with actual primitive coverage). As alpha goes from 0 to 1, the resulting coverages should generally increase monotonically, however hardware may or may not perform area dithering to provide some better quantization of alpha values at the cost of spatial resolution and noise. An alpha value of NaN results in a no coverage (zero) mask.\n\nChapter Contents\n\n\n\n(back to top)\n\n\n\n 18.1 Compute Shader Instruction Set\n\n 18.2 Compute Shader Definition\n\n 18.3 Graphics Features Not Supported\n\n 18.4 Graphics Features Supported\n\n 18.5 Compute Features Added\n\n 18.6 Compute Shader Invocation\n\n 18.7 Compute Shaders + Raw and Structured Buffers on D3D10.x Hardware\n\n \n\n\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 18.2.1 Overview\n\n 18.2.2 Value Proposition and Business Rationale\n\n 18.2.3 Scenarios\n\n\n\nA compute shader is a separate logical shader type analogous to the current graphics shaders: the vertex, geometry, and pixel shaders. However, while it uses the same classes of input and output data, it is not directly connected to other shaders in the same pipeline during use. Its purpose is to enable more general processing operations than those enabled by the graphics shaders.\n\nSince many currently identified mass-market applications for compute shader involve presenting results at interactive rates. The additional overhead of transitioning back and forth to a separate graphics API (and associated software stack) would consume too much CPU compute overhead in these tightly coupled scenarios. Furthermore, adding a separate API presents a more difficult adoption problem and requires a more complex installation process. Therefore, the Compute Shader is integrated into Direct3D – accessible directly through the Direct3D device. The compute shader can directly share memory resources with graphics shaders through the Direct3D Device.\n\nA Compute Shader is provided as a separate shader from the graphics shaders to impose different policies and reduce the complexity of interactions with other pipeline state. Like other shaders, it has its own set of state.\n\nA compute shader does not necessarily have a forced 1-1 mapping to either input records (like a vertex shader does) or output records (like the pixel shader does).\n\nSome features of the graphics shaders are supported, but others have been removed in order to enable new compute-specific features to be added.\n\nThis section lists a set of functions typically supported by fixed-function hardware that are not expected to be needed for compute shader execution:\n\nThis section lists functions typically supported by fixed-function hardware units that may be interesting for implementations to have operational during execution of compute shaders:\n• Texture sampling (equivalent to what is available in all other shader stages except Pixel Shader)\n• Reduction operations on device memory locations are exposed as an atomic operation so that blend hardware can be used if the implementation prefers.\n\nThis section lists new features supported in the compute shader that are not supported by graphics shaders, aside from the Pixel Shader(16) in a few cases:\n• Ability to decouple thread invocations from input or output domains.\n• Random access writes (scatter operations) and atomic operations. (available to Pixel Shaders too)\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 18.6.1 Overview\n\n 18.6.2 Dispatch\n\n 18.6.3 Anatomy of a Compute Shader Dispatch Call\n\n 18.6.4 Input ID Values in Compute Shader\n\n 18.6.5 DispatchIndirect\n\n\n\nTo support more general usage and higher performance, a compute shader is not necessarily invoked once per input data value (as vertex shaders are), or invoked once per output value (as pixel shaders are). There is a new invocation method that specifies exactly the number of shader threads that will be dispatched to execute using that shader.\n\nThe API syntax for compute shader invocation is:\n\nThe invocation process dispatches the specified number of groups in the array. (The number of threads in each group is not specified in the Dispatch() call, but is specified in the shader to allow the compiler to optimize register pressure).\n\nThese arguments identify the x-, y-, and z- dimensions of the array of thread groups to be dispatched.\n\nIf any of the Dispatch arguments are 0, while the command will be sent to the driver, the effect is that nothing happens.\n\nThe upper bound on each dimension is . Larger values produce undefined behavior.\n\nSuppose a Compute Shader program has been compiled having thread group dimensions 10 x 8 x 3. The HLSL code would look roughly like this pseudocode:\n\nNote that as a convenience to the programmer, sets of threads in an invocation batch can be thought of as being organized into an array of 1-, 2-, or 3-dimensions (with the possibility of more in future releases).\n\nTo continue the above example, the shader could be invoked with the following parameters in the Dispatch call:\n\nThis launches a grid of 30 groups that is 5 groups wide by 3 groups high, by 2 group deep. Each group contains a block that is 10 threads wide by 8 threads high by 3 threads deep, as declared in the Compute Shader code.\n\nIn Direct3D11, Shader Model 5.0, there is an upper limit of for the X dimension, for the Y dimension and for the Z dimension of the thread group's thread counts in the Compute Shader declaration above. Further, the total number of threads in a thread group (X * Y * Z) must be less than or equal . Any shaders that declare numbers beyond these limits will fail compilation.\n\nA given thread is aware of where it fits in its thread group and in the overall grid of thread groups via a few input System Generated Values(4.4.4) analogous to the SV_PrimitiveID currently supported in graphics shaders.\n\nBelow is a visual depiction of the example of how the Compute Shader program and Dispatch call discussed above would manifest on hardware.\n\nThe following values are available as input to the Compute Shader to identify the current thread executing and where it is relative to all the other threads dispatched:\n\nEach component of each ID value is a -bit unsigned integer.\n\nA similar entry point is provided that takes the information about how many thread groups to dispatch from a Buffer on the GPU. When the command reaches the GPU for execution, at that time the paramters are read from the GPU Buffer. The point is that the parameters may have been written by some other GPU operation, possibly after the actual issuing of DispatchIndirect call from the CPU.\n\nIf any of the DispatchIndirect arguments are 0, the Dispatch does nothing.\n\nThe upper bound on each dimension is . Larger values produce undefined behavior.\n\nIf the address range in the Buffer where DispatchIndirect's parameters will be fetched from go out of bounds of the Buffer, or the starting offset is not 4-byte aligned, behavior is undefined.\n\nThe related calls for graphics are DrawInstancedIndirect(8.7) and DrawIndexedInstancedIndirect(8.8)\"\n\nWhile most parameters to Draw*InstancedIndirect/DispatchIndirect can be initialized via standard ways of writing data into Buffers, such as Copy* commands, or rendering, a special-purpose Copy command is needed in some scenarios.\n\nThese scenarios involve a variable amount of data that has been written to a Buffer, via Pixel Shader/Compute Shader Unordered Access Views with Append or Counter semantics. The resource receiving the data has hidden counters that track how much has been written. One might want to issue DrawInstancedIndirect/DispatchIndirect in such a way that all of the entries in a variable length array of structures written to a Buffer are fed back into the pipeline.\n\nTo accomplish this, a new API/DDI CopyStructureCount is introduced:\n\npDstResource is any Buffer resource that other Copy commands are able to write to, such as CopySubresourceRegion or CopyResource.\n\nDstAlignedByteOffset is the offset from the start of pDstResource to write 32-bit UINT structure (vertex) count from pSrcResource.\n\npSrcResource is an UnorderedAccessView of a Structured Buffer resource created with either D3D11_BUFFER_UAV_FLAG_APPEND or D3D11_BUFFER_UAV_FLAG_COUNTER specified when the UAV was created. These types of resources have hidden (implementation maintained) counters tracking \"how many\" records have been written.\n\nThe hardware tracks a single number with an unordered access view: a UINT32 count reflecting how many times a structure was written. The count value will be copied directly to pDstResource at DstAlignedByteOffset.\n\nWhen CopyStructureCount is used as a way to recirculate variable length arrays of structures back into the pipeline, the application must be aware that there is no indication of whether the Buffer holding the variable length data ran out of space. If the count is too high for the amount of space in the Buffer, it means that during initialization when the Buffer got full, subsequent writes were discarded, yet the counter continues going. The intent here is to efficiently enable scenarios where the application knows the worst case amount of data that could be written and allocates appropriately (or is otherwise somehow robust to having the last elements missing due to Buffer full). Calling Draw*Indirect with a vertex count that is too high behaves predicably – attempts to read past the end of a Buffer have well defined semantics (spec’d elsewhere).\n\nCurrent mass-market applications for GPUs (that are not 3-D shading) are substantially GPU memory i/o bound. This means that 50-80% of the available processing power in current GPUs cannot be brought to bear on these common problems. Adding support for sharing of small amounts of data between threads can reduce the effects of this i/o bottleneck, as it allows the shader to re-use data that was already brought into registers by a previous thread. This saves the i/o work involved and allows the full processing power of the GPU’s ALUs to operate, producing a potential 4-8x performance improvement for key scenarios.\n\nCurrent trends in silicon architecture will enable compute performance to grow faster than bandwidth performance. This will increase the ratio of compute performance to bandwidth performance significantly.\n\nThe hardware functionality required to address this in the DirectX11 shader model 5.0 compute shader is a predefined block of 32kB ( DWORDs) of register space that can be declared within a shader to be of storage class “groupShared”. Registers declared to be of this class can be shared between threads in the group.\n\nDue to contention issues it is not ideal for all threads in a given invocation ( Dispatch() call) to access the same set of shared registers. Therefore, a mechanism is defined to partition the threads into smaller groups that can all share access to a given 32kB set of shared register space. This partitioning mechanism is a regular division. The size of the group is specified in the HLSL as specified as specified in here(18.6.13). Any thread in the subset has read-write access to any register in the shared register space.\n\nThe compiler will validate at compile time that the total amount of shared variable space declared does not exceed the limit defined for the shader model.\n\nThere is a maximum limit to the number of threads in a thread group , ie that can be permitted to exchange information through a single set of shared register space. In DirectX 11 shader model 5.0 this limit is set at threads.\n\nThese shared registers are assumed to be a physically separate from, and in addition to the pool of general purpose/temp registers, but should have similar performance characteristics (access times).\n\nThe compiler will validate usage patterns of shared memory. See here(18.6.11) for details.\n\nValues stored in this shared memory are not preserved across/between shader invocations, nor between thread groups. They must be initialized by the shader before use, and any results to be persisted must be written out to video memory.\n\n18.6.7 Synchronization of All Threads in a Group\n\nAn explicit execution barrier intrinsic is added to compute shader HLSL to identify a barrier point. All threads within a single thread group (those that can share access to a common set of shared register space) will all be executed up to the point where they reach this barrier before any of them can continue beyond it. For example:\n\nThis barrier will be present in the Intermediate Language emitted by the Microsoft shader compiler. There will be cases where it is inserted by the compiler without being explicitly inserted by the shader programmer. In such cases, a warning will be issued.\n\nA barrier intrinsic cannot appear inside of dynamic flow control. A barrier can be within uniform flow control (ie flow control based on non-per-pixel variables). The HLSL compiler will validate this and will fail compilation if barriers are placed within dynamic flow control.\n\nNo automatic mechanism for synchronizing between or enforcing ordering between thread groups is specified for implementations at this time. Synchronization across thread groups is up to the application.\n\nFor more concrete details (taking precedence over any text here) see the Shader Memory Consistency Model(7.14)\n\nDevice memory can be accessed by a compute shader for read and write operations. This section outlines the operations supported. Device memory can be defined to support read and write operations on the same surface simultaneously.\n\nAn output resource can be declared to be of one of several supported types.\n\nThe following resource object classes are supported for declaring output resources in HLSL compute shaders:\n\nThe maximum dimensions of a resource are the same as the limits(21) on render targets for graphics with the same shader model.\n\nUnlike texture resources, a buffer resource that is bound via a writeable shader resource view may also be read from using the correct read intrinsic. However, no resource (texture or buffer) can be bound simultaneously via writeable and readable views. The Direct3D device API implementation enforces this at buffer bind time by unbinding the conflicting view.\n\nIf a resource is swizzled at the time of being written to, then the implementation is responsible for swizzling writes to that surface.\n\nBuffer resources used for output from the compute shader must be created with the D3D11_BIND_RENDER_TARGET flag. Such resources may be read from, however.\n\nBuffer resources created with the D3D11_BIND_SHADER_RESOURCE flag may only be used as inputs to the compute shader.\n\nReading data from device (video) memory is supported using the same mechanisms as graphics shaders of the same shader model version.\n\nFor example, in shader model 5, up to resources can be bound to the compute shader for read operations.\n\nAny input port can have a resource assigned. Texture resources can be used with load(), gather(), or sample() instructions. Input resources that are buffers (not textures) can also be bound for input, but filtering operations may not be used on such resources.\n\nBuffer resources (not texture resources) may also be read from, even though they are created as output resources.\n\nWhen reading addresses outside of range, 0 is returned.\n\nShader threads are able to write information out to device memory using mechanisms analogous to those used by graphics shaders in stream-out, and in rendering. In addition, shaders can write data to a run-time computed address in graphics memory. This capability is sometimes referred to as scatter. Once a resource has been declared and assigned to the shader output, then a set of intrinsic methods can be used to write information out to that resource. The resource definition restricts the range of addresses that can be accessed to a clearly defined limit.\n\nMultiple mechanisms are supported for output operations on Device Memory:\n• Random access element writes to buffer resources or texture resources of 1-, 2-, or 3-dimensions.\n• Random access atomic reduction operations to buffer resources or texture resources of 1-, 2-, or 3-dimensions of single element 32-bit integer types (do not return result).\n• Random access atomic immediate reduction operations to buffer resources or texture resources of 1-, 2-, or 3-dimensions of single element 32-bit integer types (return result available to subsequent shader statements).\n• Streaming output writes. These are records emitted to a data buffer that is predefined for structured output, and for which performance is optimized, but record ordering is explicitly not preserved.\n\nWhen writing to device memory, out-of-bounds array indices cause the write to be ignored, though out of bounds offsets within individual structs cause the entire contents of the resource to be undefined.\n\nNote: The Pixel Shader(16) specification also includes the output operations to device memory as described here. The total number of such buffer and all MRTs is specified to be no more than a fixed limit of .\n\nRandom access writes to device memory are accomplished via the IL instructions: store_raw(22.4.11), store_uav_structured(22.4.13), or store_uav_typed(22.4.9) depending on the type resource is used (bound as an Unordered Access View(5.3.9)).\n\nOne way to do writes to device memory is through defined reduction operations known to be order-independent. These operations must be atomic (in the sense that they must complete fully before another thread executes on the same data), however, they do not return the result of the incremented address back to the shader code.\n\nAtomic operations in the shader IL are listed here(22.1.2.14), named atomic_*.\n\nIt is required that implementations make these operations atomic, ie no other thread can access the same location during the execution of this intrinsic.\n\nUpdates to device memory are also enabled through immediate reduction operations, i.e. that immediately return a result to the shader for use by subsequent instructions in the same thread. These operations must be atomic (in the sense that they must complete fully before another thread executes on the same data).\n\nImmediate atomic operations in the shader IL are listed here(22.1.2.14), named imm_atomic_*.\n\nThe result returned by these intrinsics is the value of the destination before the operation is performed. There is at least one exception where the value after the operation was performed is returned: imm_atomic_consume(22.17.18).\n\nA capability is provided to enable threads to efficiently emit records (structs) to a compacted stream in device memory with no guarantee of ordering.\n\nWhen all elements in a wave-front write 32-bit quantities sequentially to global memory, writes should be as performant as a pixel shader writing to a render target.\n\nA shader intrinsic is provided to force completion of all writes queued from the currently executing thread group.\n\nA compute shader requires the ability to read and write data, and to access state information that may be updated between invocations. Data resources are managed using the same scheme as the graphics API.\n\nIn addition to the conventional texture resource binding that are common to all shader stages, such as the Shader Resource View bindpoints, surfaces can be bound as output texture resources (Unordered Access Views(5.3.9)) for which scattered read/write and atomic operations are permitted.\n\nOne limitation on typed texture2D UAVs is that Automatic gamma conversion to/from gamma-corrected texture formats (e.g. DXGI_FORMAT_R8G8B8A8_UNORM_SRGB) is not supported when accessed by the compute shader. Any gamma conversion required by the application must be implemented in shader code. Another limitation due to the hardware of this generation is that for typed UAVs, writes as supported, but not reads, unless the format is DXGI_FORMAT_R32_UINT/SINT/FLOAT (in which there is no type conversion required).\n\nShared memory can be updated via either variable assignment, variable reduction operations, or through indexed array assignment as defined below.\n\nShared memory registers can be updated using standard scalar variable assignment, which the implementation guarantees is atomic.\n\nOne way to perform writes to shared memory are through defined reduction operations known to be order-independent. These operations must be atomic (in the sense that they must complete fully before another thread executes on the same data), however, they do not return the result of the incremented address back to the shader code.\n\nAtomic operations in the shader IL are listed here(22.1.2.14), named atomic_*.\n\nIt is required that implementations make these operations atomic, ie no other thread can access the same location during the execution of this intrinsic.\n\nUpdates to shared memory are also enabled through immediate reduction operations, i.e. that immediately return a result to the shader for use by subsequent instructions in the same thread. These operations must be atomic (in the sense that they must complete fully before another thread executes on the same data).\n\nImmediate atomic operations in the shader IL are listed here(22.1.2.14), named imm_atomic_*.\n\nThe result returned by these intrinsics is the value of the destination before the operation is performed.\n\nFor example, the compute shader can use such an intrinsic to atomically increment a shared address. This is commonly used to compact data into device memory. Below is pseudo code for how it might be exposed in the API. Behind the scenes the hardware could apply parallel constructs to make this fast.\n\nAlthough this intrinsic updates the contents of the shared register used, the return result of the intrinsic is the pre-operation modified value.\n\nNote: In DX11 shader model 5.0, these intrinsic are only supported on 32-bit integer shared memory variables.\n\nArrays declared in shared memory can be written to directly via write/copy operations or the above reduction operators using indexed array assignment.\n\nShared memory should be performant when all elements of a wave front are writing out sequential 32 bit quantities. Writes should not have to serialize when threads write non-sequentially to shared memory.\n\nThe following registers are available in the cs_ _ model:\n\nFor graphics shaders the maximum number of general purpose/temporary registers per thread is set at float4s. This limit remains the same for each thread in the Compute Shader.\n\nIn practice, hardware implementations may spill temp register storage to slower memory behind the scenes if the combination of # of temps per thread and # of threads in the group goes too high. Functionally, however, it will always appear to the shader as if storage for temp registers is available per Compute Shader thread independent of how many threads are in a group.\n\nThe HLSL compiler may blindly print a warning (not an error) when the number of temps used by a Compute Shader exceeds the threshold: min( / threads in group, ). This is just a rough guess that spilling of temp storage is likely to happen beyond this point, but it could even happen with fewer temps. Such a warning does not take into account the actual threshold(s) where the number of temps impacts performance on any given hardware architecture.\n\nThe compute shader supports two different areas where order-dependent results can arise. Threads may contend for the same write address in both the shared memory register space, and in the output memory resource. Separate mechanisms can be offered as outlined below. These mechanisms are not defined in detail in this functional spec as their syntax can be decided by the programmer.\n\nIt is the intent that this functional specification enable much more freedom than may be exposed in initial versions of the compiler. This will enable compiler updates to expose more general functionality over time, if such is discovered to be sufficiently important.\n\nThe compiler can identify 3 separate cases for the addresses written into shared memory (indices of arrays declared in the shared register space).\n\n1. If these addresses are known to not overlap, (e.g. are computed solely based on the vThreadID and some constants and no mod () operations, then compilation will succeed.\n\n2. If these addresses are known to repeat and will overlap then compilation will fail with a fatal error.\n\n3. If these addresses cannot be determined to be free of conflict, the compiler will issue a warning\n\nIn the case where the operation on the destination is a reduction operator (such as an atomic add), then the compiler need not validate the address computation logic. In this case collisions will not produce order-dependent output, but programmers need to be aware that they may still produce performance issues due to port contention, or locks taken by the implementation to assure atomicity of the operator.\n\nThe same mechanisms are available as used in the Shared Register Space validation above.\n\nIt is illegal to have a loop inside of a divergent branch with termination dependent on thread communication. This is to prevent deadlock and will be validated by the HLSL compiler.\n\nFor optimal performance, it is expected that the number of threads per group is between 200 and 1024 for shader model 5. The number of thread groups per invocation should be over 128 ideally.\n\nAll API state for the Compute Shader is unique to it, just as Pixel Shader state is kept separate from Vertex Shader state. This state is of the following four categories:\n• buffers that store data that does not change during shader execution (constants)\n• state that governs how any texture resources are sampled\n\nLike any other shader type, there are methods on the D3DDevice to specify the additional state specific to the compute shader:\n\nThe syntax for these methods is the same as the corresponding calls for other Direct3D11 shaders.\n\nAll state of the compute shader is like state for any other shader and is independent of the state of all other shaders.\n\nThe following example shows how the thread count is specified as an attribute in HLSL.\n\nAs a convenience to the programmer, sets of threads in an invocation batch can be thought of as being organized into an array of 1-, 2-, or 3-dimensions (with the possibility of more in future releases).\n\nTo continue the above example, the shader could be invoked with the following parameters in the dispatch call:\n\nThis launches a grid of 30 groups that is 5 groups wide by 3 groups high, by 2 group deep. Each group contains a block that is 10 threads wide by 8 threads high by 3 deep.\n\nThe diagram here(18.6.3) shows what this would look like, including thread ID's in the Compute Shader threads that identify where they are.\n\nIn DirectX11 shader model 5.0, there is an upper limit of on the last dimension (Z) of the thread group thread counts. Any 5.0 shaders that specify a larger value here will fail compilation.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 18.7.1 Overview\n\n 18.7.2 How Relevant D3D11 Features Work on Downlevel HW\n\n\n\nThis section defines a subset of the D3D11 hardware Compute Shader as well as Raw(5.1.4) and \"Structured Buffer(5.1.3) features that can work on some D3D10.x hardware. D3D11 drivers on D3D10.x hardware can opt-in to supporting this functionality via the D3D11 API. No changes were made to the D3D10.x API/DDIs for this.\n\nExample of known D3D10.x hardware that should be able to support this at the time of implementation are all of nVidia’s D3D10+ hardware, and for AMD, all 48xx Series D3D10.1 hardware and beyond. The features exposed are basically an intersection of the features on known existing hardware, while being a clean subset of D3D11 hardware’s feature set. The feature intersection does mean that not all of the expressiveness of IHV-specific APIs is available.\n\nThe rest of this section refers to D3D11 drivers for D3D10.x hardware which have opted into supporting the features as \"downlevel HW\". Note this does not mean all D3D10.x hardware.\n\nThe Dispatch() API/DDI on D3D11 for invoking the Compute Shader will function identically on downlevel HW, with the X and Y dimensions of the grid of Thread Groups invoked allowed to be up to , however the Z dimension can be no more than (larger gives undefined behavior), as opposed to on D3D11 hardware.\n\nDispatchIndirect() is unsupported on downlevel HW, so the runtime will do nothing on such HW when this API is called.\n\nThe CSInvocations pipeline statistic(20.4.7) will count identically for downlevel and D3D11 HW. Given that DispatchIndirect() is not available on downlevel HW, this is admittedly not of much value, since the application can trivially track how many threads it invoked via Dispatch() calls.\n\nDownlevel HW supports Raw and Structured UAVs (but not Typed UAVs) with identical semantics to D3D11 HW, except that only a single UAV can be bound to the pipeline at a time via CSSetUnorderedAccessViews() API/DDI.\n\nNote the lack of support for Typed UAVs on downlevel HW also means that Texture1D/2D/3D UAVs are not supported.\n\nPixel Shaders on downlevel HW do not support UAV access.\n\nThe base offset for a RAW UAV must be byte aligned (whereas full D3D11 HW requires only byte alignment). RAW SRV’s (below) do not have any corresponding additional restriction.\n\nAll shader stages on downlevel HW: Vertex Shader, Geometry Shader, Pixel Shader and Compute Shader (CS described later) support binding Raw and Structured Buffers as SRVs for read-only access, just as on D3D11 hardware.\n\nThis is useful not only as a way of re-circulating Compute Shader outputs, but also in general as a way of reading generic data into Shaders.\n\nWhen downlevel HW support is available, existing D3D10/D3D10.1 shader models 4_0 and 4_1 gain additional functionality via D3D11. The reason this additional functionality is not placed in a separate shader model (such as defining a new 4_3 model) is that some of the targeted hardware is 4_0 class, and some is 4_1 class. So these additional features are orthogonal to shader models.\n\nThis way of exposing additional functionality to a given shader model is similar to the way double precision instruction support is made available optionally through shader model 5_0.\n\nFor VS/GS/PS 4_0/4_1, the additional functionality is the ability to read from raw and structured buffers, described earlier. This means the addition of the following bytecode instructions from shader model 5 are added to these shader models:\n\nBeyond the VS/GS/PS, an additional shader type is available on downlevel HW: Compute Shader, via shader models CS_4_0 and CS_4_1. The next section describes this in detail.\n\nCS_4_0 takes the VS_4_0 instruction set, except it has Compute Shader style inputs:\n\nThe output is a single UAV, u#, where # is the RT/UAV slot where the UAV is bound. vThreadIDInGroupFlattened is defined later on (it has not been described before) – it will also be in CS_5_0 for forward compatibility.\n\nCS_4_1 is like CS_4_0, except it uses the VS_4_1 instruction set instead of VS_4_0.\n\nFor both CS_4_0 and CS_4_1, the following additional instructions are present:\n\nNote in particular the absence of atomic operations, append/consume, or typed UAV access from the above list. All of these are present in CS_5_0.\n\nFurther, note the absence of double precision arithmetic operations – drivers may opt to expose double precision arithmetic operations support via 5_0 shaders, but even if that is the case, CS_4_0 does not expose doubles (nor do any 4_x shaders for that matter).\n\nThe sync instruction behaves the same as in CS_5_0, including the stipulation that the _ugroup option will not be exposed via HLSL unless it is deemed necessary (see sync instruction specs).\n\nDownlevel HW supports X and Y dimensions of at most for the set of threads in the Thread Group (as opposed to for D3D11 HW). The Z dimension is unchanged at a maximum of .\n\nThe total number of threads in the group (X*Y*Z) is limited to , as opposed to for D3D11 HW.\n\nExceeding these limits is enforced simply by failing shader compilation, since the numbers are declared as part of the shader.\n\nThere is only 16kB total Thread Group Shared Memory on downlevel HW, as opposed to 32kB for D3D11 HW.\n\nA given Compute Shader thread can only write to its own region of TGSM. This write-only region has a maximum size of bytes or less, depending on the number of threads declared for the group. This per-thread size maximum is given by the table below. Instructions that write to the shared memory must use a literal offset into the region.\n\nIn contrast, any thread can read the TGSM for the entire thread group.\n\nAccesses to UAVs from cs_4_0/cs_4_1 do not have these constraints.\n\nFirst, recall that in cs_5_0, the Thread Group Shared Memory (TGSM) space is made visible to compute shader threads by declaring ranges of the space, each named g#. All threads can see all the g# ranges. The reason to be able to define multiple g# is to allow different ranges to be organized differently – like with different structure strides. A given g# range can be declared as either RAW (just a flat count of bytes in size, multiple of 4 bytes), or STRUCTURED (given a structure count and a structure stride that is a multiple of 4 bytes).\n\nFor cs_4_0 and cs_4_1, RAW g# memory is not available at all. All g# declarations must be STRUCTURED, but as a way of exposing per-thread RAW memory, rather than as a way of having an array of structures that a given thread could write to.\n\nRecall that STRUCTURED g# declarations look like:\n\nRecall the Compute Shader declares its thread group size statically via 3 integers defining the dimensions of the grid of threads – x,y,z. The number of threads in the group is x*y*z.\n\nFor CS_4_0/4_1, it is required that numStructures in the dcl above must be exactly x*y*z. And it is required that the sum of the structureByteStride value for all g# declarations in the program falls within the size limits defined in the previous section.\n\nRecall that the Compute Shader has an input System Generated Value(4.4.4) (SGV) \"vThreadIDInGroup\" which tells the thread where it is in the grid as a 3D value.\n\nA new input SGV is introduced now, for CS_4_0, CS_4_1 and CS_5_0 (forward compatibility): \"vThreadIDInGroupFlattened(23.14)\". This is the 1D equivalent of vThreadIDInGroup:\n\nIt is required that any writes to g# memory in CS_4_0 and CS_4_1, which must be done via the store_structured instruction, must specify the structureIndex parameter as vThreadIDInGroupFlattened, and the byte offset must be a literal.\n\nJust as optional double precision math support in shader model 5 is reported through the CheckFeatureSupport API/DDI, in the same way a driver can report support for the Compute Shader and Raw/Structured Buffers on Shader 4_x. The support is all or none.\n\nThe particular bit in the caps structure reported by drivers, shown below, is D3D11DDICAPS_SHADER_COMPUTE_PLUS_RAW_AND_STRUCTURED_BUFFERS_IN_SHADER_4_X. D3D11 Hardware must report this bit, as it represents a subset of D3D11’s features.\n\nThis information is bubbled up to the D3D11 API via CheckFeatureSupport(), where there is an entry in the D3D11_FEATURE enum: D3D11_FEATURE_D3D10_X_HARDWARE_OPTIONS\n\nThe data structure associated with this feature query would be:\n\n18.7.3.2 How Valid D3D11 API Usage is Enforced on Downlevel Shaders\n\nCS_4_0 and CS_4_1 shaders and Raw or Structured Buffers will be allowed to be created on devices that report TRUE for ComputeShaders_Plus_RawAndStructuredBuffers_Via_Shader_4_x.\n\nTo enable use of Raw/Structured Buffers as SRVs in VS, GS or PS, a new flag can be present in the following shader models: VS_4_0, VS_4_1, GS_4_0, GS_4_1, PS_4_0, PS_4_1. Recall that at the IL level, shader 4_0+ already has a \"global flags\" declaration: dcl_globalFlags(22.3.2). In D3D10.x APIs the only flag that could be specified here is REFACTORING_ALLOWED. For all the shader models listed in this paragraph an additional flag can be used (only with the D3D11 APIs):\n\nShaders that set this flag will only be allowed to be Created on a device that reports TRUE for ComputeShaders_Plus_RawAndStructuredBuffers_Via_Shader_4_x.\n\nThe Dispatch() API will be dropped by the runtime for devices that do not report TRUE for ComputeShaders_Plus_RawAndStructuredBuffers_Via_Shader_4_x. As mentioned previously, DispatchIndirect() will always be dropped on pre-D3D11 hardware.\n\nThis section describes D3D . Element(4.4) data format layout and interpretations. A large number of data layouts and interpretations are available. In addition, there is facility to view the same data with different interpretations (e.g. raw bits vs. normalized integer), or to represent data in a general way (just the bit layouts) without committing to a particular interpretation of the data (e.g. normalized) until as late as possible (e.g. a Shader using the data).\n\nIn D3D . , it is possible to create partially typeless where the number of bits per component is specified, but not the data interpretation for those bits. An example of a partially typeless format is DXGI_FORMAT_R8G8B8A8_TYPELESS. This format has several subformats (making up a second tier) which fully resolve the interpretation of the data, including DXGI_FORMAT_R8G8B8A8_UNORM, DXGI_FORMAT_R8G8B8A8_UNORM_SRGB, among others.\n\nWhen a resource with a partially typeless format is bound to a Shader for output or input, it must be fully qualified as one of the subformats which has the same bit counts for each component but now defines a specific type for each components.\n\nSince partially typeless formats have the number of bits per Element (BPE) specified, resource dimensions provided on creation are enough to determine memory allocation requirements. Note that some complex formats, such as Block Compression(19.5) formats, require the format to be specified permanently on resource creation.\n\nD3D . defines a long list of Element format names (the DXGI_FORMAT_* enum below). Each format which does not have TYPELESS in its name describes the data representation at both ends (source/target) when transferring data from a resource Element into a Shader register or transporting data from a Shader register out to a resource. Sometimes the transport path involves some mathematical operations in the middle (such as filtering or blending), but such middle steps do not change what the data is to be represented as at both ends of the transport, as defined by the DXGI_FORMAT_*. Should there be ambiguity regarding how to go about intermediary steps (filtering or blending) for particular formats, it will likely be clarified in this spec, and certainly in the D3D . reference rasterizer.\n\nConsider a number expressed in some data format sitting in memory.\n\nSuppose this number travels along the following path: First it is input into a Shader by a mechanism that does no other transformation on the data (exampes: sampling without filtering, or an Input Assembler fetch) except perhaps conversion into a format compatible with the target Shader register. Then, the value is passed from Shader to Shader in the Pipeline unmodified. Finally, this unmodified value sitting in a Shader is written back out to memory by a mechanism that does no other transformation on it (i.e. blending disabled when rasterizing) except perhaps conversion to the output format.\n\nOther examples of operations on read or write that qualify in the path above are:\n• blending enabled with a 1.0 multiplier for the source color\n• filtered texture read with texture coordinates precisely on a texel (weighted 1, with any other texels weighted 0)\n\nD3D . requires that for the path described, if the output data format is the same as the original input format, then the input and the output data must be identical in memory.\n• The SNORM data format (defined here(3.2.3.1), and conversions defined here: SNORM -> FLOAT(3.2.3.3), FLOAT -> SNORM(3.2.3.3)) contains two encodings representations for -1.0f when converting from SNORM to float32. When converting from float32 back to SNORM, only one of the -1.0f encodings is achievable. As long as SNORM data is originally encoded in a form that does not use the \"extra\" -1.0f encoding (the minimum integer), Data Invertability from SNORM to float32 and back is guaranteed.\n• Data Invertability holds for NaN, except that the exact bit pattern of NaN is not required to be maintained when round-trip data conversions are involved.\n\nThe naming convention followed by most formats (aside from special formats like Block Compression (BC*)(19.5) Formats, or YUV among others) in the table below is as follows:\n\nThe default value for missing components in an Element format is \"0\" for any component except A, which gets \"1\". The way \"1\" appears in the Shader depends on the Element format, in that it takes the specified data interpretation of the first typed component that is actually present in the format (starting from the left in RGBA order). If this interpretation is UNORM or FLOAT, then 1.0f is used for missing components. If the interpretation is SINT or UINT, then 0x1 is used.\n\nFor example, when the format DXGI_FORMAT_R24_UNORM_X8_TYPELESS is read into a Shader, the values for G and B are 0, and A is 1.0f. For DXGI_FORMAT_R16G16_UINT, the B gets 0 and A gets 0x00000001. DXGI_FORMAT_R16_SINT provides 0 for G and B, and 0x00000001 for A.\n\nThe _SRGB format modifier is ignored for display scan-out, so for the purposes of scan-out the _SRGB and non-_SRGB formats are identical. It is up to the application to appropriately set the display scan-out controls to accommodate _SRGB formats.\n\nThe following links are to Excels spreadsheets with a complete listing of available D3D . formats, by feature level.\n\nThe meaning of the column, \"Cast within Bit Layout\" is that Pre-Structured+Typeless(5.1.5) or Prestructured+Typed(5.1.6) resources having a particular format can have the format reinterpreted using a Resource View(5.2) to be any other format, as long as the number of bits per-component are identical, but interpretations of the bits can be different. The new format must be compatible with the usages (such as RenderTarget) specified when originally creating the resource.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 19.2.1 Overview\n\n 19.2.2 Multisample RenderTarget/Resource Load Support vs. Multisample Resolve Support\n\n 19.2.3 Optional Multisample Support\n\n 19.2.4 Specification of Sample Positions\n\n\n\nObserve in the Format List(19.1.4) that a superset of formats that support Multisample resolve can be used for Multisampling. For example, integer formats do not have a fixed-function resolve permitted, yet they can still be supported for Multisample resources. The point is that these formats can be used as RenderTargets and subsequently be read back into shaders via Multisample Resource Load(7.17.1); a path where no resolving of the Multisample resource is needed given the individual samples are accessed by the shader. Note that depth formats are not supported for multisample resource load and are thus restricted to be RenderTargets only.\n\nTypeless formats such as R8G8B8A8_TYPELESS support multisampling as well, to enable blindly interpreting the data in the resource different ways. Note that this ability to change the format interpretation of a resource is pervasive in the system; Multisampling happens to be one instance where the concept applies. A specific example with Multisampling would be to create a Multisample resource with the format R8G8B8A8_TYPELESS, render to it resource with a R8G8B8A8_UINT RenderTarget View, then later resolve the contents to another resource by telling the Resolve operation that the data format is R8G8B8A8_UNORM. Note that R8G8B8A8_UNORM can support Multisample Resolve, while R8B8B8A8_UINT cannot. No data conversion happens from UINT to UNORM for this example, just raw interpretation of the data as UNORM (ignoring that it happened to be rendered as UINT). The application is assumed to be taking advantage of this behavior by requesting it.\n\nObserve in the Format List(19.1.4) that Multisample support appears optional for a large set of formats, never required. The meaning of this is as follows:\n\nHardware can report support or non-support of Multisampling for any format listed in the format list. This is exposed through the API/DDI via a method for hardware to report, for any given format + sample count (up to samples), a number indicating how many \"Quality Levels\" are supported. For example R8G8B8A8_UNORM with 2-sample Multisampling may support 3 quality levels on some hypothetical hardware implementation. This means the hardware happens to support 3 different sample layouts and/or resolve algorithms for 2-sample Multisampling for R8G8B8A8_UNORM. The definition of each reported Quality Level is up to the hardware vendor to define, however no facility is provided by D3D to help discover this information.\n\nHardware can report 0 quality levels for a given format + sample count, which means the hardware does not support multisampling at all for that combination of format + sample count.\n\nThere are some limitations in the flexibilty given to hardware for not supporting Multisampling on a format:\n\n(1) Given any related family of formats sharing a typeless parent, for example the set {R8G8B8A8_TYPELESS, R8G8B8A8_UNORM, R8G8B8A8_UNORM_SRGB, R8G8B8A8_UINT, R8G8B8A8_SNORM, R8G8B8A8_SINT}, the reported set of quality levels for each sample count for any one format in the family must be the same for the rest of the formats in the family.\n\n(2) Any format that supports multisampling and which has type _UNORM, _UNORM_SRGB, _SNORM or _FLOAT must support Resolve. This is reflected in the Format List(19.1.4) in that Resolve support is shown as \"Required\" for such formats. Of course, if the hardware does not report Multisampling support for some formats at all, the \"Required\" Resolve support becomes moot for those formats.\n\nIn addition to the \"Quality Level\" mechanism for IHVs to expose custom multisample modes, as of D3D10.1, fixed sample patterns are defined for certain sample counts. For IHVs that expose the fixed patterns, sample postions will be at known locations defined here, and thus consistent across IHVs. If the hardware is asked to perform a Resolve() on a fixed pattern, that is defined as a simple average of the samples within each pixel. For every fixed sample pattern that has sample locations spread over the area of a pixel, there is a sibling fixed pattern with the same number of samples, except with all samples located overlapping the center of the pixel.\n\nApplications can check for support of standard patterns via the exising CheckFormatSupport() method, in the following slightly awkward way: As long as the driver reports NumQualityLevels *gt; 0, and there are fixed sample patterns defined for that sample count, then the application can request the fixed patterns by specifying QualityLevel as either D3D11_STANDARD_MULTISAMPLE_PATTERN (0xffffffff) or D3D11_CENTER_MULTISAMPLE_PATTERN (0xfffffffe). In the DDI the name for these QualityLevel values are (D3D10.1 DDI names still apply) D3D10_1_DDIARG_STANDARD_MULTISAMPLE_PATTERN and D3D10_1_DDIARG_CENTER_MULTISAMPLE_PATTERN. When the driver reports NumQualityLevels > 0, this exposes support of the usual range of QualityLevel values [0... (NumQualityLevels-1)] in addition to the new fixed patterns. If the hardware only supports the fixed patterns but no additional vendor-specific patterns, NumQualityLevels can be reported as 1, and the hardware can pretend QualityLevel = 0 behaves the same as QualityLevel = D3D11_STANDARD_MULTISAMPLE_PATTERN.\n\nStandard sample patterns are defined for the following sample counts: 1(trivial), 2, 4, 8, 16. As stated here(19.2.5), the only sample counts required by hardware are 1 and 4 and 8 samples (with some caveats). Vendors can expose any sample counts beyond these, but if they happen to support 2, 4(required), 8(required) or 16 each of those means support for the corresponding standard pattern or center pattern is required.\n\nThe standard center sample patterns (D3D11_CENTER_MULTISAMPLE_PATTERN) that have more than one sample overlapping at the center of the pixel have a couple of usage restrictions:\n\nD3D11 requires support for 1x(trivial), 4x and 8x MSAA, with at minimum support for the standard patterns for these MSAA counts. At 4x MSAA, all output (RenderTarget/DepthStencil-able) formats must be supported. At 8x MSAA, only output formats with less than 128 bits per sample must be supported. Support for 128+ bits per sample formats with 8x MSAA is optional. Other MSAA counts and patterns are optional as before.\n\nD3D . supports a couple of high dynamic range pixel formats. This section defines these formats.\n\nA color is represented by 3 mantissas and an exponent as follows:\n• Three 9-bit fields representing the fractional component (frac) of each channel, with no implied 1 before the decimal\n• a 5-bit exponent (e), with bias 15, similar to the normalized exponent in 16-bit floating point(3.1.5)\n• There are no INF's or NAN's in this format.\n\nFor each component in {R,G,B}, the value \"v\" of the component is:\n\nThis format cannot be a RenderTarget.\n\nDue to the lack of an implied 1, all RGBE colors can be represented by legal 16-bit floating point numbers. In particular, values with an unbiased exponent of 31 may not be treated as INF or NAN.\n• it is required that the exponent of the most significant channel be chosen as the shared exponent to ensure proper and consistent mapping of the color space.\n• conversion from 32-bit and 16-bit floating point formats should be done with nearest even. Round to nearest is allowed\n• since the exponent bias is 15, and since there is no implied 1 to the left of the decimal point, we have MAXFLOAT defined as follows:\n• there are no NAN's or INF's in this format.\n• There is no sign bit. All negative numbers must clamp to zero.\n• values outside of the representable range (including INF) must saturate to RGBE range.\n\nThis format consists of 3 independent, reduced-mantissa floating point channels. See the 11-bit and 10-bit Floating Point(3.1.6) section for a description of the mechanics of these reduced precision numbers.\n• Only the R11G11B10_FLOAT supports blending (RGBE cannot be a RenderTarget).\n• Blending with R11G11B10_FLOAT is defined to occur exactly as if the data is converted to 16f per channel before the blend, and then back afterward.\n• As a result, all rules that apply to 16-bit floating point blending also applies to B10G11R11_FLOAT.\n• Since R11G11B10_FLOAT does not store alpha, it is always implied to be 1.0f on read into the blender.\n\nThe sub-sampled formats (such as R8G8_B8G8) are reconstructed via replication to per-pixel RGB values prior to use.\n\nThe G component is taken from the currently addressed pixel value. The R component is taken from the current pixel value for even x resource addresses, and from the previous ('-1'th) x dimension pixel value for odd x resource addresses. The B component is taken from the next ('+1'th) x dimension pixel value for even x resource addresses, and from the current pixel value for odd x resource addresses.\n\nResources in these formats are required to be a multiple of 2 in the x dimension, rounding up to an x dimension of 2 for the smallest mipmap levels. For mipmaps, the sizing and sampling hardware behavior is similar to Block Compressed Formats(19.5), where the top level map must be a multiple of 2 size in the x dimension, and for smaller maps the virtual x dimension size may be odd while the physical size is always even.\n\nThe regions being sourced and modified by the Resource Manipulation(5.6) operations are required to be a multiple of 2 in the x dimension.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 19.5.1 Overview\n\n 19.5.2 Error Tolerance\n\n 19.5.3 Promotion to wider UNORM values:\n\n 19.5.4 Promotion to wider SNORM values:\n\n 19.5.5 Memory Layout\n\n 19.5.6 BC1{U|G}: 2(+2 Derived) Opaque Colors or 2(+1 Derived) Opaque Colors + Transparent Black\n\n 19.5.7 BC2{U|G}: 2(+2 Derived) Colors, 16 Alphas\n\n 19.5.8 BC3{U|G}: 2(+2 Derived) Colors, 2(+6 Derived) Alphas or 2(+4 Derived + Transparent + Opaque) Alphas\n\n 19.5.9 BC4U: 2(+6 Derived) Single Component UNORM Values\n\n 19.5.10 BC4S: 2(+6 Derived) Single Component SNORM Values\n\n 19.5.11 BC5U: 2(+6 Derived) Dual (Independent) Component UNORM Values\n\n 19.5.12 BC5S: 2(+6 Derived) Dual (Independent) Component SNORM Values\n\n 19.5.13 BC6H / DXGI_FORMAT_BC6H\n\n\n\nThis section describes various block-based compression formats. A surface is divided into 4x4 texel blocks, and each 16-texel block is encoded in a particular manner as an atomic unit. Each distinct encoding method is given a unique format name (identified by a four-character code and matching DXGI_FORMAT_BC* name).\n\nBlock Compressed formats can be used for Texture2D (including arrays), Texture3D or TextureCube (including arrays), including mipmap surfaces in these Resources.\n\nBC format surfaces are always multiples of full blocks, each block representing 4x4 pixels. For mipmaps, the top level map is required to be a multiple of 4 size in all dimensions. The sizes for the lower level maps are computed as they are for all mipmapped surfaces, and thus may not be a multiple of 4, for example a top level map of 20 results in a second level map size of 10. For these cases, there is a differing 'physical' size and a 'virtual' size. The virtual size is that computed for each mip level without adjustment, which is 10 for the example. The physical size is the virtual size rounded up to the next multiple of 4, which is 12 for the example, and this represents the actual memory size. The sampling hardware will apply texture address processing based on the virtual size (using, for example, border color if specified for accesses beyond 10), and thus for the example case will not access the 11th and 12th row of the resource. So for mipmap chains when an axis becomes < 4 in size, only texels 'a','b','e','f' (see diagram below) are used for a 2x2 map, and texel 'a' is used for 1x1. Note that this is similar to, but distinct from, the surface pitch, which can encompass additional padding beyond the physical surface size.\n\nThe regions of BC formats being sourced and/or modified by the Resource Manipulation(5.6) operations are required to be a multiple of 4.\n\nValid implementations of BC formats other than BC6H and BC7 may optionally promote or do round-to-nearest division, so long as they meet the following equation for all channels of all texels:\n\nabsolute_error is defined in the description of each format.\n\nendpoint_0, endpoint_1, and their promoted counterparts have been converted to float from either UNORM or SNORM as specified in the Integer Conversion(3.2.3) rules. Values that the reference decodes to 0.0, 1.0 or -1.0 must always be exact.\n\nFor BC6H and BC7, decompression hardware is required to be bit accurate; the hardware must give results that are identical to the decoder described in this specification.\n\nPromotion is defined to utilize MSB extension to define the new LSBs as follows.\n\nThe following diagram depicts the overall layout of data in a Block Compressed surface. After that, the per-block memory encoding for each BC* format is individually illustrated.\n• Filtering must occur with at least UNORM8 precision.\n• Values specified as 0.0f or 1.0f must be exact.\n\nSame as BC1U, but colors are in sRGB space, linearized pre-filter on read. sRGB conversion should occur the same as with uncompressed UNORM8 formats. If an implementation provides more precise palette entries than it can linearize, it may have up to 1 UNORM8 ULP error in conversion on input to linearization.\n• Implementations must follow the same precision rules as BC1 for color palette entries.\n• Alpha values must be filtered with at least UNORM8 precision.\n• Color is decoded prior to gamma conversion the same as BC2U.\n• Gamma conversion then occurs the same way as BC1G.\n• Alpha decodes the same as BC2U.\n• Implementations must follow the same guidelines as BC1 for color palette entries.\n• Filtering must occur with at least UNORM8 precision.\n• Color is decoded prior to gamma conversion the same as BC3U.\n• Gamma conversion then occurs the same way as BC1G.\n• Alpha decodes the same as BC3U.\n• Filtering must occur with at least UNORM16 precision.\n• red_0, red_1, 0.0f and 1.0f must be exact.\n• Filtering must occur with at least SNORM16 precision.\n• red_0, red_1, -1.0f and 1.0f must be exact.\n\nThe following DXGI_FORMATs are in this category: DXGI_FORMAT_BC6H_TYPELESS, DXGI_FORMAT_BC6H_UF16, and DXGI_FORMAT_BC6H_SF16.\n\nThe BC6H format can be used for Texture2D (including arrays), Texture3D or TextureCube (incl. arrays). All of these uses include mipmap surfaces in these resources.\n\nBC6H uses a fixed block size of 16 bytes and a fixed tile size of 4x4 pixels. Just as with previous BC formats, images larger than BC6H's tile size are compressed using multiple blocks. The same addressing identity also applies to three-dimensional images as well as mip-maps, cubemaps, and texture arrays.\n\nBC6H compresses three-channel images that have high dynamic range greater than 8 bits per channel. The supported per-channel formats are:\n\nAll image tiles must be of the same format.\n\nNote that the 16 bit floating point format is often referred to as \"half\" format, containing 1 sign bit, 5 exponent bits, and 10 mantissa bits.\n\nBC6H supports floating point denorms, but INF and NaN are not supported. The exception is the signed mode of BC6H, which can represent ±INF. While this ±INF \"support\" was unintentional, it is baked into the format. So it is valid for encoders to intentionally use ±INF, but they also have the option to clamp during encode to avoid it. In general, faced with ±INF or NaN input data to deal with, encoders are loosely encouraged to clamp ±INFs to the corresponding maximum non-INF representable value, and map NaN to 0 prior to compression.\n\nBC6H does not store any alpha data.\n\nThe BC6H decoder decompresses to the specified format prior to filtering.\n\nBC6H decompression hardware is required to be bit accurate; the hardware must give results that are identical to the decoder described in this specification.\n\nA BC6H block consists of mode bits, compressed endpoints, sometimes a partition index, and compressed indices.\n\nBC6H stores endpoint colors as a red, green, and blue (RGB) triplet, defining a palette of colors on an approximate line between two endpoints. Depending upon the mode, a tile is divided into one or two regions, each having its own pair of endpoints. BC6H stores one palette index per pixel.\n\nIn the two region case (hereafter referred to as TWO), there are 32 possible partitions. (The one region case will hereafter be referred to as ONE.)\n\nThe pseudocode below outlines the steps to decompress the pixel at (x,y) given the 16-byte BC6H block.\n\nThe diagram above shows the 14 possible formats for BC6H blocks. The formats can be uniquely identified by the Mode bits.The first ten modes are used by TWO, and the mode field can be either 2 or 5 bits long. These blocks also have fields for the compressed endpoints (75 bits), partition (5 bits), and indices (46 bits). As an example, the code \"11.555 11.444 11.444\" indicates both the precision of the red, green, and blue endpoints stored (11), as well as the number of bits used to store the delta values for the transformed endpoints (5, 4, and 4 bits for red, green, and blue, respectively, for 3 delta values.) The \"6666\" mode handles the case when the endpoints cannot be transformed; only the quantized endpoints are stored.\n\nThe last four modes are used by ONE, and the mode field is 5 bits. These blocks have fields for the endpoints (60 bits) and indices (63 bits). For ONE, the example endpoint code \"11.9 11.9 11.9\" indicates both the precision of the red, green, and blue endpoints stored (11), as well as the number of bits used to store the delta values for the transformed endpoints (9 bits for red, green, and blue, respectively, for 1 delta value.) The \"10.10\" mode handles the case when the endpoints cannot be transformed; only the quantized endpoints are stored.\n\nModes 10011, 10111, 11011, and 11111 are reserved and should not be used by the encoder. If hardware is given these modes, the resulting decompressed block must contain zeroes in all channels except the alpha channel. For BC6H, the alpha channel should always return 1.0 regardless of the mode.\n\nThere are 32 partition sets for TWO, which are defined by Table 1 below. Each 4x4 block represents a single shape. Note that this table is equivalent to the first 32 entries of BC7's 2 subset partition table.\n\nIn the table of partitions above, the bolded and underlined entry is the location of the fix-up index for subset 1 which is specified with one less bit. The fix-up index for subset 0 is always index 0 (i.e. the partitioning is arranged so that index 0 is always in subset 0). Partition order goes from top-left to bottom right, walking left-to-right, then top-to-bottom.\n\nTable 2 above shows the bit fields for the packed compressed endpoints as a function of the endpoint format. This takes up 82 bits for TWO and 65 bits for ONE. As an example, the first 5 bits of the header for the last encoding above (i.e. the right-most column) are bits m[4:0], the next 10 bits of the header are the bits rw[9:0], and so forth.\n\nThe field names are defined by the following table\n\nEndpt[i] refers to the 0th or 1st pair of endpoints. A is one endpoint of 3 channels A[0]..A[2], and similarly B is the other endpoint of 3 channels.\n\nFor TWO, there are four endpoint values to possibly sign-extend. endpts[0].A is signed only if the format is a signed format. The other endpoints are signed only if the endpoint was transformed, or the format is a signed format.\n\nThe code for ONE is similar and just removes endpts[1].\n\nThere is also sign extending for signed formats in the transform_inverse step shown below.\n\nFor TWO, the transform applies the inverse of the difference encoding, adding the base value at endpt[0].A to the other three entries, for a total of 9 adds. In the diagram below, the base value is represented as A0 and has the highest precision. A1, B0, and B1 are all deltas off of the anchor value, and these deltas are represented with lower precision. (A0 corresponds to endpt[0].A, B0 to endpt[0].B, and similarly for A1 and B1.)\n\nThe ONE case is similar, except there is only 1 delta offset, and thus a total of only 3 adds.\n\nThe decompressor should ensure that the results of the inverse transform will not overflow the precision of endpt[0].A. In the case of overflow, the values resulting from the inverse transform should wrap within the same number of bits. If the precision of A0 is 'p' bits, the transform is:\n\nand similarly for the other cases.\n\nFor signed formats the results of the delta arithmetic must be sign extended as well. If the sign extend operation is thought of as extending both signs: 1 (negative) and 0 (positive), then the sign extending of 0 takes care of the clamp above. Or equivalently after the clamp above, only 1 (negative) needs to be extended.\n\nGiven the uncompressed endpoints, the next steps are to perform an initial unquantization step, interpolate, and then do a final unquantize. Seperating the unquantize step into two substeps reduces the number of multiplications required compared to doing a full unquantize before interpolating.\n\nThe code below illustrates the unquantizing process to retrieve estimates of the original 16 bit value, and then using the specified weights to get 6 additional values into the palette. The same operation is performed on each channel.\n\nSince the full range of the unquantize function is -32768 to 65535, the interpolator is implemented using 17 bit signed arithmetic.\n\nAfter interpolation, the values are passed to the finish_unquantize function, which applies the final scaling.\n\nAll hardware decompressors are required to return bit accurate results with this functon.\n\nThe following describes how unquantize works. For UF16, 'comp' is unquantized into 0x0000 ~ 0xFFFF range to maximize the usage of bits.\n\nfinish_unquantize is called after palette interpolation. The unquantize function postpones the scaling by 31/32 for signed, 31/64 for unsigned. This is needed to get the final value into valid half range(-0x7BFF ~ 0x7BFF) after the palette interpolation is completed to reduce the number of necessary multiplications. finish_unquantize applies the final scaling and returns an unsigned short value that gets reinterpreted into half.\n\nThe following DXGI_FORMATs are in this category: DXGI_FORMAT_BC7_TYPELESS, DXGI_FORMAT_BC7_UNORM, and DXGI_FORMAT_BC7_UNORM_SRGB\n\nThe BC7 format can be used for Texture2D (including arrays), Texture3D or TextureCube (incl. arrays). All of these uses include mipmap surfaces in these resources.\n\nBC7 uses a fixed block size of 16 bytes and a fixed tile size of 4x4 pixels. As with other BC formats, images larger than BC7's tile size are compressed using multiple blocks. The same addressing identity also applies to three-dimensional images as well as mip-maps, cubemaps, and texture arrays.\n\nBC7 compresses both three-channel and four-channel fixed-point data images. Typically source data will be 8-bits per component fixed point, although the format is capable of encoding source data with higher bits per component. All image tiles must be of the same format.\n\nThe BC7 decoder decompresses to the specified format prior to filtering.\n\nBC7 decompression hardware is required to be bit accurate; the hardware must give results that are identical to the decoder described in this specification.\n\nA BC7 block can take one of 8 modes, and the block mode is always stored in the LSBs of the 128-bit block. The block mode is encoded by zero or more \"0\"'s followed by a \"1\". This mode string starts from the block LSB.\n\nA BC7 block may contain multiple endpoint pairs. For the purposes of this document, the set of indices that correspond to an endpoint pair may be referred to as a subset.\n\nIn some block modes the endpoint representation is encoded in a form that for the purposes of this document will be called RGBP – in these cases the P bit represents a shared LSB for the components of the endpoint. For example, if the endpoint representation for the format was RGBP 5.5.5.1 then the endpoint would be interpreted as an RGB 6.6.6 value, with the LSB of each component being taken from the state of the P bit. If the representation was RGBAP 5.5.5.5.1 then the endpoint would be interpreted as an RGBA 6.6.6.6 value. Depending on the block mode the shared LSB may either be specified for both endpoints of a subset individually (2 P-bits per subset), or shared between the endpoints of the subset (1 P-bit per subset)\n\nFor BC7 blocks that do not explicity encode alpha, a BC7 block consists of mode bits, partition bits, compressed endpoints, sometimes a P-bit, and compressed indices. In these blocks the endpoints have an R.G.B-only representation and alpha is decoded as 1.0 for all texels\n\nFor BC7 blocks that encode combined color and alpha, a block consists of mode bits, sometimes partition bits, compressed endpoints, and compressed indices. In these blocks the endpoint color values are specified in an R.G.B.A format, and alpha values are interpolated along with the color values.\n\nFor BC7 blocks that separately encode color and alpha, a block consists of mode bits, rotation bits, sometimes an index selector bit, compressed endpoints, and compressed indices. These blocks effectively have a vector channel (R.G.B) and a scalar channel (A) separately encoded.\n\nBC7 defines a palette of colors on an approximate line between two endpoints. The mode specifies the number of interpolating endpoint pairs per block. BC7 stores one palette index per pixel.\n\nFor each subset of indices that corresponds to a pair of endpoints, the encoder fixes the state of one bit of the compressed index data for that subset. This is done by choosing an endpoint order that allows the index for the designated fixup index to have 0 as its MSB, which can therefore be discarded saving one bit per subset. The indices with the \"fix-up\" bit are noted in the partition tables for 2 subsets(19.5.14.5) and 3 subsets(19.5.14.6) below. For block modes with only a single subset, the fix-up index is always index 0.\n\nThe pseudocode below outlines the steps to decompress the pixel at (x,y) given the 16-byte BC7 block.\n\nThe pseudocode below outlines the steps to fully decode endpoint color and alpha for each subset given the 16-byte BC7 block.\n\nBelow is a list of the 8 block modes and bit allocations for the 8 possible BC7 blocks. The colors for each subset within a block are represented using two explicit endpoint colors and a set of interpolated colors between them. Depending on the block's index precision, each subset may have 4, 8 or 16 possible colors.\n\nMode 8 (LSB 0x00) is reserved and should not be used by the encoder. If this mode is given to the hardware, an all 0 block will be returned.\n\nAs previously discussed, in some block modes the endpoint representation is encoded in a form called RGBP – in these cases the P bit represents a shared LSB for the components of the endpoint. For example, if the endpoint representation for the format was RGBP 5.5.5.1 then the endpoint would be interpreted as an RGB 6.6.6 value, with the LSB of each component being taken from the state of the P bit. If the representation was RGBAP 5.5.5.5.1 then the endpoint would be interpreted as an RGBA 6.6.6.6 value. Depending on the block mode the shared LSB may either be specified for both endpoints of a subset individually (2 P-bits per subset), or shared between the endpoints of the subset (1 P-bit per subset)\n\nIn BC7, alpha can be encoded in several different ways:\n• Block types without explicit alpha encoding: In these blocks the endpoints have an R.G.B-only representation and alpha is decoded as 1.0 for all texels.\n• Block types with combined color and alpha: In these blocks the endpoint color values are specified in an R.G.B.A format, and alpha values are interpolated along with the color values.\n• Block types with separated color and alpha: In these blocks the alpha values and color values are specified separately, each with their own sets of indices. These blocks effectively have a vector channel (R.G.B) and a scalar channel (A) separately encoded. In these blocks a separate 2-bit field is also encoded that allows specification on a per-block basis of the channel that is encoded separately as a scalar, so the block can have 4 different representations – RGB|A, AGB|R, RAB|G, RGA|B. The channel order is swizzled back to RGBA after decoding, so the internal block format is transparent to the developer. Blocks with separate color and alpha also have two sets of index data – one for the vector channel and one for the scalar channel. In the case of Mode 4, these indices are of differing widths (3 or 2 bits). Mode 4 contains is a one-bit selector which chooses whether the vector or scalar data uses the 3-bit indices.\n\nIn the table of partitions above, the bolded, underlined entry is the location of the fix-up index for subset 1 which is specified with one less bit. The fix-up index for subset 0 is always index 0 (the partitioning is arranged so that index 0 is always in subset 0). Partition order goes from top-left to bottom right, walking left-to-right, then top-to-bottom.\n\nFor this table of partitions, underneath the entry in each subset, printed in bold and underlined, is the location of the fix-up index which is specified with one less bit. Index 0 always contains the fixed index bit for subset 0. Partition order goes from top-left to bottom right, walking left-to-right, then top-to-bottom.\n\nThree formats were added back to D3D11 which all existing GPUs support:\n\nRequired support for these formats depending on the hardware feature level:\n\nThere exists the need to retrieve other data from the graphics accelerator, other than an output RenderTarget or output vertex buffer. Considering the graphics accelerator executes in parallel with the CPU, an API is necessary to expose the asynchronous nature of communication with the graphics accelerator efficiently. As a degenerate case, any data retrieval which needs to occur in a synchronous fashion can use the same API.\n\nThe basic resource related to the asynchronous notification API is the Query. Each Query object instance will be in one of three states: \"signaled\", \"issued\", and \"building\". Transitions to \"building\" and \"issued\" are achieved by the application with the use of the Issue(20.3.4) command. Transitions back to the \"signaled\" state are detected by the driver during the GetData(20.3.5) command. When the Query is in the \"signaled\" state, the data is available to pass back to the application.\n\nWell-defined statistics for the Pipeline stages will be continuously calculated throughout the usage of the graphics accelerator. This typically indicates the need for hardware counters for each stage of the Pipeline. Such counters would be associated with the graphics context, so they require the ability to be context switched. Typically, drivers use the standard graphics Pipeline available on the graphics accelerator in order to implement some sort of functionality. For example, a Blt may actually be implemented as a textured quad rendering. In such a case, the graphics accelerator should not calculate statistics for such an operation. For example, such an emulated Blt operation should not appear to draw 2 triangles. This indicates the graphics accelerator needs to be able to toggle actual statistics calculation in an efficient manner. Most important, the graphics Pipeline should run at the same speed regardless of whether statistics are calculated or not; as the hardware counters will be expected to always be tabulating (except as previously mentioned, where the tabulation should be muted when performing emulation). The Pipeline statistics will be collected through the asynchronous notification mechanism. Note that D3D11_QUERY_OCCLUSION and D3D11_QUERY_SO_STATISTICS are considered to be well-defined Pipeline statistics, even though it is kept separate from D3D11_QUERY_DATA_PIPELINE_STATISTICS.\n\nRendering and draw operations are able to be predicated from the command stream, including Clear, UpdateSubresourceUP, CopySubresourceRegion, CopyResource. During Query creation, the Predicate Query is specified as to whether future predication must be guaranteed to execute by the presence of a flag. So, there are guaranteed predicates and separate predication hints. Allowing a guaranteed predicated rendering operation to proceed because of timing issues is unacceptable. However, a predicated rendering operation can proceed because of timing issues if a predication hint is used. In addition, hints will not be able to return any data to the application, like other queries and predicates will. Predicate Queries are introduced through the asynchronous notification mechanism, and all have the same data type: BOOL. In general, Predicates use the bracketing mechanism of Queries to generate a predicate BOOL value. This value can then be used to predicate drawing commands. It should be noted that one can generate a predicate value with a predicated rendering operation, as long as the Predicates involved are not the same. However, the Issue command is not able to be predicated. In addition, state modification operations, Present, Map/ Lock, and naturally Creates are not affected by the predication, so something like changing the RenderTarget always occurs even if within a predication range.\n\nAll the rest of the operations do not honor predication. Here's a non-comprehensive list of such operations, for clarity:\n• CreateResource (CreateResource takes a UP pointer in order to initially load data, which is the only way IMMUTABLE Resources are populated. This initial load operation, roughly equivalent to UpdateSubresourceUP, does NOT honor predication)\n\nQueryHandle is a non-zero handle which indicates the handle of a newly created Query. During creation, each Query is associated with a Type (D3D11_QUERY) parameter which defines what type of Query to make, for the entire lifetime of the Query. The QueryType indicates which question is being asked of the graphics accelerator or driver. It determines the size and type of data that will be returned to the application. It also determines which D3DISSUE flags can be used, along with whether the Query can be used for predication(20.2). Since Query creation implies memory allocation, the application is expected to optimize and reuse Query objects. Infinite Queries instances need to be supported. Realistically, the number of outstanding Queries will probably be limited more by video & AGP memory size than by system memory. CreateQueryFlags is typically zero. It can have a bit set (D3DCREATEQUERY_PREDICATEHINT) when the Type is a PREDICATE, in order to indicate that the Predicate is a hint. The driver should return the appropriate failure if there is insufficient resources for the Query, or if the any of parameters are invalid. A newly created Query will start out in the \"signaled\" state.\n\nQueryHandle is a non-zero handle and has previously been \"created\" and indicates that all resources associated with the Query are to be destroyed. A Query can be deleted while in any state. When the Query is in the \"building\" or \"issued\" states and is deleted, the Query is referred to as abandoned.\n\nQueryHandle is a non-zero handle and has previously been \"created\". Issue is used by application to cause transitions to the \"building\" and \"issued\" states. Passing IssueFlags with the D3DISSUE_END bit set causes the Query to enter the \"issued\" state. From the \"issued\" state, the driver and graphics accelerator can cause the transition back to the \"signaled\" state. Passing IssueFlags with only the D3DISSUE_BEGIN bit set causes the Query to enter the \"building\" state (regardless of whatever state it was in before). A second D3DISSUE_BEGIN will result in the range being reset (the first D3DISSUE_BEGIN is effectively discarded/ ignored). Some Query Types only support D3DISSUE_END. When the Query is in the \"signaled\" state, the Query Type supports D3DISSUE_BEGIN, and Issue is invoked with just the D3DISSUE_END flag: it is equivalent to invoking Issue with both D3DISSUE_BEGIN and D3DISSUE_END bits set, as well as being equivalent to an invocation of Issue with D3DISSUE_BEGIN followed immediately by another invocation of Issue with D3DISSUE_END. Issue with no IssueFlags bits set is invalid.\n\nThe valid usage of the IssueFlags (BEGIN and END) define a bracketing of graphics commands. Bracketings of Queries are allowed to overlap and nest.\n\nGetData asks the driver what state the Query is in, typically to detect when the Query transitions from the \"issued\" state to the \"signaled\" state. Returning S_OK indicates the Query is \"signaled\", while returning S_FALSE indicates the Query is still in the \"issued\" state. If the Query is \"signaled\", the data associated with the Query is expected to be returned/ copied out through the pData parameter.\n\nNote: GetData must also not block until query reaches a \"signaled\" sate. It should return immediately indicating the \"issued\" state if the query is not yet \"signaled\". WGF11Async helps validate this behavior.\n\nFurthermore, all Queries of the same D3D11_QUERY are FIFO (first-in, first-out); however, queries of different types can complete or signal in an overlapping order. For example, a Query of type EVENT can complete before a Query of type OCCLUSION, even if the EVENT were issued after the OCCLUSION was issued. But, all Queries of type EVENT (or any other D3D11_QUERY) complete in FIFO order based off of their issued order.\n\nSetPredication is used to denote that the following drawing commands are predicated if the result of the Query associated with the QueryHandle is equal to the passed-in bPredicateValue. This allows an application to predicate rendering when the predicate results either in TRUE or FALSE. A QueryHandle of zero is reserved to indicate \"no predication\", and is the default state after Device creation. The bPredicateValue parameter is ignored when designating \"no predication\". The Query associated with the QueryHandle must be in the \"issued\" or \"signaled\" state; and while the Query is set for predication, Issue commands against it are invalid.\n\nThe following is the list of queries that must be supported:\n\nThis type provides a synchronization primitive that many of the following Queries mimic to deal with the asynchronous nature of the GPU. An issued EVENT becomes \"signaled\" after the GPU is finished with all of the previously issued commands, generally from the backend of the graphics Pipeline. The data associated with this Query is a BOOL, but the BOOL value is redundant, as whenever an EVENT query is \"signaled\", the value of the BOOL is always TRUE. The driver should always send back the BOOL data value of TRUE when signaling the EVENT.\n\nTIMESTAMP functions similar to EVENT, as it is another type of synchronization primitive. Like EVENT, TIMESTAMP should become \"signaled\" when the GPU is finished with all the previously issued workload. However, TIMESTAMP differs from EVENT by returning a 64-bit timestamp value. This 64-bit timestamp value should be sampled from a GPU counter, which increments at a consistent frequency. The value should be sampled at the instant that the GPU is finished with all the preceding workload. The GPU need not ensure that all caches are flushed to memory to realize work as \"done\". This is so that satisfying multiple high-frequency TIMESTAMPs does not heavily disturb the pipeline. However, attention to well-defined memory write-ordering should be given between the CPU and GPU, especially when thinking of supporting EVENT. If the CPU were to realize that the GPU wrote a certain value (especially a fence value), the CPU would assume all previous memory writes issued prior to the fence write should be flushed to memory and able to be seen immediately by the CPU. The type of flush that may be required to get data out of GPU caches and into CPU visible memory should not need to be done every TIMESTAMP; but probably more at the end of every command buffer.\n\nThe frequency of the counter is provided within the context of a TIMESTAMP_DISJOINT Query. The frequency of this counter should be greater than 10 MHz, and be resistant to high-frequency dynamic throttling of the GPU. See TIMESTAMP_DISJOINT for more details. The counter should be global, so does not need to take into account the GPU time slicing of contexts.\n\nThe initial value of the counter is unspecified, so the absolute value of the counter is generally meaningless by itself. However, the relative value generated from the difference of two absolute values quantifies an elapsed amount of time. The difference of two timestamp values is only accurate when the two TIMESTAMP Queries are bracketed within a TIMESTAMP_DISJOINT range; and the Query Disjoint value of the TIMESTAMP_DISJOINT Query returns FALSE.\n\nTIMESTAMP_DISJOINT allows a bracketing to be defined by the application to not only request the frequency of the TIMESTAMP clock, but also to detect if that frequency were consistent throughout the entire bracketed range of graphics commands. The Disjoint member variable, essentially, detects when something has caused the TIMESTAMP counter to become discontinuous or disjoint. A few examples of an event which should trigger TIMESTAMP_DISJOINT are a power down, or throttling up/ down due to laptop power saving events, an unplugged AC cord, or overheating. Such occurrences should be rare enough during a steady graphics application execution state to be avoided by controlling the system execution environment. Keep in mind that if such events occur, they effectively reduce the usefulness of the TIMESTAMP functionality. After an event which would trigger a TIMESTAMP_DISJOINT query, proceeding TIMESTAMP queries after such an event are not expected to be meaningful compared to TIMESTAMP queries preceding such an event. The value associated with the Disjoint member variable is a BOOL, which should be TRUE if the values from TIMESTAMP queries cannot be guaranteed to be continuous throughout the duration of the TIMESTAMP_DISJOINT query. Otheriwse, the result should be FALSE. Naturally, the value of the Frequency member variable should be equal to the frequency of the TIMESTAMP clock.\n\nA new type of EVENT Query is introduced: DEVICEREMOVED. DEVICEREMOVED will function similar to EVENT, as it is another type of synchronization primitive. Like EVENT, DEVICEREMOVED should become \"signaled\" when the GPU is effectively removed from the system. Since the physical device has been removed from the system, it can no longer be utilized; and resources may no longer be able to be accessed (since they may have existed in video memory). While the software objects associated with this device will appear to continue to operate normally, they will all be in the state of silent failure. Only a few entry points will actually return this type of status as an error condition, specifically when an application should be made aware of the fact.\n\nThe data associated with this Query Type is a UINT64. This value contains the number of multisamples which passed depth and stencil testing, also known as \"visible\" multisamples, for all primitives since the creation of the device context. If the rendertarget is not multisampled, then the counter, naturally is incremented by the number of whole pixels that are \"visible\". The counter should wrap around when it overflows. Note that this statistic can be requested at any time, so it must be continually calculated accurately. See Pipeline Statistics(20.1). Naturally, though, only the difference between two independant statistic requests will provide meaningful information; and the driver will be asked to calculate the difference between two requests (one request for Issue( BEGIN ), and one request for Issue( END )).\n\nFor the purposes of calcuating visible multisamples, disabled depth tests or stencil tests should behave as if the multisamples \"passes\" the disabled test. This produces equivalent results as if the test units were enabled with the test function set to \"always\". In addition, these values should be tabulated as normal even if there are no render targets bound. Since the Depth and Stencil tests logically occur in the Output Merger stage of the pipeline, pixels which are discarded during Pixel Shader execution, naturally, do not increment this counter. Discarded pixels, logically, do not even reach the Output Merger. There are pipeline configurations where the only effective results that are produced from the pipeline is the tabulation of the occlusion counter. This is intentional.\n\nIf ForcedSampleCount(3.5.6.1) is used (> 0) recall that the pass count reflects how many rasterizer samples are covered (indepenent of the output sample count). If SampleMask (which applies to the output) is configured to turn off output writes (or pixel discard, output coverage mask or alpha-to-coverage turns off all output samples), the count of samples recorded into the query may be either 0 or the number of rasterizer samples covered, as the specific behavior was never tightly specified. It is recommended for implementations to count 0 in this case for consistency with known implementations.\n\nThe data associated with this Query Type is D3D11_QUERY_DATA_PIPELINE_STATISTICS. This structure contains statistics for each stage of the graphics Pipeline. For each stage, the value for number of invocations must fall between two numbers: infinite cache & no cache. The clipper will appear to behave as the GS. The clipper will execute for each triangle. For each invocation, 0 primitives will be generated if the original triangle is fully clipped, 1 primitive will be generated if the original triangle is not clipped at all (or the clipping results in only 1 triangle), 2 primitives will be generated if the original triangle were clipped and resulted in 2 triangles, etc. In typical configurations of the pipeline, GSPrimitives would be equal to CInvocations. If rasterization is disabled(15.2) and the pipeline is configured to only send primitives to Stream Output, GSPrimitives would naturally deviate from CInvocations, since CInvocations would not increment.\n\nThe clipping stats will be flexible with regards to guard band implementations. So, when rendering triangles that extend beyond the viewport, the tests will ensure clipping falls between a range of values (numbers assuming an infinite guard band; and numbers assuming a tight clipping rect around the viewport). All the values contain the number of events since the creation of the device context. Note that these statistics can be requested at any time, so it must be continually calculated accurately. See Pipeline Statistics(20.1). Naturally, though, only the difference between two independant statistic requests will provide meaningful information; and the driver will be asked to calculate the difference between two requests (one request for Issue( BEGIN ), and one request for Issue( END )).\n\nHere's some examples of the interaction between the IAVertices, IAPrimitives, and VSInvocations with respect to Post-VS caching\n• Draw Indexed Tri Strip of 4 prims (with all indices the same value): valid IAVertices only 6, valid IAPrimitives only 4, valid VSInvocations 1 - 12.\n• Draw Indexed Tri List of 4 prims (with all indices the same value): valid IAVertices only 12, valid IAPrimitives only 4, valid VSInvocations 1 - 12.\n• Draw Tri Strip of 4 prims: valid IAVertices only 6, valid IAPrimitives only 4, valid VSInvocations 6 - 12\n• Draw Tri List of 4 prims: valid IAVertices only 12, valid IAPrimitives only 4, valid VSInvocations only 12\n\nPartial primitives will be allowed to fall within range of values, similar to the way vertex caching behaves. So, when partial primitives are possible, statistics should fall between a pipeline that clips them as soon as possible (before even the IA counts them), or as late as possible (post clipper/ pre-PS). Stream Output and a NULL GS is flexible as to whether it actually causes GS invocations to occur or not.\n\nThe value of PSInvocations may include or exclude helper pixels(3.5.7) for 2x2 stamps.\n\nWith respect to PSInvocations, early Depth/ Stencil optimizations may or may not prevent the work from the pixel shader from being realized. So, when pixels fail the depth tests, PSInvocations may or may not be incremented depending on where the Depth test is actually occuring in the pipeline. If the Pixel Shader outputs depth, then PSInvocations must increment as expected, even if the output depth fails. The following is an example of how PSInvocations will be tested: Consider the quantities DSP (number of pixels that pass the Depth and Stencil tests) and DSF (number of pixels that fail either the Depth or Stencil tests). DSP is roughly equivalent to the OCCLUSION Query, except that OCCLUSION measures multi-samples (not pixels). In all cases, DSP <= PSInvocations <= ( DSP + DSF ). When the Pixel Shader outputs depth, PSInvocations = ( DSP + DSF ). In addition, when a NULL pixel shader is bound to the pipeline, PSInvocations does not increment.\n\nWith respect to IAVertices and VSInvocations, adjacent vertex processing may be optimized out if the GS does not declare the adjacency vertices as inputs to the GS. So, when the GS does not declare adjacent vertices as inputs, IAVertices and VSInvocations may or may not reflect the work implied by the adjacent vertices. If the GS declares adjacent vertices, then the IAVertices should include the adjacent vertices (with no regard to any post-VS caching); and VSInvocations should include the adjacent vertices (along with any effects of post-VS caching).\n\nHSInvocations increments once per patch that causes the Hull Shader to run.\n\nFor the DSInvocations statistic, note that hardware may generate identical points in a patch multiple times in the course of tessellating the domain, and each repeated point counts as an additional DSInvocation. If the Tessellator's output primitive is points (as opposed to triangles or lines), that scenario requires only unique points within a patch to be generated, so the DSInvocations count will increment by exactly the number of unique points tessellated for the patch. The one exception is points that are on the threshold of merging, if TessFactors were to incrementally decrease, may appear in the system as duplicated points (with the same U/V coords) in an implementation dependent way.\n\nCSInvocations: For example, if a Compute Shader is declared with a thread group size of (3,4,5), a Dispatch(2,1,1) call would increment the CSInvocations value by 3*4*5*2*1*1=120.\n\nCSInvocations must honor Compute Shader invocations from both Dispatch() and DispatchIndirect() APIs.\n\nSince the Compute Shader honors predicated rendering, if a Dispatch() or DispatchIndirect() call is predicated off, then CSInvocations will not increment, given the Compute Shader will not be invoked.\n\nThe data associated with this Query Type is a BOOL. This Predicate mirrors the specification for the OCCLUSION Query. If the OCCLUSION Query for the same bracketed range would return 0, the OCCLUSION Predicate would return FALSE. Otherwise, the OCCLUSION Predicate would return TRUE, indicating that at least one multisample is \"visible\". If the Predicate has been indicated to be a hint versus guaranteed, then no result is ever propagated back to the application. This Query is a Predicate and can be used to predicate rendering commands.\n\nThe data associated with each of the Query Types D3D10_QUERY_SO_STATISTICS, D3D11_QUERY_SO_STATISTICS_STREAM0... _STREAM3 is D3D11_QUERY_DATA_SO_STATISTICS. D3D10_QUERY_SO_STATISTICS is a synonym for D3D11_QUERY_SO_STATISTICS_STREAM0 (in D3D10 there was only a single stream, so going forward it is equivalent to _STREAM0). This structure contains statistics for monitoring the amount of data streamed out to the given Stream at the Stream Output(14) stage of the Pipeline. Only complete primitives (e.g. points, lines or triangles) are Streamed Out, as counted by these stats. Should the primitive type change (e.g. lines to triangles), the counting is not adjusted in any way; the count is always total primitives, regardless of type. Note that these statistics can be requested at any time, so it must be continually calculated accurately. See Pipeline Statistics(20.1). Naturally, though, only the difference between two independant statistic requests will provide meaningful information; and the driver will be asked to calculate the difference between two requests (one request for Issue( BEGIN ), and one request for Issue( END )).\n\nThe data associated with each Query Type D3D11_QUERY_SO_OVERFLOWPREDIATE, and D3D11_QUERY_SO_OVERFLOW_PREDICATE_STREAM0 ... SO_OVERFLOW_PREDICATE_STREAM3 is a BOOL. This BOOL will be TRUE if the given stream (_STREAM#) overflowed, or in the case of SO_OVERFLOW_PREDICATE the BOOL is TRUE if any of the 4 Streams overflowed. If two D3D11_QUERY_SO_STATISTICS_* were used to simultaneously monitor the same bracketed range as an OVERFLOW_PREDICATE*, the PrimitiveStorageNeeded difference would have resulted in a larger difference than the NumPrimitivesWritten difference. The OVERFLOW_PREDICATE Predicate type does not support the ability to be used as a hint; so must be guaranteed. Naturally, this Query is a Predicate and can be used to predicate rendering commands, preventing what is probably a garbage frame from being shown to the application.\n\nHardware always writes complete primitives to Buffers. If multiple Buffers are bound to a Stream and an output primitive will not fit into any one of the Buffers, writes to all of the Buffers bound to that Stream are stopped, while counters continue indicating how much storage would have been needed continue to increment. If multiple Streams are being used, and output to a given Stream’s Buffers have been halted because one of its Buffers is full, this does not affect output to other Streams.\n\nIn general, the following optional features exist to help quickly determine bottlenecks and identify the performance characteristics of an application running on a particular graphics adapter. These optional features expect to leverage any hardware counters that can divulge any interesting performance information. Since the existance of these counters and what they actually measure is also highly dependent on the graphics adapter, they are exposed in a flexible manner, where the primary consumer is expected to be some type of profiling application. The profiling application will then present such information to the user. The mechanism for using counters will most likely be exposed in the Asynchronous Notification, as optional statistics with special properties.\n\nCounter IDs, like Asynchronous Notification Query IDs, uniquely identify each type of counter. However, the driver publishes it's own Counter IDs, along with describing what the counter measures, in what units, and what data type and size the counter is.\n\nIt is not expected that it is possible for an application to measure from each and every Counter ID simultaneously. For example, an architecture may have hundreds of different possible native counters to measure; but only two of these hundreds may actually be monitored simultaneously. The number of Simultaneously Active Counters is published by the driver as part of the adapter capabilities. Additionally, the driver must indicate the number of active counters used by monitoring each supported Counter ID. For example, the driver may indicate that monitoring FillRateUtilized requires three of the maximum four Simultaneously Active Counters. The application may try to also monitor another Counter ID, as long as the number of active counters it requires is one or less. If a Counter ID may always be monitored (and does not interfere with monitoring any other Counter IDs), the number of simultaneous active counters required by the Counter ID may be zero to indicate such.\n\nOnly one Device Context may monitor any Counter IDs that require one or more of the Simultaneously Active Counters. The first creation of a Counter ID that requires one or more of the Simultaneously Active Counters denotes the request for Counter ID exclusivity. If another Device Context is currently monitoring Counters, the driver may fail with an error indicating such a condition. The actual DDI may actually assist the user mode driver with this concept.\n\nThis feature tries to solve the problem of enabling “real-time, low overhead” GPU performance data gathering and at the same time, provide enough information to measure when an API call was made by an application and exactly when it was rendered on the GPU, even using multiple engines. The goal is to also have enough information to reconstruct the exact order of operations executed by the GPU, so that tools can accurately identify shared surface ownership and potential synchronization issues in D3D applications.\n\nOut of the following set of goals, the Priority 1 goals were addressed initially, and the Priority 2 goals are ideals (possibly for future releases).\n• A high resolution GPU timestamp, meaning a frequency of 12.5 MHz (80ns resolution) or greater. This is required for all GPU hardware at all feature levels.\n• At least 32-bits of timestamp resolution to prevent multiple rollovers within a command buffer. This is required for all GPU hardware at all feature levels.\n• An invariant timestamp, which is not affected by p-state transitions. This is required for all GPU hardware when the maximum supported feature level of the device is 10.0 or greater.\n• The ability to sample the GPU timestamp from all engines. This is required for all GPU hardware at all feature levels.\n• The ability to sample the GPU timestamp at the end of the GPU pipeline. This is required for all GPU hardware at all feature levels.\n• The ability to sample the GPU timestamp via the CPU through MMIO to accurately calibrate the GPU timestamps against CPU-accessible timers. This is required for all GPU hardware at all feature levels.\n\nMicrosoft may drive toward these goals by enforcing greater capabilities using methods like the addition of feature levels over future Windows releases and HCK tests.\n• For this feature, the ability to write out the low 32-bits to memory and fully utilize that precision is most ideal.\n• Enough GPU timer resolution so that individual primitives in a RenderCb will each be given different time stamps.\n• Efficiency Improvements to reduce the overhead of logging many events and reduce the effects of the Heisenberg uncertainty principle.\n• In the wild, Windows Desktop applications have been reported to issue ~15,000 Draw calls per 60Hz frame as of this writing, and each of those Draw calls would generate a timestamp or pair of timestamps when instrumented. Our goals are to evolve Windows Desktop to eventually support ~100,000 Draw calls per 60Hz frame and incur no more than 5% overhead while instrumented.\n\nThese requirements apply to all WDDM 1.3 drivers.\n• Implement a DDI that leverages the above hardware requirements to sample from the GPU timestamp which correlates with previously issued graphics commands.\n• Implement a DDI that turns the instrumentation on & off, at any time. The instrumentation must default to off to avoid any performance impact when not profiling.\n• The ability for the driver to insert custom attributed timestamps and annotations into the same sequence of timestamps requested of the driver via the SetMarker DDI.\n• The overhead of using the ETW technique is no slower than using the Timestamp Query technique exposed in the existing D3D9 and D3D10+ DDI.\n• D3D9 driver support is required unless the hardware supports feature level 10+\n• Tile Based Deferred Renderers may require the driver to do additional processing in order to make the renderer appear like an immediate mode renderer when profiling.\n\nMicrosoft may drive toward these goals by enforcing greater capabilities using methods like the addition of feature levels over future Windows releases and HCK tests.\n• Ideally zero processing of the counter data by the runtime or IHV driver. Given the volume of data being collected, we cannot afford to require “fixups” or manipulation of the data as this would be too expensive.\n• Efficiency Improvements to reduce the CPU overhead of logging many events and reduce the effects of the Heisenberg uncertainty principle.\n• In the wild, Windows Desktop applications have been reported to issue ~15,000 Draw calls per 60Hz frame today, and each of those Draw calls would additionally call a new DDI when instrumented. Our goals are to evolve Windows Desktop to eventually support ~100,000 Draw calls per 60Hz frame and incur no more than 5% overhead while instrumented. Some Desktop systems are capable of ~250,000 Draw calls per 60Hz frame, if that’s all they do.\n\nThis section lists several numerical system limits in the D3D . graphics system. It is not an exhaustive list yet (some limits are inherently implied by other parts of the spec, such as the tables describing the registers available in the shaders, though in a few cases they show up duplicated in this section).\n\nNote about the number of texels in a Buffer (listed above as 2 Texels). Since the format type which defines and element, or texel, is only assigned when a View of a Buffer is created, this limit only applies to the creation of Views. D3D11 has a couple of new classes of Buffers – Raw and Structured(5.1.3) buffers. Structured buffer Views are held the the 2 limit (how many structures are allowed in the view). Raw Buffer Views, however, are not subject to the 2 texel limit – Raw views, which have no type, but are addressed at 32-bit granularity, can span the entire size of a Buffer – where the size of a Buffer is only constrained by the maximum resource size formula above.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 22.1.1 Summary of All Stages\n\n 22.1.2 Instructions Common to All Stages\n\n\n\nsync[_uglobal|_ugroup][_g][_t] (Synchronization)(22.17.7)\n\n atomic_and (Atomic Bitwise AND To Memory)(22.17.8)\n\n atomic_or (Atomic Bitwise OR To Memory)(22.17.9)\n\n atomic_xor (Atomic Bitwise XOR To Memory(22.17.10)\n\n atomic_cmp_store (Atomic Compare/Write To Memory)(22.17.11)\n\n atomic_iadd (Atomic Integer Add To Memory)(22.17.12)\n\n atomic_imax (Atomic Signed Max To Memory)(22.17.13)\n\n atomic_imin (Atomic Signed Min To Memory)(22.17.14)\n\n atomic_umax (Atomic Unsigned Max To Memory)(22.17.15)\n\n atomic_umin (Atomic Unsigned Min To Memory)(22.17.16)\n\n imm_atomic_alloc (Immediate Atomic Alloc)(22.17.17)\n\n imm_atomic_consume (Immediate Atomic Consume)(22.17.18)\n\n imm_atomic_and (Immediate Atomic Bitwise AND To/From Memory)(22.17.19)\n\n imm_atomic_or (Immediate Atomic Bitwise OR To/From Memory)(22.17.20)\n\n imm_atomic_xor (Immediate Atomic Bitwise XOR To/From Memory(22.17.21)\n\n imm_atomic_exch (Immediate Atomic Exchange To/From Memory)(22.17.22)\n\n imm_atomic_cmp_exch (Immediate Atomic Compare/Exchange To/From Memory)(22.17.23)\n\n imm_atomic_iadd (Immediate Atomic Integer Add To/From Memory)(22.17.24)\n\n imm_atomic_imax (Immediate Atomic Signed Max To/From Memory)(22.17.25)\n\n imm_atomic_imin (Immediate Atomic Signed Min To/From Memory)(22.17.26)\n\n imm_atomic_umax (Immediate Atomic Unsigned Max To/From Memory)(22.17.27)\n\n imm_atomic_umin (Immediate Atomic Unsigned Min To/From Memory)(22.17.28)\n\n\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 22.3.1 Overview\n\n 22.3.2 Global Flags Declaration Statement\n\n 22.3.3 Constant Buffer Declaration Statement\n\n 22.3.4 Immediate Constant Buffer Declaration Statement\n\n 22.3.5 GS Maximum Output Vertex Count Declaration\n\n 22.3.6 GS Input Primitive Declaration Statement\n\n 22.3.7 GS Instance ID (GS Instancing) Declaration Statement\n\n 22.3.8 GS Output Topology Declaration Statement\n\n 22.3.9 GS Stream Declaration Statement\n\n 22.3.10 Input Attribute Declaration Statement\n\n 22.3.11 Input Attribute Declaration Statement w/System Interpreted or System Generated Value\n\n 22.3.12 Input Resource Declaration Statement\n\n 22.3.13 Input Primitive Data Declaration Statement\n\n 22.3.14 HS Declarations Phase Start\n\n 22.3.15 Tessellator Output Primitive Declaration\n\n 22.3.16 Tessellator Domain Declaration\n\n 22.3.17 Tessellator Partitioning Declaration\n\n 22.3.18 Hull Shader Input Control Point Count Declaration\n\n 22.3.19 Hull Shader Output Control Point Count Declaration\n\n 22.3.20 MaxTessFactor Declaration\n\n 22.3.21 HS Control Point Phase Start\n\n 22.3.22 HS Input OutputControlPointID Declaration\n\n 22.3.23 HS Fork Phase Start\n\n 22.3.24 HS Input Fork Phase Instance Count\n\n 22.3.25 HS Input Fork Instance ID Declaration\n\n 22.3.26 HS Join Phase Start\n\n 22.3.27 HS Input Join Phase Instance Count\n\n 22.3.28 HS Input Join Instance ID Declaration\n\n 22.3.29 Input Cycle Counter Declaration (debug only)\n\n 22.3.30 Input/Output Indexing Range Declaration\n\n 22.3.31 Output Attribute Declaration Statement\n\n 22.3.32 Output Attribute Declaration Statement w/System Interpreted Value\n\n 22.3.33 Output Attribute Declaration Statement w/System Generated Value\n\n 22.3.34 Sampler Declaration Statement\n\n 22.3.35 Temporary Register Declaration Statement\n\n 22.3.36 Indexable Temporary Register Array Declaration Statement\n\n 22.3.37 Output Depth Register Declaration Statement\n\n 22.3.38 Conservative Output Depth Register Declaration Statement\n\n 22.3.39 Output Mask Register Declaration Statement\n\n 22.3.40 dcl_thread_group (Thread Group Declaration))\n\n 22.3.41 dcl_input vThread* (Compute Shader Input Thread/Group ID Declarations)\n\n 22.3.42 dcl_uav_typed[_glc] (Typed UnorderedAccessView (u#) Declaration)\n\n 22.3.43 dcl_uav_raw[_glc] (Raw UnorderedAccessView (u#) Declaration)\n\n 22.3.44 dcl_uav_structured[_glc] (Structured UnorderedAccessView (u#) Declaration)\n\n 22.3.45 dcl_tgsm_raw (Raw Thread Group Shared Memory (g#) Declaration)\n\n 22.3.46 dcl_tgsm_structured (Structured Thread Group Shared Memory (g#) Declaration)\n\n 22.3.47 dcl_resource_raw (Raw Input Resource (Shader Resource View, t#) Declaration)\n\n 22.3.48 dcl_resource_structured (Structured Input Resource (Shader Resource View, t#) Declaration)\n\n 22.3.49 dcl_function_body (Function Body Declaration)\n\n 22.3.50 dcl_function_table (Function Table Declaration)\n\n 22.3.51 dcl_interface/dcl_interface_dynamicindexed (Interface Declaration)\n\n\n\nThe following statement types must precede other instructions.\n\nSection Contents\n\n\n\n(back to chapter)\n\n\n\n 22.7.1 Branch based on boolean condition: if_condition\n\n 22.7.2 else\n\n 22.7.3 endif\n\n 22.7.4 loop\n\n 22.7.5 endloop\n\n 22.7.6 continue\n\n 22.7.7 continuec (conditional)\n\n 22.7.8 break\n\n 22.7.9 breakc (conditional)\n\n 22.7.10 call\n\n 22.7.11 callc (conditional)\n\n 22.7.12 case (in switch)\n\n 22.7.13 default (in switch)\n\n 22.7.14 endswitch\n\n 22.7.15 label\n\n 22.7.16 ret\n\n 22.7.17 retc (conditional)\n\n 22.7.18 switch\n\n 22.7.19 fcall fp#[arrayIndex][callSite]\n\n 22.7.20 \"this\" Register\n\n\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nBeware of the two allowed implementations of divide: a/b and a*(1/b).\n\nOne outcome of this is there are exceptions to the table below for large denominator values (greater than 8.5070592e+37), where 1/denominator is a denorm. Since implementations may perform divide as a*(1/b), instead of a/b directly, and 1/[large value] is a denorm that could get flushed, some cases in the table would produce different results. For example (+/-)INF / (+/-)[value > 8.5070592e+37] may produce NaN on some implementations, but (+/-)INF on other implementations.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following table shows the results obtained when executing the instruction with various classes of numbers, assuming that neither overflow or underflow occurs.\n\nThe following features of D3D10.x are not available as of D3D11:\n\nD3D10_FILTER_MONO_1BIT filter type removed from the enum for D3D11 texture filter modes. This feature was never adopted in D3D10.\n\nIn the Performance Monitoring and Counters(20.5) section, removed the optinal Microsoft defined counters that were defined in D3D10 but never adopted. Hardware vendors can continue to optinally expose hardware-specific counters in D3D11.\n\nThere is a subtle change in how a couple of the Rasterizer State(3.5.2) members are interpreted from D3D10 to D3D10+, discussed here: State Interaction With Point/Line/Triangle Rasterization Behavior(15.14).\n\nThe following features of D3D9 are not available as of D3D10:\n• Triangle fans. The application must convert any existing content to use lists or strips on their own. Some emulation for older APIs can be done if needed at least for DrawPrimitive() fans, using D3D10+'s DrawIndexed().\n• W Buffering. Hardware support is sparse, and with high precision depth buffers, the need for W Buffers is not very great.\n• D3DFILLMODE point. D3D10+ has solid and wireframe only, and GS can emulate point mode.\n• BOTHSRCALPHA and BOTHINVSRCALPHA D3D10_BLEND modes. These are redundant.\n• SeparateAlphaBlendEnable toggle. This is now always enabled (so alpha and color blend are programmed separately).\n• Dithering of data written to the renderTarget is no longer supported.\n• Pointsprites. The Geometry Shader can handle this.\n• \"None\" as a mip filtering mode removed. This can be achieved by using a texture with only a single mipmap, or by setting the MaxLOD Sampler State to 0.\n• Clip planes in the DX9 sense. In their place, up to 8 components in up to 2 elements of vertex attributes can be declared as clip distances or cull distances.\n• texldp instruction from DX9 Shader models removed in D3D10+. An application can achieve projected texture load with a couple of extra Shader instructions.\n• Pre-D3D10+ Block Compression Formats mapped to D3D10+ Formats:\n• Note that the distinction between pre-multiplied alpha or non-premultiplied alpha (DXT2 vs. DXT3 and DXT4 vs. DXT5) is no longer made in D3D10+, since the concept has no meaning (never had meaning) in the graphics system. e.g. hardware always treated DXT2 identically to DXT3 and DXT4 identically to DXT5.\n• Legacy NT GDI line rasterization rules discarded in favor of new rules which have cleaner properties and are simpler to define. Along with this, the LastPixel control for lines has been removed.\n• D3D10+ does not have a mechanism for allowing IHVs to expose new formats on their own.\n\nThe following table lists how D3D9 formats map to formats(19.1) in D3D10+, if at all. Note, that this table only is true about the effective format definitions for little-endian host CPU systems. The D3D10+ specification for formats has diverged from the D3D9 format definitions, as a response to merging the vertex and texture formats and desiring a cross-endianness solution.\n\nMany numbers appearing in this spec link to constants defined in the table below. These constants are made available to applications via D3D headers."
    },
    {
        "link": "https://vfxdoc.readthedocs.io/en/latest/textures/sampling",
        "document": "Texture sampling is the process of reading textures through the GPU. Graphics Hardware embeds a set of texture units that are able to read texture pixels directly or sample these textures using different algorithms.\n\nTexture sampling options are not texture options per-se but more configuration on how the shader needs to read the texture using its texture sampler.\n\nAddressing Mode is the configuration of the sampler that handles the behavior out of the 0..1 UV range. Different Wrap modes can be set per-axis : Repeat (Wrap) , Clamp or Mirror.\n\nFiltering modes is the way your pixels will behave while being drawn on screen. Old devices (PlayStation 1) were not able for instance to perform texture filtering in order to blend one pixel color to another, the lack of filtering induced an effect of all pixels being visible. Modern engines can allow you to disable filtering at all what is called point filtering.\n\nBilinear filtering is a long known method that will blend the pixels of a texture horizontally and vertically so the transition between one pixel and another is smooth.\n\nBilinear Filtering is extremely fast but can lead to artifacts when used on objects set in perspective. Because the Graphics card will switch to lower Mip-Maps in the distance (see Texture Sampling section in shaders) : the transition between a Mip and another can appear hard as a visible line in the depth of your scene such as in the example below. (Some mip fading to plain gray were added in order to highlight this artifact even more).\n\nTrilinear Filtering adds another dimension to the texture filtering by blending the already blended pixels from one Mip to another. This method is a bit more expensive but today well assumed for textures that need to be tiled over large distances.\n\nAnisotropic Filtering enhances Bilinear and Trilinear filtering by increasing the samples used for the texture sampling in order to enhance the grazing angle blurring that occurs automatically when the GPU tries to fetch the correct mip-map based on view angle and distance.\n\nOften, the engine lets you set a sRGB/Linear flag on your textures. This flag is mainly used for texture samplers for reading color values. As your engine processes color in a linear way, your color textues have to be converted from Gamma Space (the visible color space, see Gamma / Linear rendering section) to linear space, in order for the lighting to be applied accurately and the colors to blend correctly.\n\nThis concept is really often confusing for the artists so if you have to decide if the texture has to be sRGB or Linear, ask yourself if the contained data are actual colors that you have authored in another software. If the answer is true, you’ll most likely have to use your texture as sRGB.\n\nAs there are some exceptions and a lot of cases where you could ask yourself about how to configure your texture, here is a table that you could use as reference. (Please note that specific features to your engine could invalidate its contents, so if you have a doubt, ask your CTO).\n\nWhen the texture is imported, the engine can generate a set of Mip-Maps for the imported texture. Every Mip-Map of a texture is a lower resolution version of the imported texture at half, quarter, eighth, 16th… of the initial resolution.\n\nThe point in using Mip-Maps is to reduce texture filter aliasing when texel/pixel ratio is greater than one (if the displayed texture is too dense for the final pixel density), these kind of artifacts were present on older consoles and showed a lot of noise on distant textures, because of the inability of the texture units to find a proper pixel value to read.\n\nBy using lower resolution textures, color data becomes blended and aliasing become reduced as the same pixel from the texture is read every time..\n\nSamplers and Texture are two distinct elements when it comes to read textures in shaders. Textures are data containers (N Dimensional Arrays of pixels (1D/2D/3D/...)) while Samplers are GPU Objects that contain settings in order to Sample a texture. Every one of these settings is called a Sampler State.\n\nSampler states allows to control over filtering, out of range coordinates, mipmap control, texture array controls, or sRGB/linear handling.\n\nDepending on your engine, some, many or most of these options are hidden or wrapped under more artist-friendly options in order to maintain good quality and performance\n\nTexture Wrapping Mode handles how to read texture when using out-of range (0..1) coordinates. Basically a texture read in the 0..1 range will be read as-is and will wrap every unit of coordinates using a Wrapping mode named Repeat, Some engines will allow you to Clamp so the latest pixel of the texture will repeat indefinitely after the edge of the texture, or even Mirror.\n\nAddressing mode can be set per-axis using the ADDRESSU, ADDRESSV, and ADDRESSW states but often, game engines lets you choose this setting once for all axises (some also provide per-axis as an advanced setting).\n\nAddressing modes can be set using a CLAMP (clamps coordinates between 0.0 and 1.0), REPEAT (Wrapping the texture out of range), or MIRROR (flips the axis every odd and even).\n\nFiltering can be adjusted in various ways, depending on the Texel/Pixel ratio. Three filtering modes can be adjusted depending on various cases : When enlarging : MAGFILTER is used when the Texel/pixel ratio is < 1.0, When the Texel/pixel ratio is > 1, we use MINFILTER to set it when the kernel is shrunk down. In the case of using mip-maps, the MIPFILTER can be used to blend between mip-maps (thus enabling tri-linear filtering)\n\nFiltering modes can be set to POINT (No filtering), LINEAR (Smooth) and ANISOTROPIC. The latter relies on view angle to filter using different (higher) mip-maps.\n\nWhen reading a texture, provided coordinates will help the GPU determine which mip to read depending on the texel/pixel ratio and the view angle. Mip-maps are selected automatically to be used using a operation.\n\nTex2D is the auto-mip function that will read a texture using its sampler s and coordinates t. This function is meant to be used in pixel shader only as the rasterization will determine the UV coordinate screen space derivative (how much UV changes from one pixel to its neighbor). For more information, see the tex2d section below.\n\nTexture objects can be of many dimensions (1D, 2D, 3D, Cube) but can also be stored into Texture Arrays, using the slice index enables the user to select which element of the array of textures you want to read from.\n\nsRGB/Linear Sampling flag enables the texture sampler to apply inverse-gamma correction for sRGB Textures (or not in the case of linear textures such as Motion Vectors, Masks or Normals).\n\nsRGB Sampling roughly applies the inverse function of the gamma correction in order to transform all color data into linear data that is mathematically correct for blending, lighting computations or even simple display.\n\nFor more information see Color Correction\n\nIn order to sample a texture using a sampler, a set of coordinates need to be provided.\n\nUV and UVW Coordinates are the artist-friendly name of the mesh texture coordinates, a set of values stored per-vertex that correlate a particular vertex in a 2D Texture Space. These coordinates are used for texture mapping.\n\nTexture Coordinates (UV) are stored in the mesh data for every vertex. Some software enable storing many different texture coordinates for the same geometry. Every set of coordinates is set in a different Texture Coordinate Channel.\n\nSometimes, placing many object in the world require these objects to have continuous mapping from one instance to another. In order to sample these coordinates, we do not require any Texture Coordinate Channel.\n\nWorld coordinates can be used for :\n\nScreen Space coordinates are a way to map textures in screen-space. These coordinates are aligned to the screen rectangle:\n• With the origin being in the top (or bottom) left corner, stretching up to 1 in every direction (horizontal and vertical)\n• With its origin being in the center of the screen, and coordinates stretching between -1 and 1 in every direction (horizontal and vertical)\n\nLookup Coordinates is a custom way to sample a texture. Instead of using coordinates, we can use anything to sample a texture : time, exposed shader property, textures.\n\nUsing a texture value as a lookup coordinate enable performing Gradient Mapping : using this black and white value interpreted as coordinate (0 = black, 1= white), we can sample a texture (namely a Look-Up Texture or Look-Up Table) to remap the value.\n\nDeforming Sampling Coordinates enable performing distortion on textures. This concept assumes that you mix texture coordinates and values sampled from a texture as deforming weights.\n\nFor instance adding the values sampled from a normal map to the texture coordinates will deform the coordinates based on the tilted angles.\n\nSampling a texture in shaders can be achieved using:\n• The direct access to the texture array of pixels : texture load\n• Access to filtering, mip-maps and other features through texture samples\n\nVery often, texture reads are made through a texture sampler that is bound to the texture. It is an object created when the texture is imported, and bound automatically to the shader by the engine.\n• Optionally : a reference to its texture. Because it's optional, generic samplers can be created and use with various textures.\n\nIn HLSL a Texture.Load() is achieved by providing a reference to the texture and a integer vector containing the following:\n• Z : mip level which we want to read from\n\nThe prototype of a Texture Load is as follows\n• Texture Load is also compatible with Compute Shaders\n\nIn HLSL, these 3 functions will access a texture's data for a given coordinate, based on three different computations of mip-maps:\n• tex2d will determine the correct mip-map based on rasterization texel density.\n• tex2dbias will determine the correct mip-map based on rasterization texel density and will apply a bias on it\n• tex2dlod will not determine automatically the mip-map but instead will use the one specified\n\ntex2d and tex2dbias are functions that can only be called in pixel shaders, because of their need of knowledge of texel density. During rasterization, texture coordinates are interpolated from the values stored in vertices, and, depending on the size of the triangle on screen, and the uv space visible, a UV derivative will be computed.\n\nThe UV Derivative is a per-pixel value that tells how much UV coordinate changes from one pixel to its neighbors. When applying this mapping using a texture, the hardware can tell the texel per pixel ratio.\n\nWhile the ideal case would be 1 Texture Texel per Screen Pixel, it is rarely the case in 3d graphics, due to perspective and shapes in the space. By computing the derivative, depending on how close to the camera is the part of geometry being rendered, the correct mip-map will be chosen.\n\nFailing to do so would imply to use a higher definition mip-map for the current pixel, and would lead into two major problems:\n• Texture Aliasing : by having a Texel/pixel density ratio greater than one, it means that one pixel of the screen is supposed to draw a part of a texture that is composed of more than one pixel, for example an area of 8 by 8 pixels. In this case, the texture sample will have to decide of one pixel out of these 64 to get, but this value will not be an average of the 8 by 8, and so it will lead into texture aliasing.\n• Texture Cache Stall : in addition to texture aliasing, a texture cache stall will happen for texel/pixel ratios greater than one. Short version of this issue is the GPU needs to put in cache more memory than it's able to, so it will do it several times, slowing down the execution of the shader. For more information, this performance pitfall is detailed more in the Performance section."
    },
    {
        "link": "https://developer.download.nvidia.com/cg/tex2D.html",
        "document": "tex2D - performs a texture lookup in a given 2D sampler and, in some cases, a shadow comparison. May also use pre computed derivatives if those are provided. Coordinates to perform the lookup. If an extra coordinate compared to the texture dimensionality is present it is used to perform a shadow comparison. The value used in the shadow comparison is always the last component of the coordinate vector. Offset to be added to obtain the final texel. Performs a texture lookup in sampler samp using coordinates s, may use and derivatives dx and dy, also may perform shadow comparison and use texel offset texelOff to compute final texel. tex2D is supported in all fragment profiles and all vertex profiles starting with vp40, variants with shadow comparison are only supported in fp40 and newer profiles, variants with texel offsets are only supported in gp4 and newer profiles. Variants with integer textures are also only supported in gp4 and newer profiles."
    }
]