[
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html",
        "document": "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: . If you want to pass in a path object, pandas accepts any . By file-like object, we refer to objects with a method, such as a file handle (e.g. via builtin function) or . Deprecated since version 2.1.0: Passing byte strings is deprecated. To read from a byte string, wrap it in a object.\n\nColumn (0-indexed) to use as the row labels of the DataFrame. Pass None if there is no such column. If a list is passed, those columns will be combined into a . If a subset of data is selected with , index_col is based on the subset. Missing values will be forward filled to allow roundtripping with for . To avoid forward filling the missing values use after reading the data instead of .\n\nData type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32} Use to preserve data as stored in Excel and not interpret dtype, which will necessarily result in dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. If you use , it will infer the dtype of each column based on the data.\n\nIf io is not a buffer or path, this must be set to identify io. Engine compatibility : When , the following logic will be used to determine the engine:\n• None If is an OpenDocument format (.odf, .ods, .odt), then odf will be used.\n• None Otherwise if is an xls format, will be used.\n• None Otherwise if is in xlsb format, will be used.\n• None Otherwise will be used.\n\nWhether or not to include the default NaN values when parsing the data. Depending on whether is passed in, the behavior is as follows:\n• None If is True, and are specified, is appended to the default NaN values used for parsing.\n• None If is True, and are not specified, only the default NaN values are used for parsing.\n• None If is False, and are specified, only the NaN values specified are used for parsing.\n• None If is False, and are not specified, no strings will be parsed as NaN. Note that if is passed in as False, the and parameters will be ignored.\n\nThe behavior is as follows:\n• None . If True -> try parsing the index.\n• None of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.\n• None of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. If you don`t want to parse some cells as date just change their type in Excel to “Text”. For non-standard datetime parsing, use after .\n\nFunction to use for converting a sequence of string columns to an array of datetime instances. The default uses to do the conversion. Pandas will try to call in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by ) as arguments; 2) concatenate (row-wise) the string values from the columns defined by into a single array and pass that; and 3) call once for each row using one or more strings (corresponding to the columns defined by ) as arguments. Deprecated since version 2.0.0: Use instead, or read in as and then apply as-needed."
    },
    {
        "link": "https://stackoverflow.com/questions/26521266/using-pandas-to-pd-read-excel-for-multiple-but-not-all-worksheets-of-the-sam",
        "document": "There are various options depending on the use case:\n• None If one doesn't know the sheets names.\n• None If the sheets name is not relevant.\n• None If one knows the name of the sheets.\n\nBelow we will look closely at each of the options.\n\nSee the Notes section for information such as finding out the sheet names.\n\nIf one doesn't know the sheets names\n\nThen, depending on the sheet one wants to read, one can pass each of them to a specific , such as\n\nIf the name is not relevant and all one cares about is the position of the sheet. Let's say one wants only the first sheet\n\nThen, depending on the sheet name, one can pass each it to a specific , such as\n\nHere we will consider the case where one knows the name of the sheets. For the examples, one will consider that there are three sheets named , , and . The content in each is the same, and looks like this\n\nWith this, depending on one's goals, there are multiple approaches:\n• None Store everything in same dataframe. One approach would be to concat the sheets as follows sheets = ['Sheet1', 'Sheet2', 'Sheet3'] df = pd.concat([pd.read_excel('FILENAME.xlsx', sheet_name = sheet) for sheet in sheets], ignore_index = True) [Out]: 0 1 2 0 85 January 2000 1 95 February 2001 2 105 March 2002 3 115 April 2003 4 125 May 2004 5 135 June 2005 6 85 January 2000 7 95 February 2001 8 105 March 2002 9 115 April 2003 10 125 May 2004 11 135 June 2005 12 85 January 2000 13 95 February 2001 14 105 March 2002 15 115 April 2003 16 125 May 2004 17 135 June 2005\n• None Store each sheet in a different dataframe (let's say, , , ...) sheets = ['Sheet1', 'Sheet2', 'Sheet3'] for i, sheet in enumerate(sheets): globals()['df' + str(i + 1)] = pd.read_excel('FILENAME.xlsx', sheet_name = sheet) [Out]: # df1 0 1 2 0 85 January 2000 1 95 February 2001 2 105 March 2002 3 115 April 2003 4 125 May 2004 5 135 June 2005 # df2 0 1 2 0 85 January 2000 1 95 February 2001 2 105 March 2002 3 115 April 2003 4 125 May 2004 5 135 June 2005 # df3 0 1 2 0 85 January 2000 1 95 February 2001 2 105 March 2002 3 115 April 2003 4 125 May 2004 5 135 June 2005\n• None If one wants to know the sheets names, one can use the class as follows\n• None In this case one is assuming that the file is on the same directory as the script one is running.\n• None If the file is in a folder of the current directory called Data, one way would be to use create a variable, such as as follows\n• None This might be a relevant read."
    },
    {
        "link": "https://stackoverflow.com/questions/73843473/reading-a-specific-sheet-tab-using-pandas-python-off-a-xls-file",
        "document": "I need to read the second tab(sheet two) only of the XLS file I have. I am able to read the file but it always gives the default 1st tab(sheet one). Below are the codes I wrote.\n\nI tried both ways but only shows data from Tab one(Sheet one = 0) *Please note the type of the file is Microsoft excel 97-2003 worksheet(xls)"
    },
    {
        "link": "https://digitalocean.com/community/tutorials/pandas-read_excel-reading-excel-file-in-python",
        "document": "We can use the pandas module read_excel() function to read the excel file data into a DataFrame object. If you look at an excel sheet, it’s a two-dimensional table. The DataFrame object also represents a two-dimensional tabular data structure.\n\nLet’s say we have an excel file with two sheets - Employees and Cars. The top row contains the header of the table.\n\nHere is the example to read the “Employees” sheet data and printing it.\n• The first parameter is the name of the excel file.\n• The sheet_name parameter defines the sheet to be read from the excel file.\n• When we print the DataFrame object, the output is a two-dimensional table. It looks similar to an excel sheet records.\n\n2. List of Columns Headers of the Excel Sheet\n\nWe can get the list of column headers using the property of the dataframe object.\n\nWe can get the column data and convert it into a list of values.\n\nWe can specify the column names to be read from the excel file. It’s useful when you are interested in only a few of the columns of the excel sheet.\n\nIf the excel sheet doesn’t have any header row, pass the header parameter value as None.\n\nIf you pass the header value as an integer, let’s say 3. Then the third row will be treated as the header row and the values will be read from the next row onwards. Any data before the header row will be discarded.\n\nThe DataFrame object has various utility methods to convert the tabular data into Dict, CSV, or JSON format."
    },
    {
        "link": "https://pandas.pydata.org/docs/dev/reference/api/pandas.read_excel.html",
        "document": "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: . If you want to pass in a path object, pandas accepts any . By file-like object, we refer to objects with a method, such as a file handle (e.g. via builtin function) or . Deprecated since version 2.1.0: Passing byte strings is deprecated. To read from a byte string, wrap it in a object.\n\nColumn (0-indexed) to use as the row labels of the DataFrame. Pass None if there is no such column. If a list is passed, those columns will be combined into a . If a subset of data is selected with , index_col is based on the subset. Missing values will be forward filled to allow roundtripping with for . To avoid forward filling the missing values use after reading the data instead of .\n\nData type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32} Use to preserve data as stored in Excel and not interpret dtype, which will necessarily result in dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. If you use , it will infer the dtype of each column based on the data.\n\nIf io is not a buffer or path, this must be set to identify io. Engine compatibility : When , the following logic will be used to determine the engine:\n• None If is an OpenDocument format (.odf, .ods, .odt), then odf will be used.\n• None Otherwise if is an xls format, will be used.\n• None Otherwise if is in xlsb format, will be used.\n• None Otherwise will be used.\n\nLine numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file. If callable, the callable function will be evaluated against the row indices, returning True if the row should be skipped and False otherwise. An example of a valid callable argument would be .\n\nWhether or not to include the default NaN values when parsing the data. Depending on whether is passed in, the behavior is as follows:\n• None If is True, and are specified, is appended to the default NaN values used for parsing.\n• None If is True, and are not specified, only the default NaN values are used for parsing.\n• None If is False, and are specified, only the NaN values specified are used for parsing.\n• None If is False, and are not specified, no strings will be parsed as NaN. Note that if is passed in as False, the and parameters will be ignored.\n\nThe behavior is as follows:\n• None . If True -> try parsing the index.\n• None of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.\n• None of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. If you don`t want to parse some cells as date just change their type in Excel to “Text”. For non-standard datetime parsing, use after ."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/merging.html",
        "document": "pandas provides various methods for combining and comparing or .\n• None : Merge multiple or objects along a shared index or column\n• None : Update missing values with non-missing values in the same location\n• None : Combine two or objects with SQL-style joining\n• None : Combine two or objects along an ordered axis\n• None : Combine two or objects by near instead of exact matching keys\n• None and : Show differences in values between two or objects\n\nThe function concatenates an arbitrary amount of or objects along an axis while performing optional set logic (union or intersection) of the indexes on the other axes. Like , takes a list or dict of homogeneously-typed objects and concatenates them. makes a full copy of the data, and iteratively reusing can create unnecessary copies. Collect all or objects in a list before using . When concatenating with named axes, pandas will attempt to preserve these index/column names whenever possible. In the case where all inputs share a common name, this name will be assigned to the result. When the input names do not all agree, the result will be unnamed. The same is true for , but the logic is applied separately on a level-by-level basis. The keyword specifies how to handle axis values that don’t exist in the first . takes the union of all axis values takes the intersection of the axis values To perform an effective “left” join using the exact index from the original , result can be reindexed. For objects which don’t have a meaningful index, the ignores overlapping indexes. You can concatenate a mix of and objects. The will be transformed to with the column name as the name of the . will drop all name references. The argument adds another axis level to the resulting index or column (creating a ) associate specific keys with each original . The argument cane override the column names when creating a new based on existing . You can also pass a dict to in which case the dict keys will be used for the argument unless other argument is specified: The created has levels that are constructed from the passed keys and the index of the pieces: argument allows specifying resulting levels associated with the If you have a that you want to append as a single row to a , you can convert the row into a and use\n\nperforms join operations similar to relational databases like SQL. Users who are familiar with SQL but new to pandas can reference a comparison with SQL.\n• None one-to-one: joining two objects on their indexes which must contain unique values.\n• None many-to-one: joining a unique index to one or more columns in a different . When joining columns on columns, potentially a many-to-many join, any indexes on the passed objects will be discarded. For a many-to-many join, if a key combination appears more than once in both tables, the will have the Cartesian product of the associated data. The argument to specifies which keys are included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be . Here is a summary of the options and their SQL equivalent names: Use keys from left frame only Use keys from right frame only Use union of keys from both frames Use intersection of keys from both frames Create the cartesian product of rows of both frames You can and a with a if the names of the correspond to the columns from the . Transform the to a using before merging Performing an outer join with duplicate join keys in Merging on duplicate keys significantly increase the dimensions of the result and can cause a memory overflow. The argument checks whether the uniqueness of merge keys. Key uniqueness is checked before merge operations and can protect against memory overflows and unexpected key duplication. Traceback (most recent call last) in in # check if columns specified as unique in \"Merge keys are not unique in left dataset; not a one-to-one merge\" \"Merge keys are not unique in right dataset; not a one-to-one merge\" : Merge keys are not unique in right dataset; not a one-to-one merge If the user is aware of the duplicates in the right but wants to ensure there are no duplicates in the left , one can use the argument instead, which will not raise an exception. accepts the argument . If , a Categorical-type column called will be added to the output object that takes on values: A string argument to will use the value as the name for the indicator column. The merge argument takes a tuple of list of strings to append to overlapping column names in the input to disambiguate the result columns:\n\ncombines the columns of multiple, potentially differently-indexed into a single result . takes an optional argument which may be a column or multiple column names that the passed is to be aligned. To join on multiple keys, the passed must have a : The default for is to perform a left join which uses only the keys found in the calling . Other join types can be specified with . You can join a with a to a with a on a level. The of the with match the level name of the . The of the input argument must be completely used in the join and is a subset of the indices in the left argument. Merging on a combination of columns and index levels# Strings passed as the , , and parameters may refer to either column names or index level names. This enables merging instances on a combination of index levels and columns without resetting indexes. When are joined on a string that matches an index level in both arguments, the index level is preserved as an index level in the resulting . When are joined using only some of the levels of a , the extra levels will be dropped from the resulting join. To preserve those levels, use on those level names to move those levels to columns prior to the join. A list or tuple of can also be passed to to join them together on their indexes. update missing values from one with the non-missing values in another in the corresponding location.\n\nis similar to an ordered left-join except that mactches are on the nearest key rather than equal keys. For each row in the , the last row in the are selected where the key is less than the left’s key. Both must be sorted by the key. Optionally an can perform a group-wise merge by matching the key in addition to the nearest match on the key. within between the quote time and the trade time. within between the quote time and the trade time and exclude exact matches on time. Note that though we exclude the exact matches (of the quotes), prior quotes do propagate to that point in time.\n\nThe and methods allow you to compare two or , respectively, and summarize their differences. By default, if two corresponding values are equal, they will be shown as . Furthermore, if all values in an entire row / column, the row / column will be omitted from the result. The remaining differences will be aligned on columns. Keep all original rows and columns with self other self other self other Keep all the original values even if they are equal. self other self other self other"
    },
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html",
        "document": "Type of merge to be performed.\n• None left: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n• None right: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n• None outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n• None inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\n• None cross: creates the cartesian product from both frames, preserves the order of the left keys.\n\nColumn or index level names to join on. These must be found in both DataFrames. If is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.\n\nColumn or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.\n\nColumn or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.\n\nUse the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.\n\nA length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in and respectively. Pass a value of instead of a string to indicate that the column name from or should be left as-is, with no suffix. At least one of the values must not be None.\n\nIf False, avoid copy if possible. The keyword will change behavior in pandas 3.0. Copy-on-Write will be enabled by default, which means that all methods with a keyword will use a lazy copy mechanism to defer the copy and ignore the keyword. The keyword will be removed in a future version of pandas. You can already get the future behavior and improvements through enabling copy on write\n\nIf True, adds a column to the output DataFrame called “_merge” with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of “left_only” for observations whose merge key only appears in the left DataFrame, “right_only” for observations whose merge key only appears in the right DataFrame, and “both” if the observation’s merge key is found in both DataFrames.\n\nIf specified, checks if merge is of specified type.\n• None “one_to_one” or “1:1”: check if merge keys are unique in both left and right datasets.\n• None “one_to_many” or “1:m”: check if merge keys are unique in left dataset.\n• None “many_to_one” or “m:1”: check if merge keys are unique in right dataset.\n• None “many_to_many” or “m:m”: allowed, but does not result in checks."
    },
    {
        "link": "https://stackoverflow.com/questions/44327999/how-to-merge-multiple-dataframes",
        "document": "I have different dataframes and need to merge them together based on the date column. If I only had two dataframes, I could use , to do it with three dataframes, I use , however it becomes really complex and unreadable to do it with multiple dataframes.\n\nAll dataframes have one column in common - , but they don't have the same number of rows nor columns and I only need those rows in which each date is common to every dataframe.\n\nSo, I'm trying to write a recursion function that returns a dataframe with all data but it didn't work. How should I merge multiple dataframes then?\n\nI tried different ways and got errors like , and can not merge DataFrame with instance of type <class 'NoneType'> .\n\nThis is the script I wrote:"
    },
    {
        "link": "https://stackoverflow.com/questions/37697195/how-to-merge-two-data-frames-based-on-particular-column-in-pandas-python",
        "document": "In order to successfully merge two data frames based on common column(s), the dtype for common column(s) in both data frames must be the same! dtype for a column can be changed by:"
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.3/user_guide/merging.html",
        "document": "pandas provides various facilities for easily combining together Series or DataFrame with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.\n\nIn addition, pandas also provides utilities to compare two Series or DataFrame and summarize their differences.\n\nThe function (in the main pandas namespace) does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic (union or intersection) of the indexes (if any) on the other axes. Note that I say “if any” because there is only a single possible axis of concatenation for Series. Before diving into all of the details of and what it can do, here is a simple example: Like its sibling function on ndarrays, , takes a list or dict of homogeneously-typed objects and concatenates them with some configurable handling of “what to do with the other axes”:\n• None : a sequence or mapping of Series or DataFrame objects. If a dict is passed, the sorted keys will be used as the argument, unless it is passed, in which case the values will be selected (see below). Any None objects will be dropped silently unless they are all None in which case a ValueError will be raised.\n• None : {0, 1, …}, default 0. The axis to concatenate along.\n• None : {‘inner’, ‘outer’}, default ‘outer’. How to handle indexes on other axis(es). Outer for union and inner for intersection.\n• None : boolean, default False. If True, do not use the index values on the concatenation axis. The resulting axis will be labeled 0, …, n - 1. This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information. Note the index values on the other axes are still respected in the join.\n• None : sequence, default None. Construct hierarchical index using the passed keys as the outermost level. If multiple levels passed, should contain tuples.\n• None : list of sequences, default None. Specific levels (unique values) to use for constructing a MultiIndex. Otherwise they will be inferred from the keys.\n• None : list, default None. Names for the levels in the resulting hierarchical index.\n• None : boolean, default False. Check whether the new concatenated axis contains duplicates. This can be very expensive relative to the actual data concatenation.\n• None : boolean, default True. If False, do not copy data unnecessarily. Without a little bit of context many of these arguments don’t make much sense. Let’s revisit the above example. Suppose we wanted to associate specific keys with each of the pieces of the chopped up DataFrame. We can do this using the argument: As you can see (if you’ve read the rest of the documentation), the resulting object’s index has a hierarchical index. This means that we can now select out each chunk by key: It’s not a stretch to see how this can be very useful. More detail on this functionality below. It is worth noting that (and therefore ) makes a full copy of the data, and that constantly reusing this function can create a significant performance hit. If you need to use the operation over several datasets, use a list comprehension. When concatenating DataFrames with named axes, pandas will attempt to preserve these index/column names whenever possible. In the case where all inputs share a common name, this name will be assigned to the result. When the input names do not all agree, the result will be unnamed. The same is true for , but the logic is applied separately on a level-by-level basis. Set logic on the other axes¶ When gluing together multiple DataFrames, you have a choice of how to handle the other axes (other than the one being concatenated). This can be done in the following two ways:\n• None Take the union of them all, . This is the default option as it results in zero information loss. Here is an example of each of these methods. First, the default behavior: Here is the same thing with : Lastly, suppose we just wanted to reuse the exact index from the original DataFrame: Similarly, we could index before the concatenation: A useful shortcut to are the instance methods on and . These methods actually predated . They concatenate along , namely the index: In the case of , the indexes must be disjoint but the columns do not need to be: may take multiple objects to concatenate: Unlike the method, which appends to the original list and returns , here does not modify and returns its copy with appended. For objects which don’t have a meaningful index, you may wish to append them and ignore the fact that they may have overlapping indexes. To do this, use the argument: This is also a valid argument to : You can concatenate a mix of and objects. The will be transformed to with the column name as the name of the . Since we’re concatenating a to a , we could have achieved the same result with . To concatenate an arbitrary number of pandas objects ( or ), use . If unnamed are passed they will be numbered consecutively. Passing will drop all name references. A fairly common use of the argument is to override the column names when creating a new based on existing . Notice how the default behaviour consists on letting the resulting inherit the parent ’ name, when these existed. Through the argument we can override the existing column names. Let’s consider a variation of the very first example presented: You can also pass a dict to in which case the dict keys will be used for the argument (unless other keys are specified): The MultiIndex created has levels that are constructed from the passed keys and the index of the pieces: If you wish to specify other levels (as will occasionally be the case), you can do so using the argument: This is fairly esoteric, but it is actually necessary for implementing things like GroupBy where the order of a categorical variable is meaningful. While not especially efficient (since a new object must be created), you can append a single row to a by passing a or dict to , which returns a new as above. You should use with this method to instruct DataFrame to discard its index. If you wish to preserve the index, you should construct an appropriately-indexed DataFrame and append or concatenate those objects. You can also pass a list of dicts or Series:\n\npandas has full-featured, high performance in-memory join operations idiomatically very similar to relational databases like SQL. These methods perform significantly better (in some cases well over an order of magnitude better) than other open source implementations (like in R). The reason for this is careful algorithmic design and the internal layout of the data in . See the cookbook for some advanced strategies. Users who are familiar with SQL but new to pandas might be interested in a comparison with SQL. pandas provides a single function, , as the entry point for all standard database join operations between or named objects:\n• None : Column or index level names to join on. Must be found in both the left and right DataFrame and/or Series objects. If not passed and and are , the intersection of the columns in the DataFrames and/or Series will be inferred to be the join keys.\n• None : Columns or index levels from the left DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.\n• None : Columns or index levels from the right DataFrame or Series to use as keys. Can either be column names, index level names, or arrays with length equal to the length of the DataFrame or Series.\n• None : If , use the index (row labels) from the left DataFrame or Series as its join key(s). In the case of a DataFrame or Series with a MultiIndex (hierarchical), the number of levels must match the number of join keys from the right DataFrame or Series.\n• None : Same usage as for the right DataFrame or Series\n• None : One of , , , , . Defaults to . See below for more detailed description of each method.\n• None : Sort the result DataFrame by the join keys in lexicographical order. Defaults to , setting to will improve performance substantially in many cases.\n• None : A tuple of string suffixes to apply to overlapping columns. Defaults to .\n• None : Always copy data (default ) from the passed DataFrame or named Series objects, even when reindexing is not necessary. Cannot be avoided in many cases but may improve performance / memory usage. The cases where copying can be avoided are somewhat pathological but this option is provided nonetheless.\n• None : Add a column to the output DataFrame called with information on the source of each row. is Categorical-type and takes on a value of for observations whose merge key only appears in DataFrame or Series, for observations whose merge key only appears in DataFrame or Series, and if the observation’s merge key is found in both.\n• None : string, default None. If specified, checks if merge is of specified type.\n• None “one_to_one” or “1:1”: checks if merge keys are unique in both left and right datasets.\n• None “one_to_many” or “1:m”: checks if merge keys are unique in left dataset.\n• None “many_to_one” or “m:1”: checks if merge keys are unique in right dataset.\n• None “many_to_many” or “m:m”: allowed, but does not result in checks. Support for specifying index levels as the , , and parameters was added in version 0.23.0. Support for merging named objects was added in version 0.24.0. The return type will be the same as . If is a or named and is a subclass of , the return type will still be . is a function in the pandas namespace, and it is also available as a instance method , with the calling being implicitly considered the left object in the join. The related method, uses internally for the index-on-index (by default) and column(s)-on-index join. If you are joining on index only, you may wish to use to save yourself some typing. Experienced users of relational databases like SQL will be familiar with the terminology used to describe join operations between two SQL-table like structures ( objects). There are several cases to consider which are very important to understand:\n• None one-to-one joins: for example when joining two objects on their indexes (which must contain unique values).\n• None many-to-one joins: for example when joining an index (unique) to one or more columns in a different . When joining columns on columns (potentially a many-to-many join), any indexes on the passed objects will be discarded. It is worth spending some time understanding the result of the many-to-many join case. In SQL / standard relational algebra, if a key combination appears more than once in both tables, the resulting table will have the Cartesian product of the associated data. Here is a very basic example with one unique key combination: Here is a more complicated example with multiple join keys. Only the keys appearing in and are present (the intersection), since by default. The argument to specifies how to determine which keys are to be included in the resulting table. If a key combination does not appear in either the left or right tables, the values in the joined table will be . Here is a summary of the options and their SQL equivalent names: Use keys from left frame only Use keys from right frame only Use union of keys from both frames Use intersection of keys from both frames Create the cartesian product of rows of both frames You can merge a mult-indexed Series and a DataFrame, if the names of the MultiIndex correspond to the columns from the DataFrame. Transform the Series to a DataFrame using before merging, as shown in the following example. Here is another example with duplicate join keys in DataFrames: Joining / merging on duplicate keys can cause a returned frame that is the multiplication of the row dimensions, which may result in memory overflow. It is the user’ s responsibility to manage duplicate values in keys before joining large DataFrames. Users can use the argument to automatically check whether there are unexpected duplicates in their merge keys. Key uniqueness is checked before merge operations and so should protect against memory overflows. Checking key uniqueness is also a good way to ensure user data structures are as expected. In the following example, there are duplicate values of in the right . As this is not a one-to-one merge – as specified in the argument – an exception will be raised. MergeError: Merge keys are not unique in right dataset; not a one-to-one merge If the user is aware of the duplicates in the right but wants to ensure there are no duplicates in the left DataFrame, one can use the argument instead, which will not raise an exception. accepts the argument . If , a Categorical-type column called will be added to the output object that takes on values: The argument will also accept string arguments, in which case the indicator function will use the value of the passed string as the name for the indicator column. Merging will preserve the dtype of the join keys. We are able to preserve the join keys: Of course if you have missing values that are introduced, then the resulting dtype will be upcast. Merging will preserve dtypes of the mergands. See also the section on categoricals. The category dtypes must be exactly the same, meaning the same categories and the ordered attribute. Otherwise the result will coerce to the categories’ dtype. Merging on dtypes that are the same can be quite performant compared to dtype merging. is a convenient method for combining the columns of two potentially differently-indexed into a single result . Here is a very basic example: The same as above, but with . The data alignment here is on the indexes (row labels). This same behavior can be achieved using plus additional arguments instructing it to use the indexes: takes an optional argument which may be a column or multiple column names, which specifies that the passed is to be aligned on that column in the . These two function calls are completely equivalent: Obviously you can choose whichever form you find more convenient. For many-to-one joins (where one of the ’s is already indexed by the join key), using may be more convenient. Here is a simple example: To join on multiple keys, the passed DataFrame must have a : Now this can be joined by passing the two key column names: The default for is to perform a left join (essentially a “VLOOKUP” operation, for Excel users), which uses only the keys found in the calling DataFrame. Other join types, for example inner join, can be just as easily performed: As you can see, this drops any rows where there was no match. You can join a singly-indexed with a level of a MultiIndexed . The level will match on the name of the index of the singly-indexed frame against a level name of the MultiIndexed frame. This is equivalent but less verbose and more memory efficient / faster than this. This is supported in a limited way, provided that the index for the right argument is completely used in the join, and is a subset of the indices in the left argument, as in this example: If that condition is not satisfied, a join with two multi-indexes can be done using the following code. Merging on a combination of columns and index levels¶ Strings passed as the , , and parameters may refer to either column names or index level names. This enables merging instances on a combination of index levels and columns without resetting indexes. When DataFrames are merged on a string that matches an index level in both frames, the index level is preserved as an index level in the resulting DataFrame. When DataFrames are merged using only some of the levels of a , the extra levels will be dropped from the resulting merge. In order to preserve those levels, use on those level names to move those levels to columns prior to doing the merge. If a string matches both a column name and an index level name, then a warning is issued and the column takes precedence. This will result in an ambiguity error in a future version. The merge argument takes a tuple of list of strings to append to overlapping column names in the input s to disambiguate the result columns: has and arguments which behave similarly. A list or tuple of can also be passed to to join them together on their indexes. Merging together values within Series or DataFrame columns¶ Another fairly common situation is to have two like-indexed (or similarly indexed) or objects and wanting to “patch” values in one object from values for matching indices in the other. Here is an example: For this, use the method: Note that this method only takes values from the right if they are missing in the left . A related method, , alters non-NA values in place:\n\nThe and methods allow you to compare two DataFrame or Series, respectively, and summarize their differences. This feature was added in V1.1.0. For example, you might want to compare two and stack their differences side by side. By default, if two corresponding values are equal, they will be shown as . Furthermore, if all values in an entire row / column, the row / column will be omitted from the result. The remaining differences will be aligned on columns. If you wish, you may choose to stack the differences on rows. If you wish to keep all original rows and columns, set argument to . self other self other self other You may also keep all the original values even if they are equal. self other self other self other"
    }
]