[
    {
        "link": "https://docs.vulkan.org/spec/latest/chapters/pipelines.html",
        "document": "When a pipeline is created, its state and shaders are compiled into zero or more device-specific executables, which are used when executing commands against that pipeline. To query the properties of these pipeline executables, call:\n• is the device that created the pipeline.\n• is a pointer to an integer related to the number of pipeline executables available or queried, as described below.\n• is either or a pointer to an array of VkPipelineExecutablePropertiesKHR structures. If is , then the number of pipeline executables associated with the pipeline is returned in . Otherwise, must point to a variable set by the application to the number of elements in the array, and on return the variable is overwritten with the number of structures actually written to . If is less than the number of pipeline executables associated with the pipeline, at most structures will be written, and will be returned instead of , to indicate that not all the available properties were returned.\n• VUID-vkGetPipelineExecutablePropertiesKHR-pipelineExecutableInfo-03270\n\n The feature must be enabled\n• VUID-vkGetPipelineExecutablePropertiesKHR-pipeline-03271\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutablePropertiesKHR-pPipelineInfo-parameter\n\n must be a valid pointer to a valid VkPipelineInfoKHR structure\n• VUID-vkGetPipelineExecutablePropertiesKHR-pExecutableCount-parameter\n\n must be a valid pointer to a value\n• VUID-vkGetPipelineExecutablePropertiesKHR-pProperties-parameter\n\n If the value referenced by is not , and is not , must be a valid pointer to an array of VkPipelineExecutablePropertiesKHR structures The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is a bitmask of zero or more VkShaderStageFlagBits indicating which shader stages (if any) were principally used as inputs to compile this pipeline executable.\n• is an array of containing a null-terminated UTF-8 string which is a short human readable name for this pipeline executable.\n• is an array of containing a null-terminated UTF-8 string which is a human readable description for this pipeline executable.\n• is the subgroup size with which this pipeline executable is dispatched. Not all implementations have a 1:1 mapping between shader stages and pipeline executables and some implementations may reduce a given shader stage to fixed function hardware programming such that no pipeline executable is available. No guarantees are provided about the mapping between shader stages and pipeline executables and should be considered a best effort hint. Because the application cannot rely on the field to provide an exact description, and provide a human readable name and description which more accurately describes the given pipeline executable.\n• is the logical device that created the pipeline.\n• is a pointer to a VkPipelineInfoEXT structure which describes the pipeline being queried.\n• is a pointer to a VkBaseOutStructure structure in which the pipeline properties will be written. To query a pipeline’s pass a VkPipelinePropertiesIdentifierEXT structure in . Each pipeline is associated with a and the identifier is implementation specific.\n• VUID-vkGetPipelinePropertiesEXT-pipeline-06738\n\n The member of must have been created with\n• VUID-vkGetPipelinePropertiesEXT-pPipelineProperties-06739\n\n must be a valid pointer to a VkPipelinePropertiesIdentifierEXT structure\n• VUID-vkGetPipelinePropertiesEXT-None-06766\n\n The feature must be enabled\n• VUID-vkGetPipelinePropertiesEXT-pPipelineInfo-parameter\n\n must be a valid pointer to a valid VkPipelineInfoEXT structure The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is an array of values into which the pipeline identifier will be written. The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure. Each pipeline executable may have a set of statistics associated with it that are generated by the pipeline compilation process. These statistics may include things such as instruction counts, amount of spilling (if any), maximum number of simultaneous threads, or anything else which may aid developers in evaluating the expected performance of a shader. To query the compile time statistics associated with a pipeline executable, call:\n• is the device that created the pipeline.\n• is a pointer to an integer related to the number of statistics available or queried, as described below.\n• is either or a pointer to an array of VkPipelineExecutableStatisticKHR structures. If is , then the number of statistics associated with the pipeline executable is returned in . Otherwise, must point to a variable set by the application to the number of elements in the array, and on return the variable is overwritten with the number of structures actually written to . If is less than the number of statistics associated with the pipeline executable, at most structures will be written, and will be returned instead of , to indicate that not all the available statistics were returned.\n• VUID-vkGetPipelineExecutableStatisticsKHR-pipelineExecutableInfo-03272\n\n The feature must be enabled\n• VUID-vkGetPipelineExecutableStatisticsKHR-pipeline-03273\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutableStatisticsKHR-pipeline-03274\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutableStatisticsKHR-pExecutableInfo-parameter\n\n must be a valid pointer to a valid VkPipelineExecutableInfoKHR structure\n• VUID-vkGetPipelineExecutableStatisticsKHR-pStatisticCount-parameter\n\n must be a valid pointer to a value\n• VUID-vkGetPipelineExecutableStatisticsKHR-pStatistics-parameter\n\n If the value referenced by is not , and is not , must be a valid pointer to an array of VkPipelineExecutableStatisticKHR structures The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is the pipeline to query.\n• is the index of the pipeline executable to query in the array of executable properties returned by vkGetPipelineExecutablePropertiesKHR.\n• VUID-VkPipelineExecutableInfoKHR-executableIndex-03275\n\n must be less than the number of pipeline executables associated with as returned in the parameter of The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is an array of containing a null-terminated UTF-8 string which is a short human readable name for this statistic.\n• is an array of containing a null-terminated UTF-8 string which is a human readable description for this statistic.\n• is a VkPipelineExecutableStatisticFormatKHR value specifying the format of the data found in .\n• is the value of this statistic. The enum is defined as:\n• specifies that the statistic is returned as a 32-bit boolean value which must be either or and should be read from the field of .\n• specifies that the statistic is returned as a signed 64-bit integer and should be read from the field of .\n• specifies that the statistic is returned as an unsigned 64-bit integer and should be read from the field of .\n• specifies that the statistic is returned as a 64-bit floating-point value and should be read from the field of . The union is defined as:\n• is the 32-bit boolean value if the is .\n• is the signed 64-bit integer value if the is .\n• is the unsigned 64-bit integer value if the is .\n• is the 64-bit floating-point value if the is . Each pipeline executable may have one or more text or binary internal representations associated with it which are generated as part of the compile process. These may include the final shader assembly, a binary form of the compiled shader, or the shader compiler’s internal representation at any number of intermediate compile steps. To query the internal representations associated with a pipeline executable, call:\n• is the device that created the pipeline.\n• is a pointer to an integer related to the number of internal representations available or queried, as described below.\n• is either or a pointer to an array of VkPipelineExecutableInternalRepresentationKHR structures. If is , then the number of internal representations associated with the pipeline executable is returned in . Otherwise, must point to a variable set by the application to the number of elements in the array, and on return the variable is overwritten with the number of structures actually written to . If is less than the number of internal representations associated with the pipeline executable, at most structures will be written, and will be returned instead of , to indicate that not all the available representations were returned. While the details of the internal representations remain implementation-dependent, the implementation should order the internal representations in the order in which they occur in the compiled pipeline with the final shader assembly (if any) last.\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pipelineExecutableInfo-03276\n\n The feature must be enabled\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pipeline-03277\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pipeline-03278\n\n The member of must have been created with\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pExecutableInfo-parameter\n\n must be a valid pointer to a valid VkPipelineExecutableInfoKHR structure\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pInternalRepresentationCount-parameter\n\n must be a valid pointer to a value\n• VUID-vkGetPipelineExecutableInternalRepresentationsKHR-pInternalRepresentations-parameter\n\n If the value referenced by is not , and is not , must be a valid pointer to an array of VkPipelineExecutableInternalRepresentationKHR structures The structure is defined as:\n• is a VkStructureType value identifying this structure.\n• is or a pointer to a structure extending this structure.\n• is an array of containing a null-terminated UTF-8 string which is a short human readable name for this internal representation.\n• is an array of containing a null-terminated UTF-8 string which is a human readable description for this internal representation.\n• specifies whether the returned data is text or opaque data. If is then the data returned in is text and is guaranteed to be a null-terminated UTF-8 string.\n• is an integer related to the size, in bytes, of the internal representation’s data, as described below.\n• is either or a pointer to a block of data into which the implementation will write the internal representation. If is , then the size, in bytes, of the internal representation data is returned in . Otherwise, must be the size of the buffer, in bytes, pointed to by and on return is overwritten with the number of bytes of data actually written to including any trailing null character. If is less than the size, in bytes, of the internal representation’s data, at most bytes of data will be written to , and will be returned instead of , to indicate that not all the available representation was returned. If is and is not and is not zero, the last byte written to will be a null character. Information about a particular shader that has been compiled as part of a pipeline object can be extracted by calling:\n• is the device that created .\n• is the target of the query.\n• is a VkShaderStageFlagBits specifying the particular shader within the pipeline about which information is being queried.\n• describes what kind of information is being queried.\n• is a pointer to a value related to the amount of data the query returns, as described below.\n• is either or a pointer to a buffer. If is , then the maximum size of the information that can be retrieved about the shader, in bytes, is returned in . Otherwise, must point to a variable set by the application to the size of the buffer, in bytes, pointed to by , and on return the variable is overwritten with the amount of data actually written to . If is less than the maximum size that can be retrieved by the pipeline cache, then at most bytes will be written to , and will be returned, instead of , to indicate that not all required of the pipeline cache was returned. Not all information is available for every shader and implementations may not support all kinds of information for any shader. When a certain type of information is unavailable, the function returns . If information is successfully and fully queried, the function will return . For , a structure will be written to the buffer pointed to by . This structure will be populated with statistics regarding the physical device resources used by that shader along with other miscellaneous information and is described in further detail below. For , is a pointer to a null-terminated UTF-8 string containing human-readable disassembly. The exact formatting and contents of the disassembly string are vendor-specific. The formatting and contents of all other types of information, including , are left to the vendor and are not further specified by this extension. This query does not behave consistently with the behavior described in Opaque Binary Data Results, for historical reasons. If the amount of data available is larger than the passed , the query returns up to the size of the passed buffer, and signals overflow with a success status instead of returning a error status.\n• VUID-vkGetShaderInfoAMD-shaderStage-parameter\n\n must be a valid VkShaderStageFlagBits value\n• VUID-vkGetShaderInfoAMD-infoType-parameter\n\n must be a valid VkShaderInfoTypeAMD value\n• VUID-vkGetShaderInfoAMD-pInfoSize-parameter\n\n must be a valid pointer to a value\n• VUID-vkGetShaderInfoAMD-pInfo-parameter\n\n If the value referenced by is not , and is not , must be a valid pointer to an array of bytes\n• VUID-vkGetShaderInfoAMD-pipeline-parent\n\n must have been created, allocated, or retrieved from Possible values of vkGetShaderInfoAMD:: , specifying the information being queried from a shader, are:\n• specifies that device resources used by a shader will be queried.\n• specifies that implementation-specific information will be queried. The structure is defined as:\n• are the combination of logical shader stages contained within this shader.\n• is a VkShaderResourceUsageAMD structure describing internal physical device resources used by this shader.\n• is the maximum number of vector instruction general-purpose registers (VGPRs) available to the physical device.\n• is the maximum number of scalar instruction general-purpose registers (SGPRs) available to the physical device.\n• is the maximum limit of VGPRs made available to the shader compiler.\n• is the maximum limit of SGPRs made available to the shader compiler.\n• is the local workgroup size of this shader in { X, Y, Z } dimensions. Some implementations may merge multiple logical shader stages together in a single shader. In such cases, will contain a bitmask of all of the stages that are active within that shader. Consequently, if specifying those stages as input to vkGetShaderInfoAMD, the same output information may be returned for all such shader stage queries. The number of available VGPRs and SGPRs ( and respectively) are the shader-addressable subset of physical registers that is given as a limit to the compiler for register assignment. These values may further be limited by implementations due to performance optimizations where register pressure is a bottleneck. The structure is defined as:\n• is the number of vector instruction general-purpose registers used by this shader.\n• is the number of scalar instruction general-purpose registers used by this shader.\n• is the maximum local data store size per work group in bytes.\n• is the LDS usage size in bytes per work group by this shader.\n• is the scratch memory usage in bytes by this shader."
    },
    {
        "link": "https://reddit.com/r/vulkan/comments/13h16id/question_regarding_vulkan_pipeline_best_practices",
        "document": "From what I could gather, in Vulkan if you want to use a different shader you have to bind a different pipeline. In following this philosophy I have found myself with increasingly complex code with what I feel is an excessive number of vulkan pipelines.\n\nIt becomes worse when you consider that different primitives require different pipelines alongside all the different combinations in between.\n\nCurrently I have this many different pipelines and it's already a little overwhelming...\n\nSo i'm wondering if there is a better approach? I did hear about so that's probably something I might try next."
    },
    {
        "link": "https://github.com/SaschaWillems/Vulkan",
        "document": "A comprehensive collection of open source C++ examples for Vulkan®, the low-level graphics and compute API from Khronos.\n\nKhronos has made an official Vulkan Samples repository available to the public (press release).\n\nYou can find this repository at https://github.com/KhronosGroup/Vulkan-Samples\n\nAs I've been involved with getting the official repository up and running, I'll be mostly contributing to that repository from now, but may still add samples that don't fit there in here and I'll of course continue to maintain these samples.\n\nThis repository contains submodules for external dependencies and assets, so when doing a fresh clone you need to clone recursively:\n\nThe repository contains everything required to compile and build the examples on Windows, Linux, Android, iOS and macOS (using MoltenVK) using a C++ compiler that supports C++20.\n\nSee BUILD.md for details on how to build for the different platforms.\n\nOnce built, examples can be run from the bin directory. The list of available command line options can be brought up with :\n\nNote that some examples require specific device features, and if you are on a multi-gpu system you might need to use the and to select a gpu that supports them.\n\nVulkan consumes shaders in an intermediate representation called SPIR-V. This makes it possible to use different shader languages by compiling them to that bytecode format. The primary shader language used here is GLSL but most samples also come with HLSL shader sources.\n\nSynchronization in the master branch currently isn't optimal und uses at the end of each frame. This is a heavy operation and is suboptimal in regards to having CPU and GPU operations run in parallel. I'm currently reworking this in the this branch. While still work-in-progress, if you're interested in a more proper way of synchronization in Vulkan, please take a look at that branch.\n• Basic and verbose example for getting a colored triangle rendered to the screen using Vulkan. This is meant as a starting point for learning Vulkan from the ground up. A huge part of the code is boilerplate that is abstracted away in later examples.\n• Vulkan 1.3 version of the basic and verbose example for getting a colored triangle rendered to the screen. This makes use of features like dynamic rendering simplifying api usage.\n• Using pipeline state objects (pso) that bake state information (rasterization states, culling modes, etc.) along with the shaders into a single object, making it easy for an implementation to optimize usage (compared to OpenGL's dynamic state machine). Also demonstrates the use of pipeline derivatives.\n• Descriptors are used to pass data to shader binding points. Sets up descriptor sets, layouts, pools, creates a single pipeline based on the set layout and renders multiple objects with different descriptor sets.\n• Dynamic uniform buffers are used for rendering multiple objects with multiple matrices stored in a single uniform buffer object. Individual matrices are dynamically addressed upon descriptor binding time, minimizing the number of required descriptor sets.\n• Uses push constants, small blocks of uniform data stored within a command buffer, to pass data to a shader without the need for uniform buffers.\n• Uses SPIR-V specialization constants to create multiple pipelines with different lighting paths from a single \"uber\" shader.\n• Loads a 2D texture from disk (including all mip levels), uses staging to upload it into video memory and samples from it using combined image samplers.\n• Loads a 2D texture array containing multiple 2D texture slices (each with its own mip chain) and renders multiple meshes each sampling from a different layer of the texture. 2D texture arrays don't do any interpolation between the slices.\n• Loads a cube map texture from disk containing six different faces. All faces and mip levels are uploaded into video memory, and the cubemap is displayed on a skybox as a backdrop and on a 3D model as a reflection.\n• Loads an array of cube map textures from a single file. All cube maps are uploaded into video memory with their faces and mip levels, and the selected cubemap is displayed on a skybox as a backdrop and on a 3D model as a reflection.\n• Generates a 3D texture on the cpu (using perlin noise), uploads it to the device and samples it to render an animation. 3D textures store volumetric data and interpolate in all three dimensions.\n• Uses input attachments to read framebuffer contents from a previous sub pass at the same pixel position within a single render pass. This can be used for basic post processing or image composition (blog entry).\n• Advanced example that uses sub passes and input attachments to write and read back data from framebuffer attachments (same location only) in single render pass. This is used to implement deferred render composition with added forward transparency in a single pass.\n• Basic offscreen rendering in two passes. First pass renders the mirrored scene to a separate framebuffer with color and depth attachments, second pass samples from that color attachment for rendering a mirror surface.\n• Implements a simple CPU based particle system. Particle data is stored in host memory, updated on the CPU per-frame and synchronized with the device before it's rendered using pre-multiplied alpha.\n• Uses the stencil buffer and its compare functionality for rendering a 3D model with dynamic outlines.\n• Demonstrates two different ways of passing vertices to the vertex shader using either interleaved or separate vertex attributes.\n\nThese samples show how implement different features of the glTF 2.0 3D format 3D transmission file format in detail.\n• Shows how to load a complete scene from a glTF 2.0 file. The structure of the glTF 2.0 scene is converted into the data structures required to render the scene with Vulkan.\n• Demonstrates how to do GPU vertex skinning from animation data stored in a glTF 2.0 model. Along with reading all the data structures required for doing vertex skinning, the sample also shows how to upload animation data to the GPU and how to render it using shaders.\n• Renders a complete scene loaded from an glTF 2.0 file. The sample is based on the glTF model loading sample, and adds data structures, functions and shaders required to render a more complex scene using Crytek's Sponza model with per-material pipelines and normal mapping.\n• Implements multisample anti-aliasing (MSAA) using a renderpass with multisampled attachments and resolve attachments that get resolved into the visible frame buffer.\n• Implements a high dynamic range rendering pipeline using 16/32 bit floating point precision for all internal formats, textures and calculations, including a bloom pass, manual exposure and tone mapping.\n• Rendering shadows for a directional light source. First pass stores depth values from the light's pov, second pass compares against these to check if a fragment is shadowed. Uses depth bias to avoid shadow artifacts and applies a PCF filter to smooth shadow edges.\n• Uses multiple shadow maps (stored as a layered texture) to increase shadow resolution for larger scenes. The camera frustum is split up into multiple cascades with corresponding layers in the shadow map. Layer selection for shadowing depth compare is then done by comparing fragment depth with the cascades' depths ranges.\n• Uses a dynamic floating point cube map to implement shadowing for a point light source that casts shadows in all directions. The cube map is updated every frame and stores distance to the light source for each fragment used to determine if a fragment is shadowed.\n• Generating a complete mip-chain at runtime instead of loading it from a file, by blitting from one mip level, starting with the actual texture image, down to the next smaller size until the lower 1x1 pixel end of the mip chain.\n• Capturing and saving an image after a scene has been rendered using blits to copy the last swapchain image from optimal device to host local linear memory, so that it can be stored into a ppm image.\n• Implements order independent transparency based on linked lists. To achieve this, the sample uses storage buffers in combination with image load and store atomic operations in the fragment shader.\n• Multi threaded parallel command buffer generation. Instead of prebuilding and reusing the same command buffers this sample uses multiple hardware threads to demonstrate parallel per-frame recreation of secondary command buffers that are executed and submitted in a primary buffer once all threads have finished.\n• Uses the instancing feature for rendering many instances of the same mesh from a single vertex buffer with variable parameters and textures (indexing a layered texture). Instanced data is passed using a secondary vertex buffer.\n• Rendering thousands of instanced objects with different geometry using one single indirect draw call instead of issuing separate draws. All draw commands to be executed are stored in a dedicated indirect draw buffer object (storing index count, offset, instance count, etc.) that is uploaded to the device and sourced by the indirect draw command for rendering.\n• Using query pool objects to get number of passed samples for rendered primitives got determining on-screen visibility.\n• Using query pool objects to gather statistics from different stages of the pipeline like vertex, fragment shader and tessellation evaluation shader invocations depending on payload.\n\nPhysical based rendering as a lighting technique that achieves a more realistic and dynamic look by applying approximations of bidirectional reflectance distribution functions based on measured real-world material parameters and environment lighting.\n• Demonstrates a basic specular BRDF implementation with solid materials and fixed light sources on a grid of objects with varying material parameters, demonstrating how metallic reflectance and surface roughness affect the appearance of pbr lit objects.\n• Adds image based lighting from an hdr environment cubemap to the PBR equation, using the surrounding environment as the light source. This adds an even more realistic look the scene as the light contribution used by the materials is now controlled by the environment. Also shows how to generate the BRDF 2D-LUT and irradiance and filtered cube maps from the environment map.\n• Renders a model specially crafted for a metallic-roughness PBR workflow with textures defining material parameters for the PRB equation (albedo, metallic, roughness, baked ambient occlusion, normal maps) in an image based lighting environment.\n• Uses multiple render targets to fill all attachments (albedo, normals, position, depth) required for a G-Buffer in a single pass. A deferred pass then uses these to calculate shading and lighting in screen space, so that calculations only have to be done for visible fragments independent of no. of lights.\n• Adds multi sampling to a deferred renderer using manual resolve in the fragment shader.\n• Adds shadows from multiple spotlights to a deferred renderer using a layered depth attachment filled in one pass using multiple geometry shader invocations.\n• Adds ambient occlusion in screen space to a 3D scene. Depth values from a previous deferred pass are used to generate an ambient occlusion texture that is blurred before being applied to the scene in a final composition path.\n\nAll Vulkan implementations support compute shaders, a more generalized way of doing workloads on the GPU. These samples demonstrate how to use those compute shaders.\n• Uses a compute shader along with a separate compute queue to apply different convolution kernels (and effects) on an input image in realtime.\n• Attraction based 2D GPU particle system using compute shaders. Particle data is stored in a shader storage buffer and only modified on the GPU using memory barriers for synchronizing compute particle updates with graphics pipeline vertex access.\n• N-body simulation based particle system with multiple attractors and particle-to-particle interaction using two passes separating particle movement calculation and final integration. Shared compute shader memory is used to speed up compute calculations.\n• Simple GPU ray tracer with shadows and reflections using a compute shader. No scene geometry is rendered in the graphics pass.\n• Mass-spring based cloth system on the GPU using a compute shader to calculate and integrate spring forces, also implementing basic collision with a fixed scene object.\n• Purely GPU based frustum visibility culling and level-of-detail system. A compute shader is used to modify draw commands stored in an indirect draw commands buffer to toggle model visibility and select its level-of-detail based on camera distance, no calculations have to be done on and synced with the CPU.\n• Visualizing per-vertex model normals (for debugging). First pass renders the plain model, second pass uses a geometry shader to generate colored lines based on per-vertex model normals,\n• Renders a scene to multiple viewports in one pass using a geometry shader to apply different matrices per viewport to simulate stereoscopic rendering (left/right). Requires a device with support for .\n• Uses a height map to dynamically generate and displace additional geometric detail for a low-poly mesh.\n• Renders a terrain using tessellation shaders for height displacement (based on a 16-bit height map), dynamic level-of-detail (based on triangle screen space size) and per-patch frustum culling.\n• Uses curved PN-triangles (paper) for adding details to a low-polygon model.\n\nVulkan supports GPUs with dedicated hardware for ray tracing. These sampples show different parts of that functionality.\n• Basic example for doing hardware accelerated ray tracing using the and extensions. Shows how to setup acceleration structures, ray tracing pipelines and the shader binding table needed to do the actual ray tracing.\n• Adds ray traced shadows casting using the new ray tracing extensions to a more complex scene. Shows how to add multiple hit and miss shaders and how to modify existing shaders to add shadow calculations.\n• Renders a complex scene with reflective surfaces using the new ray tracing extensions. Shows how to do recursion inside of the ray tracing shaders for implementing real time reflections.\n• Renders a texture mapped quad with transparency using the new ray tracing extensions. Shows how to do texture mapping in a closes hit shader, how to cancel intersections for transparency in an any hit shader and how to access mesh data in those shaders using buffer device addresses.\n• Callable shaders can be dynamically invoked from within other ray tracing shaders to execute different shaders based on dynamic conditions. The example ray traces multiple geometries, with each calling a different callable shader from the closest hit shader.\n• Uses an intersection shader for procedural geometry. Instead of using actual geometry, this sample on passes bounding boxes and object definitions. An intersection shader is then used to trace against the procedural objects.\n• Renders a textured glTF model using ray traying instead of rasterization. Makes use of frame accumulation for transparency and anti aliasing.\n• Ray queries add acceleration structure intersection functionality to non ray tracing shader stages. This allows for combining ray tracing with rasterization. This example makes uses ray queries to add ray casted shadows to a rasterized sample in the fragment shader.\n• Uses the extension to fetch vertex position data from the acceleration structure from within a shader, instead of having to manually unpack vertex information.\n\nExamples that run one-time tasks and don't make use of visual output (no window system integration). These can be run in environments where no user interface is available (blog entry).\n• Renders a basic scene to a (non-visible) frame buffer attachment, reads it back to host memory and stores it to disk without any on-screen presentation, showing proper use of memory barriers required for device to host image synchronization.\n• Only uses compute shader capabilities for running calculations on an input data set (passed via SSBO). A fibonacci row is calculated based on input data via the compute shader, stored back and displayed via command line.\n• Load and render a 2D text overlay created from the bitmap glyph data of a stb font file. This data is uploaded as a texture and used for displaying text on top of a 3D scene in a second pass.\n• Uses a texture that stores signed distance field information per character along with a special fragment shader calculating output based on that distance data. This results in crisp high quality font rendering independent of font size and scale.\n• Generates and renders a complex user interface with multiple windows, controls and user interaction on top of a 3D scene. The UI is generated using Dear ImGUI and updated each frame.\n\nVulkan is an extensible api with lots of functionality added by extensions. These samples demonstrate the usage of such extensions.\n\nNote: Certain extensions may become core functionality for newer Vulkan versions. The samples will still work with these.\n• Uses conservative rasterization to change the way fragments are generated by the gpu. The example enables overestimation to generate fragments for every pixel touched instead of only pixels that are fully covered (blog post).\n• Uses push descriptors apply the push constants concept to descriptor sets. Instead of creating per-object descriptor sets for rendering multiple objects, this example passes descriptors at command buffer creation time.\n• Makes use of inline uniform blocks to pass uniform data directly at descriptor set creation time and also demonstrates how to update data for those descriptors at runtime.\n• Renders a scene to to multiple views (layers) of a single framebuffer to simulate stereoscopic rendering in one pass. Broadcasting to the views is done in the vertex shader using .\n• Demonstrates the use of VK_EXT_conditional_rendering to conditionally dispatch render commands based on values from a dedicated buffer. This allows e.g. visibility toggles without having to rebuild command buffers (blog post).\n• Shows how to use printf in a shader to output additional information per invocation. This information can help debugging shader related issues in tools like RenderDoc. Note: This sample should be run from a graphics debugger like RenderDoc.\n• Shows how to use debug utils for adding labels and colors to Vulkan objects for graphics debuggers. This information helps to identify resources in tools like RenderDoc. Note: This sample should be run from a graphics debugger like RenderDoc.\n• Shows how to render a scene using a negative viewport height, making the Vulkan render setup more similar to other APIs like OpenGL. Also has several options for changing relevant pipeline state, and displaying meshes with OpenGL or Vulkan style coordinates. Details can be found in this tutorial.\n• Uses a special image that contains variable shading rates to vary the number of fragment shader invocations across the framebuffer. This makes it possible to lower fragment shader invocations for less important/less noisy parts of the framebuffer.\n• Demonstrates the use of VK_EXT_descriptor_indexing for creating descriptor sets with a variable size that can be dynamically indexed in a shader using and .\n• Shows usage of the VK_KHR_dynamic_rendering extension, which simplifies the rendering setup by no longer requiring render pass objects or framebuffers.\n• Based on the dynamic rendering sample, this sample shows how to do implement multi sampling with dynamic rendering.\n• Uses the graphics pipeline library extensions to improve run-time pipeline creation. Instead of creating the whole pipeline at once, this sample pre builds shared pipeline parts like like vertex input state and fragment output state. These are then used to create full pipelines at runtime, reducing build times and possible hick-ups.\n• Basic sample demonstrating how to use the mesh shading pipeline as a replacement for the traditional vertex pipeline.\n• Basic sample showing how to use descriptor buffers to replace descriptor sets.\n• Basic sample showing how to use shader objects that can be used to replace pipeline state objects. Instead of baking all state in a PSO, shaders are explicitly loaded and bound as separate objects and state is set using dynamic state extensions. The sample also stores binary shader objets and loads them on consecutive runs.\n• Shows how to do host image copies, which heavily simplify the host to device image process by fully skipping the staging process.\n• Demonstrates the use of virtual GPU addresses to directly access buffer data in shader. Instead of e.g. using descriptors to access uniforms, with this extension you simply provide an address to the memory you want to read from in the shader and that address can be arbitrarily changed e.g. via a push constant.\n• Shows how to use a new semaphore type that has a way of setting and identifying a given point on a timeline. Compared to the core binary semaphores, this simplifies synchronization as a single timeline semaphore can replace multiple binary semaphores.\n• Demonstrates the basics of fullscreen shader effects. The scene is rendered into an offscreen framebuffer at lower resolution and rendered as a fullscreen quad atop the scene using a radial blur fragment shader.\n• Advanced fullscreen effect example adding a bloom effect to a scene. Glowing scene parts are rendered to a low res offscreen framebuffer that is applied atop the scene using a two pass separated gaussian blur.\n• Implements multiple texture mapping methods to simulate depth based on texture information: Normal mapping, parallax mapping, steep parallax mapping and parallax occlusion mapping (best quality, worst performance).\n• Uses a spherical material capture texture array defining environment lighting and reflection information to fake complex lighting.\n• Renders a Vulkan demo scene with logos and mascots. Not an actual example but more of a playground and showcase.\n\nSee CREDITS.md for additional credits and attributions."
    },
    {
        "link": "https://edw.is/learning-vulkan",
        "document": "tl;dr: I learned some Vulkan and made a game engine with two small game demos in 3 months.\n\nThe code for the engine and the games can be found here: https://github.com/eliasdaler/edbr\n\nThis article documents my experience of learning Vulkan and writing a small game/engine with it. It took me around 3 months to do it without any previous knowledge of Vulkan (I had previous OpenGL experience and some experience with making game engines, though).\n\nThe engine wasn’t implemented as a general purpose engine, which is probably why it took me a few months (and not years) to achieve this. I started by making a small 3D game and separated reusable parts into the “engine” afterwards. I can recommend everyone to follow the same process to not get stuck in the weeds (see “Bike-shedding” section below for more advice).\n\nI’m a professional programmer, but I’m self-taught in graphics programming. I started studying graphics programming around 1.5 years ago by learning OpenGL and writing a 3D engine in it.\n\nThe engine I wrote in Vulkan is mostly suited for smaller level-based games. I’ll explain things which worked for me, but they might not be the most efficient. My implementation would probably still be a good starting point for many people.\n\nIf you haven’t done any graphics programming before, you should start with OpenGL. It’s much easier to learn it and not get overwhelmed by all the complexity that Vulkan has. A lot of your OpenGL and graphics programming knowledge will be useful when you start doing things with Vulkan later.\n\nIdeally, you should at least get a textured model displayed on the screen with some simple Blinn-Phong lighting. I can also recommend doing some basic shadow mapping too, so that you learn how to render your scene from a different viewpoint and to a different render target, how to sample from depth textures and so on.\n\nI can recommend using the following resources to learn OpenGL:\n• Thorsten Thormählen’s lectures lectures (watch the first 6 videos, the rest might be a bit too advanced)\n\nSadly, most OpenGL resources don’t teach the latest OpenGL 4.6 practices. They make writing OpenGL a lot more enjoyable. If you learn them, transitioning to Vulkan will be much easier (I only learned about OpenGL 3.3 during my previous engine development, though, so it’s not a necessity).\n\nHere are some resources which teach you the latest OpenGL practices:\n\nBike-shedding and how to avoid it\n\nAh, bike-shedding… Basically, it’s a harmful pattern of overthinking and over-engineering even the simplest things. It’s easy to fall into this trap when doing graphics programming (especially when doing Vulkan since you need to make many choices when implementing an engine with it).\n• Always ask yourself “Do I really need this?”, “Will this thing ever become a bottleneck?”.\n• Remember that you can always rewrite any part of your game/engine later.\n• Don’t implement something unless you need it right now. Don’t think “Well, a good engine needs X, right…?”.\n• Don’t try to make a general purpose game engine. It’s probably even better to not think about “the engine” at first and write a simple game.\n• Make a small game first - a Breakout clone, for example. Starting your engine development by doing a Minecraft clone with multiplayer support is probably not a good idea.\n• Be wary of people who tend to suggest complicated solutions to simple problems.\n• Don’t look too much at what other people do. I’ve seen many over-engineered engines on GitHub - sometimes they’re that complex for a good reason (and there are years of work behind them). But you probably don’t need most of that complexity, especially for simpler games.\n• Don’t try to make magical wrappers around Vulkan interfaces prematurely, especially while you’re still learning Vulkan.\n\nGet it working first. Leave “TODO”/“FIXME” comments in some places. Then move on to the next thing. Try to fix “TODO”/“FIXME” places only when they really become problematic or bottleneck your performance. You’ll be surprised to see how many things won’t become a problem at all.\n\nThe situation with graphic APIs in 2024 is somewhat complicated. It all depends on the use case: DirectX seems like the most solid choice for most AAA games. WebGL or WebGPU are the only two choices for doing 3D graphics on the web. Metal is the go-to graphics API on macOS and iOS (though you can still do Vulkan there via MoltenVK).\n\nMy use case is simple: I want to make small 3D games for desktop platforms (Windows and Linux mostly). I also love open source technology and open standards. So, it was a choice between OpenGL and Vulkan for me.\n\nOpenGL is a good enough choice for many small games. But it’s very unlikely that it’ll get new versions in the future (so you can’t use some newest GPU capabilities like ray tracing), it’s deprecated on macOS and its future is uncertain.\n\nWebGPU was also a possible choice. Before learning Vulkan, I learned some of it. It’s a pretty solid API, but I had some problems with it:\n• It’s still not stable and there’s not a lot of tutorials and examples for it. This tutorial is fantastic, though.\n• WGSL is an okay shading language, but I just find its syntax not as pleasant as GLSL’s (note that you can write in GLSL and then load compiled SPIR-V on WebGPU native).\n• On desktop, it’s essentially a wrapper around other graphic APIs (DirectX, Vulkan, Metal).This introduces additional problems for me:\n• It can’t do things some things that Vulkan or DirectX can do.\n• It has more limitations than native graphic APIs since it needs to behave similarly between them.\n• RenderDoc captures become confusing as they differ between the platforms (you can get DirectX capture on Windows and Vulkan capture on Linux) and you don’t have 1-to-1 mapping between WebGPU calls and native API calls.\n• Using Dawn and WGPU feels like using bgfx or sokol. You don’t get the same degree of control over the GPU and some of the choices/abstractions might not be the most pleasant for you.\n\nStill, I think that WebGPU is a better API than OpenGL/WebGL and can be more useful to you than Vulkan in some use cases:\n• Validation errors are much better than in OpenGL/WebGL and not having global state helps a lot.\n• It’s also kind of similar to Vulkan in many things, so learning a bit of it before diving into Vulkan also helped me a lot.\n• It requires a lot less boilerplate to get things on the screen (compared to Vulkan).\n• You don’t have to deal with explicit synchronization which makes things much simpler.\n• You can make your games playable inside the browser.\n\nLearning Vulkan seemed like an impossible thing for me previously. It felt like you needed to have many years of AAA game graphics programming experience to be able to do things in it. You also hear people saying “you’re basically writing a graphics driver when writing in Vulkan” which also made Vulkan sounds like an incredibly complicated thing.\n\nI have also checked out some engines written in Vulkan before and was further demotivated by seeing tons of scary abstractions and files named like or which had thousands of lines of scary C++ code.\n\nThe situation has changed over the years. Vulkan is not as complicated as it was before. First of all, Khronos realized that some parts of Vulkan were indeed very complex and introduced some newer features which made many things much simpler (for example, dynamic rendering). Secondly, some very useful libraries which reduce boilerplate were implemented. And finally, there are a lot of fantastic resources which make learning Vulkan much easier than it was before.\n\nThe best Vulkan learning resource which helped me get started was vkguide. If you’re starting from scratch, just go through it all (you might stop at “GPU driver rendering” chapter at first - many simple games probably won’t need this level of complexity)\n\nVulkan Lecture Series by TU Wien also nicely teaches Vulkan basics (you can probably skip “Real-Time Ray Tracing” chapter for now). I especially found a lecture on synchronization very helpful.\n\nHere are some more advanced Vulkan books that also helped me:\n• 3D Graphics Rendering Cookbook by Sergey Kosarevsky and Viktor Latypov. There is the second edition in the writing and it’s promising to be better than the first one. The second edition is not released yet, but the source code for it can be found here: https://github.com/PacktPublishing/3D-Graphics-Rendering-Cookbook-Second-Edition\n• Mastering Graphics Programming with Vulkan by Marco Castorina, Gabriel Sassone. Very advanced book which explains some of the “cutting edge” graphics programming concepts (I mostly read it to understand where to go further, but didn’t have time to implement most of it). The source code for it can be found here: https://github.com/PacktPublishing/Mastering-Graphics-Programming-with-Vulkan\n\nHere’s the result of my first month of learning Vulkan:\n\nBy this point I had:\n\nOf course, doing it for the 3rd time (I had it implemented it all in OpenGL and WebGPU before) certainly helped. Once you get to this point, Vulkan won’t seem as scary anymore.\n\nLet’s see how the engine works and some useful things I learned.\n\nMy engine is called EDBR (Elias Daler’s Bikeshed Engine) and was initially started as a project for learning Vulkan. It quickly grew into a somewhat usable engine which I’m going to use for my further projects.\n\nI copy-pasted some non-graphics related stuff from my previous engine (e.g. input handling and audio system) but all of the graphics and many other core systems were rewritten from scratch. I feel like it was a good way to do it instead of trying to cram Vulkan into my old OpenGL abstractions.\n\nLet’s see how this frame in rendered:\n\nFirst, models with skeletal animations are skinned in the compute shader. The compute shader takes unskinned mesh and produces a buffer of vertices which are then used instead of the original mesh in later rendering steps. This allows me to treat static and skinned meshes similarly in shaders and not do skinning repeatedly in different rendering steps.\n\nI use a 4096x4096 depth texture with 3 slices for cascaded shadow mapping. The first slice looks like this:\n\nAll the models are drawn and shading is calculated using the shadow map and light info. I use a PBR model which is almost identical to the one described in Physically Based Rendering in Filament. The fragment shader is quite big and does calculation for all the lights affecting the drawn mesh in one draw call:\n\nEverything is drawn into a multi-sampled texture. Here’s how it looks after resolve:\n\nDepth resolve step is performed manually via a fragment shader. I just go through all the fragments of multi-sample depth texture and write the minimum value into the non-MS depth texture (it’ll be useful in the next step).\n\nSome post FX is applied - right now it’s only depth fog (I use “depth resolve” texture from the previous step here), afterwards tone-mapping and bloom will also be done here.\n\nDialogue UI is drawn. Everything is done in one draw call (more is explained in “Drawing many sprites” section)\n\nAnd that’s it! It’s pretty basic right now and would probably become much more complex in the future (see “Future work” section).\n\nThere are a couple of libraries which greatly improve the experience of writing Vulkan. Most of them are already used in vkguide, but I still want to highlight how helpful they were to me.\n\nvk-bootstrap simplifies a lot of Vulkan boilerplate: physical device selection, swapchain creation and so on.\n\nI don’t like big wrappers around graphic APIs because they tend to be very opinionated. Plus, you need to keep a mental map of “wrapper function vs function in the API spec” in your head at all times.\n\nThankfully, vk-bootstrap is not like this. It mostly affects the initialization step of your program and doesn’t attempt to be a wrapper around every Vulkan function.\n\nI’ll be honest, I used VMA without even learning about how to allocate memory in Vulkan manually. I read about it in the Vulkan spec later - I’m glad that I didn’t have to do it on my own.\n\nVolk was very useful for me for simplifying extension function loading. For example, if you want to use very useful for setting debug names for your objects (useful for RenderDoc captures and validation errors), you’ll need to do this if you don’t use volk:\n\nWith volk, all the extensions are immediately loaded after you call and you don’t need to store these pointers everywhere. You just include and call - beautiful!\n\nI have a class which encapsulates most of the commonly used functionality and stores many objects that you need for calling Vulkan functions ( , and so on). A single instance is created on the startup and then gets passed around.\n• returns a new which is later used in all the drawing steps.\n• does drawing to the swapchain and does sync between the frames.\n\nThat’s… a lot of things. However, it’s not that big: is only 714 lines at the time of writing this article. It’s more convenient to pass one object into the function instead of many ( , , and so on).\n\nIn Vulkan, you can use any shading language which compiles to SPIR-V - that means that you can use GLSL, HLSL and others. I chose GLSL because I already knew it from my OpenGL experience.\n\nYou can pre-compile your shaders during the build step or compile them on the fly. I do it during the build so that my shader loading runtime code is simpler. I also don’t have an additional runtime dependency on the shader compiler. Also, shader errors are detected during the build step and I don’t get compile errors during the runtime.\n\nI use glslc (from shaderc project, it’s included in Vulkan SDK) which allows you to specify a in CMake which is incredibly useful when you use shader includes. If you change a shader file, all files which include it are recompiled automatically. Without the , CMake won’t be able to see which files shader files need to be recompiled and will only recompile the file which was changed.\n\nMy CMake script for building shaders looks like this:\n\nand then in the main CMakeLists file:\n\nNow, when you build a target, shaders get built automatically and the resulting SPIR-V files are put into the binary directory.\n\nPassing data to shaders in OpenGL is much simpler than it is in Vulkan. In OpenGL, you could just do this:\n\nYou can also use explicit uniform location like this.\n\nIn Vulkan, you need to group your uniforms into “descriptor sets”:\n\nNow, this makes things a lot more complicated, because you need to specify descriptor set layout beforehand, use descriptor set pools and allocate descriptor sets with them, do the whole + thing, call for each descriptor set and so on.\n\nI’ll explain later how I avoided using descriptor sets by using bindless descriptors and buffer device access. Basically, I only have one “global” descriptor set for bindless textures and samplers, and that’s it. Everything else is passed via push constants which makes everything much easier to handle.\n\nMost of them look like this:\n\nThe function is usually called once during the engine initialization. abstraction is described in vkguide here. I modified it a bit to use the Builder pattern to be able to chain the calls.\n• does all the needed cleanup. It usually simply destroys the pipeline and its layout:\n• is called each frame and all the needed inputs are passed as arguments. It’s assumed that the sync is performed outside of the call (see “Synchronization” section below). Some pipelines are only called once per frame - some either take of objects to draw or are called like this:\n\nThe typical function looks like this:\n\nNote another thing: it’s assumed that is called between and - the render pass itself doesn’t care what texture it renders to - the caller of is responsible for that. It makes things simpler and allows you to do several draws to the same render target, e.g.:\n\nI have one vertex type for all the meshes. It looks like this:\n\nThe vertices are accessed in the shader like this:\n\nPVP frees you from having to define vertex format (no more VAOs like in OpenGL or + in Vulkan). BDA also frees you from having to bind a buffer to a descriptor set - you just pass an address to your buffer which contains vertices in push constants and that’s it.\n\nTextures were painful to work with even in OpenGL - you had “texture slots” which were awkward to work with. You couldn’t just sample any texture from the shader if it wasn’t bound to a texture slot beforehand. changed that and made many things easier.\n\nVulkan doesn’t have the exact same functionality, but it has something similar. You can create big descriptor sets which look like this:\n\nYou’ll need to maintain a list of all your textures using some “image manager” and when a new texture is loaded, you need to insert it into the array. The index at which you inserted it becomes a bindless “texture id” which then can be used to sample it in shaders. Now you can pass these ids in your push constants like this:\n\nand then you can sample your texture in the fragment shader like this:\n• I chose separate image samplers so that I could sample any texture using different samplers. Common samplers (nearest, linear with anisotropy, depth texture samplers) are created and put into array on the startup.\n• The wrapper function makes the process of sampling a lot more convenient.\n\nI use bindless ids for the mesh material buffer which looks like this:\n\nNow I can only pass material ID in my push constants and then sample texture like this in the fragment shader:\n\nNeat! No more bulky descriptor sets, just one int per material in the push constants.\n\nYou can also put different texture types into the same set like this (this is needed for being able to access textures of types other than ):\n\nAnd here’s how you can sample with a linear sampler (note that we use here instead of ):\n\nHere’s a very good article on using bindless textures in Vulkan:\n\nHandling dynamic data which needs to be uploaded every frame\n\nI find it useful to pre-allocate big arrays of things and push stuff to them in every frame. Basically, you can pre-allocate an array of N structs (or matrices) and then start at index 0 at each new frame and push things to it from the CPU. Then, you can access all these items in your shaders. For example, I have all joint matrices stored in one big array and the skinning compute shader accesses joint matrices of a particular mesh using start index passed via push constants (more about it will be explained later).\n\nHere are two ways of doing this:\n• \n• Have N buffers on GPU and swap between them.\n\nvkguide explains the concept of “in flight” frames pretty well. To handle this parallelism properly, you need to have one buffer for the “currently drawing” frame and one buffer for “currently recording new drawing commands” frame to not have races. (If you have more frames in flight, you’ll need to allocate more than 2 buffers)\n\nThis means that you need to preallocate 2 buffers on GPU. You write data from CPU to GPU to the first buffer during the first frame. While you record the second frame, GPU reads from the first buffer while you write new data to the second buffer. On the third frame, GPU reads from the second buffer and you write new info to the first buffer… and so on.\n• \n• One buffer on GPU and N “staging” buffers on CPU\n\nThis might be useful if you need to conserve some memory on the GPU.\n\nLet’s see how it works in my engine:\n\nNote how staging buffers are created using VMA’s flag and the “main” buffer from which we read in the shader is using the flag.\n\nHere’s how new data is uploaded (full implementation):\n\nI’d go with the first approach for most cases (more data on GPU, but no need for manual sync) unless you need to conserve GPU memory for some reason. I’ve found no noticeable difference in performance between two approaches, but it might matter if you are uploading huge amounts of data to GPU on each frame.\n\nNow, this might be somewhat controversial… but I didn’t find much use of the deletion queue pattern used in vkguide. I don’t really need to allocated/destroy new objects on every frame.\n\nUsing C++ destructors for Vulkan object cleanup is not very convenient either. You need to wrap everything in custom classes, add move constructors and move … It adds an additional layer of complexity.\n\nIn most cases, the cleanup of Vulkan objects happens in one place - and you don’t want to accidentally destroy some in-use object mid-frame by accidentally destroying some wrapper object.\n\nIt’s also harder to manage lifetimes when you have cleanup in happening in the destructor. For example, suppose you have a case like this:\n\nIf you want to cleanup resources (e.g. the instance of has a object) during , you can’t do that if the cleanup of is performed in its destructor.\n\nOf course, you can do this:\n\n… but I don’t like how it introduces a dynamic allocation and requires you to do write more code (and it’s not that much different from calling a function manually).\n\nRight now, I prefer to clean up stuff directly, e.g.\n\nThis approach is not perfect - first of all, it’s easy to forget to call function, This is not a huge problem since you get a validation error in case you forget to cleanup some Vulkan resources on shutdown:\n\nVMA also triggers asserts if you forget to free some buffer/image allocated with it.\n\nI find it convenient to have all the Vulkan cleanup happening explicitly in one place. It makes it easy to track when the objects get destroyed.\n\nSynchronization in Vulkan is difficult. OpenGL and WebGPU do it for you - if you read from some texture/buffer, you know that it will have the correct data and you won’t get problems with data races. With Vulkan, you need to be explicit and this is usually where things tend to get complicated.\n\nRight now I manage most of the complexities of sync manually in one place. I separate my drawing into “passes”/pipelines (as described above) and then insert barriers between them. For example, the skinning pass writes new vertex data into GPU memory. Shadow mapping pass reads this data to render skinned meshes into the shadow map. Sync in my code looks like this:\n\nOf course, this can be automated/simplified using render graphs. This is something that I might implement in the future. Right now I’m okay with doing manual sync. vkconfig’s “synchronization” validation layer also helps greatly in finding sync errors.\n\nThe following resources were useful for understanding synchronization:\n\nWith bindless textures, it’s easy to draw many sprites using one draw call without having to allocate vertex buffers at all.\n\nFirst of all, you can emit vertex coordinates and UVs using in your vertex shader like this:\n\nThis snippet produces this set of values:\n\nAll the sprite draw calls are combined into which looks like this in GLSL:\n\nOn CPU/C++ side, it looks almost the same:\n\nI create two fixed size buffers on the GPU and then upload the contents of (using techniques described above in the “Handling dynamic data” section).\n\nThe sprite renderer is used like this:\n\nAnd finally, here’s how the command to do the drawing looks like inside :\n\nThe complete sprite.vert looks like this:\n\nAll the parameters of the sprite draw command are self-explanatory, but needs a bit of clarification. Currently, I use it to branch inside the fragment shader:\n\nThis allows me to draw sprites differently depending on this ID without having to change pipelines. Of course, it can be potentially bad for the performance. This can be improved by drawing sprites with the same shader ID in batches. You’ll only need to switch pipelines when you encounter a draw command with a different shader ID.\n\nThe sprite renderer is very efficient: it can draw 10 thousand sprites in just 315 microseconds.\n\nI do skinning for skeletal animation in a compute shader. This allows me to have the same vertex format for all the meshes.\n\nBasically, I just take the mesh’s vertices (not skinned) and joint matrices and produce a new buffer of vertices which are used in later rendering stages.\n\nSuppose you spawn three cats with identical meshes:\n\nAll three of them can have different animations. They all have an identical “input” mesh. But the “output” vertex buffer will differ between them, which means that you need to pre-allocate a vertex buffer for each instance of the mesh.\n\nHere’s how the skinning compute shader looks like:\n• I store all joint matrices in a big array and populate it every frame (and also pass the starting index in the array for each skinned mesh, ).\n• Skinning data is not stored inside each mesh vertex, a separate buffer of elements is used.\n\nAfter the skinning is performed, all the later rendering stages use this set of vertices Thee rendering process for static and skinned meshes becomes identical, thanks to that.\n\nI have a game/renderer separation which uses a simple concept of “draw commands”. In the game logic, I use entt, but the renderer doesn’t know anything about entities or “game objects”. It only knows about the lights, some scene parameters (like fog, which skybox texture to use etc) and meshes it needs to draw.\n\nThe renderer’s API looks like this in action:\n\nWhen you call or , the renderer creates a mesh draw command and puts it in which are then iterated through during the drawing process. The looks like this:\n• is used for looking up static meshes in - it’s a simple of references to vertex buffers on GPU.\n• If the mesh has a skeleton, is used during compute skinning and is used for all the rendering afterwards (instead of )\n• is used for frustum culling.\n\nThis separation is nice because the renderer is clearly separated from the game logic. You can also do something more clever as described here if sorting draw commands becomes a bottleneck.\n\nI use Blender as a level editor and export it as glTF. It’s easy to place objects, colliders and lights there. Here’s how it looks like:\n\nWriting your own level editor would probably take months (years!), so using Blender instead saved me quite a lot of time.\n\nIt’s important to mention how I use node names for spawning some objects. For example, you can see an object named selected in the screenshot above. The part before the first dot is the prefab name (in this case “Interact”). The “Sphere” part is used by the physics system to create a sphere physics body for the object (“Capsule” and “Box” can also be used, otherwise the physics shape is created using mesh vertices).\n\nSome models are pretty complex and I don’t want to place them directly into the level glTF file as it’ll greatly increase each level’s size. I just place an “Empty->Arrows” object and name it something like “Cat.NearStore”. This will spawn “Cat” prefab and attach “NearStore” tag to it for runtime identification.\n\nPrefabs are written in JSON and look like this:\n\nDuring the level loading process, if the node doesn’t have a corresponding prefab, it’s loaded as-is and its mesh data is taken from the glTF file itself (this is mostly used for static geometry). If the node has a corresponding prefab loaded, it’s created instead. Its mesh data is loaded from the external glTF file - only transform is copied from the original glTF node (the one in the level glTF file).\n\nUsing forward rendering allowed me to easily implement MSAA. Here’s a comparison of how the game looks without AA and with MSAA on:\n\nMSAA is explained well here: https://vulkan-tutorial.com/Multisampling\n\nHere’s another good article about MSAA: https://therealmjp.github.io/posts/msaa-overview/ and potential problems you can have with it (especially with HDR and tone-mapping).\n\nMy UI system was inspired by Roblox’s UI API: https://create.roblox.com/docs/ui\n\nBasically, the UI can calculate its own layout without me having to hard code each individual element’s size and position. Basically it relies on the following concepts:\n• Origin is an anchor around which the UI element is positioned. If origin is , setting UI element’s position to be will make its upper-left pixel have (x,y) pixel coordinate. If the origin is , then the element’s bottom-right corner will be positioned at . If the origin is (0.5, 1) then it will be positioned using bottom-center point as the reference.\n• Relative size makes the children’s be proportional to parent’s size. If (1,1) then the child element will have the same size as the parent element. If it’s (0.5, 0.5) then it’ll have half the size of the parent. If the parent uses children’s size as a guide, then if a child has (0.5, 0.25) relative size, the parent’s width will be 2x larger and the height will be 4x larger.\n• Relative position uses parent’s size as a guide for positioning. It’s useful for centering elements, for example if you have an element with (0.5, 0.5) origin and (0.5, 0.5) relative position, it’ll be centered inside its parent element.\n• You can also set pixel offsets for both position and size separately (they’re called and in my codebase).\n• You can also set a fixed size for the elements if you don’t want them to ever be resized.\n• The label/image element size is determined using its content.\n\nHere are some examples of how it can be used to position child elements:\n\na) The child (yellow) has relative size (0.5, 1), relative position of (0.5, 0.5) and origin (0.5, 0.5) (alternatively, the relative position can be (0.5, 0.0) and origin at (0.5, 0.0) in this case). Its parent (green) will be two times wider, but will have the same height. The child element will be centered inside the parent.\n\nb) The child (yellow) has origin (1, 1), fixed size (w,h) and absolute offset of (x,y) - this way, the item can be positioned relative to the bottom-right corner of its parent (green)\n\nLet’s see how sizes and positions of UI elements are calculated (implementation in EDBR).\n\nFirst, sizes of all elements are calculated recursively. Then positions are computed based on the previously computed sizes and specified offset positions. Afterwards all elements are drawn recursively - parent element first, then its children etc.\n\nWhen calculating the size, most elements either have a “fixed” size (which you can set manually, e.g. you can set some button to always be 60x60 pixels) or their size is computed based on their content. For example, for label elements, their size is computed using the text’s bounding box. For image elements, their size equals the image size and so on.\n\nIf an element has an “Auto-size” property, it needs to specify which child will be used to calculate its size. For example, the menu nine-slice can have several text labels inside the “vertical layout” element - the bounding boxes will be calculated first, then their sizes will be summed up - then, the parent’s size is calculated.\n\nLet’s take a look at a simple menu with bounding boxes displayed:\n\nHere, root is marked as “Auto-size”. To compute its size, it first computes the size of its child ( ). This recursively computes the sizes of each button, sums them up and adds some padding ( also makes the width of each button the same based on the maximum width in the list).\n\nI love Dear ImGui. I used it to implement many useful dev and debug tools (open the image in a new tab to see them better):\n\nIt has some problems with sRGB, though. I won’t explain it in detail, but basically if you use sRGB framebuffer, Dear ImGui will look wrong in many ways, see the comparison:\n\nSometimes you can see people doing hacks by doing with Dear ImGui’s colors but it still doesn’t work properly with alpha and produces incorrect color pickers.\n\nI ended up writing my own Dear ImGui backend and implementing DilligentEngine’s workaround which is explained in detail here and here.\n\nThere are some additional benefits of having my own backend:\n• It supports bindless texture ids, so I can draw images by simply calling . Dear ImGui’s Vulkan backend requires you to “register” textures by calling for each texture before you can call .\n• It can properly draw linear and non-linear images by passing their format into backend (so that sRGB images are not gamma corrected twice when they’re displayed)\n• Initializing and dealing with it is easier as it does Vulkan things in the same way as the rest of my engine.\n\nThere are many parts of the engine not covered there because they’re not related to Vulkan. I still feel like it’s good to mention them briefly for the sake of completion.\n\nIntegrating it into the engine was pretty easy. Right now I mostly use it for collision resolution and basic character movement.\n\nThe samples are fantastic. The docs are very good too.\n\nI especially want to point out how incredible is. It handles basic character movement so well. I remember spending days trying to get proper slope movement in Bullet to work. With Jolt, it just worked “out of the box”.\n\nHere’s how it basically works (explaining how it works properly would probably require me to write quite a big article):\n• You add your shapes to Jolt’s world.\n• You get new positions of your physics objects and use these positions to render objects in their current positions.\n• I use entt for the entity-component-system part.\n\nIt has worked great for me so far. Previously I had my own ECS implementation, but decided to experiment with a 3rd party ECS library to have less code to maintain.\n• I use openal-soft, libogg and libvorbis for audio.\n\nThe audio system is mostly based on these articles: https://indiegamedev.net/2020/02/15/the-complete-guide-to-openal-with-c-part-1-playing-a-sound/\n\nIntegrating it was very easy (read the PDF doc, it’s fantastic!) and it helped me avoid tons of bike-shedding by seeing how little time something, which I thought was “inefficient”, really took.\n\nWhat I gained from switching to Vulkan\n\nThere are many nice things I got after switching to Vulkan:\n\nThis makes abstractions a lot easier. With OpenGL abstractions/engines, you frequently see “shader.bind()” calls, state trackers, magic RAII, which automatically binds/unbinds objects and so on. There’s no need for that in Vulkan - it’s easy to write functions which take some objects as an input and produce some output - stateless, more explicit and easier to reason about.\n• API is more pleasant to work with overall - I didn’t like “binding” things and the whole “global state machine” of OpenGL.\n• You need to write less abstractions overall.\n\nWith OpenGL, you need to write a lot of abstractions to make it all less error-prone… Vulkan’s API requires a lot less of this, in my experience. And usually the abstractions that you write map closer to Vulkan’s “raw” functions, compared to OpenGL abstractions which hide manipulation of global state and usually call several functions (and might do some stateful things for optimization).\n\nValidation errors are very good in Vulkan. While OpenGL has , it doesn’t catch that many issues and you’re left wondering why your texture looks weird, why your lighting is broken and so on. Vulkan has more extensive validation which makes the debugging process much better.\n\nI can now debug shaders in RenderDoc. It looks like this:\n\nWith OpenGL I had to output the values to some texture and color-pick them… which took a lot of time. But now I can debug vertex and fragment shaders easily.\n• More consistent experience across different GPUs and OSes.\n\nWith OpenGL, drivers on different GPUs and OSes worked differently from each other which made some bugs pop up only on certain hardware configurations. It made the process of debugging them hard. I still experienced some slight differences between different GPUs in Vulkan, but it’s much less prevalent compared to OpenGL.\n• Ability to use better shading languages in the future\n\nGLSL is a fine shading language, but there are some new shading languages which promise to be more feature-complete, convenient and readable, for example:\n\nI might explore them in the future and see if they offer me something that GLSL lacks.\n• More control over every aspect of the graphics pipeline.\n\nMy first OpenGL engine was written during the process of learning graphics programming from scratch. Many abstractions were not that good and rewriting them with some graphics programming knowledge (and some help from vkguide) helped me implement a much cleaner system.\n\nAnd finally, it makes me proud to be able to say “I have a custom engine written in Vulkan and it works”. Sometimes people start thinking about you as a coding wizard and it makes me happy and proud of my work. :)\n\nThere are many things that I plan to do in the future, here’s a list of some of them:\n• Loading many images and generating mipmaps in parallel (or use image formats which already have mipmaps stored inside of them)\n\nOverall, I’m quite satisfied with what I managed to accomplish. Learning Vulkan was quite difficult, but it wasn’t as hard as I imagined. It taught me a lot about graphics programming and modern APIs and now I have a strong foundation to build my games with."
    },
    {
        "link": "https://vulkan-tutorial.com/resources/vulkan_tutorial_en.pdf",
        "document": ""
    },
    {
        "link": "https://glfw.org/docs/3.3/vulkan_guide.html",
        "document": "This guide is intended to fill the gaps between the official Vulkan resources and the rest of the GLFW documentation and is not a replacement for either. It assumes some familiarity with Vulkan concepts like loaders, devices, queues and surfaces and leaves it to the Vulkan documentation to explain the details of Vulkan functions.\n\nTo develop for Vulkan you should download the LunarG Vulkan SDK for your platform. Apart from headers and link libraries, they also provide the validation layers necessary for development.\n\nThe Vulkan Tutorial has more information on how to use GLFW and Vulkan. The Khronos Vulkan Samples also use GLFW, although with a small framework in between.\n\nFor details on a specific Vulkan support function, see the Vulkan support reference. There are also guides for the other areas of the GLFW API.\n\nBy default, GLFW will look for the Vulkan loader on demand at runtime via its standard name ( on Windows, on Linux and other Unix-like systems and on macOS). This means that GLFW does not need to be linked against the loader. However, it also means that if you are using the static library form of the Vulkan loader GLFW will either fail to find it or (worse) use the wrong one.\n\nThe GLFW_VULKAN_STATIC CMake option makes GLFW call the Vulkan loader directly instead of dynamically loading it at runtime. Not linking against the Vulkan loader will then be a compile-time error.\n\nmacOS: To make your application be redistributable you will need to set up the application bundle according to the LunarG SDK documentation. This is explained in more detail in the SDK documentation for macOS.\n\nTo include the Vulkan header, define GLFW_INCLUDE_VULKAN before including the GLFW header.\n\nIf you instead want to include the Vulkan header from a custom location or use your own custom Vulkan header then do this before the GLFW header.\n\nUnless a Vulkan header is included, either by the GLFW header or above it, any GLFW functions that take or return Vulkan types will not be declared.\n\nThe macros do not need to be defined for the Vulkan part of GLFW to work. Define them only if you are using these extensions directly.\n\nIf you are linking directly against the Vulkan loader then you can skip this section. The canonical desktop loader library exports all Vulkan core and Khronos extension functions, allowing them to be called directly.\n\nIf you are loading the Vulkan loader dynamically instead of linking directly against it, you can check for the availability of a loader and ICD with glfwVulkanSupported.\n\nThis function returns if the Vulkan loader and any minimally functional ICD was found.\n\nIf one or both were not found, calling any other Vulkan related GLFW function will generate a GLFW_API_UNAVAILABLE error.\n\nTo load any Vulkan core or extension function from the found loader, call glfwGetInstanceProcAddress. To load functions needed for instance creation, pass as the instance.\n\nOnce you have created an instance, you can load from it all other Vulkan core functions and functions from any instance extensions you enabled.\n\nThis function in turn calls . If that fails, the function falls back to a platform-specific query of the Vulkan loader (i.e. or ). If that also fails, the function returns . For more information about , see the Vulkan documentation.\n\nVulkan also provides for loading device-specific versions of Vulkan function. This function can be retrieved from an instance with glfwGetInstanceProcAddress.\n\nDevice-specific functions may execute a little faster, due to not having to dispatch internally based on the device passed to them. For more information about , see the Vulkan documentation.\n\nTo do anything useful with Vulkan you need to create an instance. If you want to use Vulkan to render to a window, you must enable the instance extensions GLFW requires to create Vulkan surfaces.\n\nThese extensions must all be enabled when creating instances that are going to be passed to glfwGetPhysicalDevicePresentationSupport and glfwCreateWindowSurface. The set of extensions will vary depending on platform and may also vary depending on graphics drivers and other factors.\n\nIf it fails it will return and GLFW will not be able to create Vulkan window surfaces. You can still use Vulkan for off-screen rendering and compute work.\n\nIf successful the returned array will always include , so if you don't require any additional extensions you can pass this list directly to the struct.\n\nAdditional extensions may be required by future versions of GLFW. You should check whether any extensions you wish to enable are already in the returned array, as it is an error to specify an extension more than once in the struct.\n\nmacOS: MoltenVK is (as of July 2022) not yet a fully conformant implementation of Vulkan. As of Vulkan SDK 1.3.216.0, this means you must also enable the instance extension and set the bit in the instance creation info flags for MoltenVK to show up in the list of physical devices. For more information, see the Vulkan and MoltenVK documentation.\n\nNot every queue family of every Vulkan device can present images to surfaces. To check whether a specific queue family of a physical device supports image presentation without first having to create a window and surface, call glfwGetPhysicalDevicePresentationSupport.\n\nThe extension additionally provides the function, which performs the same test on an existing Vulkan surface.\n\nUnless you will be using OpenGL or OpenGL ES with the same window as Vulkan, there is no need to create a context. You can disable context creation with the GLFW_CLIENT_API hint.\n\nSee Windows without contexts for more information.\n\nYou can create a Vulkan surface (as defined by the extension) for a GLFW window with glfwCreateWindowSurface.\n\nIf an OpenGL or OpenGL ES context was created on the window, the context has ownership of the presentation on the window and a Vulkan surface cannot be created.\n\nIt is your responsibility to destroy the surface. GLFW does not destroy it for you. Call function from the same extension to destroy it."
    },
    {
        "link": "https://vulkan-tutorial.com/Drawing_a_triangle/Setup/Base_code",
        "document": "In the previous chapter you've created a Vulkan project with all of the proper configuration and tested it with the sample code. In this chapter we're starting from scratch with the following code:\n\nWe first include the Vulkan header from the LunarG SDK, which provides the functions, structures and enumerations. The and headers are included for reporting and propagating errors. The header provides the and macros.\n\nThe program itself is wrapped into a class where we'll store the Vulkan objects as private class members and add functions to initiate each of them, which will be called from the function. Once everything has been prepared, we enter the main loop to start rendering frames. We'll fill in the function to include a loop that iterates until the window is closed in a moment. Once the window is closed and returns, we'll make sure to deallocate the resources we've used in the function.\n\nIf any kind of fatal error occurs during execution then we'll throw a exception with a descriptive message, which will propagate back to the function and be printed to the command prompt. To handle a variety of standard exception types as well, we catch the more general . One example of an error that we will deal with soon is finding out that a certain required extension is not supported.\n\nRoughly every chapter that follows after this one will add one new function that will be called from and one or more new Vulkan objects to the private class members that need to be freed at the end in .\n\nJust like each chunk of memory allocated with requires a call to , every Vulkan object that we create needs to be explicitly destroyed when we no longer need it. In C++ it is possible to perform automatic resource management using RAII or smart pointers provided in the header. However, I've chosen to be explicit about allocation and deallocation of Vulkan objects in this tutorial. After all, Vulkan's niche is to be explicit about every operation to avoid mistakes, so it's good to be explicit about the lifetime of objects to learn how the API works.\n\nAfter following this tutorial, you could implement automatic resource management by writing C++ classes that acquire Vulkan objects in their constructor and release them in their destructor, or by providing a custom deleter to either or , depending on your ownership requirements. RAII is the recommended model for larger Vulkan programs, but for learning purposes it's always good to know what's going on behind the scenes.\n\nVulkan objects are either created directly with functions like , or allocated through another object with functions like . After making sure that an object is no longer used anywhere, you need to destroy it with the counterparts and . The parameters for these functions generally vary for different types of objects, but there is one parameter that they all share: . This is an optional parameter that allows you to specify callbacks for a custom memory allocator. We will ignore this parameter in the tutorial and always pass as argument.\n\nVulkan works perfectly fine without creating a window if you want to use it for off-screen rendering, but it's a lot more exciting to actually show something! First replace the line with\n\nThat way GLFW will include its own definitions and automatically load the Vulkan header with it. Add a function and add a call to it from the function before the other calls. We'll use that function to initialize GLFW and create a window.\n\nThe very first call in should be , which initializes the GLFW library. Because GLFW was originally designed to create an OpenGL context, we need to tell it to not create an OpenGL context with a subsequent call:\n\nBecause handling resized windows takes special care that we'll look into later, disable it for now with another window hint call:\n\nAll that's left now is creating the actual window. Add a private class member to store a reference to it and initialize the window with:\n\nThe first three parameters specify the width, height and title of the window. The fourth parameter allows you to optionally specify a monitor to open the window on and the last parameter is only relevant to OpenGL.\n\nIt's a good idea to use constants instead of hardcoded width and height numbers because we'll be referring to these values a couple of times in the future. I've added the following lines above the class definition:\n\nand replaced the window creation call with\n\nYou should now have a function that looks like this:\n\nTo keep the application running until either an error occurs or the window is closed, we need to add an event loop to the function as follows:\n\nThis code should be fairly self-explanatory. It loops and checks for events like pressing the X button until the window has been closed by the user. This is also the loop where we'll later call a function to render a single frame.\n\nOnce the window is closed, we need to clean up resources by destroying it and terminating GLFW itself. This will be our first code:\n\nWhen you run the program now you should see a window titled show up until the application is terminated by closing the window. Now that we have the skeleton for the Vulkan application, let's create the first Vulkan object!"
    },
    {
        "link": "https://vulkan-tutorial.com/Development_environment",
        "document": "In this chapter we'll set up your environment for developing Vulkan applications and install some useful libraries. All of the tools we'll use, with the exception of the compiler, are compatible with Windows, Linux and MacOS, but the steps for installing them differ a bit, which is why they're described separately here.\n\nIf you're developing for Windows, then I will assume that you are using Visual Studio to compile your code. For complete C++17 support, you need to use either Visual Studio 2017 or 2019. The steps outlined below were written for VS 2017.\n\nThe most important component you'll need for developing Vulkan applications is the SDK. It includes the headers, standard validation layers, debugging tools and a loader for the Vulkan functions. The loader looks up the functions in the driver at runtime, similarly to GLEW for OpenGL - if you're familiar with that.\n\nThe SDK can be downloaded from the LunarG website using the buttons at the bottom of the page. You don't have to create an account, but it will give you access to some additional documentation that may be useful to you.\n\nProceed through the installation and pay attention to the install location of the SDK. The first thing we'll do is verify that your graphics card and driver properly support Vulkan. Go to the directory where you installed the SDK, open the directory and run the demo. You should see the following:\n\nIf you receive an error message then ensure that your drivers are up-to-date, include the Vulkan runtime and that your graphics card is supported. See the introduction chapter for links to drivers from the major vendors.\n\nThere is another program in this directory that will be useful for development. The and programs will be used to compile shaders from the human-readable GLSL to bytecode. We'll cover this in depth in the shader modules chapter. The directory also contains the binaries of the Vulkan loader and the validation layers, while the directory contains the libraries.\n\nLastly, there's the directory that contains the Vulkan headers. Feel free to explore the other files, but we won't need them for this tutorial.\n\nAs mentioned before, Vulkan by itself is a platform agnostic API and does not include tools for creating a window to display the rendered results. To benefit from the cross-platform advantages of Vulkan and to avoid the horrors of Win32, we'll use the GLFW library to create a window, which supports Windows, Linux and MacOS. There are other libraries available for this purpose, like SDL, but the advantage of GLFW is that it also abstracts away some of the other platform-specific things in Vulkan besides just window creation.\n\nYou can find the latest release of GLFW on the official website. In this tutorial we'll be using the 64-bit binaries, but you can of course also choose to build in 32 bit mode. In that case make sure to link with the Vulkan SDK binaries in the directory instead of . After downloading it, extract the archive to a convenient location. I've chosen to create a directory in the Visual Studio directory under documents.\n\nUnlike DirectX 12, Vulkan does not include a library for linear algebra operations, so we'll have to download one. GLM is a nice library that is designed for use with graphics APIs and is also commonly used with OpenGL.\n\nGLM is a header-only library, so just download the latest version and store it in a convenient location. You should have a directory structure similar to the following now:\n\nNow that you've installed all of the dependencies we can set up a basic Visual Studio project for Vulkan and write a little bit of code to make sure that everything works.\n\nStart Visual Studio and create a new project by entering a name and pressing .\n\nMake sure that is selected as application type so that we have a place to print debug messages to, and check to prevent Visual Studio from adding boilerplate code.\n\nPress to create the project and add a C++ source file. You should already know how to do that, but the steps are included here for completeness.\n\nNow add the following code to the file. Don't worry about trying to understand it right now; we're just making sure that you can compile and run Vulkan applications. We'll start from scratch in the next chapter.\n\nLet's now configure the project to get rid of the errors. Open the project properties dialog and ensure that is selected, because most of the settings apply to both and mode.\n\nGo to and press in the dropdown box.\n\nAdd the header directories for Vulkan, GLFW and GLM:\n\nNext, open the editor for library directories under :\n\nAnd add the locations of the object files for Vulkan and GLFW:\n\nGo to and press in the dropdown box.\n\nEnter the names of the Vulkan and GLFW object files:\n\nAnd finally change the compiler to support C++17 features:\n\nYou can now close the project properties dialog. If you did everything right then you should no longer see any more errors being highlighted in the code.\n\nFinally, ensure that you are actually compiling in 64 bit mode:\n\nPress to compile and run the project and you should see a command prompt and a window pop up like this:\n\nThe number of extensions should be non-zero. Congratulations, you're all set for playing with Vulkan!\n\nThese instructions will be aimed at Ubuntu, Fedora and Arch Linux users, but you may be able to follow along by changing the package manager-specific commands to the ones that are appropriate for you. You should have a compiler that supports C++17 (GCC 7+ or Clang 5+). You'll also need .\n\nThe most important components you'll need for developing Vulkan applications on Linux are the Vulkan loader, validation layers, and a couple of command-line utilities to test whether your machine is Vulkan-capable:\n• or : Command-line utilities, most importantly and . Run these to confirm your machine supports Vulkan.\n• or : Installs Vulkan loader. The loader looks up the functions in the driver at runtime, similarly to GLEW for OpenGL - if you're familiar with that.\n• or : Installs the standard validation layers and required SPIR-V tools. These are crucial when debugging Vulkan applications, and we'll discuss them in the upcoming chapter.\n\nOn Arch Linux, you can run to install all the required tools above.\n\nIf installation was successful, you should be all set with the Vulkan portion. Remember to run and ensure you see the following pop up in a window:\n\nIf you receive an error message then ensure that your drivers are up-to-date, include the Vulkan runtime and that your graphics card is supported. See the introduction chapter for links to drivers from the major vendors.\n\nAs mentioned before, Vulkan by itself is a platform agnostic API and does not include tools for creation a window to display the rendered results. To benefit from the cross-platform advantages of Vulkan and to avoid the horrors of X11, we'll use the GLFW library to create a window, which supports Windows, Linux and MacOS. There are other libraries available for this purpose, like SDL, but the advantage of GLFW is that it also abstracts away some of the other platform-specific things in Vulkan besides just window creation.\n\nWe'll be installing GLFW from the following command:\n\nUnlike DirectX 12, Vulkan does not include a library for linear algebra operations, so we'll have to download one. GLM is a nice library that is designed for use with graphics APIs and is also commonly used with OpenGL.\n\nIt is a header-only library that can be installed from the or package:\n\nWe have just about all we need, except we'll want a program to compile shaders from the human-readable GLSL to bytecode.\n\nTwo popular shader compilers are Khronos Group's and Google's . The latter has a familiar GCC- and Clang-like usage, so we'll go with that: on Ubuntu, download Google's unofficial binaries and copy to your . Note you may need to depending on your permissions. On Fedora use , while on Arch Linux run . To test, run and it should rightfully complain we didn't pass any shaders to compile:\n\nWe'll cover in depth in the shader modules chapter.\n\nNow that you have installed all of the dependencies, we can set up a basic makefile project for Vulkan and write a little bit of code to make sure that everything works.\n\nCreate a new directory at a convenient location with a name like . Create a source file called and insert the following code. Don't worry about trying to understand it right now; we're just making sure that you can compile and run Vulkan applications. We'll start from scratch in the next chapter.\n\nNext, we'll write a makefile to compile and run this basic Vulkan code. Create a new empty file called . I will assume that you already have some basic experience with makefiles, like how variables and rules work. If not, you can get up to speed very quickly with this tutorial.\n\nWe'll first define a couple of variables to simplify the remainder of the file. Define a variable that will specify the basic compiler flags:\n\nWe're going to use modern C++ ( ), and we'll set optimization level to O2. We can remove -O2 to compile programs faster, but we should remember to place it back for release builds.\n\nThe flag is for GLFW, links with the Vulkan function loader and the remaining flags are low-level system libraries that GLFW needs. The remaining flags are dependencies of GLFW itself: the threading and window management.\n\nIt is possible that the and libraries are not yet installed on your system. You can find them in the following packages:\n\nSpecifying the rule to compile is straightforward now. Make sure to use tabs for indentation instead of spaces.\n\nVerify that this rule works by saving the makefile and running in the directory with and . This should result in a executable.\n\nWe'll now define two more rules, and , where the former will run the executable and the latter will remove a built executable:\n\nRunning should show the program running successfully, and displaying the number of Vulkan extensions. The application should exit with the success return code ( ) when you close the empty window. You should now have a complete makefile that resembles the following:\n\nYou can now use this directory as a template for your Vulkan projects. Make a copy, rename it to something like and remove all of the code in .\n\nYou are now all set for the real adventure.\n\nThese instructions will assume you are using Xcode and the Homebrew package manager. Also, keep in mind that you will need at least MacOS version 10.11, and your device needs to support the Metal API.\n\nThe most important component you'll need for developing Vulkan applications is the SDK. It includes the headers, standard validation layers, debugging tools and a loader for the Vulkan functions. The loader looks up the functions in the driver at runtime, similarly to GLEW for OpenGL - if you're familiar with that.\n\nThe SDK can be downloaded from the LunarG website using the buttons at the bottom of the page. You don't have to create an account, but it will give you access to some additional documentation that may be useful to you.\n\nThe SDK version for MacOS internally uses MoltenVK. There is no native support for Vulkan on MacOS, so what MoltenVK does is actually act as a layer that translates Vulkan API calls to Apple's Metal graphics framework. With this you can take advantage of debugging and performance benefits of Apple's Metal framework.\n\nAfter downloading it, simply extract the contents to a folder of your choice (keep in mind you will need to reference it when creating your projects on Xcode). Inside the extracted folder, in the folder you should have some executable files that will run a few demos using the SDK. Run the executable and you will see the following:\n\nAs mentioned before, Vulkan by itself is a platform agnostic API and does not include tools for creation a window to display the rendered results. We'll use the GLFW library to create a window, which supports Windows, Linux and MacOS. There are other libraries available for this purpose, like SDL, but the advantage of GLFW is that it also abstracts away some of the other platform-specific things in Vulkan besides just window creation.\n\nTo install GLFW on MacOS we will use the Homebrew package manager to get the package:\n\nVulkan does not include a library for linear algebra operations, so we'll have to download one. GLM is a nice library that is designed for use with graphics APIs and is also commonly used with OpenGL.\n\nIt is a header-only library that can be installed from the package:\n\nNow that all the dependencies are installed we can set up a basic Xcode project for Vulkan. Most of the instructions here are essentially a lot of \"plumbing\" so we can get all the dependencies linked to the project. Also, keep in mind that during the following instructions whenever we mention the folder we are refering to the folder where you extracted the Vulkan SDK.\n\nStart Xcode and create a new Xcode project. On the window that will open select Application > Command Line Tool.\n\nSelect , write a name for the project and for select .\n\nPress and the project should have been created. Now, let's change the code in the generated file to the following code:\n\nKeep in mind you are not required to understand all this code is doing yet, we are just setting up some API calls to make sure everything is working.\n\nXcode should already be showing some errors such as libraries it cannot find. We will now start configuring the project to get rid of those errors. On the Project Navigator panel select your project. Open the Build Settings tab and then:\n• Find the Header Search Paths field and add a link to (this is where Homebrew installs headers, so the glm and glfw3 header files should be there) and a link to for the Vulkan headers.\n• Find the Library Search Paths field and add a link to (again, this is where Homebrew installs libraries, so the glm and glfw3 lib files should be there) and a link to .\n\nIt should look like so (obviously, paths will be different depending on where you placed on your files):\n\nNow, in the Build Phases tab, on Link Binary With Libraries we will add both the and the frameworks. To make things easier we will be adding the dynamic libraries in the project (you can check the documentation of these libraries if you want to use the static frameworks).\n• For glfw open the folder and there you will find a file name like (\"x\" is the library's version number, it might be different depending on when you downloaded the package from Homebrew). Simply drag that file to the Linked Frameworks and Libraries tab on Xcode.\n• For vulkan, go to . Do the same for the both files and (where \"x\" will be the version number of the the SDK you downloaded).\n\nAfter adding those libraries, in the same tab on Copy Files change to \"Frameworks\", clear the subpath and deselect \"Copy only when installing\". Click on the \"+\" sign and add all those three frameworks here aswell.\n\nYour Xcode configuration should look like:\n\nThe last thing you need to setup are a couple of environment variables. On Xcode toolbar go to > > , and in the tab add the two following environment variables:\n\nIt should look like so:\n\nFinally, you should be all set! Now if you run the project (remembering to setting the build configuration to Debug or Release depending on the configuration you chose) you should see the following:\n\nThe number of extensions should be non-zero. The other logs are from the libraries, you might get different messages from those depending on your configuration.\n\nYou are now all set for the real thing."
    },
    {
        "link": "https://reddit.com/r/cpp/comments/g6hg2h/my_crossplatform_c17_window_library_eseed_window",
        "document": "Hah, I tried to post this a lil earlier and was wondering why nothing was happening, but I had accidentally posted it to my own profile B) so many intellect!!\n\nAnyway, here I am, hello cool programmers!\n\n4 days ago I made a post about a \"preview\" of my cross platform window library, ESeed Window (GitHub). I called it a preview then because, well, it only actually had one platform supported! Anyways, I am quite pleased to announce that after 4 cursed days of learning X11, ESeed Window has graduated from a concept and is now officially cross-platform for both X11 and Win32!\n\nThe entirety of the ESeed Window interface has been implemented in X11 with the expected behavior, including:\n\nThe library has currently been tested in GCC on Ubuntu 18.04 & MSVC for Windows. Other distros will likely need additional testing, and for this, outside contributions would be immensely helpful. In any case, I'm very happy with the progress being made so far.\n\nHere are a few code examples! :)\n\nCreating a Vulkan surface is also very simple!\n\nMacOS is next on the implementation list. I hope to finish it before too long.\n\nAs usual, more details can be found on the Github Page. If you're interested in the project and updates, consider starring and following me on GitHub! More documentation is on the way after the MacOS implementation is complete <3\n\nI think that's all for this time. Hope everyone is well, have a great day!"
    },
    {
        "link": "https://github.com/Noxagonal/Vulkan-API-Tutorials/blob/master/Tutorial%20-%200005%20-%20GLFW%20Example/main.cpp",
        "document": "and no one associated with this source code can be held responsible for any possible\n\nWe'll need the swapchain for sure if we want to display anything\n\nFor windows this would be vkCreateWin32SurfaceKHR() or on linux XCB window library this would be vkCreateXcbSurfaceKHR()\n\nAll regular Vulkan API stuff goes here, no more GLFW commands needed for the window.\n\nWe still need to initialize the swapchain, it's images and all the rest\n\njust like we would have done with OS native windows."
    },
    {
        "link": "https://learnopengl.com/Model-Loading/Assimp",
        "document": "In all the scenes so far we've been extensively playing with our little container friend, but over time, even our best friends can get a little boring. In bigger graphics applications, there are usually lots of complicated and interesting models that are much prettier to look at than a static container. However, unlike the container object, we can't really manually define all the vertices, normals, and texture coordinates of complicated shapes like houses, vehicles, or human-like characters. What we want instead, is to import these models into the application; models that were carefully designed by 3D artists in tools like Blender, 3DS Max or Maya.\n\nThese so called allow artists to create complicated shapes and apply textures to them via a process called . The tools then automatically generate all the vertex coordinates, vertex normals, and texture coordinates while exporting them to a model file format we can use. This way, artists have an extensive toolkit to create high quality models without having to care too much about the technical details. All the technical aspects are hidden in the exported model file. We, as graphics programmers, do have to care about these technical details though.\n\nIt is our job to parse these exported model files and extract all the relevant information so we can store them in a format that OpenGL understands. A common issue is that there are dozens of different file formats where each exports the model data in its own unique way. Model formats like the Wavefront .obj only contains model data with minor material information like model colors and diffuse/specular maps, while model formats like the XML-based Collada file format are extremely extensive and contain models, lights, many types of materials, animation data, cameras, complete scene information, and much more. The wavefront object format is generally considered to be an easy-to-parse model format. It is recommended to visit the Wavefront's wiki page at least once to see how such a file format's data is structured. This should give you a basic perception of how model file formats are generally structured.\n\nAll by all, there are many different file formats where a common general structure between them usually does not exist. So if we want to import a model from these file formats, we'd have to write an importer ourselves for each of the file formats we want to import. Luckily for us, there just happens to be a library for this.\n\nA very popular model importing library out there is called Assimp that stands for Open Asset Import Library. Assimp is able to import dozens of different model file formats (and export to some as well) by loading all the model's data into Assimp's generalized data structures. As soon as Assimp has loaded the model, we can retrieve all the data we need from Assimp's data structures. Because the data structure of Assimp stays the same, regardless of the type of file format we imported, it abstracts us from all the different file formats out there.\n\nWhen importing a model via Assimp it loads the entire model into a scene object that contains all the data of the imported model/scene. Assimp then has a collection of nodes where each node contains indices to data stored in the scene object where each node can have any number of children. A (simplistic) model of Assimp's structure is shown below:\n• All the data of the scene/model is contained in the object like all the materials and the meshes. It also contains a reference to the root node of the scene.\n• The of the scene may contain children nodes (like all other nodes) and could have a set of indices that point to mesh data in the scene object's array. The scene's array contains the actual objects, the values in the array of a node are only indices for the scene's meshes array.\n• A object itself contains all the relevant data required for rendering, think of vertex positions, normal vectors, texture coordinates, faces, and the material of the object.\n• A mesh contains several faces. A represents a render primitive of the object (triangles, squares, points). A face contains the indices of the vertices that form a primitive. Because the vertices and the indices are separated, this makes it easy for us to render via an index buffer (see Hello Triangle).\n• Finally a mesh also links to a object that hosts several functions to retrieve the material properties of an object. Think of colors and/or texture maps (like diffuse and specular maps).\n\nWhat we want to do is: first load an object into a object, recursively retrieve the corresponding objects from each of the nodes (we recursively search each node's children), and process each object to retrieve the vertex data, indices, and its material properties. The result is then a collection of mesh data that we want to contain in a single object.\n\nIn the next chapters we'll create our own and class that load and store imported models using the structure we've just described. If we then want to draw a model, we do not render the model as a whole, but we render all of the individual meshes that the model is composed of. However, before we can start importing models, we first need to actually include Assimp in our project.\n\nYou can download Assimp from their GitHub page and choose the corresponding version. For this writing, the Assimp version used was version . It is advised to compile the libraries by yourself, since their pre-compiled libraries don't always work on all systems. Review the Creating a window chapter if you forgot how to compile a library by yourself via CMake.\n\nA few issues can come up while building Assimp, so I'll note them down here with their solutions in case any of you get the same errors:\n• CMake continually gives errors while retrieving the configuration list about DirectX libraries missing, messages like: Could not locate DirectX CMake Error at cmake-modules/FindPkgMacros.cmake:110 (message): Required library DirectX not found! Install the library (including dev packages) and try again. If the library is already installed, set the missing variables manually in cmake. The solution here is to install the DirectX SDK in case you haven't installed this before. You can download the SDK from here.\n• While installing the DirectX SDK, a possible error code of could pop up. In that case you first want to de-install the C++ Redistributable package(s) before installing the SDK.\n\nOnce the configuration is completed, you can generate a solution file, open it, and compile the libraries (either as a release version or a debug version, whatever floats your boat). Be sure to compile it for 64-bit as all LearnOpenGL code is 64 bit.\n\nThe default configuration builds Assimp as a dynamic library so we need to include the resulting DLL named (or with some post-fix) alongside the application's binaries. You can simply copy the DLL to the same folder where your application's executable is located.\n\nAfter compiling the generated solution, the resulting library and DLL file are located in the or folder. Then simply move the lib and DLL to their appropriate locations, link them from your solution, and be sure to copy Assimp's headers to your directory (the header files are found in the folder in the files downloaded from Assimp).\n\nBy now you should have compiled Assimp and linked it to your application. If you still received any unreported error, feel free to ask for help in the comments."
    },
    {
        "link": "https://assimp-docs.readthedocs.io/en/latest/usage/use_the_lib.html",
        "document": "The Asset-Importer-Lib can be accessed by both a class or flat function interface. The C++ class the interface is the preferred way of interaction: you create an instance of the class Assimp::Importer, maybe adjust some settings of it and then just call The class will read the files and process its data, handing back the imported data as a pointer to an aiScene to you. You can now extract the data you need from the file. The importer manages all the resources for itself. If the importer is destroyed, all the data that was created/read by it will be destroyed, too. So the easiest way to use the Importer is to create an instance locally, use its results and then simply let it go out of scope. 're done. Everything will be cleaned up by the importer destructor What exactly is read from the files and how you interpret it is described at the:ref: . The post-processing steps that the Assimp library can apply to the imported data are listed at #aiPostProcessSteps. See the @ref pp Post processing page for more details. Note that the aiScene data structure returned is declared ‘const’. Yes, you can get rid of these 5 letters with a simple cast. Yes, you may do that. No, it’s not recommended (and it’s suicide in DLL builds if you try to use new or delete on any of the arrays in the scene). The plain function interface is just as simple but requires you to manually call the clean-up after you’re done with the imported data. To start the import process, call aiImportFile() with the filename in question and the desired postprocessing flags like above. If the call is successful, an aiScene pointer with the imported data is handed back to you. When you’re done with the extraction of the data you’re interested in, call aiReleaseImport() on the imported scene to clean up all resources associated with the import. 're done. Release all resources associated with this import Using custom IO logic with the C++ class interface The Assimp library needs to access files internally. This of course applies to the file you want to read, but also to additional files in the same folder for certain file formats. By default, standard C/C++ IO logic is used to access these files. If your application works in a special environment where custom logic is needed to access the specified files, you have to supply custom implementations of IOStream and IOSystem. A shortened example might look like this: Now that your IO system is implemented, supply an instance of it to the Importer object by calling Using custom IO logic with the plain-c function interface The C interface also provides a way to override the file system. Control is not as fine-grained as for C++ although surely enough for almost any purpose. The process is simple:\n• None Fill an aiFileIO structure with custom file system callbacks (they’re self-explanatory as they work similarly to the CRT’s fXXX functions)\n• None and pass it as a parameter to #aiImportFileEx The Assimp-library provides an easy mechanism to log messages. For instance, if you want to check the state of your import and you just want to see, after which preprocessing step the import process was aborted you can take a look into the log. Per default, the Assimp-library provides a default log implementation, where you can log your user-specific message by calling it a singleton with the requested logging type. To see how this works take a look at this: At first, you have to create the default-logger-instance (create). Now you are ready to rock and can log a little bit around. After that, you should kill it to release the singleton instance. If you want to integrate the assimp-log into your own GUI it may be helpful to have a mechanism writing the logs into your own log windows. The logger interface provides this by implementing an interface called LogStream. You can attach and detach this log stream to the default-logger instance or any implementation derived from Logger. Just derivate your own logger from the abstract base-class LogStream and overwrite the write method: The severity level controls the kind of message which will be written into the attached stream. If you just want to log errors and warnings set the warn and error severity flag for those severities. It is also possible to remove a self-defined log stream from an error severity by detaching it with the severity flag set: If you want to implement your own logger just derive from the abstract base class Logger and overwrite the methods debug, info, warn, and error. If you want to see the debug messages in a debug-configured build, the Logger-interface provides a logging severity. You can set it by calling the following method: The normal logging severity supports just the basic stuff like info, warnings, and errors. In the verbose level very fine-grained debug messages will be logged, too. Note that this kind of logging might decrease import performance.\n\nThe Assimp-Library returns the imported data in a collection of structures. aiScene forms the root of the data, from here you gain access to all the nodes, meshes, materials, animations, or textures that was read from the imported file. The aiScene is returned from a successful call to Assimp::Importer::ReadFile(), aiImportFile() or aiImportFileEx() - see the ai_usage for further information on how to use the library. By default, all 3D data is provided in a right-handed coordinate system such as OpenGL uses. In this coordinate system, +X points to the right, +Y points upwards and +Z points out of the screen towards the viewer. Several modeling packages such as 3D Studio Max use this coordinate system as well (or a rotated variant of it). By contrast, some other environments use left-handed coordinate systems, a prominent example being DirectX. If you need the imported data to be in a left-handed coordinate system, supply the #aiProcess_MakeLeftHanded flag to the ReadFile() function call. The output face winding is counterclockwise. Use #aiProcess_FlipWindingOrder to get CW data. Outputted polygons can be literally everything: they’re probably concave, self-intersecting or non-planar, although our built-in triangulation (#aiProcess_Triangulate postprocessing step) doesn’t handle the two latter. The output UV coordinate system has its origin in the lower-left corner: Use the #aiProcess_FlipUVs flag to get UV coordinates with the upper-left corner as origin. A typical 4x4 matrix including a translational part looks like this: with <tt>(X1, X2, X3)</tt> being the local X base vector, <tt>(Y1, Y2, Y3)</tt> being the local Y base vector, <tt>(Z1, Z2, Z3)</tt> being the local Z base vector and <tt>(T1, T2, T3)</tt> being the offset of the local origin (the translational part). All matrices in the library use row-major storage order. That means that the matrix elements are stored row-by-row, i.e. they end up like this in memory: <tt>[X1, Y1, Z1, T1, X2, Y2, Z2, T2, X3, Y3, Z3, T3, 0, 0, 0, 1]</tt>. Note that this is neither the OpenGL format nor the DirectX format, because both of them specify the matrix layout such that the translational part occupies three consecutive addresses in memory (so those matrices end with <tt>[…, T1, T2, T3, 1]</tt>), whereas the translation in an Assimp matrix is found at the offsets 3, 7 and 11 (spread across the matrix). You can transpose an Assimp matrix to end up with the format that OpenGL and DirectX mandate. To be very precise: The transposition has nothing to do with a left-handed or right-handed coordinate system but ‘converts’ between row-major and column-major storage formats. <b>11.24.09:</b> We changed the orientation of our quaternions to the most common convention to avoid confusion. However, if you’re a previous user of Assimp and you update the library to revisions beyond SVNREV 502, you have to adapt your animation loading code to match the new quaternion orientation. Nodes are little-named entities in the scene that have a place and orientation relative to their parents. Starting from the scene’s root node all nodes can have 0 to x child nodes, thus forming a hierarchy. They form the base on which the scene is built: a node can refer to 0..x meshes, can be referred to by a bone of a mesh, or can be animated by a key sequence of animation. DirectX calls them “frames”, others call them “objects”, and we call them aiNode. A node can potentially refer to single or multiple meshes. The meshes are not stored inside the node, but instead in an array of aiMesh inside the aiScene. A node only refers to them by their array index. This also means that multiple nodes can refer to the same mesh, which provides a simple form of instancing. A mesh referred to in this way lives in the node’s local coordinate system. If you want the mesh’s orientation in global space, you’d have to concatenate the transformations from the referring node and all of its parents. Most of the file formats don’t really support complex scenes, though, but a single model only. But there are more complex formats such as .3ds, .x, or .collada scenes which may contain an arbitrarily complex hierarchy of nodes and meshes. I myself would suggest a recursive filter function such as the following pseudocode: This function copies a node into the scene graph if it has children. If yes, a new scene object is created for the import node and the node’s meshes are copied over. If not, no object is created. Potential child objects will be added to the old targetParent, but their transformation will be correct in respect to the global space. This function also works great in filtering the bone nodes - nodes that form the bone hierarchy for another mesh/node, but don’t have any mesh themselves. All meshes of an imported scene are stored in an array of aiMesh* inside the aiScene. Nodes refer to them by their index in the array and provide the coordinate system for them, too. One mesh uses only a single material everywhere - if parts of the model use a different material, this part is moved to a separate mesh at the same node. The mesh refers to its material in the same way as the node refers to its meshes: materials are stored in an array inside aiScene, and the mesh stores only an index into this array. An aiMesh is defined by a series of data channels. The presence of these data channels is defined by the contents of the imported file: by default, there are only those data channels present in the mesh that were also found in the file. The only channels guaranteed to be always present are aiMesh::mVertices and aiMesh::mFaces. You can test for the presence of other data by testing the pointers against NULL or using the helper functions provided by aiMesh. You may also specify several post-processing flags at Importer::ReadFile() to let Assimp calculate or recalculate additional data channels for you. At the moment, a single aiMesh may contain a set of triangles and polygons. A single vertex does always have a position. In addition, it may have one normal, one tangent, and bitangent, zero to AI_MAX_NUMBER_OF_TEXTURECOORDS (4 at the moment) texture coords and zero to AI_MAX_NUMBER_OF_COLOR_SETS (4) vertex colors. In addition, a mesh may or may not have a set of bones described by an array of aiBone structures. How to interpret the bone information is described later on. A mesh may have a set of bones in the form of instances from the aiBone objects. Bones are a means to deform a mesh according to the movement of a skeleton. Each bone has a name and a set of vertices on which it has influence. Its offset matrix declares the transformation needed to transform from mesh space to the local space of this bone. Using the bones name you can find the corresponding node in the node hierarchy. This node in relation to the other bones’ nodes defines the skeleton of the mesh. Unfortunately, there might also be nodes that are not used by a bone in the mesh but still affect the pose of the skeleton because they have child nodes which are bones. So when creating the skeleton hierarchy for a mesh I suggest the following method: a. Create a map or a similar container to store which nodes are necessary for the skeleton. Pre-initialize it for all nodes with a “no”.\n• None For each bone in the mesh: b1. Find the corresponding node in the scene’s hierarchy by comparing their names. b2. Mark this node as “yes” in the necessityMap. b3. Mark all of its parents the same way until you\n• None the parent of the mesh’s node. c1. If the node is marked as necessary, copy it into the skeleton and check its children. c2. If the node is marked as not necessary, skip it and do not iterate over its children. Reasons: you need all the parent nodes to keep the transformation chain intact. For most file formats and modeling packages, the node hierarchy of the skeleton is either a child of the mesh node or a sibling of the mesh node but this is by no means a requirement so you shouldn’t rely on it. The node closest to the root node is your skeleton root, from there, you start copying the hierarchy. You can skip every branch without a node being a bone in the mesh - that’s why the algorithm skips the whole branch if the node is marked as “not necessary”. You should now have a mesh in your engine with a skeleton that is a subset of the imported hierarchy. An imported scene may contain zero to n aiAnimation entries. An animation in this context is a set of keyframe sequences where each sequence describes the orientation of a single node in the hierarchy over a limited time span. Animations of this kind are usually used to animate the skeleton of a skinned mesh, but there are other uses as well. An aiAnimation has a duration. The duration as well as all time stamps are given in ticks. To get the correct timing, all time stamps thus have to be divided by aiAnimation::mTicksPerSecond. Beware, though, that certain combinations of file format and exporter don’t always store this information in the exported file. In this case, mTicksPerSecond is set to 0 to indicate the lack of knowledge. The aiAnimation consists of a series of aiNodeAnim. Each bone animation affects a single node in the node hierarchy only, the name specifying which node is affected. For this node the structure stores three separate key sequences: a vector key sequence for the position, a quaternion key sequence for the rotation, and another vector key sequence for the scaling. All 3d data is local to the coordinate space of the node’s parent, which means in the same space as the node’s transformation matrix. There might be cases where animation tracks refer to a non-existent node by their name, but this should not be the case in your everyday data. To apply such an animation you need to identify the animation tracks that refer to actual bones in your mesh. Then for every track: * Find the keys that lay right before the current anim time. * Optional: interpolate between these and the following keys. * Combine the calculated position, rotation, and scaling into a transformation matrix * Set the affected node’s transformation to the calculated matrix. If you need hints on how to convert to or from quaternions, have a look at the Matrix & Quaternion FAQ. I suggest using logarithmic interpolation for the scaling keys if you happen to need them - usually, you don’t need them at all. Normally textures used by assets are stored in separate files, however, there are file formats embedding their textures directly into the model file. Such textures are loaded into an aiTexture structure. In previous versions, the path from the query for would be where is the index of the texture in aiScene::mTextures. Now this call will return a file path for embedded textures in FBX files. To test if it is an embedded texture use aiScene::GetEmbeddedTexture. If the returned pointer is not null, it is embedded and can be loaded from the data structure. If it is null, search for a separate file. Other file types still use the old behavior. If you rely on the old behavior, you can use Assimp::Importer::SetPropertyBool with the key #AI_CONFIG_IMPORT_FBX_EMBEDDED_TEXTURES_LEGACY_NAMING to force the old behavior. There are two cases:\n• None The texture is NOT compressed. Its color data is directly stored in the aiTexture structure as an array of aiTexture::mWidth * aiTexture::mHeight aiTexel structures. Each aiTexel represents a pixel (or “texel”) of the texture image. The color data is stored in an unsigned RGBA8888 format, which can be easily used for both Direct3D and OpenGL (swizzling the order of the color components might be necessary). RGBA8888 has been chosen because it is well-known, easy to use , and natively supported by nearly all graphics APIs.\n• None This applies if aiTexture::mHeight == 0 is fulfilled. Then, the texture is stored in a compressed format such as DDS or PNG. The term “compressed” does not mean that the texture data must actually be compressed, however, the texture was found in the model file as if it was stored in a separate file on the hard disk. Appropriate decoders (such as libjpeg, libpng, D3DX, DevIL) are required to load these textures. aiTexture::mWidth specifies the size of the texture data in bytes, aiTexture::pcData is a pointer to the raw image data and aiTexture::achFormatHint is either zeroed or contains the most common file extension of the embedded texture’s format. This value is only set if Assimp is able to determine the file format.\n\nAll materials are stored in an array of aiMaterial inside the aiScene. Each aiMesh refers to one material by its index in the array. Due to the vastly diverging definitions and usages of material parameters, there is no hard definition of a material structure. Instead, a material is defined by a set of properties accessible by their names. Have a look at assimp/material.h to see what types of properties are defined. In this file, there are also various functions defined to test for the presence of certain properties in a material and retrieve their values. Textures are organized in stacks, each stack being evaluated independently. The final color value from a particular texture stack is used in the shading equation. For example, the computed color value of the diffuse texture stack (aiTextureType_DIFFUSE) is multiplied with the amount of incoming diffuse light to obtain the final diffuse color of a pixel. All material key constants start with ‘AI_MATKEY’ as a prefix. The name of the material, if available. Ignored by <tt>aiProcess_RemoveRedundantMaterials. Materials are considered equal even if their names are different. Diffuse color of the material. This is typically scaled by the amount of incoming diffuse light (e.g. using gouraud shading) Specular color of the material. This is typically scaled by the amount of incoming specular light (e.g. using phong shading) Ambient color of the material. This is typically scaled by the amount of ambient light Emissive color of the material. This is the amount of light emitted by the object. In real time applications it will usually not affect surrounding objects, but raytracing applications may wish to treat emissive objects as light sources. Defines the transparent color of the material, this is the color to be multiplied with the color of translucent light to construct the final ‘destination color’ for a particular position in the screen buffer. Defines the reflective color of the material. This is typically scaled by the amount of incoming light from the direction of mirror reflection. Usually combined with an environment lightmap of some kind for real-time applications. Scales the reflective color of the material. Specifies whether wireframe rendering must be turned on for the material. 0 for false, !0 for true. Specifies whether meshes using this material must be rendered without backface culling. 0 for false, !0 for true. Some importers set this property if they don’t know whether the output face order is right. As long as it is not set, you may safely enable backface culling. One of the #aiShadingMode enumerated values. Defines the library shading model to use for (real time) rendering to approximate the original look of the material as closely as possible. The presence of this key might indicate a more complex material. If absent, assume phong shading only if a specular exponent is given. One of the #aiBlendMode enumerated values. Defines how the final color value in the screen buffer is computed from the given color at that position and the newly computed color from the material. Simply said, alpha blending settings. Defines the opacity of the material in a range between 0..1. Use this value to decide whether you have to activate alpha blending for rendering. OPACITY!=1 usually also implies TWOSIDED=1 to avoid cull artifacts. Defines the shininess of a phong-shaded material. This is actually the exponent of the phong specular equation Scales the specular color of the material. This value is kept separate from the specular color by most modelers, and so do we. Defines the Index Of Refraction for the material. That’s not supported by most file formats. Might be of interest for raytracing. Defines the path of the n’th texture on the stack ‘t’, where ‘n’ is any value >= 0 and ‘t’ is one of the #aiTextureType enumerated values. A file path to an external file or an embedded texture. Use aiScene::GetEmbeddedTexture to test if it is embedded for FBX files, in other cases embedded textures start with ‘*’ followed by an index into aiScene::mTextures. See the @ref mat_tex section above. Also see @ref textures for a more information about texture retrieval. Defines the strength the n’th texture on the stack ‘t’. All color components (rgb) are multiplied with this factor before any further processing is done. One of the #aiTextureOp enumerated values. Defines the arithmetic operation to be used to combine the n’th texture on the stack ‘t’ with the n-1’th. TEXOP(t,0) refers to the blend operation between the base color for this stack (e.g. COLOR_DIFFUSE for the diffuse stack) and the first texture. Defines how the input mapping coordinates for sampling the n’th texture on the stack ‘t’ are computed. Usually explicit UV coordinates are provided, but some model file formats might also be using basic shapes, such as spheres or cylinders, to project textures onto meshes. See the ‘Textures’ section below. #aiProcess_GenUVCoords can be used to let Assimp compute proper UV coordinates from projective mappings. Defines the UV channel to be used as input mapping coordinates for sampling the n’th texture on the stack ‘t’. All meshes assigned to this material share the same UV channel setup Presence of this key implies MAPPING(t,n) to be #aiTextureMapping_UV. See @ref uvwsrc for more details. Any of the #aiTextureMapMode enumerated values. Defines the texture wrapping mode on the x axis for sampling the n’th texture on the stack ‘t’. ‘Wrapping’ occurs whenever UVs lie outside the 0..1 range. Wrap mode on the v axis. See MAPPINGMODE_U. Defines the base axis to to compute the mapping coordinates for the n’th texture on the stack ‘t’ from. This is not required for UV-mapped textures. For instance, if <tt>MAPPING(t,n)</tt> is #aiTextureMapping_SPHERE, U and V would map to longitude and latitude of a sphere around the given axis. The axis is given in local mesh space. Defines miscellaneous flag for the n’th texture on the stack ‘t’. This is a bitwise combination of the #aiTextureFlags enumerated values. Retrieving a property from a material is done using various utility functions. For C++ it’s simply calling aiMaterial::Get() Simple, isn’t it? To get the name of a material you would use Or for the diffuse color (‘color’ won’t be modified if the property is not set) <b>Note:</b> Get() is actually a template with explicit specializations for aiColor3D, aiColor4D, aiString, float, int and some others. Make sure that the type of the second parameter matches the expected data type of the material property (no compile-time check yet!). Don’t follow this advice if you wish to encounter very strange results. For good old C it’s slightly different. Take a look at the aiGetMaterialGet<data-type> functions. To get the name of a material you would use Or for the diffuse color (‘color’ won’t be modified if the property is not set) How to map UV channels to textures (MATKEY_UVWSRC) The MATKEY_UVWSRC property is only present if the source format doesn’t specify an explicit mapping from textures to UV channels. Many formats don’t do this and assimp is not aware of a perfect rule either. Your handling of UV channels needs to be flexible therefore. Our recommendation is to use logic like this to handle most cases properly: have only one uv channel? assign channel 0 to all textures and break have uvwsrc for this texture? assign channels in ascending order for all texture stacks, i.e. diffuse1 gets channel 1, opacity0 gets channel 0. For completeness, the following is a very rough pseudo-code sample showing how to evaluate Assimp materials in your shading pipeline. You’ll probably want to limit your handling of all those material keys to a reasonable subset suitable for your purposes (for example most 3d engines won’t support highly complex multi-layer materials, but many 3d modellers do). Also note that this sample is targeted at a (shader-based) rendering pipeline for real-time graphics. You can get the shadercode like a texture (AI_MATKEY_GLOBAL_SHADERLANG and AI_MATKEY_SHADER_VERTEX, …) You can get assigned shader sources by using the following material keys:\n• None AI_MATKEY_GLOBAL_SHADERLANG To get the used shader language.\n\nAssimp has built-in support for <i>very</i> basic profiling and time measurement. To turn it on, set the <tt>GLOB_MEASURE_TIME</tt> configuration switch to <tt>true</tt> (nonzero). Results are dumped to the log file, so you need to set up an appropriate logger implementation with at least one output stream first (see the @:ref: for the details.). Note that these measurements are based on a single run of the importer and each of the post-processing steps, so a single result set is far away from being significant in a statistical sense. While precision can be improved by running the test multiple times, the low accuracy of the timings may render the results useless for smaller files. A sample report looks like this (some unrelated log messages omitted, entries grouped for clarity): Debug, T5488: START `total` Info, T5488: Found a matching importer for this file format Debug, T5488: START `import` Info, T5488: BlendModifier: Applied the `Subdivision` modifier to `OBMonkey` Debug, T5488: END `import`, dt= 3.516 s Debug, T5488: START `preprocess` Debug, T5488: END `preprocess`, dt= 0.001 s Info, T5488: Entering post processing pipeline Debug, T5488: START `postprocess` Debug, T5488: RemoveRedundantMatsProcess begin Debug, T5488: RemoveRedundantMatsProcess finished Debug, T5488: END `postprocess`, dt= 0.001 s Debug, T5488: START `postprocess` Debug, T5488: TriangulateProcess begin Info, T5488: TriangulateProcess finished. All polygons have been triangulated. Debug, T5488: END `postprocess`, dt= 3.415 s Debug, T5488: START `postprocess` Debug, T5488: SortByPTypeProcess begin Info, T5488: Points: 0, Lines: 0, Triangles: 1, Polygons: 0 (Meshes, X = removed) Debug, T5488: SortByPTypeProcess finished Debug, T5488: START `postprocess` Debug, T5488: JoinVerticesProcess begin Debug, T5488: Mesh 0 (unnamed) | Verts in: 503808 out: 126345 | ~74.922 Info, T5488: JoinVerticesProcess finished | Verts in: 503808 out: 126345 | ~74.9 Debug, T5488: END `postprocess`, dt= 2.052 s Debug, T5488: START `postprocess` Debug, T5488: FlipWindingOrderProcess begin Debug, T5488: FlipWindingOrderProcess finished Debug, T5488: END `postprocess`, dt= 0.006 s Debug, T5488: START `postprocess` Debug, T5488: LimitBoneWeightsProcess begin Debug, T5488: LimitBoneWeightsProcess end Debug, T5488: END `postprocess`, dt= 0.001 s Debug, T5488: START `postprocess` Debug, T5488: ImproveCacheLocalityProcess begin Debug, T5488: Mesh 0 | ACMR in: 0.851622 out: 0.718139 | ~15.7 Info, T5488: Cache relevant are 1 meshes (251904 faces). Average output ACMR is 0.718139 Debug, T5488: ImproveCacheLocalityProcess finished. Debug, T5488: END `postprocess`, dt= 1.903 s Info, T5488: Leaving post processing pipeline Debug, T5488: END `total`, dt= 11.269 s In this particular example only one fourth of the total import time was spent on the actual importing, while the rest of the time got consumed by the #aiProcess_Triangulate, #aiProcess_JoinIdenticalVertices and #aiProcess_ImproveCacheLocality postprocessing steps. A wise selection of postprocessing steps is therefore essential to getting good performance. Of course this depends on the individual requirements of your application, in many of the typical use cases of assimp performance won’t matter (i.e. in an offline content pipeline). You can use the Asset-Importer-Library in a separate thread context. Just make sure that the resources used by the thread are not shared. At this moment, assimp will not make sure that it is safe over different thread contexts. This page discusses both Assimp scalability in threaded environments and the precautions to be taken in order to use it from multiple threads concurrently. Thread-safety / using Assimp concurrently from several threads The library can be accessed by multiple threads simultaneously, as long as the following prerequisites are fulfilled:\n• None Users of the C++-API should ensure that they use a dedicated #Assimp::Importer instance for each thread. Constructing instances of #Assimp::Importer is expensive, so it might be a good idea to let every thread maintain its own thread-local instance (which can be used to load as many files as necessary).\n• None When supplying custom IO logic, one must make sure the underlying implementation is thread-safe.\n• None Custom log streams or logger replacements have to be thread-safe, too. Multiple concurrent imports may or may not be beneficial, however. For certain file formats in conjunction with little or no post-processing IO times tend to be the performance bottleneck. Intense post-processing together with ‘slow’ file formats like X or Collada might scale well with multiple concurrent imports. Internal multi-threading is not currently implemented. This page lists some useful resources for Assimp. Note that, even though the core team has an eye on them, we cannot guarantee the accuracy of third-party information. If in doubt, it’s best to ask either on the mailing list or on our forums on SF.net.\n• None Assimp comes with some sample applications, these can be found in the <i>./samples</i> folder. Don’t forget to read the <i>README</i> file.\n• None Assimp-Animation-Loader is another utility to simplify animation playback.\n• None Assimp-Animations - Tutorial “Loading models using the Open Asset Import Library”, out of a series of OpenGL tutorials.\n\nThis section contains implementation notes for the Blender3D importer. Assimp provides a self-contained reimplementation of Blender’s so-called SDNA system ( `Notes on SDNA http://www.blender.org/development/architecture/notes-on-sdna/`_ ). SDNA allows Blender to be fully backward and forward-compatible and to exchange files across all platforms. The BLEND format is thus a non-trivial binary monster and the loader tries to read the most of it, naturally limited by the scope of the #aiScene output data structure. Consequently, if Blender is the only modeling tool in your asset workflow, consider writing a custom exporter from Blender if Assimp format coverage does not meet the requirements. The Blender loader does not support animations yet, but is apart from that considered relatively stable. When filing bugs on the Blender loader, always give the Blender version (or, even better, post the file that caused the error). This section contains implementation notes on the IFC-STEP importer. The library provides a partial implementation of the IFC2x3 industry standard for automatized exchange of CAE/architectural data sets. See IFC for more information on the format. We aim at getting as much 3D data out of the files as possible. IFC support is new and considered experimental. Please report any bugs you may encounter.\n• None Only the STEP-based encoding is supported. IFCZIP and IFCXML are not (but IFCZIP can simply be unzipped to get a STEP file).\n• None The importer leaves vertex coordinates untouched, but applies a global scaling to the root transform to convert from whichever unit the IFC file uses to <i>metres</i>.\n• None If multiple geometric representations are provided, the choice of which one to load is based on how expensive a representation seems to be in terms of import time. The loader also avoids representation types for which it has known deficits.\n• None Not supported are arbitrary binary operations (binary clipping is implemented, though).\n• None Of the various relationship types that IFC knows, only aggregation, containment and material assignment are resolved and mapped to the output graph.\n• None The implementation knows only about IFC2X3 and applies this rule set to all models it encounters, regardless of their actual version. Loading of older or newer files may fail with parsing errors. IFC file properties (IfcPropertySet) are kept as per-node metadata, see aiNode::mMetaData. ATTENTION: The Ogre-Loader is currently under development, many things have changed after this documentation was written, but they are not final enough to rewrite the documentation. So things may have changed by now! This section contains implementations notes for the OgreXML importer. Ogre importer is currently optimized for the Blender Ogre exporter because that’s the only one that I use. You can find the Blender Ogre exporter at: OGRE3D forum What will be loaded? Mesh: Faces, Positions, Normals, and all TexCoords. The Materialname will be used to load the material. Material: The right material in the file will be searched, the importer should work with materials who have 1 technique and 1 pass in this technique. From there, the texture name (for 1 color- and 1 normal map) and the materialcolors (but not in custom materials) will be loaded. Also, the materialname will be set. Skeleton: Skeleton with Bone hierarchy (Position and Rotation, but no Scaling in the skeleton is supported), names and transformations, animations with rotation, translation and scaling keys. How to export Files from Blender You can find information about how to use the Ogreexporter on your own, so here are just some options that you need, so the assimp importer will load everything correctly:\n• None Use either “Rendering Material” or “Custom Material” see @ref material\n• None do not use “Flip Up Axies to Y” There is a binary and an XML mesh Format from Ogre. This loader can only Handle XML files, but don’t panic, there is a command line converter, which you can use to create XML files from Binary Files. Just look on the Ogre page for it. Currently, you can only load meshes. So you will need to import the .mesh.xml file, the loader will try to find the appendant material and skeleton file. The skeleton file must have the same name as the mesh file, e.g. fish.mesh.xml and fish.skeleton.xml. @subsection material Materials The material file can have the same name as the mesh file (if the file is model.mesh or model.mesh.xml the loader will try to load model.material), or you can use to specify the name of the material file. This is especially useful if multiple materials a stored in a single file. The importer will first try to load the material with the same name as the mesh and only if this can’t be opened try to load the alternate material file. The default material filename is “Scene.material”. We suggest that you use custom materials, because they support multiple textures (like colormap and normalmap). First of all you should read the custom material sektion in the Ogre Blender exporter Help File, and than use the assimp.tlp template, which you can find in scripts/OgreImpoter/Assimp.tlp in the assimp source. If you don’t set all values, don’t worry, they will be ignored during import. If you want more properties in custom materials, you can easily expand the ogre material loader, it will be just a few lines for each property. Just look in OgreImporterMaterial.cpp\n• None IMPORT_OGRE_TEXTURETYPE_FROM_FILENAME: Normally, a texture is loaded as a colormap, if no target is specified in the materialfile. Is this switch is enabled, texture names ending with _n, _l, _s are used as normalmaps, lightmaps or specularmaps.\n• None IMPORT_OGRE_MATERIAL_FILE: Ogre Meshes contain only the MaterialName, not the MaterialFile. If there is no material file with the same name as the material, Ogre Importer will try to load this file and search the material in it. You can use properties to chance the behavior of you importer. In order to do so, you have to override BaseImporter::SetupProperties, and specify you custom properties in config.h. Just have a look to the other AI_CONFIG_IMPORT_* defines and you will understand, how it works. The properties can be set with Importer::SetProperty***() and can be accessed in your SetupProperties function with Importer::GetProperty***(). You can store the properties as a member variable of your importer, they are thread safe.\n• None Try to make your parser as flexible as possible. Don’t rely on particular layout, whitespace/tab style, except if the file format has a strict definition, in which case you should always warn about spec violations. But the general rule of thumb is be strict in what you write and tolerant in what you accept.\n• None Call Assimp::BaseImporter::ConvertToUTF8() before you parse anything to convert foreign encodings to UTF-8. That’s not necessary for XML importers, which must use the provided XML-Parser for reading.\n• None Take care of endianness issues! Assimp importers mostly support big-endian platforms, which define the <tt>AI_BUILD_BIG_ENDIAN</tt> constant. See the next section for a list of utilities to simplify this task. Mixed stuff for internal use by loaders, mostly documented (most of them are already included by AssimpPCH.h):\n• None XmlParser (XmlParser.hh) - The XML-Parser used in Asset-importer-Lib\n• None fast_atof, strtoul10, strtoul16, SkipSpaceAndLineEnd, SkipToNextToken .. large family of low-level parsing functions, mostly declared in fast_atof.h, StringComparison.h and ParsingUtils.h (a collection that grew historically, so don’t expect perfect organization).\n• None StandardShapes (StandardShapes.h) - generate meshes for standard solids, such as platonic primitives, cylinders or spheres.\n• None BatchLoader (BaseImporter.h) - manage imports from external files. Useful for file formats which spread their data across multiple files.\n• None SceneCombiner (SceneCombiner.h) - exhaustive toolset to merge multiple scenes. Useful for file formats which spread their data across multiple files. The required definitions zo set/remove/query keys in #ai_material structures are declared in MaterialSystem.h, in a #aiMaterial derivate called #aiMaterial. The header is included by AssimpPCH.h, so you don’t need to bother."
    },
    {
        "link": "https://stackoverflow.com/questions/28595852/how-do-i-get-models-to-animate-using-assimp",
        "document": "This is an old question but I'm sure it could be of future use to others so I'll try to outline some options you have for doing animation with the Assimp library.\n\nFirst off, I'd just like to mention that you can do animation without the Assimp library. The library just provides you with a nice way to load your models, but as you've discovered it won't do the animation for you.\n\nConceptually animation is going to be fairly similar regardless of whether or not you use Assimp. For example; if you have written your own model loader you could easily use that instead of Assimp and still be able to do animation. However, as there is more than one way to do animation, you may be more limited in the way you can achieve it as doing skeletal animation without Assimp would involve writing a model loader that can get the bone transforms, weights and various data out of the model files, and that could take a while.\n\nThere are multiple ways to do the animation; both in terms of technique and whether you want to do it with hardware acceleration (on the GPU or on the CPU). I'm going to mention a few of the options you have here since most people use Assimp to do skeletal animation which can be pretty intimidating if you aren't strong in your math skills and just want something that is easy to put together.\n\nGenerally there are three accepted ways to do animation:\n\nKeyframe animation is when you create a separate model for each frame of the animation, similar to a 2D sprite sheet. You render the models in succession to produce the animation. This method is probably the simplest, but most naive implementation, since you need to load multiple models for each animation. The transitions between these frames may be noticeable depending on how many frames you produce and you may need to export several model files before it looks acceptable. Another downside to this method is that you would likely need to produce your own models.\n\nThis method is similar to the above however rather than producing each key frame as a separate model, just a few key frames are produced and the \"missing\" models are produced with the engine using interpolation. We can do this because if we know the starting point of a vertex and the ending point of a vertex we can interpolate to find out where the vertex should be at time = t.\n\nThis tutorial does a great job of explaining how to do key frame animation:\n\nAgain, it doesn't talk about Assimp but the concepts are the same, and you can still use Assimp to load your models. This form of animation is fairly simple to implement and is quite good for a beginner. However it does come with some drawbacks. If you choose to go this route you may be limited by memory as this method can consume a lot of memory with VBO's, this will depend on how detailed your models are. If you choose to create your own models you will also want to preserve the vertex order in the model files so that interpolating between vertex 2 of one model file (key frame 1) to vertex 2 of another model file (key frame 2) will be correct.\n\nThis is probably the most difficult way to do animation but it deals with a lot of the issues in method 1 and 2. You'll also find that by doing skeletal animation you are able to load a lot of the newer file formats; those that specify bone transformation and rotations rather than having to load new files for each key frame.\n\nThis is one case where I think having Assimp will be of great benefit. Assimp is very well equipped to deal with getting the data you need out of the model file to do skeletal animation.\n\nIf you are interested in doing skeletal animation this tutorial is a fantastic way to go about it, and even uses Assimp as well.\n\nI used this tutorial myself to achieve skeletal animation in my own engine. So I strongly encourage you to read this if you decide to go down that path.\n\nThe last thing I will mention that I've noticed confuses some people is that animation can be done using hardware acceleration, but it's not strictly necessary.\n\nIn the previous tutorial links I provided, both of these are doing animation using hardware acceleration. In this context, it means that the vertex computations are being done on the GPU in the body of the vertex shader.\n\nHowever I know that a lot people may not be familiar with modern OpenGL in which case you can still do these same calculations on the CPU. The idea here being be to look at what is happening in the vertex shader and create a function that performs those calculations for you.\n\nYou also asked about file formats for animation; this is going to depend what route you take to do the animation. If you want to do animation for formats like .fbx and .md5 you will likely be doing skeletal animation. If you go for key frame animation I would probably stick with .obj, this is the format I find the easiest to work with as the format specification is quite easy to understand.\n\nWhile you're debugging the animation in your engine make sure you have a file which you know works; another pitfall is that downloading free models on the internet can contain any old format, absolute texture paths and different coordinate systems (Y is up or Z is up) etc."
    },
    {
        "link": "https://github.com/assimp/assimp",
        "document": "Open Asset Import Library is a library that loads various 3D file formats into a shared, in-memory format. It supports more than 40 file formats for import and a growing selection of file formats for export.\n\nAPIs are provided for C and C++. Various bindings exist to other languages (C#, Java, Python, Delphi, D). Assimp also runs on Android and iOS. Additionally, assimp features various mesh post-processing tools: normals and tangent space generation, triangulation, vertex cache locality optimization, removal of degenerate primitives and duplicate vertices, sorting by primitive type, merging of redundant materials and many more.\n• Ask questions at the Assimp Discussion Board.\n• Ask the Assimp community on Reddit.\n• Ask on StackOverflow with the assimp-tag.\n• Nothing has worked? File a question or an issue report at The Assimp-Issue Tracker\n\nSee the complete list of supported formats.\n\nStart by reading our build instructions. We are available in vcpkg, and our build system is CMake; if you used CMake before there is a good chance you know what to do.\n\nQt5-ModelViewer is a powerful viewer based on Qt5 and Assimp's import and export abilities.\n\n Assimp-Viewer is an experimental implementation for an Asset-Viewer based on ImGUI and Assimp (experimental).\n\nOpen Asset Import Library is implemented in C++. The directory structure looks like this:\n\nThe source code is organized in the following way:\n\nI would greatly appreciate contributing to assimp. The easiest way to get involved is to submit a pull request with your changes against the main repository's branch.\n\nThis project exists thanks to all the people who contribute. [Contribute].\n\nBecome a financial contributor and help us sustain our community. [Contribute]\n\nYou can support the project with your organization. Your logo will show up here with a link to your website. [Contribute]\n\nOur license is based on the modified, 3-clause BSD-License.\n\nAn informal summary is: do whatever you want, but include Assimp's license text with your product - and don't sue us if our code doesn't work. Note that, unlike LGPLed code, you may link statically to Assimp. For the legal details, see the file."
    },
    {
        "link": "https://gamedev.stackexchange.com/questions/26382/i-cant-figure-out-how-to-animate-my-loaded-model-with-assimp",
        "document": "Stack Exchange network consists of 183 Q&A communities including Stack Overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers."
    }
]