[
    {
        "link": "https://ffmpeg.org/ffmpeg.html",
        "document": "is a universal media converter. It can read a wide variety of inputs - including live grabbing/recording devices - filter, and transcode them into a plethora of output formats.\n\nreads from an arbitrary number of inputs (which can be regular files, pipes, network streams, grabbing devices, etc.), specified by the option, and writes to an arbitrary number of outputs, which are specified by a plain output url. Anything found on the command line which cannot be interpreted as an option is considered to be an output url.\n\nEach input or output can, in principle, contain any number of elementary streams of different types (video/audio/subtitle/attachment/data), though the allowed stream counts and/or types may be limited by the container format. Selecting which streams from which inputs will go into which output is either done automatically or with the option (see the Stream selection chapter).\n\nTo refer to inputs/outputs in options, you must use their indices (0-based). E.g. the first input is , the second is , etc. Similarly, streams within an input/output are referred to by their indices. E.g. refers to the fourth stream in the third input or output. Also see the Stream specifiers chapter.\n\nAs a general rule, options are applied to the next specified file. Therefore, order is important, and you can have the same option on the command line multiple times. Each occurrence is then applied to the next input or output file. Exceptions from this rule are the global options (e.g. verbosity level), which should be specified first.\n\nDo not mix input and output files – first specify all input files, then all output files. Also do not mix options which belong to different files. All options apply ONLY to the next input or output file and are reset between files.\n• Convert an input media file to a different format, by re-encoding media streams:\n• Set the video bitrate of the output file to 64 kbit/s:\n• Force the frame rate of the output file to 24 fps:\n• Force the frame rate of the input file (valid for raw formats only) to 1 fps and the frame rate of the output file to 24 fps:\n\nThe format option may be needed for raw input files.\n\nbuilds a transcoding pipeline out of the components listed below. The program’s operation then consists of input data chunks flowing from the sources down the pipes towards the sinks, while being transformed by the components they encounter along the way.\n\nThe following kinds of components are available:\n• Demuxers (short for \"demultiplexers\") read an input source in order to extract\n• global properties such as metadata or chapters;\n• list of input elementary streams and their properties One demuxer instance is created for each option, and sends encoded packets to decoders or muxers. In other literature, demuxers are sometimes called splitters, because their main function is splitting a file into elementary streams (though some files only contain one elementary stream). A schematic representation of a demuxer looks like this: ┌──────────┬───────────────────────┐ │ demuxer │ │ packets for stream 0 ╞══════════╡ elementary stream 0 ├──────────────────────► │ │ │ │ global ├───────────────────────┤ │properties│ │ packets for stream 1 │ and │ elementary stream 1 ├──────────────────────► │ metadata │ │ │ ├───────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├───────────────────────┤ │ │ │ packets for stream N │ │ elementary stream N ├──────────────────────► │ │ │ └──────────┴───────────────────────┘ ▲ │ │ read from file, network stream, │ grabbing device, etc. │\n• Decoders receive encoded (compressed) packets for an audio, video, or subtitle elementary stream, and decode them into raw frames (arrays of pixels for video, PCM for audio). A decoder is typically associated with (and receives its input from) an elementary stream in a demuxer, but sometimes may also exist on its own (see Loopback decoders). A schematic representation of a decoder looks like this:\n• Filtergraphs process and transform raw audio or video frames. A filtergraph consists of one or more individual filters linked into a graph. Filtergraphs come in two flavors - simple and complex, configured with the and options, respectively. A simple filtergraph is associated with an output elementary stream; it receives the input to be filtered from a decoder and sends filtered output to that output stream’s encoder. A simple video filtergraph that performs deinterlacing (using the deinterlacer) followed by resizing (using the filter) can look like this: ┌────────────────────────┐ │ simple filtergraph │ frames from ╞════════════════════════╡ frames for a decoder │ ┌───────┐ ┌───────┐ │ an encoder ────────────►├─►│ yadif ├─►│ scale ├─►│────────────► │ └───────┘ └───────┘ │ └────────────────────────┘ A complex filtergraph is standalone and not associated with any specific stream. It may have multiple (or zero) inputs, potentially of different types (audio or video), each of which receiving data either from a decoder or another complex filtergraph’s output. It also has one or more outputs that feed either an encoder or another complex filtergraph’s input. The following example diagram represents a complex filtergraph with 3 inputs and 2 outputs (all video): Frames from second input are overlaid over those from the first. Frames from the third input are rescaled, then the duplicated into two identical streams. One of them is overlaid over the combined first two inputs, with the result exposed as the filtergraph’s first output. The other duplicate ends up being the filtergraph’s second output.\n• Encoders receive raw audio, video, or subtitle frames and encode them into encoded packets. The encoding (compression) process is typically lossy - it degrades stream quality to make the output smaller; some encoders are lossless, but at the cost of much higher output size. A video or audio encoder receives its input from some filtergraph’s output, subtitle encoders receive input from a decoder (since subtitle filtering is not supported yet). Every encoder is associated with some muxer’s output elementary stream and sends its output to that muxer. A schematic representation of an encoder looks like this:\n• Muxers (short for \"multiplexers\") receive encoded packets for their elementary streams from encoders (the transcoding path) or directly from demuxers (the streamcopy path), interleave them (when there is more than one elementary stream), and write the resulting bytes into the output file (or pipe, network stream, etc.). A schematic representation of a muxer looks like this: ┌──────────────────────┬───────────┐ packets for stream 0 │ │ muxer │ ──────────────────────►│ elementary stream 0 ╞═══════════╡ │ │ │ ├──────────────────────┤ global │ packets for stream 1 │ │properties │ ──────────────────────►│ elementary stream 1 │ and │ │ │ metadata │ ├──────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├──────────────────────┤ │ packets for stream N │ │ │ ──────────────────────►│ elementary stream N │ │ │ │ │ └──────────────────────┴─────┬─────┘ │ write to file, network stream, │ grabbing device, etc. │ │ ▼\n\nThe simplest pipeline in is single-stream streamcopy, that is copying one input elementary stream’s packets without decoding, filtering, or encoding them. As an example, consider an input file called with 3 elementary streams, from which we take the second and write it to file . A schematic representation of such a pipeline looks like this:\n\nThe above pipeline can be constructed with the following commandline:\n• there are no input options for this input;\n• there are two output options for this output:\n• selects the input stream to be used - from input with index 0 (i.e. the first one) the stream with index 1 (i.e. the second one);\n• selects the encoder, i.e. streamcopy with no decoding or encoding.\n\nStreamcopy is useful for changing the elementary stream count, container format, or modifying container-level metadata. Since there is no decoding or encoding, it is very fast and there is no quality loss. However, it might not work in some cases because of a variety of factors (e.g. certain information required by the target container is not available in the source). Applying filters is obviously also impossible, since filters work on decoded frames.\n\nMore complex streamcopy scenarios can be constructed - e.g. combining streams from two input files into a single output:\n\nthat can be built by the commandline\n\nThe output option is used twice here, creating two streams in the output file - one fed by the first input and one by the second. The single instance of the option selects streamcopy for both of those streams. You could also use multiple instances of this option together with Stream specifiers to apply different values to each stream, as will be demonstrated in following sections.\n\nA converse scenario is splitting multiple streams from a single input into multiple outputs:\n\nNote how a separate instance of the option is needed for every output file even though their values are the same. This is because non-global options (which is most of them) only apply in the context of the file before which they are placed.\n\nThese examples can of course be further generalized into arbitrary remappings of any number of inputs into any number of outputs.\n\nTranscoding is the process of decoding a stream and then encoding it again. Since encoding tends to be computationally expensive and in most cases degrades the stream quality (i.e. it is lossy), you should only transcode when you need to and perform streamcopy otherwise. Typical reasons to transcode are:\n• you want to feed the stream to something that cannot decode the original codec.\n\nNote that will transcode all audio, video, and subtitle streams unless you specify for them.\n\nConsider an example pipeline that reads an input file with one audio and one video stream, transcodes the video and copies the audio into a single output file. This can be schematically represented as follows\n\nand implemented with the following commandline:\n\nNote how it uses stream specifiers and to select input streams and apply different values of the option to them; see the Stream specifiers section for more details.\n\nWhen transcoding, audio and video streams can be filtered before encoding, with either a simple or complex filtergraph.\n\nSimple filtergraphs are those that have exactly one input and output, both of the same type (audio or video). They are configured with the per-stream option (with and aliases for (video) and (audio) respectively). Note that simple filtergraphs are tied to their output stream, so e.g. if you have multiple audio streams, will create a separate filtergraph for each one.\n\nTaking the trancoding example from above, adding filtering (and omitting audio, for clarity) makes it look like this:\n\nComplex filtergraphs are those which cannot be described as simply a linear processing chain applied to one stream. This is the case, for example, when the graph has more than one input and/or output, or when output stream type is different from input. Complex filtergraphs are configured with the option. Note that this option is global, since a complex filtergraph, by its nature, cannot be unambiguously associated with a single stream or file. Each instance of creates a new complex filtergraph, and there can be any number of them.\n\nA trivial example of a complex filtergraph is the filter, which has two video inputs and one video output, containing one video overlaid on top of the other. Its audio counterpart is the filter.\n\nWhile decoders are normally associated with demuxer streams, it is also possible to create \"loopback\" decoders that decode the output from some encoder and allow it to be fed back to complex filtergraphs. This is done with the directive, which takes as a parameter the index of the output stream that should be decoded. Every such directive creates a new loopback decoder, indexed with successive integers starting at zero. These indices should then be used to refer to loopback decoders in complex filtergraph link labels, as described in the documentation for .\n\nDecoding AVOptions can be passed to loopback decoders by placing them before , analogously to input/output options.\n\nE.g. the following example:\n• (line 2) encodes it with at low quality;\n• (line 4) places decoded video side by side with the original input video;\n• (line 5) combined video is then losslessly encoded and written into .\n\nSuch a transcoding pipeline can be represented with the following diagram:\n\nprovides the option for manual control of stream selection in each output file. Users can skip and let ffmpeg perform automatic stream selection as described below. The options can be used to skip inclusion of video, audio, subtitle and data streams respectively, whether manually mapped or automatically selected, except for those streams which are outputs of complex filtergraphs.\n\nThe sub-sections that follow describe the various rules that are involved in stream selection. The examples that follow next show how these rules are applied in practice.\n\nWhile every effort is made to accurately reflect the behavior of the program, FFmpeg is under continuous development and the code may have changed since the time of this writing.\n\nIn the absence of any map options for a particular output file, ffmpeg inspects the output format to check which type of streams can be included in it, viz. video, audio and/or subtitles. For each acceptable stream type, ffmpeg will pick one stream, when available, from among all the inputs.\n\nIt will select that stream based upon the following criteria:\n• for video, it is the stream with the highest resolution,\n• for audio, it is the stream with the most channels,\n• for subtitles, it is the first subtitle stream found but there’s a caveat. The output format’s default subtitle encoder can be either text-based or image-based, and only a subtitle stream of the same type will be chosen.\n\nIn the case where several streams of the same type rate equally, the stream with the lowest index is chosen.\n\nData or attachment streams are not automatically selected and can only be included using .\n\nWhen is used, only user-mapped streams are included in that output file, with one possible exception for filtergraph outputs described below.\n\nIf there are any complex filtergraph output streams with unlabeled pads, they will be added to the first output file. This will lead to a fatal error if the stream type is not supported by the output format. In the absence of the map option, the inclusion of these streams leads to the automatic stream selection of their types being skipped. If map options are present, these filtergraph streams are included in addition to the mapped streams.\n\nComplex filtergraph output streams with labeled pads must be mapped once and exactly once.\n\nStream handling is independent of stream selection, with an exception for subtitles described below. Stream handling is set via the option addressed to streams within a specific output file. In particular, codec options are applied by ffmpeg after the stream selection process and thus do not influence the latter. If no option is specified for a stream type, ffmpeg will select the default encoder registered by the output file muxer.\n\nAn exception exists for subtitles. If a subtitle encoder is specified for an output file, the first subtitle stream found of any type, text or image, will be included. ffmpeg does not validate if the specified encoder can convert the selected stream or if the converted stream is acceptable within the output format. This applies generally as well: when the user sets an encoder manually, the stream selection process cannot check if the encoded stream can be muxed into the output file. If it cannot, ffmpeg will abort and all output files will fail to be processed.\n\nThe following examples illustrate the behavior, quirks and limitations of ffmpeg’s stream selection methods.\n\nThey assume the following three input files.\n\nThere are three output files specified, and for the first two, no options are set, so ffmpeg will select streams for these two files automatically.\n\nis a Matroska container file and accepts video, audio and subtitle streams, so ffmpeg will try to select one of each type.\n\n For video, it will select from , which has the highest resolution among all the input video streams.\n\n For audio, it will select from , since it has the greatest number of channels.\n\n For subtitles, it will select from , which is the first subtitle stream from among and .\n\naccepts only audio streams, so only from is selected.\n\nFor , since a option is set, no automatic stream selection will occur. The option will select all audio streams from the second input . No other streams will be included in this output file.\n\nFor the first two outputs, all included streams will be transcoded. The encoders chosen will be the default ones registered by each output format, which may not match the codec of the selected input streams.\n\nFor the third output, codec option for audio streams has been set to , so no decoding-filtering-encoding operations will occur, or can occur. Packets of selected streams shall be conveyed from the input file and muxed within the output file.\n\nAlthough is a Matroska container file which accepts subtitle streams, only a video and audio stream shall be selected. The subtitle stream of is image-based and the default subtitle encoder of the Matroska muxer is text-based, so a transcode operation for the subtitles is expected to fail and hence the stream isn’t selected. However, in , a subtitle encoder is specified in the command and so, the subtitle stream is selected, in addition to the video stream. The presence of disables audio stream selection for .\n\nA filtergraph is setup here using the option and consists of a single video filter. The filter requires exactly two video inputs, but none are specified, so the first two available video streams are used, those of and . The output pad of the filter has no label and so is sent to the first output file . Due to this, automatic selection of the video stream is skipped, which would have selected the stream in . The audio stream with most channels viz. in , is chosen automatically. No subtitle stream is chosen however, since the MP4 format has no default subtitle encoder registered, and the user hasn’t specified a subtitle encoder.\n\nThe 2nd output file, , only accepts text-based subtitle streams. So, even though the first subtitle stream available belongs to , it is image-based and hence skipped. The selected stream, in , is the first text-based subtitle stream.\n\nThe above command will fail, as the output pad labelled has been mapped twice. None of the output files shall be processed.\n\nThis command above will also fail as the hue filter output has a label, , and hasn’t been mapped anywhere.\n\nThe command should be modified as follows,\n\nThe video stream from is sent to the hue filter, whose output is cloned once using the split filter, and both outputs labelled. Then a copy each is mapped to the first and third output files.\n\nThe overlay filter, requiring two video inputs, uses the first two unused video streams. Those are the streams from and . The overlay output isn’t labelled, so it is sent to the first output file , regardless of the presence of the option.\n\nThe aresample filter is sent the first unused audio stream, that of . Since this filter output is also unlabelled, it too is mapped to the first output file. The presence of only suppresses automatic or manual stream selection of audio streams, not outputs sent from filtergraphs. Both these mapped streams shall be ordered before the mapped stream in .\n\nThe video, audio and subtitle streams mapped to are entirely determined by automatic stream selection.\n\nconsists of the cloned video output from the hue filter and the first audio stream from . \n\n\n\nAll the numerical options, if not specified otherwise, accept a string representing a number as input, which may be followed by one of the SI unit prefixes, for example: ’K’, ’M’, or ’G’.\n\nIf ’i’ is appended to the SI unit prefix, the complete prefix will be interpreted as a unit prefix for binary multiples, which are based on powers of 1024 instead of powers of 1000. Appending ’B’ to the SI unit prefix multiplies the value by 8. This allows using, for example: ’KB’, ’MiB’, ’G’ and ’B’ as number suffixes.\n\nOptions which do not take arguments are boolean options, and set the corresponding value to true. They can be set to false by prefixing the option name with \"no\". For example using \"-nofoo\" will set the boolean option with name \"foo\" to false.\n\nOptions that take arguments support a special syntax where the argument given on the command line is interpreted as a path to the file from which the actual argument value is loaded. To use this feature, add a forward slash ’/’ immediately before the option name (after the leading dash). E.g.\n\nwill load a filtergraph description from the file named .\n\nSome options are applied per-stream, e.g. bitrate or codec. Stream specifiers are used to precisely specify which stream(s) a given option belongs to.\n\nA stream specifier is a string generally appended to the option name and separated from it by a colon. E.g. contains the stream specifier, which matches the second audio stream. Therefore, it would select the ac3 codec for the second audio stream.\n\nA stream specifier can match several streams, so that the option is applied to all of them. E.g. the stream specifier in matches all audio streams.\n\nAn empty stream specifier matches all streams. For example, or would copy all the streams without reencoding.\n\nPossible forms of stream specifiers are:\n\nThese options are shared amongst the ff* tools.\n\nThese options are provided directly by the libavformat, libavdevice and libavcodec libraries. To see the list of available AVOptions, use the option. They are separated into two categories:\n\nFor example to write an ID3v2.3 header instead of a default ID3v2.4 to an MP3 file, use the private option of the MP3 muxer:\n\nAll codec AVOptions are per-stream, and thus a stream specifier should be attached to them:\n\nIn the above example, a multichannel audio stream is mapped twice for output. The first instance is encoded with codec ac3 and bitrate 640k. The second instance is downmixed to 2 channels and encoded with codec aac. A bitrate of 128k is specified for it using absolute index of the output stream.\n\nNote: the syntax cannot be used for boolean AVOptions, use / .\n\nNote: the old undocumented way of specifying per-stream AVOptions by prepending v/a/s to the options name is now obsolete and will be removed soon.\n\nForce input or output file format. The format is normally auto detected for input files and guessed from the file extension for output files, so this option is not needed in most cases. Do not overwrite output files, and exit immediately if a specified output file already exists. Set number of times input stream shall be looped. Loop 0 means no loop, loop -1 means infinite loop. Allow forcing a decoder of a different media type than the one detected or designated by the demuxer. Useful for decoding media data muxed as data streams. Select an encoder (when used before an output file) or a decoder (when used before an input file) for one or more streams. is the name of a decoder/encoder or a special value (output only) to indicate that the stream is not to be re-encoded. encodes all video streams with libx264 and copies all audio streams. For each stream, the last matching option is applied, so will copy all the streams except the second video, which will be encoded with libx264, and the 138th audio, which will be encoded with libvorbis. When used as an input option (before ), limit the of data read from the input file. When used as an output option (before an output url), stop writing the output after its duration reaches . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. -to and -t are mutually exclusive and -t has priority. Stop writing the output or reading the input at . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. -to and -t are mutually exclusive and -t has priority. Set the file size limit, expressed in bytes. No further chunk of bytes is written after the limit is exceeded. The size of the output file is slightly more than the requested file size. When used as an input option (before ), seeks in this input file to . Note that in most formats it is not possible to seek exactly, so will seek to the closest seek point before . When transcoding and is enabled (the default), this extra segment between the seek point and will be decoded and discarded. When doing stream copy or when is used, it will be preserved. When used as an output option (before an output url), decodes but discards input until the timestamps reach . must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. Like the option but relative to the \"end of file\". That is negative values are earlier in the file, 0 is at EOF. This will take the difference between the start times of the target and reference inputs and offset the timestamps of the target file by that difference. The source timestamps of the two inputs should derive from the same clock source for expected results. If is set then must also be set. If either of the inputs has no starting timestamp then no sync adjustment is made. Acceptable values are those that refer to a valid ffmpeg input index. If the sync reference is the target index itself or , then no adjustment is made to target timestamps. A sync reference may not itself be synced to any other input. must be a time duration specification, see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual. The offset is added to the timestamps of the input files. Specifying a positive offset means that the corresponding streams are delayed by the time duration specified in . Set the recording timestamp in the container. must be a date specification, see (ffmpeg-utils)the Date section in the ffmpeg-utils(1) manual. An optional may be given to set metadata on streams, chapters or programs. See documentation for details. This option overrides metadata set with . It is also possible to delete metadata by using an empty value. For example, for setting the title in the output file: To set the language of the first audio stream: Default value: by default, all disposition flags are copied from the input stream, unless the output stream this option applies to is fed by a complex filtergraph - in that case no disposition flags are set by default. is a sequence of disposition flags separated by ’+’ or ’-’. A ’+’ prefix adds the given disposition, ’-’ removes it. If the first flag is also prefixed with ’+’ or ’-’, the resulting disposition is the default value updated by . If the first flag is not prefixed, the resulting disposition is . It is also possible to clear the disposition by setting it to 0. If no options were specified for an output file, ffmpeg will automatically set the ’default’ disposition flag on the first stream of each type, when there are multiple streams of this type in the output file and no stream of that type is already marked as default. The option lists the known disposition flags. For example, to make the second audio stream the default stream: To make the second subtitle stream the default stream and remove the default disposition from the first subtitle stream: To add the ’original’ and remove the ’comment’ disposition flag from the first audio stream without removing its other disposition flags: To remove the ’original’ and add the ’comment’ disposition flag to the first audio stream without removing its other disposition flags: To set only the ’original’ and ’comment’ disposition flags on the first audio stream (and remove its other disposition flags): To remove all disposition flags from the first audio stream: Not all muxers support embedded thumbnails, and those who do, only support a few formats, like JPEG or PNG. Creates a program with the specified , and adds the specified (s) to it. Creates a stream group of the specified and , or by ping an input group, adding the specified (s) and/or previously defined (s) to it. can be one of the following: Groups s that belong to the same IAMF Audio Element For this group , the following options are available The Audio Element type. The following values are supported: Demixing information used to reconstruct a scalable channel audio representation. This option must be separated from the rest with a ’,’, and takes the following key=value options An identifier parameters blocks in frames may refer to Recon gain information used to reconstruct a scalable channel audio representation. This option must be separated from the rest with a ’,’, and takes the following key=value options An identifier parameters blocks in frames may refer to A layer defining a Channel Layout in the Audio Element. This option must be separated from the rest with a ’,’. Several ’,’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options The following flags are available: Wether to signal if recon_gain is present as metadata in parameter blocks within frames Which channels output_gain applies to. The following flags are available: The ambisonics mode. This has no effect if audio_element_type is set to channel. The following values are supported: Each ambisonics channel is coded as an individual mono stream in the group Groups s that belong to all IAMF Audio Element the same IAMF Mix Presentation references For this group , the following options are available A sub-mix within the Mix Presentation. This option must be separated from the rest with a ’,’. Several ’,’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options An identifier parameters blocks in frames may refer to, for post-processing the mixed audio signal to generate the audio signal for playback The sample rate duration fields in parameters blocks in frames that refer to this are expressed as Default mix gain value to apply when there are no parameter blocks sharing the same for a given frame References an Audio Element used in this Mix Presentation to generate the final output audio signal for playback. This option must be separated from the rest with a ’|’. Several ’|’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options: The for an Audio Element which this sub-mix refers to An identifier parameters blocks in frames may refer to, for applying any processing to the referenced and rendered Audio Element before being summed with other processed Audio Elements The sample rate duration fields in parameters blocks in frames that refer to this are expressed as Default mix gain value to apply when there are no parameter blocks sharing the same for a given frame A key=value string describing the sub-mix element where \"key\" is a string conforming to BCP-47 that specifies the language for the \"value\" string. \"key\" must be the same as the one in the mix’s Indicates whether the input channel-based Audio Element is rendered to stereo loudspeakers or spatialized with a binaural renderer when played back on headphones. This has no effect if the referenced Audio Element’s is set to channel. The following values are supported: Specifies the layouts for this sub-mix on which the loudness information was measured. This option must be separated from the rest with a ’|’. Several ’|’ separated entries can be defined, and at least one must be set. It takes the following \":\"-separated key=value options: The layout follows the loudspeaker sound system convention of ITU-2051-3. Channel layout matching one of Sound Systems A to J of ITU-2051-3, plus 7.1.2 and 3.1.2 This has no effect if is set to binaural. The program integrated loudness information, as defined in ITU-1770-4. The digital (sampled) peak value of the audio signal, as defined in ITU-1770-4. The true peak of the audio signal, as defined in ITU-1770-4. The Dialogue loudness information, as defined in ITU-1770-4. The Album loudness information, as defined in ITU-1770-4. A key=value string string describing the mix where \"key\" is a string conforming to BCP-47 that specifies the language for the \"value\" string. \"key\" must be the same as the ones in all sub-mix element’s s E.g. to create an scalable 5.1 IAMF file from several WAV input files To copy the two stream groups (Audio Element and Mix Presentation) from an input IAMF file with four streams into an mp4 output Specify target file type ( , , , , ). may be prefixed with , or to use the corresponding standard. All the format options (bitrate, codecs, buffer sizes) are then set automatically. You can just type: Nevertheless you can specify additional options as long as you know they do not conflict with the standard, as in: The parameters set for each target are as follows. The target is identical to the target except that the pixel format set is for all three standards. Any user-set value for a parameter above will override the target preset value. In that case, the output may not comply with the target standard. As an input option, blocks all data streams of a file from being filtered or being automatically selected or mapped for any output. See option to disable streams individually. As an output option, disables data recording i.e. automatic selection or mapping of any data stream. For full manual control see the option. Set the number of data frames to output. This is an obsolete alias for , which you should use instead. Stop writing to the stream after frames. Use fixed quality scale (VBR). The meaning of / is codec-dependent. If is used without a then it applies only to the video stream, this is to maintain compatibility with previous behavior and as specifying the same codec specific value to 2 different codecs that is audio and video generally is not what is intended when no stream_specifier is used. Create the filtergraph specified by and use it to filter the stream. is a description of the filtergraph to apply to the stream, and must have a single input and a single output of the same type of the stream. In the filtergraph, the input is associated to the label , and the output to the label . See the ffmpeg-filters manual for more information about the filtergraph syntax. See the -filter_complex option if you want to create filtergraphs with multiple inputs and/or outputs. This boolean option determines if the filtergraph(s) to which this stream is fed gets reinitialized when input frame parameters change mid-stream. This option is enabled by default as most video and all audio filters cannot handle deviation in input frame properties. Upon reinitialization, existing filter state is lost, like e.g. the frame count reference available in some filters. Any frames buffered at time of reinitialization are lost. The properties where a change triggers reinitialization are, for video, frame resolution or pixel format; for audio, sample format, sample rate, channel count or channel layout. Defines how many threads are used to process a filter pipeline. Each pipeline will produce a thread pool with this many threads available for parallel processing. The default is the number of available CPUs. Specify the preset for matching stream(s). Log encoding progress/statistics as \"info\"-level log (see ). It is on by default, to explicitly disable it you need to specify . Set period at which encoding progress/statistics are updated. Default is 0.5 seconds. Progress information is written periodically and at the end of the encoding process. It is made of \" = \" lines. consists of only alphanumeric characters. The last key of a sequence of progress information is always \"progress\" with the value \"continue\" or \"end\". The update period is set using . For example, log progress information to stdout: Enable interaction on standard input. On by default unless standard input is used as an input. To explicitly disable interaction you need to specify . Disabling interaction on standard input is useful, for example, if ffmpeg is in the background process group. Roughly the same result can be achieved with but it requires a shell. Print timestamp/latency information. It is off by default. This option is mostly useful for testing and debugging purposes, and the output format may change from one version to another, so it should not be employed by portable scripts. See also the option . Add an attachment to the output file. This is supported by a few formats like Matroska for e.g. fonts used in rendering subtitles. Attachments are implemented as a specific type of stream, so this option will add a new stream to the file. It is then possible to use per-stream options on this stream in the usual way. Attachment streams created with this option will be created after all the other streams (i.e. those created with or automatic mappings). Note that for Matroska you also have to set the mimetype metadata tag: (assuming that the attachment stream will be third in the output file). Extract the matching attachment stream into a file named . If is empty, then the value of the metadata tag will be used. E.g. to extract the first attachment to a file named ’out.ttf’: To extract all attachments to files determined by the tag: Technical note – attachments are implemented as codec extradata, so this option can actually be used to extract extradata from any stream, not just attachments.\n\nSet pixel format. Use to show all the supported pixel formats. If the selected pixel format can not be selected, ffmpeg will print a warning and select the best pixel format supported by the encoder. If is prefixed by a , ffmpeg will exit with an error if the requested pixel format can not be selected, and automatic conversions inside filtergraphs are disabled. If is a single , ffmpeg selects the same pixel format as the input (or graph output) and automatic conversions are disabled. Set default flags for the libswscale library. These flags are used by automatically inserted filters and those within simple filtergraphs, if not overridden within the filtergraph definition. See the (ffmpeg-scaler)ffmpeg-scaler manual for a list of scaler options. Rate control override for specific intervals, formatted as \"int,int,int\" list separated with slashes. Two first values are the beginning and end frame numbers, last one is quantizer to use if positive, or quality factor if negative. Dump video coding statistics to . See the vstats file format section for the format description. Dump video coding statistics to . See the vstats file format section for the format description. Specify which version of the vstats format to use. Default is . See the vstats file format section for the format description. Force video tag/fourcc. This is an alias for . can take arguments of the following form: If the argument consists of timestamps, ffmpeg will round the specified times to the nearest output timestamp as per the encoder time base and force a keyframe at the first frame having timestamp equal or greater than the computed timestamp. Note that if the encoder time base is too coarse, then the keyframes may be forced on frames with timestamps lower than the specified time. The default encoder time base is the inverse of the output framerate but may be set otherwise via . If one of the times is \" [ ]\", it is expanded into the time of the beginning of all chapters in the file, shifted by , expressed as a time in seconds. This option can be useful to ensure that a seek point is present at a chapter mark or any other designated place in the output file. For example, to insert a key frame at 5 minutes, plus key frames 0.1 second before the beginning of every chapter: If the argument is prefixed with , the string is interpreted like an expression and is evaluated for each frame. A key frame is forced in case the evaluation is non-zero. The expression in can contain the following constants: the number of current processed frame, starting from 0 the number of the previous forced frame, it is when no keyframe was forced yet the time of the previous forced frame, it is when no keyframe was forced yet the time of the current processed frame For example to force a key frame every 5 seconds, you can specify: To force a key frame 5 seconds after the time of the last forced one, starting from second 13: If the argument is , ffmpeg will force a key frame if the current frame being encoded is marked as a key frame in its source. In cases where this particular source frame has to be dropped, enforce the next available frame to become a key frame instead. Note that forcing too many keyframes is very harmful for the lookahead algorithms of certain encoders: using fixed-GOP options or similar would be more efficient. Automatically crop the video after decoding according to file metadata. Default is all. Apply both codec and container level croppping. This is the default mode. When doing stream copy, copy also non-key frames found at the beginning. Initialise a new hardware device of type called , using the given device parameters. If no name is specified it will receive a default name of the form \" %d\". The meaning of and the following arguments depends on the device type: is the number of the CUDA device. The following options are recognized: If set to 1, uses the primary device context instead of creating a new one. Choose the second device on the system. Choose the first device and use the primary device context. is the number of the Direct3D 9 display adapter. is the number of the Direct3D 11 display adapter. If not specified, it will attempt to use the default Direct3D 11 display adapter or the first Direct3D 11 display adapter whose hardware VendorId is specified by ‘ ’. Create a d3d11va device on the Direct3D 11 display adapter specified by index 1. Create a d3d11va device on the first Direct3D 11 display adapter whose hardware VendorId is 0x8086. is either an X11 display name, a DRM render node or a DirectX adapter index. If not specified, it will attempt to open the default X11 display ($DISPLAY) and then the first DRM render node (/dev/dri/renderD128), or the default DirectX adapter on Windows. The following options are recognized: When is not specified, use this option to specify the name of the kernel driver associated with the desired device. This option is available only when the hardware acceleration method drm and vaapi are enabled. When and are not specified, use this option to specify the vendor id associated with the desired device. This option is available only when the hardware acceleration method drm and vaapi are enabled and kernel_driver is not specified. Create a vaapi device on a device associated with kernel driver ‘ ’. Create a vaapi device on a device associated with vendor id ‘ ’. is an X11 display name. If not specified, it will attempt to open the default X11 display ($DISPLAY). selects a value in ‘ ’. Allowed values are: If not specified, ‘ ’ is used. (Note that it may be easier to achieve the desired result for QSV by creating the platform-appropriate subdevice (‘ ’ or ‘ ’ or ‘ ’) and then deriving a QSV device from that.) The following options are recognized: Specify a DRM render node on Linux or DirectX adapter on Windows. Choose platform-appropriate subdevice type. On Windows ‘ ’ is used as default subdevice type when is specified at configuration time, ‘ ’ is used as default subdevice type when is specified at configuration time. On Linux user can use ‘ ’ only as subdevice type. Choose the GPU subdevice with type ‘ ’ and create QSV device with ‘ ’. Choose the GPU subdevice with type ‘ ’ and create QSV device with ‘ ’. Create a QSV device with ‘ ’ on DirectX adapter 1 with subdevice type ‘ ’. Create a VAAPI device called ‘ ’ on , then derive a QSV device called ‘ ’ from device ‘ ’. selects the platform and device as platform_index.device_index. The set of devices can also be filtered using the key-value pairs to find only devices matching particular platform or device strings. The strings usable as filters are: The indices and filters must together uniquely select a device. Choose the second device on the first platform. Choose the device with a name containing the string Foo9000. Choose the GPU device on the second platform supporting the cl_khr_fp16 extension. If is an integer, it selects the device by its index in a system-dependent list of devices. If is any other string, it selects the first device with a name containing that string as a substring. The following options are recognized: If set to 1, enables the validation layer, if installed. If set to 1, images allocated by the hwcontext will be linear and locally mappable. A plus separated list of additional instance extensions to enable. A plus separated list of additional device extensions to enable. Choose the second device on the system. Choose the first device with a name containing the string RADV. Choose the first device and enable the Wayland and XCB instance extensions. Initialise a new hardware device of type called , deriving it from the existing device with the name . List all hardware device types supported in this build of ffmpeg. Pass the hardware device called to all filters in any filter graph. This can be used to set the device to upload to with the filter, or the device to map to with the filter. Other filters may also make use of this parameter when they require a hardware device. Note that this is typically only required when the input is not already in hardware frames - when it is, filters will derive the device they require from the context of the frames they receive as input. This is a global setting, so all filters will receive the same device. Use hardware acceleration to decode the matching stream(s). The allowed values of are: Do not use any hardware acceleration (the default). Use VDPAU (Video Decode and Presentation API for Unix) hardware acceleration. Use the Intel QuickSync Video acceleration for video transcoding. Unlike most other values, this option does not enable accelerated decoding (that is used automatically whenever a qsv decoder is selected), but accelerated transcoding, without copying the frames into the system memory. For it to work, both the decoder and the encoder must support QSV acceleration and no filters must be used. This option has no effect if the selected hwaccel is not available or not supported by the chosen decoder. Note that most acceleration methods are intended for playback and will not be faster than software decoding on modern CPUs. Additionally, will usually need to copy the decoded frames from the GPU memory into the system memory, resulting in further performance loss. This option is thus mainly useful for testing. Select a device to use for hardware acceleration. This option only makes sense when the option is also specified. It can either refer to an existing device created with by name, or it can create a new device as if ‘ ’ : were called immediately before. List all hardware acceleration components enabled in this build of ffmpeg. Actual runtime availability depends on the hardware and its suitable driver being installed. Set a specific output video stream as the heartbeat stream according to which to split and push through currently in-progress subtitle upon receipt of a random access packet. This lowers the latency of subtitles for which the end packet or the following subtitle has not yet been received. As a drawback, this will most likely lead to duplication of subtitle events in order to cover the full duration, so when dealing with use cases where latency of when the subtitle event is passed on to output is not relevant this option should not be utilized. Requires to be set for the relevant input subtitle stream for this to have any effect, as well as for the input subtitle stream having to be directly mapped to the same output in which the heartbeat stream resides.\n\nCreate one or more streams in the output file. This option has two forms for specifying the data source(s): the first selects one or more streams from some input file (specified with ), the second takes an output from some complex filtergraph (specified with ). In the first form, an output stream is created for every stream from the input file with the index . If is given, only those streams that match the specifier are used (see the Stream specifiers section for the syntax). A character before the stream identifier creates a \"negative\" mapping. It disables matching streams from already created mappings. An optional may be given after the stream specifier, which for multiview video specifies the view to be used. The view specifier may have one of the following formats: select a view by its ID; may be set to ’all’ to use all the views interleaved into one stream; select a view by its index; i.e. 0 is the base view, 1 is the first non-base view, etc. select a view by its display position; may be or The default for transcoding is to only use the base view, i.e. the equivalent of . For streamcopy, view specifiers are not supported and all views are always copied. A trailing after the stream index will allow the map to be optional: if the map matches no streams the map will be ignored instead of failing. Note the map will still fail if an invalid input file index is used; such as if the map refers to a non-existent input. An alternative form will map outputs from complex filter graphs (see the option) to the output file. must correspond to a defined output link label in the graph. This option may be specified multiple times, each adding more streams to the output file. Any given input stream may also be mapped any number of times as a source for different output streams, e.g. in order to use different encoding options and/or filters. The streams are created in the output in the same order in which the options are given on the commandline. Using this option disables the default mappings for this output file. To map ALL streams from the first input file to output If you have two audio streams in the first input file, these streams are identified by and . You can use to select which streams to place in an output file. For example: will map the second input stream in to the (single) output stream in . To select the stream with index 2 from input file (specified by the identifier ), and stream with index 6 from input (specified by the identifier ), and copy them to the output file : To select all video and the third audio stream from an input file: To map all the streams except the second audio, use negative mappings To map the video and audio streams from the first input, and using the trailing , ignore the audio mapping if no audio streams exist in the first input: Ignore input streams with unknown type instead of failing if copying such streams is attempted. Allow input streams with unknown type to be copied instead of failing if copying such streams is attempted. Set metadata information of the next output file from . Note that those are file indices (zero-based), not filenames. Optional parameters specify, which metadata to copy. A metadata specifier can have the following forms: global metadata, i.e. metadata that applies to the whole file per-stream metadata. is a stream specifier as described in the Stream specifiers chapter. In an input metadata specifier, the first matching stream is copied from. In an output metadata specifier, all matching streams are copied to. If metadata specifier is omitted, it defaults to global. By default, global metadata is copied from the first input file, per-stream and per-chapter metadata is copied along with streams/chapters. These default mappings are disabled by creating any mapping of the relevant type. A negative file index can be used to create a dummy mapping that just disables automatic copying. For example to copy metadata from the first stream of the input file to global metadata of the output file: To do the reverse, i.e. copy global metadata to all audio streams: Note that simple would work as well in this example, since global metadata is assumed by default. Copy chapters from input file with index to the next output file. If no chapter mapping is specified, then chapters are copied from the first input file with at least one chapter. Use a negative file index to disable any chapter copying. Show benchmarking information at the end of an encode. Shows real, system and user time used and maximum memory consumption. Maximum memory consumption is not supported on all systems, it will usually display as 0 if not supported. Show benchmarking information during the encode. Shows real, system and user time used in various steps (audio/video encode/decode). Exit after ffmpeg has been running for seconds in CPU user time. When dumping packets, also dump the payload. Its value is a floating-point positive number which represents the maximum duration of media, in seconds, that should be ingested in one second of wallclock time. Default value is zero and represents no imposed limitation on speed of ingestion. Value represents real-time speed and is equivalent to . Mainly used to simulate a capture device or live input stream (e.g. when reading from a file). Should not be used with a low value when input is an actual capture device or live stream as it may cause packet loss. It is useful for when flow speed of output packets is important, such as live streaming. Read input at native frame rate. This is equivalent to setting . Set an initial read burst time, in seconds, after which will be enforced. If either the input or output is blocked leading to actual read speed falling behind the specified readrate, then this rate takes effect till the input catches up with the specified readrate. Must not be lower than the primary readrate. Set video sync method / framerate mode. vsync is applied to all output video streams but can be overridden for a stream by setting fps_mode. vsync is deprecated and will be removed in the future. For compatibility reasons some of the values for vsync can be specified as numbers (shown in parentheses in the following table). Each frame is passed with its timestamp from the demuxer to the muxer. Frames will be duplicated and dropped to achieve exactly the requested constant frame rate. Frames are passed through with their timestamp or dropped so as to prevent 2 frames from having the same timestamp. Chooses between cfr and vfr depending on muxer capabilities. This is the default method. Note that the timestamps may be further modified by the muxer, after this. For example, in the case that the format option is enabled. With -map you can select from which stream the timestamps should be taken. You can leave either video or audio unchanged and sync the remaining stream(s) to the unchanged one. Frame drop threshold, which specifies how much behind video frames can be before they are dropped. In frame rate units, so 1.0 is one frame. The default is -1.1. One possible usecase is to avoid framedrops in case of noisy timestamps or to increase frame drop precision in case of exact timestamps. Pad the output audio stream(s). This is the same as applying . Argument is a string of filter parameters composed the same as with the filter. must be set for this output for the option to take effect. Do not process input timestamps, but keep their values without trying to sanitize them. In particular, do not remove the initial start time offset value. Note that, depending on the option or on specific muxer processing (e.g. in case the format option is enabled) the output timestamps may mismatch with the input timestamps even when this option is selected. When used with , shift input timestamps so they start at zero. This means that using e.g. will make output timestamps start at 50 seconds, regardless of what timestamp the input file started at. Specify how to set the encoder timebase when stream copying. is an integer numeric value, and can assume one of the following values: The time base is copied to the output encoder from the corresponding input demuxer. This is sometimes required to avoid non monotonically increasing timestamps when copying video streams with variable frame rate. The time base is copied to the output encoder from the corresponding input decoder. Try to make the choice automatically, in order to generate a sane output. Set the encoder timebase. can assume one of the following values: Assign a default value according to the media type. For video - use 1/framerate, for audio - use 1/samplerate. Use the timebase from the demuxer. Use the timebase from the filtergraph. Use the provided number as the timebase. This field can be provided as a ratio of two integers (e.g. 1:24, 1:48000) or as a decimal number (e.g. 0.04166, 2.0833e-5) Note that this option may require buffering frames, which introduces extra latency. The maximum amount of this latency may be controlled with the option. The option may require buffering potentially large amounts of data when at least one of the streams is \"sparse\" (i.e. has large gaps between frames – this is typically the case for subtitles). This option controls the maximum duration of buffered frames in seconds. Larger values may allow the option to produce more accurate results, but increase memory use and latency. The default value is 10 seconds. The timestamp discontinuity correction enabled by this option is only applied to input formats accepting timestamp discontinuity (for which the flag is enabled), e.g. MPEG-TS and HLS, and is automatically disabled when employing the option (unless wrapping is detected). If a timestamp discontinuity is detected whose absolute value is greater than , ffmpeg will remove the discontinuity by decreasing/increasing the current DTS and PTS by the corresponding delta value. The timestamp correction enabled by this option is only applied to input formats not accepting timestamp discontinuity (for which the flag is not enabled). If a timestamp discontinuity is detected whose absolute value is greater than , ffmpeg will drop the PTS/DTS timestamp value. The default value is (30 hours), which is arbitrarily picked and quite conservative. Assign a new stream-id value to an output stream. This option should be specified prior to the output filename to which it applies. For the situation where multiple output files exist, a streamid may be reassigned to a different value. For example, to set the stream 0 PID to 33 and the stream 1 PID to 36 for an output mpegts file: Apply bitstream filters to matching streams. The filters are applied to each packet as it is received from the demuxer (when used as an input option) or before it is sent to the muxer (when used as an output option). is a comma-separated list of bitstream filter specifications, each of the form Any of the ’,=:’ characters that are to be a part of an option value need to be escaped with a backslash. Use the option to get the list of bitstream filters. applies the bitstream filter (which converts MP4-encapsulated H.264 stream to Annex B) to the input video stream. applies the bitstream filter (which extracts text from MOV subtitles) to the output subtitle stream. Note, however, that since both examples use , it matters little whether the filters are applied on input or output - that would change if transcoding was happening. Specify Timecode for writing. is ’:’ for non drop timecode and ’;’ (or ’.’) for drop. Define a complex filtergraph, i.e. one with arbitrary number of inputs and/or outputs. For simple graphs – those with one input and one output of the same type – see the options. is a description of the filtergraph, as described in the “Filtergraph syntax” section of the ffmpeg-filters manual. This option may be specified multiple times - each use creates a new complex filtergraph. Inputs to a complex filtergraph may come from different source types, distinguished by the format of the corresponding link label:\n• To connect an input stream, use (i.e. the same syntax as ). If matches multiple streams, the first one will be used. For multiview video, the stream specifier may be followed by the view specifier, see documentation for the option for its syntax.\n• To connect a loopback decoder use [dec: ], where is the index of the loopback decoder to be connected to given input. For multiview video, the decoder index may be followed by the view specifier, see documentation for the option for its syntax.\n• To connect an output from another complex filtergraph, use its link label. E.g the following example:\n• (line 2) uses a complex filtergraph with one input and two outputs to scale the video to 1920x1080 and duplicate the result to both outputs;\n• (line 3) encodes one scaled output with and writes the result to ;\n• (line 5) places the output of the loopback decoder (i.e. the -encoded video) side by side with the scaled original input;\n• (line 6) combined video is then losslessly encoded and written into . Note that the two filtergraphs cannot be combined into one, because then there would be a cycle in the transcoding pipeline (filtergraph output goes to encoding, from there to decoding, then back to the same graph), and such cycles are not allowed. An unlabeled input will be connected to the first unused input stream of the matching type. Output link labels are referred to with . Unlabeled outputs are added to the first output file. Note that with this option it is possible to use only lavfi sources without normal input files. For example, to overlay an image over video Here refers to the first video stream in the first input file, which is linked to the first (main) input of the overlay filter. Similarly the first video stream in the second input is linked to the second (overlay) input of overlay. Assuming there is only one video stream in each input file, we can omit input labels, so the above is equivalent to Furthermore we can omit the output label and the single output from the filter graph will be added to the output file automatically, so we can simply write As a special exception, you can use a bitmap subtitle stream as input: it will be converted into a video with the same size as the largest video in the file, or 720x576 if no video is present. Note that this is an experimental and temporary solution. It will be removed once libavfilter has proper support for subtitles. For example, to hardcode subtitles on top of a DVB-T recording stored in MPEG-TS format, delaying the subtitles by 1 second: To generate 5 seconds of pure red video using lavfi source: Defines how many threads are used to process a filter_complex graph. Similar to filter_threads but used for graphs only. The default is the number of available CPUs. Define a complex filtergraph, i.e. one with arbitrary number of inputs and/or outputs. Equivalent to . This option enables or disables accurate seeking in input files with the option. It is enabled by default, so seeking is accurate when transcoding. Use to disable it, which may be useful e.g. when copying some streams and transcoding the others. This option enables or disables seeking by timestamp in input files with the option. It is disabled by default. If enabled, the argument to the option is considered an actual timestamp, and is not offset by the start time of the file. This matters only for files which do not start from timestamp 0, such as transport streams. For input, this option sets the maximum number of queued packets when reading from the file or device. With low latency / high rate live streams, packets may be discarded if they are not read in a timely manner; setting this value can force ffmpeg to use a separate input thread and read packets as soon as they arrive. By default ffmpeg only does this if multiple inputs are specified. For output, this option specified the maximum number of packets that may be queued to each muxing thread. Print sdp information for an output stream to . This allows dumping sdp information when at least one output isn’t an rtp stream. (Requires at least one of the output formats to be rtp). Allows discarding specific streams or frames from streams. Any input stream can be fully discarded, using value whereas selective discarding of frames from a stream occurs at the demuxer and is not supported by all demuxers. Stop and abort on various conditions. The following flags are available: No packets were passed to the muxer, the output is empty. No packets were passed to the muxer in some of the output streams. Set fraction of decoding frame failures across all inputs which when crossed ffmpeg will return exit code 69. Crossing this threshold does not terminate processing. Range is a floating-point number between 0 to 1. Default is 2/3. When transcoding audio and/or video streams, ffmpeg will not begin writing into the output until it has one packet for each such stream. While waiting for that to happen, packets for other streams are buffered. This option sets the size of this buffer, in packets, for the matching output stream. The default value of this option should be high enough for most uses, so only touch this option if you are sure that you need it. This is a minimum threshold until which the muxing queue size is not taken into account. Defaults to 50 megabytes per stream, and is based on the overall size of packets passed to the muxer. Enable automatically inserting format conversion filters in all filter graphs, including those defined by , , and . If filter format negotiation requires a conversion, the initialization of the filters will fail. Conversions can still be performed by inserting the relevant conversion filter (scale, aresample) in the graph. On by default, to explicitly disable it you need to specify . Declare the number of bits per raw sample in the given output stream to be . Note that this option sets the information provided to the encoder/muxer, it does not change the stream to conform to this value. Setting values that do not match the stream properties may result in encoding failures or invalid output files. Write per-frame encoding information about the matching streams into the file given by . writes information about raw video or audio frames right before they are sent for encoding, while writes information about encoded packets as they are received from the encoder. writes information about packets just as they are about to be sent to the muxer. Every frame or packet produces one line in the specified file. The format of this line is controlled by / / . When stats for multiple streams are written into a single file, the lines corresponding to different streams will be interleaved. The precise order of this interleaving is not specified and not guaranteed to remain stable between different invocations of the program, even with the same options. Specify the format for the lines written with / / . is a string that may contain directives of the form . is backslash-escaped — use \\{, \\}, and \\\\ to write a literal {, }, or \\, respectively, into the output. The directives given with may be one of the following: Index of the output stream in the file. Frame number. Pre-encoding: number of frames sent to the encoder so far. Post-encoding: number of packets received from the encoder so far. Muxing: number of packets submitted to the muxer for this stream so far. Input frame number. Index of the input frame (i.e. output by a decoder) that corresponds to this output frame or packet. -1 if unavailable. Timebase in which this frame/packet’s timestamps are expressed, as a rational number . Note that encoder and muxer may use different timebases. Timebase for , as a rational number . Available when is available, otherwise. Presentation timestamp of the frame or packet, as an integer. Should be multiplied by the timebase to compute presentation time. Presentation timestamp of the input frame (see ), as an integer. Should be multiplied by to compute presentation time. Printed as (2^63 - 1 = 9223372036854775807) when not available. Presentation time of the frame or packet, as a decimal number. Equal to multiplied by . Presentation time of the input frame (see ), as a decimal number. Equal to multiplied by . Printed as inf when not available. Decoding timestamp of the packet, as an integer. Should be multiplied by the timebase to compute presentation time. Decoding time of the frame or packet, as a decimal number. Equal to multiplied by . Number of audio samples sent to the encoder so far. Number of audio samples in the frame. Size of the encoded packet in bytes. Current bitrate in bits per second. Average bitrate for the whole stream so far, in bits per second, -1 if it cannot be determined at this point. Character ’K’ if the packet contains a keyframe, character ’N’ otherwise. Directives tagged with packet may only be used with and . Directives tagged with frame may only be used with . Directives tagged with audio may only be used with audio streams. In the future, new items may be added to the end of the default formatting strings. Users who depend on the format staying exactly the same, should prescribe it manually. Note that stats for different streams written into the same file may have different formats.\n\nA preset file contains a sequence of = pairs, one for each line, specifying a sequence of options which would be awkward to specify on the command line. Lines starting with the hash (’#’) character are ignored and are used to provide comments. Check the directory in the FFmpeg source tree for examples.\n\nThere are two types of preset files: ffpreset and avpreset files.\n\nffpreset files are specified with the , , , and options. The option takes the filename of the preset instead of a preset name as input and can be used for any kind of codec. For the , , and options, the options specified in a preset file are applied to the currently selected codec of the same type as the preset option.\n\nThe argument passed to the , , and preset options identifies the preset file to use according to the following rules:\n\nFirst ffmpeg searches for a file named .ffpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ) or in a folder along the executable on win32, in that order. For example, if the argument is , it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named - .ffpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\navpreset files are specified with the option. They work similar to ffpreset files, but they only allow encoder- specific options. Therefore, an = pair specifying an encoder cannot be used.\n\nWhen the option is specified, ffmpeg will look for files with the suffix .avpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ), in that order.\n\nFirst ffmpeg searches for a file named - .avpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named .avpreset in the same directories.\n\nThe and options enable generation of a file containing statistics about the generated video outputs.\n\nThe option controls the format version of the generated file.\n\nWith version the format is:\n\nWith version the format is:\n\nThe value corresponding to each key is described below:\n\nSee also the -stats_enc options for an alternative way to show encoding statistics.\n\nIf you specify the input format and device then ffmpeg can grab video and audio directly.\n\nOr with an ALSA audio source (mono input, card id 1) instead of OSS:\n\nNote that you must activate the right video source and channel before launching ffmpeg with any TV viewer such as xawtv by Gerd Knorr. You also have to set the audio recording levels correctly with a standard mixer.\n\nGrab the X11 display with ffmpeg via\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable.\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable. 10 is the x-offset and 20 the y-offset for the grabbing.\n\nAny supported file format and protocol can serve as input to ffmpeg:\n• You can use YUV files as input: It will use the files: The Y files use twice the resolution of the U and V files. They are raw files, without header. They can be generated by all decent video decoders. You must specify the size of the image with the option if ffmpeg cannot guess it.\n• You can input from a raw YUV420P file: test.yuv is a file containing raw YUV planar data. Each frame is composed of the Y plane followed by the U and V planes at half vertical and horizontal resolution.\n• You can output to a raw YUV420P file:\n• You can set several input files and output files: Converts the audio file a.wav and the raw YUV video file a.yuv to MPEG file a.mpg.\n• You can also do audio and video conversions at the same time:\n• You can encode to several formats at the same time and define a mapping from input stream to output streams: Converts a.wav to a.mp2 at 64 kbits and to b.mp2 at 128 kbits. ’-map file:index’ specifies which input stream is used for each output stream, in the order of the definition of output streams.\n• You can transcode decrypted VOBs: This is a typical DVD ripping example; the input is a VOB file, the output an AVI file with MPEG-4 video and MP3 audio. Note that in this command we use B-frames so the MPEG-4 stream is DivX5 compatible, and GOP size is 300 which means one intra frame every 10 seconds for 29.97fps input video. Furthermore, the audio stream is MP3-encoded so you need to enable LAME support by passing to configure. The mapping is particularly useful for DVD transcoding to get the desired audio language. NOTE: To see the supported input formats, use .\n• You can extract images from a video, or create a video from many images: This will extract one video frame per second from the video and will output them in files named , , etc. Images will be rescaled to fit the new WxH values. If you want to extract just a limited number of frames, you can use the above command in combination with the or option, or in combination with -ss to start extracting from a certain point in time. For creating a video from many images: The syntax specifies to use a decimal number composed of three digits padded with zeroes to express the sequence number. It is the same syntax supported by the C printf function, but only formats accepting a normal integer are suitable. When importing an image sequence, -i also supports expanding shell-like wildcard patterns (globbing) internally, by selecting the image2-specific option. For example, for creating a video from filenames matching the glob pattern :\n• You can put many streams of the same type in the output: The resulting output file will contain the first four streams from the input files in reverse order.\n• The four options lmin, lmax, mblmin and mblmax use ’lambda’ units, but you may use the QP2LAMBDA constant to easily convert from ’q’ units:\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://ffmpeg.org/ffmpeg-all.html",
        "document": "is a universal media converter. It can read a wide variety of inputs - including live grabbing/recording devices - filter, and transcode them into a plethora of output formats.\n\nreads from an arbitrary number of inputs (which can be regular files, pipes, network streams, grabbing devices, etc.), specified by the option, and writes to an arbitrary number of outputs, which are specified by a plain output url. Anything found on the command line which cannot be interpreted as an option is considered to be an output url.\n\nEach input or output can, in principle, contain any number of elementary streams of different types (video/audio/subtitle/attachment/data), though the allowed stream counts and/or types may be limited by the container format. Selecting which streams from which inputs will go into which output is either done automatically or with the option (see the Stream selection chapter).\n\nTo refer to inputs/outputs in options, you must use their indices (0-based). E.g. the first input is , the second is , etc. Similarly, streams within an input/output are referred to by their indices. E.g. refers to the fourth stream in the third input or output. Also see the Stream specifiers chapter.\n\nAs a general rule, options are applied to the next specified file. Therefore, order is important, and you can have the same option on the command line multiple times. Each occurrence is then applied to the next input or output file. Exceptions from this rule are the global options (e.g. verbosity level), which should be specified first.\n\nDo not mix input and output files – first specify all input files, then all output files. Also do not mix options which belong to different files. All options apply ONLY to the next input or output file and are reset between files.\n• Convert an input media file to a different format, by re-encoding media streams:\n• Set the video bitrate of the output file to 64 kbit/s:\n• Force the frame rate of the output file to 24 fps:\n• Force the frame rate of the input file (valid for raw formats only) to 1 fps and the frame rate of the output file to 24 fps:\n\nThe format option may be needed for raw input files.\n\nbuilds a transcoding pipeline out of the components listed below. The program’s operation then consists of input data chunks flowing from the sources down the pipes towards the sinks, while being transformed by the components they encounter along the way.\n\nThe following kinds of components are available:\n• Demuxers (short for \"demultiplexers\") read an input source in order to extract\n• global properties such as metadata or chapters;\n• list of input elementary streams and their properties One demuxer instance is created for each option, and sends encoded packets to decoders or muxers. In other literature, demuxers are sometimes called splitters, because their main function is splitting a file into elementary streams (though some files only contain one elementary stream). A schematic representation of a demuxer looks like this: ┌──────────┬───────────────────────┐ │ demuxer │ │ packets for stream 0 ╞══════════╡ elementary stream 0 ├──────────────────────► │ │ │ │ global ├───────────────────────┤ │properties│ │ packets for stream 1 │ and │ elementary stream 1 ├──────────────────────► │ metadata │ │ │ ├───────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├───────────────────────┤ │ │ │ packets for stream N │ │ elementary stream N ├──────────────────────► │ │ │ └──────────┴───────────────────────┘ ▲ │ │ read from file, network stream, │ grabbing device, etc. │\n• Decoders receive encoded (compressed) packets for an audio, video, or subtitle elementary stream, and decode them into raw frames (arrays of pixels for video, PCM for audio). A decoder is typically associated with (and receives its input from) an elementary stream in a demuxer, but sometimes may also exist on its own (see Loopback decoders). A schematic representation of a decoder looks like this:\n• Filtergraphs process and transform raw audio or video frames. A filtergraph consists of one or more individual filters linked into a graph. Filtergraphs come in two flavors - simple and complex, configured with the and options, respectively. A simple filtergraph is associated with an output elementary stream; it receives the input to be filtered from a decoder and sends filtered output to that output stream’s encoder. A simple video filtergraph that performs deinterlacing (using the deinterlacer) followed by resizing (using the filter) can look like this: ┌────────────────────────┐ │ simple filtergraph │ frames from ╞════════════════════════╡ frames for a decoder │ ┌───────┐ ┌───────┐ │ an encoder ────────────►├─►│ yadif ├─►│ scale ├─►│────────────► │ └───────┘ └───────┘ │ └────────────────────────┘ A complex filtergraph is standalone and not associated with any specific stream. It may have multiple (or zero) inputs, potentially of different types (audio or video), each of which receiving data either from a decoder or another complex filtergraph’s output. It also has one or more outputs that feed either an encoder or another complex filtergraph’s input. The following example diagram represents a complex filtergraph with 3 inputs and 2 outputs (all video): Frames from second input are overlaid over those from the first. Frames from the third input are rescaled, then the duplicated into two identical streams. One of them is overlaid over the combined first two inputs, with the result exposed as the filtergraph’s first output. The other duplicate ends up being the filtergraph’s second output.\n• Encoders receive raw audio, video, or subtitle frames and encode them into encoded packets. The encoding (compression) process is typically lossy - it degrades stream quality to make the output smaller; some encoders are lossless, but at the cost of much higher output size. A video or audio encoder receives its input from some filtergraph’s output, subtitle encoders receive input from a decoder (since subtitle filtering is not supported yet). Every encoder is associated with some muxer’s output elementary stream and sends its output to that muxer. A schematic representation of an encoder looks like this:\n• Muxers (short for \"multiplexers\") receive encoded packets for their elementary streams from encoders (the transcoding path) or directly from demuxers (the streamcopy path), interleave them (when there is more than one elementary stream), and write the resulting bytes into the output file (or pipe, network stream, etc.). A schematic representation of a muxer looks like this: ┌──────────────────────┬───────────┐ packets for stream 0 │ │ muxer │ ──────────────────────►│ elementary stream 0 ╞═══════════╡ │ │ │ ├──────────────────────┤ global │ packets for stream 1 │ │properties │ ──────────────────────►│ elementary stream 1 │ and │ │ │ metadata │ ├──────────────────────┤ │ │ │ │ │ ........... │ │ │ │ │ ├──────────────────────┤ │ packets for stream N │ │ │ ──────────────────────►│ elementary stream N │ │ │ │ │ └──────────────────────┴─────┬─────┘ │ write to file, network stream, │ grabbing device, etc. │ │ ▼\n\nThe simplest pipeline in is single-stream streamcopy, that is copying one input elementary stream’s packets without decoding, filtering, or encoding them. As an example, consider an input file called with 3 elementary streams, from which we take the second and write it to file . A schematic representation of such a pipeline looks like this:\n\nThe above pipeline can be constructed with the following commandline:\n• there are no input options for this input;\n• there are two output options for this output:\n• selects the input stream to be used - from input with index 0 (i.e. the first one) the stream with index 1 (i.e. the second one);\n• selects the encoder, i.e. streamcopy with no decoding or encoding.\n\nStreamcopy is useful for changing the elementary stream count, container format, or modifying container-level metadata. Since there is no decoding or encoding, it is very fast and there is no quality loss. However, it might not work in some cases because of a variety of factors (e.g. certain information required by the target container is not available in the source). Applying filters is obviously also impossible, since filters work on decoded frames.\n\nMore complex streamcopy scenarios can be constructed - e.g. combining streams from two input files into a single output:\n\nthat can be built by the commandline\n\nThe output option is used twice here, creating two streams in the output file - one fed by the first input and one by the second. The single instance of the option selects streamcopy for both of those streams. You could also use multiple instances of this option together with Stream specifiers to apply different values to each stream, as will be demonstrated in following sections.\n\nA converse scenario is splitting multiple streams from a single input into multiple outputs:\n\nNote how a separate instance of the option is needed for every output file even though their values are the same. This is because non-global options (which is most of them) only apply in the context of the file before which they are placed.\n\nThese examples can of course be further generalized into arbitrary remappings of any number of inputs into any number of outputs.\n\nTranscoding is the process of decoding a stream and then encoding it again. Since encoding tends to be computationally expensive and in most cases degrades the stream quality (i.e. it is lossy), you should only transcode when you need to and perform streamcopy otherwise. Typical reasons to transcode are:\n• you want to feed the stream to something that cannot decode the original codec.\n\nNote that will transcode all audio, video, and subtitle streams unless you specify for them.\n\nConsider an example pipeline that reads an input file with one audio and one video stream, transcodes the video and copies the audio into a single output file. This can be schematically represented as follows\n\nand implemented with the following commandline:\n\nNote how it uses stream specifiers and to select input streams and apply different values of the option to them; see the Stream specifiers section for more details.\n\nWhen transcoding, audio and video streams can be filtered before encoding, with either a simple or complex filtergraph.\n\nSimple filtergraphs are those that have exactly one input and output, both of the same type (audio or video). They are configured with the per-stream option (with and aliases for (video) and (audio) respectively). Note that simple filtergraphs are tied to their output stream, so e.g. if you have multiple audio streams, will create a separate filtergraph for each one.\n\nTaking the trancoding example from above, adding filtering (and omitting audio, for clarity) makes it look like this:\n\nComplex filtergraphs are those which cannot be described as simply a linear processing chain applied to one stream. This is the case, for example, when the graph has more than one input and/or output, or when output stream type is different from input. Complex filtergraphs are configured with the option. Note that this option is global, since a complex filtergraph, by its nature, cannot be unambiguously associated with a single stream or file. Each instance of creates a new complex filtergraph, and there can be any number of them.\n\nA trivial example of a complex filtergraph is the filter, which has two video inputs and one video output, containing one video overlaid on top of the other. Its audio counterpart is the filter.\n\nWhile decoders are normally associated with demuxer streams, it is also possible to create \"loopback\" decoders that decode the output from some encoder and allow it to be fed back to complex filtergraphs. This is done with the directive, which takes as a parameter the index of the output stream that should be decoded. Every such directive creates a new loopback decoder, indexed with successive integers starting at zero. These indices should then be used to refer to loopback decoders in complex filtergraph link labels, as described in the documentation for .\n\nDecoding AVOptions can be passed to loopback decoders by placing them before , analogously to input/output options.\n\nE.g. the following example:\n• (line 2) encodes it with at low quality;\n• (line 4) places decoded video side by side with the original input video;\n• (line 5) combined video is then losslessly encoded and written into .\n\nSuch a transcoding pipeline can be represented with the following diagram:\n\nprovides the option for manual control of stream selection in each output file. Users can skip and let ffmpeg perform automatic stream selection as described below. The options can be used to skip inclusion of video, audio, subtitle and data streams respectively, whether manually mapped or automatically selected, except for those streams which are outputs of complex filtergraphs.\n\nThe sub-sections that follow describe the various rules that are involved in stream selection. The examples that follow next show how these rules are applied in practice.\n\nWhile every effort is made to accurately reflect the behavior of the program, FFmpeg is under continuous development and the code may have changed since the time of this writing.\n\nIn the absence of any map options for a particular output file, ffmpeg inspects the output format to check which type of streams can be included in it, viz. video, audio and/or subtitles. For each acceptable stream type, ffmpeg will pick one stream, when available, from among all the inputs.\n\nIt will select that stream based upon the following criteria:\n• for video, it is the stream with the highest resolution,\n• for audio, it is the stream with the most channels,\n• for subtitles, it is the first subtitle stream found but there’s a caveat. The output format’s default subtitle encoder can be either text-based or image-based, and only a subtitle stream of the same type will be chosen.\n\nIn the case where several streams of the same type rate equally, the stream with the lowest index is chosen.\n\nData or attachment streams are not automatically selected and can only be included using .\n\nWhen is used, only user-mapped streams are included in that output file, with one possible exception for filtergraph outputs described below.\n\nIf there are any complex filtergraph output streams with unlabeled pads, they will be added to the first output file. This will lead to a fatal error if the stream type is not supported by the output format. In the absence of the map option, the inclusion of these streams leads to the automatic stream selection of their types being skipped. If map options are present, these filtergraph streams are included in addition to the mapped streams.\n\nComplex filtergraph output streams with labeled pads must be mapped once and exactly once.\n\nStream handling is independent of stream selection, with an exception for subtitles described below. Stream handling is set via the option addressed to streams within a specific output file. In particular, codec options are applied by ffmpeg after the stream selection process and thus do not influence the latter. If no option is specified for a stream type, ffmpeg will select the default encoder registered by the output file muxer.\n\nAn exception exists for subtitles. If a subtitle encoder is specified for an output file, the first subtitle stream found of any type, text or image, will be included. ffmpeg does not validate if the specified encoder can convert the selected stream or if the converted stream is acceptable within the output format. This applies generally as well: when the user sets an encoder manually, the stream selection process cannot check if the encoded stream can be muxed into the output file. If it cannot, ffmpeg will abort and all output files will fail to be processed.\n\nThe following examples illustrate the behavior, quirks and limitations of ffmpeg’s stream selection methods.\n\nThey assume the following three input files.\n\nThere are three output files specified, and for the first two, no options are set, so ffmpeg will select streams for these two files automatically.\n\nis a Matroska container file and accepts video, audio and subtitle streams, so ffmpeg will try to select one of each type.\n\n For video, it will select from , which has the highest resolution among all the input video streams.\n\n For audio, it will select from , since it has the greatest number of channels.\n\n For subtitles, it will select from , which is the first subtitle stream from among and .\n\naccepts only audio streams, so only from is selected.\n\nFor , since a option is set, no automatic stream selection will occur. The option will select all audio streams from the second input . No other streams will be included in this output file.\n\nFor the first two outputs, all included streams will be transcoded. The encoders chosen will be the default ones registered by each output format, which may not match the codec of the selected input streams.\n\nFor the third output, codec option for audio streams has been set to , so no decoding-filtering-encoding operations will occur, or can occur. Packets of selected streams shall be conveyed from the input file and muxed within the output file.\n\nAlthough is a Matroska container file which accepts subtitle streams, only a video and audio stream shall be selected. The subtitle stream of is image-based and the default subtitle encoder of the Matroska muxer is text-based, so a transcode operation for the subtitles is expected to fail and hence the stream isn’t selected. However, in , a subtitle encoder is specified in the command and so, the subtitle stream is selected, in addition to the video stream. The presence of disables audio stream selection for .\n\nA filtergraph is setup here using the option and consists of a single video filter. The filter requires exactly two video inputs, but none are specified, so the first two available video streams are used, those of and . The output pad of the filter has no label and so is sent to the first output file . Due to this, automatic selection of the video stream is skipped, which would have selected the stream in . The audio stream with most channels viz. in , is chosen automatically. No subtitle stream is chosen however, since the MP4 format has no default subtitle encoder registered, and the user hasn’t specified a subtitle encoder.\n\nThe 2nd output file, , only accepts text-based subtitle streams. So, even though the first subtitle stream available belongs to , it is image-based and hence skipped. The selected stream, in , is the first text-based subtitle stream.\n\nThe above command will fail, as the output pad labelled has been mapped twice. None of the output files shall be processed.\n\nThis command above will also fail as the hue filter output has a label, , and hasn’t been mapped anywhere.\n\nThe command should be modified as follows,\n\nThe video stream from is sent to the hue filter, whose output is cloned once using the split filter, and both outputs labelled. Then a copy each is mapped to the first and third output files.\n\nThe overlay filter, requiring two video inputs, uses the first two unused video streams. Those are the streams from and . The overlay output isn’t labelled, so it is sent to the first output file , regardless of the presence of the option.\n\nThe aresample filter is sent the first unused audio stream, that of . Since this filter output is also unlabelled, it too is mapped to the first output file. The presence of only suppresses automatic or manual stream selection of audio streams, not outputs sent from filtergraphs. Both these mapped streams shall be ordered before the mapped stream in .\n\nThe video, audio and subtitle streams mapped to are entirely determined by automatic stream selection.\n\nconsists of the cloned video output from the hue filter and the first audio stream from . \n\n\n\nAll the numerical options, if not specified otherwise, accept a string representing a number as input, which may be followed by one of the SI unit prefixes, for example: ’K’, ’M’, or ’G’.\n\nIf ’i’ is appended to the SI unit prefix, the complete prefix will be interpreted as a unit prefix for binary multiples, which are based on powers of 1024 instead of powers of 1000. Appending ’B’ to the SI unit prefix multiplies the value by 8. This allows using, for example: ’KB’, ’MiB’, ’G’ and ’B’ as number suffixes.\n\nOptions which do not take arguments are boolean options, and set the corresponding value to true. They can be set to false by prefixing the option name with \"no\". For example using \"-nofoo\" will set the boolean option with name \"foo\" to false.\n\nOptions that take arguments support a special syntax where the argument given on the command line is interpreted as a path to the file from which the actual argument value is loaded. To use this feature, add a forward slash ’/’ immediately before the option name (after the leading dash). E.g.\n\nwill load a filtergraph description from the file named .\n\nSome options are applied per-stream, e.g. bitrate or codec. Stream specifiers are used to precisely specify which stream(s) a given option belongs to.\n\nA stream specifier is a string generally appended to the option name and separated from it by a colon. E.g. contains the stream specifier, which matches the second audio stream. Therefore, it would select the ac3 codec for the second audio stream.\n\nA stream specifier can match several streams, so that the option is applied to all of them. E.g. the stream specifier in matches all audio streams.\n\nAn empty stream specifier matches all streams. For example, or would copy all the streams without reencoding.\n\nPossible forms of stream specifiers are:\n\nThese options are shared amongst the ff* tools.\n\nThese options are provided directly by the libavformat, libavdevice and libavcodec libraries. To see the list of available AVOptions, use the option. They are separated into two categories:\n\nFor example to write an ID3v2.3 header instead of a default ID3v2.4 to an MP3 file, use the private option of the MP3 muxer:\n\nAll codec AVOptions are per-stream, and thus a stream specifier should be attached to them:\n\nIn the above example, a multichannel audio stream is mapped twice for output. The first instance is encoded with codec ac3 and bitrate 640k. The second instance is downmixed to 2 channels and encoded with codec aac. A bitrate of 128k is specified for it using absolute index of the output stream.\n\nNote: the syntax cannot be used for boolean AVOptions, use / .\n\nNote: the old undocumented way of specifying per-stream AVOptions by prepending v/a/s to the options name is now obsolete and will be removed soon.\n\nA preset file contains a sequence of = pairs, one for each line, specifying a sequence of options which would be awkward to specify on the command line. Lines starting with the hash (’#’) character are ignored and are used to provide comments. Check the directory in the FFmpeg source tree for examples.\n\nThere are two types of preset files: ffpreset and avpreset files.\n\nffpreset files are specified with the , , , and options. The option takes the filename of the preset instead of a preset name as input and can be used for any kind of codec. For the , , and options, the options specified in a preset file are applied to the currently selected codec of the same type as the preset option.\n\nThe argument passed to the , , and preset options identifies the preset file to use according to the following rules:\n\nFirst ffmpeg searches for a file named .ffpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ) or in a folder along the executable on win32, in that order. For example, if the argument is , it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named - .ffpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\navpreset files are specified with the option. They work similar to ffpreset files, but they only allow encoder- specific options. Therefore, an = pair specifying an encoder cannot be used.\n\nWhen the option is specified, ffmpeg will look for files with the suffix .avpreset in the directories (if set), and , and in the datadir defined at configuration time (usually ), in that order.\n\nFirst ffmpeg searches for a file named - .avpreset in the above-mentioned directories, where is the name of the codec to which the preset file options will be applied. For example, if you select the video codec with and use , then it will search for the file .\n\nIf no such file is found, then ffmpeg will search for a file named .avpreset in the same directories.\n\nThe and options enable generation of a file containing statistics about the generated video outputs.\n\nThe option controls the format version of the generated file.\n\nWith version the format is:\n\nWith version the format is:\n\nThe value corresponding to each key is described below:\n\nSee also the -stats_enc options for an alternative way to show encoding statistics.\n\nIf you specify the input format and device then ffmpeg can grab video and audio directly.\n\nOr with an ALSA audio source (mono input, card id 1) instead of OSS:\n\nNote that you must activate the right video source and channel before launching ffmpeg with any TV viewer such as xawtv by Gerd Knorr. You also have to set the audio recording levels correctly with a standard mixer.\n\nGrab the X11 display with ffmpeg via\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable.\n\n0.0 is display.screen number of your X11 server, same as the DISPLAY environment variable. 10 is the x-offset and 20 the y-offset for the grabbing.\n\nAny supported file format and protocol can serve as input to ffmpeg:\n• You can use YUV files as input: It will use the files: The Y files use twice the resolution of the U and V files. They are raw files, without header. They can be generated by all decent video decoders. You must specify the size of the image with the option if ffmpeg cannot guess it.\n• You can input from a raw YUV420P file: test.yuv is a file containing raw YUV planar data. Each frame is composed of the Y plane followed by the U and V planes at half vertical and horizontal resolution.\n• You can output to a raw YUV420P file:\n• You can set several input files and output files: Converts the audio file a.wav and the raw YUV video file a.yuv to MPEG file a.mpg.\n• You can also do audio and video conversions at the same time:\n• You can encode to several formats at the same time and define a mapping from input stream to output streams: Converts a.wav to a.mp2 at 64 kbits and to b.mp2 at 128 kbits. ’-map file:index’ specifies which input stream is used for each output stream, in the order of the definition of output streams.\n• You can transcode decrypted VOBs: This is a typical DVD ripping example; the input is a VOB file, the output an AVI file with MPEG-4 video and MP3 audio. Note that in this command we use B-frames so the MPEG-4 stream is DivX5 compatible, and GOP size is 300 which means one intra frame every 10 seconds for 29.97fps input video. Furthermore, the audio stream is MP3-encoded so you need to enable LAME support by passing to configure. The mapping is particularly useful for DVD transcoding to get the desired audio language. NOTE: To see the supported input formats, use .\n• You can extract images from a video, or create a video from many images: This will extract one video frame per second from the video and will output them in files named , , etc. Images will be rescaled to fit the new WxH values. If you want to extract just a limited number of frames, you can use the above command in combination with the or option, or in combination with -ss to start extracting from a certain point in time. For creating a video from many images: The syntax specifies to use a decimal number composed of three digits padded with zeroes to express the sequence number. It is the same syntax supported by the C printf function, but only formats accepting a normal integer are suitable. When importing an image sequence, -i also supports expanding shell-like wildcard patterns (globbing) internally, by selecting the image2-specific option. For example, for creating a video from filenames matching the glob pattern :\n• You can put many streams of the same type in the output: The resulting output file will contain the first four streams from the input files in reverse order.\n• The four options lmin, lmax, mblmin and mblmax use ’lambda’ units, but you may use the QP2LAMBDA constant to easily convert from ’q’ units:\n\nThis section documents the syntax and formats employed by the FFmpeg libraries and tools.\n\nFFmpeg adopts the following quoting and escaping mechanism, unless explicitly specified. The following rules are applied:\n• ‘ ’ and ‘ ’ are special characters (respectively used for quoting and escaping). In addition to them, there might be other special characters depending on the specific syntax where the escaping and quoting are employed.\n• A special character is escaped by prefixing it with a ‘ ’.\n• All characters enclosed between ‘ ’ are included literally in the parsed string. The quote character ‘ ’ itself cannot be quoted, so you may need to close the quote and escape it.\n• Leading and trailing whitespaces, unless escaped or quoted, are removed from the parsed string.\n\nNote that you may need to add a second level of escaping when using the command line or a script, which depends on the syntax of the adopted shell language.\n\nThe function defined in can be used to parse a token quoted or escaped according to the rules defined above.\n\nThe tool in the FFmpeg source tree can be used to automatically quote or escape a string in a script.\n• Escape the string containing the special character:\n• The string above contains a quote, so the needs to be escaped when quoting it:\n• Include leading or trailing whitespaces using quoting: ' this string starts and ends with whitespaces '\n• Escaping and quoting can be mixed together:\n• To include a literal ‘ ’ you can use either escaping or quoting: 'c:\\foo' can be written as c:\\\\foo\n\nIf the value is \"now\" it takes the current time.\n\nTime is local time unless Z is appended, in which case it is interpreted as UTC. If the year-month-day part is not specified it takes the current year-month-day.\n\nThere are two accepted syntaxes for expressing time duration.\n\nexpresses the number of hours, the number of minutes for a maximum of 2 digits, and the number of seconds for a maximum of 2 digits. The at the end expresses decimal value for .\n\nexpresses the number of seconds, with the optional decimal part . The optional literal suffixes ‘ ’, ‘ ’ or ‘ ’ indicate to interpret the value as seconds, milliseconds or microseconds, respectively.\n\nIn both expressions, the optional ‘ ’ indicates negative duration.\n\nThe following examples are all valid time duration:\n\nSpecify the size of the sourced video, it may be a string of the form x , or the name of a size abbreviation.\n\nThe following abbreviations are recognized:\n\nSpecify the frame rate of a video, expressed as the number of frames generated per second. It has to be a string in the format / , an integer number, a float number or a valid video frame rate abbreviation.\n\nThe following abbreviations are recognized:\n\nA ratio can be expressed as an expression, or in the form : .\n\nNote that a ratio with infinite (1/0) or negative value is considered valid, so you should check on the returned value if you want to exclude those values.\n\nThe undefined value can be expressed using the \"0:0\" string.\n\nIt can be the name of a color as defined below (case insensitive match) or a sequence, possibly followed by @ and a string representing the alpha component.\n\nThe alpha component may be a string composed by \"0x\" followed by an hexadecimal number or a decimal number between 0.0 and 1.0, which represents the opacity value (‘ ’ or ‘ ’ means completely transparent, ‘ ’ or ‘ ’ completely opaque). If the alpha component is not specified then ‘ ’ is assumed.\n\nThe string ‘ ’ will result in a random color.\n\nThe following names of colors are recognized:\n\nA channel layout specifies the spatial disposition of the channels in a multi-channel audio stream. To specify a channel layout, FFmpeg makes use of a special syntax.\n\nIndividual channels are identified by an id, as given by the table below:\n\nStandard channel layout compositions can be specified by using the following identifiers:\n\nA custom channel layout can be specified as a sequence of terms, separated by ’+’. Each term can be:\n• the name of a single channel (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.), each optionally containing a custom name after a ’@’, (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n\nA standard channel layout can be specified by the following:\n• the name of a single channel (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n• the name of a standard channel layout (e.g. ‘ ’, ‘ ’, ‘ ’, ‘ ’, ‘ ’, etc.)\n• a number of channels, in decimal, followed by ’c’, yielding the default channel layout for that number of channels (see the function ). Note that not all channel counts have a default layout.\n• a number of channels, in decimal, followed by ’C’, yielding an unknown channel layout with the specified number of channels. Note that not all channel layout specification strings support unknown channel layouts.\n• a channel layout mask, in hexadecimal starting with \"0x\" (see the macros in .\n\nBefore libavutil version 53 the trailing character \"c\" to specify a number of channels was optional, but now it is required, while a channel layout mask can also be specified as a decimal number (if and only if not followed by \"c\" or \"C\").\n\nSee also the function defined in .\n\nWhen evaluating an arithmetic expression, FFmpeg uses an internal formula evaluator, implemented through the interface.\n\nAn expression may contain unary, binary operators, constants, and functions.\n\nTwo expressions and can be combined to form another expression \" ; \". and are evaluated in turn, and the new expression evaluates to the value of .\n\nThe following binary operators are available: , , , , .\n\nThe following unary operators are available: , .\n\nSome internal variables can be used to store and load intermediary results. They can be accessed using the and functions with an index argument varying from 0 to 9 to specify which internal variable to access.\n\nThe following functions are available:\n\nThe following constants are available:\n\nAssuming that an expression is considered \"true\" if it has a non-zero value, note that:\n\nFor example the construct:\n\nIn your C code, you can extend the list of unary and binary functions, and define recognized constants, so that they are available for your expressions.\n\nThe evaluator also recognizes the International System unit prefixes. If ’i’ is appended after the prefix, binary prefixes are used, which are based on powers of 1024 instead of powers of 1000. The ’B’ postfix multiplies the value by 8, and can be appended after a unit prefix or used alone. This allows using for example ’KB’, ’MiB’, ’G’ and ’B’ as number postfix.\n\nThe list of available International System prefixes follows, with indication of the corresponding powers of 10 and of 2.\n\nlibavcodec provides some generic global options, which can be set on all the encoders and decoders. In addition, each codec may support so-called private options, which are specific for a given codec.\n\nSometimes, a global option may only affect a specific kind of codec, and may be nonsensical or ignored by another, so you need to be aware of the meaning of the specified options. Also some options are meant only for decoding or encoding.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nDecoders are configured elements in FFmpeg which allow the decoding of multimedia streams.\n\nWhen you configure your FFmpeg build, all the supported native decoders are enabled by default. Decoders requiring an external library must be enabled manually via the corresponding option. You can list all available decoders using the configure option .\n\nYou can disable all the decoders with the configure option and selectively enable / disable single decoders with the options / .\n\nThe option of the ff* tools will display the list of enabled decoders.\n\nA description of some of the currently available video decoders follows.\n\nThe decoder supports MV-HEVC multiview streams with at most two views. Views to be output are selected by supplying a list of view IDs to the decoder (the option). This option may be set either statically before decoder init, or from the callback - useful for the case when the view count or IDs change dynamically during decoding.\n\nOnly the base layer is decoded by default.\n\nNote that if you are using the CLI tool, you should be using view specifiers as documented in its manual, rather than the options documented here.\n\nlibdav1d allows libavcodec to decode the AOMedia Video 1 (AV1) codec. Requires the presence of the libdav1d headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libdav1d wrapper.\n\nThis decoder allows libavcodec to decode AVS2 streams with davs2 library.\n\nlibuavs3d allows libavcodec to decode AVS3 streams. Requires the presence of the libuavs3d headers and library during configuration. You need to explicitly configure the build with .\n\nThe following option is supported by the libuavs3d wrapper.\n\nThis decoder requires the presence of the libxevd headers and library during configuration. You need to explicitly configure the build with .\n\nThe xevd project website is at https://github.com/mpeg5/xevd.\n\nThe following options are supported by the libxevd wrapper. The xevd-equivalent options or values are listed in parentheses for easy migration.\n\nTo get a more accurate and extensive documentation of the libxevd options, invoke the command or consult the libxevd documentation.\n\nThe following options are supported by all qsv decoders.\n\nA description of some of the currently available audio decoders follows.\n\nThis decoder implements part of ATSC A/52:2010 and ETSI TS 102 366, as well as the undocumented RealAudio 3 (a.k.a. dnet).\n\nThis decoder aims to implement the complete FLAC specification from Xiph.\n\nThis decoder generates wave patterns according to predefined sequences. Its use is purely internal and the format of the data it accepts is not publicly documented.\n\nlibcelt allows libavcodec to decode the Xiph CELT ultra-low delay audio codec. Requires the presence of the libcelt headers and library during configuration. You need to explicitly configure the build with .\n\nlibgsm allows libavcodec to decode the GSM full rate audio codec. Requires the presence of the libgsm headers and library during configuration. You need to explicitly configure the build with .\n\nThis decoder supports both the ordinary GSM and the Microsoft variant.\n\nlibilbc allows libavcodec to decode the Internet Low Bitrate Codec (iLBC) audio codec. Requires the presence of the libilbc headers and library during configuration. You need to explicitly configure the build with .\n\nThe following option is supported by the libilbc wrapper.\n\nlibopencore-amrnb allows libavcodec to decode the Adaptive Multi-Rate Narrowband audio codec. Using it requires the presence of the libopencore-amrnb headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for AMR-NB exists, so users can decode AMR-NB without this library.\n\nlibopencore-amrwb allows libavcodec to decode the Adaptive Multi-Rate Wideband audio codec. Using it requires the presence of the libopencore-amrwb headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for AMR-WB exists, so users can decode AMR-WB without this library.\n\nlibopus allows libavcodec to decode the Opus Interactive Audio Codec. Requires the presence of the libopus headers and library during configuration. You need to explicitly configure the build with .\n\nAn FFmpeg native decoder for Opus exists, so users can decode Opus without this library.\n\nImplements profiles A and C of the ARIB STD-B24 standard.\n\nYet another ARIB STD-B24 caption decoder using external libaribcaption library.\n\nImplements profiles A and C of the Japanse ARIB STD-B24 standard, Brazilian ABNT NBR 15606-1, and Philippines version of ISDB-T.\n\nRequires the presence of the libaribcaption headers and library (https://github.com/xqq/libaribcaption) during configuration. You need to explicitly configure the build with . If both libaribb24 and libaribcaption are enabled, libaribcaption decoder precedes.\n\nThis codec decodes the bitmap subtitles used in DVDs; the same subtitles can also be found in VobSub file pairs and in some Matroska files.\n\nLibzvbi allows libavcodec to decode DVB teletext pages and DVB teletext subtitles. Requires the presence of the libzvbi headers and library during configuration. You need to explicitly configure the build with .\n\nEncoders are configured elements in FFmpeg which allow the encoding of multimedia streams.\n\nWhen you configure your FFmpeg build, all the supported native encoders are enabled by default. Encoders requiring an external library must be enabled manually via the corresponding option. You can list all available encoders using the configure option .\n\nYou can disable all the encoders with the configure option and selectively enable / disable single encoders with the options / .\n\nThe option of the ff* tools will display the list of enabled encoders.\n\nA description of some of the currently available audio encoders follows.\n\nThis encoder is the default AAC encoder, natively implemented into FFmpeg.\n\nThese encoders implement part of ATSC A/52:2010 and ETSI TS 102 366.\n\nThe encoder uses floating-point math, while the encoder only uses fixed-point integer math. This does not mean that one is always faster, just that one or the other may be better suited to a particular system. The encoder is not the default codec for any of the output formats, so it must be specified explicitly using the option in order to use it.\n\nThe AC-3 metadata options are used to set parameters that describe the audio, but in most cases do not affect the audio encoding itself. Some of the options do directly affect or influence the decoding and playback of the resulting bitstream, while others are just for informational purposes. A few of the options will add bits to the output stream that could otherwise be used for audio data, and will thus affect the quality of the output. Those will be indicated accordingly with a note in the option list below.\n\nThese parameters are described in detail in several publicly-available documents.\n• A/54 - Guide to the Use of the ATSC Digital Television Standard\n\nAudio Production Information is optional information describing the mixing environment. Either none or both of the fields are written to the bitstream.\n\nThe extended bitstream options are part of the Alternate Bit Stream Syntax as specified in Annex D of the A/52:2010 standard. It is grouped into 2 parts. If any one parameter in a group is specified, all values in that group will be written to the bitstream. Default values are used for those that are written but have not been specified. If the mixing levels are written, the decoder will use these values instead of the ones specified in the and options if it supports the Alternate Bit Stream Syntax.\n\nThese options are only valid for the floating-point encoder and do not exist for the fixed-point encoder due to the corresponding features not being implemented in fixed-point.\n\nThe following options are supported by FFmpeg’s FFv1 encoder.\n\nThe following options are supported by FFmpeg’s flac encoder.\n\nThis is a native FFmpeg encoder for the Opus format. Currently, it’s in development and only implements the CELT part of the codec. Its quality is usually worse and at best is equal to the libopus encoder.\n\nThe libfdk-aac library is based on the Fraunhofer FDK AAC code from the Android project.\n\nRequires the presence of the libfdk-aac headers and library during configuration. You need to explicitly configure the build with . The library is also incompatible with GPL, so if you allow the use of GPL, you should configure with .\n\nThis encoder has support for the AAC-HE profiles.\n\nVBR encoding, enabled through the or options, is experimental and only works with some combinations of parameters.\n\nSupport for encoding 7.1 audio is only available with libfdk-aac 0.1.3 or higher.\n\nFor more information see the fdk-aac project at http://sourceforge.net/p/opencore-amr/fdk-aac/.\n\nThe following options are mapped on the shared FFmpeg codec options.\n\nThe following are private options of the libfdk_aac encoder.\n• Use to convert an audio file to VBR AAC in an M4A (MP4) container:\n• Use to convert an audio file to CBR 64k kbps AAC, using the High-Efficiency AAC profile:\n\nRequires the presence of the liblc3 headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder has support for the Bluetooth SIG LC3 codec for the LE Audio protocol, and the following features of LC3plus:\n\nFor more information see the liblc3 project at https://github.com/google/liblc3.\n\nThe following options are mapped on the shared FFmpeg codec options.\n\nRequires the presence of the libmp3lame headers and library during configuration. You need to explicitly configure the build with .\n\nSee libshine for a fixed-point MP3 encoder, although with a lower quality.\n\nThe following options are supported by the libmp3lame wrapper. The -equivalent of the options are listed in parentheses.\n\nRequires the presence of the libopencore-amrnb headers and library during configuration. You need to explicitly configure the build with .\n\nThis is a mono-only encoder. Officially it only supports 8000Hz sample rate, but you can override it by setting to ‘ ’ or lower.\n\nRequires the presence of the libopus headers and library during configuration. You need to explicitly configure the build with .\n\nMost libopus options are modelled after the utility from opus-tools. The following is an option mapping chart describing options supported by the libopus wrapper, and their -equivalent in parentheses.\n\nShine is a fixed-point MP3 encoder. It has a far better performance on platforms without an FPU, e.g. armel CPUs, and some phones and tablets. However, as it is more targeted on performance than quality, it is not on par with LAME and other production-grade encoders quality-wise. Also, according to the project’s homepage, this encoder may not be free of bugs as the code was written a long time ago and the project was dead for at least 5 years.\n\nThis encoder only supports stereo and mono input. This is also CBR-only.\n\nThe original project (last updated in early 2007) is at http://sourceforge.net/projects/libshine-fxp/. We only support the updated fork by the Savonet/Liquidsoap project at https://github.com/savonet/shine.\n\nRequires the presence of the libshine headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libshine wrapper. The -equivalent of the options are listed in parentheses.\n\nRequires the presence of the libtwolame headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libtwolame wrapper. The -equivalent options follow the FFmpeg ones and are in parentheses.\n\nRequires the presence of the libvo-amrwbenc headers and library during configuration. You need to explicitly configure the build with .\n\nThis is a mono-only encoder. Officially it only supports 16000Hz sample rate, but you can override it by setting to ‘ ’ or lower.\n\nRequires the presence of the libvorbisenc headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libvorbis wrapper. The -equivalent of the options are listed in parentheses.\n\nTo get a more accurate and extensive documentation of the libvorbis options, consult the libvorbisenc’s and ’s documentations. See http://xiph.org/vorbis/, http://wiki.xiph.org/Vorbis-tools, and oggenc(1).\n\nThe equivalent options for command line utility are listed in parentheses.\n\nThe following shared options are effective for this encoder. Only special notes about this particular encoder will be documented here. For the general meaning of the options, see the Codec Options chapter.\n\nA description of some of the currently available video encoders follows.\n\nThe native jpeg 2000 encoder is lossy by default, the option can be used to set the encoding quality. Lossless encoding can be selected with .\n\nRequires the presence of the rav1e headers and library during configuration. You need to explicitly configure the build with .\n\nRequires the presence of the libaom headers and library during configuration. You need to explicitly configure the build with .\n\nThe wrapper supports the following standard libavcodec options:\n\nThe wrapper also has some specific options:\n\nRequires the presence of the SVT-AV1 headers and library during configuration. You need to explicitly configure the build with .\n\nRequires the presence of the libjxl headers and library during configuration. You need to explicitly configure the build with .\n\nThe libjxl wrapper supports the following options:\n\nRequires the presence of the libkvazaar headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder requires the presence of the libopenh264 headers and library during configuration. You need to explicitly configure the build with . The library is detected using .\n\nFor more information about the library see http://www.openh264.org.\n\nThe following FFmpeg global options affect the configurations of the libopenh264 encoder.\n\nRequires the presence of the libtheora headers and library during configuration. You need to explicitly configure the build with .\n\nFor more information about the libtheora project see http://www.theora.org/.\n\nThe following global options are mapped to internal libtheora options which affect the quality and the bitrate of the encoded stream.\n\nRequires the presence of the libvpx headers and library during configuration. You need to explicitly configure the build with .\n\nThe following options are supported by the libvpx wrapper. The -equivalent options or values are listed in parentheses for easy migration.\n\nTo reduce the duplication of documentation, only the private options and some others requiring special attention are documented here. For the documentation of the undocumented generic options, see the Codec Options chapter.\n\nTo get more documentation of the libvpx options, invoke the command , or . Further information is available in the libvpx API documentation.\n\nFor more information about libvpx see: http://www.webmproject.org/\n\nThis encoder requires the presence of the libvvenc headers and library during configuration. You need to explicitly configure the build with .\n\nThe VVenC project website is at https://github.com/fraunhoferhhi/vvenc.\n\nVVenC supports only 10-bit color spaces as input. But the internal (encoded) bit depth can be set to 8-bit or 10-bit at runtime.\n\nlibwebp is Google’s official encoder for WebP images. It can encode in either lossy or lossless mode. Lossy images are essentially a wrapper around a VP8 frame. Lossless images are a separate codec developed by Google.\n\nCurrently, libwebp only supports YUV420 for lossy and RGB for lossless due to limitations of the format and libwebp. Alpha is supported for either mode. Because of API limitations, if RGB is passed in when encoding lossy or YUV is passed in for encoding lossless, the pixel format will automatically be converted using functions from libwebp. This is not ideal and is done only for convenience.\n\nThis encoder requires the presence of the libx264 headers and library during configuration. You need to explicitly configure the build with .\n\nlibx264 supports an impressive number of features, including 8x8 and 4x4 adaptive spatial transform, adaptive B-frame placement, CAVLC/CABAC entropy coding, interlacing (MBAFF), lossless mode, psy optimizations for detail retention (adaptive quantization, psy-RD, psy-trellis).\n\nMany libx264 encoder options are mapped to FFmpeg global codec options, while unique encoder options are provided through private options. Additionally the and private options allows one to pass a list of key=value tuples as accepted by the libx264 function.\n\nThe x264 project website is at http://www.videolan.org/developers/x264.html.\n\nThe libx264rgb encoder is the same as libx264, except it accepts packed RGB pixel formats as input instead of YUV.\n\nx264 supports 8- to 10-bit color spaces. The exact bit depth is controlled at x264’s configure time.\n\nThe following options are supported by the libx264 wrapper. The -equivalent options or values are listed in parentheses for easy migration.\n\nTo reduce the duplication of documentation, only the private options and some others requiring special attention are documented here. For the documentation of the undocumented generic options, see the Codec Options chapter.\n\nTo get a more accurate and extensive documentation of the libx264 options, invoke the command or consult the libx264 documentation.\n\nIn the list below, note that the option name is shown in parentheses after the libavcodec corresponding name, in case there is a direct mapping.\n\nEncoding ffpresets for common usages are provided so they can be used with the general presets system (e.g. passing the option).\n\nThis encoder requires the presence of the libx265 headers and library during configuration. You need to explicitly configure the build with .\n\nThis encoder requires the presence of the libxavs2 headers and library during configuration. You need to explicitly configure the build with .\n\nThe following standard libavcodec options are used:\n\nThe encoder also has its own specific options:\n\neXtra-fast Essential Video Encoder (XEVE) MPEG-5 EVC encoder wrapper. The xeve-equivalent options or values are listed in parentheses for easy migration.\n\nThis encoder requires the presence of the libxeve headers and library during configuration. You need to explicitly configure the build with .\n\nThe xeve project website is at https://github.com/mpeg5/xeve.\n\nThe following options are supported by the libxeve wrapper. The xeve-equivalent options or values are listed in parentheses for easy migration.\n\nThis encoder requires the presence of the libxvidcore headers and library during configuration. You need to explicitly configure the build with .\n\nThe native encoder supports the MPEG-4 Part 2 format, so users can encode to this format without this library.\n\nThe following options are supported by the libxvid wrapper. Some of the following options are listed but are not documented, and correspond to shared codec options. See the Codec Options chapter for their documentation. The other shared options which are not listed have no effect for the libxvid encoder.\n\nThis provides wrappers to encoders (both audio and video) in the MediaFoundation framework. It can access both SW and HW encoders. Video encoders can take input in either of nv12 or yuv420p form (some encoders support both, some support only either - in practice, nv12 is the safer choice, especially among HW encoders).\n\nMicrosoft RLE aka MSRLE encoder. Only 8-bit palette mode supported. Compatible with Windows 3.1 and Windows 95.\n\nFFmpeg contains 2 ProRes encoders, the prores-aw and prores-ks encoder. The used encoder can be chosen with the option.\n\nIn the default mode of operation the encoder has to honor frame constraints (i.e. not produce frames with size bigger than requested) while still making output picture as good as possible. A frame containing a lot of small details is harder to compress and the encoder would spend more time searching for appropriate quantizers for each slice.\n\nFor the fastest encoding speed set the parameter (4 is the recommended value) and do not set a size constraint.\n\nThe ratecontrol method is selected as follows:\n• When is specified, a quality-based mode is used. Specifically this means either\n• - - constant quantizer scale, when the codec flag is also set (the ffmpeg option).\n• - - intelligent constant quality with lookahead, when the option is also set.\n• - – intelligent constant quality otherwise. For the ICQ modes, global quality range is 1 to 51, with 1 being the best quality.\n• Otherwise when the desired average bitrate is specified with the option, a bitrate-based mode is used.\n• - - VBR with lookahead, when the option is specified.\n• - - video conferencing mode, when the option is set.\n• - - constant bitrate, when is specified and equal to the average bitrate.\n• - - variable bitrate, when is specified, but is higher than the average bitrate.\n• - - average VBR mode, when is not specified, both and are set to non-zero. This mode is available for H264 and HEVC on Windows.\n• Otherwise the default ratecontrol method is used.\n\nNote that depending on your system, a different mode than the one you specified may be selected by the encoder. Set the verbosity level to or higher to see the actual settings used by the QSV runtime.\n\nAdditional libavcodec global options are mapped to MSDK options as follows:\n• For the mode, the and set the difference between and , and and respectively.\n• Setting the option to the value will make the H.264 encoder use CAVLC instead of CABAC.\n\nFollowing options are used by all qsv encoders.\n\nFollowing options can be used durning qsv encoding.\n\nThese options are used by h264_qsv\n\nThese options are used by hevc_qsv\n\nThese options are used by mpeg2_qsv\n\nThese options are used by vp9_qsv\n\nThese options are used by av1_qsv (requires libvpl).\n\nThese encoders only accept input in VAAPI hardware surfaces. If you have input in software frames, use the filter to upload them to the GPU.\n\nThe following standard libavcodec options are used:\n• If not set, this will be determined automatically from the format of the input frames and the profiles supported by the driver.\n\nAll encoders support the following options:\n\nEach encoder also has its own specific options:\n\nThis format is used by the broadcast vendor Vizrt for quick texture streaming. Advanced features of the format such as LZW compression of texture data or generation of mipmaps are not supported.\n\nSMPTE VC-2 (previously BBC Dirac Pro). This codec was primarily aimed at professional broadcasting but since it supports yuv420, yuv422 and yuv444 at 8 (limited range or full range), 10 or 12 bits, this makes it suitable for other tasks which require low overhead and low compression (like screen recording).\n\nThis codec encodes the bitmap subtitle format that is used in DVDs. Typically they are stored in VOBSUB file pairs (*.idx + *.sub), and they can also be used in Matroska files.\n\nWhen you configure your FFmpeg build, all the supported bitstream filters are enabled by default. You can list all available ones using the configure option .\n\nYou can disable all the bitstream filters using the configure option , and selectively enable any bitstream filter using the option , or you can disable a particular bitstream filter using the option .\n\nThe option of the ff* tools will display the list of all the supported bitstream filters included in your build.\n\nThe ff* tools have a -bsf option applied per stream, taking a comma-separated list of filters, whose parameters follow the filter name after a ’=’.\n\nBelow is a description of the currently available bitstream filters, with their parameters, if any.\n\nThis filter creates an MPEG-4 AudioSpecificConfig from an MPEG-2/4 ADTS header and removes the ADTS header.\n\nThis filter is required for example when copying an AAC stream from a raw ADTS AAC or an MPEG-TS container to MP4A-LATM, to an FLV file, or to MOV/MP4 files and related formats such as 3GP or M4A. Please note that it is auto-inserted for MP4A-LATM and MOV/MP4 and related formats.\n\nRemove zero padding at the end of a packet.\n\nExtract the core from a DCA/DTS stream, dropping extensions such as DTS-HD.\n\nAdd extradata to the beginning of the filtered packets except when said packets already exactly begin with the extradata that is intended to be added.\n\nIf not specified it is assumed ‘ ’.\n\nFor example the following command forces a global header (thus disabling individual packet headers) in the H.264 packets generated by the encoder, but corrects them by adding the header stored in extradata to the key packets:\n\nBlocks in DV which are marked as damaged are replaced by blocks of the specified color.\n\nCertain codecs allow the long-term headers (e.g. MPEG-2 sequence headers, or H.264/HEVC (VPS/)SPS/PPS) to be transmitted either \"in-band\" (i.e. as a part of the bitstream containing the coded frames) or \"out of band\" (e.g. on the container level). This latter form is called \"extradata\" in FFmpeg terminology.\n\nThis bitstream filter detects the in-band headers and makes them available as extradata.\n\nRemove units with types in or not in a given set from the stream.\n\nThe types used by pass_types and remove_types correspond to NAL unit types (nal_unit_type) in H.264, HEVC and H.266 (see Table 7-1 in the H.264 and HEVC specifications or Table 5 in the H.266 specification), to marker values for JPEG (without 0xFF prefix) and to start codes without start code prefix (i.e. the byte following the 0x000001) for MPEG-2. For VP8 and VP9, every unit has type zero.\n\nExtradata is unchanged by this transformation, but note that if the stream contains inline parameter sets then the output may be unusable if they are removed.\n\nFor example, to remove all non-VCL NAL units from an H.264 stream:\n\nTo remove all AUDs, SEI and filler from an H.265 stream:\n\nTo remove all user data from a MPEG-2 stream, including Closed Captions:\n\nTo remove all SEI from a H264 stream, including Closed Captions:\n\nTo remove all prefix and suffix SEI from a HEVC stream, including Closed Captions and dynamic HDR:\n\nExtract Rgb or Alpha part of an HAPQA file, without recompression, in order to create an HAPQ or an HAPAlphaOnly file.\n\nConvert an H.264 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.264 specification).\n\nThis is required by some streaming formats, typically the MPEG-2 transport stream format (muxer ).\n\nFor example to remux an MP4 file containing an H.264 stream to mpegts format with , you can use the command:\n\nPlease note that this filter is auto-inserted for MPEG-TS (muxer ) and raw H.264 (muxer ) output formats.\n\nThis applies a specific fixup to some Blu-ray BDMV H264 streams which contain redundant PPSs. The PPSs modify irrelevant parameters of the stream, confusing other transformations which require the correct extradata.\n\nThe encoder used on these impacted streams adds extra PPSs throughout the stream, varying the initial QP and whether weighted prediction was enabled. This causes issues after copying the stream into a global header container, as the starting PPS is not suitable for the rest of the stream. One side effect, for example, is seeking will return garbled output until a new PPS appears.\n\nThis BSF removes the extra PPSs and rewrites the slice headers such that the stream uses a single leading PPS in the global header, which resolves the issue.\n\nConvert an HEVC/H.265 bitstream from length prefixed mode to start code prefixed mode (as defined in the Annex B of the ITU-T H.265 specification).\n\nThis is required by some streaming formats, typically the MPEG-2 transport stream format (muxer ).\n\nFor example to remux an MP4 file containing an HEVC stream to mpegts format with , you can use the command:\n\nPlease note that this filter is auto-inserted for MPEG-TS (muxer ) and raw HEVC/H.265 (muxer or ) output formats.\n\nModifies the bitstream to fit in MOV and to be usable by the Final Cut Pro decoder. This filter only applies to the mpeg2video codec, and is likely not needed for Final Cut Pro 7 and newer with the appropriate .\n\nFor example, to remux 30 MB/sec NTSC IMX to MOV:\n\nMJPEG is a video codec wherein each video frame is essentially a JPEG image. The individual frames can be extracted without loss, e.g. by\n\nUnfortunately, these chunks are incomplete JPEG images, because they lack the DHT segment required for decoding. Quoting from http://www.digitalpreservation.gov/formats/fdd/fdd000063.shtml:\n\nAvery Lee, writing in the rec.video.desktop newsgroup in 2001, commented that \"MJPEG, or at least the MJPEG in AVIs having the MJPG fourcc, is restricted JPEG with a fixed – and *omitted* – Huffman table. The JPEG must be YCbCr colorspace, it must be 4:2:2, and it must use basic Huffman encoding, not arithmetic or progressive. . . . You can indeed extract the MJPEG frames and decode them with a regular JPEG decoder, but you have to prepend the DHT segment to them, or else the decoder won’t have any idea how to decompress the data. The exact table necessary is given in the OpenDML spec.\"\n\nThis bitstream filter patches the header of frames extracted from an MJPEG stream (carrying the AVI1 header ID and lacking a DHT segment) to produce fully qualified JPEG images.\n\nAdd an MJPEG A header to the bitstream, to enable decoding by Quicktime.\n\nExtract a representable text file from MOV subtitles, stripping the metadata header from each subtitle packet.\n\nSee also the text2movsub filter.\n\nDivX-style packed B-frames are not valid MPEG-4 and were only a workaround for the broken Video for Windows subsystem. They use more space, can cause minor AV sync issues, require more CPU power to decode (unless the player has some decoded picture queue to compensate the 2,0,2,0 frame per packet style) and cause trouble if copied into a standard container like mp4 or mpeg-ps/ts, because MPEG-4 decoders may not be able to decode them, since they are not valid MPEG-4.\n\nFor example to fix an AVI file containing an MPEG-4 stream with DivX-style packed B-frames using , you can use the command:\n\nDamages the contents of packets or simply drops them without damaging the container. Can be used for fuzzing or testing error resilience/concealment.\n\nBoth and accept expressions containing the following variables:\n\nApply modification to every byte but don’t drop any packets.\n\nDrop every video packet not marked as a keyframe after timestamp 30s but do not modify any of the remaining packets.\n\nDrop one second of audio every 10 seconds and add some random noise to the rest.\n\nThis bitstream filter passes the packets through unchanged.\n\nRepacketize PCM audio to a fixed number of samples per packet or a fixed packet rate per second. This is similar to the (ffmpeg-filters)asetnsamples audio filter but works on audio packets instead of audio frames.\n\nYou can generate the well known 1602-1601-1602-1601-1602 pattern of 48kHz audio for NTSC frame rate using the option.\n\nMerge a sequence of PGS Subtitle segments ending with an \"end of display set\" segment into a single packet.\n\nThis is required by some containers that support PGS subtitles (muxer ).\n\nSet Rec709 colorspace for each frame of the file\n\nSet Hybrid Log-Gamma parameters for each frame of the file\n\nIt accepts the following parameter:\n\nIt accepts the following parameters:\n\nThe expressions are evaluated through the eval API and can contain the following constants:\n\nFor example, to set PTS equal to DTS (not recommended if B-frames are involved):\n\nLog basic packet information. Mainly useful for testing, debugging, and development.\n\nConvert text subtitles to MOV subtitles (as used by the codec) with metadata headers.\n\nSee also the mov2textsub filter.\n\nLog trace output containing all syntax elements in the coded stream headers (everything above the level of individual coded blocks). This can be useful for debugging low-level stream issues.\n\nSupports AV1, H.264, H.265, (M)JPEG, MPEG-2 and VP9, but depending on the build only a subset of these may be available.\n\nMerge VP9 invisible (alt-ref) frames back into VP9 superframes. This fixes merging of split/segmented VP9 streams where the alt-ref frame was split from its visible counterpart.\n\nGiven a VP9 stream with correct timestamps but possibly out of order, insert additional show-existing-frame packets to correct the ordering.\n\nThe libavformat library provides some generic global options, which can be set on all the muxers and demuxers. In addition each muxer or demuxer may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nFormat stream specifiers allow selection of one or more streams that match specific properties.\n\nThe exact semantics of stream specifiers is defined by the function declared in the header and documented in the (ffmpeg)Stream specifiers section in the ffmpeg(1) manual.\n\nDemuxers are configured elements in FFmpeg that can read the multimedia streams from a particular type of file.\n\nWhen you configure your FFmpeg build, all the supported demuxers are enabled by default. You can list all available ones using the configure option .\n\nYou can disable all the demuxers using the configure option , and selectively enable a single demuxer with the option , or disable it with the option .\n\nThe option of the ff* tools will display the list of enabled demuxers. Use to view a combined list of enabled demuxers and muxers.\n\nThe description of some of the currently available demuxers follows.\n\nThis demuxer is used to demux Audible Format 2, 3, and 4 (.aa) files.\n\nThis demuxer is used to demux an ADTS input containing a single AAC stream alongwith any ID3v1/2 or APE tags in it.\n\nThis demuxer is used to demux APNG files. All headers, but the PNG signature, up to (but not including) the first fcTL chunk are transmitted as extradata. Frames are then split as being all the chunks between two fcTL ones, or between the last fcTL and IEND chunks.\n\nThis demuxer is used to demux ASF files and MMS network streams.\n\nThis demuxer reads a list of files and other directives from a text file and demuxes them one after the other, as if all their packets had been muxed together.\n\nThe timestamps in the files are adjusted so that the first file starts at 0 and each next file starts where the previous one finishes. Note that it is done globally and may cause gaps if all streams do not have exactly the same length.\n\nAll files must have the same streams (same codecs, same time base, etc.).\n\nThe duration of each file is used to adjust the timestamps of the next file: if the duration is incorrect (because it was computed using the bit-rate or because the file is truncated, for example), it can cause artifacts. The directive can be used to override the duration stored in each file.\n\nThe script is a text file in extended-ASCII, with one directive per line. Empty lines, leading spaces and lines starting with ’#’ are ignored. The following directive is recognized:\n\nThis demuxer accepts the following option:\n• Use absolute filenames and include some comments: # my first filename file /mnt/share/file-1.wav # my second filename including whitespace file '/mnt/share/file 2.wav' # my third filename including whitespace plus single quote file '/mnt/share/file 3'\\''.wav'\n• Allow for input format auto-probing, use safe filenames and set the duration of the first file:\n\nThis demuxer presents all AVStreams found in the manifest. By setting the discard flags on AVStreams the caller can decide which streams to actually receive. Each stream mirrors the and properties from the as metadata keys named \"id\" and \"variant_bitrate\" respectively.\n\nThis demuxer accepts the following option:\n\nCan directly ingest DVD titles, specifically sequential PGCs, into a conversion pipeline. Menu assets, such as background video or audio, can also be demuxed given the menu’s coordinates (at best effort).\n\nBlock devices (DVD drives), ISO files, and directory structures are accepted. Activate with in front of one of these inputs.\n\nThis demuxer does NOT have decryption code of any kind. You are on your own working with encrypted DVDs, and should not expect support on the matter.\n\nUnderlying playback is handled by libdvdnav, and structure parsing by libdvdread. FFmpeg must be built with GPL library support available as well as the configure switches and .\n\nYou will need to provide either the desired \"title number\" or exact PGC/PG coordinates. Many open-source DVD players and tools can aid in providing this information. If not specified, the demuxer will default to title 1 which works for many discs. However, due to the flexibility of the format, it is recommended to check manually. There are many discs that are authored strangely or with invalid headers.\n\nIf the input is a real DVD drive, please note that there are some drives which may silently fail on reading bad sectors from the disc, returning random bits instead which is effectively corrupt data. This is especially prominent on aging or rotting discs. A second pass and integrity checks would be needed to detect the corruption. This is not an FFmpeg issue.\n\nDVD-Video is not a directly accessible, linear container format in the traditional sense. Instead, it allows for complex and programmatic playback of carefully muxed MPEG-PS streams that are stored in headerless VOB files. To the end-user, these streams are known simply as \"titles\", but the actual logical playback sequence is defined by one or more \"PGCs\", or Program Group Chains, within the title. The PGC is in turn comprised of multiple \"PGs\", or Programs\", which are the actual video segments (and for a typical video feature, sequentially ordered). The PGC structure, along with stream layout and metadata, are stored in IFO files that need to be parsed. PGCs can be thought of as playlists in easier terms.\n\nAn actual DVD player relies on user GUI interaction via menus and an internal VM to drive the direction of demuxing. Generally, the user would either navigate (via menus) or automatically be redirected to the PGC of their choice. During this process and the subsequent playback, the DVD player’s internal VM also maintains a state and executes instructions that can create jumps to different sectors during playback. This is why libdvdnav is involved, as a linear read of the MPEG-PS blobs on the disc (VOBs) is not enough to produce the right sequence in many cases.\n\nThere are many other DVD structures (a long subject) that will not be discussed here. NAV packets, in particular, are handled by this demuxer to build accurate timing but not emitted as a stream. For a good high-level understanding, refer to: https://code.videolan.org/videolan/libdvdnav/-/blob/master/doc/dvd_structures\n\nThis demuxer accepts the following options:\n• Open chapters 3-6 from title 1 from a given DVD structure:\n• Open only chapter 5 from title 1 from a given DVD structure:\n• Demux menu with language 1 from VTS 1, PGC 1, starting at PG 1:\n\nThis format is used by various Electronic Arts games.\n\nThis demuxer presents audio and video streams found in an IMF Composition, as specified in SMPTE ST 2067-2.\n\nIf is not specified, the demuxer looks for a file called in the same directory as the CPL.\n\nThis demuxer is used to demux FLV files and RTMP network streams. In case of live network streams, if you force format, you may use live_flv option instead of flv to survive timestamp discontinuities. KUX is a flv variant used on the Youku platform.\n\nIt accepts the following options:\n\nFor example, with the overlay filter, place an infinitely looping GIF over another video:\n\nNote that in the above example the shortest option for overlay filter is used to end the output video at the length of the shortest input file, which in this case is as the GIF in this example loops infinitely.\n\nThis demuxer presents all AVStreams from all variant streams. The id field is set to the bitrate variant index number. By setting the discard flags on AVStreams (by pressing ’a’ or ’v’ in ffplay), the caller can decide which variant streams to actually receive. The total bitrate of the variant that the stream belongs to is available in a metadata key named \"variant_bitrate\".\n\nIt accepts the following options:\n\nThis demuxer reads from a list of image files specified by a pattern. The syntax and meaning of the pattern is specified by the option .\n\nThe pattern may contain a suffix which is used to automatically determine the format of the images contained in the files.\n\nThe size, the pixel format, and the format of each image must be the same for all the files in the sequence.\n\nThis demuxer accepts the following options:\n• Use for creating a video from the images in the file sequence , , ..., assuming an input frame rate of 10 frames per second:\n• As above, but start by reading from a file with index 100 in the sequence:\n• Read images matching the \"*.png\" glob pattern , that is all the files terminating with the \".png\" suffix:\n\nThe Game Music Emu library is a collection of video game music file emulators.\n\nSee https://bitbucket.org/mpyne/game-music-emu/overview for more information.\n\nIt accepts the following options:\n\nIt will export one 2-channel 16-bit 44.1 kHz audio stream. Optionally, a 16-color video stream can be exported with or without printed metadata.\n\nIt accepts the following options:\n\nSee https://lib.openmpt.org/libopenmpt/ for more information.\n\nSome files have multiple subsongs (tracks) this can be set with the option.\n\nIt accepts the following options:\n\nDemuxer for Quicktime File Format & ISO/IEC Base Media File Format (ISO/IEC 14496-12 or MPEG-4 Part 12, ISO/IEC 15444-12 or JPEG 2000 Part 12).\n\nThis demuxer accepts the following options:\n\nAudible AAX files are encrypted M4B files, and they can be decrypted by specifying a 4 byte activation secret.\n\nThis demuxer accepts the following options:\n\nThis demuxer allows reading of MJPEG, where each frame is represented as a part of multipart/x-mixed-replace stream.\n\nThis demuxer allows one to read raw video data. Since there is no header specifying the assumed video parameters, the user must specify them in order to be able to decode the data correctly.\n\nThis demuxer accepts the following options:\n\nFor example to read a rawvideo file with , assuming a pixel format of , a video size of , and a frame rate of 10 images per second, use the command:\n\nRCWT (Raw Captions With Time) is a format native to ccextractor, a commonly used open source tool for processing 608/708 Closed Captions (CC) sources. For more information on the format, see (ffmpeg-formats)rcwtenc.\n\nThis demuxer implements the specification as of March 2024, which has been stable and unchanged since April 2014.\n• Render CC to ASS using the built-in decoder: Note that if your output appears to be empty, you may have to manually set the decoder’s option to pick the desired CC substream.\n• Convert an RCWT backup to Scenarist (SCC) format: Note that the SCC format does not support all of the possible CC extensions that can be stored in RCWT (such as EIA-708).\n\nThis demuxer reads the script language used by SBaGen http://uazu.net/sbagen/ to generate binaural beats sessions. A SBG script looks like that:\n\nA SBG script can mix absolute and relative timestamps. If the script uses either only absolute timestamps (including the script start time) or only relative ones, then its layout is fixed, and the conversion is straightforward. On the other hand, if the script mixes both kind of timestamps, then the reference for relative timestamps will be taken from the current time of day at the time the script is read, and the script layout will be frozen according to that reference. That means that if the script is directly played, the actual times will match the absolute timestamps up to the sound controller’s clock accuracy, but if the user somehow pauses the playback or seeks, all times will be shifted accordingly.\n\nTED does not provide links to the captions, but they can be guessed from the page. The file from the FFmpeg source tree contains a bookmarklet to expose them.\n\nThis demuxer accepts the following option:\n\nExample: convert the captions to a format most players understand:\n\nDue to security concerns, Vapoursynth scripts will not be autodetected so the input format has to be forced. For ff* CLI tools, add before the input .\n\nThis demuxer accepts the following option:\n\nThis demuxer accepts the following options:\n\nThis demuxer accepts the following options:\n\nMuxers are configured elements in FFmpeg which allow writing multimedia streams to a particular type of file.\n\nWhen you configure your FFmpeg build, all the supported muxers are enabled by default. You can list all available muxers using the configure option .\n\nYou can disable all the muxers with the configure option and selectively enable / disable single muxers with the options / .\n\nThe option of the ff* tools will display the list of enabled muxers. Use to view a combined list of enabled demuxers and muxers.\n\nA description of some of the currently available muxers follows.\n\nThis section covers raw muxers. They accept a single stream matching the designated codec. They do not store timestamps or metadata. The recognized extension is the same as the muxer name unless indicated otherwise.\n\nIt comprises the following muxers. The media type and the eventual extensions used to automatically selects the muxer from the output extensions are also shown.\n• Store raw video frames with the ‘ ’ muxer using : Since the rawvideo muxer do not store the information related to size and format, this information must be provided when demuxing the file:\n\nThey accept a single stream matching the designated codec. They do not store timestamps or metadata. The recognized extension is the same as the muxer name.\n\nIt comprises the following muxers. The optional additional extension used to automatically select the muxer from the output extension is also shown in parentheses.\n\nThis section covers formats belonging to the MPEG-1 and MPEG-2 Systems family.\n\nThe MPEG-1 Systems format (also known as ISO/IEEC 11172-1 or MPEG-1 program stream) has been adopted for the format of media track stored in VCD (Video Compact Disc).\n\nThe MPEG-2 Systems standard (also known as ISO/IEEC 13818-1) covers two containers formats, one known as transport stream and one known as program stream; only the latter is covered here.\n\nThe MPEG-2 program stream format (also known as VOB due to the corresponding file extension) is an extension of MPEG-1 program stream: in addition to support different codecs for the audio and video streams, it also stores subtitles and navigation metadata. MPEG-2 program stream has been adopted for storing media streams in SVCD and DVD storage devices.\n\nThis section comprises the following muxers.\n\nThis section covers formats belonging to the QuickTime / MOV family, including the MPEG-4 Part 14 format and ISO base media file format (ISOBMFF). These formats share a common structure based on the ISO base media file format (ISOBMFF).\n\nThe MOV format was originally developed for use with Apple QuickTime. It was later used as the basis for the MPEG-4 Part 1 (later Part 14) format, also known as ISO/IEC 14496-1. That format was then generalized into ISOBMFF, also named MPEG-4 Part 12 format, ISO/IEC 14496-12, or ISO/IEC 15444-12.\n\nIt comprises the following muxers.\n\nThe ‘ ’, ‘ ’, and ‘ ’ muxers support fragmentation. Normally, a MOV/MP4 file has all the metadata about all packets stored in one location.\n\nThis data is usually written at the end of the file, but it can be moved to the start for better playback by adding to the , or using the tool).\n\nA fragmented file consists of a number of fragments, where packets and metadata about these packets are stored together. Writing a fragmented file has the advantage that the file is decodable even if the writing is interrupted (while a normal MOV/MP4 is undecodable if it is not properly finished), and it requires less memory when writing very long files (since writing normal MOV/MP4 files stores info about every single packet in memory until the file is closed). The downside is that it is less compatible with other applications.\n\nFragmentation is enabled by setting one of the options that define how to cut the file into fragments:\n\nIf more than one condition is specified, fragments are cut when one of the specified conditions is fulfilled. The exception to this is the option , which has to be fulfilled for any of the other conditions to apply.\n• Push Smooth Streaming content in real time to a publishing point on IIS with the ‘ ’ muxer using :\n\nThis muxer accepts a single ATRAC1 audio stream with either one or two channels and a sample rate of 44100Hz.\n\nAs AEA supports storing the track title, this muxer will also write the title from stream’s metadata to the container.\n\nIt accepts a single ADPCM_IMA_ALP stream with no more than 2 channels and a sample rate not greater than 44100 Hz.\n\nIt accepts a single audio stream containing an AMR NB stream.\n• Use to generate an APNG output with 2 repetitions, and with a delay of half a second after the first repetition:\n\nThe and options set the corresponding flags in the header which can be later retrieved to process the audio stream accordingly.\n\nThe ‘ ’ variant should be selected for streaming.\n\nNote that Windows Media Audio (wma) and Windows Media Video (wmv) use this muxer too.\n\nThis format is used to play audio on some Nintendo Wii games.\n\nThe and options can be used to define a section of the file to loop for players honoring such options.\n\nAVI is a proprietary format developed by Microsoft, and later formally specified through the Open DML specification.\n\nBecause of differences in players implementations, it might be required to set some options to make sure that the generated output can be correctly played by the target player.\n\nThis muxers stores images encoded using the AV1 codec.\n\nIt accepts one or two video streams. In case two video streams are provided, the second one shall contain a single plane storing the alpha mask.\n\nIn case more than one image is provided, the generated output is considered an animated AVIF and the number of loops can be specified with the option.\n\nThis is based on the specification by Alliance for Open Media at url https://aomediacodec.github.io/av1-avif.\n\nIt accepts one audio stream, one video stream, or both.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis muxer feeds audio data to the Chromaprint library, which generates a fingerprint for the provided audio data. See: https://acoustid.org/chromaprint\n\nIt takes a single signed native-endian 16-bit raw audio stream of at most 2 channels.\n\nThis muxer computes and prints the Adler-32 CRC of all the input audio and video frames. By default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the CRC.\n\nThe output of the muxer consists of a single line of the form: CRC=0x , where is a hexadecimal number 0-padded to 8 digits containing the CRC for all the decoded input frames.\n\nSee also the framecrc muxer.\n• Use to compute the CRC of the input, and store it in the file :\n• Use to print the CRC to stdout with the command:\n• You can select the output format of each frame with by specifying the audio and video codec and format. For example, to compute the CRC of the input audio converted to PCM unsigned 8-bit and the input video converted to MPEG-2 video, use the command:\n\nThis muxer creates segments and manifest files according to the MPEG-DASH standard ISO/IEC 23009-1:2014 and following standard updates.\n\nFor more information see:\n\nThis muxer creates an MPD (Media Presentation Description) manifest file and segment files for each stream. Segment files are placed in the same directory of the MPD manifest file.\n\nThe segment filename might contain pre-defined identifiers used in the manifest section as defined in section 5.3.9.4.4 of the standard.\n\nAvailable identifiers are , , , and . In addition to the standard identifiers, an ffmpeg-specific identifier is also supported. When specified, will replace in the file name with muxing format’s extensions such as , etc.\n\nGenerate a DASH output reading from an input source in realtime using .\n\nTwo multimedia streams are generated from the input file, both containing a video stream encoded through ‘ ’, and an audio stream encoded with ‘ ’. The first multimedia stream contains video with a bitrate of 800k and audio at the default rate, the second with video scaled to 320x170 pixels at 300k and audio resampled at 22005 Hz.\n\nThe option keeps only the latest 5 segments with the default duration of 5 seconds.\n\nIt accepts a single 6-channels audio stream resampled at 96000 Hz encoded with the ‘ ’ codec.\n\nUse to mux input audio to a ‘ ’ channel layout resampled at 96000Hz:\n\nFor ffmpeg versions before 7.0 you might have to use the ‘ ’ filter to limit the muxed packet size, because this format does not support muxing packets larger than 65535 bytes (3640 samples). For newer ffmpeg versions audio is automatically packetized to 36000 byte (2000 sample) packets.\n\nIt accepts exactly one ‘ ’ video stream and at most two ‘ ’ audio streams. More constraints are defined by the property of the video, which must correspond to a DV video supported profile, and on the framerate.\n\nUse to convert the input:\n\nThis muxer writes the streams metadata in the ‘ ’ format.\n\nSee (ffmpeg-formats)the Metadata chapter for information about the format.\n\nUse to extract metadata from an input file to a file in ‘ ’ format:\n\nThe ‘ ’ pseudo-muxer allows the separation of encoding and muxing by using a first-in-first-out queue and running the actual muxer in a separate thread.\n\nThis is especially useful in combination with the tee muxer and can be used to send data to several destinations with different reliability/writing speed/latency.\n\nThe target muxer is either selected from the output name or specified through the option.\n\nThe behavior of the ‘ ’ muxer if the queue fills up or if the output fails (e.g. if a packet cannot be written to the output) is selectable:\n• Output can be transparently restarted with configurable delay between retries based on real time or time of the processed stream.\n• Encoding can be blocked during temporary failure, or continue transparently dropping packets in case the FIFO queue fills up.\n\nAPI users should be aware that callback functions ( , and ) used within its must be thread-safe.\n\nUse to stream to an RTMP server, continue processing the stream at real-time rate even in case of temporary failure (network outage) and attempt to recover streaming every second indefinitely:\n\nThis format was used as internal format for several Sega games.\n\nFor more information regarding the Sega film file format, visit http://wiki.multimedia.cx/index.php?title=Sega_FILM.\n\nIt accepts at maximum one ‘ ’ or raw video stream, and at maximum one audio stream.\n\nThis format is used by several Adobe tools to store a generated filmstrip export. It accepts a single raw video stream.\n\nThis image format is used to store astronomical data.\n\nFor more information regarding the format, visit https://fits.gsfc.nasa.gov.\n\nThis muxer accepts exactly one FLAC audio stream. Additionally, it is possible to add images with disposition ‘ ’.\n\nUse to store the audio stream from an input file, together with several pictures used with ‘ ’ disposition:\n\nThis muxer computes and prints the Adler-32 CRC for each audio and video packet. By default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the CRC.\n\nThe output of the muxer consists of a line for each audio and video packet of the form:\n\nis a hexadecimal number 0-padded to 8 digits containing the CRC of the packet.\n\nFor example to compute the CRC of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, use the command:\n\nWith , you can select the output format to which the audio and video frames are encoded before computing the CRC for each packet by specifying the audio and video codec. For example, to compute the CRC of each decoded input audio frame converted to PCM unsigned 8-bit and of each decoded input video frame converted to MPEG-2 video, use the command:\n\nSee also the crc muxer.\n\nThis muxer computes and prints a cryptographic hash for each audio and video packet. This can be used for packet-by-packet equality checks without having to individually do a binary comparison on each.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of a line for each audio and video packet of the form:\n\nis a hexadecimal number representing the computed hash for the packet.\n\nTo compute the SHA-256 hash of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, using the MD5 hash function, use the command:\n\nSee also the hash muxer.\n\nThis is a variant of the framehash muxer. Unlike that muxer, it defaults to using the MD5 hash function.\n\nTo compute the MD5 hash of the audio and video frames in , converted to raw audio and video packets, and store it in the file :\n\nTo print the information to stdout, use the command:\n\nSee also the framehash and md5 muxers.\n\nNote that the GIF format has a very large time base: the delay between two frames can therefore not be smaller than one centi second.\n\nEncode a gif looping 10 times, with a 5 seconds delay between the loops:\n\nNote 1: if you wish to extract the frames into separate GIF files, you need to force the image2 muxer:\n\nGXF was developed by Grass Valley Group, then standardized by SMPTE as SMPTE 360M and was extended in SMPTE RDD 14-2007 to include high-definition video resolutions.\n\nIt accepts at most one video stream with codec ‘ ’, or ‘ ’, or ‘ ’, or ‘ ’ with resolution ‘ ’ or ‘ ’, and several audio streams with rate 48000Hz and codec ‘ ’.\n\nThis muxer computes and prints a cryptographic hash of all the input audio and video frames. This can be used for equality checks without having to do a complete binary comparison.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. Timestamps are ignored. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of a single line of the form: = , where is a short string representing the hash function used, and is a hexadecimal number representing the computed hash.\n\nTo compute the SHA-256 hash of the input converted to raw audio and video, and store it in the file :\n\nTo print an MD5 hash to stdout use the command:\n\nSee also the framehash muxer.\n\nHTTP dynamic streaming, or HDS, is an adaptive bitrate streaming method developed by Adobe. HDS delivers MP4 video content over HTTP connections. HDS can be used for on-demand streaming or live streaming.\n\nThis muxer creates an .f4m (Adobe Flash Media Manifest File) manifest, an .abst (Adobe Bootstrap File) for each stream, and segment files in a directory specified as the output.\n\nThese needs to be accessed by an HDS player throuhg HTTPS for it to be able to perform playback on the generated stream.\n\nUse to generate HDS files to the directory in real-time rate:\n\nApple HTTP Live Streaming muxer that segments MPEG-TS according to the HTTP Live Streaming (HLS) specification.\n\nIt creates a playlist file, and one or more segment files. The output filename specifies the playlist filename.\n\nBy default, the muxer creates a file for each segment produced. These files have the same name as the playlist, followed by a sequential number and a .ts extension.\n\nMake sure to require a closed GOP when encoding and to set the GOP size to fit your segment time constraint.\n\nFor example, to convert an input file with :\n\nThis example will produce the playlist, , and segment files: , , , etc.\n\nSee also the segment muxer, which provides a more generic and flexible implementation of a segmenter, and can be used to perform HLS segmentation.\n\nIAMF is used to provide immersive audio content for presentation on a wide range of devices in both streaming and offline applications. These applications include internet audio streaming, multicasting/broadcasting services, file download, gaming, communication, virtual and augmented reality, and others. In these applications, audio may be played back on a wide range of devices, e.g., headphones, mobile phones, tablets, TVs, sound bars, home theater systems, and big screens.\n\nThis format was promoted and desgined by Alliance for Open Media.\n\nFor more information about this format, see https://aomedia.org/iamf/.\n\nMicrosoft’s icon file format (ICO) has some strict limitations that should be noted:\n• Size cannot exceed 256 pixels in any dimension\n• Only BMP and PNG images can be stored\n• If a BMP image is used, it must be one of the following pixel formats:\n• If a BMP image is used, it must use the BITMAPINFOHEADER DIB header\n• If a PNG image is used, it must use the rgba pixel format\n\nThe output filenames are specified by a pattern, which can be used to produce sequentially numbered series of files. The pattern may contain the string \"%d\" or \"%0 d\", this string specifies the position of the characters representing a numbering in the filenames. If the form \"%0 d\" is used, the string representing the number in each filename is 0-padded to digits. The literal character ’%’ can be specified in the pattern with the string \"%%\".\n\nIf the pattern contains \"%d\" or \"%0 d\", the first filename of the file list specified will contain the number 1, all the following numbers will be sequential.\n\nThe pattern may contain a suffix which is used to automatically determine the format of the image files to write.\n\nFor example the pattern \"img-%03d.bmp\" will specify a sequence of filenames of the form , , ..., , etc. The pattern \"img%%-%d.jpg\" will specify a sequence of filenames of the form , , ..., , etc.\n\nThe image muxer supports the .Y.U.V image file format. This format is special in that each image frame consists of three files, for each of the YUV420P components. To read or write this image file format, specify the name of the ’.Y’ file. The muxer will automatically open the ’.U’ and ’.V’ files as required.\n\nThe ‘ ’ muxer accepts the same options as the ‘ ’ muxer, but ignores the pattern verification and expansion, as it is supposed to write to the command output rather than to an actual stored file.\n• Use for creating a sequence of files , , ..., taking one image every second from the input video: Note that with , if the format is not specified with the option and the output filename specifies an image file format, the image2 muxer is automatically selected, so the previous command can be written as: Note also that the pattern must not necessarily contain \"%d\" or \"%0 d\", for example to create a single image file from the start of the input video you can employ the command:\n• The option allows you to expand the filename with date and time information. Check the documentation of the function for the syntax. To generate image files from the \"%Y-%m-%d_%H-%M-%S\" pattern, the following command can be used:\n• Set the file name with current frame’s PTS:\n• Publish contents of your desktop directly to a WebDAV server every second:\n\nThe Berkeley/IRCAM/CARL Sound Format, developed in the 1980s, is a result of the merging of several different earlier sound file formats and systems including the csound system developed by Dr Gareth Loy at the Computer Audio Research Lab (CARL) at UC San Diego, the IRCAM sound file system developed by Rob Gross and Dan Timis at the Institut de Recherche et Coordination Acoustique / Musique in Paris and the Berkeley Fast Filesystem.\n\nIt was developed initially as part of the Berkeley/IRCAM/CARL Sound Filesystem, a suite of programs designed to implement a filesystem for audio applications running under Berkeley UNIX. It was particularly popular in academic music research centres, and was used a number of times in the creation of early computer-generated compositions.\n\nIVF was developed by On2 Technologies (formerly known as Duck Corporation), to store internally developed codecs.\n\nFor more information about the format, see http://unicorn.us.com/jacosub/jscripts.html.\n\nThis custom VAG container is used by some Simon & Schuster Interactive games such as \"Real War\", and \"Real War: Rogue States\".\n\nLRC (short for LyRiCs) is a computer file format that synchronizes song lyrics with an audio file, such as MP3, Vorbis, or MIDI.\n\nThe following metadata tags are converted to the format corresponding metadata:\n\nIf ‘ ’ is not explicitly set, it is automatically set to the libavformat version.\n\nThis muxer implements the matroska and webm container specs.\n\nThe recognized metadata settings in this muxer are:\n\nFor example a 3D WebM clip can be created using the following command line:\n\nThis is a variant of the hash muxer. Unlike that muxer, it defaults to using the MD5 hash function.\n\nSee also the hash and framemd5 muxers.\n• To compute the MD5 hash of the input converted to raw audio and video, and store it in the file :\n• To print the MD5 hash to stdout:\n\nSMAF is a music data format specified by Yamaha for portable electronic devices, such as mobile phones and personal digital assistants.\n\nThe MP3 muxer writes a raw MP3 stream with the following optional features:\n• An ID3v2 metadata header at the beginning (enabled by default). Versions 2.3 and 2.4 are supported, the private option controls which one is used (3 or 4). Setting to 0 disables the ID3v2 header completely. The muxer supports writing attached pictures (APIC frames) to the ID3v2 header. The pictures are supplied to the muxer in form of a video stream with a single packet. There can be any number of those streams, each will correspond to a single APIC frame. The stream metadata tags and map to APIC and respectively. See http://id3.org/id3v2.4.0-frames for allowed picture types. Note that the APIC frames must be written at the beginning, so the muxer will buffer the audio frames until it gets all the pictures. It is therefore advised to provide the pictures as soon as possible to avoid excessive buffering.\n• A Xing/LAME frame right after the ID3v2 header (if present). It is enabled by default, but will be written only if the output is seekable. The private option can be used to disable it. The frame contains various information that may be useful to the decoder, like the audio duration or encoder delay.\n• A legacy ID3v1 tag at the end of the file (disabled by default). It may be enabled with the private option, but as its capabilities are very limited, its usage is not recommended.\n\nWrite an mp3 with an ID3v2.3 header and an ID3v1 footer:\n\nTo attach a picture to an mp3 file select both the audio and the picture stream with :\n\nThis muxer implements ISO 13818-1 and part of ETSI EN 300 468.\n\nThe recognized metadata settings in mpegts muxer are and . If they are not set the default for is ‘ ’ and the default for is ‘ ’.\n\nThis muxer does not generate any output file, it is mainly useful for testing or benchmarking purposes.\n\nFor example to benchmark decoding with you can use the command:\n\nNote that the above command does not read or write the file, but specifying the output file is required by the syntax.\n\nAlternatively you can write the command as:\n\nRCWT (Raw Captions With Time) is a format native to ccextractor, a commonly used open source tool for processing 608/708 Closed Captions (CC) sources. It can be used to archive the original extracted CC bitstream and to produce a source file for later processing or conversion. The format allows for interoperability between ccextractor and FFmpeg, is simple to parse, and can be used to create a backup of the CC presentation.\n\nThis muxer implements the specification as of March 2024, which has been stable and unchanged since April 2014.\n\nThis muxer will have some nuances from the way that ccextractor muxes RCWT. No compatibility issues when processing the output with ccextractor have been observed as a result of this so far, but mileage may vary and outputs will not be a bit-exact match.\n\nA free specification of RCWT can be found here: https://github.com/CCExtractor/ccextractor/blob/master/docs/BINARY_FILE_FORMAT.TXT\n\nThis muxer outputs streams to a number of separate files of nearly fixed duration. Output filename pattern can be set in a fashion similar to image2, or by using a template if the option is enabled.\n\nis a variant of the muxer used to write to streaming output formats, i.e. which do not require global headers, and is recommended for outputting e.g. to MPEG transport stream segments. is a shorter alias for .\n\nEvery segment starts with a keyframe of the selected reference stream, which is set through the option.\n\nNote that if you want accurate splitting for a video file, you need to make the input key frames correspond to the exact splitting times expected by the segmenter, or the segment muxer will start the new segment with the key frame found next after the specified start time.\n\nThe segment muxer works best with a single constant frame rate video.\n\nOptionally it can generate a list of the created segments, by setting the option . The list type is specified by the option. The entry filenames in the segment list are set by default to the basename of the corresponding segment files.\n\nSee also the hls muxer, which provides a more specific implementation for HLS segmentation.\n\nThe segment muxer supports the following options:\n\nMake sure to require a closed GOP when encoding and to set the GOP size to fit your segment time constraint.\n• Remux the content of file to a list of segments , , etc., and write the list of generated segments to :\n• Segment input and set output format options for the output segments:\n• Segment the input file according to the split points specified by the option:\n• Use the option to force key frames in the input at the specified location, together with the segment option to account for possible roundings operated when setting key frame times. In order to force key frames on the input file, transcoding is required.\n• Segment the input file by splitting the input file according to the frame numbers sequence specified with the option:\n• Convert the to TS segments using the and encoders:\n• Segment the input file, and create an M3U8 live playlist (can be used as live HLS source):\n\nSmooth Streaming muxer generates a set of files (Manifest, chunks) suitable for serving with conventional web server.\n\nThis muxer computes and prints a cryptographic hash of all the input frames, on a per-stream basis. This can be used for equality checks without having to do a complete binary comparison.\n\nBy default audio frames are converted to signed 16-bit raw audio and video frames to raw video before computing the hash, but the output of explicit conversions to other codecs can also be used. Timestamps are ignored. It uses the SHA-256 cryptographic hash function by default, but supports several other algorithms.\n\nThe output of the muxer consists of one line per stream of the form: , , = , where is the index of the mapped stream, is a single character indicating the type of stream, is a short string representing the hash function used, and is a hexadecimal number representing the computed hash.\n\nTo compute the SHA-256 hash of the input converted to raw audio and video, and store it in the file :\n\nTo print an MD5 hash to stdout use the command:\n\nSee also the hash and framehash muxers.\n\nThe tee muxer can be used to write the same data to several outputs, such as files or streams. It can be used, for example, to stream a video over a network and save it to disk at the same time.\n\nIt is different from specifying several outputs to the command-line tool. With the tee muxer, the audio and video data will be encoded only once. With conventional multiple outputs, multiple encoding operations in parallel are initiated, which can be a very expensive process. The tee muxer is not useful when using the libavformat API directly because it is then possible to feed the same packets to several muxers directly.\n\nSince the tee muxer does not represent any particular output format, ffmpeg cannot auto-select output streams. So all streams intended for output must be specified using . See the examples below.\n\nSome encoders may need different options depending on the output format; the auto-detection of this can not work with the tee muxer, so they need to be explicitly specified. The main example is the flag.\n\nThe slave outputs are specified in the file name given to the muxer, separated by ’|’. If any of the slave name contains the ’|’ separator, leading or trailing spaces or any special character, those must be escaped (see (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual).\n\nMuxer options can be specified for each slave by prepending them as a list of = pairs separated by ’:’, between square brackets. If the options values contain a special character or the ’:’ separator, they must be escaped; note that this is a second level escaping.\n\nThe following special options are also recognized:\n• Encode something and both archive it in a WebM file and stream it as MPEG-TS over UDP:\n• As above, but continue streaming even if output to local file fails (for example local drive fills up):\n• Use to encode the input, and send the output to three different destinations. The bitstream filter is used to add extradata information to all the output video keyframes packets, as requested by the MPEG-TS format. The select option is applied to in order to make it contain only audio packets.\n• As above, but select only stream for the audio output. Note that a second level escaping must be performed, as \":\" is a special character used to separate options.\n\nThis muxer writes out WebM headers and chunks as separate files which can be consumed by clients that support WebM Live streams via DASH.\n\nThis muxer supports the following options:\n\nThis muxer implements the WebM DASH Manifest specification to generate the DASH manifest XML. It also supports manifest generation for DASH live streams.\n\nFor more information see:\n\nThis muxer supports the following options:\n\nFFmpeg is able to dump metadata from media files into a simple UTF-8-encoded INI-like text file and then load it back using the metadata muxer/demuxer.\n\nThe file format is as follows:\n• A file consists of a header and a number of metadata tags divided into sections, each on its own line.\n• The header is a ‘ ’ string, followed by a version number (now 1).\n• Metadata tags are of the form ‘ ’\n• After global metadata there may be sections with per-stream/per-chapter metadata.\n• A section starts with the section name in uppercase (i.e. STREAM or CHAPTER) in brackets (‘ ’, ‘ ’) and ends with next section or end of file.\n• At the beginning of a chapter section there may be an optional timebase to be used for start/end values. It must be in form ‘ ’, where and are integers. If the timebase is missing then start/end times are assumed to be in nanoseconds. Next a chapter section must contain chapter start and end times in form ‘ ’, ‘ ’, where is a positive integer.\n• Empty lines and lines starting with ‘ ’ or ‘ ’ are ignored.\n• Metadata keys or values containing special characters (‘ ’, ‘ ’, ‘ ’, ‘ ’ and a newline) must be escaped with a backslash ‘ ’.\n• Note that whitespace in metadata (e.g. ‘ ’) is considered to be a part of the tag (in the example above key is ‘ ’, value is ‘ ’).\n\nA ffmetadata file might look like this:\n\nBy using the ffmetadata muxer and demuxer it is possible to extract metadata from an input file to an ffmetadata file, and then transcode the file into an output file with the edited ffmetadata file.\n\nExtracting an ffmetadata file with goes as follows:\n\nReinserting edited metadata information from the FFMETADATAFILE file can be done as:\n\nThe libavformat library provides some generic global options, which can be set on all the protocols. In addition each protocol may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the options or using the API for programmatic use.\n\nThe list of supported options follows:\n\nProtocols are configured elements in FFmpeg that enable access to resources that require specific protocols.\n\nWhen you configure your FFmpeg build, all the supported protocols are enabled by default. You can list all available ones using the configure option \"–list-protocols\".\n\nYou can disable all the protocols using the configure option \"–disable-protocols\", and selectively enable a protocol using the option \"–enable-protocol= \", or you can disable a particular protocol using the option \"–disable-protocol= \".\n\nThe option \"-protocols\" of the ff* tools will display the list of supported protocols.\n\nAll protocols accept the following options:\n\nA description of the currently available protocols follows.\n\nFFmpeg must be compiled with –enable-librabbitmq to support AMQP. A separate AMQP broker must also be run. An example open-source AMQP broker is RabbitMQ.\n\nAfter starting the broker, an FFmpeg client may stream data to the broker using the command:\n\nWhere hostname and port (default is 5672) is the address of the broker. The client may also set a user/password for authentication. The default for both fields is \"guest\". Name of virtual host on broker can be set with vhost. The default value is \"/\".\n\nMuliple subscribers may stream from the broker using the command:\n\nIn RabbitMQ all data published to the broker flows through a specific exchange, and each subscribing client has an assigned queue/buffer. When a packet arrives at an exchange, it may be copied to a client’s queue depending on the exchange and routing_key fields.\n\nThe following options are supported:\n\nFill data in a background thread, to decouple I/O operation from demux thread.\n\nRead angle 2 of playlist 4 from BluRay mounted to /mnt/bluray, start from chapter 2:\n\nCache the input stream to temporary file. It brings seeking capability to live streams.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere , , ..., are the urls of the resource to be concatenated, each one possibly specifying a distinct protocol.\n\nFor example to read a sequence of files , , with use the command:\n\nNote that you may need to escape the character \"|\" which is special for many shells.\n\nRead and seek from many resources in sequence as if they were a unique resource.\n\nA URL accepted by this protocol has the syntax:\n\nwhere is the url containing a line break delimited list of resources to be concatenated, each one possibly specifying a distinct protocol. Special characters must be escaped with backslash or single quotes. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual.\n\nFor example to read a sequence of files , , listed in separate lines within a file with use the command:\n\nWhere contains the lines:\n\nData in-line in the URI. See http://en.wikipedia.org/wiki/Data_URI_scheme.\n\nFor example, to convert a GIF file given inline with :\n\nIf is not specified, by default the stdout file descriptor will be used for writing, stdin for reading. Unlike the pipe protocol, fd protocol has seek support if it corresponding to a regular file. fd protocol doesn’t support pass file descriptor via URL for security.\n\nThis protocol accepts the following options:\n\nRead from or write to a file.\n\nA file URL can have the form:\n\nwhere is the path of the file to read.\n\nAn URL that does not have a protocol prefix will be assumed to be a file URL. Depending on the build, an URL that looks like a Windows path with the drive letter at the beginning will also be assumed to be a file URL (usually not the case in builds for unix-like systems).\n\nFor example to read from a file with use the command:\n\nThis protocol accepts the following options:\n\nRead from or write to remote resources using FTP protocol.\n\nThis protocol accepts the following options.\n\nNOTE: Protocol can be used as output, but it is recommended to not do it, unless special care is taken (tests, customized server configuration etc.). Different FTP servers behave in different way during seek operation. ff* tools may produce incomplete content due to server limitations.\n\nRead Apple HTTP Live Streaming compliant segmented stream as a uniform one. The M3U8 playlists describing the segments can be remote HTTP resources or local files, accessed using the standard file protocol. The nested protocol is declared by specifying \"+ \" after the hls URI scheme name, where is either \"file\" or \"http\".\n\nUsing this protocol is discouraged - the hls demuxer should work just as well (if not, please report the issues) and is more complete. To use the hls demuxer instead, simply use the direct URLs to the m3u8 files.\n\nThis protocol accepts the following options:\n\nSome HTTP requests will be denied unless cookie values are passed in with the request. The option allows these cookies to be specified. At the very least, each cookie must specify a value along with a path and domain. HTTP requests that match both the domain and path will automatically include the cookie value in the HTTP Cookie header field. Multiple cookies can be delimited by a newline.\n\nThe required syntax to play a stream specifying a cookie is:\n\nThis protocol accepts the following options:\n\nInterPlanetary File System (IPFS) protocol support. One can access files stored on the IPFS network through so-called gateways. These are http(s) endpoints. This protocol wraps the IPFS native protocols (ipfs:// and ipns://) to be sent to such a gateway. Users can (and should) host their own node which means this protocol will use one’s local gateway to access files on the IPFS network.\n\nThis protocol accepts the following options:\n\nOne can use this protocol in 2 ways. Using IPFS:\n\nOr the IPNS protocol (IPNS is mutable IPFS):\n\nComputes the MD5 hash of the data to be written, and on close writes this to the designated output or stdout if none is specified. It can be used to test muxers without writing an actual file.\n\nNote that some formats (typically MOV) require the output protocol to be seekable, so they will fail with the MD5 output protocol.\n\nIf isn’t specified, is the number corresponding to the file descriptor of the pipe (e.g. 0 for stdin, 1 for stdout, 2 for stderr). If is not specified, by default the stdout file descriptor will be used for writing, stdin for reading.\n\nFor example to read from stdin with :\n\nFor writing to stdout with :\n\nThis protocol accepts the following options:\n\nNote that some formats (typically MOV), require the output protocol to be seekable, so they will fail with the pipe output protocol.\n\nThe Pro-MPEG CoP#3 FEC is a 2D parity-check forward error correction mechanism for MPEG-2 Transport Streams sent over RTP.\n\nThis protocol must be used in conjunction with the muxer and the protocol.\n\nThe destination UDP ports are for the column FEC stream and for the row FEC stream.\n\nThis protocol accepts the following options:\n\nThe Real-Time Messaging Protocol (RTMP) is used for streaming multimedia content across a TCP/IP network.\n\nAdditionally, the following parameters can be set via command line options (or in code via s):\n\nFor example to read with a multimedia resource named \"sample\" from the application \"vod\" from an RTMP server \"myserver\":\n\nTo publish to a password protected server, passing the playpath and app names separately:\n\nThe Encrypted Real-Time Messaging Protocol (RTMPE) is used for streaming multimedia content within standard cryptographic primitives, consisting of Diffie-Hellman key exchange and HMACSHA256, generating a pair of RC4 keys.\n\nThe Real-Time Messaging Protocol (RTMPS) is used for streaming multimedia content across an encrypted connection.\n\nThe Real-Time Messaging Protocol tunneled through HTTP (RTMPT) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Encrypted Real-Time Messaging Protocol tunneled through HTTP (RTMPTE) is used for streaming multimedia content within HTTP requests to traverse firewalls.\n\nThe Real-Time Messaging Protocol tunneled through HTTPS (RTMPTS) is used for streaming multimedia content within HTTPS requests to traverse firewalls.\n\nThis protocol accepts the following options.\n\nFor more information see: http://www.samba.org/.\n\nRead from or write to remote resources using SFTP protocol.\n\nThis protocol accepts the following options.\n\nReal-Time Messaging Protocol and its variants supported through librtmp.\n\nRequires the presence of the librtmp headers and library during configuration. You need to explicitly configure the build with \"–enable-librtmp\". If enabled this will replace the native RTMP protocol.\n\nThis protocol provides most client functions and a few server functions needed to support RTMP, RTMP tunneled in HTTP (RTMPT), encrypted RTMP (RTMPE), RTMP over SSL/TLS (RTMPS) and tunneled variants of these encrypted types (RTMPTE, RTMPTS).\n\nwhere is one of the strings \"rtmp\", \"rtmpt\", \"rtmpe\", \"rtmps\", \"rtmpte\", \"rtmpts\" corresponding to each RTMP variant, and , , and have the same meaning as specified for the RTMP native protocol. contains a list of space-separated options of the form = .\n\nSee the librtmp manual page (man 3 librtmp) for more information.\n\nFor example, to stream a file in real-time to an RTMP server using :\n\nTo play the same stream using :\n\nThe required syntax for an RTP URL is:\n\nspecifies the RTP port to use.\n\ncontains a list of &-separated options of the form = .\n\nThe following URL options are supported:\n• If is not set the RTCP port will be set to the RTP port value plus 1.\n• If (the local RTP port) is not set any available port will be used for the local RTP and RTCP ports.\n• If (the local RTCP port) is not set it will be set to the local RTP port value plus 1.\n\nRTSP is not technically a protocol handler in libavformat, it is a demuxer and muxer. The demuxer supports both normal RTSP (with data transferred over RTP; this is used by e.g. Apple and Microsoft) and Real-RTSP (with data transferred over RDT).\n\nThe muxer can be used to send a stream using RTSP ANNOUNCE to a server supporting it (currently Darwin Streaming Server and Mischa Spiegelmock’s RTSP server).\n\nThe required syntax for a RTSP url is:\n\nOptions can be set on the / command line, or set in code via s or in .\n\nThe following options are supported.\n\nThe following options are supported.\n\nWhen receiving data over UDP, the demuxer tries to reorder received packets (since they may arrive out of order, or packets may get lost totally). This can be disabled by setting the maximum demuxing delay to zero (via the field of AVFormatContext).\n\nWhen watching multi-bitrate Real-RTSP streams with , the streams to display can be chosen with and for video and audio respectively, and can be switched on the fly by pressing and .\n\nThe following examples all make use of the and tools.\n• Watch a stream over UDP, with a max reordering delay of 0.5 seconds:\n• Send a stream in realtime to a RTSP server, for others to watch:\n\nSession Announcement Protocol (RFC 2974). This is not technically a protocol handler in libavformat, it is a muxer and demuxer. It is used for signalling of RTP streams, by announcing the SDP for the streams regularly on a separate port.\n\nThe syntax for a SAP url given to the muxer is:\n\nThe RTP packets are sent to on port , or to port 5004 if no port is specified. is a -separated list. The following options are supported:\n\nTo broadcast a stream on the local subnet, for watching in VLC:\n\nAnd for watching in , over IPv6:\n\nThe syntax for a SAP url given to the demuxer is:\n\nis the multicast address to listen for announcements on, if omitted, the default 224.2.127.254 (sap.mcast.net) is used. is the port that is listened on, 9875 if omitted.\n\nThe demuxers listens for announcements on the given address and port. Once an announcement is received, it tries to receive that particular stream.\n\nTo play back the first stream announced on the normal SAP multicast address:\n\nTo play back the first stream announced on one the default IPv6 SAP multicast address:\n\nThe protocol accepts the following options:\n\nThe supported syntax for a SRT URL is:\n\ncontains a list of &-separated options of the form = .\n\nThis protocol accepts the following options.\n\nFor more information see: https://github.com/Haivision/srt.\n\nVirtually extract a segment of a file or another stream. The underlying stream must be seekable.\n\nExtract a chapter from a DVD VOB file (start and end sectors obtained externally and multiplied by 2048):\n\nWrites the output to multiple protocols. The individual outputs are separated by |\n\nThe required syntax for a TCP url is:\n\ncontains a list of &-separated options of the form = .\n\nThe list of supported options follows.\n\nThe following example shows how to setup a listening TCP connection with , which is then accessed with :\n\nThe required syntax for a TLS/SSL url is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nTo create a TLS/SSL server that serves an input stream.\n\nTo play back a stream from the TLS/SSL server using :\n\nThe required syntax for an UDP URL is:\n\ncontains a list of &-separated options of the form = .\n\nIn case threading is enabled on the system, a circular buffer is used to store the incoming data, which allows one to reduce loss of data due to UDP socket buffer overruns. The and options are related to this buffer.\n\nThe list of supported options follows.\n• Use to stream over UDP to a remote endpoint:\n• Use to stream in mpegts format over UDP using 188 sized UDP packets, using a large input buffer:\n• Use to receive over UDP from a remote endpoint:\n\nThe required syntax for a Unix socket URL is:\n\nThe following parameters can be set via command line options (or in code via s):\n\nThis library supports unicast streaming to multiple clients without relying on an external server.\n\nThe required syntax for streaming or connecting to a stream is:\n\nMultiple clients may connect to the stream using:\n\nStreaming to multiple clients is implemented using a ZeroMQ Pub-Sub pattern. The server side binds to a port and publishes data. Clients connect to the server (via IP address/port) and subscribe to the stream. The order in which the server and client start generally does not matter.\n\nffmpeg must be compiled with the –enable-libzmq option to support this protocol.\n\nOptions can be set on the / command line. The following options are supported:\n\nThe libavdevice library provides the same interface as libavformat. Namely, an input device is considered like a demuxer, and an output device like a muxer, and the interface and generic device options are the same provided by libavformat (see the ffmpeg-formats manual).\n\nIn addition each input or output device may support so-called private options, which are specific for that component.\n\nOptions may be set by specifying - in the FFmpeg tools, or by setting the value explicitly in the device options or using the API for programmatic use.\n\nInput devices are configured elements in FFmpeg which enable accessing the data coming from a multimedia device attached to your system.\n\nWhen you configure your FFmpeg build, all the supported input devices are enabled by default. You can list all available ones using the configure option \"–list-indevs\".\n\nYou can disable all the input devices using the configure option \"–disable-indevs\", and selectively enable an input device using the option \"–enable-indev= \", or you can disable a particular input device using the option \"–disable-indev= \".\n\nThe option \"-devices\" of the ff* tools will display the list of supported input devices.\n\nA description of the currently available input devices follows.\n\nTo enable this input device during configuration you need libasound installed on your system.\n\nThis device allows capturing from an ALSA device. The name of the device to capture has to be an ALSA card identifier.\n\nAn ALSA identifier has the syntax:\n\nwhere the and components are optional.\n\nThe three arguments (in order: , , ) specify card number or identifier, device number and subdevice number (-1 means any).\n\nTo see the list of cards currently recognized by your system check the files and .\n\nFor example to capture with from an ALSA device with card id 0, you may run the command:\n\nFor more information see: http://www.alsa-project.org/alsa-doc/alsa-lib/pcm.html\n\nThis input devices uses the Android Camera2 NDK API which is available on devices with API level 24+. The availability of android_camera is autodetected during configuration.\n\nThis device allows capturing from all cameras on an Android device, which are integrated into the Camera2 NDK API.\n\nThe available cameras are enumerated internally and can be selected with the parameter. The input file string is discarded.\n\nGenerally the back facing camera has index 0 while the front facing camera has index 1.\n\nAVFoundation is the currently recommended framework by Apple for streamgrabbing on OSX >= 10.7 as well as on iOS.\n\nThe input filename has to be given in the following syntax:\n\nThe first entry selects the video input while the latter selects the audio input. The stream has to be specified by the device name or the device index as shown by the device list. Alternatively, the video and/or audio input device can be chosen by index using the and/or , overriding any device name or index given in the input filename.\n\nAll available devices can be enumerated by using , listing all device names and corresponding indices.\n\nThere are two device name aliases:\n• Print the list of AVFoundation supported devices and exit:\n• Record video from video device 0 and audio from audio device 0 into out.avi:\n• Record video from video device 2 and audio from audio device 1 into out.avi:\n• Record video from the system default video device using the pixel format bgr0 and do not record any audio into out.avi:\n• Record raw DV data from a suitable input device and write the output into out.dv:\n\nBSD video input device. Deprecated and will be removed - please contact the developers if you are interested in maintaining it.\n\nThe decklink input device provides capture capabilities for Blackmagic DeckLink devices.\n\nTo enable this input device, you need the Blackmagic DeckLink SDK and you need to configure with the appropriate and . On Windows, you need to run the IDL files through .\n\nDeckLink is very picky about the formats it supports. Pixel format of the input can be set with . Framerate and video size must be determined for your device with . Audio sample rate is always 48 kHz and the number of channels can be 2, 8 or 16. Note that all audio channels are bundled in one single audio track.\n\nDirectShow support is enabled when FFmpeg is built with the mingw-w64 project. Currently only audio and video devices are supported.\n\nMultiple devices may be opened as separate inputs, but they may also be opened on the same input, which should improve synchronism between them.\n\nThe input name should be in the format:\n\nwhere can be either or , and is the device’s name or alternative name..\n\nIf no options are specified, the device’s defaults are used. If the device does not support the requested options, it will fail to open.\n• Print the list of DirectShow supported devices and exit:\n• Open second video device with name :\n• Print the list of supported options in selected device and exit:\n• Specify pin names to capture by name or alternative name, specify alternative device name:\n• Configure a crossbar device, specifying crossbar pins, allow user to adjust video capture properties at startup:\n\nThe Linux framebuffer is a graphic hardware-independent abstraction layer to show graphics on a computer monitor, typically on the console. It is accessed through a file device node, usually .\n\nFor more detailed information read the file Documentation/fb/framebuffer.txt included in the Linux source tree.\n\nSee also http://linux-fbdev.sourceforge.net/, and fbset(1).\n\nTo record from the framebuffer device with :\n\nYou can take a single screenshot image with the command:\n\nThis device allows you to capture a region of the display on Windows.\n\nAmongst options for the imput filenames are such elements as:\n\nThe first option will capture the entire desktop, or a fixed region of the desktop. The second and third options will instead capture the contents of a single window, regardless of its position on the screen.\n\nFor example, to grab the entire desktop using :\n\nGrab the contents of the window named \"Calculator\"\n\nTo enable this input device, you need libiec61883, libraw1394 and libavc1394 installed on your system. Use the configure option to compile with the device enabled.\n\nThe iec61883 capture device supports capturing from a video device connected via IEEE1394 (FireWire), using libiec61883 and the new Linux FireWire stack (juju). This is the default DV/HDV input method in Linux Kernel 2.6.37 and later, since the old FireWire stack was removed.\n\nSpecify the FireWire port to be used as input file, or \"auto\" to choose the first port connected.\n• Grab and show the input of a FireWire DV/HDV device.\n• Grab and record the input of a FireWire DV/HDV device, using a packet buffer of 100000 packets if the source is HDV.\n\nTo enable this input device during configuration you need libjack installed on your system.\n\nA JACK input device creates one or more JACK writable clients, one for each audio channel, with name :input_ , where is the name provided by the application, and is a number which identifies the channel. Each writable client will send the acquired data to the FFmpeg input device.\n\nOnce you have created one or more JACK readable clients, you need to connect them to one or more JACK writable clients.\n\nTo connect or disconnect JACK clients you can use the and programs, or do it through a graphical interface, for example with .\n\nTo list the JACK clients and their properties you can invoke the command .\n\nFollows an example which shows how to capture a JACK readable client with .\n\nCaptures the KMS scanout framebuffer associated with a specified CRTC or plane as a DRM object that can be passed to other hardware functions.\n\nRequires either DRM master or CAP_SYS_ADMIN to run.\n\nIf you don’t understand what all of that means, you probably don’t want this. Look at instead.\n• Capture from the first active plane, download the result to normal frames and encode. This will only work if the framebuffer is both linear and mappable - if not, the result may be scrambled or fail to download.\n• Capture from CRTC ID 42 at 60fps, map the result to VAAPI, convert to NV12 and encode as H.264.\n• To capture only part of a plane the output can be cropped - this can be used to capture a single window, as long as it has a known absolute position and size. For example, to capture and encode the middle quarter of a 1920x1080 plane:\n\nThis input device reads data from the open output pads of a libavfilter filtergraph.\n\nFor each filtergraph open output, the input device will create a corresponding stream which is mapped to the generated output. The filtergraph is specified through the option .\n• Create a color video stream and play it back with :\n• As the previous example, but use filename for specifying the graph description, and omit the \"out0\" label:\n• Create three different video test filtered sources and play them:\n• Read an audio stream from a file using the amovie source and play it back with :\n• Read an audio stream and a video stream and play it back with :\n• Dump decoded frames to images and Closed Captions to an RCWT backup:\n\nTo enable this input device during configuration you need libcdio installed on your system. It requires the configure option .\n\nThis device allows playing and grabbing from an Audio-CD.\n\nFor example to copy with the entire Audio-CD in , you may run the command:\n\nThe OpenAL input device provides audio capture on all systems with a working OpenAL 1.1 implementation.\n\nTo enable this input device during configuration, you need OpenAL headers and libraries installed on your system, and need to configure FFmpeg with .\n\nOpenAL headers and libraries should be provided as part of your OpenAL implementation, or as an additional download (an SDK). Depending on your installation you may need to specify additional flags via the and for allowing the build system to locate the OpenAL headers and libraries.\n\nAn incomplete list of OpenAL implementations follows:\n\nThis device allows one to capture from an audio input device handled through OpenAL.\n\nYou need to specify the name of the device to capture in the provided filename. If the empty string is provided, the device will automatically select the default device. You can get the list of the supported devices by using the option .\n\nPrint the list of OpenAL supported devices and exit:\n\nCapture from the default device (note the empty string ” as filename):\n\nCapture from two devices simultaneously, writing to two different files, within the same command:\n\nNote: not all OpenAL implementations support multiple simultaneous capture - try the latest OpenAL Soft if the above does not work.\n\nThe filename to provide to the input device is the device node representing the OSS input device, and is usually set to .\n\nFor example to grab from using use the command:\n\nFor more information about OSS see: http://manuals.opensound.com/usersguide/dsp.html\n\nTo enable this output device you need to configure FFmpeg with .\n\nThe filename to provide to the input device is a source device or the string \"default\"\n\nTo list the PulseAudio source devices and their properties you can invoke the command .\n\nMore information about PulseAudio can be found on http://www.pulseaudio.org.\n\nTo enable this input device during configuration you need libsndio installed on your system.\n\nThe filename to provide to the input device is the device node representing the sndio input device, and is usually set to .\n\nFor example to grab from using use the command:\n\n\"v4l2\" can be used as alias for \"video4linux2\".\n\nIf FFmpeg is built with v4l-utils support (by using the configure option), it is possible to use it with the input device option.\n\nThe name of the device to grab is a file device node, usually Linux systems tend to automatically create such nodes when the device (e.g. an USB webcam) is plugged into the system, and has a name of the kind , where is a number associated to the device.\n\nVideo4Linux2 devices usually support a limited set of x sizes and frame rates. You can check which are supported using for Video4Linux2 devices. Some devices, like TV cards, support one or more standards. It is possible to list all the supported standards using .\n\nThe time base for the timestamps is 1 microsecond. Depending on the kernel version and configuration, the timestamps may be derived from the real time clock (origin at the Unix Epoch) or the monotonic clock (origin usually at boot time, unaffected by NTP or manual changes to the clock). The or option can be used to force conversion into the real time clock.\n\nSome usage examples of the video4linux2 device with and :\n• Grab and show the input of a video4linux2 device:\n• Grab and record the input of a video4linux2 device, leave the frame rate and size as previously set:\n\nFor more information about Video4Linux, check http://linuxtv.org/.\n\nThe filename passed as input is the capture driver number, ranging from 0 to 9. You may use \"list\" as filename to print a list of drivers. Any other filename will be interpreted as device number 0.\n\nTo enable this input device during configuration you need libxcb installed on your system. It will be automatically detected during configuration.\n\nThis device allows one to capture a region of an X11 display.\n\nThe filename passed as input has the syntax:\n\n: . specifies the X11 display name of the screen to grab from. can be omitted, and defaults to \"localhost\". The environment variable contains the default display name.\n\nand specify the offsets of the grabbed area with respect to the top-left border of the X11 screen. They default to 0.\n\nCheck the X11 documentation (e.g. ) for more detailed information.\n\nUse the program for getting basic information about the properties of your X11 display (e.g. grep for \"name\" or \"dimensions\").\n\nFor example to grab from using :\n\nOutput devices are configured elements in FFmpeg that can write multimedia data to an output device attached to your system.\n\nWhen you configure your FFmpeg build, all the supported output devices are enabled by default. You can list all available ones using the configure option \"–list-outdevs\".\n\nYou can disable all the output devices using the configure option \"–disable-outdevs\", and selectively enable an output device using the option \"–enable-outdev= \", or you can disable a particular input device using the option \"–disable-outdev= \".\n\nThe option \"-devices\" of the ff* tools will display the list of enabled output devices.\n\nA description of the currently available output devices follows.\n\nAllows native output to CoreAudio devices on OSX.\n\nThe output filename can be empty (or ) to refer to the default system output device or a number that refers to the device index as shown using: .\n\nAlternatively, the audio input device can be chosen by index using the , overriding any device name or index given in the input filename.\n\nAll available devices can be enumerated by using , listing all device names, UIDs and corresponding indices.\n• Print the list of supported devices and output a sine wave to the default device:\n• Output a sine wave to the device with the index 2, overriding any output filename:\n\nThis output device allows one to show a video stream in CACA window. Only one CACA window is allowed per application, so you can have only one instance of this output device in an application.\n\nTo enable this output device you need to configure FFmpeg with . libcaca is a graphics library that outputs text instead of pixels.\n\nFor more information about libcaca, check: http://caca.zoy.org/wiki/libcaca\n• The following command shows the output is an CACA window, forcing its size to 80x25:\n• Show the list of available drivers and exit:\n• Show the list of available dither colors and exit:\n\nThe decklink output device provides playback capabilities for Blackmagic DeckLink devices.\n\nTo enable this output device, you need the Blackmagic DeckLink SDK and you need to configure with the appropriate and . On Windows, you need to run the IDL files through .\n\nDeckLink is very picky about the formats it supports. Pixel format is always uyvy422, framerate, field order and video size must be determined for your device with . Audio sample rate is always 48 kHz.\n\nThe Linux framebuffer is a graphic hardware-independent abstraction layer to show graphics on a computer monitor, typically on the console. It is accessed through a file device node, usually .\n\nFor more detailed information read the file included in the Linux source tree.\n\nSee also http://linux-fbdev.sourceforge.net/, and fbset(1).\n\nOpenGL output device. Deprecated and will be removed.\n\nTo enable this output device you need to configure FFmpeg with .\n\nThis output device allows one to render to OpenGL context. Context may be provided by application or default SDL window is created.\n\nWhen device renders to external context, application must implement handlers for following messages: - create OpenGL context on current thread. - make OpenGL context current. - swap buffers. - destroy OpenGL context. Application is also required to inform a device about current resolution by sending message.\n\nTo enable this output device you need to configure FFmpeg with .\n\nMore information about PulseAudio can be found on http://www.pulseaudio.org\n\nSDL (Simple DirectMedia Layer) output device. Deprecated and will be removed.\n\nFor monitoring purposes in FFmpeg, pipes and a video player such as ffplay can be used:\n\n\"sdl2\" can be used as alias for \"sdl\".\n\nThis output device allows one to show a video stream in an SDL window. Only one SDL window is allowed per application, so you can have only one instance of this output device in an application.\n\nTo enable this output device you need libsdl installed on your system when configuring your build.\n\nFor more information about SDL, check: http://www.libsdl.org/\n\nThe window created by the device can be controlled through the following interactive commands.\n\nThe following command shows the output is an SDL window, forcing its size to the qcif format:\n\nThis output device allows one to show a video stream in a X Window System window.\n\nFor more information about XVideo see http://www.x.org/.\n• Decode, display and encode video input with at the same time:\n• Decode and display the input video to multiple X11 windows:\n\nThe audio resampler supports the following named options.\n\nOptions may be set by specifying - in the FFmpeg tools, = for the aresample filter, by setting the value explicitly in the options or using the API for programmatic use.\n\nThe video scaler supports the following named options.\n\nOptions may be set by specifying - in the FFmpeg tools, with a few API-only exceptions noted below. For programmatic use, they can be set explicitly in the options or through the API.\n\nFiltering in FFmpeg is enabled through the libavfilter library.\n\nIn libavfilter, a filter can have multiple inputs and multiple outputs. To illustrate the sorts of things that are possible, we consider the following filtergraph.\n\nThis filtergraph splits the input stream in two streams, then sends one stream through the crop filter and the vflip filter, before merging it back with the other stream by overlaying it on top. You can use the following command to achieve this:\n\nThe result will be that the top half of the video is mirrored onto the bottom half of the output video.\n\nFilters in the same linear chain are separated by commas, and distinct linear chains of filters are separated by semicolons. In our example, are in one linear chain, and are separately in another. The points where the linear chains join are labelled by names enclosed in square brackets. In the example, the split filter generates two outputs that are associated to the labels and .\n\nThe stream sent to the second output of , labelled as , is processed through the filter, which crops away the lower half part of the video, and then vertically flipped. The filter takes in input the first unchanged output of the split filter (which was labelled as ), and overlay on its lower half the output generated by the filterchain.\n\nSome filters take in input a list of parameters: they are specified after the filter name and an equal sign, and are separated from each other by a colon.\n\nThere exist so-called that do not have an audio/video input, and that will not have audio/video output.\n\nThe program included in the FFmpeg directory can be used to parse a filtergraph description and issue a corresponding textual representation in the dot language.\n\nto see how to use .\n\nYou can then pass the dot description to the program (from the graphviz suite of programs) and obtain a graphical representation of the filtergraph.\n\nFor example the sequence of commands:\n\ncan be used to create and display an image representing the graph described by the string. Note that this string must be a complete self-contained graph, with its inputs and outputs explicitly defined. For example if your command line is of the form:\n\nyour string will need to be of the form:\n\nyou may also need to set the parameters and add a filter in order to simulate a specific input file.\n\nA filtergraph is a directed graph of connected filters. It can contain cycles, and there can be multiple links between a pair of filters. Each link has one input pad on one side connecting it to one filter from which it takes its input, and one output pad on the other side connecting it to one filter accepting its output.\n\nEach filter in a filtergraph is an instance of a filter class registered in the application, which defines the features and the number of input and output pads of the filter.\n\nA filter with no input pads is called a \"source\", and a filter with no output pads is called a \"sink\".\n\nA filtergraph has a textual representation, which is recognized by the / / and options in and / in , and by the function defined in .\n\nA filterchain consists of a sequence of connected filters, each one connected to the previous one in the sequence. A filterchain is represented by a list of \",\"-separated filter descriptions.\n\nA filtergraph consists of a sequence of filterchains. A sequence of filterchains is represented by a list of \";\"-separated filterchain descriptions.\n\nA filter is represented by a string of the form: [ ]...[ ] @ = [ ]...[ ]\n\nis the name of the filter class of which the described filter is an instance of, and has to be the name of one of the filter classes registered in the program optionally followed by \"@ \". The name of the filter class is optionally followed by a string \"= \".\n\nis a string which contains the parameters used to initialize the filter instance. It may have one of two forms:\n• A ’:’-separated list of . In this case, the keys are assumed to be the option names in the order they are declared. E.g. the filter declares three options in this order – , and . Then the parameter list means that the value is assigned to the option , to and to .\n• A ’:’-separated list of mixed direct and long pairs. The direct must precede the pairs, and follow the same constraints order of the previous point. The following pairs can be set in any preferred order.\n\nIf the option value itself is a list of items (e.g. the filter takes a list of pixel formats), the items in the list are usually separated by ‘ ’.\n\nThe list of arguments can be quoted using the character ‘ ’ as initial and ending mark, and the character ‘ ’ for escaping the characters within the quoted text; otherwise the argument string is considered terminated when the next special character (belonging to the set ‘ ’) is encountered.\n\nA special syntax implemented in the CLI tool allows loading option values from files. This is done be prepending a slash ’/’ to the option name, then the supplied value is interpreted as a path from which the actual value is loaded. E.g.\n\nwill load the text to be drawn from . API users wishing to implement a similar feature should use the functions together with custom IO code.\n\nThe name and arguments of the filter are optionally preceded and followed by a list of link labels. A link label allows one to name a link and associate it to a filter output or input pad. The preceding labels ... , are associated to the filter input pads, the following labels ... , are associated to the output pads.\n\nWhen two link labels with the same name are found in the filtergraph, a link between the corresponding input and output pad is created.\n\nIf an output pad is not labelled, it is linked by default to the first unlabelled input pad of the next filter in the filterchain. For example in the filterchain\n\nthe split filter instance has two output pads, and the overlay filter instance two input pads. The first output pad of split is labelled \"L1\", the first input pad of overlay is labelled \"L2\", and the second output pad of split is linked to the second input pad of overlay, which are both unlabelled.\n\nIn a filter description, if the input label of the first filter is not specified, \"in\" is assumed; if the output label of the last filter is not specified, \"out\" is assumed.\n\nIn a complete filterchain all the unlabelled filter input and output pads must be connected. A filtergraph is considered valid if all the filter input and output pads of all the filterchains are connected.\n\nLeading and trailing whitespaces (space, tabs, or line feeds) separating tokens in the filtergraph specification are ignored. This means that the filtergraph can be expressed using empty lines and spaces to improve redability.\n\nFor example, the filtergraph:\n\ncan be represented as:\n\nLibavfilter will automatically insert scale filters where format conversion is required. It is possible to specify swscale flags for those automatically inserted scalers by prepending to the filtergraph description.\n\nHere is a BNF description of the filtergraph syntax:\n\nFiltergraph description composition entails several levels of escaping. See (ffmpeg-utils)the \"Quoting and escaping\" section in the ffmpeg-utils(1) manual for more information about the employed escaping procedure.\n\nA first level escaping affects the content of each filter option value, which may contain the special character used to separate values, or one of the escaping characters .\n\nA second level escaping affects the whole filter description, which may contain the escaping characters or the special characters used by the filtergraph description.\n\nFinally, when you specify a filtergraph on a shell commandline, you need to perform a third level escaping for the shell special characters contained within it.\n\nFor example, consider the following string to be embedded in the drawtext filter description value:\n\nThis string contains the special escaping character, and the special character, so it needs to be escaped in this way:\n\nA second level of escaping is required when embedding the filter description in a filtergraph description, in order to escape all the filtergraph special characters. Thus the example above becomes:\n\n(note that in addition to the escaping special characters, also needs to be escaped).\n\nFinally an additional level of escaping is needed when writing the filtergraph description in a shell command, which depends on the escaping rules of the adopted shell. For example, assuming that is special and needs to be escaped with another , the previous string will finally result in:\n\nIn order to avoid cumbersome escaping when using a commandline tool accepting a filter specification as input, it is advisable to avoid direct inclusion of the filter or options specification in the shell.\n\nFor example, in case of the drawtext filter, you might prefer to use the option in place of to specify the text to render.\n\nSome filters support a generic option. For the filters supporting timeline editing, this option can be set to an expression which is evaluated before sending a frame to the filter. If the evaluation is non-zero, the filter will be enabled, otherwise the frame will be sent unchanged to the next filter in the filtergraph.\n\nThe expression accepts the following values:\n\nAdditionally, these filters support an command that can be used to re-define the expression.\n\nLike any other filtering option, the option follows the same rules.\n\nFor example, to enable a blur filter (smartblur) from 10 seconds to 3 minutes, and a curves filter starting at 3 seconds:\n\nSee to view which filters have timeline support.\n\nSome options can be changed during the operation of the filter using a command. These options are marked ’T’ on the output of . The name of the command is the name of the option and the argument is the new value.\n\n35 Options for filters with several inputs (framesync)\n\nSome filters with several inputs support a common set of options. These options can only be set by name, not with the short notation.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the audio filters included in your build.\n\nBelow is a description of the currently available audio filters.\n\nApply Affine Projection algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to estimate unknown audio based on multiple input audio samples. Affine projection algorithm can make trade-offs between computation complexity with convergence speed.\n\nA description of the accepted options follows.\n\nA compressor is mainly used to reduce the dynamic range of a signal. Especially modern music is mostly compressed at a high ratio to improve the overall loudness. It’s done to get the highest attention of a listener, \"fatten\" the sound and bring more \"power\" to the track. If a signal is compressed too much it may sound dull or \"dead\" afterwards or it may start to \"pump\" (which could be a powerful effect but can also destroy a track completely). The right compression is the key to reach a professional sound and is the high art of mixing and mastering. Because of its complex settings it may take a long time to get the right feeling for this kind of effect.\n\nCompression is done by detecting the volume above a chosen level and dividing it by the factor set with . So if you set the threshold to -12dB and your signal reaches -6dB a ratio of 2:1 will result in a signal at -9dB. Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over the time. This is done by setting \"Attack\" and \"Release\". determines how long the signal has to rise above the threshold before any reduction will occur and sets the time the signal has to fall below the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched. The overall reduction of the signal can be made up afterwards with the setting. So compressing the peaks of a signal about 6dB and raising the makeup to this level results in a signal twice as loud than the source. To gain a softer entry in the compression the flattens the hard edge at the threshold in the range of the chosen decibels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nCopy the input audio source unchanged to the output. This is mainly useful for testing purposes.\n\nApply cross fade from one input audio stream to another input audio stream. The cross fade is applied for specified duration near the end of first stream.\n\nThe filter accepts the following options:\n• Cross fade from one input to another:\n• Cross fade from one input to another but without overlapping:\n\nThis filter splits audio stream into two or more frequency ranges. Summing all streams back will give flat output.\n\nThe filter accepts the following options:\n• Split input audio stream into two bands (low and high) with split frequency of 1500 Hz, each band will be in separate stream:\n• Same as above, but with higher filter order:\n• Same as above, but also with additional middle band (frequencies between 1500 and 8000):\n\nThis filter is bit crusher with enhanced functionality. A bit crusher is used to audibly reduce number of bits an audio signal is sampled with. This doesn’t change the bit depth at all, it just produces the effect. Material reduced in bit depth sounds more harsh and \"digital\". This filter is able to even round to continuous values instead of discrete bit depths. Additionally it has a D/C offset which results in different crushing of the lower and the upper half of the signal. An Anti-Aliasing setting is able to produce \"softer\" crushing sounds.\n\nAnother feature of this filter is the logarithmic mode. This setting switches from linear distances between bits to logarithmic ones. The result is a much more \"natural\" sounding crusher which doesn’t gate low signals for example. The human ear has a logarithmic perception, so this kind of crushing is much more pleasant. Logarithmic crushing is also able to get anti-aliased.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDelay audio filtering until a given wallclock timestamp. See the cue filter.\n\nSamples detected as impulsive noise are replaced by interpolated samples using autoregressive modelling.\n\nSamples detected as clipped are replaced by interpolated samples using autoregressive modelling.\n\nThe filter accepts the following options:\n\nDelay one or more audio channels.\n\nSamples in delayed channel are filled with silence.\n\nThe filter accepts the following option:\n• Delay first channel by 1.5 seconds, the third channel by 0.5 seconds and leave the second channel (and any other channels that may be present) unchanged.\n• Delay second channel by 500 samples, the third channel by 700 samples and leave the first channel (and any other channels that may be present) unchanged.\n• Delay all channels by same number of samples:\n\nThis filter shall be placed before any filter that can produce denormals.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nApplying both filters one after another produces original audio.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n• Apply spectral compression to all frequencies with threshold of -50 dB and 1:6 ratio:\n• Similar to above but with 1:2 ratio and filtering only front center channel:\n• Apply spectral noise gate to all frequencies with threshold of -85 dB and with short attack time and short release time:\n• Apply spectral expansion to all frequencies with threshold of -10 dB and 1:2 ratio:\n• Apply limiter to max -60 dB to all frequencies, with attack of 2 ms and release of 10 ms:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nEchoes are reflected sound and can occur naturally amongst mountains (and sometimes large buildings) when talking or shouting; digital echo effects emulate this behaviour and are often used to help fill out the sound of a single instrument or vocal. The time difference between the original signal and the reflection is the , and the loudness of the reflected signal is the . Multiple echoes can have different delays and decays.\n\nA description of the accepted parameters follows.\n• Make it sound as if there are twice as many instruments as are actually playing:\n• If delay is very short, then it sounds like a (metallic) robot playing music:\n• A longer delay will sound like an open air concert in the mountains:\n• Same as above but with one more mountain:\n\nAudio emphasis filter creates or restores material directly taken from LPs or emphased CDs with different filter curves. E.g. to store music on vinyl the signal has to be altered by a filter first to even out the disadvantages of this recording medium. Once the material is played back the inverse filter has to be applied to restore the distortion of the frequency response.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nModify an audio signal according to the specified expressions.\n\nThis filter accepts one or more expressions (one for each channel), which are evaluated and used to modify a corresponding audio signal.\n\nIt accepts the following parameters:\n\nEach expression in can contain the following constants and functions:\n\nNote: this filter is slow. For faster processing you should use a dedicated filter.\n• Invert phase of the second channel:\n\nAn exciter is used to produce high sound that is not present in the original signal. This is done by creating harmonic distortions of the signal which are restricted in range and added to the original signal. An Exciter raises the upper end of an audio signal without simply raising the higher frequencies like an equalizer would do to create a more \"crisp\" or \"brilliant\" sound.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n• Fade in first 15 seconds of audio:\n• Fade out last 25 seconds of a 900 seconds audio:\n\nA description of the accepted parameters follows.\n\nThis filter supports the some above mentioned options as commands.\n• Reduce white noise by 10dB, and use previously measured noise floor of -40dB:\n• Reduce white noise by 10dB, also set initial noise floor to -80dB and enable automatic tracking of noise floor so noise floor will gradually change during processing:\n• Reduce noise by 20dB, using noise floor of -40dB and using commands to take noise profile of first 0.4 seconds of input audio:\n• Leave almost only low frequencies in audio:\n\nThis filter is designed for applying long FIR filters, up to 60 seconds long.\n\nIt can be used as component for digital crossover filters, room equalization, cross talk cancellation, wavefield synthesis, auralization, ambiophonics, ambisonics and spatialization.\n\nThis filter uses the streams higher than first one as FIR coefficients. If the non-first stream holds a single channel, it will be used for all input channels in the first stream, otherwise the number of channels in the non-first stream must be same as the number of channels in the first stream.\n\nIt accepts the following parameters:\n• Apply reverb to stream using mono IR file as second input, complete command using ffmpeg:\n• Apply true stereo processing given input stereo stream, and two stereo impulse responses for left and right channel, the impulse response files are files with names l_ir.wav and r_ir.wav, and setting irnorm option value:\n• Similar to above example, but with explicitly set to estimated value and with disabled:\n\nSet output format constraints for the input audio. The framework will negotiate the most appropriate format to minimize conversions.\n\nIt accepts the following parameters:\n\nIf a parameter is omitted, all values are allowed.\n\nForce the output to either unsigned 8-bit or signed 16-bit stereo\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nA gate is mainly used to reduce lower parts of a signal. This kind of signal processing reduces disturbing noise between useful signals.\n\nGating is done by detecting the volume below a chosen level and dividing it by the factor set with . The bottom of the noise floor is set via . Because an exact manipulation of the signal would cause distortion of the waveform the reduction can be levelled over time. This is done by setting and .\n\ndetermines how long the signal has to fall below the threshold before any reduction will occur and sets the time the signal has to rise above the threshold to reduce the reduction again. Shorter signals than the chosen attack time will be left untouched.\n\nThis filter supports the all above options as commands.\n\nIt accepts the following parameters:\n\nCoefficients in and format are separated by spaces and are in ascending order.\n\nCoefficients in format are separated by spaces and order of coefficients doesn’t matter. Coefficients in format are complex numbers with imaginary unit.\n\nDifferent coefficients and gains can be provided for every channel, in such case use ’|’ to separate coefficients or gains. Last provided coefficients will be used for all remaining channels.\n• Apply 2 pole elliptic notch at around 5000Hz for 48000 Hz sample rate:\n• Same as above but in format:\n\nThe limiter prevents an input signal from rising over a desired threshold. This limiter uses lookahead technology to prevent your signal from distorting. It means that there is a small delay after the signal is processed. Keep in mind that the delay it produces is the attack time you set.\n\nThe filter accepts the following options:\n\nDepending on picked setting it is recommended to upsample input 2x or 4x times with aresample before applying this filter.\n\nApply a two-pole all-pass filter with central frequency (in Hz) , and filter-width . An all-pass filter changes the audio’s frequency to phase relationship without changing its frequency to amplitude relationship.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nMerge two or more audio streams into a single multi-channel stream.\n\nThe filter accepts the following options:\n\nIf the channel layouts of the inputs are disjoint, and therefore compatible, the channel layout of the output will be set accordingly and the channels will be reordered as necessary. If the channel layouts of the inputs are not disjoint, the output will have all the channels of the first input then all the channels of the second input, in that order, and the channel layout of the output will be the default value corresponding to the total number of channels.\n\nFor example, if the first input is in 2.1 (FL+FR+LF) and the second input is FC+BL+BR, then the output will be in 5.1, with the channels in the following order: a1, a2, b1, a3, b2, b3 (a1 is the first channel of the first input, b1 is the first channel of the second input).\n\nOn the other hand, if both input are in stereo, the output channels will be in the default order: a1, a2, b1, b2, and the channel layout will be arbitrarily set to 4.0, which may or may not be the expected value.\n\nAll inputs must have the same sample rate, and format.\n\nIf inputs do not have the same duration, the output will stop with the shortest.\n\nNote that this filter only supports float samples (the and audio filters support many formats). If the input has integer samples then aresample will be automatically inserted to perform the conversion to float samples.\n\nIt accepts the following parameters:\n• This will mix 3 input audio streams to a single output with the same duration as the first input and a dropout transition time of 3 seconds:\n• This will mix one vocal and one music input audio stream to a single output with the same duration as the longest input. The music will have quarter the weight as the vocals, and the inputs are not normalized:\n\nThis filter supports the following commands:\n\nMultiply first audio stream with second audio stream and store result in output audio stream. Multiplication is done by multiplying each sample from first stream with sample at same position from second stream.\n\nWith this element-wise multiplication one can create amplitude fades and amplitude modulations.\n\nIt accepts the following parameters:\n• Lower gain by 10 of central frequency 200Hz and width 100 Hz for first 2 channels using Chebyshev type 1 filter:\n\nThis filter supports the following commands:\n\nEach sample is adjusted by looking for other samples with similar contexts. This context similarity is defined by comparing their surrounding patches of size . Patches are searched in an area of around the sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply Normalized Least-Mean-(Squares|Fourth) algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean square of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n• One of many usages of this filter is noise reduction, input audio is filtered with same samples that are delayed by fixed amount, one such example for stereo audio is:\n\nThis filter supports the same commands as options, excluding option .\n\nPass the audio source unchanged to the output.\n\nPad the end of an audio stream with silence.\n\nThis can be used together with to extend audio streams to the same length as the video stream.\n\nA description of the accepted options follows.\n\nIf neither the nor the nor nor option is set, the filter will add silence to the end of the input stream indefinitely.\n\nNote that for ffmpeg 4.4 and earlier a zero or also caused the filter to add silence indefinitely.\n• Add 1024 samples of silence to the end of the input:\n• Make sure the audio output will contain at least 10000 samples, pad the input with silence if required:\n• Use to pad the audio input with silence, so that the video stream will always result the shortest and will be converted until the end in the output file when using the option:\n\nA phaser filter creates series of peaks and troughs in the frequency spectrum. The position of the peaks and troughs are modulated so that they vary over time, creating a sweeping effect.\n\nA description of the accepted parameters follows.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAudio pulsator is something between an autopanner and a tremolo. But it can produce funny stereo effects as well. Pulsator changes the volume of the left and right channel based on a LFO (low frequency oscillator) with different waveforms and shifted phases. This filter have the ability to define an offset between left and right channel. An offset of 0 means that both LFO shapes match each other. The left and right channel are altered equally - a conventional tremolo. An offset of 50% means that the shape of the right channel is exactly shifted in phase (or moved backwards about half of the frequency) - pulsator acts as an autopanner. At 1 both curves match again. Every setting in between moves the phase shift gapless between all stages and produces some \"bypassing\" sounds with sine and triangle waveforms. The more you set the offset near 1 (starting from the 0.5) the faster the signal passes from the left to the right speaker.\n\nThe filter accepts the following options:\n\nResample the input audio to the specified parameters, using the libswresample library. If none are specified then the filter will automatically convert between its input and output.\n\nThis filter is also able to stretch/squeeze the audio data to make it match the timestamps or to inject silence / cut out audio to make it match the timestamps, do a combination of both or do neither.\n\nThe filter accepts the syntax [ :] , where expresses a sample rate and is a list of = pairs, separated by \":\". See the (ffmpeg-resampler)\"Resampler Options\" section in the ffmpeg-resampler(1) manual for the complete list of supported options.\n• Stretch/squeeze samples to the given timestamps, with a maximum of 1000 samples per second compensation:\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nApply Recursive Least Squares algorithm to the first audio stream using the second audio stream.\n\nThis adaptive filter is used to mimic a desired filter by recursively finding the filter coefficients that relate to producing the minimal weighted linear least squares cost function of the error signal (difference between the desired, 2nd input audio stream and the actual signal, the 1st input audio stream).\n\nA description of the accepted options follows.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSet the number of samples per each output audio frame.\n\nThe last output packet may contain a different number of samples, as the filter will flush all the remaining samples when the input audio signals its end.\n\nThe filter accepts the following options:\n\nFor example, to set the number of per-frame samples to 1234 and disable padding for the last frame, use:\n\nSet the sample rate without altering the PCM data. This will result in a change of speed and pitch.\n\nThe filter accepts the following options:\n\nShow a line containing various information for each input audio frame. The input audio is not modified.\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nThis filter takes two audio streams for input, and outputs first audio stream. Results are in dB per channel at end of either input.\n\nSoft clipping is a type of distortion effect where the amplitude of a signal is saturated along a smooth curve, rather than the abrupt shape of hard-clipping.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay frequency domain statistical information about the audio channels. Statistics are calculated and stored as metadata for each audio channel and for each audio frame.\n\nIt accepts the following option:\n\nA list of each metadata key follows:\n\nThis filter uses PocketSphinx for speech recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized speech as the frame metadata .\n\nDisplay time domain statistical information about the audio channels. Statistics are calculated and displayed for each audio channel and, where applicable, an overall figure is also given.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter allows to set custom, steeper roll off than highpass filter, and thus is able to more attenuate frequency content in stop-band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts exactly one parameter, the audio tempo. If not specified then the filter will assume nominal 1.0 tempo. Tempo must be in the [0.5, 100.0] range.\n\nNote that tempo greater than 2 will skip some samples rather than blend them in. If for any reason this is a concern it is always possible to daisy-chain several instances of atempo to achieve the desired product tempo.\n• To speed up audio to 300% tempo:\n• To speed up audio to 300% tempo by daisy-chaining two atempo instances:\n\nThis filter supports the following commands:\n\nThis filter apply any spectral roll-off slope over any specified frequency band.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _sample options simply count the samples that pass through the filter. So start/end_pts and start/end_sample will give different results when the timestamps are wrong, inexact or do not start at zero. Also note that this filter does not modify the timestamps. If you wish to have the output timestamps start at zero, insert the asetpts filter after the atrim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all samples that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple atrim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first 1000 samples:\n\nResulted samples are always between -1 and 1 inclusive. If result is 1 it means two input samples are highly correlated in that selected segment. Result 0 means they are not correlated at all. If result is -1 it means two input samples are out of phase, which means they cancel each other.\n\nThe filter accepts the following options:\n\nApply a two-pole Butterworth band-pass filter with central frequency , and (3dB-point) band-width width. The option selects a constant skirt gain (peak gain = Q) instead of the default: constant 0dB peak gain. The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a two-pole Butterworth band-reject filter with central frequency , and (3dB-point) band-width . The filter roll off at 6dB per octave (20dB per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nBoost or cut the bass (lower) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nApply a biquad IIR filter with the given coefficients. Where , , and , , are the numerator and denominator coefficients respectively. and , specify which channels to filter, by default all available are filtered.\n\nThis filter supports the following commands:\n\nBauer stereo to binaural transformation, which improves headphone listening of stereo audio records.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nIf no mapping is present, the filter will implicitly map input channels to output channels, preserving indices.\n• For example, assuming a 5.1+downmix input MOV file, will create an output WAV file tagged as stereo from the downmix channels of the input.\n\nSplit each channel from an input audio stream into a separate output stream.\n\nIt accepts the following parameters:\n• For example, assuming a stereo input MP3 file, will create an output Matroska file with two audio streams, one containing only the left channel and the other the right channel.\n\nCan make a single vocal sound like a chorus, but can also be applied to instrumentation.\n\nChorus resembles an echo effect with a short delay, but whereas with echo the delay is constant, with chorus, it is varied using using sinusoidal or triangular modulation. The modulation depth defines the range the modulated delay is played before or after the delay. Hence the delayed sound will sound slower or faster, that is the delayed sound tuned around the original one, like in a chorus where some vocals are slightly off key.\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n• Make music with both quiet and loud passages suitable for listening to in a noisy environment: Another example for audio with whisper and explosion parts:\n• A noise gate for when the noise is at a lower level than the signal:\n• Here is another noise gate, this time for when the noise is at a higher level than the signal (making it, in some ways, similar to squelch):\n\nCompensation Delay Line is a metric based delay to compensate differing positions of microphones or speakers.\n\nFor example, you have recorded guitar with two microphones placed in different locations. Because the front of sound wave has fixed speed in normal conditions, the phasing of microphones can vary and depends on their location and interposition. The best sound mix can be achieved when these microphones are in phase (synchronized). Note that a distance of ~30 cm between microphones makes one microphone capture the signal in antiphase to the other microphone. That makes the final mix sound moody. This filter helps to solve phasing problems by adding different delays to each microphone track and make them synchronized.\n\nThe best result can be reached when you take one track as base and synchronize other tracks one by one with it. Remember that synchronization/delay tolerance depends on sample rate, too. Higher sample rates will give more tolerance.\n\nThe filter accepts the following parameters:\n\nThis filter supports the all above options as commands.\n\nCrossfeed is the process of blending the left and right channels of stereo audio recording. It is mainly used to reduce extreme stereo separation of low frequencies.\n\nThe intent is to produce more speaker like sound to the listener.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter linearly increases differences between each audio sample.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis can be useful to remove a DC offset (caused perhaps by a hardware problem in the recording chain) from the audio. The effect of a DC offset is reduced headroom and hence volume. The astats filter can be used to determine if a signal has a DC offset.\n\nThis filter accepts stereo input and produce surround (3.0) channels output. The newly produced front center channel have enhanced speech dialogue originally available in both stereo channels. This filter outputs front left and front right channels same as available in stereo input.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDR values of 14 and higher is found in very dynamic material. DR of 8 to 13 is found in transition material. And anything less that 8 have very poor dynamics and is very compressed.\n\nThe filter accepts the following options:\n\nThis filter applies a certain amount of gain to the input audio in order to bring its peak magnitude to a target level (e.g. 0 dBFS). However, in contrast to more \"simple\" normalization algorithms, the Dynamic Audio Normalizer *dynamically* re-adjusts the gain factor to the input audio. This allows for applying extra gain to the \"quiet\" sections of the audio while avoiding distortions or clipping the \"loud\" sections. In other words: The Dynamic Audio Normalizer will \"even out\" the volume of quiet and loud sections, in the sense that the volume of each section is brought to the same target level. Note, however, that the Dynamic Audio Normalizer achieves this goal *without* applying \"dynamic range compressing\". It will retain 100% of the dynamic range *within* each section of the audio file.\n\nThis filter supports the all above options as commands.\n\nMake audio easier to listen to on headphones.\n\nThis filter adds ‘cues’ to 44.1kHz stereo (i.e. audio CD format) audio so that when listened to on headphones the stereo image is moved from inside your head (standard for headphones) to outside and in front of the listener (standard for speakers).\n\nApply a two-pole peaking equalisation (EQ) filter. With this filter, the signal-level at and around a selected frequency can be increased or decreased, whilst (unlike bandpass and bandreject filters) that at all other frequencies is unchanged.\n\nIn order to produce complex equalisation curves, this filter can be given several times, each with a different central frequency.\n\nThe filter accepts the following options:\n• Attenuate 10 dB at 1000 Hz, with a bandwidth of 200 Hz:\n• Apply 2 dB gain at 1000 Hz with Q 1 and attenuate 5 dB at 100 Hz with Q 2:\n\nThis filter supports the following commands:\n\nLinearly increases the difference between left and right channels which adds some sort of \"live\" effect to playback.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n• higher delay with zero phase to compensate delay:\n• lowpass on left channel, highpass on right channel:\n\nThe filter accepts the following options:\n\nNote that this makes most sense to apply on mono signals. With this filter applied to mono signals it give some directionality and stretches its stereo image.\n\nThe filter accepts the following options:\n\nDecodes High Definition Compatible Digital (HDCD) data. A 16-bit PCM stream with embedded HDCD codes is expanded into a 20-bit PCM stream.\n\nThe filter supports the Peak Extend and Low-level Gain Adjustment features of HDCD, and detects the Transient Filter flag.\n\nWhen using the filter with wav, note the default encoding for wav is 16-bit, so the resulting 20-bit stream will be truncated back to 16-bit. Use something like after the filter to get 24-bit PCM output.\n\nThe filter accepts the following options:\n\nApply head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones. The HRIRs are provided via additional streams, for each channel one stereo input stream is needed.\n\nThe filter accepts the following options:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, each amovie filter use stereo file with IR coefficients as input. The files give coefficients for each position of virtual loudspeaker:\n• Full example using wav files as coefficients with amovie filters for 7.1 downmix, but now in format.\n\nApply a high-pass filter with 3dB point frequency. The filter can be either single-pole, or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nIt accepts the following parameters:\n\nThe filter will attempt to guess the mappings when they are not specified explicitly. It does so by first trying to find an unused matching input channel and if that fails it picks the first unused input channel.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• List all available plugins within amp (LADSPA example plugin) library:\n• List all available controls and their valid ranges for plugin from library:\n• Add reverberation to the audio using TAP-plugins (Tom’s Audio Processing plugins):\n• Generate 20 bpm clicks using plugin from the (CAPS) library:\n• Increase volume by 20dB using fast lookahead limiter from Steve Harris collection:\n• Reduce stereo image using from the (CAPS) library:\n• Another white noise, now using (CAPS) library:\n\nThis filter supports the following commands:\n\nEBU R128 loudness normalization. Includes both dynamic and linear normalization modes. Support for both single pass (livestreams, files) and double pass (files) modes. This algorithm can target IL, LRA, and maximum true peak. In dynamic mode, to accurately detect true peaks, the audio stream will be upsampled to 192 kHz. Use the option or filter to explicitly set an output sample rate.\n\nThe filter accepts the following options:\n\nApply a low-pass filter with 3dB point frequency. The filter can be either single-pole or double-pole (the default). The filter roll off at 6dB per pole per octave (20dB per pole per decade).\n\nThe filter accepts the following options:\n• Lowpass only LFE channel, it LFE is not present it does nothing:\n\nThis filter supports the following commands:\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter supports all options that are exported by plugin as commands.\n\nThe input audio is divided into bands using 4th order Linkwitz-Riley IIRs. This is akin to the crossover of a loudspeaker, and results in flat frequency response when absent compander action.\n\nIt accepts the following parameters:\n\nMix channels with specific gain levels. The filter accepts the output channel layout followed by a set of channels definitions.\n\nThis filter is also designed to efficiently remap the channels of an audio stream.\n\nThe filter accepts parameters of the form: \" | | |...\"\n\nIf the ‘=’ in a channel specification is replaced by ‘<’, then the gains for that specification will be renormalized so that the total is 1, thus avoiding clipping noise.\n\nFor example, if you want to down-mix from stereo to mono, but with a bigger factor for the left channel:\n\nA customized down-mix to stereo that works automatically for 3-, 4-, 5- and 7-channels surround:\n\nNote that integrates a default down-mix (and up-mix) system that should be preferred (see \"-ac\" option) unless you have very specific needs.\n\nThe channel remapping will be effective if, and only if:\n• gain coefficients are zeroes or ones,\n• only one input per channel output,\n\nIf all these conditions are satisfied, the filter will notify the user (\"Pure channel mapping detected\"), and use an optimized and lossless method to do the remapping.\n\nFor example, if you have a 5.1 source and want a stereo audio stream by dropping the extra channels:\n\nGiven the same source, you can also switch front left and front right channels and keep the input channel layout:\n\nIf the input is a stereo audio stream, you can mute the front left channel (and still keep the stereo channel layout) with:\n\nStill with a stereo audio stream input, you can copy the right channel in both front left and right:\n\nReplayGain scanner filter. This filter takes an audio stream as an input and outputs it unchanged. At end of filtering it displays and .\n\nThe filter accepts the following exported read-only options:\n\nConvert the audio sample format, sample rate and channel layout. It is not meant to be used directly.\n\nTo enable compilation of this filter, you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter acts like normal compressor but has the ability to compress detected signal using second input signal. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal. The filtered signal then can be filtered with other filters in later stages of processing. See pan and amerge filter.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Full ffmpeg example taking 2 audio inputs, 1st input to be compressed depending on the signal of 2nd input and later compressed signal to be merged with 2nd input:\n\nA sidechain gate acts like a normal (wideband) gate but has the ability to filter the detected signal before sending it to the gain reduction stage. Normally a gate uses the full range signal to detect a level above the threshold. For example: If you cut all lower frequencies from your sidechain signal the gate will decrease the volume of your track only if not enough highs appear. With this technique you are able to reduce the resonation of a natural drum or remove \"rumbling\" of muted strokes from a heavily distorted guitar. It needs two input streams and returns one output stream. First input stream will be processed depending on second stream signal.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter logs a message when it detects that the input audio volume is less or equal to a noise tolerance value for a duration greater or equal to the minimum detected noise duration.\n\nThe printed times and duration are expressed in seconds. The or metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the silence.\n\nThe or and or metadata keys are set on the first frame after the silence. If is enabled, and each channel is evaluated separately, the suffixed keys are used, and corresponds to the channel number.\n\nThe filter accepts the following options:\n• Complete example with to detect silence with 0.0001 noise tolerance in :\n\nRemove silence from the beginning, middle or end of the audio.\n\nThe filter accepts the following options:\n• The following example shows how this filter can be used to start a recording that does not contain the delay at the start which usually occurs between pressing the record button and the start of the performance:\n• Trim all silence encountered from beginning to end where there is more than 1 second of silence in audio:\n• Trim all digital silence samples, using peak detection, from beginning to end where there is more than 0 samples of digital silence in audio and digital silence is detected in all channels at same positions in stream:\n• Trim every 2nd encountered silence period from beginning to end where there is more than 1 second of silence per silence period in audio:\n• Similar as above, but keep maximum of 0.5 seconds of silence from each trimmed period:\n• Similar as above, but keep maximum of 1.5 seconds of silence from start of audio:\n\nThis filter supports some above options as commands.\n\nSOFAlizer uses head-related transfer functions (HRTFs) to create virtual loudspeakers around the user for binaural listening via headphones (audio formats up to 9 channels supported). The HRTFs are stored in SOFA files (see http://www.sofacoustics.org/ for a database). SOFAlizer is developed at the Acoustics Research Institute (ARI) of the Austrian Academy of Sciences.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThe filter accepts the following options:\n• Using ClubFritz12 sofa file and bigger radius with small rotation:\n• Similar as above but with custom speaker positions for front left, front right, back left and back right and also with custom gain:\n\nThis filter expands or compresses each half-cycle of audio samples (local set of samples all above or all below zero and between two nearest zero crossings) depending on threshold value, so audio reaches target peak value under conditions controlled by below options.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter has some handy utilities to manage stereo signals, for converting M/S stereo recordings to L/R signal while having control over the parameters or spreading the stereo image of master track.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter enhance the stereo effect by suppressing signal common to both channels and by delaying the signal of left into right and vice versa, thereby widening the stereo effect.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options except as commands.\n\nThe filter accepts the following options:\n\nThis filter allows to produce multichannel output from audio stream.\n\nThe filter accepts the following options:\n\nBoost or cut the lower frequencies and cut or boost higher frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports some options as commands.\n\nBoost or cut treble (upper) frequencies of the audio using a two-pole shelving filter with a response similar to that of a standard hi-fi’s tone-controls. This is also known as shelving equalisation (EQ).\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter accepts stereo input and produce stereo with LFE (2.1) channels output. The newly produced LFE channel have enhanced virtual bass originally obtained from both stereo channels. This filter outputs front left and front right channels unchanged as available in stereo input.\n\nThe filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe volume expression can contain the following parameters.\n\nNote that when is set to ‘ ’ only the and variables are available, all other variables will evaluate to NAN.\n\nThis filter supports the following commands:\n• Halve the input audio volume: In all the above example the named key for can be omitted, for example like in:\n• Fade volume after time 10 with an annihilation period of 5 seconds:\n\nDetect the volume of the input video.\n\nThe filter has no parameters. It supports only 16-bit signed integer samples, so the input will be converted when needed. Statistics about the volume will be printed in the log when the input stream end is reached.\n\nIn particular it will show the mean volume (root mean square), maximum volume (on a per-sample basis), and the beginning of a histogram of the registered volume values (from the maximum value to a cumulated 1/1000 of the samples).\n\nAll volumes are in decibels relative to the maximum PCM value.\n\nHere is an excerpt of the output:\n• The mean square energy is approximately -27 dB, or 10^-2.7.\n• The largest sample is at -4 dB, or more precisely between -4 dB and -5 dB.\n• There are 6 samples at -4 dB, 62 at -5 dB, 286 at -6 dB, etc.\n\nIn other words, raising the volume by +4 dB does not cause any clipping, raising it by +5 dB causes clipping for 6 samples, etc.\n\nBelow is a description of the currently available audio sources.\n\nBuffer audio frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept planar 16bit signed stereo at 44100Hz. Since the sample format with name \"s16p\" corresponds to the number 6 and the \"stereo\" channel layout corresponds to the value 0x3, this is equivalent to:\n\nGenerate an audio signal specified by an expression.\n\nThis source accepts in input one or more expressions (one for each channel), which are evaluated and used to generate a corresponding audio signal.\n\nThis source accepts the following options:\n\nEach expression in can contain the following constants:\n• Generate a sin signal with frequency of 440 Hz, set sample rate to 8000 Hz:\n• Generate a two channels signal, specify the channel layout (Front Center + Back Center) explicitly:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nThe null audio source, return unprocessed audio frames. It is mainly useful as a template and to be employed in analysis / debugging tools, or as the source for filters which ignore the input data (for example the sox synth filter).\n\nThis source accepts the following options:\n• Set the sample rate to 48000 Hz and the channel layout to AV_CH_LAYOUT_MONO.\n• Do the same operation with a more obvious syntax:\n\nAll the parameters need to be explicitly defined.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nNote that versions of the flite library prior to 2.0 are not thread-safe.\n\nThe filter accepts the following options:\n• Read from file , and synthesize the text using the standard flite voice:\n• Read the specified text selecting the voice: flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Input text to ffmpeg: ffmpeg -f lavfi -i flite=text='So fare thee well, poor devil of a Sub-Sub, whose commentator I am':voice=slt\n• Make speak the specified text, using and the device: ffplay -f lavfi flite=text='No more be grieved for which that thou hast done.'\n\nFor more information about libflite, check: http://www.festvox.org/flite/\n\nThe filter accepts the following options:\n• Generate 60 seconds of pink noise, with a 44.1 kHz sampling rate and an amplitude of 0.5:\n\nThe resulting stream can be used with afir filter for phase-shifting the signal by 90 degrees.\n\nThis is used in many matrix coding schemes and for analytic signal generation. The process is often written as a multiplication by i (or j), the imaginary unit.\n\nThe filter accepts the following options:\n\nThe resulting stream can be used with afir filter for filtering the audio signal.\n\nThe filter accepts the following options:\n\nGenerate an audio signal made of a sine wave with amplitude 1/8.\n\nThe filter accepts the following options:\n• Generate a 220 Hz sine wave with a 880 Hz beep each second, for 5 seconds:\n\nBelow is a description of the currently available audio sinks.\n\nBuffer audio frames, and make them available to the end of filter chain.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVABufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull audio sink; do absolutely nothing with the input audio. It is mainly useful as a template and for use in analysis / debugging tools.\n\nWhen you configure your FFmpeg build, you can disable any of the existing filters using . The configure output will show the video filters included in your build.\n\nBelow is a description of the currently available video filters.\n\nThe frame data is passed through unchanged, but metadata is attached to the frame indicating regions of interest which can affect the behaviour of later encoding. Multiple regions can be marked by applying the filter multiple times.\n• Mark the centre quarter of the frame as interesting.\n• Mark the 100-pixel-wide region on the left edge of the frame as very uninteresting (to be encoded at much lower quality than the rest of the frame).\n\nExtract the alpha component from the input as a grayscale video. This is especially useful with the filter.\n\nAdd or replace the alpha component of the primary input with the grayscale value of a second input. This is intended for use with to allow the transmission or storage of frame sequences that have alpha in a format that doesn’t support an alpha channel.\n\nFor example, to reconstruct full frames from a normal YUV-encoded video and a separate video created with , you might use:\n\nAmplify differences between current pixel and pixels of adjacent frames in same pixel location.\n\nThis filter accepts the following options:\n\nThis filter supports the following commands that corresponds to option of same name:\n\nSame as the subtitles filter, except that it doesn’t require libavcodec and libavformat to work. On the other hand, it is limited to ASS (Advanced Substation Alpha) subtitles files.\n\nThis filter accepts the following option in addition to the common options from the subtitles filter:\n\nApply an Adaptive Temporal Averaging Denoiser to the video input.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options except option . The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nCompute the bounding box for the non-black pixels in the input frame luma plane.\n\nThis filter computes the bounding box containing all the pixels with a luma value greater than the minimum allowed value. The parameters describing the bounding box are printed on the filter log.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nDetect video intervals that are (almost) completely black. Can be useful to detect chapter transitions, commercials, or invalid recordings.\n\nThe filter outputs its detection analysis to both the log as well as frame metadata. If a black segment of at least the specified minimum duration is found, a line with the start and end timestamps as well as duration is printed to the log with level . In addition, a log line with level is printed per frame showing the black amount detected for that frame.\n\nThe filter also attaches metadata to the first frame of a black segment with key and to the first frame after the black segment ends with key . The value is the frame’s timestamp. This metadata is added regardless of the minimum duration specified.\n\nThe filter accepts the following options:\n\nThe following example sets the maximum pixel threshold to the minimum value, and detects only black intervals of 2 or more seconds:\n\nDetect frames that are (almost) completely black. Can be useful to detect chapter transitions or commercials. Output lines consist of the frame number of the detected frame, the percentage of blackness, the position in the file if known or -1 and the timestamp in seconds.\n\nIn order to display the output lines, you need to set the loglevel at least to the AV_LOG_INFO value.\n\nThis filter exports frame metadata . The value represents the percentage of pixels in the picture that are below the threshold value.\n\nIt accepts the following parameters:\n\nBlend two video frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nThe (time blend) filter takes two consecutive frames from one single stream, and outputs the result obtained by blending the new frame on top of the old frame.\n\nA description of the accepted options follows.\n\nThe filter also supports the framesync options.\n• Apply transition from bottom layer to top layer in first 10 seconds:\n• Split diagonally video and shows top and bottom layer on each side:\n• Display differences between the current and the previous frame:\n\nThis filter supports same commands as options.\n\nDetermines blockiness of frames without altering the input frames.\n\nBased on Remco Muijs and Ihor Kirenko: \"A no-reference blocking artifact measure for adaptive video processing.\" 2005 13th European signal processing conference.\n\nThe filter accepts the following options:\n• Determine blockiness for the first plane and search for periods within [8,32]:\n\nDetermines blurriness of frames without altering the input frames.\n\nBased on Marziliano, Pina, et al. \"A no-reference perceptual blur metric.\" Allows for a block-based abbreviation.\n\nThe filter accepts the following options:\n• Determine blur for 80% of most significant 32x32 blocks:\n\nThe filter accepts the following options.\n• Same as above, but filtering only luma:\n• Same as above, but with both estimation modes:\n• Same as above, but prefilter with nlmeans filter instead:\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n• Apply a boxblur filter with the luma, chroma, and alpha radii set to 2:\n• Set the luma radius to 2, and alpha and chroma radius to 0:\n• Set the luma and chroma radii to a fraction of the video dimension:\n\nMotion adaptive deinterlacing based on yadif with the use of w3fdif and cubic interpolation algorithms. It accepts the following parameters:\n\nThis filter fixes various issues seen with commerical encoders related to upstream malformed CEA-708 payloads, specifically incorrect number of tuples (wrong cc_count for the target FPS), and incorrect ordering of tuples (i.e. the CEA-608 tuples are not at the first entries in the payload).\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nRemove all color information for all colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n• Make every green pixel in the input image transparent:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplay CIE color diagram with pixels overlaid onto it.\n\nThe filter accepts the following options:\n\nSome codecs can export information through frames using side-data or other means. For example, some MPEG based codecs export motion vectors through the flag in the codec option.\n\nThe filter accepts the following option:\n• Visualize forward predicted MVs of all frames using :\n• Visualize multi-directionals MVs of P and B-Frames using :\n\nModify intensity of primary colors (red, green and blue) of input frames.\n\nThe filter allows an input frame to be adjusted in the shadows, midtones or highlights regions for the red-cyan, green-magenta or blue-yellow balance.\n\nA positive adjustment value shifts the balance towards the primary color, a negative value towards the complementary color.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nAdjust color white balance selectively for blacks and whites. This filter operates in YUV colorspace.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter modifies a color channel by adding the values associated to the other channels of the same pixels. For example if the value to modify is red, the output value will be:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nRGB colorspace color keying. This filter operates on 8-bit RGB format frames by setting the alpha component of each pixel which falls within the similarity radius of the key color to 0. The alpha value for pixels outside the similarity radius depends on the value of the blend option.\n\nThe filter accepts the following options:\n• Make every green pixel in the input image transparent:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nRemove all color information for all RGB colors except for certain one.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter needs three input video streams. First stream is video stream that is going to be filtered out. Second and third video stream specify color patches for source color to target color mapping.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nFor example to convert from BT.601 to SMPTE-240M, use the command:\n\nConvert colorspace, transfer characteristics or color primaries. Input video needs to have an even size.\n\nThe filter accepts the following options:\n\nThe filter converts the transfer characteristics, color space and color primaries to the specified user values. The output value, if not specified, is set to a default value based on the \"all\" property. If that property is also not specified, the filter will log an error. The output color range and format default to the same value as the input color range and format. The input transfer characteristics, color space, color primaries and color range should be set on the input data. If any of these are missing, the filter will log an error and no conversion will take place.\n\nFor example to convert the input to SMPTE-240M, use the command:\n\nAdjust color temperature in video to simulate variations in ambient color temperature.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nApply convolution of 3x3, 5x5, 7x7 or horizontal/vertical up to 49 elements.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply 2D convolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nCopy the input video source unchanged to the output. This is mainly useful for testing purposes.\n\nVideo filtering on GPU using Apple’s CoreImage API on OSX.\n\nHardware acceleration is based on an OpenGL context. Usually, this means it is processed by video hardware. However, software-based OpenGL implementations exist which means there is no guarantee for hardware processing. It depends on the respective OSX.\n\nThere are many filters and image generators provided by Apple that come with a large variety of options. The filter has to be referenced by its name along with its options.\n\nThe coreimage filter accepts the following options:\n\nSeveral filters can be chained for successive processing without GPU-HOST transfers allowing for fast processing of complex filter chains. Currently, only filters with zero (generators) or exactly one (filters) input image and one output image are supported. Also, transition filters are not yet usable as intended.\n\nSome filters generate output images with additional padding depending on the respective filter kernel. The padding is automatically removed to ensure the filter output has the same size as the input image.\n\nFor image generators, the size of the output image is determined by the previous output image of the filter chain or the input image of the whole filterchain, respectively. The generators do not use the pixel information of this image to generate their output. However, the generated output is blended onto this image, resulting in partial or complete coverage of the output image.\n\nThe coreimagesrc video source can be used for generating input images which are directly fed into the filter chain. By using it, providing input images by another video source or an input video is not required.\n• Use the CIBoxBlur filter with default options to blur an image:\n• Use a filter chain with CISepiaTone at default values and CIVignetteEffect with its center at 100x100 and a radius of 50 pixels:\n• Use nullsrc and CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell:\n\nObtain the correlation between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max correlation is printed through the logging system.\n\nThe filter stores the calculated correlation of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n\nCrop the input video to given dimensions.\n\nIt accepts the following parameters:\n\nThe , , , parameters are expressions containing the following constants:\n\nThe expression for may depend on the value of , and the expression for may depend on , but they cannot depend on and , as and are evaluated after and .\n\nThe and parameters specify the expressions for the position of the top-left corner of the output (non-cropped) area. They are evaluated for each frame. If the evaluated value is not valid, it is approximated to the nearest valid value.\n\nThe expression for may depend on , and the expression for may depend on .\n• Crop area with size 100x100 at position (12,34). Using named options, the example above becomes:\n• Crop the central input area with size 2/3 of the input video:\n• Delimit the rectangle with the top-left corner placed at position 100:100 and the right-bottom corner corresponding to the right-bottom corner of the input image.\n• Crop 10 pixels from the left and right borders, and 20 pixels from the top and bottom borders\n• Keep only the bottom right quarter of the input image:\n• Set x depending on the value of y:\n\nThis filter supports the following commands:\n\nIt calculates the necessary cropping parameters and prints the recommended parameters via the logging system. The detected dimensions correspond to the non-black or video area of the input video according to .\n\nIt accepts the following parameters:\n• Find an embedded video area, use motion vectors from decoder:\n\nThis filter supports the following commands:\n\nDelay video filtering until a given wallclock timestamp. The filter first passes on amount of frames, then it buffers at most amount of frames and waits for the cue. After reaching the cue it forwards the buffered frames and also any subsequent frames coming in its input.\n\nThe filter can be used synchronize the output of multiple ffmpeg processes for realtime output devices like decklink. By putting the delay in the filtering chain and pre-buffering frames the process can pass on data to output almost immediately after the target wallclock timestamp is reached.\n\nPerfect frame accuracy cannot be guaranteed, but the result is good enough for some use cases.\n\nThis filter is similar to the Adobe Photoshop and GIMP curves tools. Each component (red, green and blue) has its values defined by key points tied from each other using a smooth curve. The x-axis represents the pixel values from the input frame, and the y-axis the new pixel values to be set for the output frame.\n\nBy default, a component curve is defined by the two points and . This creates a straight line where each original pixel value is \"adjusted\" to its own value, which means no change to the image.\n\nThe filter allows you to redefine these two points and add some more. A new curve will be defined to pass smoothly through all these new coordinates. The new defined points need to be strictly increasing over the x-axis, and their and values must be in the interval. The curve is formed by using a natural or monotonic cubic spline interpolation, depending on the option (default: ). The spline produces a smoother curve in general while the monotonic ( ) spline guarantees the transitions between the specified points to be monotonic. If the computed curves happened to go outside the vector spaces, the values will be clipped accordingly.\n\nThe filter accepts the following options:\n\nTo avoid some filtergraph syntax conflicts, each key points list need to be defined using the following syntax: .\n\nThis filter supports same commands as options.\n• Vintage effect: Here we obtain the following coordinates for each components:\n• The previous example can also be achieved with the associated built-in preset:\n• Use a Photoshop preset and redefine the points of the green component:\n• Check out the curves of the profile using and :\n\nThis filter shows hexadecimal pixel values of part of video.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options excluding option.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThis filter is not designed for real time.\n\nThe filter accepts the following options:\n\nThe same operation can be achieved using the expression system:\n\nRemove banding artifacts from input video. It works by replacing banded pixels with average value of referenced pixels.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n• Deblock using weak filter and block size of 4 pixels.\n• Deblock using strong filter, block size of 4 pixels and custom thresholds for deblocking more edges.\n• Similar as above, but filter only first plane.\n• Similar as above, but filter only second and third plane.\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nApply 2D deconvolution of video stream in frequency domain using second stream as impulse.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nIt accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values lower than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt accepts the following options:\n\nJudder can be introduced, for instance, by pullup filter. If the original source was partially telecined content then the output of will have a variable frame rate. May change the recorded frame rate of the container. Aside from that change, this filter will not affect constant frame rate video.\n\nThe option available in this filter is:\n\nSuppress a TV station logo by a simple interpolation of the surrounding pixels. Just set a rectangle covering the logo and watch it disappear (and sometimes something even uglier appear - your mileage may vary).\n\nIt accepts the following parameters:\n• Set a rectangle covering the area with top left corner coordinates 0,0 and size 100x77:\n\nRemove the rain in the input image/video by applying the derain methods based on convolutional neural networks. Supported models:\n\nTraining as well as model generation scripts are provided in the repository at https://github.com/XueweiMeng/derain_filter.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nAttempt to fix small changes in horizontal and/or vertical shift. This filter helps remove camera shake from hand-holding a camera, bumping a tripod, moving on a vehicle, etc.\n\nThe filter accepts the following options:\n\nRemove unwanted contamination of foreground colors, caused by reflected color of greenscreen or bluescreen.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nApply an exact inverse of the telecine operation. It requires a predefined pattern specified using the pattern option which must be the same as that passed to the telecine filter.\n\nThis filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nDisplace pixels as indicated by second and third input stream.\n\nIt takes three input streams and outputs one stream, the first input is the source, and second and third input are displacement maps.\n\nThe second input specifies how much to displace pixels along the x-axis, while the third input specifies how much to displace pixels along the y-axis. If one of displacement map streams terminates, last frame from that displacement map will be used.\n\nNote that once generated, displacements maps can be reused over and over again.\n\nA description of the accepted options follows.\n\nDo classification with deep neural networks based on bounding boxes.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nDo image processing with deep neural networks. It works together with another filter which converts the pixel format of the Frame to what the dnn network requires.\n\nThe filter accepts the following options:\n• Remove rain in rgb24 frame with can.pb (see derain filter):\n• Handle the Y channel with srcnn.pb (see sr filter) for frame with yuv420p (planar YUV formats supported):\n• Handle the Y channel with espcn.pb (see sr filter), which changes frame size, for format yuv420p (planar YUV formats supported), please use tools/python/tf_sess_config.py to get the configs of TensorFlow backend for your system.\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nIt accepts the following parameters:\n\nExample using metadata from signalstats filter:\n\nExample using metadata from ebur128 filter:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a grid with cell 100x100 pixels, thickness 2 pixels, with color red and an opacity of 50%:\n• Draw a white 3x3 grid with an opacity of 50%:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nDraw a text string or text from a specified file on top of a video, using the libfreetype library.\n\nTo enable compilation of this filter, you need to configure FFmpeg with and . To enable default font fallback and the option you need to configure FFmpeg with . To enable the option, you need to configure FFmpeg with .\n\nIt accepts the following parameters:\n\nThe parameters for and are expressions containing the following constants and functions:\n\nIf is set to , the filter recognizes sequences accepted by the C function in the provided text and expands them accordingly. Check the documentation of . This feature is deprecated in favor of expansion with the or expansion functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n\nThe following options are also supported as commands:\n• Draw \"Test Text\" with font FreeSerif, using the default values for the optional parameters.\n• Draw ’Test Text’ with font FreeSerif of size 24 at position x=100 and y=50 (counting from the top-left corner of the screen), text is yellow with a red box around it. Both the text and the box have an opacity of 20%. Note that the double quotes are not necessary if spaces are not used within the parameter list.\n• Show the text at the center of the video frame:\n• Show the text at a random position, switching to a new position every 30 seconds:\n• Show a text line sliding from right to left in the last row of the video frame. The file is assumed to contain a single line with no newlines.\n• Show the content of file off the bottom of the frame and scroll up.\n• Draw a single green letter \"g\", at the center of the input video. The glyph baseline is placed at half screen height.\n• Show text for 1 second every 3 seconds:\n• Use fontconfig to set the font. Note that the colons need to be escaped.\n• Draw \"Test Text\" with font size dependent on height of the video.\n• Print the date of a real-time encoding (see documentation for the C function):\n• Show text fading in and out (appearing/disappearing):\n• Horizontally align multiple separate texts. Note that and the value are included in the offset.\n• Plot special metadata onto each frame if such metadata exists. Otherwise, plot the string \"NA\". Note that image2 demuxer must have option for the special metadata fields to be available for filters.\n\nFor more information about libfreetype, check: http://www.freetype.org/.\n\nFor more information about fontconfig, check: http://freedesktop.org/software/fontconfig/fontconfig-user.html.\n\nFor more information about libfribidi, check: http://fribidi.org/.\n\nFor more information about libharfbuzz, check: https://github.com/harfbuzz/harfbuzz.\n\nDetect and draw edges. The filter uses the Canny Edge Detection algorithm.\n\nThe filter accepts the following options:\n• Standard edge detection with custom values for the hysteresis thresholding:\n\nFor each input image, the filter will compute the optimal mapping from the input to the output given the codebook length, that is the number of distinct output colors.\n\nThis filter accepts the following options.\n\nMeasure graylevel entropy in histogram of color channels of video frames.\n\nIt accepts the following parameters:\n\nApply the EPX magnification filter which is designed for pixel art.\n\nIt accepts the following option:\n\nThe filter accepts the following options:\n\nThe expressions accept the following parameters:\n\nThe filter supports the following commands:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSpatial only filter that uses edge slope tracing algorithm to interpolate missing lines. It accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nThe filter accepts the following option:\n• Extract luma, u and v color channel component from input video frame into 3 grayscale outputs:\n\nIt accepts the following parameters:\n• Fade in the first 30 frames of video: The command above is equivalent to:\n• Fade out the last 45 frames of a 200-frame video:\n• Fade in the first 25 frames and fade out the last 25 frames of a 1000-frame video:\n• Make the first 5 frames yellow, then fade in from frame 5-24:\n• Fade in alpha over first 25 frames of video:\n• Make the first 5.5 seconds black, then fade in for 0.5 seconds:\n\nThis filter pass cropped input frames to 2nd output. From there it can be filtered with other video filters. After filter receives frame from 2nd input, that frame is combined on top of original frame from 1st input and passed to 1st output.\n\nThe typical usage is filter only part of frame.\n\nThe filter accepts the following options:\n• Blur only top left rectangular part of video frame size 100x100 with gblur filter.\n• Draw black box on top left part of video frame of size 100x100 with drawbox filter.\n• Pixelize rectangular part of video frame of size 100x100 with pixelize filter.\n\nThe filter accepts the following options:\n\nExtract a single field from an interlaced image using stride arithmetic to avoid wasting CPU time. The output frames are marked as non-interlaced.\n\nThe filter accepts the following options:\n\nCreate new frames by copying the top and bottom fields from surrounding frames supplied as numbers by the hint file.\n\nExample of first several lines of file for mode:\n\nField matching filter for inverse telecine. It is meant to reconstruct the progressive frames from a telecined stream. The filter does not drop duplicated frames, so to achieve a complete inverse telecine needs to be followed by a decimation filter such as decimate in the filtergraph.\n\nThe separation of the field matching and the decimation is notably motivated by the possibility of inserting a de-interlacing filter fallback between the two. If the source has mixed telecined and real interlaced content, will not be able to match fields for the interlaced parts. But these remaining combed frames will be marked as interlaced, and thus can be de-interlaced by a later filter such as yadif before decimation.\n\nIn addition to the various configuration options, can take an optional second stream, activated through the option. If enabled, the frames reconstruction will be based on the fields and frames from this second stream. This allows the first input to be pre-processed in order to help the various algorithms of the filter, while keeping the output lossless (assuming the fields are matched properly). Typically, a field-aware denoiser, or brightness/contrast adjustments can help.\n\nNote that this filter uses the same algorithms as TIVTC/TFM (AviSynth project) and VIVTC/VFM (VapourSynth project). The later is a light clone of TFM from which is based on. While the semantic and usage are very close, some behaviour and options names can differ.\n\nThe decimate filter currently only works for constant frame rate input. If your input has mixed telecined (30fps) and progressive content with a lower framerate like 24fps use the following filterchain to produce the necessary cfr stream: .\n\nThe filter accepts the following options:\n\nWe assume the following telecined stream:\n\nThe numbers correspond to the progressive frame the fields relate to. Here, the first two frames are progressive, the 3rd and 4th are combed, and so on.\n\nWhen is configured to run a matching from bottom ( = ) this is how this input stream get transformed:\n\nAs a result of the field matching, we can see that some frames get duplicated. To perform a complete inverse telecine, you need to rely on a decimation filter after this operation. See for instance the decimate filter.\n\nThe same operation now matching from top fields ( = ) looks like this:\n\nIn these examples, we can see what , and mean; basically, they refer to the frame and field of the opposite parity:\n• matches the field of the opposite parity in the previous frame\n• matches the field of the opposite parity in the current frame\n• matches the field of the opposite parity in the next frame\n\nThe and matching are a bit special in the sense that they match from the opposite parity flag. In the following examples, we assume that we are currently matching the 2nd frame (Top:2, bottom:2). According to the match, a ’x’ is placed above and below each matched fields.\n\nAdvanced IVTC, with fallback on yadif for still combed frames:\n\nTransform the field order of the input video.\n\nIt accepts the following parameters:\n\nThe default value is ‘ ’.\n\nThe transformation is done by shifting the picture content up or down by one line, and filling the remaining line with appropriate picture content. This method is consistent with most broadcast field order converters.\n\nIf the input video is not flagged as being interlaced, or it is already flagged as being of the required output field order, then this filter does not alter the incoming video.\n\nIt is very useful when converting to or from PAL DV material, which is bottom field first.\n\nFill borders of the input video, without changing video stream dimensions. Sometimes video can have garbage at the four edges and you may not want to crop video input to keep size multiple of some number.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe object to search for must be specified as a gray8 image specified with the option.\n\nFor each possible match, a score is computed. If the score reaches the specified threshold, the object is considered found.\n\nIf the input video contains multiple instances of the object, the filter will find only one of them.\n\nWhen an object is found, the following metadata entries are set in the matching frame:\n\nIt accepts the following options:\n• Cover a rectangular object by the supplied image of a given video using :\n• Find the position of an object in each frame using and write it to a log file:\n\nFlood area with values of same pixel components with another values.\n\nIt accepts the following options:\n\nConvert the input video to one of the specified pixel formats. Libavfilter will try to pick one that is suitable as input to the next filter.\n\nIt accepts the following parameters:\n• Convert the input video to the format Convert the input video to any of the formats in the list\n\nConvert the video to specified constant frame rate by duplicating or dropping frames as necessary.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: [: ]].\n\nSee also the setpts filter.\n• A typical usage in order to set the fps to 25:\n• Sets the fps to 24, using abbreviation and rounding method to round to nearest:\n\nPack two different video streams into a stereoscopic video, setting proper metadata on supported codecs. The two views should have the same size and framerate and processing will stop when the shorter video ends. Please note that you may conveniently adjust view properties with the scale and fps filters.\n\nIt accepts the following parameters:\n\nChange the frame rate by interpolating new video output frames from the source frames.\n\nThis filter is not designed to function correctly with interlaced media. If you wish to change the frame rate of interlaced media then you are required to deinterlace before this filter and re-interlace after this filter.\n\nA description of the accepted options follows.\n\nThis filter accepts the following option:\n\nThis filter logs a message and sets frame metadata when it detects that the input video has no significant change in content during a specified duration. Video freeze detection calculates the mean average absolute difference of all the components of video frames and compares it to a noise floor.\n\nThe printed times and duration are expressed in seconds. The metadata key is set on the first frame whose timestamp equals or exceeds the detection duration and it contains the timestamp of the first frame of the freeze. The and metadata keys are set on the first frame after the freeze.\n\nThe filter accepts the following options:\n\nThis filter freezes video frames using frame from 2nd input.\n\nThe filter accepts the following options:\n\nTo enable the compilation of this filter, you need to install the frei0r header and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nA frei0r effect parameter can be a boolean (its value is either \"y\" or \"n\"), a double, a color (specified as / / , where , , and are floating point numbers between 0.0 and 1.0, inclusive) or a color description as specified in the (ffmpeg-utils)\"Color\" section in the ffmpeg-utils manual, a position (specified as / , where and are floating point numbers) and/or a string.\n\nThe number and types of parameters depend on the loaded effect. If an effect parameter is not specified, the default value is set.\n• Apply the distort0r effect, setting the first two double parameters:\n• Apply the colordistance effect, taking a color as the first parameter:\n• Apply the perspective effect, specifying the top left and top right image positions:\n\nFor more information, see http://frei0r.dyne.org\n\nThis filter supports the option as commands.\n\nApply fast and simple postprocessing. It is a faster version of spp.\n\nIt splits (I)DCT into horizontal/vertical passes. Unlike the simple post- processing filter, one of them is performed once per block, not per pixel. This allows for much higher speed.\n\nThe filter accepts the following options:\n\nSynchronize video frames with an external mapping from a file.\n\nFor each input PTS given in the map file it either drops or creates as many frames as necessary to recreate the sequence of output frames given in the map file.\n\nThis filter is useful to recreate the output frames of a framerate conversion by the fps filter, recorded into a map file using the ffmpeg option , and do further processing to the corresponding frames e.g. quality comparison.\n\nEach line of the map file must contain three items per input frame, the input PTS (decimal), the output PTS (decimal) and the output TIMEBASE (decimal/decimal), seperated by a space. This file format corresponds to the output of .\n\nThe filter assumes the map file is sorted by increasing input PTS.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts the following options:\n\nThe colorspace is selected according to the specified options. If one of the , , or options is specified, the filter will automatically select a YCbCr colorspace. If one of the , , or options is specified, it will select an RGB colorspace.\n\nIf one of the chrominance expression is not defined, it falls back on the other one. If no alpha expression is specified it will evaluate to opaque value. If none of chrominance expressions are specified, they will evaluate to the luma expression.\n\nThe expressions can use the following variables and functions:\n\nFor functions, if and are outside the area, the value will be automatically clipped to the closer edge.\n\nPlease note that this filter can use multiple threads in which case each slice will have its own expression state. If you want to use only a single expression state because your expressions depend on previous state then you should limit the number of filter threads to 1.\n• Generate a bidimensional sine wave, with angle and a wavelength of 100 pixels:\n• Create a radial gradient that is the same size as the input (also see the vignette filter):\n\nFix the banding artifacts that are sometimes introduced into nearly flat regions by truncation to 8-bit color depth. Interpolate the gradients that should go where the bands are, and dither them.\n\nIt is designed for playback only. Do not use it prior to lossy compression, because compression tends to lose the dither and bring back the bands.\n\nIt accepts the following parameters:\n\nAlternatively, the options can be specified as a flat string: [: ]\n• Apply the filter with a strength and radius of :\n• Specify radius, omitting the strength (which will fall-back to the default value):\n\nWith this filter one can debug complete filtergraph. Especially issues with links filling with queued frames.\n\nThe filter accepts the following options:\n\nA color constancy filter that applies color correction based on the grayworld assumption\n\nThe algorithm uses linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nA color constancy variation filter which estimates scene illumination via grey edge algorithm and corrects the scene colors accordingly.\n\nThe filter accepts the following options:\n\nApply guided filter for edge-preserving smoothing, dehazing and so on.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Dehazing, structure-transferring filtering, detail enhancement with guided filter. For the generation of guidance image, refer to paper \"Guided Image Filtering\". See: http://kaiminghe.com/publications/pami12guidedfilter.pdf.\n\nFirst input is the video stream to process, and second one is the Hald CLUT. The Hald CLUT input can be a simple picture or a complete video stream.\n\nThe filter accepts the following options:\n\nalso has the same interpolation options as lut3d (both filters share the same internals).\n\nThis filter also supports the framesync options.\n\nMore information about the Hald CLUT can be found on Eskil Steenberg’s website (Hald CLUT author) at http://www.quelsolaar.com/technology/clut.html.\n\nThis filter supports the option as commands.\n\nGenerate an identity Hald CLUT stream altered with various effects:\n\nNote: make sure you use a lossless codec.\n\nThen use it with to apply it on some random stream:\n\nThe Hald CLUT will be applied to the 10 first seconds (duration of ), then the latest picture of that CLUT stream will be applied to the remaining frames of the stream.\n\nA Hald CLUT is supposed to be a squared image of by pixels. For a given Hald CLUT, FFmpeg will select the biggest possible square starting at the top left of the picture. The remaining padding pixels (bottom or right) will be ignored. This area can be used to add a preview of the Hald CLUT.\n\nTypically, the following generated Hald CLUT will be supported by the filter:\n\nIt contains the original and a preview of the effect of the CLUT: SMPTE color bars are displayed on the right-top, and below the same color bars processed by the color changes.\n\nThen, the effect of this Hald CLUT can be visualized with:\n\nFor example, to horizontally flip the input video with :\n\nIt can be used to correct video that has a compressed range of pixel intensities. The filter redistributes the pixel intensities to equalize their distribution across the intensity range. It may be viewed as an \"automatically adjusting contrast filter\". This filter is useful only for correcting degraded or poorly captured source video.\n\nThe filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nStandard histogram displays the color components distribution in an image. Displays color graph for each color component. Shows distribution of the Y, U, V, A or R, G, B components, depending on input format, in the current frame. Below each graph a color component scale meter is shown.\n\nThe filter accepts the following options:\n\nThis is a high precision/quality 3d denoise filter. It aims to reduce image noise, producing smooth images and making still images really still. It should enhance compressibility.\n\nIt accepts the following optional parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe input must be in hardware frames, and the output a non-hardware format. Not all formats will be supported on the output - it may be necessary to insert an additional filter immediately following in the graph to get the output in a supported format.\n\nMap hardware frames to system memory or to another device.\n\nThis filter has several different modes of operation; which one is used depends on the input and output formats:\n• Hardware frame input, normal frame output Map the input frames to system memory and pass them to the output. If the original hardware frame is later required (for example, after overlaying something else on part of it), the filter can be used again in the next mode to retrieve it.\n• Normal frame input, hardware frame output If the input is actually a software-mapped hardware frame, then unmap it - that is, return the original hardware frame. Otherwise, a device must be provided. Create new hardware surfaces on that device for the output, then map them back to the software format at the input and give those frames to the preceding filter. This will then act like the filter, but may be able to avoid an additional copy when the input is already in a compatible format.\n• Hardware frame input and output A device must be supplied for the output, either directly or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card). If the input frames were originally created on the output device, then unmap to retrieve the original frames. Otherwise, map the frames to the output device - create new hardware frames on the output corresponding to the frames on the input.\n\nThe following additional parameters are accepted:\n\nThe device to upload to must be supplied when the filter is initialised. If using ffmpeg, select the appropriate device with the option or with the option. The input and output devices must be of different types and compatible - the exact meaning of this is system-dependent, but typically it means that they must refer to the same underlying hardware context (for example, refer to the same graphics card).\n\nThe following additional parameters are accepted:\n\nIt accepts the following optional parameters:\n\nApply a high-quality magnification filter designed for pixel art. This filter was originally created by Maxim Stepin.\n\nIt accepts the following option:\n\nAll streams must be of same pixel format and of same height.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following option:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to be gray or not.\n\nThe filter accepts the following options:\n\nThis filter measures color difference between set HSV color in options and ones measured in video stream. Depending on options, output colors can be changed to transparent by adding alpha channel.\n\nThe filter accepts the following options:\n\nModify the hue and/or the saturation of the input.\n\nIt accepts the following parameters:\n\nand are mutually exclusive, and can’t be specified at the same time.\n\nThe , , and option values are expressions containing the following constants:\n• Set the hue to 90 degrees and the saturation to 1.0:\n• Same command but expressing the hue in radians:\n• Rotate hue and make the saturation swing between 0 and 2 over a period of 1 second:\n• Apply a 3 seconds saturation fade-in effect starting at 0: The general fade-in expression can be written as:\n• Apply a 3 seconds saturation fade-out effect starting at 5 seconds: The general fade-out expression can be written as:\n\nThis filter supports the following commands:\n\nThis filter accepts the following options:\n\nGrow first stream into second stream by connecting components. This makes it possible to build more robust edge masks.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nDetect the colorspace from an embedded ICC profile (if present), and update the frame’s tags accordingly.\n\nThis filter accepts the following options:\n\nGenerate ICC profiles and attach them to frames.\n\nThis filter accepts the following options:\n\nObtain the identity score between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max identity score is printed through the logging system.\n\nThe filter stores the calculated identity scores of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThis filter tries to detect if the input frames are interlaced, progressive, top or bottom field first. It will also try to detect fields that are repeated between adjacent frames (a sign of telecine).\n\nSingle frame detection considers only immediately adjacent frames when classifying each frame. Multiple frame detection incorporates the classification history of previous frames.\n\nThe filter will log these metadata values:\n\nThe filter accepts the following options:\n\nInspect the field order of the first 360 frames in a video, in verbose detail:\n\nThe idet filter will add analysis metadata to each frame, which will then be discarded. At the end, the filter will also print a final report with statistics.\n\nThis filter allows one to process interlaced images fields without deinterlacing them. Deinterleaving splits the input frame into 2 fields (so called half pictures). Odd lines are moved to the top half of the output image, even lines to the bottom half. You can process (filter) them independently and then re-interleave them.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter replaces the pixel by the local(3x3) average by taking into account only values higher than the pixel.\n\nIt accepts the following options:\n\nThis filter supports the all above options as commands.\n\nSimple interlacing filter from progressive contents. This interleaves upper (or lower) lines from odd frames with lower (or upper) lines from even frames, halving the frame rate and preserving image height.\n\nIt accepts the following optional parameters:\n\nDeinterlace input video by applying Donald Graft’s adaptive kernel deinterling. Work on interlaced parts of a video to produce progressive frames.\n\nThe description of the accepted parameters follows.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter makes short flashes of light appear longer. This filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThis filter can be used to correct for radial distortion as can result from the use of wide angle lenses, and thereby re-rectify the image. To find the right parameters one can use tools available for example as part of opencv or simply trial-and-error. To use opencv use the calibration sample (under samples/cpp) from the opencv sources and extract the k1 and k2 coefficients from the resulting matrix.\n\nNote that effectively the same filter is available in the open-source tools Krita and Digikam from the KDE project.\n\nIn contrast to the vignette filter, which can also be used to compensate lens errors, this filter corrects the distortion of the image, whereas vignette corrects the brightness distribution, so you may want to use both filters together in certain cases, though you will have to take care of ordering, i.e. whether vignetting should be applied before or after lens correction.\n\nThe filter accepts the following options:\n\nThe formula that generates the correction is:\n\nwhere is halve of the image diagonal and and are the distances from the focal point in the source and target images, respectively.\n\nThis filter supports the all above options as commands.\n\nThe filter requires the camera make, camera model, and lens model to apply the lens correction. The filter will load the lensfun database and query it to find the corresponding camera and lens entries in the database. As long as these entries can be found with the given options, the filter can perform corrections on frames. Note that incomplete strings will result in the filter choosing the best match with the given options, and the filter will output the chosen camera and lens models (logged with level \"info\"). You must provide the make, camera model, and lens model as they are required.\n\nTo obtain a list of available makes and models, leave out one or both of and options. The filter will send the full list to the log with level . The first column is the make and the second column is the model. To obtain a list of available lenses, set any values for make and model and leave out the option. The filter will send the full list of lenses in the log with level . The ffmpeg tool will exit after the list is printed.\n\nThe filter accepts the following options:\n• Apply lens correction with make \"Canon\", camera model \"Canon EOS 100D\", and lens model \"Canon EF-S 18-55mm f/3.5-5.6 IS STM\" with focal length of \"18\" and aperture of \"8.0\".\n• Apply the same as before, but only for the first 5 seconds of video.\n\nThe options for this filter are divided into the following sections:\n\nThese options control the overall output mode. By default, libplacebo will try to preserve the source colorimetry and size as best as it can, but it will apply any embedded film grain, dolby vision metadata or anamorphic SAR present in source frames.\n\nIn addition to the expression constants documented for the scale filter, the , , , , , , and options can also contain the following constants:\n\nThe options in this section control how libplacebo performs upscaling and (if necessary) downscaling. Note that libplacebo will always internally operate on 4:4:4 content, so any sub-sampled chroma formats such as will necessarily be upsampled and downsampled as part of the rendering process. That means scaling might be in effect even if the source and destination resolution are the same.\n\nDeinterlacing is automatically supported when frames are tagged as interlaced, however frames are not deinterlaced unless a deinterlacing algorithm is chosen.\n\nLibplacebo comes with a built-in debanding filter that is good at counteracting many common sources of banding and blocking. Turning this on is highly recommended whenever quality is desired.\n\nA collection of subjective color controls. Not very rigorous, so the exact effect will vary somewhat depending on the input primaries and colorspace.\n\nTo help deal with sources that only have static HDR10 metadata (or no tagging whatsoever), libplacebo uses its own internal frame analysis compute shader to analyze source frames and adapt the tone mapping function in realtime. If this is too slow, or if exactly reproducible frame-perfect results are needed, it’s recommended to turn this feature off.\n\nThe options in this section control how libplacebo performs tone-mapping and gamut-mapping when dealing with mismatches between wide-gamut or HDR content. In general, libplacebo relies on accurate source tagging and mastering display gamut information to produce the best results.\n\nBy default, libplacebo will dither whenever necessary, which includes rendering to any integer format below 16-bit precision. It’s recommended to always leave this on, since not doing so may result in visible banding in the output, even if the filter is enabled. If maximum performance is needed, use instead of disabling dithering.\n\nlibplacebo supports a number of custom shaders based on the mpv .hook GLSL syntax. A collection of such shaders can be found here: https://github.com/mpv-player/mpv/wiki/User-Scripts#user-shaders\n\nA full description of the mpv shader format is beyond the scope of this section, but a summary can be found here: https://mpv.io/manual/master/#options-glsl-shader\n\nAll of the options in this section default off. They may be of assistance when attempting to squeeze the maximum performance at the cost of quality.\n\nThis filter supports almost all of the above options as commands.\n• Rescale input to fit into standard 1080p, with high quality scaling:\n• Run this filter on the CPU, on systems with Mesa installed (and with the most expensive options disabled):\n• Suppress CPU-based AV1/H.274 film grain application in the decoder, in favor of doing it with this filter. Note that this is only a gain if the frames are either already on the GPU, or if you’re using libplacebo for other purposes, since otherwise the VRAM roundtrip will more than offset any expected speedup.\n• Interop with VAAPI hwdec to avoid round-tripping through RAM:\n\nCalculate the VMAF (Video Multi-Method Assessment Fusion) score for a reference/distorted pair of input videos.\n\nThe first input is the distorted video, and the second input is the reference video.\n\nThe obtained VMAF score is printed through the logging system.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nThe filter has following options:\n\nThis filter also supports the framesync options.\n• In the examples below, a distorted video is compared with a reference file .\n• Example with options and different containers:\n\nThis is the CUDA variant of the libvmaf filter. It only accepts CUDA frames.\n\nIt requires Netflix’s vmaf library (libvmaf) as a pre-requisite. After installing the library it can be enabled using: .\n\nApply limited difference filter using second and optionally third video stream.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except option ‘ ’.\n\nLimits the pixel components values to the specified range [min, max].\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the option as commands.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nCompute a look-up table for binding each pixel component input value to an output value, and apply it to the input video.\n\napplies a lookup table to a YUV input video, to an RGB input video.\n\nThese filters accept the following parameters:\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in input.\n\nThe filter requires either YUV or RGB pixel formats in input, requires RGB pixel formats in input, and requires YUV.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports same commands as options.\n• Negate input video: The above is the same as:\n\nThe filter takes two input streams and outputs one stream.\n\nThe (time lut2) filter takes two consecutive frames from one single stream.\n\nThis filter accepts the following parameters:\n\nThe filter also supports the framesync options.\n\nEach of them specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe exact component associated to each of the options depends on the format in inputs.\n\nThe expressions can contain the following constants:\n\nThis filter supports the all above options as commands except option .\n\nClamp the first input stream with the second input and third input stream.\n\nReturns the value of first stream to be between second input stream - and third input stream + .\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is greater than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the first input stream with the second input stream using per pixel weights in the third input stream.\n\nA value of 0 in the third stream pixel component means that pixel component from first stream is returned unchanged, while maximum value (eg. 255 for 8-bit videos) means that pixel component from second stream is returned unchanged. Intermediate values define the amount of merging between both input stream’s pixel components.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nMerge the second and third input stream into output stream using absolute differences between second input stream and first input stream and absolute difference between third input stream and first input stream. The picked value will be from second input stream if second absolute difference is less than first one or from third input stream otherwise.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nPick pixels comparing absolute difference of two video streams with fixed threshold.\n\nIf absolute difference between pixel component of first and second video stream is equal or lower than user supplied threshold than pixel component from first video stream is picked, otherwise pixel component from second video stream is picked.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nFor example it is useful to create motion masks after filter.\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nIt needs one field per frame as input and must thus be used together with yadif=1/3 or equivalent.\n\nThis filter accepts the following options:\n\nPick median pixel from certain rectangle defined by radius.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nThe filter accepts up to 4 input streams, and merge selected input planes to the output video.\n\nThis filter accepts the following options:\n• Merge three gray video streams of same width and height into single video stream:\n\nEstimate and export motion vectors using block matching algorithms. Motion vectors are stored in frame side data to be used by other filters.\n\nThis filter accepts the following options:\n\nMidway Image Equalization adjusts a pair of images to have the same histogram, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a pair of stereo cameras.\n\nThis filter has two inputs and one output, which must be of same pixel format, but may be of different sizes. The output of filter is first input adjusted with midway histogram of both inputs.\n\nThis filter accepts the following option:\n\nConvert the video to specified frame rate using motion interpolation.\n\nThis filter accepts the following options:\n\nMix several video input streams into one video stream.\n\nA description of the accepted options follows.\n\nThis filter supports the following commands:\n\nA description of the accepted options follows.\n\nThis filter supports the all above options as commands.\n\nThis filter allows to apply main morphological grayscale transforms, erode and dilate with arbitrary structures set in second input stream.\n\nUnlike naive implementation and much slower performance in erosion and dilation filters, when speed is critical filter should be used instead.\n\nThe filter also supports the framesync options.\n\nThis filter supports same commands as options.\n\nDrop frames that do not differ greatly from the previous frame in order to reduce frame rate.\n\nThe main use of this filter is for very-low-bitrate encoding (e.g. streaming over dialup modem), but it could in theory be used for fixing movies that were inverse-telecined incorrectly.\n\nA description of the accepted options follows.\n\nObtain the MSAD (Mean Sum of Absolute Differences) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained per component, average, min and max MSAD is printed through the logging system.\n\nThe filter stores the calculated MSAD of each frame in frame metadata.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nMultiply first video stream pixels values with second video stream pixels values.\n\nThe filter accepts the following options:\n\nThis filter supports same commands as options.\n\nIt accepts the following option:\n\nThis filter supports same commands as options.\n\nEach pixel is adjusted by looking for other pixels with similar contexts. This context similarity is defined by comparing their surrounding patches of size x . Patches are searched in an area of x around the pixel.\n\nNote that the research area defines centers for patches, which means some patches will be made of pixels outside that research area.\n\nThe filter accepts the following options.\n\nThis filter accepts the following options:\n\nThis filter supports same commands as options, excluding option.\n\nForce libavfilter not to use any of the specified pixel formats for the input to the next filter.\n\nIt accepts the following parameters:\n• Force libavfilter to use a format different from for the input to the vflip filter:\n• Convert the input video to any of the formats not contained in the list:\n\nThe filter accepts the following options:\n\nFor each channel of each frame, the filter computes the input range and maps it linearly to the user-specified output range. The output range defaults to the full dynamic range from pure black to pure white.\n\nTemporal smoothing can be used on the input range to reduce flickering (rapid changes in brightness) caused when small dark or bright objects enter or leave the scene. This is similar to the auto-exposure (automatic gain control) on a video camera, and, like a video camera, it may cause a period of over- or under-exposure of the video.\n\nThe R,G,B channels can be normalized independently, which may cause some color shifting, or linked together as a single channel, which prevents color shifting. Linked normalization preserves hue. Independent normalization does not, so it can be used to remove some color casts. Independent and linked normalization can be combined in any ratio.\n\nThe normalize filter accepts the following options:\n\nThis filter supports same commands as options, excluding option. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nStretch video contrast to use the full dynamic range, with no temporal smoothing; may flicker depending on the source content:\n\nAs above, but with 50 frames of temporal smoothing; flicker should be reduced, depending on the source content:\n\nAs above, but with hue-preserving linked channel normalization:\n\nAs above, but with half strength:\n\nMap the darkest input color to red, the brightest input color to cyan:\n\nPass the video source unchanged to the output.\n\nThis filter uses Tesseract for optical character recognition. To enable compilation of this filter, you need to configure FFmpeg with .\n\nIt accepts the following options:\n\nThe filter exports recognized text as the frame metadata . The filter exports confidence of recognized words as the frame metadata .\n\nTo enable this filter, install the libopencv library and headers and configure FFmpeg with .\n\nIt accepts the following parameters:\n\nRefer to the official libopencv documentation for more precise information: http://docs.opencv.org/master/modules/imgproc/doc/filtering.html\n\nSeveral libopencv filters are supported; see the following subsections.\n\nDilate an image by using a specific structuring element. It corresponds to the libopencv function .\n\nrepresents a structuring element, and has the syntax: x + x /\n\nand represent the number of columns and rows of the structuring element, and the anchor point, and the shape for the structuring element. must be \"rect\", \"cross\", \"ellipse\", or \"custom\".\n\nIf the value for is \"custom\", it must be followed by a string of the form \"= \". The file with name is assumed to represent a binary image, with each printable character corresponding to a bright pixel. When a custom is used, and are ignored, the number or columns and rows of the read file are assumed instead.\n\nThe default value for is \"3x3+0x0/rect\".\n\nspecifies the number of times the transform is applied to the image, and defaults to 1.\n\nErode an image by using a specific structuring element. It corresponds to the libopencv function .\n\nIt accepts the parameters: : , with the same syntax and semantics as the dilate filter.\n\nThe filter takes the following parameters: | | | | .\n\nis the type of smooth filter to apply, and must be one of the following values: \"blur\", \"blur_no_scale\", \"median\", \"gaussian\", or \"bilateral\". The default value is \"gaussian\".\n\nThe meaning of , , , and depends on the smooth type. and accept integer positive values or 0. and accept floating point values.\n\nThe default value for is 3. The default value for the other parameters is 0.\n\nThese parameters correspond to the parameters assigned to the libopencv function .\n\nUseful to measure spatial impulse, step responses, chroma delays, etc.\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options. The command accepts the same syntax of the corresponding option.\n\nIf the specified expression is not valid, it is kept at its current value.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nThe , and expressions can contain the following parameters.\n\nThis filter also supports the framesync options.\n\nNote that the , variables are available only when evaluation is done per frame, and will evaluate to NAN when is set to ‘ ’.\n\nBe aware that frames are taken from each input video in timestamp order, hence, if their initial timestamps differ, it is a good idea to pass the two inputs through a filter to have them begin in the same zero timestamp, as the example for the filter does.\n\nYou can chain together more overlays but you should test the efficiency of such approach.\n\nThis filter supports the following commands:\n• Draw the overlay at 10 pixels from the bottom right corner of the main video: Using named options the example above becomes:\n• Insert a transparent PNG logo in the bottom left corner of the input, using the tool with the option:\n• Insert 2 different transparent PNG logos (second logo on bottom right corner) using the tool:\n• Add a transparent color layer on top of the main video; must specify the size of the main input to the overlay filter:\n• Play an original video and a filtered version (here with the deshake filter) side by side using the tool: The above command is the same as:\n• Make a sliding overlay appearing from the left to the right top part of the screen starting since time 2:\n• Compose output by putting two input videos side to side:\n• Mask 10-20 seconds of a video by applying the delogo filter to a section\n\nThe filter accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following parameters:\n\nThe value for the , , , and options are expressions containing the following constants:\n• Add paddings with the color \"violet\" to the input video. The output video size is 640x480, and the top-left corner of the input video is placed at column 0, row 40 The example above is equivalent to the following command:\n• Pad the input to get an output with dimensions increased by 3/2, and put the input video at the center of the padded area:\n• Pad the input to get a squared output with size equal to the maximum value between the input width and height, and put the input video at the center of the padded area:\n• Pad the input to get a final w/h ratio of 16:9:\n• In case of anamorphic video, in order to set the output display aspect correctly, it is necessary to use in the expression, according to the relation: Thus the previous example needs to be modified to:\n• Double the output size and put the input video in the bottom-right corner of the output padded area:\n\nGenerate one palette for a whole video stream.\n\nIt accepts the following options:\n\nThe filter also exports the frame metadata ( ) which you can use to evaluate the degree of color quantization of the palette. This information is also visible at logging level.\n• Generate a representative palette of a given video using :\n\nUse a palette to downsample an input video stream.\n\nThe filter takes two inputs: one video stream and a palette. The palette must be a 256 pixels image.\n\nIt accepts the following options:\n• Use a palette (generated for example with palettegen) to encode a GIF using :\n\nCorrect perspective of video not recorded perpendicular to the screen.\n\nA description of the accepted parameters follows.\n\nDelay interlaced video by one field time so that the field order changes.\n\nThe intended use is to fix PAL movies that have been captured with the opposite field order to the film-to-video transfer.\n\nA description of the accepted parameters follows.\n\nThis filter supports the all above options as commands.\n\nReduce various flashes in video, so to help users with epilepsy.\n\nIt accepts the following options:\n\nPixel format descriptor test filter, mainly useful for internal testing. The output video should be equal to the input video.\n\ncan be used to test the monowhite pixel format descriptor definition.\n\nThe filter accepts the following options:\n\nThis filter supports all options as commands.\n\nDisplay sample values of color channels. Mainly useful for checking color and levels. Minimum supported resolution is 640x480.\n\nThe filters accept the following options:\n\nThis filter supports same commands as options.\n\nEnable the specified chain of postprocessing subfilters using libpostproc. This library should be automatically selected with a GPL build ( ). Subfilters must be separated by ’/’ and can be disabled by prepending a ’-’. Each subfilter and some options have a short and a long name that can be used interchangeably, i.e. dr/dering are the same.\n\nThe filters accept the following options:\n\nAll subfilters share common options to determine their scope:\n\nThese options can be appended after the subfilter name, separated by a ’|’.\n\nThe horizontal and vertical deblocking filters share the difference and flatness values so you cannot set different horizontal and vertical thresholds.\n• Apply deblocking on luma only, and switch vertical deblocking on or off automatically depending on available CPU time:\n\nApply Postprocessing filter 7. It is variant of the spp filter, similar to spp = 6 with 7 point DCT, where only the center sample is used after IDCT.\n\nThe filter accepts the following options:\n\nApply alpha premultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nEach of the expression options specifies the expression to use for computing the lookup table for the corresponding pixel component values.\n\nThe expressions can contain the following constants and functions:\n\nThis filter supports the all above options as commands.\n\nObtain the average, maximum and minimum PSNR (Peak Signal to Noise Ratio) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the PSNR.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average PSNR is printed through the logging system.\n\nThe filter stores the accumulated MSE (mean squared error) of each frame, and at the end of the processing it is averaged across all frames equally, and the following formula is applied to obtain the PSNR:\n\nWhere MAX is the average of the maximum values of each component of the image.\n\nThe description of the accepted parameters follows.\n\nThis filter also supports the framesync options.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nIf a greater than 1 is specified, a header line precedes the list of per-frame-pair stats, with key value pairs following the frame format with the following parameters:\n\nA description of each shown per-frame-pair parameter follows:\n• For example: On this example the input file being processed is compared with the reference file . The PSNR of each individual frame is stored in .\n• Another example with different containers:\n\nThe pullup filter is designed to take advantage of future context in making its decisions. This filter is stateless in the sense that it does not lock onto a pattern to follow, but it instead looks forward to the following fields in order to identify matches and rebuild progressive frames.\n\nTo produce content with an even framerate, insert the fps filter after pullup, use if the input frame rate is 29.97fps, for 30fps and the (rare) telecined 25fps input.\n\nThe filter accepts the following options:\n\nFor best results (without duplicated frames in the output file) it is necessary to change the output frame rate. For example, to inverse telecine NTSC input:\n\nThe filter accepts the following option:\n\nThe expression is evaluated through the eval API and can contain, among others, the following constants:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/), and overlay it on top of the current frame.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and overlayed into the video output according to the specified options.\n\nIn case no text is specified, no QR code is overlaied.\n\nThis filter accepts the following options:\n\nThe expressions set by the options contain the following constants and functions.\n\nIf is set to , the text is printed verbatim.\n\nIf is set to (which is the default), the following expansion mechanism is used.\n\nThe backslash character ‘ ’, followed by any character, always expands to the second character.\n\nSequences of the form are expanded. The text between the braces is a function name, possibly followed by arguments separated by ’:’. If the arguments contain special characters or delimiters (’:’ or ’}’), they should be escaped.\n\nNote that they probably must also be escaped as the value for the option in the filter argument string and as the filter argument in the filtergraph description, and possibly also for the shell, that makes up to four levels of escaping; using a text file with the option avoids these problems.\n\nThe following functions are available:\n• Generate a QR code encoding the specified text with the default size, overalaid in the top left corner of the input video, with the default size:\n• Same as below, but select blue on pink colors:\n• Place the QR code in the bottom right corner of the input video:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n• Make the QR code a fraction of the input video width:\n\nIdentify and decode a QR code using the libquirc library (see https://github.com/dlbeer/quirc/), and print the identified QR codes positions and payload as metadata.\n\nTo enable the compilation of this filter, you need to configure FFmpeg with .\n\nFor each found QR code in the input video, some metadata entries are added with the prefix , where is the index, starting from 0, associated to the QR code.\n\nA description of each metadata value follows:\n\nFlush video frames from internal cache of frames into a random order. No frame is discarded. Inspired by frei0r nervous filter.\n\nRead closed captioning (EIA-608) information from the top lines of a video frame.\n\nThis filter adds frame metadata for and , where is the number of the identified line with EIA-608 data (starting from 0). A description of each metadata value follows:\n\nThis filter accepts the following options:\n\nThis filter supports the all above options as commands.\n• Output a csv with presentation time and the first two lines of identified EIA-608 captioning data.\n\nRead vertical interval timecode (VITC) information from the top lines of a video frame.\n\nThe filter adds frame metadata key with the timecode value, if a valid timecode has been detected. Further metadata key is set to 0/1 depending on whether timecode data has been found or not.\n\nThis filter accepts the following options:\n• Detect and draw VITC data onto the video frame; if no valid VITC is detected, draw as a placeholder:\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 16bit depth, single channel.\n\nThe removegrain filter is a spatial denoiser for progressive video.\n\nRange of mode is from 0 to 24. Description of each mode follows:\n\nSuppress a TV station logo, using an image file to determine which pixels comprise the logo. It works by filling in the pixels that comprise the logo with neighboring pixels.\n\nThe filter accepts the following options:\n\nPixels in the provided bitmap image with a value of zero are not considered part of the logo, non-zero pixels are considered part of the logo. If you use white (255) for the logo and black (0) for the rest, you will be safe. For making the filter bitmap, it is recommended to take a screen capture of a black frame with the logo visible, and then using a threshold filter followed by the erode filter once or twice.\n\nIf needed, little splotches can be fixed manually. Remember that if logo pixels are not covered, the filter quality will be much reduced. Marking too many pixels as part of the logo does not hurt as much, but it will increase the amount of blurring needed to cover over the image and will destroy more information than necessary, and extra pixels will slow things down on a large logo.\n\nThis filter uses the repeat_field flag from the Video ES headers and hard repeats fields based on its value.\n\nWarning: This filter requires memory to buffer the entire clip, so trimming is suggested.\n• Take the first 5 seconds of a clip, and reverse it.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nRotate video by an arbitrary angle expressed in radians.\n\nThe filter accepts the following options:\n\nA description of the optional parameters follows.\n\nThe expressions for the angle and the output size can contain the following constants and functions:\n• Apply a constant rotation with period T, starting from an angle of PI/3:\n• Make the input video rotation oscillating with a period of T seconds and an amplitude of A radians:\n• Rotate the video, output size is chosen so that the whole rotating input video is always completely contained in the output:\n• Rotate the video, reduce the output size so that no background is ever shown:\n\nThe filter supports the following commands:\n\nThe filter accepts the following options:\n\nEach chroma option value, if not explicitly specified, is set to the corresponding luma option value.\n\nScale (resize) the input video, using the libswscale library.\n\nThe scale filter forces the output display aspect ratio to be the same of the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the scale filter will convert the input to the requested format.\n\nThe filter accepts the following options, any of the options supported by the libswscale scaler, as well as any of the framesync options.\n\nSee (ffmpeg-scaler)the ffmpeg-scaler manual for the complete list of scaler options.\n\nThe values of the and options are expressions containing the following constants:\n• Scale the input video to a size of 200x100 This is equivalent to:\n• Specify a size abbreviation for the output size: which can also be written as:\n• The above is the same as:\n• Scale the input to 2x with forced interlaced scaling:\n• Increase the width, and set the height to the same size:\n• Increase the height, and set the width to 3/2 of the height:\n• Increase the size, making the size a multiple of the chroma subsample values:\n• Increase the width to a maximum of 500 pixels, keeping the same aspect ratio as the input:\n• Make pixels square using reset_sar, making sure the resulting resolution is even (required by some codecs):\n• Scale to target exactly, however reset SAR to 1:\n• Scale to even dimensions that fit within 400x300, preserving input SAR:\n• Scale to produce square pixels with even dimensions that fit within 400x300:\n• Scale a subtitle stream (sub) to match the main video (main) in size before overlaying. (\"scale2ref\")\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nThis filter supports the following commands:\n\nScale and convert the color parameters using VTPixelTransferSession.\n\nThe filter accepts the following options:\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nThis filter sets frame metadata with mafd between frame, the scene score, and forward the frame to the next filter, so they can use these metadata to detect scene change or others.\n\nIn addition, this filter logs a message and sets frame metadata when it detects a scene change by .\n\nmetadata keys are set with mafd for every frame.\n\nmetadata keys are set with scene change score for every frame to detect scene change.\n\nmetadata keys are set with current filtered frame time which detect scene change with .\n\nThe filter accepts the following options:\n\nAdjust cyan, magenta, yellow and black (CMYK) to certain ranges of colors (such as \"reds\", \"yellows\", \"greens\", \"cyans\", ...). The adjustment range is defined by the \"purity\" of the color (that is, how saturated it already is).\n\nThis filter is similar to the Adobe Photoshop Selective Color tool.\n\nThe filter accepts the following options:\n\nAll the adjustment settings ( , , ...) accept up to 4 space separated floating point adjustment values in the [-1,1] range, respectively to adjust the amount of cyan, magenta, yellow and black for the pixels of its range.\n• Increase cyan by 50% and reduce yellow by 33% in every green areas, and increase magenta by 27% in blue areas:\n\nThe takes a frame-based video input and splits each frame into its components fields, producing a new half height clip with twice the frame rate and twice the frame count.\n\nThis filter use field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThe filter sets the Display Aspect Ratio for the filter output video.\n\nThis is done by changing the specified Sample (aka Pixel) Aspect Ratio, according to the following equation:\n\nKeep in mind that the filter does not modify the pixel dimensions of the video frame. Also, the display aspect ratio set by this filter may be changed by later filters in the filterchain, e.g. in case of scaling or if another \"setdar\" or a \"setsar\" filter is applied.\n\nThe filter sets the Sample (aka Pixel) Aspect Ratio for the filter output video.\n\nNote that as a consequence of the application of this filter, the output display aspect ratio will change according to the equation above.\n\nKeep in mind that the sample aspect ratio set by the filter may be changed by later filters in the filterchain, e.g. if another \"setsar\" or a \"setdar\" filter is applied.\n\nIt accepts the following parameters:\n\nThe parameter is an expression containing the following constants:\n• To change the display aspect ratio to 16:9, specify one of the following:\n• To change the sample aspect ratio to 10:11, specify:\n• To set a display aspect ratio of 16:9, and specify a maximum integer value of 1000 in the aspect ratio reduction, use the command:\n\nThe filter marks the interlace type field for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters (e.g. or ).\n\nThe filter accepts the following options:\n\nThe filter marks interlace and color range for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by filters/encoders.\n\nThis filter supports the following options:\n\nThis filter supports the all above options as commands.\n\nShow a line containing various information for each input video frame. The input video is not modified.\n\nThis filter supports the following options:\n\nThe shown line contains a sequence of key/value pairs of the form : .\n\nThe following values are shown in the output:\n\nDisplays the 256 colors palette of each frame. This filter is only relevant for pixel format frames.\n\nIt accepts the following option:\n\nIt accepts the following parameters:\n\nThe first frame has the index 0. The default is to keep the input unchanged.\n• Swap second and third frame of every three frames of the input:\n• Swap 10th and 1st frame of every ten frames of the input:\n\nThis filter accepts the following options:\n\nIt accepts the following parameters:\n\nThe first plane has the index 0. The default is to keep the input unchanged.\n• Swap the second and third planes of the input:\n\nEvaluate various visual metrics that assist in determining issues associated with the digitization of analog video media.\n\nBy default the filter will log these metadata values:\n\nThe filter accepts the following options:\n• Output specific data about the minimum and maximum values of the Y plane per frame:\n• Playback video while highlighting pixels that are outside of broadcast range in red.\n• Playback video with signalstats metadata drawn over the frame. The contents of signalstat_drawtext.txt used in the command are:\n\nCalculates the MPEG-7 Video Signature. The filter can handle more than one input. In this case the matching between the inputs can be calculated additionally. The filter always passes through the first input. The signature of each stream can be written into a file.\n\nIt accepts the following options:\n• To calculate the signature of an input video and store it in signature.bin:\n• To detect whether two videos match and store the signatures in XML format in signature0.xml and signature1.xml:\n\nCalculate Spatial Information (SI) and Temporal Information (TI) scores for a video, as defined in ITU-T Rec. P.910 (11/21): Subjective video quality assessment methods for multimedia applications. Available PDF at https://www.itu.int/rec/T-REC-P.910-202111-S/en. Note that this is a legacy implementation that corresponds to a superseded recommendation. Refer to ITU-T Rec. P.910 (07/22) for the latest version: https://www.itu.int/rec/T-REC-P.910-202207-I/en\n\nIt accepts the following option:\n\nBlur the input video without impacting the outlines.\n\nIt accepts the following options:\n\nIf a chroma or alpha option is not explicitly set, the corresponding luma value is set.\n\nThe filter accepts the following option:\n\nThis filter supports the all above options as commands.\n\nApply a simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe filter accepts the following options:\n\nThis filter supports the following commands:\n\nScale the input by applying one of the super-resolution methods based on convolutional neural networks. Supported models:\n\nTraining scripts as well as scripts for model file (.pb) saving can be found at https://github.com/XueweiMeng/sr/tree/sr_dnn_native. Original repository is at https://github.com/HighVoltageRocknRoll/sr.git.\n\nThe filter accepts the following options:\n\nTo get full functionality (such as async execution), please use the dnn_processing filter.\n\nUpscale (size increasing) for the input video using AMD Advanced Media Framework library for hardware acceleration. Use advanced algorithms for upscaling with higher output quality. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n\nObtain the SSIM (Structural SImilarity Metric) between two input videos.\n\nThis filter takes in input two input videos, the first input is considered the \"main\" source and is passed unchanged to the output. The second input is used as a \"reference\" video for computing the SSIM.\n\nBoth video inputs must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe filter stores the calculated SSIM of each frame.\n\nThe description of the accepted parameters follows.\n\nThe file printed if is selected, contains a sequence of key/value pairs of the form : for each compared couple of frames.\n\nA description of each shown parameter follows:\n\nThis filter also supports the framesync options.\n• For example: On this example the input file being processed is compared with the reference file . The SSIM of each individual frame is stored in .\n• Another example with both psnr and ssim at same time:\n• Another example with different containers:\n\nThe filters accept the following options:\n• Convert input video from side by side parallel to anaglyph yellow/blue dubois:\n• Convert input video from above below (left eye above, right eye below) to side by side crosseye.\n\nThe filter accepts the following options:\n\nThe and filter supports the following commands:\n• Select first 5 seconds 1st stream and rest of time 2nd stream:\n• Same as above, but for audio:\n\nDraw subtitles on top of input video using the libass library.\n\nTo enable compilation of this filter you need to configure FFmpeg with . This filter also requires a build with libavcodec and libavformat to convert the passed subtitles file to ASS (Advanced Substation Alpha) subtitles format.\n\nThe filter accepts the following options:\n\nIf the first key is not specified, it is assumed that the first value specifies the .\n\nFor example, to render the file on top of the input video, use the command:\n\nwhich is equivalent to:\n\nTo render the default subtitles stream from file , use:\n\nTo render the second subtitles stream from that file, use:\n\nTo make the subtitles stream from appear in 80% transparent blue , use:\n\nScale the input by 2x and smooth using the Super2xSaI (Scale and Interpolate) pixel art scaling algorithm.\n\nUseful for enlarging pixel art images without reducing sharpness.\n\nThis filter accepts the following options:\n\nThe all options are expressions containing the following constants:\n\nThis filter supports the all above options as commands.\n\nThis filter accepts the following options:\n\nCompute and draw a color distribution histogram for the input video across time.\n\nUnlike histogram video filter which only shows histogram of single input frame at certain time, this filter shows also past histograms of number of frames defined by option.\n\nThe computed histogram is a representation of the color component distribution in an image.\n\nThe filter accepts the following options:\n\nThis filter needs four video streams to perform thresholding. First stream is stream we are filtering. Second stream is holding threshold values, third stream is holding min values, and last, fourth stream is holding max values.\n\nThe filter accepts the following option:\n\nFor example if first stream pixel’s component value is less then threshold value of pixel component from 2nd threshold stream, third stream value will picked, otherwise fourth stream pixel component value will be picked.\n\nUsing color source filter one can perform various types of thresholding:\n\nThis filter supports the all options as commands.\n• Threshold to zero, using gray color as threshold:\n• Inverted threshold to zero, using gray color as threshold:\n\nSelect the most representative frame in a given sequence of consecutive frames.\n\nThe filter accepts the following options:\n\nSince the filter keeps track of the whole frames sequence, a bigger value will result in a higher memory usage, so a high value is not recommended.\n• Complete example of a thumbnail creation with :\n\nThe untile filter can do the reverse.\n\nThe filter accepts the following options:\n• Produce 8x8 PNG tiles of all keyframes ( ) in a movie: The is necessary to prevent from duplicating each output frame to accommodate the originally detected frame rate.\n• Display pictures in an area of frames, with pixels between them, and pixels of initial margin, using mixed flat and named options:\n\nWhat happens when you invert time and space?\n\nNormally a video is composed of several frames that represent a different instant of time and shows a scene that evolves in the space captured by the frame. This filter is the antipode of that concept, taking inspiration from tilt and shift photography.\n\nA filtered frame contains the whole timeline of events composing the sequence, and this is obtained by placing a slice of pixels from each frame into a single one. However, since there are no infinite-width frames, this is done up the width of the input frame, and a video is recomposed by shifting away one column for each subsequent frame. In order to map space to time, the filter tilts each input frame as well, so that motion is preserved. This is accomplished by progressively selecting a different column from each input frame.\n\nThe end result is a sort of inverted parallax, so that far away objects move much faster that the ones in the front. The ideal conditions for this video effect are when there is either very little motion and the backgroud is static, or when there is a lot of motion and a very wide depth of field (e.g. wide panorama, while moving on a train).\n\nThe filter accepts the following parameters:\n\nNormally the filter shifts and tilts from the very first frame, and stops when the last one is received. However, before filtering starts, normal video may be preseved, so that the effect is slowly shifted in its place. Similarly, the last video frame may be reconstructed at the end. Alternatively it is possible to just start and end with black.\n\nFrames are counted starting from 1, so the first input frame is considered odd.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nMidway Video Equalization adjusts a sequence of video frames to have the same histograms, while maintaining their dynamics as much as possible. It’s useful for e.g. matching exposures from a video frames sequence.\n\nThis filter accepts the following option:\n\nA description of the accepted options follows.\n• Similar as above but only showing temporal differences:\n\nThis filter supports the following commands:\n\nThis filter expects data in single precision floating point, as it needs to operate on (and can output) out-of-range values. Another filter, such as zscale, is needed to convert the resulting frame to a usable format.\n\nThe tonemapping algorithms implemented only work on linear light, so input data should be linearized beforehand (and possibly correctly tagged).\n\nThe filter accepts the following options.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it.\n\nIt accepts the following parameters:\n\nFor example to rotate by 90 degrees clockwise and preserve portrait layout:\n\nThe command above can also be specified as:\n\nTrim the input so that the output contains one continuous subpart of the input.\n\nIt accepts the following parameters:\n\n, , and are expressed as time duration specifications; see (ffmpeg-utils)the Time duration section in the ffmpeg-utils(1) manual for the accepted syntax.\n\nNote that the first two sets of the start/end options and the option look at the frame timestamp, while the _frame variants simply count the frames that pass through the filter. Also note that this filter does not modify the timestamps. If you wish for the output timestamps to start at zero, insert a setpts filter after the trim filter.\n\nIf multiple start or end options are set, this filter tries to be greedy and keep all the frames that match at least one of the specified constraints. To keep only the part that matches all the constraints at once, chain multiple trim filters.\n\nThe defaults are such that all the input is kept. So it is possible to set e.g. just the end values to keep everything before the specified time.\n• Drop everything except the second minute of input:\n• Keep only the first second:\n\nApply alpha unpremultiply effect to input video stream using first plane of second stream as alpha.\n\nBoth streams must have same dimensions and same pixel format.\n\nThe filter accepts the following option:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nDecompose a video made of tiled images into the individual images.\n\nThe frame rate of the output video is the frame rate of the input video multiplied by the number of tiles.\n\nThis filter does the reverse of tile.\n\nThe filter accepts the following options:\n• Produce a 1-second video from a still image file made of 25 frames stacked vertically, like an analogic film reel:\n\nApply ultra slow/simple postprocessing filter that compresses and decompresses the image at several (or - in the case of level - all) shifts and average the results.\n\nThe way this differs from the behavior of spp is that uspp actually encodes & decodes each case with libavcodec Snow, whereas spp uses a simplified intra only 8x8 DCT similar to MJPEG.\n\nThis filter is not available in ffmpeg versions between 5.0 and 6.0.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Convert equirectangular video to cubemap with 3x2 layout and 1% padding using bicubic interpolation:\n• Convert transposed and horizontally flipped Equi-Angular Cubemap in side-by-side stereo format to equirectangular top-bottom stereo format:\n\nThis filter supports subset of above options as commands.\n\nIt transforms each frame from the video input into the wavelet domain, using Cohen-Daubechies-Feauveau 9/7. Then it applies some filtering to the obtained coefficients. It does an inverse wavelet transform after. Due to wavelet properties, it should give a nice smoothed result, and reduced noise, without blurring picture features.\n\nThis filter accepts the following options:\n\nApply variable blur filter by using 2nd video stream to set blur radius. The 2nd stream must have the same dimensions.\n\nThis filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nThis filter supports all the above options as commands.\n\nDisplay 2 color component values in the two dimensional graph (which is called a vectorscope).\n\nThis filter accepts the following options:\n\nAnalyze video stabilization/deshaking. Perform pass 1 of 2, see vidstabtransform for pass 2.\n\nThis filter generates a file with relative translation and rotation transform information about subsequent frames, which is then used by the vidstabtransform filter.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n\nThis filter accepts the following options:\n• Analyze strongly shaky movie and put the results in file :\n• Visualize the result of internal transformations in the resulting video:\n\nVideo stabilization/deshaking: pass 2 of 2, see vidstabdetect for pass 1.\n\nRead a file with transform information for each frame and apply/compensate them. Together with the vidstabdetect filter this can be used to deshake videos. See also http://public.hronopik.de/vid.stab. It is important to also use the unsharp filter, see below.\n\nTo enable compilation of this filter you need to configure FFmpeg with .\n• Use for a typical stabilization with default values: Note the use of the unsharp filter which is always recommended.\n• Zoom in a bit more and load transform data from a given file:\n• Smoothen the video even more:\n\nFor example, to vertically flip a video with :\n\nThis filter tries to detect if the input is variable or constant frame rate.\n\nAt end it will output number of frames detected as having variable delta pts, and ones with constant delta pts. If there was frames with variable delta, than it will also show min, max and average delta encountered.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands.\n\nObtain the average VIF (Visual Information Fidelity) between two input videos.\n\nBoth input videos must have the same resolution and pixel format for this filter to work correctly. Also it assumes that both inputs have the same number of frames, which are compared one by one.\n\nThe obtained average VIF score is printed through the logging system.\n\nThe filter stores the calculated VIF score of each frame.\n\nThis filter also supports the framesync options.\n\nIn the below example the input file being processed is compared with the reference file .\n\nThe filter accepts the following options:\n\nThe , and expressions can contain the following parameters.\n\nObtain the average VMAF motion score of a video. It is one of the component metrics of VMAF.\n\nThe obtained average motion score is printed through the logging system.\n\nThe filter accepts the following options:\n\nScale (resize) and convert colorspace, transfer characteristics or color primaries for the input video, using AMD Advanced Media Framework library for hardware acceleration. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Upscale to 4K and change color profile to bt2020.\n\nAll streams must be of same pixel format and of same width.\n\nNote that this filter is faster than using overlay and pad filter to create same output.\n\nThe filter accepts the following options:\n\nBased on the process described by Martin Weston for BBC R&D, and implemented based on the de-interlace algorithm written by Jim Easterbrook for BBC R&D, the Weston 3 field deinterlacing filter uses filter coefficients calculated by BBC R&D.\n\nThis filter uses field-dominance information in frame to decide which of each pair of fields to place first in the output. If it gets it wrong use setfield filter before filter.\n\nThere are two sets of filter coefficients, so called \"simple\" and \"complex\". Which set of filter coefficients is used can be set by passing an optional parameter:\n\nThis filter supports same commands as options.\n\nThe waveform monitor plots color component intensity. By default luma only. Each column of the waveform corresponds to a column of pixels in the source video.\n\nIt accepts the following options:\n\nThe takes a field-based video input and join each two sequential fields into single frame, producing a new double height clip with half the frame rate and half the frame count.\n\nThe works same as but without halving frame rate and frame count.\n\nIt accepts the following option:\n\nApply the xBR high-quality magnification filter which is designed for pixel art. It follows a set of edge-detection rules, see https://forums.libretro.com/t/xbr-algorithm-tutorial/123.\n\nIt accepts the following option:\n\nApply normalized cross-correlation between first and second input video stream.\n\nSecond input video stream dimensions must be lower than first input video stream.\n\nThe filter accepts the following options:\n\nThe filter also supports the framesync options.\n\nApply cross fade from one input video stream to another input video stream. The cross fade is applied for specified duration.\n\nBoth inputs must be constant frame-rate and have the same resolution, pixel format, frame rate and timebase.\n\nThe filter accepts the following options:\n• Cross fade from one input video to another input video, with fade transition and duration of transition of 2 seconds starting at offset of 5 seconds:\n\nThe filter accepts the following options:\n\nThis filter supports all above options as commands, excluding option .\n\nObtain the average (across all input frames) and minimum (across all color plane averages) eXtended Perceptually weighted peak Signal-to-Noise Ratio (XPSNR) between two input videos.\n\nThe XPSNR is a low-complexity psychovisually motivated distortion measurement algorithm for assessing the difference between two video streams or images. This is especially useful for objectively quantifying the distortions caused by video and image codecs, as an alternative to a formal subjective test. The logarithmic XPSNR output values are in a similar range as those of traditional psnr assessments but better reflect human impressions of visual coding quality. More details on the XPSNR measure, which essentially represents a blockwise weighted variant of the PSNR measure, can be found in the following freely available papers:\n• C. R. Helmrich, M. Siekmann, S. Becker, S. Bosse, D. Marpe, and T. Wiegand, \"XPSNR: A Low-Complexity Extension of the Perceptually Weighted Peak Signal-to-Noise Ratio for High-Resolution Video Quality Assessment,\" in Proc. IEEE Int. Conf. Acoustics, Speech, Sig. Process. (ICASSP), virt./online, May 2020. www.ecodis.de/xpsnr.htm\n• C. R. Helmrich, S. Bosse, H. Schwarz, D. Marpe, and T. Wiegand, \"A Study of the Extended Perceptually Weighted Peak Signal-to-Noise Ratio (XPSNR) for Video Compression with Different Resolutions and Bit Depths,\" ITU Journal: ICT Discoveries, vol. 3, no. 1, pp. 65 - 72, May 2020. http://handle.itu.int/11.1002/pub/8153d78b-en\n\nWhen publishing the results of XPSNR assessments obtained using, e.g., this FFmpeg filter, a reference to the above papers as a means of documentation is strongly encouraged. The filter requires two input videos. The first input is considered a (usually not distorted) reference source and is passed unchanged to the output, whereas the second input is a (distorted) test signal. Except for the bit depth, these two video inputs must have the same pixel format. In addition, for best performance, both compared input videos should be in YCbCr color format.\n\nThe obtained overall XPSNR values mentioned above are printed through the logging system. In case of input with multiple color planes, we suggest reporting of the minimum XPSNR average.\n\nThe following parameter, which behaves like the one for the psnr filter, is accepted:\n\nThis filter also supports the framesync options.\n• XPSNR analysis of two 1080p HD videos, ref_source.yuv and test_video.yuv, both at 24 frames per second, with color format 4:2:0, bit depth 8, and output of a logfile named \"xpsnr.log\":\n• XPSNR analysis of two 2160p UHD videos, ref_source.yuv with bit depth 8 and test_video.yuv with bit depth 10, both at 60 frames per second with color format 4:2:0, no logfile output:\n\nAll streams must be of same pixel format.\n\nThe filter accepts the following options:\n• Display 4 inputs into 2x2 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 4 inputs into 1x4 grid. Note that if inputs are of different widths, unused space will appear.\n• Display 9 inputs into 3x3 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n• Display 16 inputs into 4x4 grid. Note that if inputs are of different sizes, gaps or overlaps may occur.\n\nDeinterlace the input video (\"yadif\" means \"yet another deinterlacing filter\").\n\nIt accepts the following parameters:\n\nApply blur filter while preserving edges (\"yaepblur\" means \"yet another edge preserving blur filter\"). The algorithm is described in \"J. S. Lee, Digital image enhancement and noise filtering by use of local statistics, IEEE Trans. Pattern Anal. Mach. Intell. PAMI-2, 1980.\"\n\nIt accepts the following parameters:\n\nThis filter supports same commands as options.\n\nThis filter accepts the following options:\n\nEach expression can contain the following constants:\n• Zoom in up to 1.5x and pan at same time to some spot near center of picture:\n• Zoom in up to 1.5x and pan always at center of picture:\n• Same as above but without pausing:\n• Zoom in 2x into center of picture only for the first second of the input video:\n\nScale (resize) the input video, using the z.lib library: https://github.com/sekrit-twc/zimg. To enable compilation of this filter, you need to configure FFmpeg with .\n\nThe zscale filter forces the output display aspect ratio to be the same as the input, by changing the output sample aspect ratio.\n\nIf the input image format is different from the format requested by the next filter, the zscale filter will convert the input to the requested format.\n\nThe filter accepts the following options.\n\nThe values of the and options are expressions containing the following constants:\n\nThis filter supports the following commands:\n\nTo enable CUDA and/or NPP filters please refer to configuration guidelines for CUDA and for CUDA NPP filters.\n\nRunning CUDA filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of initializing second CUDA device on the system and running scale_cuda and bilateral_cuda filters.\n\nSince CUDA filters operate exclusively on GPU memory, frame data must sometimes be uploaded (hwupload) to hardware surfaces associated with the appropriate CUDA device before processing, and downloaded (hwdownload) back to normal memory afterward, if required. Whether hwupload or hwdownload is necessary depends on the specific workflow:\n• If the input frames are already in GPU memory (e.g., when using or ), explicit use of hwupload is not needed, as the data is already in the appropriate memory space.\n• If the input frames are in CPU memory (e.g., software-decoded frames or frames processed by CPU-based filters), it is necessary to use hwupload to transfer the data to GPU memory for CUDA processing.\n• If the output of the CUDA filters needs to be further processed by software-based filters or saved in a format not supported by GPU-based encoders, hwdownload is required to transfer the data back to CPU memory.\n\nNote that hwupload uploads data to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before hwupload to ensure the input is in the correct format. Similarly, hwdownload may not support all output formats, so an additional format filter may need to be inserted immediately after hwdownload in the filter graph to ensure compatibility.\n\nBelow is a description of the currently available Nvidia CUDA video filters.\n\nNote: If FFmpeg detects the Nvidia CUDA Toolkit during configuration, it will enable CUDA filters automatically without requiring any additional flags. If you want to explicitly enable them, use the following options:\n• Configure FFmpeg with . Additional requirement: lib must be installed.\n\nCUDA accelerated bilateral filter, an edge preserving filter. This filter is mathematically accurate thanks to the use of GPU acceleration. For best output quality, use one to one chroma subsampling, i.e. yuv444p format.\n\nThe filter accepts the following options:\n\nDeinterlace the input video using the bwdif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nThis filter works like normal chromakey filter but operates on CUDA frames. for more details and parameters see chromakey.\n• Make all the green pixels in the input video transparent and use it as an overlay for another video:\n\nIt is by no means feature complete compared to the software colorspace filter, and at the current time only supports color range conversion between jpeg/full and mpeg/limited range.\n\nThe filter accepts the following options:\n\nOverlay one video on top of another.\n\nThis is the CUDA variant of the overlay filter. It only accepts CUDA frames. The underlying input pixel formats have to match.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nIt accepts the following parameters:\n\nThis filter also supports the framesync options.\n\nScale (resize) and convert (pixel format) the input video, using accelerated CUDA kernels. Setting the output width and height works in the same way as for the scale filter.\n\nThe filter accepts the following options:\n• Scale input to 720p, keeping aspect ratio and ensuring the output is yuv420p.\n• Don’t do any conversion or scaling, but copy all input frames into newly allocated ones. This can be useful to deal with a filter and encode chain that otherwise exhausts the decoders frame pool.\n\nDeinterlace the input video using the yadif algorithm, but implemented in CUDA so that it can work as part of a GPU accelerated pipeline with nvdec and/or nvenc.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available NVIDIA Performance Primitives (libnpp) video filters.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform scaling and/or pixel format conversion on CUDA video frames. Setting the output width and height works in the same way as for the filter.\n\nThe following additional options are accepted:\n\nThe values of the and options are expressions containing the following constants:\n\nUse the NVIDIA Performance Primitives (libnpp) to scale (resize) the input video, based on a reference video.\n\nSee the scale_npp filter for available options, scale2ref_npp supports the same but uses the reference video instead of the main input as basis. scale2ref_npp also supports the following additional constants for the and options:\n• Scale a subtitle stream (b) to match the main video (a) in size before overlaying\n• Scale a logo to 1/10th the height of a video, while preserving its display aspect ratio.\n\nUse the NVIDIA Performance Primitives (libnpp) to perform image sharpening with border control.\n\nThe following additional options are accepted:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available OpenCL video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nRunning OpenCL filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device on the second platform and running avgblur_opencl filter with default parameters on it.\n\nSince OpenCL filters are not able to access frame data in normal memory, all frame data needs to be uploaded(hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded(hwdownload) back to normal memory. Note that hwupload will upload to a surface with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it may be necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nThe filter accepts the following options:\n• Apply average blur filter with horizontal and vertical size of 3, setting each pixel of the output to the average value of the 7x7 region centered on it in the input. For pixels on the edges of the image, the region does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n\nIt accepts the following parameters:\n\nA description of the accepted options follows.\n\nApply boxblur filter, setting each pixel of the output to the average value of box-radiuses , , for each plane respectively. The filter will apply , , times onto the corresponding plane. For pixels on the edges of the image, the radius does not extend beyond the image boundaries, and so out-of-range coordinates are not used in the calculations.\n• Apply a boxblur filter with the luma, chroma, and alpha radius set to 2 and luma, chroma, and alpha power set to 3. The filter will run 3 times with box-radius set to 2 for every plane of the image.\n• Apply a boxblur filter with luma radius set to 2, luma_power to 1, chroma_radius to 4, chroma_power to 5, alpha_radius to 3 and alpha_power to 7. For the luma plane, a 2x2 box radius will be run once. For the chroma plane, a 4x4 box radius will be run 5 times. For the alpha plane, a 3x3 box radius will be run 7 times.\n\nThe filter accepts the following options:\n• Make every semi-green pixel in the input transparent with some slight blending:\n\nThe filter accepts the following options:\n\nThis filter replaces the pixel by the local(3x3) minimum.\n\nIt accepts the following options:\n• Apply erosion filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local minimum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local minimum is more then threshold of the corresponding plane, output pixel will be set to input pixel - threshold of corresponding plane.\n\nThe filter accepts the following options:\n• Stabilize a video with debugging (both in console and in rendered video):\n\nThis filter replaces the pixel by the local(3x3) maximum.\n\nIt accepts the following options:\n• Apply dilation filter with threshold0 set to 30, threshold1 set 40, threshold2 set to 50 and coordinates set to 231, setting each pixel of the output to the local maximum between pixels: 1, 2, 3, 6, 7, 8 of the 3x3 region centered on it in the input. If the difference between input pixel and local maximum is more then threshold of the corresponding plane, output pixel will be set to input pixel + threshold of corresponding plane.\n\nNon-local Means denoise filter through OpenCL, this filter accepts same options as nlmeans.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires same memory layout for all the inputs. So, format conversion may be needed.\n\nThe filter accepts the following options:\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs are yuv420p format.\n• The inputs have same memory layout for color channels , the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nThe filter accepts the following option:\n• Apply the Prewitt operator with scale set to 2 and delta set to 10.\n\nThe filter also supports the framesync options.\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• Frame index, . This is a counter starting from zero and increasing by one for each frame.\n• Source images, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Copy the input to the output (output must be the same size as the input).\n• Apply a simple transformation, rotating the input by an amount increasing with the index counter. Pixel values are linearly interpolated by the sampler, and the output need not have the same dimensions as the input.\n• Blend two inputs together, with the amount of each input used varying with the index counter.\n\nDestination pixel at position (X, Y) will be picked from source (x, y) position where x = Xmap(X, Y) and y = Ymap(X, Y). If mapping values are out of range, zero value for pixel will be used for destination pixel.\n\nXmap and Ymap input video streams must be of same dimensions. Output video stream will have Xmap/Ymap video stream dimensions. Xmap and Ymap input video streams are 32bit float pixel format, single channel.\n\nThe filter accepts the following option:\n• Apply the Roberts cross operator with scale set to 2 and delta set to 10\n\nThe filter accepts the following option:\n• Apply sobel operator with scale set to 2 and delta set to 10\n\nIt accepts the following parameters:\n\nIt accepts the following parameters:\n\nAll parameters are optional and default to the equivalent of the string ’5:5:1.0:5:5:0.0’.\n• Apply a strong blur of both luma and chroma parameters:\n\nCross fade two videos with custom transition effect by using OpenCL.\n\nIt accepts the following options:\n\nThe program source file must contain a kernel function with the given name, which will be run once for each plane of the output. Each run on a plane gets enqueued as a separate 2D global NDRange with one work-item for each pixel to be generated. The global ID offset for each work-item is therefore the coordinates of a pixel in the destination image.\n\nThe kernel function needs to take the following arguments:\n• Destination image, . This image will become the output; the kernel should write all of it.\n• First Source image, . Second Source image, . These are the most recent images on each input. The kernel may read from them to generate the output, but they can’t be written to.\n• Transition progress, . This value is always between 0 and 1 inclusive.\n\nVAAPI Video filters are usually used with VAAPI decoder and VAAPI encoder. Below is a description of VAAPI video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with .\n\nTo use vaapi filters, you need to setup the vaapi device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/VAAPI\n\nOverlay one video on the top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid.\n\nThe filter accepts the following options:\n\nThis filter also supports the framesync options.\n• Overlay an image LOGO at the top-left corner of the INPUT video. Both inputs for this filter are yuv420p format.\n• Overlay an image LOGO at the offset (200, 100) from the top-left corner of the INPUT video. The inputs have same memory layout for color channels, the overlay has additional alpha plane, like INPUT is yuv420p, and the LOGO is yuva420p.\n\nPerform HDR-to-SDR or HDR-to-HDR tone-mapping. It currently only accepts HDR10 as input.\n\nIt accepts the following parameters:\n\nThis is the VA-API variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the VA-API variant of the xstack filter, each input stream may have different size, this filter will scale down/up each input stream to the given output size, or the size of the first input stream.\n\nIt accepts the following options:\n\nAdd paddings to the input image, and place the original input at the provided , coordinates.\n\nIt accepts the following options:\n\nThe value for the , , , and options are expressions containing the following constants:\n\nIt accepts the following parameters:\n\nThe parameters for , , and and are expressions containing the following constants:\n• Draw a black box around the edge of the input image:\n• Draw a box with color red and an opacity of 50%: The previous example can be specified as:\n\nBelow is a description of the currently available Vulkan video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with and either or .\n\nRunning Vulkan filters requires you to initialize a hardware device and to pass that device to all filters in any filter graph.\n\nFor more detailed information see https://www.ffmpeg.org/ffmpeg.html#Advanced-Video-options\n• Example of choosing the first device and running nlmeans_vulkan filter with default parameters on it.\n\nAs Vulkan filters are not able to access frame data in normal memory, all frame data needs to be uploaded (hwupload) to hardware surfaces connected to the appropriate device before being used and then downloaded (hwdownload) back to normal memory. Note that hwupload will upload to a frame with the same layout as the software frame, so it may be necessary to add a format filter immediately before to get the input into the right format and hwdownload does not support all formats on the output - it is usually necessary to insert an additional format filter immediately following in the graph to get the output in a supported format.\n\nApply an average blur filter, implemented on the GPU using Vulkan.\n\nThe filter accepts the following options:\n\nBlend two Vulkan frames into each other.\n\nThe filter takes two input streams and outputs one stream, the first input is the \"top\" layer and second input is \"bottom\" layer. By default, the output terminates when the longest input terminates.\n\nA description of the accepted options follows.\n\nDeinterlacer using bwdif, the \"Bob Weaver Deinterlacing Filter\" algorithm, implemented on the GPU using Vulkan.\n\nIt accepts the following parameters:\n\nApply an effect that emulates chromatic aberration. Works best with RGB inputs, but provides a similar effect with YCbCr inputs too.\n\nVideo source that creates a Vulkan frame of a solid color. Useful for benchmarking, or overlaying.\n\nIt accepts the following parameters:\n\nFlips an image along both the vertical and horizontal axis.\n\nThe filter accepts the following options:\n\nDenoise frames using Non-Local Means algorithm, implemented on the GPU using Vulkan. Supports more pixel formats than nlmeans or nlmeans_opencl, including alpha channel support.\n\nThe filter accepts the following options.\n\nOverlay one video on top of another.\n\nIt takes two inputs and has one output. The first input is the \"main\" video on which the second input is overlaid. This filter requires all inputs to use the same pixel format. So, format conversion may be needed.\n\nThe filter accepts the following options:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nTranspose rows with columns in the input video and optionally flip it. For more in depth examples see the transpose video filter, which shares mostly the same options.\n\nIt accepts the following parameters:\n\nBelow is a description of the currently available QSV video filters.\n\nTo enable compilation of these filters you need to configure FFmpeg with or .\n\nTo use QSV filters, you need to setup the QSV device correctly. For more information, please read https://trac.ffmpeg.org/wiki/Hardware/QuickSync\n\nThis is the QSV variant of the hstack filter, each input stream may have different height, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the vstack filter, each input stream may have different width, this filter will scale down/up each input stream while keeping the original aspect.\n\nIt accepts the following options:\n\nThis is the QSV variant of the xstack filter.\n\nIt accepts the following options:\n\nBelow is a description of the currently available video sources.\n\nBuffer video frames, and make them available to the filter chain.\n\nThis source is mainly intended for a programmatic use, in particular through the interface defined in .\n\nIt accepts the following parameters:\n\nwill instruct the source to accept video frames with size 320x240 and with format \"yuv410p\", assuming 1/24 as the timestamps timebase and square pixels (1:1 sample aspect ratio). Since the pixel format with name \"yuv410p\" corresponds to the number 6 (check the enum AVPixelFormat definition in ), this example corresponds to:\n\nAlternatively, the options can be specified as a flat string, but this syntax is deprecated:\n\nThe initial state of the cellular automaton can be defined through the and options. If such options are not specified an initial state is created randomly.\n\nAt each new frame a new row in the video is filled with the result of the cellular automaton next generation. The behavior when the whole frame is filled is defined by the option.\n\nThis source accepts the following options:\n• Read the initial state from , and specify an output of size 200x400.\n• Generate a random initial row with a width of 200 cells, with a fill ratio of 2/3:\n• Create a pattern generated by rule 18 starting by a single alive cell centered on an initial row with width 100:\n\nVideo source generated on GPU using Apple’s CoreImage API on OSX.\n\nThis video source is a specialized version of the coreimage video filter. Use a core image generator at the beginning of the applied filterchain to generate the content.\n\nThe coreimagesrc video source accepts the following options:\n\nAdditionally, all options of the coreimage video filter are accepted. A complete filterchain can be used for further processing of the generated input without CPU-HOST transfer. See coreimage documentation and examples for details.\n• Use CIQRCodeGenerator to create a QR code for the FFmpeg homepage, given as complete and escaped command-line for Apple’s standard bash shell: This example is equivalent to the QRCode example of coreimage without the need for a nullsrc video source.\n\nThe filter exclusively returns D3D11 Hardware Frames, for on-gpu encoding or processing. So an explicit hwdownload is needed for any kind of software processing.\n\nIt accepts the following options:\n\nYou can also skip the lavfi device and directly use the filter. Also demonstrates downloading the frame and encoding with libx264. Explicit output format specification is required in this case:\n\nIf you want to capture only a subsection of the desktop, this can be achieved by specifying a smaller size and its offsets into the screen:\n\nThis source supports the some above options as commands.\n\nGenerate a Mandelbrot set fractal, and progressively zoom towards the point specified with and .\n\nThis source accepts the following options:\n\nGenerate various test patterns, as generated by the MPlayer test filter.\n\nThe size of the generated video is fixed, and is 256x256. This source is useful in particular for testing encoding features.\n\nThis source accepts the following options:\n\nTo enable compilation of this filter you need to install the frei0r header and configure FFmpeg with .\n\nThis source accepts the following parameters:\n\nFor example, to generate a frei0r partik0l source with size 200x200 and frame rate 10 which is overlaid on the overlay filter main input:\n\nThis source is based on a generalization of John Conway’s life game.\n\nThe sourced input represents a life grid, each pixel represents a cell which can be in one of two possible states, alive or dead. Every cell interacts with its eight neighbours, which are the cells that are horizontally, vertically, or diagonally adjacent.\n\nAt each interaction the grid evolves according to the adopted rule, which specifies the number of neighbor alive cells which will make a cell stay alive or born. The option allows one to specify the rule to adopt.\n\nThis source accepts the following options:\n• Read a grid from , and center it on a grid of size 300x300 pixels:\n• Generate a random grid of size 200x200, with a fill ratio of 2/3:\n• Full example with slow death effect (mold) using :\n\nPerlin noise is a kind of noise with local continuity in space. This can be used to generate patterns with continuity in space and time, e.g. to simulate smoke, fluids, or terrain.\n\nIn case more than one octave is specified through the option, Perlin noise is generated as a sum of components, each one with doubled frequency. In this case the option specify the ratio of the amplitude with respect to the previous component. More octave components enable to specify more high frequency details in the generated noise (e.g. small size variations due to boulders in a generated terrain).\n• Use Perlin noise with 7 components, each one with a halved contribution to total amplitude:\n• Chain Perlin noise with the lutyuv to generate a black&white effect:\n• Stretch noise along the y axis, and convert gray level to red-only signal:\n\nGenerate a QR code using the libqrencode library (see https://fukuchi.org/works/qrencode/).\n\nTo enable the compilation of this source, you need to configure FFmpeg with .\n\nThe QR code is generated from the provided text or text pattern. The corresponding QR code is scaled and put in the video output according to the specified output size options.\n\nIn case no text is specified, the QR code is not generated, but an empty colored output is returned instead.\n\nThis source accepts the following options:\n• Generate a QR code encoding the specified text with the default size:\n• Same as below, but select blue on pink colors:\n• Generate a QR code with width of 200 pixels and padding, making the padded width 4/3 of the QR code width:\n• Generate a QR code with padded width of 200 pixels and padding, making the QR code width 3/4 of the padded width:\n\nThe source returns frames of size 4096x4096 of all rgb colors.\n\nThe source returns frames of size 4096x4096 of all yuv colors.\n\nThe source provides an uniformly colored input.\n\nThe source provides an identity Hald CLUT. See also haldclut filter.\n\nThe source returns unprocessed video frames. It is mainly useful to be employed in analysis / debugging tools, or as the source for filters which ignore the input data.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 75% color levels.\n\nThe source generates a color bars pattern, based on EBU PAL recommendations with 100% color levels.\n\nThe source generates an RGB test pattern useful for detecting RGB vs BGR issues. You should see a red, green and blue stripe from top to bottom.\n\nThe source generates a color bars pattern, based on the SMPTE Engineering Guideline EG 1-1990.\n\nThe source generates a color bars pattern, based on the SMPTE RP 219-2002.\n\nThe source generates a test video pattern, showing a color pattern, a scrolling gradient and a timestamp. This is mainly intended for testing purposes.\n\nThe source is similar to testsrc, but supports more pixel formats instead of just . This allows using it as an input for other tests without requiring a format conversion.\n\nThe source generates an YUV test pattern. You should see a y, cb and cr stripe from top to bottom.\n\nThe sources accept the following parameters:\n• Generate a video with a duration of 5.3 seconds, with size 176x144 and a frame rate of 10 frames per second:\n• The following graph description will generate a red source with an opacity of 0.2, with size \"qcif\" and a frame rate of 10 frames per second:\n• If the input content is to be ignored, can be used. The following command generates noise in the luma plane by employing the filter:\n\nThe source supports the following commands:\n\nFor details of how the program loading works, see the program_opencl filter.\n• Generate a colour ramp by setting pixel values from the position of the pixel in the output image. (Note that this will work with all pixel formats, but the generated output will not be the same.)\n• Generate a Sierpinski carpet pattern, panning by a single pixel each frame. __kernel void sierpinski_carpet(__write_only image2d_t dst, unsigned int index) { int2 loc = (int2)(get_global_id(0), get_global_id(1)); float4 value = 0.0f; int x = loc.x + index; int y = loc.y + index; while (x > 0 || y > 0) { if (x % 3 == 1 && y % 3 == 1) { value = 1.0f; break; } x /= 3; y /= 3; } write_imagef(dst, loc, value); }\n\nThis source accepts the following options:\n\nThis source accepts the following options:\n\nThis source supports the some above options as commands.\n\nBelow is a description of the currently available video sinks.\n\nBuffer video frames, and make them available to the end of the filter graph.\n\nThis sink is mainly intended for programmatic use, in particular through the interface defined in or the options system.\n\nIt accepts a pointer to an AVBufferSinkContext structure, which defines the incoming buffers’ formats, to be passed as the opaque parameter to for initialization.\n\nNull video sink: do absolutely nothing with the input video. It is mainly useful as a template and for use in analysis / debugging tools.\n\nBelow is a description of the currently available multimedia filters.\n\nThe filter accepts the following options:\n\nFilter supports the some above options as commands.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nMeasures phase of input audio, which is exported as metadata , representing mean phase of current audio frame. A video output can also be produced and is enabled by default. The audio is passed through as first output.\n\nAudio will be rematrixed to stereo if it has a different channel layout. Phase value is in range where means left and right channels are completely out of phase and means channels are in phase.\n\nThe filter accepts the following options, all related to its video output:\n\nThe filter also detects out of phase and mono sequences in stereo streams. It logs the sequence start, end and duration when it lasts longer or as long as the minimum set.\n\nThe filter accepts the following options for this detection:\n• Complete example with to detect 1 second of mono with 0.001 phase tolerance:\n\nThe filter is used to measure the difference between channels of stereo audio stream. A monaural signal, consisting of identical left and right signal, results in straight vertical line. Any stereo separation is visible as a deviation from this line, creating a Lissajous figure. If the straight (or deviation from it) but horizontal line appears this indicates that the left and right channels are out of phase.\n\nThe filter accepts the following options:\n\nThis filter supports the all above options as commands except options and .\n\nThe filter accepts the following options:\n\nConcatenate audio and video streams, joining them together one after the other.\n\nThe filter works on segments of synchronized video and audio streams. All segments must have the same number of streams of each type, and that will also be the number of streams at output.\n\nThe filter accepts the following options:\n\nThe filter has + outputs: first video outputs, then audio outputs.\n\nThere are x( + ) inputs: first the inputs for the first segment, in the same order as the outputs, then the inputs for the second segment, etc.\n\nRelated streams do not always have exactly the same duration, for various reasons including codec frame size or sloppy authoring. For that reason, related synchronized streams (e.g. a video and its audio track) should be concatenated at once. The concat filter will use the duration of the longest stream in each segment (except the last one), and if necessary pad shorter audio streams with silence.\n\nFor this filter to work correctly, all segments must start at timestamp 0.\n\nAll corresponding streams must have the same parameters in all segments; the filtering system will automatically select a common pixel format for video streams, and a common sample format, sample rate and channel layout for audio streams, but other settings, such as resolution, must be converted explicitly by the user.\n\nDifferent frame rates are acceptable but will result in variable frame rate at output; be sure to configure the output file to handle it.\n• Concatenate an opening, an episode and an ending, all in bilingual version (video in stream 0, audio in streams 1 and 2):\n• Concatenate two parts, handling audio and video separately, using the (a)movie sources, and adjusting the resolution: Note that a desync will happen at the stitch if the audio and video streams do not have exactly the same duration in the first file.\n\nThis filter supports the following commands:\n\nEBU R128 scanner filter. This filter takes an audio stream and analyzes its loudness level. By default, it logs a message at a frequency of 10Hz with the Momentary loudness (identified by ), Short-term loudness ( ), Integrated loudness ( ) and Loudness Range ( ).\n\nThe filter can only analyze streams which have sample format is double-precision floating point. The input stream will be converted to this specification, if needed. Users may need to insert aformat and/or aresample filters after this filter to obtain the original parameters.\n\nThe filter also has a video output (see the option) with a real time graph to observe the loudness evolution. The graphic contains the logged message mentioned above, so it is not printed anymore when this option is set, unless the verbose logging is set. The main graphing area contains the short-term loudness (3 seconds of analysis), and the gauge on the right is for the momentary loudness (400 milliseconds), but can optionally be configured to instead display short-term loudness (see ).\n\nThe green area marks a +/- 1LU target range around the target loudness (-23LUFS by default, unless modified through ).\n\nMore information about the Loudness Recommendation EBU R128 on http://tech.ebu.ch/loudness.\n\nThe filter accepts the following options:\n\nThese filters read frames from several inputs and send the oldest queued frame to the output.\n\nInput streams must have well defined, monotonically increasing frame timestamp values.\n\nIn order to submit one frame to output, these filters need to enqueue at least one frame for each input, so they cannot work in case one input is not yet terminated and will not receive incoming frames.\n\nFor example consider the case when one input is a filter which always drops input frames. The filter will keep reading from that input, but it will never be able to send new frames to output until the input sends an end-of-stream signal.\n\nAlso, depending on inputs synchronization, the filters will drop frames in case one input receives more frames than the other ones, and the queue is already filled.\n\nThese filters accept the following options:\n• Interleave frames belonging to different streams using :\n\nReport previous filter filtering latency, delay in number of audio samples for audio filters or number of video frames for video filters.\n\nOn end of input stream, filter will report min and max measured latency for previous running filter in filtergraph.\n\nThis filter accepts the following options:\n• Print all metadata values for frames with key with values between 0 and 1.\n• Direct all metadata to a pipe with file descriptor 4.\n\nThese filters are mainly aimed at developers to test direct path in the following filter in the filtergraph.\n\nThe filters accept the following options:\n\nNote: in case of auto-inserted filter between the permission filter and the following one, the permission might not be received as expected in that following filter. Inserting a format or aformat filter before the perms/aperms filter can avoid this problem.\n\nThese filters will pause the filtering for a variable amount of time to match the output rate with the input timestamps. They are similar to the option to .\n\nThey accept the following options:\n\nBoth filters supports the all above options as commands.\n\nThis filter does opposite of concat filters.\n\nThis filter accepts the following options:\n\nIn all cases, prefixing an each segment with ’+’ will make it relative to the previous segment.\n• Split input audio stream into three output audio streams, starting at start of input audio stream and storing that in 1st output audio stream, then following at 60th second and storing than in 2nd output audio stream, and last after 150th second of input audio stream store in 3rd output audio stream:\n\nThis filter accepts the following options:\n\nThe expression can contain the following constants:\n\nThe default value of the select expression is \"1\".\n• Select all frames in input: The example above is the same as:\n• Select only frames contained in the 10-20 time interval:\n• Select only I-frames contained in the 10-20 time interval:\n• Use aselect to select only audio frames with samples number > 100:\n• Create a mosaic of the first scenes: Comparing against a value between 0.3 and 0.5 is generally a sane choice.\n• Send even and odd frames to separate outputs, and compose them:\n• Select useful frames from an ffconcat file which is using inpoints and outpoints but where the source files are not intra frame only.\n\nSend commands to filters in the filtergraph.\n\nThese filters read commands to be sent to other filters in the filtergraph.\n\nmust be inserted between two video filters, must be inserted between two audio filters, but apart from that they act the same way.\n\nThe specification of commands can be provided in the filter arguments with the option, or in a file specified by the option.\n\nThese filters accept the following options:\n\nA commands description consists of a sequence of interval specifications, comprising a list of commands to be executed when a particular event related to that interval occurs. The occurring event is typically the current frame time entering or leaving a given time interval.\n\nAn interval is specified by the following syntax:\n\nThe time interval is specified by the and times. is optional and defaults to the maximum time.\n\nThe current frame time is considered within the specified interval if it is included in the interval [ , ), that is when the time is greater or equal to and is lesser than .\n\nconsists of a sequence of one or more command specifications, separated by \",\", relating to that interval. The syntax of a command specification is given by:\n\nis optional and specifies the type of events relating to the time interval which enable sending the specified command, and must be a non-null sequence of identifier flags separated by \"+\" or \"|\" and enclosed between \"[\" and \"]\".\n\nThe following flags are recognized:\n\nIf is not specified, a default value of is assumed.\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name.\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional list of argument for the given .\n\nBetween one interval specification and another, whitespaces, or sequences of characters starting with until the end of line, are ignored and can be used to annotate comments.\n\nA simplified BNF description of the commands specification syntax follows:\n• Specify audio tempo change at second 4:\n• Specify a list of drawtext and hue commands in a file. # show text in the interval 5-10 5.0-10.0 [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=hello world', [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text='; # desaturate the image in the interval 15-20 15.0-20.0 [enter] hue s 0, [enter] drawtext reinit 'fontfile=FreeSerif.ttf:text=nocolor', [leave] hue s 1, [leave] drawtext reinit 'fontfile=FreeSerif.ttf:text=color'; # apply an exponential saturation fade-out effect, starting from time 25 25 [enter] hue s exp(25-t) A filtergraph allowing to read and process the above command list stored in a file , can be specified with:\n\nChange the PTS (presentation timestamp) of the input frames.\n\nThis filter accepts the following options:\n\nThe expression is evaluated through the eval API and can contain the following constants:\n• Set fixed rate of 25 frames per second:\n• Apply an offset of 10 seconds to the input PTS:\n• Generate timestamps from a \"live source\" and rebase onto the current timebase:\n\nBoth filters support all above options as commands.\n\nThe filter marks the color range property for the output frames. It does not change the input frame, but only sets the corresponding property, which affects how the frame is treated by following filters.\n\nThe filter accepts the following options:\n\nSet the timebase to use for the output frames timestamps. It is mainly useful for testing timebase configuration.\n\nIt accepts the following parameters:\n\nThe value for is an arithmetic expression representing a rational. The expression can contain the constants \"AVTB\" (the default timebase), \"intb\" (the input timebase) and \"sr\" (the sample rate, audio only). Default value is \"intb\".\n\nConvert input audio to a video output representing frequency spectrum logarithmically using Brown-Puckette constant Q transform algorithm with direct frequency domain coefficient calculation (but the transform itself is not really constant Q, instead the Q factor is actually variable/clamped), with musical tone scale, from E0 to D#10.\n\nThe filter accepts the following options:\n• Same as above, but with frame rate 30 fps:\n• Same as above, but with more accuracy in frequency domain:\n• Custom gamma, now spectrum is linear to the amplitude.\n• Custom fontcolor and fontfile, C-note is colored green, others are colored blue:\n\nConvert input audio to video output representing frequency spectrum using Continuous Wavelet Transform and Morlet wavelet.\n\nThe filter accepts the following options:\n\nConvert input audio to video output representing the audio power spectrum. Audio amplitude is on Y-axis while frequency is on X-axis.\n\nThe filter accepts the following options:\n\nConvert stereo input audio to a video output, representing the spatial relationship between two channels.\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n\nThe usage is very similar to the showwaves filter; see the examples in that section.\n• Complete example for a colored and sliding spectrum per channel using :\n\nThe filter accepts the following options:\n• Extract an audio spectrogram of a whole audio track in a 1024x1024 picture using :\n\nThe filter accepts the following options:\n\nThe filter accepts the following options:\n• Output the input file audio and the corresponding video representation at the same time:\n• Create a synthetic signal and show it with showwaves, forcing a frame rate of 30 frames per second:\n\nThe filter accepts the following options:\n• Extract a channel split representation of the wave form of a whole audio track in a 1024x800 picture using :\n\nDelete frame side data, or select frames based on it.\n\nThis filter accepts the following options:\n\nSynthesize audio from 2 input video spectrums, first input stream represents magnitude across time and second represents phase across time. The filter will transform from frequency domain as displayed in videos back to time domain as presented in audio output.\n\nThis filter is primarily created for reversing processed showspectrum filter outputs, but can synthesize sound from other spectrograms too. But in such case results are going to be poor if the phase data is not available, because in such cases phase data need to be recreated, usually it’s just recreated from random noise. For best results use gray only output ( color mode in showspectrum filter) and scale for magnitude video and scale for phase video. To produce phase, for 2nd video, use option. Inputs videos should generally use slide mode as that saves resources needed for decoding video.\n\nThe filter accepts the following options:\n• First create magnitude and phase videos from audio, assuming audio is stereo with 44100 sample rate, then resynthesize videos back to audio with spectrumsynth:\n\nThe filter accepts a single parameter which specifies the number of outputs. If unspecified, it defaults to 2.\n• Create two separate outputs from the same input:\n• To create 3 or more outputs, you need to specify the number of outputs, like in:\n• Create two separate outputs from the same input, one cropped and one padded:\n• Create 5 copies of the input audio with :\n\nReceive commands sent through a libzmq client, and forward them to filters in the filtergraph.\n\nand work as a pass-through filters. must be inserted between two video filters, between two audio filters. Both are capable to send messages to any filter type.\n\nTo enable these filters you need to install the libzmq library and headers and configure FFmpeg with .\n\nFor more information about libzmq see: http://www.zeromq.org/\n\nThe and filters work as a libzmq server, which receives messages sent through a network interface defined by the (or the abbreviation \" \") option. Default value of this option is . You may want to alter this value to your needs, but do not forget to escape any ’:’ signs (see filtergraph escaping).\n\nThe received message must be in the form:\n\nspecifies the target of the command, usually the name of the filter class or a specific filter instance name. The default filter instance name uses the pattern ‘ ’, but you can override this by using the ‘ ’ syntax (see Filtergraph syntax).\n\nspecifies the name of the command for the target filter.\n\nis optional and specifies the optional argument list for the given .\n\nUpon reception, the message is processed and the corresponding command is injected into the filtergraph. Depending on the result, the filter will send a reply to the client, adopting the format:\n\nLook at for an example of a zmq client which can be used to send commands processed by these filters.\n\nConsider the following filtergraph generated by . In this example the last overlay filter has an instance name. All other filters will have default instance names.\n\nTo change the color of the left side of the video, the following command can be used:\n\nTo change the right side:\n\nTo change the position of the right side:\n\nBelow is a description of the currently available multimedia sources.\n\nThis is the same as movie source, except it selects an audio stream by default.\n\nGenerated stream periodically shows flash video frame and emits beep in audio. Useful to inspect A/V sync issues.\n\nIt accepts the following options:\n\nThis source supports the some above options as commands.\n\nIt accepts the following parameters:\n\nIt allows overlaying a second video on top of the main input of a filtergraph, as shown in this graph:\n• Skip 3.2 seconds from the start of the AVI file in.avi, and overlay it on top of the input labelled \"in\": movie=in.avi:seek_point=3.2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read from a video4linux2 device, and overlay it on top of the input labelled \"in\": movie=/dev/video0:f=video4linux2, scale=180:-1, setpts=PTS-STARTPTS [over]; [in] setpts=PTS-STARTPTS [main]; [main][over] overlay=16:16 [out]\n• Read the first video stream and the audio stream with id 0x81 from dvd.vob; the video is connected to the pad named \"video\" and the audio is connected to the pad named \"audio\":\n\nBoth movie and amovie support the following commands:\n\nFFmpeg can be hooked up with a number of external libraries to add support for more formats. None of them are used by default, their use has to be explicitly requested by passing the appropriate flags to .\n\nFFmpeg can make use of the AOM library for AV1 decoding and encoding.\n\nGo to http://aomedia.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can use the AMD Advanced Media Framework library for accelerated H.264 and HEVC(only windows) encoding on hardware with Video Coding Engine (VCE).\n\nTo enable support you must obtain the AMF framework header files(version 1.4.9+) from https://github.com/GPUOpen-LibrariesAndSDKs/AMF.git.\n\nCreate an directory in the system include path. Copy the contents of into that directory. Then configure FFmpeg with .\n\nInitialization of amf encoder occurs in this order: 1) trying to initialize through dx11(only windows) 2) trying to initialize through dx9(only windows) 3) trying to initialize through vulkan\n\nTo use h.264(AMD VCE) encoder on linux amdgru-pro version 19.20+ and amf-amdgpu-pro package(amdgru-pro contains, but does not install automatically) are required.\n\nThis driver can be installed using amdgpu-pro-install script in official amd driver archive.\n\nFFmpeg can read AviSynth scripts as input. To enable support, pass to configure after installing the headers provided by AviSynth+. AviSynth+ can be configured to install only the headers by either passing to the normal CMake-based build system, or by using the supplied .\n\nFor Windows, supported AviSynth variants are AviSynth 2.6 RC1 or higher for 32-bit builds and AviSynth+ r1718 or higher for 32-bit and 64-bit builds.\n\nFor Linux, macOS, and BSD, the only supported AviSynth variant is AviSynth+, starting with version 3.5.\n\nFFmpeg can make use of the Chromaprint library for generating audio fingerprints. Pass to configure to enable it. See https://acoustid.org/chromaprint.\n\nFFmpeg can make use of the codec2 library for codec2 decoding and encoding. There is currently no native decoder, so libcodec2 must be used for decoding.\n\nGo to http://freedv.org/, download \"Codec 2 source archive\". Build and install using CMake. Debian users can install the libcodec2-dev package instead. Once libcodec2 is installed you can pass to configure to enable it.\n\nThe easiest way to use codec2 is with .c2 files, since they contain the mode information required for decoding. To encode such a file, use a .c2 file extension and give the libcodec2 encoder the -mode option: . Playback is as simple as . For a list of supported modes, run . Raw codec2 files are also supported. To make sense of them the mode in use needs to be specified as a format option: .\n\nFFmpeg can make use of the dav1d library for AV1 video decoding.\n\nGo to https://code.videolan.org/videolan/dav1d and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the davs2 library for AVS2-P2/IEEE1857.4 video decoding.\n\nGo to https://github.com/pkuvcl/davs2 and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the uavs3d library for AVS3-P2/IEEE1857.10 video decoding.\n\nGo to https://github.com/uavs3/uavs3d and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Game Music Emu library to read audio from supported video game music file formats. Pass to configure to enable it. See https://bitbucket.org/mpyne/game-music-emu/overview.\n\nFFmpeg can use Intel QuickSync Video (QSV) for accelerated decoding and encoding of multiple codecs. To use QSV, FFmpeg must be linked against the dispatcher, which loads the actual decoding libraries.\n\nThe dispatcher is open source and can be downloaded from https://github.com/lu-zero/mfx_dispatch.git. FFmpeg needs to be configured with the option and needs to be able to locate the dispatcher’s files.\n\nFFmpeg can make use of the Kvazaar library for HEVC encoding.\n\nGo to https://github.com/ultravideo/kvazaar and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the LAME library for MP3 encoding.\n\nGo to http://lame.sourceforge.net/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the liblcevc_dec library for LCEVC enhacement layer decoding on supported bitstreams.\n\nGo to https://github.com/v-novaltd/LCEVCdec and follow the instructions for installing the library. Then pass to configure to enable it.\n\niLBC is a narrowband speech codec that has been made freely available by Google as part of the WebRTC project. libilbc is a packaging friendly copy of the iLBC codec. FFmpeg can make use of the libilbc library for iLBC decoding and encoding.\n\nGo to https://github.com/TimothyGu/libilbc and follow the instructions for installing the library. Then pass to configure to enable it.\n\nJPEG XL is an image format intended to fully replace legacy JPEG for an extended period of life. See https://jpegxl.info/ for more information, and see https://github.com/libjxl/libjxl for the library source. You can pass to configure in order enable the libjxl wrapper.\n\nFFmpeg can make use of the libvpx library for VP8/VP9 decoding and encoding.\n\nGo to http://www.webmproject.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of this library, originating in Modplug-XMMS, to read from MOD-like music files. See https://github.com/Konstanty/libmodplug. Pass to configure to enable it.\n\nSpun off Google Android sources, OpenCore, VisualOn and Fraunhofer libraries provide encoders for a number of audio codecs.\n\nFFmpeg can make use of the OpenCORE libraries for AMR-NB decoding/encoding and AMR-WB decoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the libraries. Then pass and/or to configure to enable them.\n\nFFmpeg can make use of the VisualOn AMR-WBenc library for AMR-WB encoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Fraunhofer AAC library for AAC decoding & encoding.\n\nGo to http://sourceforge.net/projects/opencore-amr/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the Google LC3 library for LC3 decoding & encoding.\n\nGo to https://github.com/google/liblc3/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the OpenH264 library for H.264 decoding and encoding.\n\nGo to http://www.openh264.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFor decoding, this library is much more limited than the built-in decoder in libavcodec; currently, this library lacks support for decoding B-frames and some other main/high profile features. (It currently only supports constrained baseline profile and CABAC.) Using it is mostly useful for testing and for taking advantage of Cisco’s patent portfolio license (http://www.openh264.org/BINARY_LICENSE.txt).\n\nFFmpeg can use the OpenJPEG libraries for decoding/encoding J2K videos. Go to http://www.openjpeg.org/ to get the libraries and follow the installation instructions. To enable using OpenJPEG in FFmpeg, pass to .\n\nFFmpeg can make use of rav1e (Rust AV1 Encoder) via its C bindings to encode videos. Go to https://github.com/xiph/rav1e/ and follow the instructions to build the C library. To enable using rav1e in FFmpeg, pass to .\n\nFFmpeg can make use of the Scalable Video Technology for AV1 library for AV1 encoding.\n\nGo to https://gitlab.com/AOMediaCodec/SVT-AV1/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the TwoLAME library for MP2 encoding.\n\nGo to http://www.twolame.org/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can read VapourSynth scripts as input. To enable support, pass to configure. Vapoursynth is detected via . Versions 42 or greater supported. See http://www.vapoursynth.com/.\n\nDue to security concerns, Vapoursynth scripts will not be autodetected so the input format has to be forced. For ff* CLI tools, add before the input .\n\nFFmpeg can make use of the x264 library for H.264 encoding.\n\nGo to http://www.videolan.org/developers/x264.html and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the x265 library for HEVC encoding.\n\nGo to http://x265.org/developers.html and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the xavs library for AVS encoding.\n\nGo to http://xavs.sf.net/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the xavs2 library for AVS2-P2/IEEE1857.4 video encoding.\n\nGo to https://github.com/pkuvcl/xavs2 and follow the instructions for installing the library. Then pass to configure to enable it.\n\nFFmpeg can make use of the XEVE library for EVC video encoding.\n\nGo to https://github.com/mpeg5/xeve and follow the instructions for installing the XEVE library. Then pass to configure to enable it.\n\nFFmpeg can make use of the XEVD library for EVC video decoding.\n\nGo to https://github.com/mpeg5/xevd and follow the instructions for installing the XEVD library. Then pass to configure to enable it.\n\nZVBI is a VBI decoding library which can be used by FFmpeg to decode DVB teletext pages and DVB teletext subtitles.\n\nGo to http://sourceforge.net/projects/zapping/ and follow the instructions for installing the library. Then pass to configure to enable it.\n\nYou can use the and options to have an exhaustive list.\n\nFFmpeg supports the following file formats through the library:\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nFFmpeg can read and write images for each frame of a video sequence. The following image formats are supported:\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the feature in that column (encoding / decoding) is supported.\n\nmeans that support is provided through an external library.\n\nmeans that an integer-only version is available, too (ensures high performance on systems without hardware floating point support).\n\nmeans that the feature is supported.\n\nmeans that support is provided through an external library.\n\nmeans that the protocol is supported.\n\nmeans that support is provided through an external library.\n\nFor details about the authorship, see the Git history of the project (https://git.ffmpeg.org/ffmpeg), e.g. by typing the command in the FFmpeg source directory, or browsing the online repository at https://git.ffmpeg.org/ffmpeg.\n\nMaintainers for the specific components are listed in the file in the source code tree.\n\nThis document was generated on March 23, 2025 using makeinfo."
    },
    {
        "link": "https://ffmpeg.org/documentation.html",
        "document": "The following documentation is regenerated nightly, and corresponds to the newest FFmpeg revision. Consult your locally installed documentation for older versions."
    },
    {
        "link": "https://github.com/leandromoreira/ffmpeg-libav-tutorial",
        "document": "I was looking for a tutorial/book that would teach me how to start to use FFmpeg as a library (a.k.a. libav) and then I found the \"How to write a video player in less than 1k lines\" tutorial. Unfortunately it was deprecated, so I decided to write this one.\n\nMost of the code in here will be in C but don't worry: you can easily understand and apply it to your preferred language. FFmpeg libav has lots of bindings for many languages like python, go and even if your language doesn't have it, you can still support it through the (here's an example with Lua).\n\nWe'll start with a quick lesson about what is video, audio, codec and container and then we'll go to a crash course on how to use command line and finally we'll write code, feel free to skip directly to the section Learn FFmpeg libav the Hard Way.\n\nSome people used to say that the Internet video streaming is the future of the traditional TV, in any case, the FFmpeg is something that is worth studying.\n• Intro\n• video - what you see!\n\nIf you have a sequence series of images and change them at a given frequency (let's say 24 images per second), you will create an illusion of movement. In summary this is the very basic idea behind a video: a series of pictures / frames running at a given rate.\n\nAlthough a muted video can express a variety of feelings, adding sound to it brings more pleasure to the experience.\n\nSound is the vibration that propagates as a wave of pressure, through the air or any other transmission medium, such as a gas, liquid or solid.\n\nBut if we chose to pack millions of images in a single file and called it a movie, we might end up with a huge file. Let's do the math:\n\nSuppose we are creating a video with a resolution of (height x width) and that we'll spend per pixel (the minimal point at a screen) to encode the color (or 24 bit color, what gives us 16,777,216 different colors) and this video runs at and it is long.\n\nThis video would require approximately of storage or of bandwidth! That's why we need to use a CODEC.\n\nA single file that contains all the streams (mostly the audio and video) and it also provides synchronization and general metadata, such as title, resolution and etc.\n\nUsually we can infer the format of a file by looking at its extension: for instance a is probably a video using the container .\n\nTo work with multimedia we can use the AMAZING tool/library called FFmpeg. Chances are you already know/use it directly or indirectly (do you use Chrome?).\n\nIt has a command line program called , a very simple yet powerful binary. For instance, you can convert from to the container just by typing the follow command:\n\nWe just made a remuxing here, which is converting from one container to another one. Technically FFmpeg could also be doing a transcoding but we'll talk about that later.\n\nFFmpeg does have a documentation that does a great job of explaining how it works.\n\nTo make things short, the FFmpeg command line program expects the following argument format to perform its actions , where:\n\nThe parts 2, 3, 4 and 5 can be as many as you need. It's easier to understand this argument format in action:\n\nThis command takes an input file containing two streams (an audio encoded with CODEC and a video encoded using CODEC) and convert it to , changing its audio and video CODECs too.\n\nWe could simplify the command above but then be aware that FFmpeg will adopt or guess the default values for you. For instance when you just type what audio/video CODEC does it use to produce the ?\n\nWerner Robitza wrote a must read/execute tutorial about encoding and editing with FFmpeg.\n\nWhile working with audio/video we usually do a set of tasks with the media.\n\nWhat? the act of converting one of the streams (audio or video) from one CODEC to another one.\n\nWhy? sometimes some devices (TVs, smartphones, console and etc) doesn't support X but Y and newer CODECs provide better compression rate.\n\nHow? converting an (AVC) video to an (HEVC).\n\nWhat? the act of converting from one format (container) to another one.\n\nWhy? sometimes some devices (TVs, smartphones, console and etc) doesn't support X but Y and sometimes newer containers provide modern required features.\n\nWhat? the act of changing the bit rate, or producing other renditions.\n\nWhy? people will try to watch your video in a (edge) connection using a less powerful smartphone or in a Internet connection on their 4K TVs therefore you should offer more than one rendition of the same video with different bit rate.\n\nHow? producing a rendition with bit rate between 964K and 3856K.\n\nUsually we'll be using transrating with transsizing. Werner Robitza wrote another must read/execute series of posts about FFmpeg rate control.\n\nWhat? the act of converting from one resolution to another one. As said before transsizing is often used with transrating.\n\nWhy? reasons are about the same as for the transrating.\n\nWhat? the act of producing many resolutions (bit rates) and split the media into chunks and serve them via http.\n\nWhy? to provide a flexible media that can be watched on a low end smartphone or on a 4K TV, it's also easy to scale and deploy but it can add latency.\n\nHow? creating an adaptive WebM using DASH.\n\nPS: I stole this example from the Instructions to playback Adaptive WebM using DASH\n\nThere are many and many other usages for FFmpeg. I use it in conjunction with iMovie to produce/edit some videos for YouTube and you can certainly use it professionally.\n\nSince the FFmpeg is so useful as a command line tool to do essential tasks over the media files, how can we use it in our programs?\n\nFFmpeg is composed by several libraries that can be integrated into our own programs. Usually, when you install FFmpeg, it installs automatically all these libraries. I'll be referring to the set of these libraries as FFmpeg libav.\n\nThis hello world actually won't show the message in the terminal 👅 Instead we're going to print out information about the video, things like its format (container), duration, resolution, audio channels and, in the end, we'll decode some frames and save them as image files.\n\nBut before we start to code, let's learn how FFmpeg libav architecture works and how its components communicate with others.\n\nHere's a diagram of the process of decoding a video:\n\nYou'll first need to load your media file into a component called (the video container is also known as format). It actually doesn't fully load the whole file: it often only reads the header.\n\nOnce we loaded the minimal header of our container, we can access its streams (think of them as a rudimentary audio and video data). Each stream will be available in a component called .\n\nSuppose our video has two streams: an audio encoded with AAC CODEC and a video encoded with H264 (AVC) CODEC. From each stream we can extract pieces (slices) of data called packets that will be loaded into components named .\n\nThe data inside the packets are still coded (compressed) and in order to decode the packets, we need to pass them to a specific .\n\nThe will decode them into and finally, this component gives us the uncompressed frame. Noticed that the same terminology/process is used either by audio and video stream.\n\nSince some people were facing issues while compiling or running the examples we're going to use as our development/runner environment, we'll also use the big buck bunny video so if you don't have it locally just run the command .\n\nWe'll skip some details, but don't worry: the source code is available at github.\n\nWe're going to allocate memory to the component that will hold information about the format (container).\n\nNow we're going to open the file and read its header and fill the with minimal information about the format (notice that usually the codecs are not opened). The function used to do this is . It expects an , a and two optional arguments: the (if you pass , FFmpeg will guess the format) and the (which are the options to the demuxer).\n\nWe can print the format name and the media duration:\n\nTo access the , we need to read data from the media. The function does that. Now, the will hold the amount of streams and the will give us the stream (an ).\n\nNow we'll loop through all the streams.\n\nFor each stream, we're going to keep the , which describes the properties of a codec used by the stream .\n\nWith the codec properties we can look up the proper CODEC querying the function and find the registered decoder for the codec id and return an , the component that knows how to enCOde and DECode the stream.\n\nNow we can print information about the codecs.\n\nWith the codec, we can allocate memory for the , which will hold the context for our decode/encode process, but then we need to fill this codec context with CODEC parameters; we do that with .\n\nOnce we filled the codec context, we need to open the codec. We call the function and then we can use it.\n\nNow we're going to read the packets from the stream and decode them into frames but first, we need to allocate memory for both components, the and .\n\nLet's feed our packets from the streams with the function while it has packets.\n\nLet's send the raw data packet (compressed frame) to the decoder, through the codec context, using the function .\n\nAnd let's receive the raw data frame (uncompressed frame) from the decoder, through the same codec context, using the function .\n\nWe can print the frame number, the PTS, DTS, frame type and etc.\n\nFinally we can save our decoded frame into a simple gray image. The process is very simple, we'll use the where the index is related to the planes Y, Cb and Cr, we just picked (Y) to save our gray image.\n\nAnd voilà! Now we have a gray scale image with 2MB:\n\nBefore we move to code a transcoding example let's talk about timing, or how a video player knows the right time to play a frame.\n\nIn the last example, we saved some frames that can be seen here:\n\nWhen we're designing a video player we need to play each frame at a given pace, otherwise it would be hard to pleasantly see the video either because it's playing so fast or so slow.\n\nTherefore we need to introduce some logic to play each frame smoothly. For that matter, each frame has a presentation timestamp (PTS) which is an increasing number factored in a timebase that is a rational number (where the denominator is known as timescale) divisible by the frame rate (fps).\n\nIt's easier to understand when we look at some examples, let's simulate some scenarios.\n\nFor a and each PTS will increase therefore the PTS real time for each frame could be (supposing it started at 0):\n\nFor almost the same scenario but with a timebase equal to .\n\nFor a and each PTS will increase and the PTS time could be:\n\nNow with the we can find a way to render this synched with audio or with a system clock. The FFmpeg libav provides these info through its API:\n\nJust out of curiosity, the frames we saved were sent in a DTS order (frames: 1,6,4,2,3,5) but played at a PTS order (frames: 1,2,3,4,5). Also, notice how cheap are B-Frames in comparison to P or I-Frames.\n\nRemuxing is the act of changing from one format (container) to another, for instance, we can change a MPEG-4 video to a MPEG-TS one without much pain using FFmpeg:\n\nIt'll demux the mp4 but it won't decode or encode it ( ) and in the end, it'll mux it into a file. If you don't provide the format the ffmpeg will try to guess it based on the file's extension.\n\nThe general usage of FFmpeg or the libav follows a pattern/architecture or workflow:\n• protocol layer - it accepts an (a for instance but it could be a or input as well)\n• format layer - it its content, revealing mostly metadata and its streams\n• pixel layer - it can also apply some to the raw frames (like resizing)optional\n• and then it does the reverse path\n• codec layer - it (or or even ) the raw framesoptional\n• format layer - it (or ) the raw streams (the compressed data)\n• protocol layer - and finally the muxed data is sent to an (another file or maybe a network remote server)\n\nNow let's code an example using libav to provide the same effect as in .\n\nWe're going to read from an input ( ) and change it to another output ( ).\n\nWe start doing the usually allocate memory and open the input format. For this specific case, we're going to open an input file and allocate memory for an output file.\n\nWe're going to remux only the video, audio and subtitle types of streams so we're holding what streams we'll be using into an array of indexes.\n\nJust after we allocated the required memory, we're going to loop throughout all the streams and for each one we need to create new out stream into our output format context, using the avformat_new_stream function. Notice that we're marking all the streams that aren't video, audio or subtitle so we can skip them after.\n\nNow we can create the output file.\n\nAfter that, we can copy the streams, packet by packet, from our input to our output streams. We'll loop while it has packets ( ), for each packet we need to re-calculate the PTS and DTS to finally write it ( ) to our output format context.\n\nTo finalize we need to write the stream trailer to an output media file with av_write_trailer function.\n\nNow we're ready to test it and the first test will be a format (video container) conversion from a MP4 to a MPEG-TS video file. We're basically making the command line with libav.\n\nIt's working!!! don't you trust me?! you shouldn't, we can check it with :\n\nTo sum up what we did here in a graph, we can revisit our initial idea about how libav works but showing that we skipped the codec part.\n\nBefore we end this chapter I'd like to show an important part of the remuxing process, you can pass options to the muxer. Let's say we want to delivery MPEG-DASH format for that matter we need to use fragmented mp4 (sometimes referred as ) instead of MPEG-TS or plain MPEG-4.\n\nWith the command line we can do that easily.\n\nAlmost equally easy as the command line is the libav version of it, we just need to pass the options when write the output header, just before the packets copy.\n\nWe now can generate this fragmented mp4 file:\n\nBut to make sure that I'm not lying to you. You can use the amazing site/tool gpac/mp4box.js or the site http://mp4parser.com/ to see the differences, first load up the \"common\" mp4.\n\nAs you can see it has a single atom/box, this is place where the video and audio frames are. Now load the fragmented mp4 to see which how it spreads the boxes.\n\nIn this chapter, we're going to create a minimalist transcoder, written in C, that can convert videos coded in H264 to H265 using FFmpeg/libav library specifically libavcodec, libavformat, and libavutil.\n\nLet's start with the simple transmuxing operation and then we can build upon this code, the first step is to load the input file.\n\nNow we're going to set up the decoder, the will give us access to all the components and for each one of them, we can get their and create the particular and finally we can open the given codec so we can proceed to the decoding process.\n\nWe need to prepare the output media file for transmuxing as well, we first allocate memory for the output . We create each stream in the output format. In order to pack the stream properly, we copy the codec parameters from the decoder.\n\nWe set the flag which tells the encoder that it can use the global headers and finally we open the output file for write and persist the headers.\n\nWe're getting the 's from the decoder, adjusting the timestamps, and write the packet properly to the output file. Even though the function says \"write frame\" we are storing the packet. We finish the transmuxing process by writing the stream trailer to the file.\n\nThe previous section showed a simple transmuxer program, now we're going to add the capability to encode files, specifically we're going to enable it to transcode videos from to .\n\nAfter we prepared the decoder but before we arrange the output media file we're going to set up the encoder.\n• Create the video in the encoder,\n• Create the based in the created codec,\n• Set up basic attributes for the transcoding session, and\n• Open the codec and copy parameters from the context to the stream. and\n\nWe need to expand our decoding loop for the video stream transcoding:\n• Send the empty to the decoder,\n• Receive the compressed, based on our codec, ,\n• Set up the timestamp, and\n• Write it to the output file.\n\nWe converted the media stream from to , as expected the version of the media file is smaller than the however the created program is capable of:"
    },
    {
        "link": "https://ffmpeg.org",
        "document": "FFmpeg 7.1 \"Péter\", a new major release, is now available! A full list of changes can be found in the release changelog.\n\nThe more important highlights of the release are that the VVC decoder, merged as experimental in version 7.0, has had enough time to mature and be optimized enough to be declared as stable. The codec is starting to gain traction with broadcast standardization bodies.\n\n Support has been added for a native AAC USAC (part of the xHE-AAC coding system) decoder, with the format starting to be adopted by streaming websites, due to its extensive volume normalization metadata.\n\n MV-HEVC decoding is now supported. This is a stereoscopic coding tool that begun to be shipped and generated by recent phones and VR headsets.\n\n LC-EVC decoding, an enhancement metadata layer to attempt to improve the quality of codecs, is now supported via an external library.\n\n\n\nSupport for Vulkan encoding, with H264 and HEVC was merged. This finally allows fully Vulkan-based decode-filter-encode pipelines, by having a sink for Vulkan frames, other than downloading or displaying them. The encoders have feature-parity with their VAAPI implementation counterparts. Khronos has announced that support for AV1 encoding is also coming soon to Vulkan, and FFmpeg is aiming to have day-one support.\n\nIn addition to the above, this release has had a lot of important internal work done. By far, the standout internally are the improvements made for full-range images. Previously, color range data had two paths, no negotiation, and was unreliably forwarded to filters, encoders, muxers. Work on cleaning the system up started more than 10 years ago, however this stalled due to how fragile the system was, and that breaking behaviour would be unacceptable. The new system fixes this, so now color range is forwarded correctly and consistently everywhere needed, and also laid the path for more advanced forms of negotiation.\n\n Cropping metadata is now supported with Matroska and MP4 formats. This metadata is important not only for archival, but also with AV1, as hardware encoders require its signalling due to the codec not natively supporting one.\n\nAs usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.\n\nThe number of issues FFmpeg has in Coverity (a static analyzer) is now lower than it has been since 2016. Our defect density is less than one 30th of the average in OSS with over a million code lines. All this was possible thanks to a grant from the Sovereign Tech Fund.\n\nFFmpeg now implements a native xHE-AAC decoder. Currently, streams without (e)SBR, USAC or MPEG-H Surround are supported, which means the majority of xHE-AAC streams in use should work. Support for USAC and (e)SBR is coming soon. Work is also ongoing to improve its stability and compatibility. During the process we found several specification issues, which were then submitted back to the authors for discussion and potential inclusion in a future errata.\n\nThe FFmpeg community is excited to announce that Germany's Sovereign Tech Fund has become its first governmental sponsor. Their support will help sustain the maintainance of the FFmpeg project, a critical open-source software multimedia component essential to bringing audio and video to billions around the world everyday.\n\nA new major release, FFmpeg 7.0 \"Dijkstra\", is now available for download. The most noteworthy changes for most users are a native VVC decoder (currently experimental, until more fuzzing is done), IAMF support, or a multi-threaded CLI tool.\n\nThis release is not backwards compatible, removing APIs deprecated before 6.0. The biggest change for most library callers will be the removal of the old bitmask-based channel layout API, replaced by the API allowing such features as custom channel ordering, or Ambisonics. Certain deprecated CLI options were also removed, and a C11-compliant compiler is now required to build the code.\n\nAs usual, there is also a number of new supported formats and codecs, new filters, APIs, and countless smaller features and bugfixes. Compared to 6.1, the repository contains almost ∼2000 new commits by ∼100 authors, touching >100000 lines in ∼2000 files — thanks to everyone who contributed. See the Changelog, APIchanges, and the git log for more comprehensive lists of changes.\n\nThe library now contains a native VVC (Versatile Video Coding) decoder, supporting a large subset of the codec's features. Further optimizations and support for more features are coming soon. The code was written by Nuo Mi, Xu Mu, Frank Plowman, Shaun Loo, and Wu Jianhua.\n\nThe library can now read and write IAMF (Immersive Audio) files. The CLI tool can configure IAMF structure with the new option. IAMF support was written by James Almer.\n\nThanks to a major refactoring of the command-line tool, all the major components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now run in parallel. This should improve throughput and CPU utilization, decrease latency, and open the way to other exciting new features.\n\nNote that you should not expect significant performance improvements in cases where almost all computational time is spent in a single component (typically video encoding).\n\nFFmpeg 6.1 \"Heaviside\", a new major release, is now available! Some of the highlights:\n• command support in the setpts and asetpts filters\n• Bitstream filter for converting VVC from MP4 to Annex B\n• support for the P_SKIP hinting to speed up libx264 encoding\n• ffmpeg CLI '-top' option deprecated in favor of the setfield filter\n• ffprobe XML output schema changed to account for multiple variable-fields elements within the same parent element\n• ffprobe -output_format option added as an alias of -of\n\nThis release had been overdue for at least half a year, but due to constant activity in the repository, had to be delayed, and we were finally able to branch off the release recently, before some of the large changes scheduled for 7.0 were merged.\n\nInternally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon).\n\n This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds.\n\n There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders, reducing overhead.\n\n RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left.\n\n There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the accurracy of variable frame rate video.\n\nNext major release will be version 7.0, scheduled to be released in February. We will attempt to better stick to the new release schedule we announced at the start of this year.\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nA few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase. This is the first vendor-generic and platform-generic decode acceleration API, enabling the same code to be used on multiple platforms, with very minimal overhead. This is also the first multi-threaded hardware decoding API, and our code makes full use of this, saturating all available decode engines the hardware exposes.\n\nThose wishing to test the code can read our documentation page. For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive a VkImage to present or manipulate, documentation and examples are available in our source tree. Currently, using the latest available git checkout of our repository is required. The functionality will be included in stable branches with the release of version 6.1, due to be released soon.\n\nAs this is also the first practical implementation of the specifications, bugs may be present, particularly in drivers, and, although passing verification, the implementation itself. New codecs, and encoding support are also being worked on, by both the Khronos organization for standardizing, and us as implementing it, and giving feedback on improving.\n\nA new major release, FFmpeg 6.0 \"Von Neumann\", is now available for download. This release has many new encoders and decoders, filters, ffmpeg CLI tool improvements, and also, changes the way releases are done. All major releases will now bump the version of the ABI. We plan to have a new major release each year. Another release-specific change is that deprecated APIs will be removed after 3 releases, upon the next major bump. This means that releases will be done more often and will be more organized.\n\nNew decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats. QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c to avoid confusion) has speed-up improvements due to threading, as well as statistics options, and the ability to pass option values for filters from a file. There are quite a few new audio and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too. Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed in the doc/APIchanges file in our tree. A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the next minor release, 6.1, which we plan to release soon, in line with our new release schedule. Some highlights are:\n• ffmpeg now requires threading to be built\n• ffmpeg now runs every muxer in a separate thread\n• Add new mode to cropdetect filter to detect crop-area based on motion vectors and edges\n• VAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9\n• QSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9\n• filtergraph syntax in ffmpeg CLI now supports passing file contents as option values\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nFFmpeg 5.1 \"Riemann\", a new major release, is now available! Some of the highlights:\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nFFmpeg 5.0 \"Lorentz\", a new major release, is now available! For this long-overdue release, a major effort underwent to remove the old encode/decode APIs and replace them with an N:M-based API, the entire libavresample library was removed, libswscale has a new, easier to use AVframe-based API, the Vulkan code was much improved, many new filters were added, including libplacebo integration, and finally, DoVi support was added, including tonemapping and remuxing. The default AAC encoder settings were also changed to improve quality. Some of the changelog highlights:\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nWe have a new IRC home at Libera Chat now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at contact#IRCChannels\n\nFFmpeg 4.4 \"Rao\", a new major release, is now available! Some of the highlights:\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nFFmpeg 4.3 \"4:3\", a new major release, is now available! Some of the highlights:\n• switch from AvxSynth to AviSynth+ on Linux\n• Support for muxing pcm and pgs in m2ts\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nNote that this filter is not FDA approved, nor are we medical professionals. Nor has this filter been tested with anyone who has photosensitive epilepsy. FFmpeg and its photosensitivity filter are not making any medical claims.\n\nThat said, this is a new video filter that may help photosensitive people watch tv, play video games or even be used with a VR headset to block out epiletic triggers such as filtered sunlight when they are outside. Or you could use it against those annoying white flashes on your tv screen. The filter fails on some input, such as the Incredibles 2 Screen Slaver scene. It is not perfect. If you have other clips that you want this filter to work better on, please report them to us on our trac.\n\nSee for yourself. Example was made with -vf photosensitivity=20:0.8\n\nWe are not professionals. Please use this in your medical studies to advance epilepsy research. If you decide to use this in a medical setting, or make a hardware hdmi input output realtime tv filter, or find another use for this, please let me know. This filter was a feature request of mine since 2013.\n\nFFmpeg 4.2 \"Ada\", a new major release, is now available! Some of the highlights:\n• Support decoding of HEVC 4:4:4 content in nvdec and cuviddec\n• mov muxer writes tracks with unspecified language instead of English by default\n• added support for using clang to compile CUDA kernels\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nFFmpeg 4.1 \"al-Khwarizmi\", a new major release, is now available! Some of the highlights:\n• Support for AV1 in MP4 and Matroska/WebM\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nFFmpeg 4.0 \"Wu\", a new major release, is now available! Some of the highlights:\n• Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streams\n• Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.\n• Removed the ffmenc and ffmdec muxer and demuxer\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nFFmpeg 3.4 \"Cantor\", a new major release, is now available! Some of the highlights:\n• support for decoding through D3D11VA in ffmpeg\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nFFmpeg 3.3 \"Hilbert\", a new major release, is now available! Some of the highlights:\n• configure now fails if autodetect-libraries are requested but not found\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nThis has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.\n\nWithout further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:\n\nStanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.\n\nPetru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.\n\nUmair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.\n\nJán Sebechlebský's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so Ján spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.\n\nJai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.\n\nDavinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.\n\nAnd that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!\n\nSupport for the SDL1 library has been dropped, due to it no longer being maintained (as of January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output devices have been updated to support SDL2.\n\nFFmpeg 3.1.2, a new point release from the 3.1 release branch, is now available! It fixes several bugs.\n\nWe recommend users, distributors, and system integrators, to upgrade unless they use current git master.\n\nAfter thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release. ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax. Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs and to contact us so we may point users to test and contribute to its development.\n\nFFmpeg 3.1.1, a new point release from the 3.1 release branch, is now available! It mainly deals with a few ABI issues introduced in the previous release.\n\nWe strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to upgrade unless they use current git master.\n\nFFmpeg 3.1 \"Laplace\", a new major release, is now available! Some of the highlights:\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nFFmpeg has been accepted as a Google Summer of Code open source organization. If you wish to participate as a student see our project ideas page. You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft. Good luck!\n\nFFmpeg 3.0 \"Einstein\", a new major release, is now available! Some of the highlights:\n• The native FFmpeg AAC encoder has seen extensive improvements and is no longer considered experimental\n• Over 30 new filters have been added\n• New DCA decoder based on libdcadec with full support for DTS-HD extensions\n• As with all major releases expect major backward incompatible API/ABI changes\n• See the Changelog for a list of more updates\n\nWe strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n\nWe have just removed support for VisualOn AAC encoder (libvo-aacenc) and libaacplus in FFmpeg master.\n\nEven before marking our internal AAC encoder as stable, it was known that libvo-aacenc was of an inferior quality compared to our native one for most samples. However, the VisualOn encoder was used extensively by the Android Open Source Project, and we would like to have a tested-and-true stable option in our code base.\n\nWhen first committed in 2011, libaacplus filled in the gap of encoding High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported by any of the encoders in FFmpeg at that time.\n\nThe circumstances for both have changed. After the work spearheaded by Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC encoder is ready to compete with much more mature encoders. The Fraunhofer FDK AAC Codec Library for Android was added in 2012 as the fourth supported external AAC encoder, and the one with the best quality and the most features supported, including HE-AAC and HE-AACv2.\n\nTherefore, we have decided that it is time to remove libvo-aacenc and libaacplus. If you are currently using libvo-aacenc, prepare to transition to the native encoder ( ) when updating to the next version of FFmpeg. In most cases it is as simple as merely swapping the encoder name. If you are currently using libaacplus, start using FDK AAC ( ) with an appropriate option to select the exact AAC profile that fits your needs. In both cases, you will enjoy an audible quality improvement and as well as fewer licensing headaches.\n\nWe have made several new point releases (2.8.5, 2.7.5, 2.6.7, 2.5.10). They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898. Please see the changelog for each release for more details.\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nDecember 5th, 2015, The native FFmpeg AAC encoder is now stable!\n\nAfter seven years the native FFmpeg AAC encoder has had its experimental flag removed and declared as ready for general use. The encoder is transparent at 128kbps for most samples tested with artifacts only appearing in extreme cases. Subjective quality tests put the encoder to be of equal or greater quality than most of the other encoders available to the public.\n\nLicensing has always been an issue with encoding AAC audio as most of the encoders have had a license making FFmpeg unredistributable if compiled with support for them. The fact that there now exists a fully open and truly free AAC encoder integrated directly within the project means a lot to those who wish to use accepted and widespread standards.\n\nThe majority of the work done to bring the encoder up to quality was started during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov. Both continued to work on the encoder with the latter joining as a developer and mainainer, working on other parts of the project as well. Also, thanks to Kamedo2 who does comparisons and tests, the original authors and all past and current contributors to the encoder. Users are suggested and encouraged to use the encoder and provide feedback or breakage reports through our bug tracker.\n\nA big thank you note goes to our newest supporters: MediaHub and Telepoint. Both companies have donated a dedicated server with free of charge internet connectivity. Here is a little bit about them in their own words:\n• Telepoint is the biggest carrier-neutral data center in Bulgaria. Located in the heart of Sofia on a cross-road of many Bulgarian and International networks, the facility is a fully featured Tier 3 data center that provides flexible customer-oriented colocation solutions (ranging from a server to a private collocation hall) and a high level of security.\n• MediaHub Ltd. is a Bulgarian IPTV platform and services provider which uses FFmpeg heavily since it started operating a year ago. \"Donating to help keep FFmpeg online is our way of giving back to the community\" .\n\nThanks Telepoint and MediaHub for their support!\n\nFFmpeg participated to the latest edition of the Google Summer of Code Project. FFmpeg got a total of 8 assigned projects, and 7 of them were successful.\n\nWe want to thank Google, the participating students, and especially the mentors who joined this effort. We're looking forward to participating in the next GSoC edition!\n\nBelow you can find a brief description of the final outcome of each single project.\n\nStephan Holljes's project for this session of Google Summer of Code was to implement basic HTTP server features for libavformat, to complement the already present HTTP client and RTMP and RTSP server code.\n\nThe first part of the project was to make the HTTP code capable of accepting a single client; it was completed partly during the qualification period and partly during the first week of the summer. Thanks to this work, it is now possible to make a simple HTTP stream using the following commands:\n\nThe next part of the project was to extend the code to be able to accept several clients, simultaneously or consecutively. Since libavformat did not have an API for that kind of task, it was necessary to design one. This part was mostly completed before the midterm and applied shortly afterwards. Since the ffmpeg command-line tool is not ready to serve several clients, the test ground for that new API is an example program serving hard-coded content.\n\nThe last and most ambitious part of the project was to update ffserver to make use of the new API. It would prove that the API is usable to implement real HTTP servers, and expose the points where more control was needed. By the end of the summer, a first working patch series was undergoing code review.\n\nMariusz finished an API prepared by the FFmpeg community and implemented Samba directory listing as qualification task.\n\nDuring the program he extended the API with the possibility to remove and rename files on remote servers. He completed the implementation of these features for file, Samba, SFTP, and FTP protocols.\n\nAt the end of the program, Mariusz provided a sketch of an implementation for HTTP directory listening.\n\nMate was working on directshow input from digital video sources. He got working input from ATSC input sources, with specifiable tuner.\n\nThe code has not been committed, but a patch of it was sent to the ffmpeg-devel mailing list for future use.\n\nThe mentor plans on cleaning it up and committing it, at least for the ATSC side of things. Mate and the mentor are still working trying to finally figure out how to get DVB working.\n\nNiklesh's project was to expand our support for 3GPP Timed Text subtitles. This is the native subtitle format for mp4 containers, and is interesting because it's usually the only subtitle format supported by the stock playback applications on iOS and Android devices.\n\nffmpeg already had basic support for these subtitles which ignored all formatting information - it just provided basic plain-text support.\n\nNiklesh did work to add support on both the encode and decode side for text formatting capabilities, such as font size/colour and effects like bold/italics, highlighting, etc.\n\nThe main challenge here is that Timed Text handles formatting in a very different way from most common subtitle formats. It uses a binary encoding (based on mp4 boxes, naturally) and stores information separately from the text itself. This requires additional work to track which parts of the text formatting applies to, and explicitly dealing with overlapping formatting (which other formats support but Timed Text does not) so it requires breaking the overlapping sections into separate non-overlapping ones with different formatting.\n\nFinally, Niklesh had to be careful about not trusting any size information in the subtitles - and that's no joke: the now infamous Android stagefright bug was in code for parsing Timed Text subtitles.\n\nAll of Niklesh's work is committed and was released in ffmpeg 2.8.\n\nPedro Arthur has modularized the vertical and horizontal scalers. To do this he designed and implemented a generic filter framework and moved the existing scaler code into it. These changes now allow easily adding removing, splitting or merging processing steps. The implementation was benchmarked and several alternatives were tried to avoid speed loss.\n\nHe also added gamma corrected scaling support. An example to use gamma corrected scaling would be:\n\nPedro has done impressive work considering the short time available, and he is a FFmpeg committer now. He continues to contribute to FFmpeg, and has fixed some bugs in libswscale after GSoC has ended.\n\nRostislav Pehlivanov has implemented PNS, TNS, I/S coding and main prediction on the native AAC encoder. Of all those extensions, only TNS was left in a less-than-usable state, but the implementation has been pushed (disabled) anyway since it's a good basis for further improvements.\n\nPNS replaces noisy bands with a single scalefactor representing the energy of that band, gaining in coding efficiency considerably, and the quality improvements on low bitrates are impressive for such a simple feature.\n\nTNS still needs some polishing, but has the potential to reduce coding artifacts by applying noise shaping in the temporal domain (something that is a source of annoying, notable distortion on low-entropy bands).\n\nIntensity Stereo coding (I/S) can double coding efficiency by exploiting strong correlation between stereo channels, most effective on pop-style tracks that employ panned mixing. The technique is not as effective on classic X-Y recordings though.\n\nFinally, main prediction improves coding efficiency by exploiting correlation among successive frames. While the gains have not been huge at this point, Rostislav has remained active even after the GSoC, and is polishing both TNS and main prediction, as well as looking for further improvements to make.\n\nIn the process, the MIPS port of the encoder was broken a few times, something he's also working to fix.\n\nDonny Yang implemented basic keyframe only APNG encoder as the qualification task. Later he wrote interframe compression via various blend modes. The current implementation tries all blend modes and picks one which takes the smallest amount of memory.\n\nSpecial care was taken to make sure that the decoder plays correctly all files found in the wild and that the encoder produces files that can be played in browsers that support APNG.\n\nDuring his work he was tasked to fix any encountered bug in the decoder due to the fact that it doesn't match APNG specifications. Thanks to this work, a long standing bug in the PNG decoder has been fixed.\n\nFor latter work he plans to continue working on the encoder, making it possible to select which blend modes will be used in the encoding process. This could speed up encoding of APNG files.\n\nWe published release 2.8 as new major version. It contains all features and bug fixes of the git master branch from September 8th. Please see the changelog for a list of the most important changes.\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nThe resignation of Michael Niedermayer as leader of FFmpeg yesterday has come by surprise. He has worked tirelessly on the FFmpeg project for many years and we must thank him for the work that he has done. We hope that in the future he will continue to contribute to the project. In the coming weeks, the FFmpeg project will be managed by the active contributors.\n\nThe last four years have not been easy for our multimedia community - both contributors and users. We should now look to the future, try to find solutions to these issues, and to have reconciliation between the forks, which have split the community for so long.\n\nUnfortunately, much of the disagreement has taken place in inappropriate venues so far, which has made finding common ground and solutions difficult. We aim to discuss this in our communities online over the coming weeks, and in person at the VideoLAN Developer Days in Paris in September: a neutral venue for the entire open source multimedia community.\n\nUPDATE: We have received more than 7 offers for hosting and servers, thanks a lot to everyone!\n\nAfter graciously hosting our projects (FFmpeg, MPlayer and rtmpdump) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.\n\nIf you want to host an open source project, please let us know, either on ffmpeg-devel mailing list or irc.freenode.net #ffmpeg-devel.\n\nWe use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, trac, samples repo, svn, etc.\n\nWe have made a new major release (2.6) and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March. Please see the Release Notes for a list of note-worthy changes.\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nFFmpeg has been accepted as a Google Summer of Code Project. If you wish to participate as a student see our project ideas page. You can already get in contact with mentors and start working on qualification tasks. Registration at Google for students will open March 16th. Good luck!\n\nWe happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.\n\nMore information can be found here\n\nWe demonstrate usage of FFmpeg, answer your questions and listen to your problems and wishes. If you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look!\n\nFor the first time in our CLT history, there will be an FFmpeg workshop! You can read the details here. The workshop is targeted at FFmpeg beginners. First the basics of multimedia will be covered. Thereafter you will learn how to use that knowledge and the FFmpeg CLI tools to analyse and process media files. The workshop is in German language only and prior registration is necessary. The workshop will be on Saturday starting at 10 o'clock.\n\nWe are looking forward to meet you (again)!\n\nWe have made a new major release (2.5) It contains all features and bugfixes of the git master branch from the 4th December. Please see the Release Notes for a list of note-worthy changes.\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nOctober 10, 2014, FFmpeg is in Debian unstable again\n\nWe wanted you to know there are FFmpeg packages in Debian unstable again. A big thank-you to Andreas Cadhalpun and all the people that made it possible. It has been anything but simple.\n\nUnfortunately that was already the easy part of this news. The bad news is the packages probably won't migrate to Debian testing to be in the upcoming release codenamed jessie. Read the argumentation over at Debian.\n\nHowever things will come out in the end, we hope for your continued remarkable support!\n\nThanks to a generous 6K USD donation by Samsung (Open Source Group), FFmpeg will be welcoming at least 1 \"Outreach Program for Women\" intern to work with our community for an initial period starting December 2014 (through March 2015).\n\nWe all know FFmpeg is used by the industry, but even while there are countless products building on our code, it is not at all common for companies to step up and help us out when needed. So a big thank-you to Samsung and the OPW program committee!\n\nIf you are thinking on participating in OPW as an intern, please take a look at our OPW wiki page for some initial guidelines. The page is still a work in progress, but there should be enough information there to get you started. If you, on the other hand, are thinking on sponsoring work on FFmpeg through the OPW program, please get in touch with us at opw@ffmpeg.org. With your help, we might be able to secure some extra intern spots for this round!\n\nWe have made a new major release (2.4) It contains all features and bugfixes of the git master branch from the 14th September. Please see the Release Notes for a list of note-worthy changes.\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nWe have made several new point releases (2.3.3, 2.2.7, 1.2.8). They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272. Please see the changelog for more details.\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nJuly 29, 2014, Help us out securing our spot in OPW\n\nFollowing our previous post regarding our participation on this year's OPW (Outreach Program for Women), we are now reaching out to our users (both individuals and companies) to help us gather the needed money to secure our spot in the program.\n\n We need to put together 6K USD as a minimum but securing more funds would help us towards getting more than one intern.\n\n You can donate by credit card using Click&Pledge and selecting the \"OPW\" option. If you would like to donate by money transfer or by check, please get in touch by e-mail and we will get back to you with instructions.\n\nThanks!\n\nThe FFmpeg project is proud to announce a brand new version of the website made by db0. While this was initially motivated by the need for a larger menu, the whole website ended up being redesigned, and most pages got reworked to ease navigation. We hope you'll enjoy browsing it.\n\nWe have made a new major release (2.3) It contains all features and bugfixes of the git master branch from the 16th July. Please see the Release Notes for a list of note-worthy changes.\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nJuly 3, 2014, FFmpeg and the Outreach Program For Women\n\nFFmpeg has started the process to become an OPW includer organization for the next round of the program, with internships starting December 9. The OPW aims to \"Help women (cis and trans) and genderqueer to get involved in free and open source software\". Part of the process requires securing funds to support at least one internship (6K USD), so if you were holding on your donation to FFmpeg, this is a great chance for you to come forward, get in touch and help both the project and a great initiative!\n\nWe have set up an email address you can use to contact us about donations and general inquires regarding our participation in the program. Hope to hear from you soon!\n\nWe have made several new point releases (2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14). They fix a security issue in the LZO implementation, as well as several other bugs. See the git log for details.\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nOnce again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will take place from 8th to 10th of May. Please note that this year's LinuxTag is at a different location closer to the city center.\n\nWe will have a shared booth with XBMC and VideoLAN. If you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look!\n\nMore information about LinuxTag can be found here\n\nWe are looking forward to see you in Berlin!\n\nOur server hosting the Trac issue tracker was vulnerable to the attack against OpenSSL known as \"heartbleed\". The OpenSSL software library was updated on 7th of April, shortly after the vulnerability was publicly disclosed. We have changed the private keys (and certificates) for all FFmpeg servers. The details were sent to the mailing lists by Alexander Strasser, who is part of the project server team. Here is a link to the user mailing list archive .\n\nWe encourage you to read up on \"OpenSSL heartbleed\". It is possible that login data for the issue tracker was exposed to people exploiting this security hole. You might want to change your password in the tracker and everywhere else you used that same password.\n\nWe have made a new point releases (2.2.1). It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as several other fixes. See the git log for details.\n\nWe have made a new major release (2.2) It contains all features and bugfixes of the git master branch from 1st March. A partial list of new stuff is below:\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master.\n\nWe happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage' in Chemnitz, Germany. The event will take place on 15th and 16th of March.\n\nMore information can be found here\n\nWe invite you to visit us at our booth located in the Linux-Live area! There we will demonstrate usage of FFmpeg, answer your questions and listen to your problems and wishes.\n\nIf you have media files that cannot be processed correctly with FFmpeg, be sure to have a sample with you so we can have a look!\n\nWe are looking forward to meet you (again)!\n\nThe server on which FFmpeg and MPlayer Trac issue trackers were installed was compromised. The affected server was taken offline and has been replaced and all software reinstalled. FFmpeg Git, releases, FATE, web and mailinglists are on other servers and were not affected. We believe that the original compromise happened to a server, unrelated to FFmpeg and MPlayer, several months ago. That server was used as a source to clone the VM that we recently moved Trac to. It is not known if anyone used the backdoor that was found.\n\nWe recommend all users to change their passwords. Especially users who use a password on Trac that they also use elsewhere, should change that password at least elsewhere.\n\nSince the splitting of Libav the Debian/Ubuntu maintainers have followed the Libav fork. Many people have requested the packaging of ffmpeg in Debian, as it is more feature-complete and in many cases less buggy.\n\nRogério Brito, a Debian developer, has proposed a Request For Package (RFP) in the Debian bug tracking system.\n\nPlease let the Debian and Ubuntu developers know that you support packaging of the real FFmpeg! See Debian ticket #729203 for more details.\n\nWe have made a new major release (2.1) It contains all features and bugfixes of the git master branch from 28th October. A partial list of new stuff is below:\n\nWe recommend users, distributors and system integrators to upgrade unless they use current git master."
    },
    {
        "link": "https://stackoverflow.com/questions/75242720/c-multithreading-mutex",
        "document": "Back in days I was working on an option that would speed up my function by multithreading. The base function finished around 15seconds, and I would like to reducing it, but I cannot logicing out how to create a good and working multithreading function.\n\nIn this case, the time that needed to finishing the function is around 15 seconds.\n\nThat I found to speeding up this function was the multithreading idea. Let me show how it is right now, and what is my problem with it.\n\nThe other functions are exactly same, so that shouldnt be related to the runtime. The runtime with the function that I showed up above is around 8-10seconds, so seems it is working fine, but sometimes the application simply closing when this function is called.\n\nThe function that is presented as way 2 of multithreading is working fine, my application is do not crashing anymore, but the run time is same like the untouched function time is ~15 seconds.\n\nI think the mutex lock is forceto wait until one thread is finishing, so it is exactly same if I'd just using the default code, but I would like really speeding up the function.\n\nI tried to speed up my function with multithreading option, but the 2 way I tried to do have different problems. The first idea is sometimes force my application crashing when the function is called. The second way that I created have the same run time than the default function has without multithreading."
    },
    {
        "link": "https://medium.com/codex/c-multithreading-the-simple-way-95aa1f7304a2",
        "document": "Multithreading is one of the most powerful and vital capabilities of nearly any computer processor that exists today. Multithreading allows software to execute different code simultaneously in the same program. Web servers, web browsers, databases, mobile applications, and just about any production grade software wouldn’t function as well as it does without multithreading.\n\nMultithreading often carries a reputation for being difficult. Compared with other concepts in software development, one could certainly make a case for that. However, multithreading isn’t really that different from general programming. It’s just potentially more dangerous . Learning to protect against the danger, though, can allow one to implement far more powerful algorithms and programs than you could in a single threaded manner.\n\nTo understand multithreading, it’s best to start from the least dangerous concepts and proceed toward the most potentially dangerous ones. This allows one to get comfortable with threading and work their way toward more critical and cautious code writing.\n\nPerhaps the least threatening form of multithreading is concurrency. Concurrency is generally meant to mean multiple threads running at the same time, but not sharing any resources. This means no data structures, memory, or another is shared between threads. Concurrency is commonly utilized for tasks that can be split up between threads and worked on independently.\n\nTo illustrate this, let’s look at the example of each thread getting a pointer to an integer, and the thread increments that integer, then stops. Each thread gets to run until it increments the number a few hundred times. Then, those threads, often called “worker” threads, get joined by the main thread. All of the threads work simultaneously.\n\nIf you are new to multithreading, there’s a few parts of this code that might not make sense. The method is probably one of them. An important detail to understand about starting new threads is that they work and function entirely separately from the main thread, the thread which begins in . Because they are entirely separate, we have to decide a point in which we want to wait for them to complete their assigned work.\n\nThink of similarly to how two people might split up to do their own separate tasks, then “join” back together later on. If you are traveling or going somewhere with a friend, you don’t want top just abandon them! You should ideally wait for them to catch up again. The same logic goes for threads. Anytime additional threads are created, there’s an obligation to direct how you want the central, main thread to act in accordance with them.\n\nDo you always have to join threads ? No. Actually, there is one other option. Just like with the friends example, it’s possible a friend might want to go their own way back home, and not meet back up with you. In the case of threads, that’s called detaching. Detaching a thread means allowing it to work and complete it’s work independently of the main thread. But, this can be dangerous. Take the following example, very similar to the one for , for instance.\n\nThe first risk here is using the heap-allocated after it’s deleted. Unlike , does not make the calling thread stop or wait for anything. This means as soon as the third call to ends, the calling thread will delete the array. If the created threads haven’t finished their work, they will be writing to a deleted array, which corrupts memory.\n\nThe second risk here is that the created threads can keep running even after the main thread finishes, if their work is not completed. Or they might be killed as soon as main ends. This is undefined behavior according to the C++ standard. Regardless what a specific compiler might guarantee, undefined behavior is something to avoid. There are valid use cases for , but any of them require some other form of synchronization between threads to be reliable.\n\nA resource where two different threads can access the same memory address is called a shared resource. It’s critical to note the emphasis on address. In the prior example shown here with multiple threads accessing the same array, that is not a shared resource because no two threads are reading or writing from the same memory address. The array could have just been four separate integer pointers, there’s nothing about an array in and of itself that makes it a shared resource.\n\nUnlike concurrency, a shared resource is used when it’s desirable for threads to perform work on the same data or object. This means objects which are not allocated on a thread’s own stack, and only one’s visible to other threads. What can make this tricky to understand is, although both threads can access some resource, they can never see the other threads accessing that resource.\n\nA great example of a shared resource in real life is an airport runway at night. A runway has blinking lights to help guide planes align with it as they prepare for landing. But it’s very difficult if not impossible for other planes to see each other at night, due to the darkness and the speed at which they travel. If a plane were to attempt to land on the runway at the same time as another plane, it would be disastrous. The only way planes avoid such collisions is to coordinate through air traffic control.\n\nThreads function the same way in the sense they depend on synchronization mechanisms to coordinate access to resources, such as not writing to them at the exact same time. The mechanism we will discuss here, which is perhaps the most common one, is a mutex. A mutex, known by the type , allows threads to acquire locks. Locks are a form of control that allows only one thread to proceed through a section of code at a time. Let’s look at this example.\n\nIn the above example, the and methods of the class both happen while the calling thread constructs a lock on the mutex associated with the queue. This lock is best used as an RAII style object, where it’s only active under some scope of code. Once the program finishes that scope, the lock guard object is destroyed, allowing another thread to construct and acquire a lock on the mutex. This pattern continues to satisfy the condition only one thread can modify the queue at a time.\n\nEven though they do sound really neat and straight forward, mutexes can still be dangerous. When a thread acquires a lock on a mutex, it’s responsible for freeing or destroying that lock so other threads can access the secured scope of code as well. What happens if a thread never frees the lock it acquired ? Well, something really bad.\n\nA leaked lock is when a thread locks a mutex but then that lock for some reason can never be unlocked. If this happens, all the threads will block and wait on the mutex indefinitely, making no progress or doing any work whatsoever.\n\nThe rule of thumb for mutexes is to think carefully and critically about what a thread does when it places a lock on a mutex. It’s vital that a thread only locks when it absolutely requires single thread access, and while doing so, does work as fast as possible. While mutexes provide a means to safely access the same resource, they do so at a performance cost.\n\nAre there other ways to prevent illegal, dual access to resources among multiple threads ? Yes. But that’s a topic for another post."
    },
    {
        "link": "https://stackoverflow.com/questions/49439929/managing-threads-while-practicing-modern-c17s-best-practices",
        "document": "Originally I had thought about designing a class to store along with the and that they would work with. The class was to be responsible for the managing of memory, access, transferring, releasing, locking, unlocking, joining, and other typical common functionalities of the associated types within the standard multithreading library. It was originally intended to associate the containing thread and its id with a specific set of resources that a particular thread has access to.\n\nAfter reading through the docs on about , , , , , etc. and now knowing that and are non copyable and the fact that if I template the class to store arbitrary , , or s as a within this class's container that the class instantiation of this intended singleton would only be able to store a specific function signature limiting it to not be able to instantiate any other declaration signature.\n\nConcerning these behaviors and properties of the multithreading library within the standard library for , , , , etc... it came to mind that I'm overthinking the overall design of this class.\n\nYou can refer to my initial design attempt via this previously asked question of mine. Storing arbitrary function objects into a class member container without knowing their declaration signature, This should give you an idea of what I was attempting to do.\n\nUnderstanding a little more about their behaviors, properties and responsibilities I would like to know if the following would be appropriate for the intended design process.\n\nInstead of storing any , , , or ; would it make more sense to just store the generated of the , and have my manager class act more like a monitoring, recording, and reporting type class instead?\n\nMy new intentions would be that the container would store the thread's ID in an associated map as its key along with an associated common struct. The struct would contain a property list of all the responsibilities and actions of the combined resources. This may then allow support for some of the following features: a priority queue, a task scheduler, a dispatcher of commands to send and fetch resources knowing if the thread is available or not, where these types of actions will not be done by this class directly but through generic function templates.\n\nWith my intention to implement this kind of design while trying to maintain the best practices of modern c++ targeting c++17; would this kind of design be appropriate for proper class design to be generic, modular, portable and efficient for use?"
    },
    {
        "link": "https://linkedin.com/pulse/mastering-multithreading-concurrency-c-comprehensive-deep-bajwa",
        "document": "Multithreading and concurrency stand as fundamental pillars in the domain of modern software development. These concepts play a pivotal role in optimizing application performance. Within the robust confines of the C++ programming language, multithreading and concurrency are not just features to explore but essential components that empower developers to craft highly efficient and responsive software. In this comprehensive article, we embark on an extensive journey into the realm of multithreading and concurrency in C++, starting with the basics and venturing into advanced topics.\n\nAt its core, multithreading is the art of executing multiple threads concurrently within a single program. Threads, akin to nimble and lightweight processes, coexist within the same memory space. This arrangement allows them to communicate and collaborate seamlessly, performing tasks simultaneously. Multithreading, particularly on multi-core processors, can yield remarkable gains in application performance.\n\nC++ equips programmers with a toolkit for managing threads. This toolkit encompasses essential functions such as thread creation, joining, detaching, and termination. Proficiency in utilizing these functions is imperative for mastering multithreading.\n\nWhen multiple threads engage in simultaneous access and modification of shared data, the peril of data races looms, resulting in undefined behavior. The salvation lies in synchronization mechanisms such as mutexes and locks.\n\nMutexes, the sentinels of multithreading, stand as guardians protecting shared data. C++ extends its benevolence with std::mutex, std::unique_lock, and std::lock_guard to streamline the management of mutexes and locks efficiently. Behold a vivid example:\n\nCondition variables (`std::condition_variable`) offer threads the ability to pause gracefully until specific conditions are met. They emerge as invaluable allies, particularly in scenarios where one thread produces data, while another thread consumes it.\n\nAn essential distinction lies in understanding the difference between parallelism (the simultaneous execution of threads on multiple cores) and concurrency (the efficient orchestration of multiple tasks). This comprehension is pivotal for optimizing application performance.\n\nThread pools usher in the age of efficiency by simplifying the management and reuse of a fixed number of threads for tasks. They mitigate the overhead of thread creation and destruction. C++ provides libraries such as <thread> and <future> to embrace the world of thread pools.\n\nMultithreading and concurrency in C++ are formidable tools for crafting high-performance applications. While they present challenges like data races and synchronization, a comprehensive understanding of these concepts, coupled with the tools and techniques furnished by C++, empowers you to unleash the full potential of multithreading. The result is software that not only exemplifies efficiency but also responsiveness. Continuous practice and experimentation are your steadfast companions on the journey to mastering this indispensable facet of C++ programming."
    },
    {
        "link": "https://reddit.com/r/cpp/comments/bnkq75/how_difficult_is_it_to_approach_multithreading",
        "document": "Create your account and connect with a world of communities.\n\nBy continuing, you agree to our\n\nand acknowledge that you understand the"
    }
]