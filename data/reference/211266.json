[
    {
        "link": "https://medium.com/enjoy-algorithm/popular-python-libraries-in-machine-learning-db9d7f433626",
        "document": "Python is considered the most used programming language in Machine Learning. But have you ever wondered why? And Why exactly is ML becoming so popular these days? It's all because of the ease in the usability of machine learning, which has become possible with the support of an enormous number of Python libraries. We can now build an entire ML model in just 10 to 15 lines of code, which would have taken hundreds and thousands of lines.\n\nWhy do we need libraries in Machine Learning?\n\nLibraries package the reusable code modules and present them via functions that take user input and provide the desired output. Not only this, these libraries provide us with the best-optimized form of the functions by using concepts like multi-threading to make execution time extremely fast. Hence, knowledge of these libraries saves time and stops us from reinventing the wheel. It allows us to focus on more advanced parts of machine learning, like developing newer architecture or coining newer algorithms.\n\nBut one question should be coming into our minds: if so many libraries exist, what should we focus on while starting the ML journey?\n\nWhich libraries shall we focus on while starting the ML journey?\n\nPython community is vast, and that's why we have the support of many Python libraries consisting of all types of functions. But we only need to know some of them to start our Machine Learning journey. Some of the most popular libraries that we should practice before doing hands-on in ML are:\n• Pandas to manage the datasets efficiently and effectively.\n• Matplotlib to visualize the dataset or perform analysis.\n• Scikit-learn to build the end-to-end pipeline for Machine Learning.\n\nHere, we will be discussing each of these libraries briefly and provide their resources to follow and learn.\n\nNumpy is a very popular open-source library in Machine Learning, and one can find its presence in almost all codes used in ML. The reason for that is that Numpy is used to perform numerical calculations. In ML, we narrow down all forms of datasets to numerical format and then feed them to our Machines to extract meaningful hidden patterns.\n\nIt is used to store and perform operations on multi-dimensional arrays or matrices. The capabilities and support it carries for various mathematical functions to perform wider operations ranging from simple linear algebra to estimating infinite Fourier series is way beyond any other libraries. Apart from this, it is swift in the computations compared with similar libraries and acts as a base for other libraries.\n\nFor beginners who have started their ML journey, the best way to learn Numpy would be to:\n• Create a dummy multi-dimensional array using Numpy and perform linear algebra operations like Matrix multiplication, addition, or subtraction.\n• Try to compare the execution time between the \"library-based solutions\" and \"self-implemented solutions\" and observe the advantages of numpy.\n\nScientific Python, popularly known as Scipy, is another open-source library built over the famous Numpy library. It is partially written in Python and partially in C language to boost the speed of mathematical computations further. It provides support for linear algebra, calculus, eigenvalue problems, statistics, and many more. Scipy is an advanced library and hence has some critical applications in Deep Learning or graphical data structures like Sparse matrix support.\n\nOne question should be coming into the mind: why use Scipy if Numpy is already there? Scipy is a full-fledged package comprising everything related to numerical data calculations like linear algebra, Fourier series, calculus, trigonometry, etc. If we require these numerical supports, then Scipy is better. Otherwise, for simple array formation, sorting indexing, and simple mathematical calculations, Numpy is more than enough.\n\nFor beginners who have started their ML journey, the best way to learn Scipy would be to:\n• The use of Scipy is mainly seen with Signal processing. Hence, in the case of audio datasets, it works the best. Therefore, load the audio datasets, perform Fourier Analysis, and extract insights.\n• Waveforms like Sine, Cosine, and Tangent can easily be formed with the Scipy library. So, to practice ML on dummy algorithms to check whether the model is learning, one can check the pattern following capabilities of that model on the Sine or Cosine datasets.\n• Perform linear algebra calculations like matrix multiplication, finding determinants, eigenvalues, and eigenvectors using Scipy, and compare the execution performance with respect to the Numpy library.\n\nPandas is a Python library mainly focusing on Data Analyses. It is also an open-source library built over the Numpy library, which supports large varieties of datasets across all domains. Machine Learning beginners need datasets to practice their learnings to perform analysis or build ML models. Pandas library provides functions to a broader variety of datasets, using which they can easily practice.\n\nIt supports two types of data structures (Series for one-dimensional and DataFrame for multi-dimensional) to process data in either structured or semi-structured format.\n• Explore the existing datasets in the Pandas library and play with basic functions like viewing the DataFrame, sorting it, preprocessing it, and then exporting it to files like CSV, JSON, or text files.\n• Read data from different file extensions like CSV, excel sheets, JSON, and Text files, apply processing and data analysis techniques, and then save the processed data to that file.\n• Use Pandas' built-in plot functions to see what support for data plotting it provides.\n\nMatplotlib is a popular Python library that plots curves for various data types, training, performance analysis, and visualization. This library helps make informed decisions about the data or processes required for better training. It is one of the most used libraries by data scientists or analysts working in this industry and acts as the base for other visualization libraries like Seaborn, which applies beautification to the matplotlib plots.\n• Start with simple plots of visualizing the relationship between two simple lists or arrays.\n• Try various data analysis techniques on different datasets and plot curves like histograms, pie charts, bar graphs, scatter plots, and box plots. Please also understand the meaning of these plots because that will improve the graphical reading ability.\n\nScikit-learn is the one-stop library for many Machine Learning beginners and professionals. It provides a lot of functionalities, including dummy data to practice ML, preprocessing modules, a wide range of pre-defined ML algorithms, and support for training evaluation and deployment. Earlier libraries like Pandas and Numpy were not providing additional support mentioned before. In our list of required ML libraries, Scikit-Learn is the only library capable of doing all the things involved in building end-to-end ML pipelines.\n\nIt is an open-source library primarily built over Scipy, Numpy, and Matplotlib libraries, which gives it the additional advantage of speedy training and evaluation. It also provides the support to visualize the complete model development pipeline, which includes the stages like how the data will be received, processed, fed to the model, training occurs, evaluation happens, and results are published.\n• Scikit-Learn is a huge library, and learning all at once would not be preferable. Hence, using the functions when we need them while building model development pipelines would be better.\n• Observe the difference between the execution time of ML algorithms when developed from scratch and Scikit-learn's inbuilt function. It will give a better understanding of how much these libraries help by making the execution fast.\n• Use label encoding methods present in Scikit-learn to convert the non-numerical features into numerical values. This saves a lot of time in the case of large datasets.\n• Try to load some datasets → preprocess them → use label encoders if required → normalize data features → play with dimensionality reduction techniques → try algorithms to train → visualize the performance of the trained model → Create the complete pipeline and visualize it.\n\nAlthough more libraries help learn Machine Learning and will be used while developing specific models.\n\nThe complete knowledge of these five libraries, Numpy, Scipy, Pandas, Matplotlib, and Scikit-learn, will be considered a good starting point for beginners. We have prepared this list based on the requirements to build Machine Learning applications shown in this ML Course. Other libraries like TensorFlow, Keras, PyTorch, and Theano were good candidates for this list, but they are mainly used for developing Deep Learning algorithms. We hope you enjoyed the article."
    },
    {
        "link": "https://scikit-learn.org",
        "document": ""
    },
    {
        "link": "https://github.com/ml-tooling/best-of-ml-python",
        "document": "To see all available qualifiers, see our documentation .\n\nSaved searches Use saved searches to filter your results more quickly\n\nWe read every piece of feedback, and take your input very seriously.\n\nYou signed in with another tab or window. Reload to refresh your session.\n\nYou signed out in another tab or window. Reload to refresh your session.\n\nYou switched accounts on another tab or window. Reload to refresh your session."
    },
    {
        "link": "https://geeksforgeeks.org/best-python-libraries-for-machine-learning",
        "document": "Machine learning has become an important component in various fields, enabling organizations to analyze data, make predictions, and automate processes. Python is known for its simplicity and versatility as it offers a wide range of libraries that facilitate machine learning tasks. These libraries allow developers and data scientists to quickly and effectively implement complex algorithms. By using Python’s tools, users can efficiently tackle machine learning projects and achieve better results.\n\nIn this article, we’ll dive into the Best Python libraries for Machine Learning, exploring how they facilitate various tasks like data preprocessing, model building, and evaluation. Whether you are a beginner just getting started or a professional looking to optimize workflows, these libraries will help you leverage the full potential of Machine Learning with Python.\n\nHere’s a list of some of the best Python libraries for Machine Learning that streamline development:\n\nNumPy is a very popular python library for large multi-dimensional array and matrix processing, with the help of a large collection of high-level mathematical functions. It is very useful for fundamental scientific computations in Machine Learning. It is particularly useful for linear algebra, Fourier transform, and random number capabilities. High-end libraries like TensorFlow uses NumPy internally for manipulation of Tensors.\n\nPandas is a popular Python library for data analysis. It is not directly related to Machine Learning. As we know that the dataset must be prepared before training.\n• None comes handy as it was developed specifically for data extraction and preparation.\n• None It provides high-level data structures and wide variety tools for data analysis. It provides many inbuilt methods for grouping, combining and filtering data.\n\nMatplotlib is a very popular Python library for data visualization. Like Pandas, it is not directly related to Machine Learning. It particularly comes in handy when a programmer wants to visualize the patterns in the data. It is a 2D plotting library used for creating 2D graphs and plots.\n• None A module named pyplot makes it easy for programmers for plotting as it provides features to control line styles, font properties, formatting axes, etc.\n• None It provides various kinds of graphs and plots for data visualization, viz., histogram, error charts, bar chats, etc,\n\nSciPy is a very popular library among Machine Learning enthusiasts as it contains different modules for optimization, linear algebra, integration and statistics. There is a difference between the SciPy library and the SciPy stack. The SciPy is one of the core packages that make up the SciPy stack. SciPy is also very useful for image manipulation.\n\nScikit-learn is one of the most popular ML libraries for classical ML algorithms. It is built on top of two basic Python libraries, viz., NumPy and SciPy. Scikit-learn supports most of the supervised and unsupervised learning algorithms. Scikit-learn can also be used for data-mining and data-analysis, which makes it a great tool who is starting out with ML.\n\nWe all know that Machine Learning is basically mathematics and statistics. Theano is a popular python library that is used to define, evaluate and optimize mathematical expressions involving multi-dimensional arrays in an efficient manner.\n• None It is achieved by optimizing the utilization of CPU and GPU. It is extensively used for unit-testing and self-verification to detect and diagnose different types of errors.\n• None Theano is a very powerful library that has been used in large-scale computationally intensive scientific projects for a long time but is simple and approachable enough to be used by individuals for their own projects.\n\nTensorFlow is a very popular open-source library for high performance numerical computation developed by the Google Brain team in Google. As the name suggests, Tensorflow is a framework that involves defining and running computations involving tensors. It can train and run deep neural networks that can be used to develop several AI applications. TensorFlow is widely used in the field of deep learning research and application.\n\nKeras is a very popular Python Libaries for Machine Learning . It is a high-level neural networks API capable of running on top of TensorFlow, CNTK, or Theano. It can run seamlessly on both CPU and GPU. Keras makes it really for ML beginners to build and design a Neural Network. One of the best thing about Keras is that it allows for easy and fast prototyping.\n\nPyTorch is a popular open-source Python Library for Machine Learning based on Torch, which is an open-source Machine Learning library that is implemented in C with a wrapper in Lua. It has an extensive choice of tools and libraries that support Computer Vision, Natural Language Processing(NLP), and many more ML programs. It allows developers to perform computations on Tensors with GPU acceleration and also helps in creating computational graphs.\n\nIn summary, Python’s versatility, simplicity, and vast ecosystem make it a go-to choice for Machine Learning tasks. From Scikit-Learn for classical algorithms to TensorFlow and PyTorch for deep learning, Python libraries cater to every stage of the Machine Learning workflow. Libraries like Pandas and NumPy streamline data preprocessing, while Matplotlib and Seaborn aid in data visualization. Specialized tools such as NLTK, XGBoost, and LightGBM further enhance the ability to solve complex problems efficiently."
    },
    {
        "link": "https://reddit.com/r/Python/comments/11q64a0/what_are_the_good_sources_to_learn_machine",
        "document": "I've recently finished learning about python and now I want to learn about machine learning... It is confusing as there are so many modules and I just can't choose between them.... If anyone could suggest me 5 best modules I should learn it would be a great help...."
    },
    {
        "link": "https://tensorflow.org/tutorials/quickstart/beginner",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nThis short introduction uses Keras to:\n• Evaluate the accuracy of the model.\n\nThis tutorial is a Google Colaboratory notebook. Python programs are run directly in the browser—a great way to learn and use TensorFlow. To follow this tutorial, run the notebook in Google Colab by clicking the button at the top of this page.\n• In Colab, connect to a Python runtime: At the top-right of the menu bar, select CONNECT.\n• To run all the code in the notebook, select Runtime > Run all. To run the code cells one at a time, hover over each cell and select the Run cell icon.\n\nImport TensorFlow into your program to get started:\n\nIf you are following along in your own development environment, rather than Colab, see the install guide for setting up TensorFlow for development.\n\nLoad and prepare the MNIST dataset. The pixel values of the images range from 0 through 255. Scale these values to a range of 0 to 1 by dividing the values by . This also converts the sample data from integers to floating-point numbers:\n\nis useful for stacking layers where each layer has one input tensor and one output tensor. Layers are functions with a known mathematical structure that can be reused and have trainable variables. Most TensorFlow models are composed of layers. This model uses the , , and layers.\n\nFor each example, the model returns a vector of logits or log-odds scores, one for each class.\n\nThe function converts these logits to probabilities for each class:\n\nThe loss function takes a vector of ground truth values and a vector of logits and returns a scalar loss for each example. This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n\nThis untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to .\n\nBefore you start training, configure and compile the model using Keras . Set the class to , set the to the function you defined earlier, and specify a metric to be evaluated for the model by setting the parameter to .\n\nUse the method to adjust your model parameters and minimize the loss:\n\nThe method checks the model's performance, usually on a validation set or test set.\n\nThe image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the TensorFlow tutorials.\n\nIf you want your model to return a probability, you can wrap the trained model, and attach the softmax to it:\n\nCongratulations! You have trained a machine learning model using a prebuilt dataset using the Keras API.\n\nFor more examples of using Keras, check out the tutorials. To learn more about building models with Keras, read the guides. If you want learn more about loading and preparing data, see the tutorials on image data loading or CSV data loading."
    },
    {
        "link": "https://tensorflow.org/api_docs",
        "document": "Stay organized with collections Save and categorize content based on your preferences.\n\nTensorFlow has APIs available in several languages both for constructing and executing a TensorFlow graph. The Python API is at present the most complete and the easiest to use, but other language APIs may be easier to integrate into projects and may offer some performance advantages in graph execution.\n\nA word of caution: the APIs in languages other than Python are not yet covered by the API stability promises.\n\nWe encourage the community to develop and maintain support for other languages with the approach recommended by the TensorFlow maintainers. For example, see the bindings for:\n\nWe also provide the C++ API reference for TensorFlow Serving:\n\nThere are also some archived or unsupported language bindings:"
    },
    {
        "link": "https://tensorflow.org/guide",
        "document": "TensorFlow 2 focuses on simplicity and ease of use, with updates like eager execution, intuitive higher-level APIs, and flexible model building on any platform.\n\nMany guides are written as Jupyter notebooks and run directly in Google Colab—a hosted notebook environment that requires no setup. Click the Run in Google Colab button."
    },
    {
        "link": "https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras",
        "document": "Predictive modeling with deep learning is a skill that modern developers need to know.\n\nTensorFlow is the premier open-source deep learning framework developed and maintained by Google. Although using TensorFlow directly can be challenging, the modern tf.keras API brings Keras’s simplicity and ease of use to the TensorFlow project.\n\nUsing tf.keras allows you to design, fit, evaluate, and use deep learning models to make predictions in just a few lines of code. It makes common deep learning tasks, such as classification and regression predictive modeling, accessible to average developers looking to get things done.\n\nIn this tutorial, you will discover a step-by-step guide to developing deep learning models in TensorFlow using the tf.keras API.\n\nAfter completing this tutorial, you will know:\n• The difference between Keras and tf.keras and how to install and confirm TensorFlow is working\n• The 5-step life-cycle of tf.keras models and how to use the sequential and functional APIs\n• How to develop MLP, CNN, and RNN models with tf.keras for regression, classification, and time series forecasting\n• How to use the advanced features of the tf.keras API to inspect and diagnose your model\n• How to improve the performance of your tf.keras model by reducing overfitting and accelerating training\n\nThis is a lengthy tutorial and a lot of fun. You might want to bookmark it.\n\nThe examples are small and focused; you can finish this tutorial in about 60 minutes.\n\nKick-start your project with my new book Deep Learning With Python, including step-by-step tutorials and the Python source code files for all examples.\n• Update Jun/2020: Updated for changes to the API in TensorFlow 2.2.0.\n\nThis tutorial is designed to be your complete introduction to tf.keras for your deep learning project.\n\nThe focus is on using the API for common deep learning model development tasks; you will not be diving into the math and theory of deep learning. For that, I recommend starting with this excellent book.\n\nThe best way to learn deep learning in Python is by doing. Dive in. You can circle back for more theory later.\n\nEach code example was designed to use best practices and to be standalone so that you can copy and paste it directly into your project and adapt it to your specific needs. This will give you a massive head start over trying to figure out the API from the official documentation alone.\n\nIt is an extensive tutorial, and as such, it is divided into five parts; they are:\n• Install TensorFlow and tf.keras\n• What Are Keras and tf.keras?\n• How to Confirm TensorFlow Is Installed\n• How to Use Advanced Model Features\n• How to Save and Load Your Model\n• How to Get Better Model Performance\n• How to Reduce Overfitting with Dropout\n• How to Accelerate Training with Batch Normalization\n• How to Halt Training at the Right Time with Early Stopping\n\nYou Can Do Deep Learning in Python!\n\nWork through the tutorial at your own pace.\n\nYou do not need to understand everything (at least not right now). Your goal is to run through the tutorial end-to-end and get results. You do not need to understand everything on the first pass. Write down your questions as you go. Make heavy use of the API documentation to learn about all of the functions that you’re using.\n\nYou do not need to know the math first. Math is a compact way of describing how algorithms work, specifically the tools from linear algebra, probability, and statistics. These are not the only tools you can use to learn how algorithms work. You can also use code and explore algorithmic behavior with different inputs and outputs. Knowing the math will not tell you what algorithm to choose or how to configure it best. You can only discover that through careful, controlled experiments.\n\nYou do not need to know how the algorithms work. Knowing the limitations and how to configure deep learning algorithms is important. But learning about algorithms can come later. You need to build up this algorithm knowledge slowly over a long period of time. Today, start by getting comfortable with the platform.\n\nYou do not need to be a Python programmer. The syntax of the Python language can be intuitive if you are new to it. Just like other languages, focus on function calls (e.g., function()) and assignments (e.g., a = “b”). This will get you most of the way. You are a developer, so you know how to pick up the basics of a language quickly. Just get started and dive into the details later.\n\nYou do not need to be a deep learning expert. You can learn about the benefits and limitations of various algorithms later. There are plenty of posts that you can read later to brush up on the steps of a deep learning project and the importance of evaluating model skills using cross-validation.\n\nIn this section, you will discover what tf.keras is, how to install it, and how to confirm that it is installed correctly.\n\n1.1 What Are Keras and tf.keras?\n\nKeras is an open-source deep learning library written in Python.\n\nThe project was started in 2015 by Francois Chollet. It quickly became a popular framework for developers, becoming one of, if not the most, popular deep learning libraries.\n\nBetween 2015 and 2019, developing deep learning models using mathematical libraries like TensorFlow, Theano, and PyTorch was cumbersome, requiring tens or even hundreds of lines of code to achieve the simplest tasks. The focus of these libraries was on research, flexibility, and speed, not ease of use.\n\nKeras was popular because the API was clean and simple, allowing standard deep learning models to be defined, fit, and evaluated in just a few lines of code.\n\nA secondary reason Keras took off was that it allowed you to use any of the range of popular deep learning mathematical libraries as the backend (e.g., used to perform the computation), such as TensorFlow, Theano, and later, CNTK. This allowed the power of these libraries to be harnessed (e.g., GPUs) with a very clean and simple interface.\n\nIn 2019, Google released a new version of their TensorFlow deep learning library (TensorFlow 2) that integrated the Keras API directly and promoted this interface as the default or standard interface for deep learning development on the platform.\n\nThis integration is commonly referred to as the tf.keras interface or API (“tf” is short for “TensorFlow“). This is to distinguish it from the so-called standalone Keras open source project.\n• Standalone Keras: The standalone open source project that supports TensorFlow, Theano, and CNTK backends\n\nNowadays, since the features of other backends are dwarfed by TensorFlow 2, the latest Keras library supports only TensorFlow, and these two are the same.\n\nThe Keras API implementation in Keras is referred to as “tf.keras” because this is the Python idiom used when referencing the API. First, the TensorFlow module is imported and named “tf“; then, Keras API elements are accessed via calls to tf.keras; for example:\n\nGiven that TensorFlow was the de facto standard backend for the Keras open source project, the integration means that a single library can now be used instead of two separate libraries. Further, the standalone Keras project now recommends all future Keras development use the tf.keras API.\n\nBefore installing TensorFlow, ensure that you have Python installed, such as Python 3.6 or higher.\n\nIf you don’t have Python installed, you can install it using Anaconda. This tutorial will show you how:\n• How to Set Up Your Python Environment for Machine Learning with Anaconda\n\nThere are many ways to install the TensorFlow open-source deep learning library.\n\nThe most common, and perhaps the simplest, way to install TensorFlow on your workstation is by using pip.\n\nFor example, on the command line, you can type:\n\nIf you prefer to use an installation method more specific to your platform or package manager, you can see a complete list of installation instructions here:\n\nThere is no need to set up the GPU now.\n\nAll examples in this tutorial will work just fine on a modern CPU. If you want to configure TensorFlow for your GPU, you can do that after completing this tutorial. Don’t get distracted!\n\n1.3 How to Confirm TensorFlow Is Installed\n\nOnce TensorFlow is installed, it is important to confirm that the library was installed successfully and that you can start using it.\n\nIf TensorFlow is not installed correctly or raises an error on this step, you won’t be able to run the examples later.\n\nCreate a new file called versions.py and copy and paste the following code into the file.\n\nSave the file, then open your command line and change the directory to where you saved the file.\n\nYou should then see output like the following:\n\nThis confirms that TensorFlow is installed correctly and that you are using the same version as this tutorial.\n\nWhat version did you get? \n\n Post your output in the comments below.\n\nThis also shows you how to run a Python script from the command line. You should run all code from the command line in this manner and not from a notebook or an IDE.\n\nIf You Get Warning Messages\n\nSometimes when you use the tf.keras API, you may see warnings printed.\n\nThis might include messages that your hardware supports features that your TensorFlow installation was not configured to use.\n\nSome examples on your workstation may include:\n\nThey are not your fault. You did nothing wrong.\n\nThese are information messages and will not prevent your code’s execution. You can safely ignore messages of this type for now.\n\nIt’s an intentional design decision made by the TensorFlow team to show these warning messages. A downside of this decision is that it confuses beginners and trains developers to ignore all messages, including those that potentially may impact the execution.\n\nNow that you know what tf.keras is, how to install TensorFlow, and how to confirm your development environment is working, let’s look at the life-cycle of deep learning models in TensorFlow.\n\nIn this section, you will discover the life-cycle for a deep learning model and the two tf.keras APIs that you can use to define models.\n\nA model has a life cycle, and this very simple knowledge provides the backbone for both modeling a dataset and understanding the tf.keras API.\n\nThe five steps in the life cycle are as follows:\n\nLet’s take a closer look at each step in turn.\n\nDefining the model requires first selecting the type of model you need and then choosing the architecture or network topology.\n\nFrom an API perspective, this involves defining the layers of the model, configuring each layer with a number of nodes and activation function, and connecting the layers together into a cohesive model.\n\nModels can be defined either with the Sequential API or the Functional API, and the next section will look at these two.\n\nCompiling the model requires first selecting a loss function that you want to optimize, such as mean squared error or cross-entropy.\n\nIt also requires that you select an algorithm to perform the optimization procedure, typically stochastic gradient descent or a modern variation, such as Adam. It may also require that you select any performance metrics to keep track of during the model training process.\n\nFrom an API perspective, this involves calling a function to compile the model with the chosen configuration, which will prepare the appropriate data structures required for the efficient use of the model you have defined.\n\nThe optimizer can be specified as a string for a known optimizer class, e.g., ‘sgd‘ for stochastic gradient descent, or you can configure an instance of an optimizer class and use that.\n\nFor a list of supported optimizers, see this:\n\nThe three most common loss functions are:\n\nFor a list of supported loss functions, see:\n\nMetrics are defined as a list of strings for known metric functions or a list of functions to call to evaluate predictions.\n\nFor a list of supported metrics, see:\n\nFitting the model requires that you first select the training configuration, such as the number of epochs (loops through the training dataset) and the batch size (number of samples in an epoch used to estimate model error).\n\nTraining applies the chosen optimization algorithm to minimize the chosen loss function and updates the model using the backpropagation of the error algorithm.\n\nFitting the model is the slow part of the whole process and can take seconds to hours to days, depending on the complexity of the model, the hardware you’re using, and the size of the training dataset.\n\nFrom an API perspective, this involves calling a function to perform the training process. This function will block (not return) until the training process has finished.\n\nFor help on how to choose the batch size, see this tutorial:\n• How to Control the Stability of Training Neural Networks with the Batch Size\n\nWhile fitting the model, a progress bar will summarize the status of each epoch and the overall training process. This can be simplified to a simple report of the model performance of each epoch by setting the “verbose” argument to 2. All output can be turned off during training by setting “verbose” to 0.\n\nEvaluating the model requires that you first choose a holdout dataset used to evaluate the model. This should be data not used in the training process so that you can get an unbiased estimate of the performance of the model when making predictions on new data.\n\nThe speed of a model evaluation is proportional to the amount of data you want to use for the evaluation, although it is much faster than training as the model is not changed.\n\nFrom an API perspective, this involves calling a function with the holdout dataset and getting a loss and perhaps other metrics that can be reported.\n\nMaking a prediction is the final step in the life cycle. It is why we wanted the model in the first place.\n\nIt requires you have new data for which a prediction is required, e.g., where you do not have the target values.\n\nFrom an API perspective, you simply call a function to make a prediction of a class label, probability, or numerical value—whatever you designed your model to predict.\n\nYou may want to save the model and later load it to make predictions. You may also choose to fit a model on all of the available data before you start using it.\n\nNow that you are familiar with the model life-cycle let’s take a look at the two main ways to use the tf.keras API to build models: sequential and functional.\n\nThe sequential model API is the simplest and the recommended API, especially when getting started.\n\nIt is referred to as “sequential” because it involves defining a Sequential class and adding layers to the model one by one in a linear manner, from input to output.\n\nThe example below defines a Sequential MLP model that accepts eight inputs, has one hidden layer with 10 nodes, and then an output layer with one node to predict a numerical value.\n\nNote that the visible layer of the network is defined by the “input_shape” argument on the first hidden layer. In the above example, the model expects the input for one sample to be a vector of eight numbers.\n\nThe sequential API is easy to use because you keep calling model.add() until you have added all your layers.\n\nFor example, here is a deep MLP with five hidden layers.\n\nThe functional API is more complex but is also more flexible.\n\nIt involves explicitly connecting the output of one layer to the input of another layer. Each connection is specified.\n\nFirst, an input layer must be defined via the Input class, and the shape of an input sample is specified. We must retain a reference to the input layer when defining the model.\n\nNext, a fully connected layer can be connected to the input by calling the layer and passing the input layer. This will return a reference to the output connection in this new layer.\n\nWe can then connect this to an output layer in the same manner.\n\nOnce connected, we define a Model object and specify the input and output layers. The complete example is listed below.\n\nAs such, it allows for more complicated model designs, such as models that may have multiple input paths (separate vectors) and models that have multiple output paths (e.g., a word and a number).\n\nThe functional API can be a lot of fun when you get used to it.\n\nFor more on the functional API, see:\n\nNow that we are familiar with the model life cycle and the two APIs that can be used to define models, let’s look at developing some standard models.\n\nIn this section, you will discover how to develop, evaluate, and make predictions with standard deep learning models, including Multilayer Perceptrons (MLP), Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs).\n\nA Multilayer Perceptron model, or MLP for short, is a standard fully connected neural network model.\n\nIt comprises layers of nodes where each node is connected to all outputs from the previous layer, and the output of each node is connected to all inputs for nodes in the next layer.\n\nAn MLP is created with one or more Dense layers. This model is appropriate for tabular data, that is data as it looks in a table or spreadsheet with one column for each variable and one row for each variable. There are three predictive modeling problems you may want to explore with an MLP; they are binary classification, multiclass classification, and regression.\n\nLet’s fit a model on a real dataset for each of these cases.\n\nNote that the models in this section are effective but not optimized. See if you can improve their performance. Post your findings in the comments below.\n\nWe will use the Ionosphere binary (two-class) classification dataset to demonstrate an MLP for binary classification.\n\nThis dataset involves predicting whether a structure is in the atmosphere or not, given radar returns.\n\nThe dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n\nWe will use a LabelEncoder to encode the string labels to integer values 0 and 1. The model will be fit on 67% of the data, and the remaining 33% will be used for evaluation, split using the train_test_split() function.\n\nIt is good practice to use ‘relu‘ activation with a ‘he_normal‘ weight initialization. This combination goes a long way in overcoming the problem of vanishing gradients when training deep neural network models. For more on ReLU, see the tutorial:\n\nThe model predicts the probability of class 1 and uses the sigmoid activation function. The model is optimized using the adam version of stochastic gradient descent and seeks to minimize the cross-entropy loss.\n\nThe complete example is listed below.\n\nRunning the example first reports the shape of the dataset then fits the model and evaluates it on the test dataset. Finally, a prediction is made for a single row of data.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWhat results did you get? Can you change the model to do better?\n\n Post your findings in the comments below.\n\nIn this case, we can see that the model achieved a classification accuracy of about 94% and then predicted a probability of 0.9 that the one row of data belongs to class 1.\n\nWe will use the Iris flowers multiclass classification dataset to demonstrate an MLP for multiclass classification.\n\nThis problem involves predicting the species of iris flower given measures of the flower.\n\nThe dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n\nGiven that it is a multiclass classification, the model must have one node for each class in the output layer and use the softmax activation function. The loss function is the ‘sparse_categorical_crossentropy‘, which is appropriate for integer encoded class labels (e.g., 0 for one class, 1 for the next class, etc.)\n\nThe complete example of fitting and evaluating an MLP on the iris flowers dataset is listed below.\n\nRunning the example first reports the shape of the dataset then fits the model and evaluates it on the test dataset. Finally, a prediction is made for a single row of data.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWhat results did you get? Can you change the model to do better?\n\n Post your findings in the comments below.\n\nIn this case, we can see that the model achieved a classification accuracy of about 98% and then predicted a probability of a row of data belonging to each class, although class 0 has the highest probability.\n\nWe will use the Boston housing regression dataset to demonstrate an MLP for regression predictive modeling.\n\nThis problem involves predicting house value based on the properties of the house and neighborhood.\n\nThe dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n\nThis is a regression problem that involves predicting a single numerical value. As such, the output layer has a single node and uses the default or linear activation function (no activation function). The mean squared error (mse) loss is minimized when fitting the model.\n\nRecall that this is a regression, not a classification; therefore, we cannot calculate classification accuracy. For more on this, see the tutorial:\n• Difference Between Classification and Regression in Machine Learning\n\nThe complete example of fitting and evaluating an MLP on the Boston housing dataset is listed below.\n\nRunning the example first reports the shape of the dataset then fits the model and evaluates it on the test dataset. Finally, a prediction is made for a single row of data.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWhat results did you get? Can you change the model to do better?\n\n Post your findings in the comments below.\n\nIn this case, we can see that the model achieved an MSE of about 60, which is an RMSE of about 7 (units are thousands of dollars). A value of about 26 is then predicted for the single example.\n\nConvolutional Neural Networks, or CNNs for short, are a type of network designed for image input.\n\nThey are comprised of models with convolutional layers that extract features (called feature maps) and pooling layers that distill features down to the most salient elements.\n\nCNNs are most well-suited to image classification tasks, although they can be used on a wide array of tasks that take images as input.\n\nA popular image classification task is the MNIST handwritten digit classification. It involves tens of thousands of handwritten digits that must be classified as a number between 0 and 9.\n\nThe tf.keras API provides a convenient function to download and load this dataset directly.\n\nThe example below loads the dataset and plots the first few images.\n\nRunning the example loads the MNIST dataset, then summarizes the default train and test datasets.\n\nA plot is then created showing a grid of examples of handwritten images in the training dataset.\n\nWe can train a CNN model to classify the images in the MNIST dataset.\n\nNote that the images are arrays of grayscale pixel data; therefore, we must add a channel dimension to the data before we can use the images as input to the model. The reason is that CNN models expect images in a channels-last format; that is, each example to the network has the dimensions [rows, columns, channels], where channels represent the color channels of the image data.\n\nIt is also a good idea to scale the pixel values from the default range of 0-255 to 0-1 when training a CNN. For more on scaling pixel values, see the tutorial:\n• How to Manually Scale Image Pixel Data for Deep Learning\n\nThe complete example of fitting and evaluating a CNN model on the MNIST dataset is listed below.\n\nRunning the example first reports the shape of the dataset then fits the model and evaluates it on the test dataset. Finally, a prediction is made for a single image.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWhat results did you get? Can you change the model to do better?\n\n Post your findings in the comments below.\n\nFirst, the shape of each image is reported along with the number of classes; we can see that each image is 28×28 pixels, and there are 10 classes, as we expected.\n\nIn this case, we can see that the model achieved a classification accuracy of about 98% on the test dataset. We can then see that the model predicted class 5 for the first image in the training set.\n\nRecurrent Neural Networks, or RNNs for short, are designed to operate upon sequences of data.\n\nThey have proven to be very effective for natural language processing problems where sequences of text are provided as input to the model. RNNs have also seen some modest success in time series forecasting and speech recognition.\n\nThe most popular type of RNN is the Long Short-Term Memory network or LSTM for short. LSTMs can be used in a model to accept a sequence of input data and make a prediction, such as assign a class label or predict a numerical value like the next value or values in the sequence.\n\nYou will use the car sales dataset to demonstrate an LSTM RNN for univariate time series forecasting.\n\nThis problem involves predicting the number of car sales per month.\n\nThe dataset will be downloaded automatically using Pandas, but you can learn more about it here.\n\nLet’s frame the problem to take a window of the last five months of data to predict the current month’s data.\n\nTo achieve this, you define a new function named split_sequence() that will split the input sequence into windows of data appropriate for fitting a supervised learning model, like an LSTM.\n\nFor example, if the sequence was:\n\nThen the samples for training the model would look like:\n\nUse the last 12 months of data as the test dataset.\n\nLSTMs expect each sample in the dataset to have two dimensions; the first is the number of time steps (in this case, it is 5), and the second is the number of observations per time step (in this case, it is 1).\n\nBecause it is a regression-type problem, we will use a linear activation function (no activation function) in the output layer and optimize the mean squared error loss function. We will also evaluate the model using the mean absolute error (MAE) metric.\n\nThe complete example of fitting and evaluating an LSTM for a univariate time series forecasting problem is listed below.\n\nRunning the example first reports the shape of the dataset then fits the model and evaluates it on the test dataset. Finally, a prediction is made for a single example.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWhat results did you get? Can you change the model to do better?\n\n Post your findings in the comments below.\n\nFirst, the shape of the train and test datasets is displayed, confirming that the last 12 examples are used for model evaluation.\n\nIn this case, the model achieved an MAE of about 2,800 and predicted the next value in the sequence from the test set as 13,199, where the expected value is 14,577 (pretty close).\n\nNote: it is good practice to scale and make the series stationary data prior to fitting the model. I recommend this as an extension to achieve better performance. For more on preparing time series data for modeling, see the tutorial:\n\n4. How to Use Advanced Model Features\n\nIn this section, you will discover how to use some of the slightly more advanced model features, such as reviewing learning curves and saving models for later use.\n\nThe architecture of deep learning models can quickly become large and complex.\n\nAs such, it is important to have a clear idea of the connections and data flow in your model. This is especially important if you are using the functional API to ensure you have indeed connected the layers of the model in the way you intended.\n\nThere are two tools you can use to visualize your model: a text description and a plot.\n\nA text description of your model can be displayed by calling the summary() function on your model.\n\nThe example below defines a small model with three layers and then summarizes the structure.\n\nRunning the example prints a summary of each layer and a total summary.\n\nThis is an invaluable diagnostic for checking the output shapes and the number of parameters (weights) in your model.\n\nYou can create a plot of your model by calling the plot_model() function.\n\nThis will create an image file that contains a box and line diagram of the layers in your model.\n\nThe example below creates a small three-layer model and saves a plot of the model architecture to ‘model.png‘ that includes input and output shapes.\n\nRunning the example creates a plot of the model showing a box for each layer with shape information and arrows that connect the layers, showing the flow of data through the network.\n\nLearning curves are a plot of neural network model performance over time, such as those calculated at the end of each training epoch.\n\nPlots of learning curves provide insight into the learning dynamics of the model, such as whether the model is learning well, underfitting the training dataset, or overfitting the training dataset.\n\nFor a gentle introduction to learning curves and how to use them to diagnose the learning dynamics of models, see the tutorial:\n• How to use Learning Curves to Diagnose Machine Learning Model Performance\n\nYou can easily create learning curves for your deep learning models.\n\nFirst, you must update your call to the fit function to include a reference to a validation dataset. This is a portion of the training set not used to fit the model and is instead used to evaluate the performance of the model during training.\n\nYou can split the data manually and specify the validation_data argument, or you can use the validation_split argument and specify a percentage split of the training dataset and let the API perform the split for you. The latter is simpler for now.\n\nThe fit function will return a history object that contains a trace of performance metrics recorded at the end of each training epoch. This includes the chosen loss function and each configured metric, such as accuracy, and each loss and metric is calculated for the training and validation datasets.\n\nA learning curve is a plot of the loss on the training dataset and the validation dataset. We can create this plot from the history object using the Matplotlib library.\n\nThe example below fits a small neural network on a synthetic binary classification problem. A validation split of 30% is used to evaluate the model during training, and the cross-entropy loss on the train and validation datasets are then graphed using a line plot.\n\nRunning the example fits the model on the dataset. At the end of the run, the history object is returned and used as the basis for creating the line plot.\n\nThe cross-entropy loss for the training dataset is accessed via the ‘loss‘ key, and the loss on the validation dataset is accessed via the ‘val_loss‘ key on the history attribute of the history object.\n\n4.3 How to Save and Load Your Model\n\nTraining and evaluating models is great, but we may want to use a model later without retraining it each time.\n\nThis can be achieved by saving the model to a file, loading it later, and using it to make predictions.\n\nThis can be achieved using the save() function on the model to save the model. It can be loaded later using the load_model() function.\n\nThe model is saved in H5 format, an efficient array storage format. As such, you must ensure that the h5py library is installed on your workstation. This can be achieved using pip, for example:\n\nThe example below fits a simple model on a synthetic binary classification problem and then saves the model file.\n\nRunning the example fits the model and saves it to a file with the name ‘model.h5‘.\n\nYou can then load the model and use it to make a prediction, continue training it, or do whatever you wish with it.\n\nThe example below loads the model and uses it to make a prediction.\n\nRunning the example loads the image from the file, then uses it to make a prediction on a new row of data and prints the result.\n\n5. How to Get Better Model Performance\n\nIn this section, you will discover some of the techniques that you can use to improve the performance of your deep learning models.\n\nA big part of improving deep learning performance involves avoiding overfitting by slowing down the learning process or stopping the learning process at the right time.\n\n5.1 How to Reduce Overfitting with Dropout\n\nDropout is a clever regularization method that reduces the overfitting of the training dataset and makes the model more robust.\n\nThis is achieved during training, where some number of layer outputs are randomly ignored or “dropped out.” This has the effect of making the layer look like—and be treated like—a layer with a different number of nodes and connectivity to the prior layer.\n\nDropout has the effect of making the training process noisy, forcing nodes within a layer to take on more or less responsibility for the inputs probabilistically.\n\nFor more on how dropout works, see this tutorial:\n\nYou can add dropout to your models as a new layer prior to the layer that you want to have input connections dropped out.\n\nThis involves adding a layer called Dropout() that takes an argument specifying the probability that each output from the previous will drop, e.g., 0.4 means 40% percent of inputs will be dropped each update to the model.\n\nYou can add Dropout layers in MLP, CNN, and RNN models, although there are also specialized versions of dropout for use with CNN and RNN models that you might also want to explore.\n\nThe example below fits a small neural network model on a synthetic binary classification problem.\n\nA dropout layer with 50% dropout is inserted between the first hidden layer and the output layer.\n\n5.2 How to Accelerate Training with Batch Normalization\n\nThe scale and distribution of inputs to a layer can greatly impact how easy or quickly that layer can be trained.\n\nThis is generally why it is a good idea to scale input data prior to modeling it with a neural network model.\n\nBatch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.\n\nFor more on how batch normalization works, see this tutorial:\n\nYou can use batch normalization in your network by adding a batch normalization layer prior to the layer that you wish to have standardized inputs. You can use batch normalization with MLP, CNN, and RNN models.\n\nThis can be achieved by adding the BatchNormalization layer directly.\n\nThe example below defines a small MLP network for a binary classification prediction problem with a batch normalization layer between the first hidden layer and the output layer.\n\nAlso, tf.keras has a range of other normalization layers you might like to explore; see:\n\n5.3 How to Halt Training at the Right Time with Early Stopping\n\nToo little training and the model is underfit; too much training and the model overfits the training dataset. Both cases result in a model that is less effective than it could be.\n\nOne approach to solving this problem is to use early stopping. This involves monitoring the loss on the training dataset and a validation dataset (a subset of the training set not used to fit the model). As soon as loss for the validation set starts to show signs of overfitting, the training process can be stopped.\n\nFor more on early stopping, see the tutorial:\n\nEarly stopping can be used with your model by first ensuring that you have a validation dataset. You can define the validation dataset manually via the validation_data argument to the fit() function, or you can use the validation_split and specify the amount of the training dataset to hold back for validation.\n\nYou can then define an EarlyStopping and instruct it on which performance measure to monitor, such as ‘val_loss‘ for loss on the validation dataset and the number of epochs to observe overfitting before taking action, e.g., 5.\n\nThis configured EarlyStopping callback can then be provided to the fit() function via the “callbacks” argument that takes a list of callbacks.\n\nThis allows you to set the number of epochs to a large number and be confident that training will end as soon as the model starts overfitting. You might also want to create a learning curve to discover more insights into the learning dynamics of the run and when training was halted.\n\nThe example below demonstrates a small neural network on a synthetic binary classification problem that uses early stopping to halt training as soon as the model starts overfitting (after about 50 epochs).\n\nThe tf.keras API provides a number of callbacks that you might like to explore; you can learn more here:\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• How to Control the Stability of Training Neural Networks With the Batch Size\n• Difference Between Classification and Regression in Machine Learning\n• How to Manually Scale Image Pixel Data for Deep Learning\n• How to use Learning Curves to Diagnose Machine Learning Model Performance\n\nIn this tutorial, you discovered a step-by-step guide to developing deep learning models in TensorFlow using the tf.keras API.\n• The difference between Keras and tf.keras and how to install and confirm TensorFlow is working\n• The 5-step life cycle of tf.keras models and how to use the sequential and functional APIs\n• How to develop MLP, CNN, and RNN models with tf.keras for regression, classification, and time series forecasting\n• How to use the advanced features of the tf.keras API to inspect and diagnose your model\n• How to improve the performance of your tf.keras model by reducing overfitting and accelerating training\n\nDo you have any questions?\n\n Ask your questions in the comments below, and I will do my best to answer."
    },
    {
        "link": "https://stackoverflow.com/questions/59029854/use-scipy-optimizer-with-tensorflow-2-0-for-neural-network-training",
        "document": "After the introduction of Tensorflow 2.0 the scipy interface (tf.contrib.opt.ScipyOptimizerInterface) has been removed. However, I would still like to use the scipy optimizer scipy.optimize.minimize(method=’L-BFGS-B’) to train a neural network (keras model sequential). In order for the optimizer to work, it requires as input a function fun(x0) with x0 being an array of shape (n,). Therefore, the first step would be to \"flatten\" the weights matrices to obtain a vector with the required shape. To this end, I modified the code provided by https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/. This provides a function factory meant to create such a function fun(x0). However, the code does not seem to work and the loss function does not decrease. I would be really grateful if someone could help me work this out.\n\nHere the piece of code I am using:\n\nHere model is an object tf.keras.Sequential.\n\nThank you in advance for any help!"
    }
]