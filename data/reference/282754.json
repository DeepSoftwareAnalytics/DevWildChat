[
    {
        "link": "https://docs.python.org/3/library/asyncio.html",
        "document": "asyncio is a library to write concurrent code using the async/await syntax.\n\nasyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\nasyncio is often a perfect fit for IO-bound and high-level structured network code.\n\nasyncio provides a set of high-level APIs to:\n• None run Python coroutines concurrently and have full control over their execution;\n\nAdditionally, there are low-level APIs for library and framework developers to:\n• None create and manage event loops, which provide asynchronous APIs for networking, running subprocesses, handling OS signals, etc;\n\nYou can experiment with an concurrent context in the REPL:\n\nRaises an auditing event with no arguments."
    },
    {
        "link": "https://realpython.com/async-io-python",
        "document": "Async IO is a concurrent programming design that has received dedicated support in Python, evolving rapidly from Python 3.4 through 3.7, and probably beyond.\n\nYou may be thinking with dread, “Concurrency, parallelism, threading, multiprocessing. That’s a lot to grasp already. Where does async IO fit in?”\n\nThis tutorial is built to help you answer that question, giving you a firmer grasp of Python’s approach to async IO.\n• Asynchronous IO (async IO): a language-agnostic paradigm (model) that has implementations across a host of programming languages\n• / : two new Python keywords that are used to define coroutines\n• : the Python package that provides a foundation and API for running and managing coroutines\n\nCoroutines (specialized generator functions) are the heart of async IO in Python, and we’ll dive into them later on.\n\nBefore you get started, you’ll need to make sure you’re set up to use and other libraries found in this tutorial.\n\nAsync IO is a bit lesser known than its tried-and-true cousins, multiprocessing and threading. This section will give you a fuller picture of what async IO is and how it fits into its surrounding landscape. Where Does Async IO Fit In? Concurrency and parallelism are expansive subjects that are not easy to wade into. While this article focuses on async IO and its implementation in Python, it’s worth taking a minute to compare async IO to its counterparts in order to have context about how async IO fits into the larger, sometimes dizzying puzzle. Parallelism consists of performing multiple operations at the same time. Multiprocessing is a means to effect parallelism, and it entails spreading tasks over a computer’s central processing units (CPUs, or cores). Multiprocessing is well-suited for CPU-bound tasks: tightly bound loops and mathematical computations usually fall into this category. Concurrency is a slightly broader term than parallelism. It suggests that multiple tasks have the ability to run in an overlapping manner. (There’s a saying that concurrency does not imply parallelism.) Threading is a concurrent execution model whereby multiple threads take turns executing tasks. One process can contain multiple threads. Python has a complicated relationship with threading thanks to its GIL, but that’s beyond the scope of this article. What’s important to know about threading is that it’s better for IO-bound tasks. While a CPU-bound task is characterized by the computer’s cores continually working hard from start to finish, an IO-bound job is dominated by a lot of waiting on input/output to complete. To recap the above, concurrency encompasses both multiprocessing (ideal for CPU-bound tasks) and threading (suited for IO-bound tasks). Multiprocessing is a form of parallelism, with parallelism being a specific type (subset) of concurrency. The Python standard library has offered longstanding support for both of these through its , , and packages. Now it’s time to bring a new member to the mix. Over the last few years, a separate design has been more comprehensively built into CPython: asynchronous IO, enabled through the standard library’s package and the new and language keywords. To be clear, async IO is not a newly invented concept, and it has existed or is being built into other languages and runtime environments, such as Go, C#, or Scala. The package is billed by the Python documentation as a library to write concurrent code. However, async IO is not threading, nor is it multiprocessing. It is not built on top of either of these. In fact, async IO is a single-threaded, single-process design: it uses cooperative multitasking, a term that you’ll flesh out by the end of this tutorial. It has been said in other words that async IO gives a feeling of concurrency despite using a single thread in a single process. Coroutines (a central feature of async IO) can be scheduled concurrently, but they are not inherently concurrent. To reiterate, async IO is a style of concurrent programming, but it is not parallelism. It’s more closely aligned with threading than with multiprocessing but is very much distinct from both of these and is a standalone member in concurrency’s bag of tricks. That leaves one more term. What does it mean for something to be asynchronous? This isn’t a rigorous definition, but for our purposes here, I can think of two properties:\n• Asynchronous routines are able to “pause” while waiting on their ultimate result and let other routines run in the meantime.\n• Asynchronous code, through the mechanism above, facilitates concurrent execution. To put it differently, asynchronous code gives the look and feel of concurrency. Here’s a diagram to put it all together. The white terms represent concepts, and the green terms represent ways in which they are implemented or effected: I’ll stop there on the comparisons between concurrent programming models. This tutorial is focused on the subcomponent that is async IO, how to use it, and the APIs that have sprung up around it. For a thorough exploration of threading versus multiprocessing versus async IO, pause here and check out Jim Anderson’s overview of concurrency in Python. Jim is way funnier than me and has sat in more meetings than me, to boot. Async IO may at first seem counterintuitive and paradoxical. How does something that facilitates concurrent code use a single thread and a single CPU core? I’ve never been very good at conjuring up examples, so I’d like to paraphrase one from Miguel Grinberg’s 2017 PyCon talk, which explains everything quite beautifully: Chess master Judit Polgár hosts a chess exhibition in which she plays multiple amateur players. She has two ways of conducting the exhibition: synchronously and asynchronously.\n• Opponents each take 55 seconds to make a move Synchronous version: Judit plays one game at a time, never two at the same time, until the game is complete. Each game takes (55 + 5) * 30 == 1800 seconds, or 30 minutes. The entire exhibition takes 24 * 30 == 720 minutes, or 12 hours. Asynchronous version: Judit moves from table to table, making one move at each table. She leaves the table and lets the opponent make their next move during the wait time. One move on all 24 games takes Judit 24 * 5 == 120 seconds, or 2 minutes. The entire exhibition is now cut down to 120 * 30 == 3600 seconds, or just 1 hour. (Source) There is only one Judit Polgár, who has only two hands and makes only one move at a time by herself. But playing asynchronously cuts the exhibition time down from 12 hours to one. So, cooperative multitasking is a fancy way of saying that a program’s event loop (more on that later) communicates with multiple tasks to let each take turns running at the optimal time. Async IO takes long waiting periods in which functions would otherwise be blocking and allows other functions to run during that downtime. (A function that blocks effectively forbids others from running from the time that it starts until the time that it returns.) I’ve heard it said, “Use async IO when you can; use threading when you must.” The truth is that building durable multithreaded code can be hard and error-prone. Async IO avoids some of the potential speedbumps that you might otherwise encounter with a threaded design. But that’s not to say that async IO in Python is easy. Be warned: when you venture a bit below the surface level, async programming can be difficult too! Python’s async model is built around concepts such as callbacks, events, transports, protocols, and futures—just the terminology can be intimidating. The fact that its API has been changing continually makes it no easier. Luckily, has matured to a point where most of its features are no longer provisional, while its documentation has received a huge overhaul and some quality resources on the subject are starting to emerge as well.\n\nNow that you have some background on async IO as a design, let’s explore Python’s implementation. Python’s package (introduced in Python 3.4) and its two keywords, and , serve different purposes but come together to help you declare, build, execute, and manage asynchronous code. A Word of Caution: Be careful what you read out there on the Internet. Python’s async IO API has evolved rapidly from Python 3.4 to Python 3.7. Some old patterns are no longer used, and some things that were at first disallowed are now allowed through new introductions. At the heart of async IO are coroutines. A coroutine is a specialized version of a Python generator function. Let’s start with a baseline definition and then build off of it as you progress here: a coroutine is a function that can suspend its execution before reaching , and it can indirectly pass control to another coroutine for some time. Later, you’ll dive a lot deeper into how exactly the traditional generator is repurposed into a coroutine. For now, the easiest way to pick up how coroutines work is to start making some. Let’s take the immersive approach and write some async IO code. This short program is the of async IO but goes a long way towards illustrating its core functionality: When you execute this file, take note of what looks different than if you were to define the functions with just and : The order of this output is the heart of async IO. Talking to each of the calls to is a single event loop, or coordinator. When each task reaches , the function yells up to the event loop and gives control back to it, saying, “I’m going to be sleeping for 1 second. Go ahead and let something else meaningful be done in the meantime.” Contrast this to the synchronous version: When executed, there is a slight but critical change in order and execution time: While using and may seem banal, they are used as stand-ins for any time-intensive processes that involve wait time. (The most mundane thing you can wait on is a call that does basically nothing.) That is, can represent any time-consuming blocking function call, while is used to stand in for a non-blocking call (but one that also takes some time to complete). As you’ll see in the next section, the benefit of awaiting something, including , is that the surrounding function can temporarily cede control to another function that’s more readily able to do something immediately. In contrast, or any other blocking call is incompatible with asynchronous Python code, because it will stop everything in its tracks for the duration of the sleep time. At this point, a more formal definition of , , and the coroutine functions that they create are in order. This section is a little dense, but getting a hold of / is instrumental, so come back to this if you need to:\n• The syntax introduces either a native coroutine or an asynchronous generator. The expressions and are also valid, and you’ll see them later on.\n• The keyword passes function control back to the event loop. (It suspends the execution of the surrounding coroutine.) If Python encounters an expression in the scope of , this is how tells the event loop, “Suspend execution of until whatever I’m waiting on—the result of —is returned. In the meantime, go let something else run.” In code, that second bullet point looks roughly like this: # Pause here and come back to g() when f() is ready There’s also a strict set of rules around when and how you can and cannot use / . These can be handy whether you are still picking up the syntax or already have exposure to using / :\n• A function that you introduce with is a coroutine. It may use , , or , but all of these are optional. Declaring is valid:\n• Using and/or creates a coroutine function. To call a coroutine function, you must it to get its results.\n• It is less common (and only recently legal in Python) to use in an block. This creates an asynchronous generator, which you iterate over with . Forget about async generators for the time being and focus on getting down the syntax for coroutine functions, which use and/or .\n• Anything defined with may not use , which will raise a .\n• Just like it’s a to use outside of a function, it is a to use outside of an coroutine. You can only use in the body of coroutines. Here are some terse examples meant to summarize the above few rules: # OK - `await` and `return` allowed in coroutines # OK - this is an async generator # Still no - SyntaxError (no `async def` here) Finally, when you use , it’s required that be an object that is awaitable. Well, that’s not very helpful, is it? For now, just know that an awaitable object is either (1) another coroutine or (2) an object defining an dunder method that returns an iterator. If you’re writing a program, for the large majority of purposes, you should only need to worry about case #1. That brings us to one more technical distinction that you may see pop up: an older way of marking a function as a coroutine is to decorate a normal function with . The result is a generator-based coroutine. This construction has been outdated since the / syntax was put in place in Python 3.5. These two coroutines are essentially equivalent (both are awaitable), but the first is generator-based, while the second is a native coroutine: If you’re writing any code yourself, prefer native coroutines for the sake of being explicit rather than implicit. Generator-based coroutines will be removed in Python 3.10. Towards the latter half of this tutorial, we’ll touch on generator-based coroutines for explanation’s sake only. The reason that / were introduced is to make coroutines a standalone feature of Python that can be easily differentiated from a normal generator function, thus reducing ambiguity. Don’t get bogged down in generator-based coroutines, which have been deliberately outdated by / . They have their own small set of rules (for instance, cannot be used in a generator-based coroutine) that are largely irrelevant if you stick to the / syntax. Without further ado, let’s take on a few more involved examples. Here’s one example of how async IO cuts down on wait time: given a coroutine that keeps producing random integers in the range [0, 10], until one of them exceeds a threshold, you want to let multiple calls of this coroutine not need to wait for each other to complete in succession. You can largely follow the patterns from the two scripts above, with slight changes: The colorized output says a lot more than I can and gives you a sense for how this script is carried out: This program uses one main coroutine, , and runs it concurrently across 3 different inputs. Most programs will contain small, modular coroutines and one wrapper function that serves to chain each of the smaller coroutines together. is then used to gather tasks (futures) by mapping the central coroutine across some iterable or pool. In this miniature example, the pool is . In a fuller example presented later, it is a set of URLs that need to be requested, parsed, and processed concurrently, and encapsulates that entire routine for each URL. While “making random integers” (which is CPU-bound more than anything) is maybe not the greatest choice as a candidate for , it’s the presence of in the example that is designed to mimic an IO-bound process where there is uncertain wait time involved. For example, the call might represent sending and receiving not-so-random integers between two clients in a message application.\n\nAsync IO comes with its own set of possible script designs, which you’ll get introduced to in this section. A key feature of coroutines is that they can be chained together. (Remember, a coroutine object is awaitable, so another coroutine can it.) This allows you to break programs into smaller, manageable, recyclable coroutines: Pay careful attention to the output, where sleeps for a variable amount of time, and begins working with the results as they become available: In this setup, the runtime of will be equal to the maximum runtime of the tasks that it gathers together and schedules. The package provides queue classes that are designed to be similar to classes of the module. In our examples so far, we haven’t really had a need for a queue structure. In , each task (future) is composed of a set of coroutines that explicitly await each other and pass through a single input per chain. There is an alternative structure that can also work with async IO: a number of producers, which are not associated with each other, add items to a queue. Each producer may add multiple items to the queue at staggered, random, unannounced times. A group of consumers pull items from the queue as they show up, greedily and without waiting for any other signal. In this design, there is no chaining of any individual consumer to a producer. The consumers don’t know the number of producers, or even the cumulative number of items that will be added to the queue, in advance. It takes an individual producer or consumer a variable amount of time to put and extract items from the queue, respectively. The queue serves as a throughput that can communicate with the producers and consumers without them talking to each other directly. Note: While queues are often used in threaded programs because of the thread-safety of , you shouldn’t need to concern yourself with thread safety when it comes to async IO. (The exception is when you’re combining the two, but that isn’t done in this tutorial.) One use-case for queues (as is the case here) is for the queue to act as a transmitter for producers and consumers that aren’t otherwise directly chained or associated with each other. The synchronous version of this program would look pretty dismal: a group of blocking producers serially add items to the queue, one producer at a time. Only after all producers are done can the queue be processed, by one consumer at a time processing item-by-item. There is a ton of latency in this design. Items may sit idly in the queue rather than be picked up and processed immediately. An asynchronous version, , is below. The challenging part of this workflow is that there needs to be a signal to the consumers that production is done. Otherwise, will hang indefinitely, because the queue will have been fully processed, but consumers won’t have any idea that production is complete. The first few coroutines are helper functions that return a random string, a fractional-second performance counter, and a random integer. A producer puts anywhere from 1 to 5 items into the queue. Each item is a tuple of where is a random string and is the time at which the producer attempts to put the tuple into the queue. When a consumer pulls an item out, it simply calculates the elapsed time that the item sat in the queue using the timestamp that the item was put in with. Keep in mind that is used to mimic some other, more complex coroutine that would eat up time and block all other execution if it were a regular blocking function. Here is a test run with two producers and five consumers: In this case, the items process in fractions of a second. A delay can be due to two reasons:\n• Situations where all consumers are sleeping when an item appears in the queue With regards to the second reason, luckily, it is perfectly normal to scale to hundreds or thousands of consumers. You should have no problem with . The point here is that, theoretically, you could have different users on different systems controlling the management of producers and consumers, with the queue serving as the central throughput. So far, you’ve been thrown right into the fire and seen three related examples of calling coroutines defined with and . If you’re not completely following or just want to get deeper into the mechanics of how modern coroutines came to be in Python, you’ll start from square one with the next section.\n\nEarlier, you saw an example of the old-style generator-based coroutines, which have been outdated by more explicit native coroutines. The example is worth re-showing with a small tweak: # No need to build these yourself, but be aware of what they are As an experiment, what happens if you call or on its own, without , or without any calls to or other “porcelain” functions? Calling a coroutine in isolation returns a coroutine object: This isn’t very interesting on its surface. The result of calling a coroutine on its own is an awaitable coroutine object. Time for a quiz: what other feature of Python looks like this? (What feature of Python doesn’t actually “do much” when it’s called on its own?) Hopefully you’re thinking of generators as an answer to this question, because coroutines are enhanced generators under the hood. The behavior is similar in this regard: # Nothing much happens - need to iterate with `.__next__()` Generator functions are, as it so happens, the foundation of async IO (regardless of whether you declare coroutines with rather than the older wrapper). Technically, is more closely analogous to than it is to . (But remember that is just syntactic sugar to replace .) One critical feature of generators as it pertains to async IO is that they can effectively be stopped and restarted at will. For example, you can out of iterating over a generator object and then resume iteration on the remaining values later. When a generator function reaches , it yields that value, but then it sits idle until it is told to yield its subsequent value. This can be fleshed out through an example: # Pause execution. We can resume later. The keyword behaves similarly, marking a break point at which the coroutine suspends itself and lets other coroutines work. “Suspended,” in this case, means a coroutine that has temporarily ceded control but not totally exited or finished. Keep in mind that , and by extension and , mark a break point in a generator’s execution. This is the fundamental difference between functions and generators. A function is all-or-nothing. Once it starts, it won’t stop until it hits a , then pushes that value to the caller (the function that calls it). A generator, on the other hand, pauses each time it hits a and goes no further. Not only can it push this value to calling stack, but it can keep a hold of its local variables when you resume it by calling on it. There’s a second and lesser-known feature of generators that also matters. You can send a value into a generator as well through its method. This allows generators (and coroutines) to call ( ) each other without blocking. I won’t get any further into the nuts and bolts of this feature, because it matters mainly for the implementation of coroutines behind the scenes, but you shouldn’t ever really need to use it directly yourself. If you’re interested in exploring more, you can start at PEP 342, where coroutines were formally introduced. Brett Cannon’s How the Heck Does Async-Await Work in Python is also a good read, as is the PYMOTW writeup on . Lastly, there’s David Beazley’s Curious Course on Coroutines and Concurrency, which dives deep into the mechanism by which coroutines run. Let’s try to condense all of the above articles into a few sentences: there is a particularly unconventional mechanism by which these coroutines actually get run. Their result is an attribute of the exception object that gets thrown when their method is called. There’s some more wonky detail to all of this, but it probably won’t help you use this part of the language in practice, so let’s move on for now. To tie things together, here are some key points on the topic of coroutines as generators:\n• Coroutines are repurposed generators that take advantage of the peculiarities of generator methods.\n• Old generator-based coroutines use to wait for a coroutine result. Modern Python syntax in native coroutines simply replaces with as the means of waiting on a coroutine result. The is analogous to , and it often helps to think of it as such.\n• The use of is a signal that marks a break point. It lets a coroutine temporarily suspend execution and permits the program to come back to it later. Along with plain / , Python also enables to iterate over an asynchronous iterator. The purpose of an asynchronous iterator is for it to be able to call asynchronous code at each stage when it is iterated over. A natural extension of this concept is an asynchronous generator. Recall that you can use , , or in a native coroutine. Using within a coroutine became possible in Python 3.6 (via PEP 525), which introduced asynchronous generators with the purpose of allowing and to be used in the same coroutine function body: Last but not least, Python enables asynchronous comprehension with . Like its synchronous cousin, this is largely syntactic sugar: # This does *not* introduce concurrent execution # It is meant to show syntax only This is a crucial distinction: neither asynchronous generators nor comprehensions make the iteration concurrent. All that they do is provide the look-and-feel of their synchronous counterparts, but with the ability for the loop in question to give up control to the event loop for some other coroutine to run. In other words, asynchronous iterators and asynchronous generators are not designed to concurrently map some function over a sequence or iterator. They’re merely designed to let the enclosing coroutine allow other tasks to take their turn. The and statements are only needed to the extent that using plain or would “break” the nature of in the coroutine. This distinction between asynchronicity and concurrency is a key one to grasp. You can think of an event loop as something like a loop that monitors coroutines, taking feedback on what’s idle, and looking around for things that can be executed in the meantime. It is able to wake up an idle coroutine when whatever that coroutine is waiting on becomes available. Thus far, the entire management of the event loop has been implicitly handled by one function call: , introduced in Python 3.7, is responsible for getting the event loop, running tasks until they are marked as complete, and then closing the event loop. There’s a more long-winded way of managing the event loop, with . The typical pattern looks like this: You’ll probably see floating around in older examples, but unless you have a specific need to fine-tune control over the event loop management, should be sufficient for most programs. If you do need to interact with the event loop within a Python program, is a good-old-fashioned Python object that supports introspection with and . You can manipulate it if you need to get more fine-tuned control, such as in scheduling a callback by passing the loop as an argument. What is more crucial is understanding a bit beneath the surface about the mechanics of the event loop. Here are a few points worth stressing about the event loop. #1: Coroutines don’t do much on their own until they are tied to the event loop. You saw this point before in the explanation on generators, but it’s worth restating. If you have a main coroutine that awaits others, simply calling it in isolation has little effect: Remember to use to actually force execution by scheduling the coroutine (future object) for execution on the event loop: #2: By default, an async IO event loop runs in a single thread and on a single CPU core. Usually, running one single-threaded event loop in one CPU core is more than sufficient. It is also possible to run event loops across multiple cores. Check out this talk by John Reese for more, and be warned that your laptop may spontaneously combust. #3. Event loops are pluggable. That is, you could, if you really wanted, write your own event loop implementation and have it run tasks just the same. This is wonderfully demonstrated in the package, which is an implementation of the event loop in Cython. That is what is meant by the term “pluggable event loop”: you can use any working implementation of an event loop, unrelated to the structure of the coroutines themselves. The package itself ships with two different event loop implementations, with the default being based on the module. (The second implementation is built for Windows only.)\n\nYou’ve made it this far, and now it’s time for the fun and painless part. In this section, you’ll build a web-scraping URL collector, , using , a blazingly fast async HTTP client/server framework. (We just need the client part.) Such a tool could be used to map connections between a cluster of sites, with the links forming a directed graph. Note: You may be wondering why Python’s package isn’t compatible with async IO. is built on top of , which in turn uses Python’s and modules. By default, socket operations are blocking. This means that Python won’t like because is not awaitable. In contrast, almost everything in is an awaitable coroutine, such as and . It’s a great package otherwise, but you’re doing yourself a disservice by using in asynchronous code. The high-level program structure will look like this:\n• Send GET requests for the URLs and decode the resulting content. If this fails, stop there for a URL.\n• Search for the URLs within tags in the HTML of the responses.\n• Do all of the above as asynchronously and concurrently as possible. (Use for the requests, and for the file-appends. These are two primary examples of IO that are well-suited for the async IO model.) Here are the contents of . It’s not huge, and contains mostly highly trafficked sites: The second URL in the list should return a 404 response, which you’ll need to handle gracefully. If you’re running an expanded version of this program, you’ll probably need to deal with much hairier problems than this, such a server disconnections and endless redirects. The requests themselves should be made using a single session, to take advantage of reusage of the session’s internal connection pool. Let’s take a look at the full program. We’ll walk through things step-by-step after: \"\"\"Find HREFs in the HTML of `url`.\"\"\" \"\"\"Write the found HREFs from `url` to `file`.\"\"\" This script is longer than our initial toy programs, so let’s break it down. The constant is a regular expression to extract what we’re ultimately searching for, tags within HTML: The coroutine is a wrapper around a GET request to make the request and decode the resulting page HTML. It makes the request, awaits the response, and raises right away in the case of a non-200 status: If the status is okay, returns the page HTML (a ). Notably, there is no exception handling done in this function. The logic is to propagate that exception to the caller and let it be handled there: We and because they’re awaitable coroutines. The request/response cycle would otherwise be the long-tailed, time-hogging portion of the application, but with async IO, lets the event loop work on other readily available jobs such as parsing and writing URLs that have already been fetched. Next in the chain of coroutines comes , which waits on for a given URL, and then extracts all of the tags from that page’s HTML, making sure that each is valid and formatting it as an absolute path. Admittedly, the second portion of is blocking, but it consists of a quick regex match and ensuring that the links discovered are made into absolute paths. In this specific case, this synchronous code should be quick and inconspicuous. But just remember that any line within a given coroutine will block other coroutines unless that line uses , , or . If the parsing was a more intensive process, you might want to consider running this portion in its own process with . Next, the coroutine takes a file object and a single URL, and waits on to return a of the parsed URLs, writing each to the file asynchronously along with its source URL through use of , a package for async file IO. Lastly, serves as the main entry point into the script’s chain of coroutines. It uses a single session, and a task is created for each URL that is ultimately read from . Here are a few additional points that deserve mention:\n• The default has an adapter with a maximum of 100 open connections. To change that, pass an instance of to . You can also specify limits on a per-host basis.\n• You can specify max timeouts for both the session as a whole and for individual requests.\n• This script also uses , which works with an asynchronous context manager. I haven’t devoted a whole section to this concept because the transition from synchronous to asynchronous context managers is fairly straightforward. The latter has to define and rather than and . As you might expect, can only be used inside a coroutine function declared with . If you’d like to explore a bit more, the companion files for this tutorial up at GitHub have comments and docstrings attached as well. Here’s the execution in all of its glory, as gets, parses, and saves results for 9 URLs in under a second: That’s not too shabby! As a sanity check, you can check the line-count on the output. In my case, it’s 626, though keep in mind this may fluctuate: Next Steps: If you’d like to up the ante, make this webcrawler recursive. You can use to keep track of which URLs have been crawled within the tree to avoid requesting them twice, and connect links with Python’s library. Remember to be nice. Sending 1000 concurrent requests to a small, unsuspecting website is bad, bad, bad. There are ways to limit how many concurrent requests you’re making in one batch, such as in using the sempahore objects of or using a pattern like this one. If you don’t heed this warning, you may get a massive batch of exceptions and only end up hurting your own program.\n\nNow that you’ve seen a healthy dose of code, let’s step back for a minute and consider when async IO is an ideal option and how you can make the comparison to arrive at that conclusion or otherwise choose a different model of concurrency. When and Why Is Async IO the Right Choice? This tutorial is no place for an extended treatise on async IO versus threading versus multiprocessing. However, it’s useful to have an idea of when async IO is probably the best candidate of the three. The battle over async IO versus multiprocessing is not really a battle at all. In fact, they can be used in concert. If you have multiple, fairly uniform CPU-bound tasks (a great example is a grid search in libraries such as or ), multiprocessing should be an obvious choice. Simply putting before every function is a bad idea if all of the functions use blocking calls. (This can actually slow down your code.) But as mentioned previously, there are places where async IO and multiprocessing can live in harmony. The contest between async IO and threading is a little bit more direct. I mentioned in the introduction that “threading is hard.” The full story is that, even in cases where threading seems easy to implement, it can still lead to infamous impossible-to-trace bugs due to race conditions and memory usage, among other things. Threading also tends to scale less elegantly than async IO, because threads are a system resource with a finite availability. Creating thousands of threads will fail on many machines, and I don’t recommend trying it in the first place. Creating thousands of async IO tasks is completely feasible. Async IO shines when you have multiple IO-bound tasks where the tasks would otherwise be dominated by blocking IO-bound wait time, such as:\n• Network IO, whether your program is the server or the client side\n• Serverless designs, such as a peer-to-peer, multi-user network like a group chatroom\n• Read/write operations where you want to mimic a “fire-and-forget” style but worry less about holding a lock on whatever you’re reading and writing to The biggest reason not to use it is that only supports a specific set of objects that define a specific set of methods. If you want to do async read operations with a certain DBMS, you’ll need to find not just a Python wrapper for that DBMS, but one that supports the / syntax. Coroutines that contain synchronous calls block other coroutines and tasks from running. For a shortlist of libraries that work with / , see the list at the end of this tutorial. Async IO It Is, but Which One? This tutorial focuses on async IO, the / syntax, and using for event-loop management and specifying tasks. certainly isn’t the only async IO library out there. This observation from Nathaniel J. Smith says a lot: [In] a few years, might find itself relegated to becoming one of those stdlib libraries that savvy developers avoid, like . What I’m arguing, in effect, is that is a victim of its own success: when it was designed, it used the best approach possible; but since then, work inspired by – like the addition of / – has shifted the landscape so that we can do even better, and now is hamstrung by its earlier commitments. (Source) To that end, a few big-name alternatives that do what does, albeit with different APIs and different approaches, are and . Personally, I think that if you’re building a moderately sized, straightforward program, just using is plenty sufficient and understandable, and lets you avoid adding yet another large dependency outside of Python’s standard library. But by all means, check out and , and you might find that they get the same thing done in a way that’s more intuitive for you as the user. Many of the package-agnostic concepts presented here should permeate to alternative async IO packages as well."
    },
    {
        "link": "https://docs.python.org/3/library/asyncio-sync.html",
        "document": "asyncio synchronization primitives are designed to be similar to those of the module with two important caveats:\n• None asyncio primitives are not thread-safe, therefore they should not be used for OS thread synchronization (use for that);\n• None methods of these synchronization primitives do not accept the timeout argument; use the function to perform operations with timeouts.\n\nasyncio has the following basic synchronization primitives:\n\nAn asyncio lock can be used to guarantee exclusive access to a shared resource. The preferred way to use a Lock is an statement: This method waits until the lock is unlocked, sets it to locked and returns . When more than one coroutine is blocked in waiting for the lock to be unlocked, only one coroutine eventually proceeds. Acquiring a lock is fair: the coroutine that proceeds will be the first coroutine that started waiting on the lock. When the lock is locked, reset it to unlocked and return. If the lock is unlocked, a is raised. Return if the lock is locked.\n\nAn asyncio event can be used to notify multiple asyncio tasks that some event has happened. An Event object manages an internal flag that can be set to true with the method and reset to false with the method. The method blocks until the flag is set to true. The flag is set to false initially. # Spawn a Task to wait until 'event' is set. # Sleep for 1 second and set the event. # Wait until the waiter task is finished. Wait until the event is set. If the event is set, return immediately. Otherwise block until another task calls . All tasks waiting for event to be set will be immediately awakened. Tasks awaiting on will now block until the method is called again. Return if the event is set.\n\nAn asyncio condition primitive can be used by a task to wait for some event to happen and then get exclusive access to a shared resource. In essence, a Condition object combines the functionality of an and a . It is possible to have multiple Condition objects share one Lock, which allows coordinating exclusive access to a shared resource between different tasks interested in particular states of that shared resource. The optional lock argument must be a object or . In the latter case a new Lock object is created automatically. The preferred way to use a Condition is an statement: This method waits until the underlying lock is unlocked, sets it to locked and returns . Wake up n tasks (1 by default) waiting on this condition. If fewer than n tasks are waiting they are all awakened. The lock must be acquired before this method is called and released shortly after. If called with an unlocked lock a error is raised. Return if the underlying lock is acquired. Wake up all tasks waiting on this condition. This method acts like , but wakes up all waiting tasks. The lock must be acquired before this method is called and released shortly after. If called with an unlocked lock a error is raised. When invoked on an unlocked lock, a is raised. If the calling task has not acquired the lock when this method is called, a is raised. This method releases the underlying lock, and then blocks until it is awakened by a or call. Once awakened, the Condition re-acquires its lock and this method returns . Note that a task may return from this call spuriously, which is why the caller should always re-check the state and be prepared to again. For this reason, you may prefer to use instead. The predicate must be a callable which result will be interpreted as a boolean value. The method will repeatedly until the predicate evaluates to true. The final value is the return value.\n\nA semaphore manages an internal counter which is decremented by each call and incremented by each call. The counter can never go below zero; when finds that it is zero, it blocks, waiting until some task calls . The optional value argument gives the initial value for the internal counter ( by default). If the given value is less than a is raised. The preferred way to use a Semaphore is an statement: If the internal counter is greater than zero, decrement it by one and return immediately. If it is zero, wait until a is called and return . Returns if semaphore can not be acquired immediately. Release a semaphore, incrementing the internal counter by one. Can wake up a task waiting to acquire the semaphore. Unlike , allows making more calls than calls.\n\nA barrier is a simple synchronization primitive that allows to block until parties number of tasks are waiting on it. Tasks can wait on the method and would be blocked until the specified number of tasks end up waiting on . At that point all of the waiting tasks would unblock simultaneously. can be used as an alternative to awaiting on . The barrier can be reused any number of times. # The third .wait() call passes the barrier Result of this example is: Pass the barrier. When all the tasks party to the barrier have called this function, they are all unblocked simultaneously. When a waiting or blocked task in the barrier is cancelled, this task exits the barrier which stays in the same state. If the state of the barrier is “filling”, the number of waiting task decreases by 1. The return value is an integer in the range of 0 to , different for each task. This can be used to select a task to do some special housekeeping, e.g.: # Only one task prints this This method may raise a exception if the barrier is broken or reset while a task is waiting. It could raise a if a task is cancelled. Return the barrier to the default, empty state. Any tasks waiting on it will receive the exception. If a barrier is broken it may be better to just leave it and create a new one. Put the barrier into a broken state. This causes any active or future calls to to fail with the . Use this for example if one of the tasks needs to abort, to avoid infinite waiting tasks. The number of tasks required to pass the barrier. The number of tasks currently waiting in the barrier while filling. A boolean that is if the barrier is in the broken state. This exception, a subclass of , is raised when the object is reset or broken. Changed in version 3.9: Acquiring a lock using or and/or statement ( , ) was removed. Use instead."
    },
    {
        "link": "https://medium.com/@kasperjuunge/semaphore-in-asyncio-1aaaf4038e30",
        "document": "In asyncio, a Semaphore is a synchronization primitive that allows you to limit the number of simultaneous operations in a section of code. It’s useful when you have a limited resource, like a network connection, that should not be accessed by too many tasks at the same time.\n\nThe basic usage of Semaphore involves two steps: initialization and acquiring/releasing within an function.\n\nWithin your asynchronous code, use and or the statement to acquire and automatically release the Semaphore.\n\nIn this example, even though there are 10 tasks, only 3 will be able to enter the critical section of code at the same time due to the Semaphore.\n\nSemaphores are particularly useful when:\n• You want to avoid overloading a system with too many simultaneous requests.\n• Using Statement: As seen in the example, using the statement automatically acquires and releases the semaphore, ensuring that the Semaphore is properly managed.\n• Manual Acquire and Release: Alternatively, you can manually acquire and release the Semaphore.\n\nBoth approaches ensure that the Semaphore is properly managed but using makes the code cleaner and less error-prone.\n• A Semaphore is used to limit the number of simultaneous operations in asyncio.\n• Acquire and release it using or manually with and .\n• Useful for controlling access to limited resources."
    },
    {
        "link": "https://stackoverflow.com/questions/69698479/python-threading-semaphore-vs-asyncio-semaphore",
        "document": "The whole goal of a semaphore is to provide exclusive access to something. Only one \"piece of code\" can access own the semaphore at any one time.\n\nWhat I mean by \"piece of code\" in the previous statement depends on whether I'm using multi-threading, multi-processing, or asyncio. And the means by which you guarantee exclusive access depends on what I'm using.\n\nAsyncio is the most restricted kind of multi-threading. Everything is running within a single Python thread. The Python interpreter is only executing one thing at a time. Each \"piece of code\" runs until it voluntarily waits for something to happen. Then another \"piece of code\" is allowed to run. Eventually the original piece of code runs again when the thing it was waiting on happens.\n\nWith multithreading, multiple pieces of code are running within the Python interpreter. Only one piece of code runs at any time, but they are not politely waiting for each other. Python switches from \"piece of code\" to \"piece of code\" as it wants.\n\nWith multiprocessing, multiple Pythons are running simultaneously. There is no sharing between the pieces of code, other than what is provided by the operating system. To set up a semaphore usually requires some support from the operating system to create a shared variable that all threads/processes can access.\n\nSo. Asyncio primitives are designed so that they are all run within a single Python process with the processes cooperating. They are not designed to work if multiple pieces of code try to use it simultaneously."
    },
    {
        "link": "http://docs.aiohttp.org/en/stable/client_quickstart.html",
        "document": "Eager to get started? This page gives a good introduction in how to get started with aiohttp client API.\n\nFirst, make sure that aiohttp is installed and up-to-date\n\nLet’s get started with some simple examples.\n\nBegin by importing the aiohttp module, and asyncio: Now, let’s try to get a web-page. For example let’s query : Now, we have a called and a object called . We can get all the information we need from the response. The mandatory parameter of coroutine is an HTTP url ( or class: instance). In order to make an HTTP POST request use coroutine: Other HTTP methods are available as well: To make several requests to the same site more simple, the parameter of constructor can be used. For example to request different endpoints of can be used the following code: Don’t create a session per request. Most likely you need a session per application which performs all requests together. More complex cases may require a session per site, e.g. one for Github and other one for Facebook APIs. Anyway making a session for every request is a very bad idea. A session contains a connection pool inside. Connection reusage and keep-alive (both are on by default) may speed up total performance. A session context manager usage is not mandatory but method should be called in this case, e.g.:\n\nYou often want to send some sort of data in the URL’s query string. If you were constructing the URL by hand, this data would be given as key/value pairs in the URL after a question mark, e.g. . Requests allows you to provide these arguments as a , using the keyword argument. As an example, if you wanted to pass and to , you would use the following code: You can see that the URL has been correctly encoded by printing the URL. For sending data with multiple values for the same key may be used; the library support nested lists ( ) alternative as well. It is also possible to pass a list of 2 item tuples as parameters, in that case you can specify multiple values for each key: You can also pass content as param, but beware – content is not encoded by library. Note that is not encoded: Canonicalization encodes host part by IDNA codec and applies requoting to path and query parts. For example is converted to . Sometimes canonicalization is not desirable if server accepts exact representation and does not requote URL itself. To disable canonicalization use parameter for URL construction: Passing params overrides , never use both options.\n\nWhile methods , and are very convenient you should use them carefully. All these methods load the whole response in memory. For example if you want to download several gigabyte sized files, these methods will load all the data in memory. Instead you can use the attribute. It is an instance of the class. The and transfer-encodings are automatically decoded for you: In general, however, you should use a pattern like this to save what is being streamed to a file: It is not possible to use , and after explicit reading from .\n\nTypically, you want to send some form-encoded data – much like an HTML form. To do this, simply pass a dictionary to the data argument. Your dictionary of data will automatically be form-encoded when the request is made: If you want to send data that is not form-encoded you can do it by passing a instead of a . This data will be posted directly and content-type set to ‘application/octet-stream’ by default: If you want to send JSON data: To send text with appropriate content-type just use argument:\n\nsupports multiple types of streaming uploads, which allows you to send large files without reading them into memory. As a simple case, simply provide a file-like object for your body: Or you can use asynchronous generator: # Then you can use file_sender as a data provider: Because the attribute is a (provides async iterator protocol), you can chain get and post requests together: Python 3.5 has no native support for asynchronous generators, use library as workaround. Deprecated since version 3.1: still supports decorator but this approach is deprecated in favor of asynchronous generators as shown above.\n\nYou have to use the coroutine for client websocket connection. It accepts a url as a first parameter and returns , with that object you can communicate with websocket server using response’s methods: You must use the only websocket task for both reading (e.g. or ) and writing but may have multiple writer tasks which can only send data asynchronously (by for example).\n\nBy default aiohttp uses a total 300 seconds (5min) timeout, it means that the whole operation should finish in 5 minutes. In order to allow time for DNS fallback, the default timeout is 30 seconds. The value could be overridden by timeout parameter for the session (specified in seconds): Timeout could be overridden for a request like : The maximal number of seconds for the whole operation including connection establishment, request sending and response reading. The maximal number of seconds for connection establishment of a new connection or for waiting for a free connection from a pool if pool connection limits are exceeded. The maximal number of seconds for connecting to a peer for a new connection, not given from a pool. The maximal number of seconds allowed for period between reading a new data portion from a peer. The threshold value to trigger ceiling of absolute timeout values. All fields are floats, or disables a particular timeout check, see the reference for defaults and additional details. Thus the default timeout is: aiohttp ceils timeout if the value is equal or greater than 5 seconds. The timeout expires at the next integer second greater than . The ceiling is done for the sake of optimization, when many concurrent tasks are scheduled to wake-up at the almost same but different absolute times. It leads to very many event loop wakeups, which kills performance. The optimization shifts absolute wakeup times by scheduling them to exactly the same time as other neighbors, the loop wakes up once-per-second for timeout expiration. Smaller timeouts are not rounded to help testing; in the real life network timeouts usually greater than tens of seconds. However, the default threshold value of 5 seconds can be configured using the parameter."
    },
    {
        "link": "https://apidog.com/blog/aiohttp-request",
        "document": "AIOHTTP is an asynchronous HTTP client library for Python, allowing its users to make asynchronous requests to web servers. With AIOHTTP, you can interact with web APIs and services with ease. However, what comprises an AIOHTTP request?\n\nAIOHTTP requests are requests used for making a large number of HTTP requests concurrently and efficiently. This means you do not need to make requests individually, thus saving much time.\n\nAIOHTTP requests have a few specialties that you should be aware of.\n• Non-Blocking I/O (Input/Output): AIOHTTP leverages asynchronous I/O, meaning it doesn't wait for a single request to finish before starting another. This allows it to handle multiple requests concurrently, improving performance compared to traditional synchronous libraries.\n• Event Loop Integration: AIOHTTP works seamlessly with Python's asyncio library and integrates with the event loop. The event loop efficiently manages concurrent tasks and network I/O, ensuring the smooth execution of multiple requests.\n• HTTP Methods Support: AIOHTTP supports all standard HTTP methods like GET, POST, PUT, DELETE, etc., allowing you to interact with web servers in various ways.\n• Customizable Headers & Data: You have full control over request headers and data. You can specify custom headers like authorization tokens or content type, and send data in various formats like JSON, form data, or raw bytes.\n• Timeouts and Retries: Define timeouts for requests to prevent hanging applications. Additionally, AIOHTTP allows implementing retry logic for failed requests, improving robustness.\n• Streaming Responses: AIOHTTP allows streaming large responses in chunks, processing data incrementally without loading the entire response into memory. This is beneficial for handling large files or real-time data streams.\n• Proxies: You can configure AIOHTTP to use proxies for making requests, providing additional flexibility for network routing or security purposes.\n• Cancellation: Requests can be canceled before completion, allowing you to gracefully stop processing if needed.\n• Asynchronous Framework Compatibility: AIOHTTP integrates well with asynchronous web frameworks like Quart or Sanic, enabling efficient handling of HTTP requests within these frameworks.\n• Third-Party Libraries: A rich ecosystem of libraries built on top of AIOHTTP exists, providing functionalities like JSON parsing, authentication handling, and more.\n• Non-Blocking I/O: Unlike synchronous libraries like Requests, AIOHTTP utilizes non-blocking I/O. This allows it to initiate multiple requests simultaneously, improving performance significantly compared to waiting for each request to finish before starting the next.\n• Event Loop Integration: AIOHTTP seamlessly integrates with Python's asyncio library and event loop. This ensures efficient management of concurrent tasks and network I/O, maximizing resource utilization and minimizing waiting times.\n• Connection Pooling: AIOHTTP's ClientSession maintains a pool of reusable connections. Instead of establishing new connections for every request, it reuses existing ones whenever possible. This significantly reduces connection overhead, leading to faster communication.\n• Simplified State Management: ClientSession automatically handles cookies between requests within a session. This eliminates the need for manual cookie management, simplifying state handling for authenticated interactions.\n• Robust Error Handling: ClientSession acts as a context manager, ensuring proper connection closure even if exceptions occur. This prevents resource leaks and improves application reliability.\n• Streaming Responses: AIOHTTP allows you to process large responses in chunks. This is particularly beneficial for handling massive files or real-time data streams, as it avoids loading the entire response into memory at once.\n• Proxy Support: You can configure AIOHTTP to use proxies for making requests. This provides additional flexibility for network routing or security purposes.\n• Cancellation Capabilities: Requests can be canceled before completion. This allows for graceful termination of operations if needed, adding another layer of control.\n• Asynchronous Framework Compatibility: AIOHTTP integrates well with asynchronous web frameworks like Quart or Sanic. This enables efficient handling of HTTP requests within these frameworks, leading to performant and scalable web applications.\n• Third-Party Library Support: A rich ecosystem of libraries built on top of AIOHTTP exists. These libraries offer functionalities like JSON parsing, authentication handling, and more, extending AIOHTTP's capabilities for various use cases.\n\n1. Simple GET request (httpbin.org is a popular service for testing HTTP requests):\n\nThis code fetches data from the endpoint of httpbin.org using a GET request. The response is later parsed as JSON, and printed.\n\nThis example sends a POST request to an API endpoint with JSON data ( dictionary). The method is there to ensure an exception is raised for any unsuccessful response (status code outside the 2xx range).\n\nThe code downloads a file ( ) from a URL, and uses the parameter to download the file in chunks, improving the memory efficiency for larger files.\n\nThe code example shows sending a GET request with custom headers (authorization token(, and a timeout of 5 seconds.\n\nIf you are struggling with client-sided coding, worry no longer!\n\nIntroducing Apidog, an all-in-one API development tool that allows users to create APIs from scratch, test them, and even run modifications on existing APIs! Once you have finished designing your API, you can also proceed with generating beautiful API documentation for your API consumers to read.\n\nApidog is particularly useful for newer developers learning how to create applications and APIs. Apidog's code generation feature can provide code templates within a few clicks of a button. Let's see how to do it on Apidog!\n\nTo utilize Apidog's code generation feature, begin by clicking the button found on the top right corner of the Apidog window, and press .\n\nNext, select the section, where you can find different frameworks for the JavaScript language. In this step, select , and copy the code. You can then paste it over to your other code platform to implement an AIOHTTP request!\n\nIf you are looking for more APIs to consume for your application or project, you can also consider checking out Apidog's API Hub, where you can find thousands of third-party APIs in the form of projects.\n\nAll you need to do is find one that interests you, and you can then check out the APIs in detail, so you can understand how the API functions - and perhaps create your very own API!\n\nAIOHTTP requests are a powerful tool for making asynchronous HTTP requests in Python. They excel at handling a high volume of concurrent requests efficiently. This is achieved through features like non-blocking I/O, event loop integration, and connection pooling within the ClientSession class.\n\nAIOHTTP also offers advanced features like streaming responses, proxy support, and cancellation capabilities, making it a versatile solution for various web communication needs.\n\nAdditionally, its rich ecosystem of third-party libraries and smooth integration with asynchronous web frameworks solidify AIOHTTP's position as a preferred choice for building performant and scalable web applications that heavily rely on asynchronous HTTP communication.\n\nLastly, Apidog can be the perfect API development tool for you if you are trying to save more time or focus on other aspects of API developing. With advanced features like code generation and multiple-step testing, you can boost your work efficiency by a mile."
    },
    {
        "link": "https://docs.aiohttp.org",
        "document": "For speeding up DNS resolving by client API you may install aiodns as well. This option is highly recommended: Installing all speedups in one command¶ The following will get you along with aiodns and in one bundle. No need to type separate commands anymore!\n\nWhen writing your code, we recommend enabling Python’s development mode ( ). In addition to the extra features enabled for asyncio, aiohttp will:\n• None Use a strict parser in the client code (which can help detect malformed responses from a server).\n• None Enable some additional checks (resulting in warnings in certain situations).\n\nGo to What’s new in aiohttp 3.0 page for aiohttp 3.0 major release changes.\n\nThe project is hosted on GitHub Please feel free to file an issue on the bug tracker if you have found a bug or have some suggestion in order to improve the library.\n\nFeel free to post your questions and ideas here. We support Stack Overflow. Please add aiohttp tag to your question there.\n\nThe package is written mostly by Nikolay Kim and Andrew Svetlov. Feel free to improve this package and send a pull request to GitHub.\n\nAfter deprecating some Public API (method, class, function argument, etc.) the library guaranties the usage of deprecated API is still allowed at least for a year and half after publishing new release with deprecation. All deprecations are reflected in documentation and raises . Sometimes we are forced to break the own rule for sake of very strong reason. Most likely the reason is a critical bug which cannot be solved without major API change, but we are working hard for keeping these changes as rare as possible."
    },
    {
        "link": "https://stackoverflow.com/questions/9110593/asynchronous-requests-with-python-requests",
        "document": "With async.map(rs) , I get the response codes, but I want to get the content of each page requested. This, for example, does not work:\n\nI tried the sample provided within the documentation of the requests library for python.\n\nThe below answer is not applicable to requests v0.13.0+. The asynchronous functionality was moved to grequests after this question was written. However, you could just replace with below and it should work. I've left this answer as is to reflect the original question which was about using requests < v0.13.0. To do multiple tasks with asynchronously you have to:\n• Define a function for what you want to do with each object (your task)\n• Add that function as an event hook in your request\n• Call on a list of all the requests / actions from requests import async # If using requests > v0.13.0, use # from grequests import async urls = [ 'http://python-requests.org', 'http://httpbin.org', 'http://python-guide.org', 'http://kennethreitz.com' ] # A simple task to do to each response object def do_something(response): print response.url # A list to hold our things to do via async async_list = [] for u in urls: # The \"hooks = {...\" part is where you define what you want to do # # Note the lack of parentheses following do_something, this is # because the response will be used as the first argument automatically action_item = async.get(u, hooks = {'response' : do_something}) # Add the task to our list of things to do via async async_list.append(action_item) # Do our list of things to do via async async.map(async_list)\n\nUnfortunately, as far as I know, the requests library is not equipped for performing asynchronous requests. You can wrap syntax around , but that will make the underlying requests no less synchronous. If you want true async requests, you must use other tooling that provides it. One such solution is (Python 3.5.3+). It works well in my experience using it with the Python 3.7 syntax. Below I write three implementations of performing n web requests using\n• Synchronous requests ( ) using the Python library wrapped in Python 3.7 syntax and\n• A truly asynchronous implementation ( ) with the Python library wrapped in Python 3.7 syntax and \"\"\" Tested in Python 3.5.10 \"\"\" import time import asyncio import requests import aiohttp from asgiref import sync def timed(func): \"\"\" records approximate durations of function calls \"\"\" def wrapper(*args, **kwargs): start = time.time() print('{name:<30} started'.format(name=func.__name__)) result = func(*args, **kwargs) duration = \"{name:<30} finished in {elapsed:.2f} seconds\".format( name=func.__name__, elapsed=time.time() - start ) print(duration) timed.durations.append(duration) return result return wrapper timed.durations = [] @timed def sync_requests_get_all(urls): \"\"\" performs synchronous get requests \"\"\" # use session to reduce network overhead session = requests.Session() return [session.get(url).json() for url in urls] @timed def async_requests_get_all(urls): \"\"\" asynchronous wrapper around synchronous requests \"\"\" session = requests.Session() # wrap requests.get into an async function def get(url): return session.get(url).json() async_get = sync.sync_to_async(get) async def get_all(urls): return await asyncio.gather(*[ async_get(url) for url in urls ]) # call get_all as a sync function to be used in a sync context return sync.async_to_sync(get_all)(urls) @timed def async_aiohttp_get_all(urls): \"\"\" performs asynchronous get requests \"\"\" async def get_all(urls): async with aiohttp.ClientSession() as session: async def fetch(url): async with session.get(url) as response: return await response.json() return await asyncio.gather(*[ fetch(url) for url in urls ]) # call get_all as a sync function to be used in a sync context return sync.async_to_sync(get_all)(urls) if __name__ == '__main__': # this endpoint takes ~3 seconds to respond, # so a purely synchronous implementation should take # little more than 30 seconds and a purely asynchronous # implementation should take little more than 3 seconds. urls = ['https://postman-echo.com/delay/3']*10 async_aiohttp_get_all(urls) async_requests_get_all(urls) sync_requests_get_all(urls) print('----------------------') [print(duration) for duration in timed.durations] On my machine, this is the output: async_aiohttp_get_all started async_aiohttp_get_all finished in 3.20 seconds async_requests_get_all started async_requests_get_all finished in 30.61 seconds sync_requests_get_all started sync_requests_get_all finished in 30.59 seconds ---------------------- async_aiohttp_get_all finished in 3.20 seconds async_requests_get_all finished in 30.61 seconds sync_requests_get_all finished in 30.59 seconds\n\nI have a lot of issues with most of the answers posted - they either use deprecated libraries that have been ported over with limited features, or provide a solution with too much magic on the execution of the request, making it difficult to error handle. If they do not fall into one of the above categories, they're 3rd party libraries or deprecated. Some of the solutions works alright purely in http requests, but the solutions fall short for any other kind of request, which is ludicrous. A highly customized solution is not necessary here. Simply using the python built-in library is sufficient enough to perform asynchronous requests of any type, as well as providing enough fluidity for complex and usecase specific error handling. import asyncio loop = asyncio.get_event_loop() def do_thing(params): async def get_rpc_info_and_do_chores(id): # do things response = perform_grpc_call(id) do_chores(response) async def get_httpapi_info_and_do_chores(id): # do things response = requests.get(URL) do_chores(response) async_tasks = [] for element in list(params.list_of_things): async_tasks.append(loop.create_task(get_chan_info_and_do_chores(id))) async_tasks.append(loop.create_task(get_httpapi_info_and_do_chores(ch_id))) loop.run_until_complete(asyncio.gather(*async_tasks)) How it works is simple. You're creating a series of tasks you'd like to occur asynchronously, and then asking a loop to execute those tasks and exit upon completion. No extra libraries subject to lack of maintenance, no lack of functionality required.\n\nDISCLAMER: Following code creates different threads for each function. This might be useful for some of the cases as it is simpler to use. But know that it is not async but gives illusion of async using multiple threads, even though decorator suggests that. You can use the following decorator to give a callback once the execution of function is completed, the callback must handle the processing of data returned by the function. Please note that after the function is decorated it will return a object. import asyncio ## Decorator implementation of async runner !! def run_async(callback, loop=None): if loop is None: loop = asyncio.get_event_loop() def inner(func): def wrapper(*args, **kwargs): def __exec(): out = func(*args, **kwargs) callback(out) return out return loop.run_in_executor(None, __exec) return wrapper return inner urls = [\"https://google.com\", \"https://facebook.com\", \"https://apple.com\", \"https://netflix.com\"] loaded_urls = [] # OPTIONAL, used for showing realtime, which urls are loaded !! def _callback(resp): print(resp.url) print(resp) loaded_urls.append((resp.url, resp)) # OPTIONAL, used for showing realtime, which urls are loaded !! # Must provide a callback function, callback func will be executed after the func completes execution # Callback function will accept the value returned by the function. @run_async(_callback) def get(url): return requests.get(url) for url in urls: get(url) If you wish to see which url are loaded in real-time then, you can add the following code at the end as well:\n\nI second the suggestion above to use HTTPX, but I often use it in a different way so am adding my answer. I personally use (introduced in Python 3.7) rather than and also prefer the approach, which can be used in combination with asyncio and httpx. As in this example I just posted, this style is helpful for processing a set of URLs asynchronously even despite the (common) occurrence of errors. I particularly like how that style clarifies where the response processing occurs and for ease of error handling (which I find async calls tend to give more of). It's easier to post a simple example of just firing off a bunch of requests asynchronously, but often you also want to handle the response content (compute something with it, perhaps with reference to the original object that the URL you requested was to do with). The core of that approach looks like:\n• is the input list (which the generator of URL strings came from), e.g. a list of objects/dictionaries\n• is a progress bar (e.g. ) [optional but useful] All of that goes in an async function which is then run by calling a synchronous 'top-level' function named e.g. which runs the coroutine [this is what's returned by an async function] and manages the event loop: Since a list passed as input (here it's ) can be modified in-place, you can effectively get output back (as we're used to from synchronous function calls)\n\nNon of the answers above helped me because they assume that you have a predefined list of requests, while in my case i need to be able to listen to requests and respond asynchronously (in similar way to how it works in nodejs). def handle_finished_request(r, **kwargs): print(r) # while True: def main(): while True: address = listen_to_new_msg() # based on your server # schedule async requests and run 'handle_finished_request' on response req = grequests.get(address, timeout=1, hooks=dict(response=handle_finished_request)) job = grequests.send(req) # does not block! for more info see https://stackoverflow.com/a/16016635/10577976 main() the callback would be called when a response is received. note: for some reason timeout (or no response) does not trigger error here This simple loop can trigger async requests similarly to how it would work in nodejs server"
    },
    {
        "link": "https://medium.com/@API4AI/aiohttp-vs-requests-comparing-python-http-libraries-b17ef3560559",
        "document": "In the realm of software development, particularly in web services and applications, the ability to handle HTTP requests efficiently and effectively is paramount. Python, renowned for its simplicity and power, offers a plethora of libraries to manage these HTTP interactions. Among these libraries, two stand out for their unique features and widespread usage: AIOHTTP and Requests. Understanding the strengths and limitations of these libraries is crucial for developers, as the choice can significantly impact the performance, scalability, and maintainability of applications.\n\nThe importance of selecting the right HTTP library cannot be overstated. Each library has its own approach to handling HTTP requests and responses, with variations in syntax, speed, ease of use, and functionality. The right choice can streamline development processes, improve application performance, and ensure better resource management. Conversely, the wrong choice can lead to unnecessary complexity, performance bottlenecks, and scalability issues.\n\nTo fairly compare AIOHTTP and Requests, we’ll consider several criteria:\n• Performance: How do these libraries perform under various loads and what is their impact on application speed and efficiency?\n• Ease of Use: The learning curve, readability, and simplicity of the libraries, which can significantly affect development time and maintenance.\n• Asynchronous Support: With the growing need for handling concurrent processes in modern web applications, understanding how these libraries manage asynchronous operations is vital.\n• Community Support and Ecosystem: The resources available, such as documentation, community support, and extensibility through additional packages or integrations.\n\nThrough this comparison, we aim to provide an understanding of AIOHTTP and Requests, guiding Python developers in choosing the most suitable library for their specific needs and project requirements. Whether you’re building a high-performance web server, a simple data fetching script, or anything in between, knowing the capabilities and limitations of these libraries is a key step in your development journey.\n\nAIOHTTP stands out in the Python landscape as an asynchronous HTTP client/server framework. It is built on top of Python’s asyncio library, allowing it to handle HTTP requests in a non-blocking, concurrent manner. This makes AIOHTTP particularly suitable for scenarios where handling many simultaneous connections is crucial.\n• Support for WebSockets: Facilitates real-time communication between client and server.\n\nAIOHTTP’s asynchronous capabilities are its standout feature, enabling efficient handling of large numbers of concurrent connections. This is a significant advantage in developing high-performance web applications where traditional synchronous handling of requests would be a bottleneck.\n\nBasic Example of Making an HTTP Request\n\nHere’s a simple example of how to use AIOHTTP to make an asynchronous HTTP GET request:\n\nThis code snippet demonstrates the typical structure of an asynchronous program using AIOHTTP, where asyncio.run() is the entry point for the asynchronous routine.\n\nThe most significant advantage of AIOHTTP is its native support for asynchronous programming. This allows for handling a large number of simultaneous network connections efficiently, making it ideal for applications like web servers, chat applications, and other real-time data processing services.\n\nDue to its non-blocking nature, AIOHTTP can offer superior performance, especially in I/O-bound and high-concurrency applications. This performance benefit becomes more pronounced as the load and the number of concurrent connections increase.\n• Real-time Web Applications: Ideal for applications requiring real-time data exchange, like chat applications or live updates.\n• Microservices Architecture: Fits well in scenarios where numerous small, independent services are concurrently communicating.\n• I/O-bound Services: Highly effective for I/O-bound workloads where handling many simultaneous connections is essential.\n\nThe asynchronous model can be challenging for developers not familiar with async/await syntax. It requires a different mindset compared to traditional synchronous programming.\n\nMixing synchronous and asynchronous code can be problematic, often leading to issues like deadlocks or performance bottlenecks. Developers need to be cautious when integrating AIOHTTP into existing synchronous Python applications.\n\nDebugging asynchronous code can be more complex than traditional synchronous code. The stack traces in asynchronous programming can be less intuitive, and tracking down bugs might require a deeper understanding of asyncio internals.\n\nRequests is one of the most popular and user-friendly HTTP libraries in the Python community. Designed with simplicity in mind, it provides an easy-to-use interface for sending HTTP requests and handling responses.\n• Robust: Can handle various types of HTTP requests with minimal lines of code.\n• Compatibility: Works well with Python’s standard libraries and various environments.\n• Extensive Documentation: Well-documented, making it accessible for beginners and professionals alike.\n\nRequests operates in a synchronous manner, meaning each HTTP request blocks the execution of subsequent lines of code until it receives a response. This makes the library intuitive and easy to use, particularly for simple scripts and applications where concurrency is not a primary concern.\n\nRequests can be installed easily using pip:\n\nBasic Example of Making an HTTP Request\n\nThe following example demonstrates making a simple GET request using Requests:\n\nThis code fetches a page content from python.org and prints the status code and response text, showcasing the library’s simplicity.\n\nEase of Use and Simplicity\n\nRequests is renowned for its simplicity. Its straightforward syntax makes it easy for developers to make HTTP requests without the overhead of handling the complexities of the underlying protocols.\n\nBeing one of the most popular Python libraries, Requests enjoys a wide user base and community support. This popularity ensures an abundance of resources, including tutorials, forums, and third-party tools, making it a safe choice for many developers.\n\nUse Cases Where Requests is Ideal\n• Simple HTTP Requests: Perfect for applications where basic HTTP requests are needed without the complexities of asynchronous programming.\n• Data Fetching and Integration: Ideal for scripts that integrate with RESTful APIs or for data fetching tasks.\n• Educational Purposes: Often used in educational settings due to its simplicity, aiding in teaching HTTP concepts without the complexity of asynchronous programming.\n\nRequests does not support asynchronous programming natively. This can be a significant drawback for applications requiring high concurrency or dealing with a large number of simultaneous connections.\n\nIn scenarios where I/O operations are a bottleneck, the synchronous nature of Requests can lead to performance issues, as each I/O operation blocks the thread until completion.\n\nWhile Requests is great for straightforward HTTP requests, handling more complex or advanced features of the HTTP protocol can be less intuitive and might require additional handling or third-party libraries.\n\nWhen comparing AIOHTTP and Requests, it’s essential to consider several key aspects: ease of use, scalability and concurrency, and suitability for large-scale applications. Let’s examine these factors using the context of using the NSFW Image Classification API developed by API4AI as an example."
    }
]