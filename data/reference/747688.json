[
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview",
        "document": "HTTP is a protocol for fetching resources such as HTML documents. It is the foundation of any data exchange on the Web and it is a client-server protocol, which means requests are initiated by the recipient, usually the Web browser. A complete document is typically constructed from resources such as text content, layout instructions, images, videos, scripts, and more. Clients and servers communicate by exchanging individual messages (as opposed to a stream of data). The messages sent by the client are called requests and the messages sent by the server as an answer are called responses. Designed in the early 1990s, HTTP is an extensible protocol which has evolved over time. It is an application layer protocol that is sent over TCP, or over a TLS-encrypted TCP connection, though any reliable transport protocol could theoretically be used. Due to its extensibility, it is used to not only fetch hypertext documents, but also images and videos or to post content to servers, like with HTML form results. HTTP can also be used to fetch parts of documents to update Web pages on demand.\n\nHTTP is a client-server protocol: requests are sent by one entity, the user-agent (or a proxy on behalf of it). Most of the time the user-agent is a Web browser, but it can be anything, for example, a robot that crawls the Web to populate and maintain a search engine index. Each individual request is sent to a server, which handles it and provides an answer called the response. Between the client and the server there are numerous entities, collectively called proxies, which perform different operations and act as gateways or caches, for example. In reality, there are more computers between a browser and the server handling the request: there are routers, modems, and more. Thanks to the layered design of the Web, these are hidden in the network and transport layers. HTTP is on top, at the application layer. Although important for diagnosing network problems, the underlying layers are mostly irrelevant to the description of HTTP.\n\nThe user-agent is any tool that acts on behalf of the user. This role is primarily performed by the Web browser, but it may also be performed by programs used by engineers and Web developers to debug their applications. The browser is always the entity initiating the request. It is never the server (though some mechanisms have been added over the years to simulate server-initiated messages). To display a Web page, the browser sends an original request to fetch the HTML document that represents the page. It then parses this file, making additional requests corresponding to execution scripts, layout information (CSS) to display, and sub-resources contained within the page (usually images and videos). The Web browser then combines these resources to present the complete document, the Web page. Scripts executed by the browser can fetch more resources in later phases and the browser updates the Web page accordingly. A Web page is a hypertext document. This means some parts of the displayed content are links, which can be activated (usually by a click of the mouse) to fetch a new Web page, allowing the user to direct their user-agent and navigate through the Web. The browser translates these directions into HTTP requests, and further interprets the HTTP responses to present the user with a clear response.\n\nBetween the Web browser and the server, numerous computers and machines relay the HTTP messages. Due to the layered structure of the Web stack, most of these operate at the transport, network or physical levels, becoming transparent at the HTTP layer and potentially having a significant impact on performance. Those operating at the application layers are generally called proxies. These can be transparent, forwarding on the requests they receive without altering them in any way, or non-transparent, in which case they will change the request in some way before passing it along to the server. Proxies may perform numerous functions:\n• caching (the cache can be public or private, like the browser cache)\n• filtering (like an antivirus scan or parental controls)\n• load balancing (to allow multiple servers to serve different requests)\n• authentication (to control access to different resources)\n\nA connection is controlled at the transport layer, and therefore fundamentally out of scope for HTTP. HTTP doesn't require the underlying transport protocol to be connection-based; it only requires it to be reliable, or not lose messages (at minimum, presenting an error in such cases). Among the two most common transport protocols on the Internet, TCP is reliable and UDP isn't. HTTP therefore relies on the TCP standard, which is connection-based. Before a client and server can exchange an HTTP request/response pair, they must establish a TCP connection, a process which requires several round-trips. The default behavior of HTTP/1.0 is to open a separate TCP connection for each HTTP request/response pair. This is less efficient than sharing a single TCP connection when multiple requests are sent in close succession. In order to mitigate this flaw, HTTP/1.1 introduced pipelining (which proved difficult to implement) and persistent connections: the underlying TCP connection can be partially controlled using the header. HTTP/2 went a step further by multiplexing messages over a single connection, helping keep the connection warm and more efficient. Experiments are in progress to design a better transport protocol more suited to HTTP. For example, Google is experimenting with QUIC which builds on UDP to provide a more reliable and efficient transport protocol.\n\nWhen a client wants to communicate with a server, either the final server or an intermediate proxy, it performs the following steps:\n• Open a TCP connection: The TCP connection is used to send a request, or several, and receive an answer. The client may open a new connection, reuse an existing connection, or open several TCP connections to the servers.\n• Send an HTTP message: HTTP messages (before HTTP/2) are human-readable. With HTTP/2, these messages are encapsulated in frames, making them impossible to read directly, but the principle remains the same. For example:\n• Read the response sent by the server, such as: HTTP/1.1 200 OK Date: Sat, 09 Oct 2010 14:28:02 GMT Server: Apache Last-Modified: Tue, 01 Dec 2009 20:18:22 GMT ETag: \"51142bc1-7449-479b075b2891b\" Accept-Ranges: bytes Content-Length: 29769 Content-Type: text/html <!doctype html>… (here come the 29769 bytes of the requested web page)\n• Close or reuse the connection for further requests. If HTTP pipelining is activated, several requests can be sent without waiting for the first response to be fully received. HTTP pipelining has proven difficult to implement in existing networks, where old pieces of software coexist with modern versions. HTTP pipelining has been superseded in HTTP/2 with more robust multiplexing requests within a frame."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/HTTP/Guides/Overview",
        "document": "HTTP is a protocol for fetching resources such as HTML documents. It is the foundation of any data exchange on the Web and it is a client-server protocol, which means requests are initiated by the recipient, usually the Web browser. A complete document is typically constructed from resources such as text content, layout instructions, images, videos, scripts, and more. Clients and servers communicate by exchanging individual messages (as opposed to a stream of data). The messages sent by the client are called requests and the messages sent by the server as an answer are called responses. Designed in the early 1990s, HTTP is an extensible protocol which has evolved over time. It is an application layer protocol that is sent over TCP, or over a TLS-encrypted TCP connection, though any reliable transport protocol could theoretically be used. Due to its extensibility, it is used to not only fetch hypertext documents, but also images and videos or to post content to servers, like with HTML form results. HTTP can also be used to fetch parts of documents to update Web pages on demand.\n\nHTTP is a client-server protocol: requests are sent by one entity, the user-agent (or a proxy on behalf of it). Most of the time the user-agent is a Web browser, but it can be anything, for example, a robot that crawls the Web to populate and maintain a search engine index. Each individual request is sent to a server, which handles it and provides an answer called the response. Between the client and the server there are numerous entities, collectively called proxies, which perform different operations and act as gateways or caches, for example. In reality, there are more computers between a browser and the server handling the request: there are routers, modems, and more. Thanks to the layered design of the Web, these are hidden in the network and transport layers. HTTP is on top, at the application layer. Although important for diagnosing network problems, the underlying layers are mostly irrelevant to the description of HTTP.\n\nThe user-agent is any tool that acts on behalf of the user. This role is primarily performed by the Web browser, but it may also be performed by programs used by engineers and Web developers to debug their applications. The browser is always the entity initiating the request. It is never the server (though some mechanisms have been added over the years to simulate server-initiated messages). To display a Web page, the browser sends an original request to fetch the HTML document that represents the page. It then parses this file, making additional requests corresponding to execution scripts, layout information (CSS) to display, and sub-resources contained within the page (usually images and videos). The Web browser then combines these resources to present the complete document, the Web page. Scripts executed by the browser can fetch more resources in later phases and the browser updates the Web page accordingly. A Web page is a hypertext document. This means some parts of the displayed content are links, which can be activated (usually by a click of the mouse) to fetch a new Web page, allowing the user to direct their user-agent and navigate through the Web. The browser translates these directions into HTTP requests, and further interprets the HTTP responses to present the user with a clear response.\n\nBetween the Web browser and the server, numerous computers and machines relay the HTTP messages. Due to the layered structure of the Web stack, most of these operate at the transport, network or physical levels, becoming transparent at the HTTP layer and potentially having a significant impact on performance. Those operating at the application layers are generally called proxies. These can be transparent, forwarding on the requests they receive without altering them in any way, or non-transparent, in which case they will change the request in some way before passing it along to the server. Proxies may perform numerous functions:\n• caching (the cache can be public or private, like the browser cache)\n• filtering (like an antivirus scan or parental controls)\n• load balancing (to allow multiple servers to serve different requests)\n• authentication (to control access to different resources)\n\nA connection is controlled at the transport layer, and therefore fundamentally out of scope for HTTP. HTTP doesn't require the underlying transport protocol to be connection-based; it only requires it to be reliable, or not lose messages (at minimum, presenting an error in such cases). Among the two most common transport protocols on the Internet, TCP is reliable and UDP isn't. HTTP therefore relies on the TCP standard, which is connection-based. Before a client and server can exchange an HTTP request/response pair, they must establish a TCP connection, a process which requires several round-trips. The default behavior of HTTP/1.0 is to open a separate TCP connection for each HTTP request/response pair. This is less efficient than sharing a single TCP connection when multiple requests are sent in close succession. In order to mitigate this flaw, HTTP/1.1 introduced pipelining (which proved difficult to implement) and persistent connections: the underlying TCP connection can be partially controlled using the header. HTTP/2 went a step further by multiplexing messages over a single connection, helping keep the connection warm and more efficient. Experiments are in progress to design a better transport protocol more suited to HTTP. For example, Google is experimenting with QUIC which builds on UDP to provide a more reliable and efficient transport protocol.\n\nWhen a client wants to communicate with a server, either the final server or an intermediate proxy, it performs the following steps:\n• Open a TCP connection: The TCP connection is used to send a request, or several, and receive an answer. The client may open a new connection, reuse an existing connection, or open several TCP connections to the servers.\n• Send an HTTP message: HTTP messages (before HTTP/2) are human-readable. With HTTP/2, these messages are encapsulated in frames, making them impossible to read directly, but the principle remains the same. For example:\n• Read the response sent by the server, such as: HTTP/1.1 200 OK Date: Sat, 09 Oct 2010 14:28:02 GMT Server: Apache Last-Modified: Tue, 01 Dec 2009 20:18:22 GMT ETag: \"51142bc1-7449-479b075b2891b\" Accept-Ranges: bytes Content-Length: 29769 Content-Type: text/html <!doctype html>… (here come the 29769 bytes of the requested web page)\n• Close or reuse the connection for further requests. If HTTP pipelining is activated, several requests can be sent without waiting for the first response to be fully received. HTTP pipelining has proven difficult to implement in existing networks, where old pieces of software coexist with modern versions. HTTP pipelining has been superseded in HTTP/2 with more robust multiplexing requests within a frame."
    },
    {
        "link": "https://netburner.com/learn/an-introduction-to-http-protocol?srsltid=AfmBOorP5yMicjbL_AaKGDwh90tYFSkPUuC2rcwHmaRyZy0UtGb1mWcS",
        "document": "HTTP is the acronym for HyperText Transfer Protocol. It was created to transfer HTML files between computers in a laboratory environment. Since then, it has evolved; nowadays, it’s used to obtain high-resolution videos, images, download software, etc. HTTP is a protocol that is used everywhere there is a Web device. For example, for you to read this article, a series of HTTP commands had to be executed to bring your browser these lines of text, the colors of the web page, the images, the links … practically everything.\n\nThese are the messages sent by the web client (also known as User-Agent), such as a browser. These messages tell the server what action to take and thereby determine the required response. An example of a request message is shown in the figure below.\n\nOptional values that give the server additional instructions on how to handle the request. The structure of a header is as follows: These can be classified into three types:\n• Request / Response headers: Describe additional information about the requested resource or unique characteristics that the client/server requires, such as user identification using tokens or cookies or the content types that the client can understand.\n• Representation headers: Describe information about the body, such as data compression, encoding, etc.\n• Payload headers: Contain information about the payload, such as the length of the data or the encoding used.\n\nWeb server: These examples can be found at the path, . Here, you’ll find examples of serving embedded web pages and handling GET and POST requests. Secure web server: In the directory, , you’ll find an example of how to start a secure web server that serves pages over a TLS connection. Web server using EFFS-FAT: In the directory, , you’ll find an example of serving a web page hosted on external SD memory and accessing the files inside it is using the path /DIR. Web server using EFFS-STD: In the path, , you’ll find an example of how to serve files located in the flash on-chip file system. Configuration Web server: In the path, , you’ll find some examples of how to use and customize the configuration server hosted on port 20034.\n\nWeb Client: In the directory, are examples of making GET and POST requests to servers hosted on the internet using auto-execution and straightforward tasks. Secure Web Client: In the directory , you’ll find and example of how to create a HTTPS request to a server. And in and you have two examples of how to use peer verify with SSL/TLS. Serial to HTTP: This is an example of a GET request sent through the serial port, used for serial to Ethernet applications. It can be found in the directory ."
    },
    {
        "link": "https://kb.globalscape.com/Knowledgebase/11312/Configuration-and-Security-Best-Practices-Checklist",
        "document": "THE INFORMATION IN THIS ARTICLE APPLIES TO:\n\nBelow is a checklist of suggestions and guidelines for installing, configuring, and deploying EFT in a production environment, including best practices for security (after the Configuration Checklist).\n\nAs with any mission-critical software or hardware, it is recommended that a testing, validation, development, or usability lab be established to provide a \"sandbox\" into which EFT and DMZ Gateway Server software can be deployed. This initial deployment allows for validation of the interoperability with other dependent components as well the validation of expected usage scenarios.\n\nThe lab environment should emulate (if not duplicate) the production environment at a network topography and application level. To do this, a clear vision of the production network and the proposed deployment of EFT and DMZ Gateway must exist. Typical deployments of EFT and DMZ Gateway consist of many other components from the enterprise, including Active Directory Server, SQL Server, SMTP Server, and a storage system such as a SAN. For DMZ Gateway, a firewall such as Microsoft ISA might be applicable. Finally, some deployments also include Clustering, in which case various components are replicated to provide clustered resources.\n\nFor increased business continuity and risk mitigation, you should use the development lab environment as the starting point for any configuration changes in the system. That is, make the change in development and validate it prior to making the change in production. A good testing tool is CuteFTP.\n\nThe installation and configuration of EFT in either a lab or a production environment should be validated by EFT administrators/operators to ensure that the functions are working as expected.\n\nMake sure that the EFT Server service is started on the computer. Make sure that the service is listening on the expected IP:PORT socket addresses on EFT. (To view the listening sockets, use \"netstat -ona\" from a command line or an application such as PrcView or TcpView.) Check the Event Viewer log to ensure that there are no errors in the Application log related to EFT or DMZ Gateway. Confirm that the administration interface shows the status of the system when it is launched and connected to EFT. For each Site on EFT, ensure that the expected user accounts exist. To ensure that authentication is working as expected, attempt to log in to EFT as a user account on the system (using any protocol). To confirm that permissions for the user account are working as expected, attempt a file transfer. For each protocol enabled on EFT, attempt a connection directly to EFT using a client that supports that protocol. For each protocol enabled through DMZ Gateway, attempt a connection to the appropriate DMZ Gateway IP:PORT and confirm that this route works as expected. View the audit traces generated by the validation steps above. Confirm that the Auditing and Reporting module database has been populated with appropriate data (using either EFT Reporting interface or direct access to the SQL Server being used). Confirm that the text log files generated by EFT have been populated with the appropriate data. Each customer has a unique set of Event Rule/workflow requirements, but these are the general validation steps. Confirm the following are working as expected: . Test e-mail notifications by triggering an Event Rule that has an e-mail notification Action to confirm that Event Rules fire and that the SMTP configuration is correct.\n\n \n\n Move/Copy/Download actions. Initiate Event Rules that perform remote file uploads/copies/download so that connectivity originating from EFT to a remote system is properly configured. In this step, also confirm that a log file is generated that audits outbound connection information (a \"cl*.log\" file in the designated Server Log File location). Custom Commands. EFT is responsible for triggering those external commands, so that is what should be validated with respect to EFT. Any actions carried out by those external tools should be validated independently. Confirm that a \"CMDOUT.LOG\" file is generated as the result of an invoked Custom Command. Folder Monitor Rules. Ensure that the Event Rules are properly enabled and responsive to files added to the folder being monitored. For failover cluster deployments, the failover and failback operations of the cluster should be confirmed. After a failover/failback, confirm that the newly active server behaves properly; that is, the failover is transparent and the configuration/operation is as expected. This can be summarized by the prior set of tests operating against the newly active node in the cluster. If you expect high volumes of traffic or back-end processing within EFT, you should verify that the resource utilization levels on the Server are within acceptable tolerances. There are numerous load-testing tools available, ranging from simple batch files running command-line FTP to highly complex synthetic transaction generators. Globalscape's Quality Assurance team performs load testing of our servers as part of our standard validation process for releasing software. It is always recommended to separate EFT’s configuration share and site data shares to different servers. If possible, use a share server that is exclusive to EFT’s configuration. This directly improves stability. ​DFS is not a supported platform for shared config. Refer to knowledgebase article #11569 Known Issues Using DFS on EFT for more information.\n\nNumerous other features can be validated within EFT. The above set represents the key elements that are most often used and are the most critical to successful operation in a production environment.\n\nThe following settings are recommended for increased security.\n\nCreate a specific AD account on which EFT’s service is to run with the minimum necessary permissions. Create an Event Rule to back up the entire Server configuration to a separate drive at least daily. Do not use any default administrator names (e.g., \"admin\"). Do not use the default administration port (1100). Only turn on remote administration if necessary. If remote administration is needed, then ban all IPs except those trusted IPs necessary to access the server for administration. Turn on SSL/TLS if using remote administration. Create sub-administrator accounts with the least amount of privileges necessary for help desk or operational administrators. Do not give sub-administrators access to COM or the ARM (report) module unless absolutely necessary If giving ARM (report) access to a sub-administrator, use the ReportsConnectionString registry override to define an alternate (least privileged) database connection string for database queries. Set administrator passwords to expire every 90 days (or according to internal best practices/policies). Enable and define a complex security scheme for administrator passwords to include a minimum password length of 12 to 16 characters. Lockout administrators for an extended period after multiple failed login attempts. Run a PCI DSS report to detect any lax security configuration settings (either manually or on a schedule with an Event Rule). Periodically check the Globalscape support site for the latest version and upgrade accordingly. One or more high priority fixes for security vulnerabilities are often included. Expire accounts that are inactive for 90 days. Set user passwords to expire every 60 or 90 days. When using HTTP/S and/or SFTP protocols, require that the user reset their password upon initial use (requires KIA support by the SFTP client. FTP/S protocol does not support password reset upon initial login). E-mail user login credentials separately or only send username and communicate password via phone or other means (i.e., out-of-band delivery). Allow users to reset their passwords and force them to do so upon first login. Segregate user’s folders. (Do not share folders/resources across users when possible.) Restrict users to their home folders and set the home folder as ROOT for that user. Use Settings Templates to inherit user permissions rather than modifying them for each user. Use Groups to simplify control over user access to resources. Limit resource permissions to the minimum necessary. Rotate logs daily and encrypt+sign using an Event Rule. Examine audit logs at least weekly for anomalous behavior Encrypt data at rest using EFS encryption, OpenPGP, or 3rd-party encryption. Never store data in the DMZ, even temporarily. (Instead, install DMZ Gateway® in the DMZ and then store/manage data on EFT and other storage locations.) Create a legacy data clean-up rule according to your company policy. Add a banned file type rule and disallow all extensions except those required by the business. Be extremely selective when choosing which IPv4 or IPv6 addresses to bind to for a specific Site (listener). Only bind to IPv6 addresses if your organization is aware of and mitigating against IPv6-specific attacks at the edge of your network. If possible, allow only secure protocols (SSL/TLS, SSH, HTTPS, AS2). Disable all unused services or features that may adversely affect security, including Web Services, any unused protocol listeners, and using username and password credentials for use in Event Rule context variables, if not needed by any Event Rule. Always choose the strongest ciphers, hashes, and key lengths. The following are considered weak ciphers: Not configuring SSL/TLS securely increases the likelihood that user credentials would be compromised by decrypting the session. If these ciphers are not required for supporting older browsers disable them. Additional changes may be required to avoid specific attacks. For example, to mitigate the BEAST exploit, move RC4 (a lesser strength but non-CBC cipher) to the top of the SSL/TLS cipher priority list, followed by AES 256, then AES128, etc. In the Server Security tab, enable the most secure SSL/TLS Compatibility settings as needed for your environment. Do not enable Clear Command Channel (CCC) nor unprotected data channel (PROT C), unless needed in your environment. Have your server’s SSL/TLS certificate signed by Certificate Authority (CA). If possible, require that the connecting clients provide a certificate proving their identify in addition to their authentication credentials. Mask the server's identity by using generic banner messages. Specify a maximum limit for connections and transfers for each template. Specify a maximum limit of 5 connections and transfers for each user. Enable EFT’s Denial of service settings, disconnecting and banning users that issue an excessive numbers of invalid commands (weighted over a given period) and permanently banning IP addresses that exceed the server's Flood/hammer value. Non HTTP/S setups should set the Flood/hammer slider to Very High, vs. the default Medium setting. Specify allowed IP address ranges for user/partner connections when possible, denying connections from all other IP addresses.\n\nThe following are guidelines for maintaining the good health of an EFT and DMZ Gateway deployment and reducing long-term costs of maintenance and operation.\n• Configuration Backup - For disaster recovery and business continuity, it is important to keep backups of the Server and DMZ Gateway configuration. Backing up the configuration can be accomplished with a variety of tools such as Symantec Backup Exec, Ghost / VMWare to make images of the system, or even a simple script file.\n• Database Backup and Truncation - If you are using the Auditing and Reporting module (ARM), the database to which the audit records are stored should include EFT ARM tables as part of the typical database maintenance plan. This includes proper monitoring of the tables and transaction logs, backing up the data and having a retention policy to archive (or purge) old data.\n• Data Archival and Retention - You should put into place and enforce a policy by which old data is periodically archived and/or purged, because no disk is limitless and performance can degenerate as more files are added to EFT. Therefore, a storage management policy should include regular inspection of available hard disk space and health (error count, fragmentation, etc.) as well as archiving and/or purging user data and Server Log Files (CMDOUT.log found in the application folder, and all other logs found in the Log folder specified on the Server).\n• Restarting Services - Given the facility of the Microsoft Cluster in failing over and failing back while providing high resource availability, it is recommended that you design a maintenance schedule in which the EFT service is cycled at least once per quarter to once per month. Failing over to the backup node, restarting the service, then failing back and restarting the other node would suffice in re-establishing a baseline state of the EFT service to ensure optimal health.\n• Event Log Alerting - EFT will log error conditions to the standard Windows Event Viewer. It is recommended that the operations team for an enterprise include EFT error checks in their monitoring techniques, looking for an ERROR event generated with a source of \"EFT,\" \"EFT Enterprise,\" or \"Globalscape.\"\n\nBelow are few recommendations for achieving a backup server image that is ready to be turned on quickly and accept \"real\" traffic.\n\nIn all situations, if you are copying a configuration file from one system to another, care must be taken with hardware-specific resources, such as IP addresses, physical paths/partitions, and so on. If possible, it is recommended that the EFT configuration use the generic \"All Incoming\" IP Address for incoming socket connections so that differences in computer IP addresses do not prevent proper operation of the system if the Cold Standby comes online.\n\nFurthermore, you must take care with the connections and IP-access restriction lists between EFT and DMZ Gateway. If DMZ Gateway is configured to allow only one EFT IP address to connect to it, then the Cold Standby server must have the same IP address to connect; alternately, the DMZ Gateway IP access list must include all possible IP addresses (possibly a Class C subnet) so that multiple servers from the approved network segment may connect.\n• Virtualization Software - A great solution from a cost- and resource-saving standpoint, virtualization software is also quite easy to manage due to the \"software\" nature of the solution. The approach would be to create an image within a virtual system (using a tool such as VMWare or Microsoft Virtual PC) by installing and activating the EFT or DMZ Gateway software. Once this is done, the steps required to bring the system online include first copying the configuration files (which were backed up using a process described above), then bringing the virtual image online and starting the service.\n• System Backup Software - Another quick and easy option is to create a disk or system image of a configured EFT or DMZ Gateway (using a product such as Norton Ghost); when a Cold standby needs to be \"stood up\" and made hot, the image can be installed on a computer, backup configuration copied, and the service started.\n• Periodic Backup to Cold Standby Machine - If resources permit, the quickest way to get a \"Cold\" computer to become \"Hot\" is to have a computer dedicated to this function. It should have EFT and/or DMZ Gateway installed and activated, but the service should be stopped. A process to copy the configuration periodically from the \"Hot\" server to the \"Cold\" server would keep the two in synch, and if the \"Hot\" system goes down, the \"Cold\" system can simply start the service."
    },
    {
        "link": "https://jscape.com/blog/how-to-set-up-an-http-file-transfer",
        "document": "A Hypertext Transfer Protocol (HTTP) file transfer, which is also known as a web file transfer, is one of the most user-friendly methods of transferring files. For users to upload files or download files with an HTTP server, all they need is a web browser. Users can employ any web browser, such as Chrome, Firefox, Microsoft Edge or Safari for that purpose.\n\nIn this tutorial, we’ll walk you through the steps for setting up an HTTP file server. But before we do that, let’s discuss why you might want to use this type of file transfer server in the first place.\n\nWhy set up an HTTP file transfer?\n\nAs mentioned earlier, users just need a web browser to carry out a web file transfer. That browser functions as a web client. Since all major desktop operating systems, including Windows, Linux and macOS, have a web browser pre-installed by default, users don’t have to install additional software. Even mobile operating systems like iOS and Android have built-in web browsers.\n\nAnd because practically all end users are familiar with web browsers, no additional training is required. This state of affairs surrounding HTTP file transfers is also beneficial for organizations, especially from an IT support standpoint. IT teams no longer have to spend time installing, configuring or troubleshooting specialized file transfer clients in order for their user base to conduct file transfers. As a result, your IT teams can reallocate the time saved to more strategic tasks.\n\nMoreover, because HTTP is the same protocol used for web browsing and all web-related activities, an HTTP file transfer won’t require additional firewall configurations. In all likelihood, the required port, i.e., port 80, will already be open on your firewall.\n\nIf your organization’s file transfer and file sharing workflows are mostly user-initiated or person-to-server, and not automated server-to-server file transfers, HTTP file transfer should suffice. When configured correctly, an HTTP server can provide the same file transfer functionality as an FTP server or an SFTP server. If you’re not familiar with those acronyms, FTP stands for File Transfer Protocol, while SFTP stands for Secure File Transfer Protocol.\n\nThe next section is only meant for those who want to understand what goes on under the hood during a typical HTTP file transfer. If you just want to know how to configure an HTTP file transfer, you may skip this part.\n\nAn HTTP file transfer’s front-end user interface is essentially a web page. So it usually involves Hypertext Markup Language (HTML) and JavaScript. The back-end, which handles tasks such as managing file uploads/downloads, interacting with storage systems and processing requests, can be implemented using practically any programming language, including Java, Python, Ruby, C, C++ and many others.\n\nAs for the HTTP file transfer itself, it mainly consists of file transfer downloads and file transfer uploads. Let’s dissect each of these processes.\n\nWhen a user attempts to download a file from an HTTP server, the user’s web client sends out an HTTP GET request to the server. That request, which typically includes the file’s URL and the HTTP protocol version used, may look like this:\n\nUpon receiving the request, the web server checks if the file is available. If the file is indeed available and accessible, the server responds with a “200 OK” HTTP status code, some response headers and the requested file. Here’s an example of a typical HTTP response:\n\nHTTP headers, such as those lines above that begin with Content-Type, Content-Length and Content-Disposition, contain metadata that describe the data being transferred and how it’s processed. This information helps the participating software applications take appropriate action.\n\nWhen a user attempts to upload a file to an HTTP server, the user’s web client sends out an HTTP POST or HTTP PUT request. That request typically includes the file data in the request body. Here’s a sample POST request:\n\nUpon receiving the request, the HTTP server processes the request, extracts the uploaded file and then stores the file on the server. It then sends back a response to the client. The response typically includes a status code (e.g., “200 OK”) and, optionally, other relevant metadata.\n\nAlright, now that you know how an HTTP file transfer works, let’s proceed with the tutorial. For this tutorial, we’ll be using JSCAPE MFT by Redwood. JSCAPE MFT is an advanced managed file transfer solution that allows you to set up an HTTP file transfer service in just a few steps.\n\nI suggest you browse through the steps below first, just to see how easy and effortless the entire process can be. Then, if you decide to try it out yourself, you can request a free trial:\n\nOr, if you want an expert to demo the set up process for you,\n\nIn the meantime, here are the three steps to setting up an HTTP file transfer using JSCAPE MFT:\n\nLog in to your JSCAPE MFT administrative user interface and then click the Settings menu at the top of the screen.\n\nNext, expand the MISCELLANEOUS drop-down menu found on the left sidebar, and then navigate to the Web > Web tab. Tick the “HTTP on host” checkbox to enable the HTTP web service. As soon as you do that, JSCAPE MFT should automatically select 0.0.0.0 for the IP address and 80 for the port number. A 0.0.0.0 IP address means your HTTP service will be listening for requests on all available network interfaces. 80 is the default HTTP port number.\n\nThe “HTTPS on host’ option is for Hypertext Transfer Protocol Secure (HTTPS), the secure version of HTTP. HTTPS file transfers are protected by the Secure Sockets Layer (SSL)/Transport Layer Security (TLS) cryptographic protocols. If you prefer to configure an HTTPS file transfer, you may read the tutorial “How To Set Up A HTTPS File Transfer”.\n\nWe’ll just be setting up a basic HTTP configuration, so you may leave all other settings such as the Private key, Theme, Session timeout (not shown on the screenshot) and others, as is for now.\n\nDon’t forget to click the Apply button at the bottom-right corner of the screen to apply those changes.\n\nAt this point, you would have already enabled the HTTP service. However, you still need to specify the domain(s) where you want it served, as you may not want to enable it on all your JSCAPE MFT domains.\n\nWhen you’re ready to proceed, navigate to the Domains screen by clicking Domains > View domains.\n\nNext, double-click the domain where you wish to add the HTTP service. In this example, we’re adding it to the ‘mftserver1’ domain.\n\nExpand the SERVICES drop-down menu and then click the Listeners submenu. Click the Add button to add a service.\n\nExpand the Protocol drop-down list and then select “HTTP/S”. Click OK to proceed.\n\nOn the next screen, you should see HTTPS selected by default. Since we just want to use HTTP for now, untick the checkbox beside HTTPS and then tick the one beside HTTP instead. Click OK to proceed.\n\nWith that, the HTTP service should be automatically added to your list of service listeners.\n\nSince you’ll need a user account to test this, verify that you already have an existing user account by navigating to ACCOUNTS > USERS. In our example, we already have a user account with user ID “user1”.\n\nWe’re now ready to test our newly activated HTTP file transfer service.\n\nLaunch your favorite web browser. To minimize external issues, start by launching a web browser that’s installed on the same machine as your JSCAPE MFT Server instance and then enter localhost, your machine’s IP address or 0.0.0.0 on your web browser. If that works, you can then try connecting remotely from a separate machine later, after you’re done with this initial test.\n\nYou should see the following login screen. Enter the JSCAPE MFT Server domain where you added your HTTP service. Enter your user account’s username and its corresponding password as well. Click the LOGIN button to log in.\n\nIf everything goes well, you should see a graphical web user interface that resembles the one below. Notice that you can perform several tasks here. For instance, you can create a new directory, rename a file’s filename, delete a file, perform a zipped download, change a directory, view your account details and so on. To test a file upload, click the Upload button.\n\nSelect the local file you wish to upload and then click Open.\n\nBarring any unforeseen circumstances, the file should upload successfully.\n\nThat’s it. Now you know how to set up an HTTP file transfer using JSCAPE MFT. Would you like to try this out?\n\nOr, if you want an expert to demo the set up process for you,\n\nWhich is better?\n\nBoth HTTP and FTP operate on top of the Transmission Control Protocol (TCP). Thus, they both benefit from the reliability, ordered data delivery, flow control and congestion control properties of TCP. These properties make the two protocols suitable for file transfer processes. Both protocols can also transfer large files. And, unfortunately, both protocols transfer files unencrypted.\n\nHowever, despite their similarities when used for file transfers, HTTP has one distinct advantage. As mentioned in the article above, user-initiated HTTP file transfers don’t require specialized client applications. Users can carry out HTTP file transfers with just a common web browser. FTP, on the other hand, usually requires an FTP client or a file transfer client that supports FTP.\n\nWeb browsers used to support FTP. However, for security reasons, recent versions of popular web browsers no longer do so.\n\nWhich is better?\n\nLike FTP and HTTP, SFTP also runs on TCP. So, SFTP is similar to HTTP in that regard. However, SFTP, which also stands for SSH File Transfer Protocol, is substantially more secure than HTTP. SFTP runs on top of Secure Shell (SSH).\n\nThus, it inherits the security features of SSH, which include: data-in-transit encryption, two factor authentication and host authentication. None of these features are present in plain FTP. So, from a security standpoint, SFTP is better than HTTP. If you want a more secure version of HTTP, use HTTPS.\n\nHow To Set Up A HTTPS File Transfer: HTTPS File Sharing\n\nHow To Auto Upload Files To A Server From A Local Directory\n\nHow To Transfer Files Between Cloud Storage Services\n\nHow To Set Up Automated AS2 File Transfers"
    },
    {
        "link": "https://developers.google.com/search/docs/crawling-indexing/url-structure",
        "document": ": For news and resources from Google Search on making your site discoverable, follow us on LinkedIn\n\nSave and categorize content based on your preferences.\n\nGoogle supports URLs as defined by RFC 3986. Characters defined by the standard as reserved must be percent encoded. Unreserved ASCII characters may be left in the non-encoded form. Additionally, characters in the non-ASCII range should be UTF-8 encoded.\n\nWhen possible, use readable words rather than long ID numbers in your URLs.\n\nRecommended: Use words in your audience's language in the URL (and, if applicable, transliterated words). For example, if your audience is searching in German, use German words in the URL:\n\nRecommended: Use UTF-8 encoding as necessary. For example, the following example uses UTF-8 encoding for Arabic characters in the URL:\n\nThe following example uses UTF-8 encoding for Chinese characters in the URL:\n\nThe following example uses UTF-8 encoding for the umlaut in the URL:\n\nThe following example uses UTF-8 encoding for emojis in the URL:\n\nNot recommended: Using non-ASCII characters in the URL:\n\nNot recommended: Unreadable, long ID numbers in the URL:\n\nDon't use fragments to change the content of a page, as Google generally doesn't support URL fragments. If you're using JavaScript to change content, use the History API instead.\n\nIf your site is multi-regional, consider using a URL structure that makes it easy to geotarget your site. For more examples of how you can structure your URLs, refer to using locale-specific URLs.\n\nConsider using hyphens to separate words in your URLs, as it helps users and search engines identify concepts in the URL more easily. We recommend that you use hyphens ( ) instead of underscores ( ) in your URLs.\n\nNot recommended: Words in the URL joined together:\n\nWhen specifying URL parameters, use the following common encoding: an equal sign ( ) to separate key-value pairs and add additional parameters with an ampersand ( ). To list multiple values for the same key within a key-value pair, you can use any character that doesn't conflict with IETF STD 66, such as a comma ( ).\n\nRecommended: Using an equal sign ( ) to separate key-value pairs and an ampersand ( ) to add additional parameters:\n\nRecommended: Using a comma ( ) to list multiple values for the same key, an equal sign ( ) to separate key-value pairs, and an ampersand ( ) to add additional parameters:\n\nNot recommended: Using a colon to separate key-value pairs and brackets to add additional parameters\n\nNot recommended: Using a single comma to separate key-value pairs and double commas to add additional parameters\n\nOverly complex URLs, especially those containing multiple parameters, can cause problems for crawlers by creating unnecessarily high numbers of URLs that point to identical or similar content on your site. As a result, Googlebot may consume much more bandwidth than necessary, or may be unable to completely index all the content on your site.\n\nUnnecessarily high numbers of URLs can be caused by a number of issues. These include:\n• Additive filtering of a set of items. Many sites provide different views of the same set of items or search results, often allowing the user to filter this set using defined criteria (for example: show me hotels on the beach). When filters can be combined in an additive manner (for example: hotels on the beach and with a fitness center), the number of URLs (views of data) in the sites explodes. Creating a large number of slightly different lists of hotels is redundant, because Googlebot needs to see only a small number of lists from which it can reach the page for each hotel. For example:\n• Hotel properties at \"value rates\" on the beach:\n• Hotel properties at \"value rates\" on the beach and with a fitness center:\n• Dynamic generation of documents. This can result in small changes because of counters, timestamps, or advertisements.\n• Problematic parameters in the URL. Session IDs, for example, can create massive amounts of duplication and a greater number of URLs.\n• Sorting parameters. Some large shopping sites provide multiple ways to sort the same items, resulting in a much greater number of URLs. For example:\n• Irrelevant parameters in the URL, such as referral parameters. For example:\n• Calendar issues. A dynamically generated calendar might generate links to future and previous dates with no restrictions on start or end dates. For example:\n• Broken relative links. Broken relative links can often cause infinite spaces. Frequently, this problem arises because of repeated path elements. For example:\n\nTo avoid potential problems with URL structure, we recommend the following:\n• Create a simple URL structure. Consider organizing your content so that URLs are constructed logically and in a manner that is most intelligible to humans.\n• Consider using a robots.txt file to block Googlebot's access to problematic URLs. Typically, consider blocking dynamic URLs, such as URLs that generate search results, or URLs that can create infinite spaces, such as calendars. Using regular expressions in your robots.txt file can allow you to easily block large numbers of URLs.\n• Wherever possible, avoid the use of session IDs in URLs. Consider using cookies instead.\n• If upper and lower case text in a URL is treated the same by the web server, convert all text to the same case so it is easier for Google to determine that URLs reference the same page.\n• Whenever possible, shorten URLs by trimming unnecessary parameters.\n• If your site has an infinite calendar, add a attribute to links to dynamically created future calendar pages."
    },
    {
        "link": "https://url.spec.whatwg.org",
        "document": "The URL Standard defines URLs, domains, IP addresses, the application/x-www-form-urlencoded format, and their API.\n\nThe URL standard takes the following approach towards making URLs fully interoperable:\n• Align RFC 3986 and RFC 3987 with contemporary implementations and obsolete the RFCs in the process. (E.g., spaces, other \"illegal\" code points, query encoding, equality, canonicalization, are all concepts not entirely shared, or defined.) URL parsing needs to become as solid as HTML parsing. [RFC3986] [RFC3987]\n• Standardize on the term URL. URI and IRI are just confusing. In practice a single algorithm is used for both so keeping them distinct is not helping anyone. URL also easily wins the search result popularity contest.\n• Define URL’s existing JavaScript API in full detail and add enhancements to make it easier to work with. Add a new object as well for URL manipulation without usage of HTML elements. (Useful for JavaScript worker environments.)\n• Ensure the combination of parser, serializer, and API guarantee idempotence. For example, a non-failure result of a parse-then-serialize operation will not change with any further parse-then-serialize operations applied to it. Similarly, manipulating a non-failure result through the API will not change from applying any number of serialize-then-parse operations to it.\n\nAs the editors learn more about the subject matter the goals might increase in scope somewhat.\n\nSome terms used in this specification are defined in the following standards and specifications:\n\nTo , represent it as the shortest possible decimal number.\n\nA indicates a mismatch between input and valid input. User agents, especially conformance checkers, are encouraged to report them somewhere.\n\nThe is a conceptual code point that signifies the end of a string or code point stream.\n\nA for a string is an integer that points to a code point within . Initially it points to the start of . If it is −1 it points nowhere. If it is greater than or equal to ’s code point length, it points to the EOF code point.\n\nWhen a pointer is used, references the code point the pointer points to as long as it does not point nowhere. When the pointer points to nowhere c cannot be used.\n\nWhen a pointer is used, references the code point substring from the pointer + 1 to the end of the string, as long as c is not the EOF code point. When c is the EOF code point remaining cannot be used.\n\nIf \" \" is a string being processed and a pointer points to @, c is U+0040 (@) and remaining is \" \".\n\nIf the empty string is being processed and a pointer points to the start and is then decreased by 1, using c or remaining would be an error.\n\nA is U+0025 (%), followed by two ASCII hex digits.\n\nIt is generally a good idea for sequences of percent-encoded bytes to be such that, when percent-decoded and then passed to UTF-8 decode without BOM or fail, they do not end up as failure. How important this is depends on where the percent-encoded bytes are used. E.g., for the host parser not following this advice is fatal, whereas for URL rendering the percent-encoded bytes would not be rendered percent-decoded.\n\nThe are the C0 controls and all code points greater than U+007E (~).\n\nThe is the C0 control percent-encode set and U+0020 SPACE, U+0022 (\"), U+003C (<), U+003E (>), and U+0060 (`).\n\nThe is the C0 control percent-encode set and U+0020 SPACE, U+0022 (\"), U+0023 (#), U+003C (<), and U+003E (>).\n\nThe query percent-encode set cannot be defined in terms of the fragment percent-encode set due to the omission of U+0060 (`).\n\nThe is the query percent-encode set and U+0027 (').\n\nThe is the query percent-encode set and U+003F (?), U+005E (^), U+0060 (`), U+007B ({), and U+007D (}).\n\nThe is the path percent-encode set and U+002F (/), U+003A (:), U+003B (;), U+003D (=), U+0040 (@), U+005B ([) to U+005D (]), inclusive, and U+007C (|).\n\nThe is the userinfo percent-encode set and U+0024 ($) to U+0026 (&), inclusive, U+002B (+), and U+002C (,).\n\nThis is used by for , and could also be used by other standards to percent-encode data that can then be embedded in a URL’s path, query, or fragment; or in an opaque host. Using it with UTF-8 percent-encode gives identical results to JavaScript’s [sic]. [HTML] [ECMA-262]\n\nThe is the component percent-encode set and U+0021 (!), U+0027 (') to U+0029 RIGHT PARENTHESIS, inclusive, and U+007E (~).\n\nThe percent-encode set contains all code points, except the ASCII alphanumeric, U+002A (*), U+002D (-), U+002E (.), and U+005F (_).\n\nThe security of a URL is a function of its environment. Care is to be taken when rendering, interpreting, and passing URLs around.\n\nWhen rendering and allocating new URLs \"spoofing\" needs to be considered. An attack whereby one host or URL can be confused for another. For instance, consider how 1/l/I, m/rn/rri, 0/O, and а/a can all appear eerily similar. Or worse, consider how U+202A LEFT-TO-RIGHT EMBEDDING and similar code points are invisible. [UTR36]\n\nWhen passing a URL from party to , both need to carefully consider what is happening. might end up leaking data it does not want to leak. might receive input it did not expect and take an action that harms the user. In particular, should never trust , as at some point URLs from can come from untrusted sources.\n\nAt a high level, a host, valid host string, host parser, and host serializer relate as follows:\n• The host parser takes an arbitrary scalar value string and returns either failure or a host.\n• A host can be seen as the in-memory representation.\n• A valid host string defines what input would not trigger a validation error or failure when given to the host parser. I.e., input that would be considered conforming or valid.\n• The host serializer takes a host and returns an ASCII string. (If that string is then parsed, the result will equal the host that was serialized.)\n\nA is a domain, an IP address, an opaque host, or an empty host. Typically a host serves as a network address, but it is sometimes used as opaque identifier in URLs where a network address is not necessary.\n\nA typical URL whose host is an opaque host is .\n\nThe RFCs referenced in the paragraphs below are for informative purposes only. They have no influence on host writing, parsing, and serialization. Unless stated otherwise in the sections that follow.\n\nA is a non-empty ASCII string that identifies a realm within a network. [RFC1034]\n\nThe of a domain are the result of strictly splitting on U+002E (.).\n\nThe and domains are not equivalent and typically treated as distinct.\n\nAn is an IPv4 address or an IPv6 address.\n\nAn is a 32-bit unsigned integer that identifies a network address. [RFC791]\n\nAn is a 128-bit unsigned integer that identifies a network address. This integer is composed of a list of 8 16-bit unsigned integers, also known as an IPv6 address’s . [RFC4291]\n\nAn is a non-empty ASCII string that can be used for further processing.\n\nAn is the empty string.\n\nSpecifications should prefer the origin concept for security decisions. The notion of \"public suffix\" and \"registrable domain\" cannot be relied-upon to provide a hard security boundary, as the public suffix list will diverge from client to client. Specifications which ignore this advice are encouraged to carefully consider whether URLs' schemes ought to be incorporated into any decisions made, i.e. whether to use the same site or schemelessly same site concepts.\n\nA must be a valid domain string, a valid IPv4-address string, or: U+005B ([), followed by a valid IPv6-address string, followed by U+005D (]).\n\nA string is a if these steps return true:\n• Let be the result of running domain to ASCII with and true.\n• Return false if is failure; otherwise true.\n\nIdeally we define this in terms of a sequence of code points that make up a valid domain rather than through a whack-a-mole: issue 245.\n\nA must be a string that is a valid domain.\n\nA must be four shortest possible strings of ASCII digits, representing a decimal number in the range 0 to 255, inclusive, separated from each other by U+002E (.).\n\nA is defined in the \"Text Representation of Addresses\" chapter of IP Version 6 Addressing Architecture. [RFC4291]\n\nA must be one of the following:\n• one or more URL units excluding forbidden host code points\n• U+005B ([), followed by a valid IPv6-address string, followed by U+005D (]).\n\nThis is not part of the definition of valid host string as it requires context to be distinguished.\n\nCertificate comparison requires a host equivalence check that ignores the trailing dot of a domain (if any). However, those hosts have also various other facets enforced, such as DNS length, that are not enforced here, as URLs do not enforce them. If anyone has a good suggestion for how to bring these two closer together, or what a good unified model would be, please file an issue.\n\nAt a high level, a URL, valid URL string, URL parser, and URL serializer relate as follows:\n• The URL parser takes an arbitrary scalar value string and returns either failure or a URL. It might also record zero or more validation errors.\n• A URL can be seen as the in-memory representation.\n• A valid URL string defines what input would not trigger a validation error or failure when given to the URL parser. I.e., input that would be considered conforming or valid.\n• The URL serializer takes a URL and returns an ASCII string. (If that string is then parsed, the result will equal the URL that was serialized.) The output of the URL serializer is not always a valid URL string.\n\nA is a struct that represents a universal identifier. To disambiguate from a valid URL string it can also be referred to as a URL record.\n\nA URL’s is an ASCII string that identifies the type of URL and can be used to dispatch a URL for further processing after parsing. It is initially the empty string.\n\nA URL’s is an ASCII string identifying a username. It is initially the empty string.\n\nA URL’s is an ASCII string identifying a password. It is initially the empty string.\n\nA URL’s is null or a host. It is initially null.\n\nA URL’s is either null or a 16-bit unsigned integer that identifies a networking port. It is initially null.\n\nA URL’s is a URL path, usually identifying a location. It is initially « ».\n\nA special URL’s path is always a list, i.e., it is never opaque.\n\nA URL’s is either null or an ASCII string. It is initially null.\n\nA URL’s is either null or an ASCII string that can be used for further processing on the resource the URL’s other components identify. It is initially null.\n\nA URL also has an associated that is either null or a blob URL entry. It is initially null.\n\nThis is used to support caching the object a \" \" URL refers to as well as its origin. It is important that these are cached as the URL might be removed from the blob URL store between parsing and fetching, while fetching will still need to succeed.\n\nA is either a URL path segment or a list of zero or more URL path segments.\n\nA is an ASCII string. It commonly refers to a directory or a file, but has no predefined meaning.\n\nA is a URL path segment that is \" \" or an ASCII case-insensitive match for \" \".\n\nA is a URL path segment that is \" \" or an ASCII case-insensitive match for \" \", \" \", or \" \".\n\nA is an ASCII string that is listed in the first column of the following table. The for a special scheme is listed in the second column on the same row. The default port for any other ASCII string is null.\n\nA URL if its scheme is a special scheme. A URL if its scheme is not a special scheme.\n\nA URL if its username or password is not the empty string.\n\nA URL has an if its path is a URL path segment.\n\nA URL if its host is null or the empty string, or its scheme is \" \".\n\nA URL can be designated as .\n\nA base URL is useful for the URL parser when the input might be a relative-URL string.\n\nA is two code points, of which the first is an ASCII alpha and the second is either U+003A (:) or U+007C (|).\n\nA is a Windows drive letter of which the second code point is U+003A (:).\n\nAs per the URL writing section, only a normalized Windows drive letter is conforming.\n\nA string if all of the following are true:\n• its length is greater than or equal to 2\n• its first two code points are a Windows drive letter\n• its length is 2 or its third code point is U+002F (/), U+005C (\\), U+003F (?), or U+0023 (#).\n\nA must be either a relative-URL-with-fragment string or an absolute-URL-with-fragment string.\n\nAn must be an absolute-URL string, optionally followed by U+0023 (#) and a URL-fragment string.\n\nAn must be one of the following:\n• a URL-scheme string that is an ASCII case-insensitive match for a special scheme and not an ASCII case-insensitive match for \" \", followed by U+003A (:) and a scheme-relative-special-URL string\n• a URL-scheme string that is not an ASCII case-insensitive match for a special scheme, followed by U+003A (:) and a relative-URL string\n• a URL-scheme string that is an ASCII case-insensitive match for \" \", followed by U+003A (:) and a scheme-relative-file-URL string\n\nany optionally followed by U+003F (?) and a URL-query string.\n\nA must be one ASCII alpha, followed by zero or more of ASCII alphanumeric, U+002B (+), U+002D (-), and U+002E (.). Schemes should be registered in the registry. [IANA-URI-SCHEMES] [RFC7595]\n\nA must be a relative-URL string, optionally followed by U+0023 (#) and a URL-fragment string.\n\nA must be one of the following, switching on base URL’s scheme:\n\nany optionally followed by U+003F (?) and a URL-query string.\n\nA non-null base URL is necessary when parsing a relative-URL string.\n\nA must be \" \", followed by a valid host string, optionally followed by U+003A (:) and a URL-port string, optionally followed by a path-absolute-URL string.\n\nA must be one of the following:\n• one or more ASCII digits representing a decimal number that is a 16-bit unsigned integer.\n\nA must be \" \", followed by an opaque-host-and-port string, optionally followed by a path-absolute-URL string.\n\nAn must be either the empty string or: a valid opaque-host string, optionally followed by U+003A (:) and a URL-port string.\n\nA must be \" \", followed by one of the following:\n\nA must be U+002F (/) followed by a path-relative-URL string.\n\nA must be a path-absolute-URL string that does not start with: U+002F (/), followed by a Windows drive letter, followed by U+002F (/).\n\nA must be zero or more URL-path-segment strings, separated from each other by U+002F (/), and not start with U+002F (/).\n\nA must be a path-relative-URL string that does not start with: a URL-scheme string, followed by U+003A (:).\n\nA must be one of the following:\n• zero or more URL units excluding U+002F (/) and U+003F (?), that together are not a single-dot URL path segment or a double-dot URL path segment.\n\nA must be zero or more URL units.\n\nA must be zero or more URL units.\n\nThe are ASCII alphanumeric, U+0021 (!), U+0024 ($), U+0026 (&), U+0027 ('), U+0028 LEFT PARENTHESIS, U+0029 RIGHT PARENTHESIS, U+002A (*), U+002B (+), U+002C (,), U+002D (-), U+002E (.), U+002F (/), U+003A (:), U+003B (;), U+003D (=), U+003F (?), U+0040 (@), U+005F (_), U+007E (~), and code points in the range U+00A0 to U+10FFFD, inclusive, excluding surrogates and noncharacters.\n\nCode points greater than U+007F DELETE will be converted to percent-encoded bytes by the URL parser.\n\nIn HTML, when the document encoding is a legacy encoding, code points in the URL-query string that are higher than U+007F DELETE will be converted to percent-encoded bytes using the document’s encoding. This can cause problems if a URL that works in one document is copied to another document that uses a different document encoding. Using the UTF-8 encoding everywhere solves this problem.\n\nThe are URL code points and percent-encoded bytes.\n\nPercent-encoded bytes can be used to encode code points that are not URL code points or are excluded from being written.\n\nThere is no way to express a username or password of a URL record within a valid URL string.\n\nSee origin’s definition in for the necessary background information. [HTML]\n\nA URL should be rendered in its serialized form, with modifications described below, when the primary purpose of displaying a URL is to have the user make a security or trust decision. For example, users are expected to make trust decisions based on a URL rendered in the browser address bar.\n\nRemove components that can provide opportunities for spoofing or distract from security-relevant information:\n• Browsers may render only a URL’s host in places where it is important for end users to distinguish between the host and other parts of the URL such as the path. Browsers may consider simplifying the host further to draw attention to its registrable domain. For example, browsers may omit a leading or domain label to simplify the host, or display its registrable domain only to remove spoofing opportunities posted by subdomains (e.g., ).\n• Browsers should not render a URL’s username and password, as they can be mistaken for a URL’s host (e.g., ).\n• Browsers may render a URL without its scheme if the display surface only ever permits a single scheme (such as a browser feature that omits because it is only enabled for secure origins). Otherwise, the scheme may be replaced or supplemented with a human-readable string (e.g., \"Not secure\"), a security indicator icon, or both.\n\nIn a space-constrained display, URLs should be elided carefully to avoid misleading the user when making a security decision:\n• Browsers should ensure that at least the registrable domain can be shown when the URL is rendered (to avoid showing, e.g., when loading ).\n• When the full host cannot be rendered, browsers should elide domain labels starting from the lowest-level domain label. For example, should be elided as , not . (Note that bidirectional text means that the lowest-level domain label may not appear on the left.)\n\nInternationalized domain names (IDNs), special characters, and bidirectional text should be handled with care to prevent spoofing:\n• Browsers should render a URL’s host by running domain to Unicode with the URL’s host and false. Various characters can be used in homograph spoofing attacks. Consider detecting confusable characters and warning when they are in use. [IDNFAQ] [UTS39]\n• URLs are particularly prone to confusion between host and path when they contain bidirectional text, so in this case it is particularly advisable to only render a URL’s host. For readability, other parts of the URL, if rendered, should have their sequences of percent-encoded bytes replaced with code points resulting from running UTF-8 decode without BOM on the percent-decoding of those sequences, unless that renders those sequences invisible. Browsers may choose to not decode certain sequences that present spoofing risks (e.g., U+1F512 (🔒)).\n• Browsers should render bidirectional text as if it were in a left-to-right embedding. [BIDI] Unfortunately, as rendered URLs are strings and can appear anywhere, a specific bidirectional algorithm for rendered URLs would not see wide adoption. Bidirectional text interacts with the parts of a URL in ways that can cause the rendering to be different from the model. Users of bidirectional languages can come to expect this, particularly in plain text environments.\n\nThe format provides a way to encode a list of tuples, each consisting of a name and a value.\n\nThe format is in many ways an aberrant monstrosity, the result of many years of implementation accidents and compromises leading to a set of requirements necessary for interoperability, but in no way representing good design practices. In particular, readers are cautioned to pay close attention to the twisted details involving repeated (and in some cases nested) conversions between character encodings and byte sequences. Unfortunately the format is in widespread use due to the prevalence of HTML forms. [HTML]\n\nA legacy server-oriented implementation might have to support encodings other than UTF-8 as well as have special logic for tuples of which the name is ` `. Such logic is not described here as only UTF-8 is conforming.\n\nThe takes a scalar value string , UTF-8 encodes it, and then returns the result of parsing it.\n\nThis section uses terminology from . Browser user agents must support this API. JavaScript implementations should support this API. Other user agents or programming languages are encouraged to use an API suitable to their needs, which might not be this one. [WEBIDL]\n\nA object has an associated:\n\nA object has an associated:\n• : a list of tuples each consisting of a name and a value, initially empty.\n\nThe value pairs to iterate over are this’s list’s tuples with the key being the name and the value being the value.\n\nThe steps are to return the serialization of this’s list.\n\nA standard that exposes URLs, should expose the URL as a string (by serializing an internal URL). A standard should not expose a URL using a object. objects are meant for URL manipulation. In IDL the USVString type should be used.\n\nThe higher-level notion here is that values are to be exposed as immutable data structures.\n\nIf a standard decides to use a variant of the name \"URL\" for a feature it defines, it should name such a feature \"url\" (i.e., lowercase and with an \"l\" at the end). Names such as \"URL\", \"URI\", and \"IRI\" should not be used. However, if the name is a compound, \"URL\" (i.e., uppercase) is preferred, e.g., \"newURL\" and \"oldURL\".\n\nThe and interfaces in are examples of proper naming. [HTML]\n\nThere have been a lot of people that have helped make URLs more interoperable over the years and thereby furthered the goals of this standard. Likewise many people have helped making this standard what it is today.\n\nWith that, many thanks to 100の人, Adam Barth, Addison Phillips, Adrián Chaves, Adrien Ricciardi, Albert Wiersch, Alex Christensen, Alexis Hunt, Alexandre Morgaut, Alexis Hunt, Alwin Blok, Andrew Sullivan, Arkadiusz Michalski, Behnam Esfahbod, Bobby Holley, Boris Zbarsky, Brad Hill, Brandon Ross, Cailyn Hansen, Chris Dumez, Chris Rebert, Corey Farwell, Dan Appelquist, Daniel Bratell, Daniel Stenberg, David Burns, David Håsäther, David Sheets, David Singer, David Walp, Domenic Denicola, Emily Schechter, Emily Stark, Eric Lawrence, Erik Arvidsson, Gavin Carothers, Geoff Richards, Glenn Maynard, Gordon P. Hemsley, hemanth, Henri Sivonen, Ian Hickson, Ilya Grigorik, Italo A. Casas, Jakub Gieryluk, James Graham, James Manger, James Ross, Jeff Hodges, Jeffrey Posnick, Jeffrey Yasskin, Joe Duarte, Joshua Bell, Jxck, Karl Wagner, Kemal Zebari, 田村健人 (Kent TAMURA), Kevin Grandon, Kornel Lesiński, Larry Masinter, Leif Halvard Silli, Mark Amery, Mark Davis, Marcos Cáceres, Marijn Kruisselbrink, Martin Dürst, Mathias Bynens, Matt Falkenhagen, Matt Giuca, Michael Peick, Michael™ Smith, Michal Bukovský, Michel Suignard, Mikaël Geljić, Noah Levitt, Peter Occil, Philip Jägenstedt, Philippe Ombredanne, Prayag Verma, Rimas Misevičius, Robert Kieffer, Rodney Rehm, Roy Fielding, Ryan Sleevi, Sam Ruby, Sam Sneddon, Santiago M. Mola, Sebastian Mayr, Shannon Booth, Simon Pieters, Simon Sapin, Steven Vachon, Stuart Cook, Sven Uhlig, Tab Atkins, 吉野剛史 (Takeshi Yoshino), Tantek Çelik, Tiancheng \"Timothy\" Gu, Tim Berners-Lee, 簡冠庭 (Tim Guan-tin Chien), Titi_Alone, Tomek Wytrębowicz, Trevor Rowbotham, Tristan Seligmann, Valentin Gosu, Vyacheslav Matva, Wei Wang, Wolf Lammen, 山岸和利 (Yamagishi Kazutoshi), Yongsheng Zhang, 成瀬ゆい (Yui Naruse), and zealousidealroll for being awesome!\n\nThis standard is written by Anne van Kesteren (Apple, annevk@annevk.nl).\n\nCopyright © WHATWG (Apple, Google, Mozilla, Microsoft). This work is licensed under a Creative Commons Attribution 4.0 International License. To the extent portions of it are incorporated into source code, such portions in the source code are licensed under the BSD 3-Clause License instead.\n\nThis is the Living Standard. Those interested in the patent-review version should view the Living Standard Review Draft."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Web/API/URL",
        "document": "This feature is well established and works across many devices and browser versions. It’s been available across browsers since July 2015 . * Some parts of this feature may have varying levels of support.\n\nNote: This feature is available in Web Workers. The interface is used to parse, construct, normalize, and encode URLs. It works by providing properties which allow you to easily read and modify the components of a URL. You normally create a new object by specifying the URL as a string when calling its constructor, or by providing a relative URL and a base URL. You can then easily read the parsed components of the URL or make changes to the URL.\n\nA string containing a followed by the fragment identifier of the URL. A string containing the domain (that is the hostname) followed by (if a port was specified) a and the port of the URL. A string containing the domain of the URL. A stringifier that returns a string containing the whole URL. Returns a string containing the origin of the URL, that is its scheme, its domain and its port. A string containing the password specified before the domain name. A string containing an initial followed by the path of the URL, not including the query string or fragment. A string containing the port number of the URL. A string containing the protocol scheme of the URL, including the final . A string indicating the URL's parameter string; if any parameters are provided, this string includes all of them, beginning with the leading character. A object which can be used to access the individual query parameters found in . A string containing the username specified before the domain name.\n\nReturns a boolean indicating whether or not a URL defined from a URL string and optional base URL string is parsable and valid. Returns a string containing a unique blob URL, that is a URL with as its scheme, followed by an opaque string uniquely identifying the object in the browser. Creates and returns a object from a URL string and optional base URL string, or returns if the passed parameters define an invalid ."
    },
    {
        "link": "https://stackoverflow.com/questions/568929/what-are-the-url-parameters-naming-convention-or-standards-to-follow",
        "document": "Standard for URI are defined by RFC2396.\n\n Anything after the standardized portion of the URL is left to you.\n\nYou probably only want to follow a particular convention on your parameters based on the framework you use.\n\n Most of the time you wouldn't even really care because these are not under your control, but when they are, you probably want to at least be consistent and try to generate user-friendly bits:\n• if they are meant to be directly accessible by users, they should be easy to remember,\n• case-insensitive (may be hard depending on the server OS).\n• follow some SEO guidelines and best practices, they may help you a lot.\n\nI would say that cleanliness and user-friendliness are laudable goals to strive for when presenting URLs.\n\n StackOverflow does a fairly good job of it."
    },
    {
        "link": "https://developer.mozilla.org/en-US/docs/Learn_web_development/Getting_started/Web_standards/The_web_standards_model",
        "document": "Brief history of the web In the late 1960s, the US military developed a communication network called ARPANET. This can be considered a forerunner of the internet, as it worked on packet switching, and featured the first implementation of the TCP/IP protocol suite. These two technologies form the basis of the infrastructure that the internet is built on. In 1980, Tim Berners-Lee (often referred to as TimBL) wrote a notebook program called ENQUIRE, which featured the concept of links between different nodes. Sound familiar? Fast forward to 1989, and TimBL wrote Information Management: A Proposal and HyperText at CERN; these two publications together provided the background for how the web would work. They received a fair amount of interest, enough to convince TimBL's bosses to allow him to go ahead and create a global hypertext system. By 1990-91, TimBL had created all the things needed to run the first version of the World Wide Web (generally referred to as the web) — HTTP, HTML, the first web browser, which was called WorldWideWeb, a web server, and some web pages to look at. Note: People sometimes use \"the web\" and \"the internet\" interchangeably, but they are different things. The internet is the infrastructure that enables information to be transported around the world between different servers and clients, whereas the web is a system built on top of the internet. The web defines types of information (content and code) that are transported via the internet and communications protocols to manage that transport. In 1994, TimBL founded the World Wide Web Consortium (W3C), an organization that brings together representatives from many different companies to work together on the creation of web technologies. The W3C worked on standardizing and improving existing web technologies such as HTML and HTTP, and creating new technologies such as CSS and JavaScript. CSS and JavaScript in particular were vital for giving the web styling and interactivity, making it look more like the web we know today. In the next few years that followed, the web exploded, with multiple browsers being released, thousands of web servers being set up, and millions of web pages being created. Other standards organizations also appeared to help standardize different aspects of web technologies. Note: If you are interested in reading a more detailed account of web history, try searching for \"history of the web\" in your favorite search engine and see what you can find.\n\nWeb standards are created by standards bodies — institutions that invite groups of people from different technology companies to come together and agree on how the technologies should work in the best way to fulfill all of their use cases. The W3C is the best known web standards body, but there are others. For example:\n• WHATWG maintains the HTML Living Standard, which describes exactly how HTML (all the HTML elements, and their associated APIs, and other surrounding technologies) should be implemented.\n• TC39 and ECMA specify and publish the standard for ECMAScript, which modern JavaScript is based on.\n• Khronos publish technologies for 3D graphics, such as WebGL. The full processes by which standards are created can get deep and complex. However, unless you want to create your own web technology features, you don't need to understand most of it. If you want to contribute to the discussion around new technologies and provide feedback, it is usually a matter of joining the relevant mailing list or other discussion mechanism. Standards discussions are carried out in public, hence the term \"Open\" standards. For now, we'll give you a general, high-level understanding of how standards processes work:\n• Someone notices the need for a new web standard feature that will make developers' lives easier. For example, maybe there is a common pattern that is commonly used in web user interfaces, but it is a pain to implement. A dedicated CSS feature would make it much easier. The someone could be anyone — an individual developer, or an engineer working for a big tech company.\n• The person discusses this feature with other developers, browser engineers, etc., and starts to create interest in implementing the feature. Usually they write an explainer document that explains the need for the feature and how it will work, and a code demo that shows what the feature would look like in action.\n• If there is enough interest in the feature, it is formally discussed inside the relevant standards body working group. For example, CSS features are usually discussed by the CSS Working Group (WG) (see also the CSS Working Group Wikipedia page for a bit more description and history). Before a new web technology is accepted, it must be rigorously evaluated to make sure it is good for the web — for example, it doesn't introduce any security problems, it is accessible and compatible with other web technologies, and it doesn't rely on patents.\n• To prove out the feature, several things happen. These points can all happen around the same time as Point 3., or even before (browser vendors sometimes implement proprietary/non-standard features and then attempt to standardize them afterwards):\n• One or more browser vendors will implement an experimental version of the new feature, often disabled by default, but which can be enabled by people who want to test it and provide feedback.\n• A working group member will also add it to a technology specification so that browser vendors are able to implement it consistently.\n• They will also seek out feedback from other browser vendors to see what issues they have with the proposal, and how likely they are to implement it. These are called Standards positions. See for example Mozilla Standards Positions.\n• Involved individuals will also write an extensive suite of tests to demonstrate that the feature works as described.\n• Eventually, if all is well, the feature will be implemented across all browsers and can start being used when creating websites. Note: It is perfectly possible that the people suggesting the feature, implementing it in a browser, creating the specification, writing tests, and collecting feedback on it, are the same person/people. You can find more information on specific standards body processes. See for example:\n\nOne of the key aspects of web standards, which TimBL and the W3C agreed on from the start, is that the web (and web technologies) should be open. This means they are free to both contribute to and use, and not encumbered by patents/licensing. This is important — if a web technology relies on patented/licensed technologies to function, the patent/owner can then charge implementing browser vendors potentially large amounts of money, and those costs would then be passed on to the browser users. In addition, because web technologies are created openly, in collaboration between many different companies, it means that no one company gets to control them, which is a really good thing. You wouldn't want a single company suddenly deciding to put the entire web behind a paywall, or releasing a new version of HTML that everyone has to buy to continue making websites, or worse still, deciding they aren't interested any more and just turning it off. Open standards enable the web to remain a freely-available public resource, where anyone can write the code to build a website for free, and anyone can contribute to the standards creation process.\n\nHTML, CSS, and JavaScript are the main three technologies you'll use to build a website.\n• HTML is for structure and semantics (meaning).\n• CSS is for styling and layout.\n• JavaScript and APIs are for controlling dynamic behavior. HyperText Markup Language, or HTML, is a markup language consisting of different elements you can wrap (mark up) content in to give it meaning (semantics) and structure. Simple HTML looks like this: <h1>This is a top-level heading</h1> <p>This is a paragraph of text.</p> <img src=\"cat.jpg\" alt=\"A picture of my cat\" /> If we adopted a house-building analogy, HTML would be like the foundations and walls of the house, which give it structure and hold it together. Cascading Style Sheets (CSS) is a rule-based language used to apply styles to your HTML — for example, setting text and background colors, adding borders, animating things, or laying out a page in a certain way. As a simple example, the following code would turn all HTML paragraphs red: In the house analogy, CSS is like the paint, wallpaper, carpets and paintings you'd use to make the house look nice. JavaScript is the programming language we use to add interactivity to websites, from dynamic style switching, to fetching updates from the server, right through to complex 3D graphics. The following simple JavaScript store a reference to a paragraph in memory and change the text inside it: let pElem = document.querySelector(\"p\"); pElem.textContent = \"We changed the text!\"; You'll also hear the term APIs along with JavaScript. API stands for Application Programming Interface. In general terms, an API is a bit of code that allows you to control other more complex bits of code or other functionality on your computer (such as hardware devices like your webcam or microphone) in a manageable way. For example, writing your own interface to communicate with your webcam and capture a video stream from it, but the JavaScript API method allows you to do this fairly easily. It does all the hard work for you, behind the scenes, so you don't need to reinvent the wheel each time. The simple code snippet above also uses an API. and are both parts of the Document Object Model (DOM) family of APIs, which allow you to use JavaScript to manipulate web documents. In the house analogy, JavaScript is like the cooker, TV, Microwave, or hairdryer — the things that give your house useful functionality.\n\nHTML, CSS, and JavaScript are front-end (or client-side) languages, which means they are run by the browser to produce a website front-end that your users can use. There is another class of languages called back-end (or server-side) languages, meaning that they are run on the server before the result is then sent to the browser to be displayed. A typical use for a server-side language is to get some data out of a database, generate some HTML to contain the data, then send the HTML over to the browser to display it to the user. Example server-side frameworks and languages include ASP.NET (C#), Django (Python), Laravel (PHP), and Next.js (JavaScript). These technologies are not considered to be \"web standards\" — they are developed by organizations outside the web standards processes of organizations such as the W3C and WHATWG — although some of them will have processes that are similarly open.\n\nWe have briefly talked about the technologies that you'll use to build websites. Now let's discuss the best practices that web developers generally employ to ensure that their websites are usable by as many people as possible. When doing web development, the main cause of uncertainty comes from the fact that you don't know what combination of technology each user will use to view your website:\n• User 1 might be looking at it on an iPhone, with a small, narrow screen.\n• User 2 might be looking at it on a Windows laptop with a widescreen monitor attached to it.\n• User 3 might be visually impaired, and using a screen reader to read and interact with the web page.\n• User 4 might be using a really old desktop machine that can't run modern browsers. Because you don't know exactly what your users will use, you need to design defensively — make your website as flexible as possible, so that all of the above users can make use of it, even if they might not all get the same experience. You'll come across the below concepts at some point in your studies, which represent best practices your websites should ideally adhere to. Don't worry about these too much for now. Throughout most of the course we try to teach these implicitly, meaning that when we teach you HTML, CSS, and JavaScript, our examples will follow the best practices where possible. Later on in your learning journey you will likely explore explicit teachings in these areas. Creating a minimal experience that provides the essential functionality to all users, and layering on a better experience and other enhancements in browsers that can support them. Progressive enhancement is often seen as unimportant, because browsers tend to support new features more consistently these days, and people tend to have faster internet connections with higher limits on data usage. However, consider examples like cutting down on decoration to make a mobile experience smoother and save on data or providing a lighter, low-bandwidth experience for users who pay by the megabyte or have metered connections. Trying to make sure your webpage works across as many devices as possible. This includes using technologies that all the browsers support, delivering better experiences to browsers that can handle them (progressive enhancement), and/or writing code so that it falls back to a simpler but still usable experience in older browsers (termed graceful degradation). It also requires testing to see if anything fails in certain browsers, and then more work to fix those failures. Putting your content (HTML), styling (CSS), and behavior (JavaScript) in different code files, rather than lumping them all together in the same place. This is a good idea for many reasons, including code management and comprehension and teamwork/separation of roles. In reality, the separation is not always clear. It is an ideal to aim for where possible, rather than an absolute. Making your functionality and layouts flexible so they can automatically adapt to different browsers. An obvious example is a website that is laid out one way in a widescreen browser on the desktop, but displays as a more compact, single-column layout on mobile phone browsers. Try adjusting the width of your browser window now, and see what happens to the site layout. Getting websites to load as quickly as possible, but also making them intuitive and easy to use so that users don't get frustrated and go somewhere else. Making websites usable by people from different cultures, who speak different languages to your own. There are technical considerations here (such as altering your layout so that it still works OK for right-to-left or top-to-bottom languages), and human ones (such as using simple, non-slang language so that diverse cultures are more likely to understand your text). These two concepts are related but different. Privacy refers to allowing people to go about their business privately and not spying on them or collecting more of their data than you absolutely need to. Security refers to constructing your website in a secure way so that malicious users cannot steal information contained on it from you or your users."
    }
]