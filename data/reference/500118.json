[
    {
        "link": "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html",
        "document": "Apply a multi-layer long short-term memory (LSTM) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n\n\\begin{array}{ll} \\\\ i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\ f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\ g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\ o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\ c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\ h_t = o_t \\odot \\tanh(c_t) \\\\ \\end{array}\n\nwhere ht​ is the hidden state at time , ct​ is the cell state at time , xt​ is the input at time , ht−1​ is the hidden state of the layer at time or the initial hidden state at time , and it​, ft​, gt​, ot​ are the input, forget, cell, and output gates, respectively. σ is the sigmoid function, and ⊙ is the Hadamard product.\n\nIn a multilayer LSTM, the input xt(l)​ of the l -th layer ( l≥2) is the hidden state ht(l−1)​ of the previous layer multiplied by dropout δt(l−1)​ where each δt(l−1)​ is a Bernoulli random variable which is 0 with probability .\n\nIf is specified, LSTM with projections will be used. This changes the LSTM cell in the following way. First, the dimension of ht​ will be changed from to (dimensions of Whi​ will be changed accordingly). Second, the output hidden state of each layer will be multiplied by a learnable projection matrix: ht​=Whr​ht​. Note that as a consequence of this, the output of LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact dimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.\n• None input: tensor of shape (L,Hin​) for unbatched input, (L,N,Hin​) when or (N,L,Hin​) when containing the features of the input sequence. The input can also be a packed variable length sequence. See or for details.\n• None h_0: tensor of shape (D∗num_layers,Hout​) for unbatched input or (D∗num_layers,N,Hout​) containing the initial hidden state for each element in the input sequence. Defaults to zeros if (h_0, c_0) is not provided.\n• None c_0: tensor of shape (D∗num_layers,Hcell​) for unbatched input or (D∗num_layers,N,Hcell​) containing the initial cell state for each element in the input sequence. Defaults to zeros if (h_0, c_0) is not provided. \\begin{aligned} N ={} & \\text{batch size} \\\\ L ={} & \\text{sequence length} \\\\ D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\ H_{in} ={} & \\text{input\\_size} \\\\ H_{cell} ={} & \\text{hidden\\_size} \\\\ H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\ \\end{aligned}\n• None output: tensor of shape (L,D∗Hout​) for unbatched input, (L,N,D∗Hout​) when or (N,L,D∗Hout​) when containing the output features from the last layer of the LSTM, for each . If a has been given as the input, the output will also be a packed sequence. When , will contain a concatenation of the forward and reverse hidden states at each time step in the sequence.\n• None h_n: tensor of shape (D∗num_layers,Hout​) for unbatched input or (D∗num_layers,N,Hout​) containing the final hidden state for each element in the sequence. When , will contain a concatenation of the final forward and reverse hidden states, respectively.\n• None c_n: tensor of shape (D∗num_layers,Hcell​) for unbatched input or (D∗num_layers,N,Hcell​) containing the final cell state for each element in the sequence. When , will contain a concatenation of the final forward and reverse cell states, respectively.\n\nThere are known non-determinism issues for RNN functions on some versions of cuDNN and CUDA. You can enforce deterministic behavior by setting the following environment variables: On CUDA 10.1, set environment variable . This may affect performance. On CUDA 10.2 or later, set environment variable (note the leading colon symbol) or . See the cuDNN 8 Release Notes for more information."
    },
    {
        "link": "https://pytorch.org/docs/stable/nn.html",
        "document": "These are the basic building blocks for graphs:\n\nApplies a 1D convolution over an input signal composed of several input planes. Applies a 2D convolution over an input signal composed of several input planes. Applies a 3D convolution over an input signal composed of several input planes. Applies a 1D transposed convolution operator over an input image composed of several input planes. Applies a 2D transposed convolution operator over an input image composed of several input planes. Applies a 3D transposed convolution operator over an input image composed of several input planes. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. A module with lazy initialization of the argument. Combines an array of sliding local blocks into a large containing tensor.\n\nApplies a 1D max pooling over an input signal composed of several input planes. Applies a 2D max pooling over an input signal composed of several input planes. Applies a 3D max pooling over an input signal composed of several input planes. Applies a 1D average pooling over an input signal composed of several input planes. Applies a 2D average pooling over an input signal composed of several input planes. Applies a 3D average pooling over an input signal composed of several input planes. Applies a 2D fractional max pooling over an input signal composed of several input planes. Applies a 3D fractional max pooling over an input signal composed of several input planes. Applies a 1D power-average pooling over an input signal composed of several input planes. Applies a 2D power-average pooling over an input signal composed of several input planes. Applies a 3D power-average pooling over an input signal composed of several input planes. Applies a 1D adaptive max pooling over an input signal composed of several input planes. Applies a 2D adaptive max pooling over an input signal composed of several input planes. Applies a 3D adaptive max pooling over an input signal composed of several input planes. Applies a 1D adaptive average pooling over an input signal composed of several input planes. Applies a 2D adaptive average pooling over an input signal composed of several input planes. Applies a 3D adaptive average pooling over an input signal composed of several input planes.\n\nPads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using the reflection of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor using replication of the input boundary. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with zero. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor boundaries with a constant value. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary. Pads the input tensor using circular padding of the input boundary.\n\nCreates a criterion that measures the mean absolute error (MAE) between each element in the input x and target y. Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input x and target y. This criterion computes the cross entropy loss between input logits and target. Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities: This loss combines a layer and the in one single class. Creates a criterion that measures the loss given inputs x1, x2, two 1D mini-batch or 0D , and a label 1D mini-batch or 0D y (containing 1 or -1). Measures the loss given an input tensor x and a labels tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 2D of target class indices). Creates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise. Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. Creates a criterion that optimizes a two-class classification logistic loss between input tensor x and target tensor y (containing 1 or -1). Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input x and target y of size (N,C). Creates a criterion that measures the loss given input tensors x1​, x2​ and a label y with values 1 or -1. Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input x (a 2D mini-batch ) and output y (which is a 1D tensor of target class indices, 0≤y≤x.size(1)−1): Creates a criterion that measures the triplet loss given an input tensors x1, x2, x3 and a margin with a value greater than 0. Creates a criterion that measures the triplet loss given input tensors a, p, and n (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").\n\nClip the gradient norm of an iterable of parameters. Clip the gradient norm of an iterable of parameters. Clip the gradients of an iterable of parameters at specified value. Compute the norm of an iterable of tensors. Scale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm. Utility functions to flatten and unflatten Module parameters to and from a single vector. Flatten an iterable of parameters into a single vector. Copy slices of a vector into an iterable of parameters. Fuse a convolutional module and a BatchNorm module into a single, new convolutional module. Fuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters. Fuse a linear module and a BatchNorm module into a single, new linear module. Fuse linear module parameters and BatchNorm module parameters into new linear module parameters. Convert of to The conversion recursively applies to nested , including . Utility functions to apply and remove weight normalization from Module parameters. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Given a module class object and args / kwargs, instantiate the module without initializing parameters / buffers. Abstract base class for creation of new pruning techniques. Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones. Prune (currently unpruned) units in a tensor at random. Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm. Prune entire (currently unpruned) channels in a tensor at random. Prune entire (currently unpruned) channels in a tensor based on their L -norm. Prune tensor by removing units with the lowest L1-norm. Prune tensor by removing random channels along the specified dimension. Prune tensor by removing channels with the lowest L -norm along the specified dimension. Globally prunes tensors corresponding to all parameters in by applying the specified . Prune tensor corresponding to parameter called in by applying the pre-computed mask in . Remove the pruning reparameterization from a module and the pruning method from the forward hook. Check if a module is pruned by looking for pruning pre-hooks. Parametrizations implemented using the new parametrization functionality in . Apply an orthogonal or unitary parametrization to a matrix or a batch of matrices. Apply weight normalization to a parameter in the given module. Apply spectral normalization to a parameter in the given module. Utility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the Parametrizations tutorial for more information on how to implement your own parametrizations. Remove the parametrizations on a tensor in a module. Context manager that enables the caching system within parametrizations registered with . A sequential container that holds and manages the original parameters or buffers of a parametrized . Utility functions to call a given Module in a stateless manner. Perform a functional call on the module by replacing the module parameters and buffers with the provided ones. Holds the data and list of of a packed sequence."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html",
        "document": "Click here to download the full example code\n\nAt this point, we have seen various feed-forward networks. That is, there is no state maintained by the network at all. This might not be the behavior we want. Sequence models are central to NLP: they are models where there is some sort of dependence through time between your inputs. The classical example of a sequence model is the Hidden Markov Model for part-of-speech tagging. Another example is the conditional random field.\n\nA recurrent neural network is a network that maintains some kind of state. For example, its output could be used as part of the next input, so that information can propagate along as the network passes over the sequence. In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state \\(h_t\\), which in principle can contain information from arbitrary points earlier in the sequence. We can use the hidden state to predict words in a language model, part-of-speech tags, and a myriad of other things.\n\nBefore getting to the example, note a few things. Pytorch’s LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indexes instances in the mini-batch, and the third indexes elements of the input. We haven’t discussed mini-batching, so let’s just ignore that and assume we will always have just 1 dimension on the second axis. If we want to run the sequence model over the sentence “The cow jumped”, our input should look like Except remember there is an additional 2nd dimension with size 1. In addition, you could go through the sequence one at a time, in which case the 1st axis will have size 1 also. # Step through the sequence one element at a time. # after each step, hidden contains the hidden state. # alternatively, we can do the entire sequence all at once. # the first value returned by LSTM is all of the hidden states throughout # the sequence. the second is just the most recent hidden state # (compare the last slice of \"out\" with \"hidden\" below, they are the same) # The reason for this is that: # \"out\" will give you access to all hidden states in the sequence # \"hidden\" will allow you to continue the sequence and backpropagate, # by passing it as an argument to the lstm at a later time\n\nIn the example above, each word had an embedding, which served as the inputs to our sequence model. Let’s augment the word embeddings with a representation derived from the characters of the word. We expect that this should help significantly, since character-level information like affixes have a large bearing on part-of-speech. For example, words with the affix -ly are almost always tagged as adverbs in English. To do this, let \\(c_w\\) be the character-level representation of word \\(w\\). Let \\(x_w\\) be the word embedding as before. Then the input to our sequence model is the concatenation of \\(x_w\\) and \\(c_w\\). So if \\(x_w\\) has dimension 5, and \\(c_w\\) dimension 3, then our LSTM should accept an input of dimension 8. To get the character level representation, do an LSTM over the characters of a word, and let \\(c_w\\) be the final hidden state of this LSTM. Hints:\n• None There are going to be two LSTM’s in your new model. The original one that outputs POS tag scores, and the new one that outputs a character-level representation of each word.\n• None To do a sequence model over characters, you will have to embed characters. The character embeddings will be the input to the character LSTM."
    },
    {
        "link": "https://medium.com/towards-data-science/pytorch-lstms-for-time-series-data-cd16190929d7",
        "document": "You might have noticed that, despite the frequency with which we encounter sequential data in the real world, there isn’t a huge amount of content online showing how to build simple LSTMs from the ground up using the Pytorch functional API. Even the LSTM example on Pytorch’s official documentation only applies it to a natural language problem, which can be disorienting when trying to get these recurrent models working on time series data. In this article, we’ll set a solid foundation for constructing an end-to-end LSTM, from tensor input and output shapes to the LSTM itself.\n\nThis article is structured with the goal of being able to implement any univariate time-series LSTM. We begin by examining the shortcomings of traditional neural networks for these tasks, and why an LSTM’s input is differently shaped to simple neural nets. We’ll then intuitively describe the mechanics that allow an LSTM to “remember.” With this approximate understanding, we can implement a Pytorch LSTM using a traditional model class structure inheriting from , and write a forward method for it. We use this to see if we can get the LSTM to learn a simple sine wave. Finally, we attempt to write code to generalise how we might initialise an LSTM based on the problem at hand, and test it on our previous examples.\n\nLet’s suppose we have the following time-series data. Rather than using complicated recurrent models, we’re going to treat the time series as a simple input-output function: the input is the time, and the output is the value of whatever dependent variable we’re measuring. This is essentially just simplifying a univariate time series.\n\nLet’s suppose that we’re trying to model the number of minutes Klay Thompson will play in his return from injury. Steve Kerr, the coach of the Golden State Warriors, doesn’t want Klay to come back and immediately play heavy minutes. Instead, he will start Klay with a few minutes per game, and ramp up the amount of time he’s allowed to play as the season goes on. We’re going to be Klay Thompson’s physio, and we need to predict how many minutes per game Klay will be playing in order to determine how much strapping to put on his knee.\n\nThus, the number of games since returning from injury (representing the input time step) is the independent variable, and Klay Thompson’s number of minutes in the game is the dependent variable. Suppose we observe Klay for 11 games, recording his minutes per game in each outing to get the following data.\n\nHere, we’ve generated the minutes per game as a linear relationship with the number of games since returning. We’re going to use 9 samples for our training set, and 2 samples for validation.\n\nWe know that the relationship between game number and minutes is linear. However, we’re still going to use a non-linear activation function, because that’s the whole point of a neural network. (Otherwise, this would just turn into linear regression: the composition of linear operations is just a linear operation.) As per usual, we use to build our model with one hidden layer, with 13 hidden neurons.\n\nWe now need to write a training loop, as we always do when using gradient descent and backpropagation to force a network to learn. To remind you, each training step has several key tasks:\n• Compute the forward pass through the network by applying the model to the training examples.\n• Calculate the loss based on the defined loss function, which compares the model output to the actual training labels.\n• Backpropagate the derivative of the loss with respect to the model parameters through the network. This is done with call on the loss, after setting the current parameter gradients to zero with .\n• Update the model parameters by subtracting the gradient times the learning rate. This is done with our optimiser, using .\n\nNow, all we need to do is instantiate the required objects, including our model, our optimiser, our loss function and the number of epochs we’re going to train for.\n\nAs we can see, the model is likely overfitting significantly (which could be solved with many techniques, such as regularisation, or lowering the number of model parameters, or enforcing a linear model form). The training loss is essentially zero. Due to the inherent random variation in our dependent variable, the minutes played taper off into a flat curve towards the last few games, leading the model to believes that the relationship more resembles a log rather than a straight line.\n\nAlthough it wasn’t very successful, this initial neural network is a proof-of-concept that we can just develop sequential models out of nothing more than inputting all the time steps together. However, without more information about the past, and without the ability to store and recall this information, model performance on sequential data will be extremely limited.\n\nThe simplest neural networks make the assumption that the relationship between the input and output is independent of previous output states. It assumes that the function shape can be learnt from the input alone. In cases such as sequential data, this assumption is not true. The function value at any one particular time step can be thought of as directly influenced by the function value at past time steps. There is a temporal dependency between such values. Long-short term memory networks, or LSTMs, are a form of recurrent neural network that are excellent at learning such temporal dependencies.\n\nThe key to LSTMs is the cell state, which allows information to flow from one cell to another. This represents the LSTM’s memory, which can be updated, altered or forgotten over time. The components of the LSTM that do this updating are called gates, which regulate the information contained by the cell. Gates can be viewed as combinations of neural network layers and pointwise operations.\n\nIf you don’t already know how LSTMs work, the maths is straightforward and the fundamental LSTM equations are available in the Pytorch docs. There are many great resources online, such as this one. As a quick refresher, here are the four main steps each LSTM cell undertakes:\n• Decide what information to remove from the cell state that is no longer relevant. This is controlled by a neural network layer (with a sigmoid activation function) called the forget gate. We feed the output of the previous cell into the forget gate, which in turn outputs a number between 0 and 1 determining how much or little to forget.\n• Update the cell state with new information. An NN layer called the input gate takes the concatenation of the previous cell’s output and the current input and decides what to update. A tanh layer takes the same concatenation and creates a vector of new candidate values that could be added to the state.\n• Update the old cell state to create a new cell state. We multiply the old state by the value determined in Step 1, forgetting the things we decided to forget earlier. Then we add the new candidate values we found in Step 2. These constitute the new cell state, scaled by how much we decided to update each state value. This is finished for this cell; we can pass this directly to the next cell in the model.\n• Generate the model output based on the previous output and the current input. First, we take our updated cell state and pass it through an NN layer. We then find the output of the output/input vector passed through the sigmoid layer, and then pointwise compose it with the modified cell state. This allows the cell full control over composing the cell state and the current cell inputs, which gives us an appropriate output.\n\nNote that we give the output twice in the diagram above. One of these outputs is to be stored as a model prediction, for plotting etc. The other is passed to the next LSTM cell, much as the updated cell state is passed to the next LSTM cell.\n\nOur problem is to see if an LSTM can “learn” a sine wave. This is actually a relatively famous (read: infamous) example in the Pytorch community. It’s the only example on Pytorch’s Examples Github repository of an LSTM for a time-series problem. However, the example is old, and most people find that the code either doesn’t compile for them, or won’t converge to any sensible output. (A quick Google search gives a litany of Stack Overflow issues and questions just on this example.) Here, we’re going to break down and alter their code step by step.\n\nWe begin by generating a sample of 100 different sine waves, each with the same frequency and amplitude but beginning at slightly different points on the x-axis.\n\nLet’s walk through the code above. N is the number of samples; that is, we are generating 100 different sine waves. Many people intuitively trip up at this point. Since we are used to training a neural network on individual data points, such as the simple Klay Thompson example from above, it is tempting to think of N here as the number of points at which we measure the sine function. This is wrong; we are generating N different sine waves, each with a multitude of points. The LSTM network learns by examining not one sine wave, but many.\n\nNext, we instantiate an empty array x. Think of this array as a sample of points along the x-axis. The array has 100 rows (representing the 100 different sine waves), and each row is 1000 elements long (representing L, or the granularity of the sine wave i.e. the number of distinct sampled points in each wave). We then fill x by sampling the first 1000 integers points and then adding a random integer in a certain range governed by T, where is just syntax to add the integer along rows. Note that we must reshape this second random integer to shape (N, 1) in order for Numpy to be able to broadcast it to each row of x.\n\nFinally, we simply apply the Numpy sine function to x, and let broadcasting apply the function to each sample in each row, creating one sine wave per row. We cast it to type . We can pick any individual sine wave and plot it using Matplotlib. Let’s pick the first sampled sine wave at index 0.\n\nTo build the LSTM model, we actually only have one module being called for the LSTM cell specifically. First, we’ll present the entire model class (inheriting from , as always), and then walk through it piece by piece.\n\nThe key step in the initialisation is the declaration of a Pytorch . You can find the documentation here. The cell has three main parameters:\n• : the number of expected features in the input x.\n• : the number of features in the hidden state h.\n• : this defaults to true, and in general we leave it that way.\n\nKeep in mind that the parameters of the LSTM cell are different from the inputs. The parameters here largely govern the shape of the expected inputs, so that Pytorch can set up the appropriate structure. The inputs are the actual training examples or prediction examples we feed into the cell.\n\nWe define two LSTM layers using two LSTM cells. Much like a convolutional neural network, the key to setting up input and hidden sizes lies in the way the two layers connect to each other. For the first LSTM cell, we pass in an input of size 1. Recall why this is so: in an LSTM, we don’t need to pass in a sliced array of inputs. We don’t need a sliding window over the data, as the memory and forget gates take care of the cell state for us. We don’t need to specifically hand feed the model with old data each time, because of the model’s ability to recall this information. This is what makes LSTMs so special.\n\nWe then give this first LSTM cell a hidden size governed by the variable when we declare our class, . This number is rather arbitrary; here, we pick 64. As mentioned above, this becomes an output of sorts which we pass to the next LSTM cell, much like in a CNN: the output size of the last step becomes the input size of the next step. In this cell, we thus have an input of size , and also a hidden layer of size . We then pass this output of size to a linear layer, which itself outputs a scalar of size one. We are outputting a scalar, because we are simply trying to predict the function value y at that particular time step.\n\nIn the forward method, once the individual layers of the LSTM have been instantiated with the correct sizes, we can begin to focus on the actual inputs moving through the network. An LSTM cell takes the following inputs:\n• : a tensor of inputs of shape , where we declared in the creation of the LSTM cell.\n• : a tensor containing the initial hidden state for each element in the batch, of shape\n• : a tensor containing the initial cell state for each element in the batch, of shape .\n\nTo link the two LSTM cells (and the second LSTM cell with the linear, fully-connected layer), we also need to know what an LSTM cell actually outputs: a tensor of shape .\n• : a tensor containing the next hidden state for each element in the batch, of shape\n• : a tensor containing the next cell state for each element in the batch, of shape .\n\nHere, our batch size is 100, which is given by the first dimension of our input; hence, we take . Since we know the shapes of the hidden and cell states are both , we can instantiate a tensor of zeros of this size, and do so for both of our LSTM cells.\n\nThe next step is arguably the most difficult. We must feed in an appropriately shaped tensor. Here, that would be a tensor of m points, where m is our training size on each sequence. However, in the Pytorch method (documentation here), if the parameter is not passed in, it will simply split each tensor into chunks of size 1. We want to split this along each individual batch, so our dimension will be the rows, which is equivalent to dimension 1.\n\nIt’s always a good idea to check the output shape when we’re vectorising an array in this way. Suppose we choose three sine curves for the test set, and use the rest for training. We can check what our training input will look like in our split method:\n\nSo, for each sample, we’re passing in an array of 97 inputs, with an extra dimension to represent that it comes from a batch. (Pytorch usually operates in this way. Even if we’re passing in a single image to the world’s simplest CNN, Pytorch expects a batch of images, and so we have to use ) We then output a new hidden and cell state. As we know from above, the hidden state output is used as input to the next LSTM cell. The hidden state output from the second cell is then passed to the linear layer.\n\nGreat — we’ve completed our model predictions based on the actual points we have data for. But the whole point of an LSTM is to predict the future shape of the curve, based on past outputs. So, in the next stage of the forward pass, we’re going to predict the next time steps. Recall that in the previous loop, we calculated the output to append to our array by passing the second LSTM output through a linear layer. This variable is still in operation — we can access it and pass it to our model again. This is good news, as we can predict the next time step in the future, one time step after the last point we have data for. The model takes its prediction for this final data point as input, and predicts the next data point.\n\nWe then do this again, with the prediction now being fed as input to the model. In total, we do this number of times, to produce a curve of length , in addition to the 1000 predictions we’ve already made on the 1000 points we actually have data for.\n\nThe last thing we do is concatenate the array of scalar tensors representing our outputs, before returning them. That’s it! We’ve built an LSTM which takes in a certain number of inputs, and, one by one, predicts a certain number of time steps into the future.\n\nDefining a training loop in Pytorch is quite homogeneous across a variety of common applications. However, in our case, we can’t really gain an intuitive understanding of how the model is converging by examining the loss. Yes, a low loss is good, but there’s been plenty of times when I’ve gone to look at the model outputs after achieving a low loss and seen absolute garbage predictions. This is usually due to a mistake in my plotting code, or even more likely a mistake in my model declaration. Thus, the most useful tool we can apply to model assessment and debugging is plotting the model predictions at each training step to see if they improve.\n\nOur first step is to figure out the shape of our inputs and our targets. We know that our data has the shape . That is, 100 different sine curves of 1000 points each. Next, we want to figure out what our train-test split is. We’ll save 3 curves for the test set, and so indexing along the first dimension of we can use the last 97 curves for the training set.\n\nNow comes time to think about our model input. One at a time, we want to input the last time step and get a new time step prediction out. To do this, we input the first 999 samples from each sine wave, because inputting the last 1000 would lead to predicting the 1001st time step, which we can’t validate because we don’t have data on it. Similarly, for the training target, we use the first 97 sine waves, and start at the 2nd sample in each wave and use the last 999 samples from each wave; this is because we need a previous time step to actually input to the model — we can’t input . Hence, the starting index for the target in the second dimension (representing the samples in each wave) is 1. This gives us two arrays of shape .\n\nThe test input and test target follow very similar reasoning, except this time, we index only the first three sine waves along the first dimension. Everything else is exactly the same, as we would expect: apart from the batch input size (97 vs 3) we need to have the same input and outputs for train and test sets.\n\nWe now need to instantiate the main components of our training loop: the model itself, the loss function, and the optimiser. The model is simply an instance of our class, and the loss function we will use for what amounts to a regression problem is . The only thing different to normal here is our optimiser. Instead of Adam, we will use what is called a limited-memory BFGS algorithm, which essentially boils down to estimating an inverse of the Hessian matrix as a guide through the variable space. You don’t need to worry about the specifics, but you do need to worry about the difference between and other optimisers. We’ll cover that in the training loop below.\n\nFinally, we get around to constructing the training loop. Fair warning, as much as I’ll try to make this look like a typical Pytorch training loop, there will be some differences. These are mainly in the function we have to pass to the optimiser, , which represents the typical forward and backward pass through the network. We update the weights with by passing in this function. According to Pytorch, the function is a callable that reevaluates the model (forward pass), and returns the loss. So this is exactly what we do.\n\nThe training loop starts out much as other garden-variety training loops do. However, notice that the typical steps of forward and backwards pass are captured in the function closure. This is just an idiosyncrasy of how the optimiser function is designed in Pytorch. We return the loss in , and then pass this function to the optimiser during . And that’s pretty much it for the training step.\n\nNext, we want to plot some predictions, so we can sanity-check our results as we go. To do this, we need to take the test input, and pass it through the model. This is where our parameter we included in the model itself is going to come in handy. Recall that passing in some non-negative integer to the forward pass through the model will give us predictions after the last output from the actual samples. This allows us to see if the model generalises into future time steps. We then detach this output from the current computational graph and store it as a numpy array.\n\nFinally, we write some simple code to plot the model’s predictions on the test set at each epoch. There are only three test sine curves, so we only need to call our function three times (we’ll draw each curve in a different colour). The plotted lines indicate future predictions, and the solid lines indicate predictions in the current range of the data.\n\nThe predictions clearly improve over time, as well as the loss going down. Our model works: by the 8th epoch, the model has learnt the sine wave. However, if you keep training the model, you might see the predictions start to do something funny. This is because, at each time step, the LSTM relies on outputs from the previous time step. If the prediction changes slightly for the 1001st prediction, this will perturb the predictions all the way up to prediction 2000, resulting in a nonsensical curve. There are many ways to counter this, but they are beyond the scope of this article. The best strategy right now would be to watch the plots to see if this error accumulation starts happening. Then, you can either go back to an earlier epoch, or train past it and see what happens.\n\nIf you’re having trouble getting your LSTM to converge, here’s a few things you can try:\n• Lower the number of model parameters (maybe even down to 15) by changing the size of the hidden layer. This reduces the model search space.\n• Try downsampling from the first LSTM cell to the second by reducing the passed to the second cell. You could also do this from the second LSTM cell to the linear fully-connected layer.\n• Add batchnorm regularisation, which limits the size of the weights by placing penalties on larger weight values, giving the loss a smoother topography.\n• Add dropout, which zeros out a random fraction of neuronal outputs across the whole model at each epoch. This generates slightly different models each time, meaning the model is forced to rely on individual neurons less.\n\nIf you implement the last two strategies, remember to call to instantiate the regularisation during training, and turn off the regularisation during prediction and evaluation using .\n\nThis whole exercise is pointless if we still can’t apply an LSTM to other shapes of input. Let’s generate some new data, except this time, we’ll randomly generate the number of curves and the samples in each curve. We won’t know what the actual values of these parameters are, and so this is a perfect way to see if we can construct an LSTM based on the relationships between input and output shapes.\n\nWe could then change the following input and output shapes by determining the percentage of samples in each curve we’d like to use for the training set.\n\nThe input and output shapes thus become:\n\nYou can verify that this works by running these inputs and targets through the LSTM (hint: make sure you instantiate a variable for future based on the length of the input).\n\nLet’s see if we can apply this to the original Klay Thompson example. We need to generate more than one set of minutes if we’re going to feed it to our LSTM. That is, we’re going to generate 100 different hypothetical sets of minutes that Klay Thompson played in 100 different hypothetical worlds. We’ll feed 95 of these in for training, and plot three of the remaining five to see how our model is learning.\n\nAfter using the code above to reshape the inputs and outputs based on L and N, we run the model and achieve the following:\n\nThis gives us the following images (we only show the first and last):\n\nVery interesting! Initially, the LSTM also thinks the curve is logarithmic. Whilst it figures out that the curve is linear on the first 11 games after a bit of training, it insists on providing a logarithmic curve for future games. What is so fascinating about that is that the LSTM is right — Klay can’t keep linearly increasing his game time, as a basketball game only goes for 48 minutes, and most processes such as this are logarithmic anyway. Obviously, there’s no way that the LSTM could know this, but regardless, it’s interesting to see how the model ends up interpreting our toy data. A future task could be to play around with the hyperparameters of the LSTM to see if it is possible to make it learn a linear function for future time steps as well. Additionally, I like to create a Python class to store all these functions in one spot. Then, you can create an object with the data, and you can write functions which read the shape of the data, and feed it to the appropriate LSTM constructors.\n\nIn summary, creating an LSTM for univariate time series data in Pytorch doesn’t need to be overly complicated. However, the lack of available resources online (particularly resources that don’t focus on natural language forms of sequential data) make it difficult to learn how to construct such recurrent models. Hopefully, this article provided guidance on setting up your inputs and targets, writing a Pytorch class for the LSTM forward method, defining a training loop with the quirks of our new optimiser, and debugging using visual tools such as plotting.\n\nIf you would like to learn more about the maths behind the LSTM cell, I highly recommend this article which sets out the fundamental equations of LSTMs beautifully (I have no connection to the author). I also recommend attempting to adapt the above code to multivariate time-series. All the core ideas are the same — you just need to think about how you might expand the dimensionality of the input."
    },
    {
        "link": "https://wandb.ai/sauravmaheshkar/LSTM-PyTorch/reports/Using-LSTM-in-PyTorch-A-Tutorial-With-Examples--VmlldzoxMDA2NTA5",
        "document": ""
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html",
        "document": "Click here to download the full example code\n\nA lot of effort in solving any machine learning problem goes into preparing the data. PyTorch provides many tools to make data loading easy and hopefully, to make your code more readable. In this tutorial, we will see how to load and preprocess/augment data from a non trivial dataset.\n\nTo run this tutorial, please make sure the following packages are installed:\n\nThe dataset we are going to deal with is that of facial pose. This means that a face is annotated like this:\n\nOver all, 68 different landmark points are annotated for each face.\n\nDataset comes with a file with annotations which looks like this:\n\nLet’s take a single image name and its annotations from the CSV, in this case row index number 65 for person-7.jpg just as an example. Read it, store the image name in and store its annotations in an (L, 2) array where L is the number of landmarks in that row.\n\nLet’s write a simple helper function to show an image and its landmarks and use it to show a sample.\n\nis an abstract class representing a dataset. Your custom dataset should inherit and override the following methods:\n• None so that returns the size of the dataset.\n• None to support the indexing such that can be used to get \\(i\\)th sample. Let’s create a dataset class for our face landmarks dataset. We will read the csv in but leave the reading of images to . This is memory efficient because all the images are not stored in the memory at once but read as required. Sample of our dataset will be a dict . Our dataset will take an optional argument so that any required processing can be applied on the sample. We will see the usefulness of in the next section. csv_file (string): Path to the csv file with annotations. root_dir (string): Directory with all the images. Let’s instantiate this class and iterate through the data samples. We will print the sizes of first 4 samples and show their landmarks.\n\nOne issue we can see from the above is that the samples are not of the same size. Most neural networks expect the images of a fixed size. Therefore, we will need to write some preprocessing code. Let’s create three transforms:\n• None : to crop from image randomly. This is data augmentation.\n• None : to convert the numpy images to torch images (we need to swap axes). We will write them as callable classes instead of simple functions so that parameters of the transform need not be passed every time it’s called. For this, we just need to implement method and if required, method. We can then use a transform like this: Observe below how these transforms had to be applied both on the image and landmarks. \"\"\"Rescale the image in a sample to a given size. output_size (tuple or int): Desired output size. If tuple, output is matched to output_size. If int, smaller of image edges is matched to output_size keeping aspect ratio the same. # h and w are swapped for landmarks because for images, # x and y axes are axis 1 and 0 respectively In the example above, uses an external library’s random number generator (in this case, Numpy’s ). This can result in unexpected behavior with (see here). In practice, it is safer to stick to PyTorch’s random number generator, e.g. by using instead. Now, we apply the transforms on a sample. Let’s say we want to rescale the shorter side of the image to 256 and then randomly crop a square of size 224 from it. i.e, we want to compose and transforms. is a simple callable class which allows us to do this. # Apply each of the above transforms on sample.\n\nLet’s put this all together to create a dataset with composed transforms. To summarize, every time this dataset is sampled:\n• None An image is read from the file on the fly\n• None Transforms are applied on the read image\n• None Since one of the transforms is random, data is augmented on sampling We can iterate over the created dataset with a loop as before. However, we are losing a lot of features by using a simple loop to iterate over the data. In particular, we are missing out on:\n• None Load the data in parallel using workers. is an iterator which provides all these features. Parameters used below should be clear. One parameter of interest is . You can specify how exactly the samples need to be batched using . However, default collate should work fine for most use cases. \"\"\"Show image with landmarks for a batch of samples.\"\"\" \\ # if you are using Windows, uncomment the next line and indent the for loop. # you might need to go back and change ``num_workers`` to 0.\n\nIn this tutorial, we have seen how to write and use datasets, transforms and dataloader. package provides some common datasets and transforms. You might not even have to write custom classes. One of the more generic datasets available in torchvision is . It assumes that images are organized in the following way: where ‘ants’, ‘bees’ etc. are class labels. Similarly generic transforms which operate on like , , are also available. You can use these to write a dataloader like this: For an example with training code, please see Transfer Learning for Computer Vision Tutorial."
    },
    {
        "link": "https://learnpytorch.io/04_pytorch_custom_datasets",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/58496535/creating-custom-dataset-in-pytorch",
        "document": "In PyTorch, I am trying to write a class that could return the entire and separately using syntax like and . The code skeleton looks like:\n\nHowever, when I use and to access the corresponding variables, nothing is returned.\n\nI am wondering why this is the case and how I can fix this.\n\nThank you for all of your attention.\n\nI have solved this problem by myself. The solution is pretty straightforward, which just utilizes the property of class variables."
    },
    {
        "link": "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html",
        "document": "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: and that allow you to use pre-loaded datasets as well as your own data. stores the samples and their corresponding labels, and wraps an iterable around the to enable easy access to the samples.\n\nPyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that subclass and implement functions specific to the particular data. They can be used to prototype and benchmark your model. You can find them here: Image Datasets, Text Datasets, and Audio Datasets\n\nHere is an example of how to load the Fashion-MNIST dataset from TorchVision. Fashion-MNIST is a dataset of Zalando’s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28×28 grayscale image and an associated label from one of 10 classes. We load the FashionMNIST Dataset with the following parameters:\n• None is the path where the train/test data is stored,\n• None downloads the data from the internet if it’s not available at .\n• None and specify the feature and label transformations\n\nA custom Dataset class must implement three functions: , , and . Take a look at this implementation; the FashionMNIST images are stored in a directory , and their labels are stored separately in a CSV file . In the next sections, we’ll break down what’s happening in each of these functions. The __init__ function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section). The labels.csv file looks like: The __len__ function returns the number of samples in our dataset. The __getitem__ function loads and returns a sample from the dataset at the given index . Based on the index, it identifies the image’s location on disk, converts that to a tensor using , retrieves the corresponding label from the csv data in , calls the transform functions on them (if applicable), and returns the tensor image and corresponding label in a tuple.\n\nPreparing your data for training with DataLoaders¶ The retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s to speed up data retrieval. is an iterable that abstracts this complexity for us in an easy API.\n\nWe have loaded that dataset into the and can iterate through the dataset as needed. Each iteration below returns a batch of and (containing features and labels respectively). Because we specified , after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers)."
    },
    {
        "link": "https://medium.com/analytics-vidhya/creating-a-custom-dataset-and-dataloader-in-pytorch-76f210a1df5d",
        "document": "Training a deep learning model requires us to convert the data into the format that can be processed by the model. For example the model might require images with a width of 512, a height of 512, but the data we collected contains images with a width of 1280, and a height of 720. We therefore need some way to be able to convert the available data we have, into the exact format required by the model.\n\nA dataloader in simple terms is a function that iterates through all our available data and returns it in the form of batches. For example if we have a dataset of 100 images, and we decide to batch the data with a size of 4. Our dataloader would process the data, and return 25 batches of 4 images each.\n\nCreating a dataloader can be done in many ways, and does not require torch by any means to work. Using torch however makes the task a lot easier. Keeping that in mind, lets start by understanding what the the Torch Dataset and Dataloder Classes contains.\n• The Torch Dataset class is basically an abstract class representing the dataset. It allows us to treat the dataset as an object of a class, rather than a set of data and labels.\n• The main task of the Dataset class is to return a pair of [input, label] every time it is called. We can define functions inside the class to preprocess the data, and return it in the format we require.\n• The class must contain two main functions:\n\n__len__(): This is a fuction that returns the length of the dataset.\n\n__getitem__(): This is a function that returns one training example.\n• The torch dataset class can be imported from torch.utils.data.Dataset\n• The Torch Dataloader not only allows us to iterate through the dataset in batches, but also gives us access to inbuilt functions for multiprocessing(allows us to load multiple batches of data in parallel, rather than loading one batch at a time), shuffling, etc.\n• The torch Dataloader takes a torch Dataset as input, and calls the __getitem__() function from the Dataset class to create a batch of data.\n• The torch dataloader class can be imported from torch.utils.data.DataLoader\n• I wont go into the entire process of training a model, but I will explain step by step, the process of creating the dataset class, and the dataloader.\n• The requirements for the code will be:\n\n numpy: pip3 install numpy \n\n opencv: pip3 insall opencv-python\n\n torch: pip3 install torch\n\n glob: pip3 install glob\n• I have created a sample dataset for the task of a classification model, to classify between cats and dogs. The folder structure is as follows. We have the Project folder that contains the code Main.py, and a folder called Dog_Cat_Dataset. This folder called Dog_Cat_Dataset is the dataset folder that contains 2 subfolders inside it called dogs and cats. Both the dogs and cats folders have 5 images each.\n• Lets start the code Main.py with a few imports.\n• glob: allows us to retrieve paths of data inside sub folders easily\n\ncv2: is used as the image processing library to read and preprocess images\n\nnumpy: is used for matrix operations\n\ntorch: is used to create the Dataset and Dataloader classes, and for converting data to tensors.\n• Lets go through this class in detail.\n• We create a class called CustomDataset, and pass the argument Dataset, to allow it to inherit the functionality of the Torch Dataset Class.\n• We define the init function to initialize our variables. The variable self.imgs_path contains the base path to our Dog_Cat_Dataset folder\n• We then use glob to retrieve the list of all folders inside the base folder we specified. In our example, the Dog_Cat_Dataset folder contains two subfolders called dogs and cats. The “ * ” term added to the self.imgs_path indicates we want to search for all folders or files inside the specified path.(Note that using the “*” operation would return every single file inside the folder, not only the folder names. So make sure there is nothing other than the dogs and cats folders in this location.)\n• The print statement would return the following output:\n\n[‘Dog_Cat_Dataset/dogs’, ‘Dog_Cat_Dataset/cats’]\n• This file_list variable contains the list of all the classes in the dataset(dogs and cats).\n• We now start creating the data list, that would contain the paths to all the images in our dataset.\n• We iterate over all the classes in our file list (dogs and cats), and for each class, we first start by extracting the actual class name. As you could see from the printed file list, each class was represented with respected to its base path; i.e Dog_Cat_Dataset/dogs. Since we would much rather refer to the class as dogs and cats, rather than with respect to its path, we create a class_name variable by splitting this class_path by “/”. (Splitting by “/” would return a list [“Dog_Cat_Dataset” , “dogs”]. Taking the [-1] index would use the last entry in the list. In this case it would be “dogs”.\n• Now that we are iterating through each class of the dataset(dog and cat), we want to retrieve each image in their folders (i.e 1.jpeg, 2.jpeg, etc). To do this we once again use glob to return all files in the folders with the extension “.jpeg”. The complete string we pass to glob is Dog_Cat_Dataset/dogs/*.jpeg .The “*.jpeg” indicates we want every file which has an extension of “.jpeg” .\n• We append the file path for each image to the list self.data, along with its corresponding class name. This gives us a way to retrieve the input image along with its corresponding label. Printing the list would return the following output.\n• We additionally define a class map, and an image dimension. The self.class_map dictionary that allows us to convert the string of the classes, to a number; i.e “dogs” corresponds to class 0, and “cats” corresponds to class 1, and the image dimension is the size that we will resize all the images to, so that they all have the same size.\n• Now that we have created a list with all our data, we start coding the function for __len__(), which is mandatory for a Torch Dataset object.\n• The size of our dataset is just the number of individual images we have, which can be obtained through the length of the self.data list. (Torch internally uses this function to understand the size of the dataset in its dataloader, to call the __getitem__() function with an index within this dataset size)\n• We start by defining the __getitem__ function to take the object of itself as input(self), and an idx. The idx is the term that this function will be called with, that corresponds to which image needs to be returned from our self.data list.\n• We retrieve the image path and class name corresponding to the idx from the self.data list.\n• We use opencv as the image processing library to load the image and resize it to the required dimension.\n• We read the image using the imread function that loads an image from the given image path, and then resize it to the dimensions we specified in the self.img_dim variable.\n• Deep learning models for classification generally make use of a number id associated with the class, rather than a name (“dogs” -> 0). The self.class_map dictionary just provides the mapping from the name to the number.\n• Once we have loaded the image, and obtained its corresponding class id, we convert the variables to tensors. Training models with torch requires us to convert variables to the torch tensor format, that contain internal methods for calculating gradients, etc.\n• We create the variable img_tensor that is the tensor form of the img we loaded. Opencv uses the library numpy to represent images as matrices, and the torch.from_numpy function allows us to convert a numpy array to a torch tensor.\n• Torch convolutions require images to be in a channel first format; i.e for example a 3 channel image(Red, Green and Blue channels) would be generally represented as: (Width, Height, Channels) in numpy, however torch requires us to convert this to: (Channels, Width, Height).\n• For this conversion we use the permute function of torch, that allows us to change the ordering of the dimensions of a torch tensor. The arguments we pass to it, correspond to the new ordering of dimensions we want. For example in our case, we have (Width, Height, Channels). \n\n(Width -> 0), (Height->1), (Channels->2)\n\nWe want to reorder these dimensions to make channels first, therefore, we use img_tensor.permute(2, 0, 1), which would make the 2nd dimension first.\n• We then convert the integer value of class_id to a torch tensor, and also increase its dimensionality by refering to it as [class_id]. This is to ensure that the data can be batched in the dimensions torch requires it. (Torch requires labels to be in the shape [batch_size, label_dimension]. Using just class_id, rather that [class_id] woud lead to us having a final size of [batch_size], as each class_id is just a single value).\n• We return the img_tensor and class_id, so that anytime the __getitem__ function is called with an idx, it is returned an image with its corresponding label.\n• Note: Additionally torch might require the tensors returned to be converted to type float. That can be done with the following modification:\n• Now that we have created the dataset, we can use this class in the torch dataloader to iterate through this data while training.\n• To test out the dataset and our dataloader, in the main function of our script, we create an instance of the CustomDataset we created, and call it dataset.\n• The Torch DataLoader takes as input this dataset, along with other arguments for batch_size, shuffle, etc, which are extra terms to specify how many images we want in each batch of data, and whether we want to randomize the way in which we generate the idx, we used before.\n• We can test the working of our dataloader by iterating through it, and printing the shape of our input images, and our labels.\n• The torch dataloader does take a lot of extra arguments like :\n• no_workers: Corresponds to how many workers in parallel we want to load the data. Using more number of workers would speed up the data loading process.\n• pin_memory: When training on a GPU device, we would want to speed up the process of passing the loaded data from CPU to GPU. Therefore setting it to true would speed up this data transfer.\n• drop_last: It skips the last batch of data, if it has samples < batch_size. For example in our case, we have 10 images in our dataset, and we specified a batch size of 4. This would mean that the last batch has only 2 images. Setting drop_last=True, would skip this last batch, as it has < 4 images.\n• The dataset class of torch can be used like any other class in python, and have any number of sub functions in it, as long as it has the 2 required functions(__len__, and __getitem__).\n• The returned data can also be passed to GPU, if available. For example: img_tensor.to(“cuda”).\n• The torch dataloader has an additional list of arguments that can be used\n\nLink\n• I have uploaded the complete code for this post on github: Code"
    }
]