[
    {
        "link": "https://github.com/lh3/miniasm",
        "document": "Miniasm is a very fast OLC-based de novo assembler for noisy long reads. It takes all-vs-all read self-mappings (typically by minimap) as input and outputs an assembly graph in the GFA format. Different from mainstream assemblers, miniasm does not have a consensus step. It simply concatenates pieces of read sequences to generate the final unitig sequences. Thus the per-base error rate is similar to the raw input reads.\n\nSo far miniasm is in early development stage. It has only been tested on a dozen of PacBio and Oxford Nanopore (ONT) bacterial data sets. Including the mapping step, it takes about 3 minutes to assemble a bacterial genome. Under the default setting, miniasm assembles 9 out of 12 PacBio datasets and 3 out of 4 ONT datasets into a single contig. The 12 PacBio data sets are PacBio E. coli sample, ERS473430, ERS544009, ERS554120, ERS605484, ERS617393, ERS646601, ERS659581, ERS670327, ERS685285, ERS743109 and a deprecated PacBio E. coli data set. ONT data are acquired from the Loman Lab.\n\nFor a C. elegans PacBio data set (only 40X are used, not the whole dataset), miniasm finishes the assembly, including reads overlapping, in ~10 minutes with 16 CPUs. The total assembly size is 105Mb; the N50 is 1.94Mb. In comparison, the HGAP3 produces a 104Mb assembly with N50 1.61Mb. This dotter plot gives a global view of the miniasm assembly (on the X axis) and the HGAP3 assembly (on Y). They are broadly comparable. Of course, the HGAP3 consensus sequences are much more accurate. In addition, on the whole data set (assembled in ~30 min), the miniasm N50 is reduced to 1.79Mb. Miniasm still needs improvements.\n\nMiniasm confirms that at least for high-coverage bacterial genomes, it is possible to generate long contigs from raw PacBio or ONT reads without error correction. It also shows that minimap can be used as a read overlapper, even though it is probably not as sensitive as the more sophisticated overlapers such as MHAP and DALIGNER. Coupled with long-read error correctors and consensus tools, miniasm may also be useful to produce high-quality assemblies.\n• Crude read selection. For each read, find the longest contiguous region covered by three good mappings. Get an approximate estimate of read coverage.\n• Fine read selection. Use the coverage information to find the good regions again but with more stringent thresholds. Discard contained reads.\n• Generate a string graph. Prune tips, drop weak overlaps and collapse short bubbles. These procedures are similar to those implemented in short-read assemblers.\n• Consensus base quality is similar to input reads (may be fixed with a consensus tool).\n• Only tested on a dozen of high-coverage PacBio/ONT data sets (more testing needed).\n• Prone to collapse repeats or segmental duplications longer than input reads (hard to fix without error correction)."
    },
    {
        "link": "https://github.com/lh3/miniasm/blob/master/README.md",
        "document": "Miniasm is a very fast OLC-based de novo assembler for noisy long reads. It takes all-vs-all read self-mappings (typically by minimap) as input and outputs an assembly graph in the GFA format. Different from mainstream assemblers, miniasm does not have a consensus step. It simply concatenates pieces of read sequences to generate the final unitig sequences. Thus the per-base error rate is similar to the raw input reads.\n\nSo far miniasm is in early development stage. It has only been tested on a dozen of PacBio and Oxford Nanopore (ONT) bacterial data sets. Including the mapping step, it takes about 3 minutes to assemble a bacterial genome. Under the default setting, miniasm assembles 9 out of 12 PacBio datasets and 3 out of 4 ONT datasets into a single contig. The 12 PacBio data sets are PacBio E. coli sample, ERS473430, ERS544009, ERS554120, ERS605484, ERS617393, ERS646601, ERS659581, ERS670327, ERS685285, ERS743109 and a deprecated PacBio E. coli data set. ONT data are acquired from the Loman Lab.\n\nFor a C. elegans PacBio data set (only 40X are used, not the whole dataset), miniasm finishes the assembly, including reads overlapping, in ~10 minutes with 16 CPUs. The total assembly size is 105Mb; the N50 is 1.94Mb. In comparison, the HGAP3 produces a 104Mb assembly with N50 1.61Mb. This dotter plot gives a global view of the miniasm assembly (on the X axis) and the HGAP3 assembly (on Y). They are broadly comparable. Of course, the HGAP3 consensus sequences are much more accurate. In addition, on the whole data set (assembled in ~30 min), the miniasm N50 is reduced to 1.79Mb. Miniasm still needs improvements.\n\nMiniasm confirms that at least for high-coverage bacterial genomes, it is possible to generate long contigs from raw PacBio or ONT reads without error correction. It also shows that minimap can be used as a read overlapper, even though it is probably not as sensitive as the more sophisticated overlapers such as MHAP and DALIGNER. Coupled with long-read error correctors and consensus tools, miniasm may also be useful to produce high-quality assemblies.\n• Crude read selection. For each read, find the longest contiguous region covered by three good mappings. Get an approximate estimate of read coverage.\n• Fine read selection. Use the coverage information to find the good regions again but with more stringent thresholds. Discard contained reads.\n• Generate a string graph. Prune tips, drop weak overlaps and collapse short bubbles. These procedures are similar to those implemented in short-read assemblers.\n• Consensus base quality is similar to input reads (may be fixed with a consensus tool).\n• Only tested on a dozen of high-coverage PacBio/ONT data sets (more testing needed).\n• Prone to collapse repeats or segmental duplications longer than input reads (hard to fix without error correction)."
    },
    {
        "link": "https://yiweiniu.github.io/blog/2018/03/Genome-assembly-pipeline-miniasm-Racon",
        "document": "There are two good examples:\n\nand a paper based on , actually, it is a consensus tool called Racon .\n\nThe pipeline consists of the following steps:\n• using / for fast all-vs-all overlap of raw reads ( for overlap detection, Overlap)\n• using , this “simply concatenates pieces of read sequences to generate the final sequences. Thus the per-base error rate is similar to the raw input reads.” ( layout for generating raw contigs, Layout)\n• mapping the raw reads back to the assembly using again ( for mapping of raw reads to raw contigs, Consensus)\n• using (‘rapid consensus’) for consensus calling ( for generating high-quality consensus sequences, Consensus)\n\nCompared with general pipelines, it achieves ‘similar or better quliaty’ while ‘being an order of magnitude faster’.\n\nAs described in the paper , published long-read assembly pipelines all include four stages:\n• assembly of error corrected reads (may involve all-vs-all read mapping again, but as the error rate is much reduced at this step, it is easier and faster than stage 1)\n\nMinimap is an experimental tool to efficiently find multiple approximate mapping positions between two sets of long sequences, such as between reads and reference genomes, between genomes and between long noisy reads. By default, it is tuned to have high sensitivity to 2kb matches around 20% divergence but with low specificity. Minimap does not generate alignments as of now and because of this, it is usually tens of times faster than mainstream aligners. With four CPU cores, minimap can map 1.6Gbp PacBio reads to human in 2.5 minutes, 1Gbp PacBio E. coli reads to pre-indexed 9.6Gbp bacterial genomes in 3 minutes, to pre-indexed >100Gbp nt database in ~1 hour (of which ~20 minutes are spent on loading index from the network filesystem; peak RAM: 10GB), map 2800 bacteria to themselves in 1 hour, and map 1Gbp E. coli reads against themselves in a couple of minutes.\n\n\n\n Minimap does not replace mainstream aligners, but it can be useful when you want to quickly identify long approximate matches at moderate divergence among a huge collection of sequences. For this task, it is much faster than most existing tools.\n\nMinimap2 is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database. Typical use cases include: (1) mapping PacBio or Oxford Nanopore genomic reads to the human genome; (2) finding overlaps between long reads with error rate up to ~15%; (3) splice-aware alignment of PacBio Iso-Seq or Nanopore cDNA or Direct RNA reads against a reference genome; (4) aligning Illumina single- or paired-end reads; (5) assembly-to-assembly alignment; (6) full-genome alignment between two closely related species with divergence below ~15%.\n\n\n\n For ~10kb noisy reads sequences, minimap2 is tens of times faster than mainstream long-read mappers such as BLASR, BWA-MEM, NGMLR and GMAP. It is more accurate on simulated long reads and produces biologically meaningful alignment ready for downstream analyses. For >100bp Illumina short reads, minimap2 is three times as fast as BWA-MEM and Bowtie2, and as accurate on simulated data. Detailed evaluations are available from the minimap2 preprint.\n\nMiniasm is a very fast OLC-based de novo assembler for noisy long reads. It takes all-vs-all read self-mappings (typically by minimap) as input and outputs an assembly graph in the GFA format. Different from mainstream assemblers, miniasm does not have a consensus step. It simply concatenates pieces of read sequences to generate the final unitig sequences. Thus the per-base error rate is similar to the raw input reads.\n\n\n\n So far miniasm is in early development stage. It has only been tested on a dozen of PacBio and Oxford Nanopore (ONT) bacterial data sets. Including the mapping step, it takes about 3 minutes to assemble a bacterial genome. Under the default setting, miniasm assembles 9 out of 12 PacBio datasets and 3 out of 4 ONT datasets into a single contig. The 12 PacBio data sets are PacBio E. coli sample, ERS473430, ERS544009, ERS554120, ERS605484, ERS617393, ERS646601, ERS659581, ERS670327, ERS685285, ERS743109 and a deprecated PacBio E. coli data set. ONT data are acquired from the Loman Lab.\n\n\n\n Miniasm confirms that at least for high-coverage bacterial genomes, it is possible to generate long contigs from raw PacBio or ONT reads without error correction. It also shows that minimap can be used as a read overlapper, even though it is probably not as sensitive as the more sophisticated overlapers such as MHAP and DALIGNER. Coupled with long-read error correctors and consensus tools, miniasm may also be useful to produce high-quality assemblies.\n\nSince is not a stand-alone genome assembly tool, it depends on minimap or minimap2. had been archived by the author, and now is the successor. But is also worth a try.\n\nIn this note I only used or as a read overlapper for assembly. Go see the docs of minimap2 for full instructions.\n\nRacon is a consensus module for raw de novo DNA assembly of long uncorrected reads.\n\nRacon is intended as a standalone consensus module to correct raw contigs generated by rapid assembly methods which do not include a consensus step. The goal of Racon is to generate genomic consensus which is of similar or better quality compared to the output generated by assembly methods which employ both error correction and consensus steps, while providing a speedup of several times compared to those methods. It supports data produced by both Pacific Biosciences and Oxford Nanopore Technologies.\n\n\n\n Racon can be used as a polishing tool after the assembly with either Illumina data or data produced by third generation of sequencing. The type of data inputed is automatically detected.\n\n\n\n Racon takes as input only three files: contigs in FASTA/FASTQ format, reads in FASTA/FASTQ format and overlaps/alignments between the reads and the contigs in MHAP/PAF/SAM format. Output is a set of polished contigs in FASTA format printed to stdout. All input files can be compressed with gzip.\n\n\n\n Racon can also be used as a read error-correction tool. In this scenario, the MHAP/PAF/SAM file needs to contain pairwise overlaps between reads with dual overlaps.\n\n\n\n A wrapper script is also available to enable easier usage to the end-user for large datasets. It has the same interface as racon but adds two additional features from the outside. Sequences can be subsampled to decrease the total execution time (accuracy might be lower) while target sequences can be split into smaller chunks and run sequentially to decrease memory consumption. Both features can be run at the same time as well.\n• The docs are not good enough\n• huge memory consumption (may not suitable for large genome)\n• bugs (at least for )\n\nThe assembling size were larger than the estimated genome size (~850M) in both runs. But this pipeline is very fast."
    },
    {
        "link": "https://academic.oup.com/bioinformatics/article/32/14/2103/1742895",
        "document": "High-throughput short-read sequencing technologies, such as Illumina, have empowered a variety of biological researches and clinical applications that would not be practical with the older Sanger sequencing. However, the short read length (typically a few hundred basepairs) has posed a great challenge to de novo assembly as many repetitive sequences and segmental duplications are longer than the read length and can hardly be resolved by short reads even with paired-end data (Alkan et al., 2011). Although with increased read length and improved algorithms we are now able to produce much better short-read assemblies than a few years ago, the contiguity and completeness of the assemblies are still not as good as Sanger assemblies (Chaisson et al., 2015).\n\nThe PacBio’s SMRT technology were developed partly as an answer to the problem with short-read de novo assembly. However, due to the high per-base error rate, around 15%, these reads were only used as a complement to short reads initially (Bashir et al., 2012; Koren et al., 2012; Ribeiro et al., 2012), until Chin et al. (2013) and Koren et al. (2013) demonstrated the feasibility of SMRT-only assembly. Since then, SMRT is becoming the preferred technology for finishing small genomes and producing high-quality Eukaryotic genomes (Berlin et al., 2015).\n\nOxford Nanopore Technologies (ONT) has recently offered another long-read sequencing technology. Although the per-base error rate was high at the early access phase (Quick et al., 2014), the latest data quality has been greatly improved. Loman et al. (2015) confirmed that we can achieve high-quality bacterial assembly with ONT data alone.\n\nPublished long-read assembly pipelines all include four stages: (i) all-vs-all raw read mapping, (ii) raw read error correction, (iii) assembly of error corrected reads and (iv) contig consensus polish. Stage (iii) may involve all-vs-all read mapping again, but as the error rate is much reduced at this step, it is easier and faster than stage (i). Table 1 shows the tools used for each stage. Notably, our tool minimap is a raw read overlapper and miniasm is an assembler. We do not correct sequencing errors, but instead directly produce unpolished and uncorrected contig sequences from raw read overlaps. The idea of correction-free assembly was inspired by talks given by Gene Myers. Sikic et al. (personal communication) are also independently exploring such an approach.\n\nAs we can see from Table 1, each stage can be achieved with multiple tools. Although we have successfully combined tools into different pipelines, we need to change or convert the input/output formats to make them work together. Another contribution of this article is the proposal of concise mapping and assembly formats, which will hopefully encourage modular design of assemblers and the associated tools.\n\nLet be the alphabet of nucleotides. For a symbol is the Watson-Crick complement of a. A string over Σ is also called a DNA sequence. Its length is ⁠; its reverse complement is ⁠. For convenience, we define strand function such that and ⁠. Here is the set of all DNA sequences.\n\nBy convention, we call a k-long DNA sequence as a k-mer. We use the notation to denote a k-long substring of s starting at i. is the set of all k-mers.\n\nBLAST (Altschul et al., 1997) and BLAT (Kent, 2002) are among the most popular sequence similarity search tools. They use one k-mer hash function to hash k-mers at the positions of a target sequence and keep the hash values in a hash table. Upon query, they use the same hash function on every k-mer of the query sequence and look up the hash table for potential matches. If there are one or multiple k-mer matches in a small window, these aligners extend the matches with dynamic programming to construct the final alignment.\n\nDALIGNER (Myers, 2014) does not use a hash table. It instead identifies k-mer matches between two sets of reads by sorting k-mers and merging the sorted lists. DALIGNER is fast primarily because sorting and merging are highly cache efficient.\n\nMHAP (Berlin et al., 2015) differs from others in the use of MinHash sketch (Broder, 1997). Briefly, given a read sequence s and m k-mer hash functions ⁠, MHAP computes with each hash function ⁠, and takes list ⁠, which is called the sketch of s, as a reduced representation of s. Suppose and are the sketches of two reads, respectively. When the two reads are similar to each other or have significant overlaps, there are likely to exist multiple j such that ⁠. Potential matches can thus be identified. A limitation of MinHash sketch is that it always selects a fixed number of hash values regardless of the length of the sequences. This may waste space or hurt sensitivity when input sequences vary greatly in lengths.\n\nMinimap is heavily influenced by all these works. It adopts the idea of sketch like MHAP but takes minimizers (Roberts et al., 2004; Schleimer et al., 2003) as a reduced representation instead; it stores k-mers in a hash table like BLAT and MHAP but also uses sorting extensively like DALIGNER. In addition, minimap is designed not only as a read overlapper but also as a read-to-genome and genome-to-genome mapper. It has more potential applications.\n\nLet be the set of minimizers of s. Algorithm 1 gives the pseudocode to compute in time. Our actual implementation is close to in average case. It uses a queue to cache the previous minimals and avoids the loops at line 1 and 2 most of time. In practice, time spent on collecting minimizers is insignificant.\n\nThis hash function always maps a k-mer to a distinct 2k-bit integer. A problem with this is that poly-A, which is often highly enriched in genomes, always gets zero, the smallest value. We may oversample these non-informative poly-A and hurt practical performance. To alleviate this issue, we use function instead, where h is an invertible integer hash function on (Algorithm 2; http://bit.ly/invihgi). The invertibility of h is not essential, but as such never maps two distinct k-mers to the same 2k-bit integer, it helps to reduce hash collisions.\n\nNote that in a window of w consecutive k-mers, there may be more than one minimizers. Algorithm 1 keeps them all with the loop at line 2. This way, a minimizer of s always corresponds to a minimizer of ⁠.\n\nFor read overlapping, we use k = 15 and w = 5 to find minimizers.\n\nAlgorithm 3 describes indexing target sequences. It keeps minimizers of all target sequences in a hash table where the key is the minimizer hash and the value is a set of target sequence index, the position of the minimizer and the strand (packed into one 64-bit integer).\n\nIn implementation, we do not directly insert minimizers to the hash table. Instead, we append minimizers to an array of two 64-bit integers (one for minimizer sequence and one for position) and sort the array after collecting all minimizers. The hash table keeps the intervals on the sorted array. This procedure dramatically reduces heap allocations and cache misses, and is supposedly faster than direct hash table insertion.\n\nGiven two sequences s and ⁠, we say we find a minimizer hit if there exist and with (⊕ is the XOR operator). Here h is the minimizer hash value, x indicates the relative strand and i and are the positions on the two sequences, respectively. We say two minimizer hits and are ϵ-away if 1) x = 0 and or 2) x = 1 and ⁠. Intuitively, ϵ-away hits are approximately colinear within a band of width ϵ (500bp by default). Given a set of minimizer hits ⁠, we can cluster for x = 0 or for x = 1 to identify long colinear matches. This procedure is inspired by Hough Transformation mentioned by Sovic et al. (2015).\n\nAlgorithm 4 gives the details of the mapping algorithm. The loop at line 1 collects minimizer hits between the query and all the target sequences. The loop at line 2 performs a single-linkage clustering to group approximately colinear hits. Some hits in a cluster may not be colinear because two minimizer hits within distance ϵ are always ϵ-away. To fix this issue, we find the maximal colinear subset of hits by solving a longest increasing sequencing problem (line 3). This subset is the final mapping result. In practical implementation, we set thresholds on the size of the subset (4 by default) and the number of matching bases in the subset to filter poor mappings (100 for read overlapping).\n\nTwo strings v and w may be mapped to each other based on their sequence similarity. If v can be mapped to a substring of w, we say w contains v. If a suffix of v and a prefix of w can be mapped to each other, we say v overlaps w, written as ⁠. If we regard strings v and w as vertices, the overlap relationship defines a directed edge between them. The length of equals the length of v’s prefix that is not in the prefix–suffix match.\n\nLet be a graph without multi-edges, where V is a set of DNA sequences (vertices), E a set of overlaps between them (edges) and is the edge length function. G is said to be Watson-Crick complete if (i) and (ii) ⁠. G is said to be containment-free if any sequence v is not contained in other sequences in V. If G is both Watson–Crick complete and containment-free, it is an assembly graph. By definition, any vertex v has a complement vertex in the graph and any edge has a complement edge ⁠. Let be the outdegree of v and be the indegree. It follows that ⁠.\n\nAn assembly graph has the same topology as a string graph (Myers, 2005), though the interpretation of the vertex set V is different. In a string graph, V is the set of the two ends of sequences, not the set of forward and reverse-complemented sequences. De Bruijn graph can be regarded as a special case of overlap graph. It is also an assembly graph.\n\nIn an assembly graph, an edge is transitive if there exist and ⁠. Removing a transitive edge does not affect the connectivity of the graph. A vertex v is a tip if and ⁠. The majority of tips are caused by artifacts or missing overlaps. A bubble is a directed acyclic subgraph with a single source v and a single sink w having at least two paths between v and w, and without connecting the rest of the graph. The bubble is tight if and ⁠. A bubble may be caused by missing overlaps or by variants between haplotypes in multi-ploidy samples or paralogs. It is preferred to collapse bubbles for high contiguity, though this introduces loss of information.\n\nRaw read sequences may contain artifacts such as untrimmed adapters and chimaera. The first step of assembly to reduce such artifacts by examining read-to-read mappings. For each read, miniasm computes per-base coverage based on good mappings against other reads (longer than 2000 bp with at least 100 bp non-redundant bases on matching minimizers). It then identifies the longest region having coverage three or more, and trims bases outside this region.\n\nFor each trimmed mapping, miniasm applies Algorithm 5 to classify the mapping (see also Fig. 1 for the explanation of input variables). It ignores internal matches, drops contained reads and adds overlaps to the assembly graph. For a pair of reads, miniasm uses the longest overlap only to avoid multi-edges.\n\nAfter constructing the assembly graph, miniasm removes transitive edges (Myers, 2005), trims tipping unitigs composed of few reads (4 by default) and pops small bubbles (Zerbino and Birney, 2008). Algorithm 6 detects bubbles where the longest path is shorter than d (50 kb by default). It is adapted from Kahn’s topological sorting algorithm (Kahn, 1962). It starts from the potential source and visits a vertex when all its incoming edges are visited before. Algorithm 6 only detects bubbles. We can keep track of the optimal parent vertex at line 1 and then backtrack to collapse bubbles to a single path. Fermi (Li, 2012) uses a similar algorithm except that it keeps two optimal paths through the bubble. Onodera et al. (2013) and Brankovic et al. (2015) have also independently found similar algorithms.\n\nIn addition, if and exist and ⁠, miniasm removes if is small enough (70% by default). When there are longer overlaps, shorter overlaps after transitive reduction may be due to repeats. However, non-repetitive overlaps may also be removed at a small chance, which leads to missing overlaps and misassemblies.\n\nIf there are no multi-edges in the assembly graph, we can use to represent a path consisting of k vertices. The sequence spelled from this path is the concatenation of vertex substrings: ⁠, where is the substring between i and j inclusive, and is the string concatenation operator.\n\nIn a transitively reduced graph, a unitig (Myers et al., 2000) is a path such that and (i) or (ii) and ⁠. Its sequence is the sequence spelled from the path. Intuitively, a unitig is a maximal path on which adjacent vertices can be ‘unambiguously merged’ without affecting the connectivity of the original assembly graph.\n\nAs miniasm does not correct sequencing errors, the error rate of unitig sequence is the same as the error rate of the raw input reads. It is in theory possible to derive a better unitig sequence by taking the advantage of read overlaps. We have not implemented such a consensus tool yet.\n\nPairwise read mapping format (PAF) is a lightweight format keeping the key mapping information (Table 2). Minimap outputs mappings in PAF, which are taken by miniasm as input for assembly. We also provide scripts to convert DALIGNER, MHAP and SAM formats to PAF.\n\nGraphical fragment assembly format (GFA) is a concise assembly format (Table 3; http://bit.ly/gfaspec) initially proposed by us prior to miniasm and later improved by community (Melsted et al., personal communication). GFA has an explicit relationship to an assembly graph—an ‘S’ line in the GFA corresponds to a vertex and its complement in the graph; an ‘L’ line corresponds to an edge and its complement. GFA is able to represent graphs produced at all the stages of an assembly pipeline, from initial read overlaps to the unitig relationship in the final assembly.\n\nFASTG (http://bit.ly/fastgfmt) is another assembly format prior to GFA. It uses different terminologies. A vertex in an assembly graph is called an edge in FASTG, and an edge is called an adjacency. In FASTG, subgraphs can be nested, though no tools work with nested graphs due to technical complications. In addition, with nesting, one assembly graph can be represented in distinct ways, which we regard as a limitation of FASTG.\n\nMiniasm outputs the approximate positions of trimmed reads on the resulting unitigs. We extract these reads, map to the true assembly with minimap (option: ‘-L100 -m0 -w5’) and select the best mapping for each read. For a read i, let be the unitig name and be its index on (i.e. read i is the th read on the unitig). If two reads i and j are mapped adjacently on the true assembly, we say the adjacency is w-consistent, if (i) and ⁠, or (ii) both read i and j are the first or the last w reads of some unitigs. We use w = 5 to detect large structural misassemblies.\n\nWe mapped a human PacBio run ‘m130928_232712_42213_*.1.*’ (http://bit.ly/chm1p5c3) with minimap and BWA-MEM (Li, 2013) against GRCh37 plus decoy sequences (http://bit.ly/GRCh37d5). We started from 23 235 reads (131 Mb), filtered out 7593 reads (10 Mb) without ≥2 kb BWA-MEM alignments, and further dropped 815 reads (11 Mb) with two or more ≥2 kb chimeric alignments and 598 reads (4 Mb) with mapping quality below 10. Of the remaining reads, we found only 2.0% not overlapping the best minimap mapping of the same read. The majority of them hit to the decoy sequence where defining the true alignment is challenging as decoy is enriched with incomplete segments of centromeric repeats. If we exclude hits to the decoy, the percentage drops to 0.7%. On this input, minimap is 50 times faster than BWA-MEM, while finding similar best mapping positions. This experiment evaluates both the sensitivity and the specificity of minimap: if minimap had low sensitivity, it would miss the BWA-MEM mapping completely; if minimap had low specificity, its best mapping would often be a wrong mapping.\n\nTo test the sensitivity for read overlapping, we aligned all reads from PBcR-PB-ec (Table 4) against the reference genome with BWA-MEM, extracted reads with mapping quality ≥10, and identified ≥2kb overlaps between the extracted reads based on their positions on the reference genome. Minimap finds 93% of these overlaps. It is more sensitive than MHAP in its sensitive mode (78%) but less than DALIGNER (98%).\n\nWe evaluated the performance of miniasm on 17 bacterial datasets (Table 4) with command line ‘minimap -Sw5 -L100 -m0 reads.fa reads.fa miniasm -f reads.fa -’. Miniasm is able to derive a single contig per chromosome/plasmid for all but four datasets: 3 extra >50 kb contigs for ERS554120, and 1 extra contig for ERS605484, PBcR-ONT-ec and MAP-006-pcr-1 each. In the dotter plot between the assembly and the reference genome (similar to Fig. 2), no large-scale misassemblies are observed. We also applied the method in Section 2.6. Except ERS473430, the miniasm layouts are 5-consistent with the reference assemblies. For ERS473430, the NCTC project page claimed the sample has a plasmid. Miniasm gives two contigs, but the NCTC assembly has one contig only. The difference in layout may be an error in the NCTC assembly.\n\nWe have also run the PBcR pipeline (Berlin et al., 2015). PBcR requires a spec file. We took ‘pacbio.spec’ from the PBcR-PB-ec example and ‘oxford.spec’ from PBcR-ONT-ec, and applied them to all datasets based on their data types. MAP* datasets only provide FASTA sequences for download. We assigned quality 9 to all bases as PBcR requires base quality. PBcR assembled all PacBio datasets without extra contigs longer than 50 kb—better than miniasm. However, on the ONT datasets, PBcR produced more fragmented assemblies for MAP-006-2, MAP-006-pcr-1 and MAP-006-pcr-2; the PBcR-ONT-ec assembly is 300 kb shorter.\n\nWith four CPU cores, it took miniasm 14 s to assemble the 30-fold PBcR-PB-ec dataset and 2 minutes to assemble the 160-fold PB-ecoli dataset. PBcR, with four CPU cores, too, is about 700 times slower on PBcR-PB-ecoli and 60 times slower on PB-ecoli. It is slower on low-coverage data because PBcR automatically switches to the slower sensitive mode. Here we should remind readers that without an error correction stage, the contig sequences generated by miniasm are of much lower accuracy in comparison to PBcR. Nonetheless, miniasm is still tens of times faster than PBcR excluding the time spent on error correction.\n\nWe assembled a 45-fold C.elegans dataset (Table 4). With 16 CPU cores, miniasm assembled the data in 9 min, achieving an N50 size 2.8 Mb. From the dotter plot (Fig. 2), we observed three structural misassemblies (readers are advised to zoom into the vector graph to see the details). PacBio has assembled the same dataset with HGAP3 (Chin et al., 2013). HGAP3 produces shorter contigs (N50 = 1.6 Mb), but does not incur large-scale misassemblies visible from the dotter plot between the C.elegans reference genome and the contigs.\n\nWhen we take the C.elegans reference genome as the truth, the method in Section 2.6 also identifies the three structural misassemblies. The method additionally finds eight intra-unitig and one inter-unitig inconsistencies. In all cases, miniasm agrees with HGAP3, suggesting these inconsistencies may be true structural variations between the reference strain and the sequenced strain.\n\nWe have also tried PBcR on this dataset. Based on the intermediate progress report, we estimated that with 16 CPU cores, it would take a week or so to finish the assembly in the automatically chosen ‘sensitive’ mode.\n\nFor this dataset, minimap takes 27 GB RAM at the peak. As minimap loads 4 Gbp bases to index, the peak RAM will be capped around 27 GB. The memory used by miniasm is proportional to the number of overlaps. Although it only takes 1.3 GB RAM here, it will become the limiting factor for larger datasets.\n\nMiniasm also works with other overlappers when we convert their output format to PAF. On the 30-fold PBcR-PB-ec dataset, we are able to produce a single contig with DALIGNER (option -k15–h50), MHAP (option –pacbio-sensitive) and GraphMap (option -w owler). DALIGNER is the fastest, taking 65 s with four CPUs. Minimap is five times as fast on this dataset and is 18 times as fast on PB-ecoli at 160-fold. Minimap is faster on larger datasets possibly because without staging all possible hits in RAM, minimap is able to process more reads in a batch while a large batch usually helps performance. We should note that DALIGNER generates alignments while minimap does not. Minimap would probably have a similar performance if it included an alignment step.\n\nMiniasm implements the ‘O’ and ‘L’ steps in the Overlap-Layout-Consensus (OLC) assembly paradigm. It confirms long noisy reads can be assembled without an error correction stage, and without this stage, the assembly process can be greatly accelerated and simplified, while achieving comparable contiguity and large-scale accuracy to existing pipelines, at least for genomes without excessive repetitive sequences. Although without the ‘C’ step, miniasm cannot produce high-quality consensus for many analyses, it opens the door to ultrafast assembly if we can develop a fast consensus tool matching the speed of minimap and miniasm. In addition, MinION has a ‘read-until’ mode, allowing users to pause sequencing and reload samples. Fast layout by miniasm could already help to decide if enough data have been collected.\n\nOur main concern with miniasm is that when we look at a low-identity match between two noisy reads, it is difficult to tell whether the low identity is caused by the stochastically higher base error rate on reads, or because reads come from two recent segmental duplications. In comparison, error correction takes the advantage of multiple reads and in theory has more power to distinguish high error rate from duplications/repeats. Bacteria and C.elegans evaluated in this article are repeat sparse. We are yet to know the performance of miniasm given repeat-rich genomes. In addition, miniasm has not been optimized for large repeat-rich genomes. It reads all hits into RAM, which may not be practical when there are too many. We need to filter repetitive hits, introduce disk-based algorithms (e.g. for sorting) or stream hits before removing contained reads. Working with large complex genomes will be an important future direction.\n\nOxford Nanopore is working on PromethION and PacBio will ship PacBio Sequel later this year. Both sequencers promise significantly reduced sequencing cost and increased throughput, which may stimulate the adoption of long-read sequencing and subsequently the development of long-read mappers and assemblers. We hope in this process, the community could standardize the input and output formats of various tools, so that a developer could focus on a component he or she understands best. Such a modular approach has been proved to be fruitful in the development of short-read tools—in fact, the best short-read pipelines all consist of components developed by different groups—and will be equally beneficial to the future development of long-read mappers and assemblers.\n\nWe thank Páll Melsted for maintaining the GFA spec and are grateful to Gene Myers, Jason Chin, Adam Phillippy, Jared Simpson, Zamin Iqbal, Nick Loman and Ivan Sovic for their presentations, talks, comments on social media and unpublished works which have greatly influenced and helped the development of minimap and miniasm."
    },
    {
        "link": "https://bv-brc.org/docs/quick_references/services/genome_assembly_service.html",
        "document": "The bacterial Genome Assembly Service allows single or multiple assemblers to be invoked to compare results. Several assembly workflows or “strategies” are available that have been tuned to fit certain data types or desired analysis criteria such as throughput or rigor. Once the assembly process has started by clicking the Assemble button, the genome is queued as a “job” for the Assembly Service to process, and will increment the count in the Jobs information box on the bottom right of the page. Once the assembly job has successfully completed, the output file will appear in the workspace, available for use in the BV-BRC comparative tools and downloaded if desired.\n\nRead File 1 & 2: Many paired read libraries are given as file pairs, with each file containing half of each read pair. Paired read files are expected to be sorted such that each read in a pair occurs in the same Nth position as its mate in their respective files. These files are specified as READ FILE 1 and READ FILE 2. For a given file pair, the selection of which file is READ 1 and which is READ 2 does not matter. Read File: The fastq file containing the reads. Allows direct upload of read files from the NCBI Sequence Read Archive to the BV-BRC Assembly Service. Entering the SRR accession number and clicking the arrow will add the file to the selected libraries box for use in the assembly.\n• None Auto - Will use Canu if only long reads are submitted. If long and short reads, as or short reads alone are submitted, Unicycler is selected.\n• None Unicycler - Can assemble Illumina-only read sets where it functions as a SPAdes-optimizer. It can also assembly long-read-only sets (PacBio or Nanopore) where it runs a miniasm plus Racon pipeline. For the best possible assemblies, give it both Illumina reads and long reads, and it will conduct a hybrid assembly.\n• None SPAdes - Designed to assemble small genomes, such as those from bacteria, and uses a multi-sized De Bruijn graph to guide assembly.\n• None Canu - Long-read assembler which works on both third and fourth generation reads. It is a successor of the old Celera Assembler that is specifically designed for noisy single-molecule sequences. It supports nanopore sequencing, halves depth-of-coverage requirements, and improves assembly continuity. It was designed for high-noise single-molecule sequencing (such as the PacBio RS II/Sequel or Oxford Nanopore MinION).\n• None metaSPAdes - Combines new algorithmic ideas with proven solutions from the SPAdes toolkit to address various challenges of metagenomic assembly.\n• None plasmidSPAdes - For assembling plasmids from whole genome sequencing data and benchmark its performance on a diverse set of bacterial genomes.\n• None MDA (single-cell) - A new assembler for both single-cell and standard (multicell) assembly, and it improves on the recently released E+V−SC assembler (specialized for single-cell data). Output Folder: The workspace folder where results will be placed. Output Name: User-provided name used to uniquely identify results. Benchmark Contigs: This optional parameter can be used to specify a FASTA contigs file to evaluate the assembly against. Racon iternations and Pilon iterations: Correct assembly errors (or “polish”) using racon and/or Pilon. Both racon and Pilon take the contigs and the reads mapped to those contigs, and look for discrepancies between the assembly and the majority of the reads. Where there is a discrepancy, racon or pilon will correct the assembly if the majority of the reads call for that. Racon is for long reads (PacBio or Nanopore) and Pilon is for shorter reads (Illumina or Ion Torrent). Once the assembly has been corrected with the reads, it is still possible to do another iteration to further improve the assembly, but each one takes time. Minimal output contig coverage: Filter out contigs with low read depth in final assembly\n\nThe Genome Assembly Service generates several files that are deposited in the Private Workspace in the designated Output Folder. These include\n• None assembly_graph.gfa. - File used to generate the assembly graph plot.\n• None assembly_report.html - Web-viewable report of the assembly including information about the submitted reads and assembly process used.\n• None p3_assembly.log - Log file providing steps used in the assembly.\n• None quast_report.html - Web-viewable Quast-generated report providing evaluation information, summary tables, and plots regarding the assembly.\n• None run_details.json - Json-formatted file containing information about the assembly process. After selecting one of the output files by clicking it, a set of options becomes available in the vertical green Action Bar on the right side of the table. These include\n• None Guide: Link to the corresponding Quick Reference Guide.\n• None View: Displays the content of the file, typically as plain text or rendered html, depending on filetype.\n• None Rename: Allows renaming of the file.\n• None Copy: Copies the selected items to the clipboard.\n• None Move: Allows moving of the file to another folder.\n• None Edit Type: Allows changing of the type of the file in terms of how BV-BRC interprets the content and uses it in other services or parts of the website. Allowable types include unspecified, contigs, nwk, reads, differential expression input data, and differential expression input metadata. More details are available in the Action Bar Quick Reference Guide."
    },
    {
        "link": "https://canu.readthedocs.io/en/latest/quick-start.html",
        "document": "Canu specializes in assembling PacBio or Oxford Nanopore sequences. Canu operates in three phases: correction, trimming and assembly. The correction phase will improve the accuracy of bases in reads. The trimming phase will trim reads to the portion that appears to be high-quality sequence, removing suspicious regions such as remaining SMRTbell adapter. The assembly phase will order the reads into contigs, generate consensus sequences and create graphs of alternate paths.\n\nFor eukaryotic genomes, coverage more than 20x is enough to outperform current hybrid methods, however, between 30x and 60x coverage is the recommended minimum. More coverage will let Canu use longer reads for assembly, which will result in better assemblies.\n\nInput sequences can be FASTA or FASTQ format, uncompressed or compressed with gzip (.gz), bzip2 (.bz2) or xz (.xz). Note that zip files (.zip) are not supported.\n\nCanu can resume incomplete assemblies, allowing for recovery from system outages or other abnormal terminations. On each restart of Canu, it will examine the files in the assembly directory to decide what to do next. For example, if all but two overlap tasks have finished, only the two that are missing will be computed. For best results, do not change Canu parameters between restarts.\n\nCanu will auto-detect computational resources and scale itself to fit, using all of the resources available and are reasonable for the size of your assembly. Memory and processors can be explicitly limited with with parameters maxMemory and maxThreads. See section Execution Configuration for more details.\n\nCanu will automatically take full advantage of any LSF/PBS/PBSPro/Torque/Slrum/SGE grid available, even submitting itself for execution. Canu makes heavy use of array jobs and requires job submission from compute nodes, which are sometimes not available or allowed. Canu option will restrict Canu to using only the current machine, while option will configure Canu for grid execution but not submit jobs to the grid. See section Execution Configuration for more details.\n\nThe Canu Tutorial has more background, and the Canu FAQ has a wealth of practical advice.\n\nPacific Biosciences released P6-C4 chemistry reads for Escherichia coli K12. You can download them from their original release, but note that you must have the SMRTpipe software installed to extract the reads as FASTQ. Instead, use a FASTQ format 25X subset (223MB). Download from the command line with: There doesn’t appear to be any “official” Oxford Nanopore sample data, but the Albertsen Lab released a run, also for Escherichia coli K12. Download the R10 data from FigShare By default, Canu will correct the reads, then trim the reads, then assemble the reads to unitigs. Canu needs to know the approximate genome size (so it can determine coverage in the input reads) and the technology used to generate the reads. Output and intermediate files will be in directories ‘ecoli-pacbio’ and ‘ecoli-nanopore’, respectively. Intermediate files are written in directories ‘correction’, ‘trimming’ and ‘unitigging’ for the respective stages. Output files are named using the ‘-p’ prefix, such as ‘ecoli.contigs.fasta’, ‘ecoli.unitigs.gfa’, etc. See section Outputs for more details on outputs (intermediate files aren’t documented).\n\nCanu has support for using parental short-read sequencing to classify and bin the F1 reads (see Trio Binning manuscript for details). This example demonstrates the functionality using a synthetic mix of two Escherichia coli datasets. First download the data The run will first bin the reads into the haplotypes ( ) and provide a summary of the classification in : Next, the haplotypes are assembled in and . By default, if the unassigned bases are > 5% of the total, they are included in both haplotypes. This can be controlled with the hapUnknownFraction option. As comparison, you can try co-assembling the datasets instead: Please note, trio binning is designed to work with raw sequences prior to correction. Do not correct the reads together and then run trio-binning, this will not work and Canu will give an error. Trio binning does not yet support inputting PacBio HiFi reads for binning as they get flagged as “corrected” and the same error as above is given. As a workaround, run specifying the HiFi reads as -pacbio-raw. This will bin the data and create shell scripts to start the assembly. Edit the shell scripts to replace -pacbio-raw with -pacbio-corrected or -pacbio-hifi and run the assemblies manually.\n\nCanu can use reads from any number of input files, which can be a mix of formats and technologies. Note that current combining PacBio HiFi data with other datatypes it not supported. We’ll assemble a mix of 10X PacBio CLR reads in two FASTQ files and 10X of Nanopore reads in one FASTA file: Sometimes, however, it makes sense to do the three top-level tasks by hand. This would allow trying multiple unitig construction parameters on the same set of corrected and trimmed reads, or skipping trimming and assembly if you only want corrected reads. We’ll use the PacBio reads from above. First, correct the raw reads: Then, trim the output of the correction: And finally, assemble the output of trimming, twice, with different stringency on which overlaps to use (see correctedErrorRate): Note that the assembly stages use different ‘-d’ directories. It is not possible to run multiple copies of canu with the same work directory. You can also try uncorrected ONT assembly which works for higher quality (95% accuracy) data, though this mode should be considered experimental:"
    },
    {
        "link": "https://pmc.ncbi.nlm.nih.gov/articles/PMC5411767",
        "document": "The goal of genome assembly is to reconstruct a complete genome from many comparatively short sequencing reads. Overlapping reads that originate from the same region of the genome can be joined together to form contigs, but genomic repeats longer than the overlap length lead to ambiguous reconstructions and fragment the assembly (Phillippy et al. 2008; Nagarajan and Pop 2009). There are two strategies for overcoming this fundamental limitation: increasing the effective read length, and separating nonexact repeats based on copy-specific variants. Recently, single-molecule sequencing has revolutionized assembly by producing reads >10 kbp (Gordon et al. 2016), which has significantly reduced the number of unresolvable repeats (Koren et al. 2012) and enabled the complete assembly of microbial genomes (Chin et al. 2013; Koren et al. 2013; Koren and Phillippy 2014). These long reads also aid assembly phasing (Chin et al. 2016), where the conserved alleles in a diploid, polyploid, or meta-genome can be thought of as a special kind of repeat. However, in contrast to improved read length, single-molecule sequencing is less accurate than past technologies (Eid et al. 2009; Schneider and Dekker 2012), requiring sensitive alignment methods and limiting the discrimination of divergent alleles and nonexact repeats. Nevertheless, PacBio single-molecule real-time (SMRT) sequencing exhibits a largely unbiased and random error model (Ross et al. 2013), enabling assemblies that exceed short-read data both in terms of quality and continuity (Chin et al. 2013; Koren et al. 2013). Oxford Nanopore strand sequencing can also produce highly continuous assemblies, but current biases in base calling prohibit an accurate consensus sequence without the addition of complementary data (Loman et al. 2015).\n\nThe increased read length and error rate of single-molecule sequencing has challenged genome assembly programs originally designed for shorter, highly accurate reads. Several new approaches have been developed to address this, roughly categorized as hybrid, hierarchical, or direct (for a review, see Koren and Phillippy 2014). Hybrid methods use single-molecule reads to reconstruct the long-range structure of the genome, but rely on complementary short reads for accurate base calls (Koren et al. 2012; Hackl et al. 2014; Lee et al. 2014; Salmela and Rivals 2014; Antipov et al. 2016; Ye et al. 2016). Hierarchical methods do not require a secondary technology and instead use multiple rounds of read overlapping (alignment) and correction to improve the quality of the single-molecule reads prior to assembly (Chin et al. 2013; Koren et al. 2013). Finally, direct methods attempt to assemble single-molecule reads from a single overlapping step without any prior correction (Li 2016; Tørresen et al. 2017). All three approaches are capable of producing an accurate final assembly. However, our goal is the complete reconstruction of entire genomes, so we focus here on the hierarchical strategy because it has produced the most continuous de novo assemblies to date (Berlin et al. 2015; Chakraborty et al. 2016).\n\nCanu is able to generate highly continuous assemblies from both PacBio and Nanopore sequencing, but signal-level polishing is required to maximize the final consensus accuracy. Such algorithms use statistical models of the sequencing process to predict base calls directly from the raw instrument data, which is a richer source of information than FASTQ Phred quality values. Currently, a PacBio base accuracy of 99.999% (QV50) is achievable with Quiver polishing (Chin et al. 2013; Koren et al. 2013), but Nanopore is limited to at most 99.9% (QV30) with Nanopolish (Loman et al. 2015) due to systematic sequencing errors (Goodwin et al. 2015). Both tools are technology specific and must be trained on each new chemistry, so future improvements are possible. Alternatively, complementary short-read sequencing can be used for consensus polishing with Pilon. On recent Nanopore sequencing data, Illumina-polished Canu assemblies can reach QV50 and exceed the base accuracy of hybrid SPAdes assemblies. Thus, the combination of Nanopore and Illumina sequencing provides a new alternative for the generation of finished microbial genomes. However, due to the difficulty of mapping short Illumina reads to repeats, signal-polished PacBio assemblies currently deliver the highest overall quality. Canu assembly followed by either single-molecule or short-read polishing is an efficient method for generating high-quality assemblies. Our results indicate that while Miniasm (Li 2016) can rapidly produce continuous and structurally accurate assemblies, the multiple rounds of polishing needed to produce an accurate consensus sequence becomes a computational bottleneck. Additionally, Canu is the only tool capable of assembling low-accuracy 1D Nanopore data while scaling to gigabase-sized genomes—an important application given the pending release of high-throughput Nanopore sequencers. Combined with Canu's adaptive k-mer weighting strategy, the assembly of repetitive heterochromatic sequence may be possible with high-coverage, long-read nanopore sequencing. Canu currently splits haplotypes into separate contigs wherever the allelic divergence is greater than the post-correction overlap error rate. This threshold is typically 1.5% for recent PacBio data. This splitting results in an assembly size larger than the haploid genome size. Although these regions are kept separate in the assembly graph, no effort is currently made to annotate such regions or phase multiple bubbles into larger haplotype blocks. Less diverged haplotypes, such as human, are collapsed, as demonstrated by the HX1 data set. Currently, only abundance is considered for k-mer weighting, which avoids the consideration of false, repetitive overlaps. However, this same scheme could be used to improve the discrimination of minor variants between repeats and haplotypes by preferring haplotype-specific k-mers during sketch construction. This would increase the power of Canu's statistical overlap filter, which prevents the merging of diverged repeats and haplotypes. For further improved haplotype reconstruction, it would be possible to apply an approach like FALCON-Unzip (Chin et al. 2016) to the Canu assembly graph to generate phased contigs based on linked variants identified within the single-molecule reads. For repeat structures, the current algorithm can resolve any repeat copy with more divergence than the post-correction overlap error rate. In the future, similar repeats could be resolved using more sophisticated graph traversals. For example, if one copy of a two-copy repeat is spanned, a correct reconstruction of the unspanned copy can be inferred given that the other copy is correctly assembled (Ukkonen 1992). Alternatively, secondary information from technologies like 10x Genomics (Zheng et al. 2016) or Hi-C (Selvaraj et al. 2013) could be used to guide walks through the Canu graph. Ultimately, because Hi-C provides megabase-scale linkage information, the integration of this technology with Canu assembly graphs could lead to complete de novo assemblies that span entire mammalian chromosomes from telomere to telomere, as was recently demonstrated for the domestic goat genome (Bickhart et al. 2016)."
    },
    {
        "link": "https://canu.readthedocs.io/en/latest/tutorial.html",
        "document": "Canu assembles reads from PacBio RS II or Oxford Nanopore MinION instruments into uniquely-assemblable contigs, unitigs. Canu owes lots of it design and code to Celera Assembler.\n\nCanu can be run using hardware of nearly any shape or size, anywhere from laptops to computational grids with thousands of nodes. Obviously, larger assemblies will take a long time to compute on laptops, and smaller assemblies can’t take advantage of hundreds of nodes, so what is being assembled plays some part in determining what hardware can be effectively used.\n\nMost algorithms in canu have been multi-threaded (to use all the cores on a single node), parallelized (to use all the nodes in a grid), or both (all the cores on all the nodes).\n\nThe canu command is the ‘executive’ program that runs all modules of the assembler. It oversees each of the three top-level tasks (correction, trimming, unitig construction), each of which consists of many steps. Canu ensures that input files for each step exist, that each step successfully finished, and that the output for each step exists. It does minor bits of processing, such as reformatting files, but generally just executes other programs. The -p option, to set the file name prefix of intermediate and output files, is mandatory. If -d is not supplied, canu will run in the current directory, otherwise, Canu will create the and run in that directory. It is _not_ possible to run two different assemblies in the same directory. The -s option will import a list of parameters from the supplied specification (‘spec’) file. These parameters will be applied before any from the command line are used, providing a method for setting commonly used parameters, but overriding them for specific assemblies. By default, all needed top-level tasks are performed (-pacbio and -nanopore are assumed to be raw and untrimmed while -pacbio-hifi are assumed to be corrected and trimmed). It is possible to run exactly one task by specifying your read characteristics and a step name. These options can be useful if you want to correct reads once and try many different assemblies. We do exactly that in the Canu Quick Start. Additionally, suppling pre-corrected reads with -pacbio -corrected or -nanopore -corrected will run only the trimming and assembling stages. Specifying reads as -corrected -untrimmed will run only the assembly step. Parameters are key=value pairs that configure the assembler. They set run time parameters (e.g., memory, threads, grid), algorithmic parameters (e.g., error rates, trimming aggressiveness), and enable or disable entire processing steps (e.g., don’t correct errors, don’t search for subreads). They are described later. One parameter is required: the genomeSize (in bases, with common SI prefixes allowed, for example, 4.7m or 2.8g; see genomeSize). Parameters are listed in the Canu Parameter Reference, but the common ones are described in this document. Reads are supplied to canu by options that options that describe how the reads were generated, and what level of quality they are, for example, -pacbio-raw indicates the reads were generated on a PacBio RS II instrument, and have had no processing done to them. Each file of reads supplied this way becomes a ‘library’ of reads. The reads should have been (physically) generated all at the same time using the same steps, but perhaps sequenced in multiple batches. In canu, each library has a set of options setting various algorithmic parameters, for example, how aggressively to trim. To explicitly set library parameters, a text ‘gkp’ file describing the library and the input files must be created. Don’t worry too much about this yet, it’s an advanced feature, fully described in Section gkp-files. The read-files contain sequence data in either FASTA or FASTQ format (or both! A quirk of the implementation allows files that contain both FASTA and FASTQ format reads). The files can be uncompressed, gzip, bzip2 or xz compressed. We’ve found that “gzip -1” provides good compression that is fast to both compress and decompress. For ‘archival’ purposes, we use “xz -9”.\n\nThere are two modes that canu runs in: locally, using just one machine, or grid-enabled, using multiple hosts managed by a grid engine. LSF, PBS/Torque, PBSPro, Sun Grid Engine (and derivations), and Slurm are supported, though LSF has had limited testing. Section Grid Engine Configuration has a few hints on how to set up a new grid engine. By default, if a grid is detected the canu pipeline will immediately submit itself to the grid and run entirely under grid control. If no grid is detected, or if option is set, canu will run on the local machine. In both cases, Canu will auto-detect available resources and configure job sizes based on the resources and genome size you’re assembling. Thus, most users should be able to run the command without modifying the defaults. Some advanced options are outlined below. Each stage has the same five configuration options, and tags are used to specialize the option to a specific stage. The options are: Run this stage on the grid, usually in parallel. Supply this string to the grid submit command. Use this many gigabytes of memory, per process. Use this many compute threads per process. If not on the grid, run this many jobs at the same time. Global grid options, applied to every job submitted to the grid, can be set with ‘gridOptions’. This can be used to add accounting information or access credentials. A name can be associated with this compute using ‘gridOptionsJobName’. Canu will work just fine with no name set, but if multiple canu assemblies are running at the same time, they will tend to wait for each others jobs to finish. For example, if two assemblies are running, at some point both will have overlap jobs running. Each assembly will be waiting for all jobs named ‘ovl_asm’ to finish. Had the assemblies specified job names, gridOptionsJobName=apple and gridOptionsJobName=orange, then one would be waiting for jobs named ‘ovl_asm_apple’, and the other would be waiting for jobs named ‘ovl_asm_orange’.\n\nCanu expects all error rates to be reported as fraction error, not as percent error. We’re not sure exactly why this is so. Previously, it used a mix of fraction error and percent error (or both!), and was a little confusing. Here’s a handy table you can print out that converts between fraction error and percent error. Not all values are shown (it’d be quite a large table) but we have every confidence you can figure out the missing values: Canu error rates always refer to the percent difference in an alignment of two reads, not the percent error in a single read, and not the amount of variation in your reads. These error rates are used in two different ways: they are used to limit what overlaps are generated, e.g., don’t compute overlaps that have more than 5% difference; and they are used to tell algorithms what overlaps to use, e.g., even though overlaps were computed to 5% difference, don’t trust any above 3% difference. There are seven error rates. Three error rates control overlap creation (corOvlErrorRate, obtOvlErrorRate and utgOvlErrorRate), and four error rates control algorithms (corErrorRate, obtErrorRate, utgErrorRate, cnsErrorRate). The three error rates for overlap creation apply to the overlap algorithm and the reAlign option used to generate alignments from or overlaps. Since is used for generating correction overlaps, the corOvlErrorRate parameter is not used by default. Overlaps for trimming and assembling use the algorithm, therefore, obtOvlErrorRate and utgOvlErrorRate are used. The four algoriothm error rates are used to select which overlaps can be used for correcting reads (corErrorRate); which overlaps can be used for trimming reads (obtErrorRate); which overlaps can be used for assembling reads (utgErrorRate). The last error rate, cnsErrorRate, tells the consensus algorithm to not trust read alignments above that value. For convenience, two meta options set the error rates used with uncorrected reads (rawErrorRate) or used with corrected reads. (correctedErrorRate). The default depends on the type of read being assembled. In practice, only correctedErrorRate is usually changed. The Canu FAQ has specific suggestions on when to change this. Canu v1.4 and earlier used the errorRate parameter, which set the expected rate of error in a single corrected read.\n\nAs Canu runs, it outputs status messages, execution logs, and some analysis to the console. Most of the analysis is captured in as well. Most of the analysis reported during assembly. This will report the histogram of read lengths, the histogram or k-mers in the raw and corrected reads, the summary of corrected data, summary of overlaps, and the summary of contig lengths. You can use the k-mer corrected read histograms with tools like GenomeScope to estimate heterozygosity and genome size. In particular, histograms with more than 1 peak likely indicate a heterozygous genome. See the Canu FAQ for some suggested parameters. The corrected read report gives a summary of the fate of all input reads. The first part:: reports the fraction of reads which had an overlap. In this case, the majority had at least one overlap, which is good. Next: reports that a total of 92.8x of raw bases are candidates for correction. By default, Canu only selects the longest 40x for correction. In this case, it selects 43.2x of raw read data which it estimates will result in 40x correction. Not all raw reads survive full-length through correction: The rescued reads are those which would not have contributed to the correction of the selected longest 40x subset. These could be short plasmids, mitochondria, etc. Canu includes them even though they’re too short by the 40x cutoff to avoid losing sequence during assembly. Lastly: are the reads which were deemed too short to correct. If you increase , you could get up to 41x more corrected sequence. However, unless the genome is very heterozygous, this does not typically improve the assembly and increases the running time. The assembly read error report summarizes how unitigging was run:: Canu selects multiple error rate thresholds and selects the most appropriate one based on how many reads end up without overlaps at each threshold. In this case, it used 0.001% or 1 error in 10 kbp after considering 1.0% and 0.0043%. The assembly statistics (NG50, etc) are reported before and after consensus calling. Note that for HiFi data, the pre-consensus statistics are in homopolymer-compressed space. Everything which could be assembled and is the full assembly, including both unique, repetitive, and bubble elements. Reads and low-coverage contigs which could not be incorporated into the primary assembly. The header line for each sequence provides some metadata on the sequence.: Canu versions prior to v1.9 created a GFA of the contig graph. However, as noted at the time, the GFA format cannot represent partial overlaps between contigs (for more details see the discussion of general edges on the GFA2 page). Because Canu contigs are not compatible with the GFA format, <prefix>.contigs.gfa has been removed. Prior to Canu v2.1, contigs split at overlap junctions were output as unitigs. However, these graphs often would be missing edges and be over-fragmented (split where there is no ambiguity). Thus <prefix>.unitigs.fasta and <prefix.unitigs.gfa have been removed. The layout provides information on where each read ended up in the final assembly, including contig and positions. It also includes the consensus sequence for each contig. The position of each read in a contig. In this case read ids 677083 and 2343812 ended up in tig00000004 and the coordinates are listed at the end (read 2343812 is reverse-complemented). You need to do a bit of work to get the original id of 2343812, look in the gkpStore/readNames.txt file, there you should find: which gives you the original read (PacBio in this case) id. A list of the contigs, lengths, coverage, number of reads and other metadata. Essentially the same information provided in the FASTA header line."
    },
    {
        "link": "https://academic.oup.com/bib/article/24/6/bbad337/7291993",
        "document": "The advent of third-generation sequencing technologies, represented by Pacific Biosciences (PacBio) and Oxford Nanopore Technologies (Nanopore), has allowed for very long de novo assemblies of complex genomes, including that of eukaryotic diploids [1]. These technologies produce long nucleotide sequence reads by reading long single-molecule nucleic acids [2]. According to the manufacturers’ websites, the average read length and total amount of the read length for PacBio (Sequel, SMRT Cell 1 M) are 30 kb and 20 Gb, respectively, while those for Nanopore (MinION) are up to 4 Mb and 50 Gb, respectively. Therefore, they have been used for de novo assembly of challenging genomes with high heterozygosity [3], repetitive regions [4], large size [5] or high ploidy [6]. However, they produce higher sequencing error rates (5–15%) [7] than Illumina short reads (0.3%) [8]. Hence, most recent assemblers have applied the hierarchical approach to correct long-read errors by detecting sequence errors from alignments between long-reads of the same sample before assembly [9]. Even after long-read-based assembly, the assembled sequences are further polished with Illumina short reads [3, 10–13]. Several polishing tools, including Pilon, POLCA and NextPolish, correct sequence errors in the assembled sequences with long reads and/or Illumina short reads [14–16]. Recently, PacBio offered a high-fidelity (HiFi) read technology, which produces an average 13.5 kb [17] of highly accurate (99.9%) reads (https://www.pacb.com/technology/hifi-sequencing/). Consequently, more de novo genome assemblers for the HiFi reads were developed [18, 19]. However, since HiFi sequencing is costlier than standard long-read sequencing, genome assembly using conventional long-read technology is preferred [20–22].\n\nDe novo assemblers for long reads are classified into long-read-only assemblers (e.g. Canu [23], Flye [24], miniasm [25], NextDenovo (https://github.com/Nextomics/NextDenovo) and Redbean [26]) and hybrid assemblers that use short and long reads (e.g. HASLR [27], MaSuRCA [28], Platanus-allee [29], SPAdes [30] and WENGAN [31]). Hybrid assemblers use two methods: (i) First correct long reads with short reads and then assemble with the corrected long reads (e.g. MaSuRCA) and (ii) Assemble short reads into contigs and then construct the scaffolds from the generated contigs with long reads (e.g. HASLR, Platanus-allee, SPAdes and WENGAN). However, obtaining the best-assembled sequences utilizing all the assemblers is challenging owing to the limited computational resources and time spent for analysing, despite the use of cluster servers.\n\nAlthough de novo assemblers have been developed for diploids, including FALCON Unzip [32], Canu, Platanus-allee and Platanus [33], diploid genome assembly remains challenging. The contig set from de novo assembly, composed of one sequence pattern between homologous chromosomes and the sequence of the hemizygous region, is the desired haploid representation. However, highly heterozygous regions are not recognized as homologous regions of two chromosomes by any assemblers and are thus assembled separately. Therefore, the assembly size will be larger than the actual genome size [34]. Some tools that distinguish allelic sequences (haplotigs) from homologous regions have been developed to solve this problem, including Purge Haplotig [34], purge_dups [35] and HaploMerger2 [36].\n\nWe evaluated assemblers based on computer resource usage (execution time and memory usage), continuity, and completeness using six genomes with various heterozygosity levels and proposed a concrete guideline for the construction of haplotype representation according to the degree of heterozygosity. The optimal genome-assembler combination is influenced by heterozygosity, repeats, genome size, as well as research purposes. Therefore, our guidelines are intended to help users select and further modify the best method to suit their genome characteristics and research purposes.\n\nSince we focused on how heterozygosity affects genome assembly, we collected sets of genomes with varying levels of heterozygosity. The six available genomes of Arabidopsis thaliana accession C24 [10], Nitzschia putrida strain NIES-4239 [37], Lates calcarifer [38], Solanum sitiens [11], A. thaliana F1 cross of Col-0 and Cvi-0 [32] and Crassostrea gigas [3] were used. Their PacBio subreads and Illumina paired-end data of whole-genome sequencing were downloaded from the public archive Sequence Read Archive of European Nucleotide Archive (ENA) [39] (Table 1). The sequencing data of the six genomes have PacBio coverage ≥39x and Illumina coverage ≥60x. For the L. calcarifer PacBio subreads, 223x out of 247x were retrieved since they are sufficient to gain PacBio coverage >39x.\n\nThe stepwise analytical processes for the construction of haploid representation comprised four processes: estimation of genome characteristics (such as genome size, heterozygosity, and repeat rate), de novo assembly, polishing, and purging of haplotigs (Figure 1). First, the genome characteristics were estimated by obtaining the k-mer counts through Jellyfish v2.2.10 [40] (-C -m 21 -s 1000000000) and using them to estimate the genome characteristics through GenomeScope [41] (k-mer_length = 21 and kmermax = 1000). The estimated genome characteristics were assessed by comparing the genome size with that in the original research.\n\nDe novo assembly was then conducted. Illumina paired-end reads used for the hybrid assembler were trimmed using fastp v0.20.0 [42] (—cut_front —cut_tail option). Thereafter, the assembled sequences that were > 500 b in length were extracted using seqkit v0.15.0 [43]. To polish the extracted assembled sequences, Illumina paired-end reads trimmed in the previous step were mapped to the assembled sequences using bwa v2.2.1 [44], and the result was determined using Pilon v1.24 [14]. The manipulation of SAM/BAM files was performed using SAMtools v1.11 [45]. Then, the haplotigs were removed from the polished sequences through Purge Haplotigs v1.1.1 [34]. The read-depth cut-off parameters for the ‘purge’ command of Purge Haplotigs comprised three types, i.e. ‘low cut-off’ for removing assembly artefacts, ‘midpoint’ for specifying between the haploid and diploid peaks, and ‘high cut-off’ for removing repeats and organelles. These were set (Supplementary Table S1) by referring to the histograms of read-depth to contigs (Supplementary Figure S1) generated using the ‘hist’ command of Purge Haplotigs. Purge Haplotigs outputs three types of FASTA format files: haploid representation (primary sequences), haplotigs and artefacts (comprising assembly artefacts, organelle genome and repeat sequences). The details of the execution commands and configuration options of each tool during the de novo assembly and thereafter are described in the Supplementary Methods. The graph of cumulative length and Nx statistics for contigs were generated using R ver. 4.0.5. Assembly ploidy, a metric that estimates the proportion of allelic sequences present in the assembled genome sequences, was calculated by dividing the total length by the estimated genome size [8].\n\nLong-read-only assemblers were Canu (v2.1.1) [23], Flye (v2.8.3) [24], miniasm (v0.3-r179) [25], NextDenovo (v2.4.0) (https://github.com/Nextomics/NextDenovo), and Redbean (v2.5) [26] with PacBio long reads. Hybrid assemblers were HASLR (v0.8a1) [27], MaSuRCA (v4.0.1) [28], Platanus-allee (v2.2.2) [29], SPAdes (v3.15) [30] and WENGAN (v0.2) [31] with both PacBio long reads and Illumina paired-end reads. MaSuRCA and WENGAN assemble using external de novo assemblers. We benchmarked using MaSuRCA_C (CABOG [46]), MaSuRCA_F (Flye) and WENGAN-M (MINIA3 [47]). These long-read-only and hybrid assemblers are summarized in the Supplementary Notes. The assemblers that finished successfully among all six genomes were selected for evaluation. If the execution time exceeded 500 h, it was evaluated as ‘time out’ and not subjected to subsequent analysis or comparison, regardless of the final result.\n\nPlatanus-allee generates phased sequences and consensus sequences that are not phased. The other assemblers generate consensus sequences. Herein, the consensus sequences were consistently used for comparison among assemblers. Additionally, Flye, MaSuRCA_F and Platanus-allee output scaffolds, whereas SPAdes and MaSuRCA_C output scaffolds and/or contigs. The other assemblers output contigs but not scaffolds. Therefore, for comparison, we utilized scaffolds from Flye, MaSuRCA_F, Platanus-allee, SPAdes and MaSuRCA_C, as well as contigs from the other tools. Hereafter, both scaffolds and contigs are referred to as ‘contigs’ without distinction.\n\nThe resultant contigs were evaluated based on continuity and completeness. Contig continuity was evaluated by N50, which is the contig length when 50% of the total contig size is reached, while the assembled contig lengths were added in the longest order. The statistics of assembled contigs were calculated using assembly-stats v1.0.1 (https://github.com/rjchallis/assembly-stats) and Merqury v1.3 [48]. The completeness was measured using Benchmarking Universal Single-Copy Orthologs (BUSCO) analysis ver. 5.0.0 [49]. BUSCO databases used ‘embryophyta_odb10’ for A. thaliana C24 and F1, ‘eukaryota_odb10’ for N. putrida NIES-4239, ‘actinopterygii_odb10’ for L. calcarifer, ‘solanales_odb10’ for S. sitiens, and ‘metazoa_odb10’ for C. gigas. The N50 and BUSCO completeness of polished contigs were ranked for each genome as the best to third best and the worst to third worst. The worst ranking for BUSCO completeness was only used for the percentage of BUSCO completeness, including ‘single-copy’ and ‘duplicated’ if there was a difference of 15 or more from the top value in each genome. The assembler that reached the timeout was ranked as the worst for both N50 and BUSCO completeness. The continuity and completeness scores for each tool were calculated separately for heterozygosity <1 and heterozygosity ≥1 by adding 3 to the best, 2 to second best, 1 to third best, −3 to worst, −2 to second worst, and − 1 to third worst. Based on these scores, the performance of BUSCO completeness and N50 for each assembler was classified as ‘high’, ‘medium’ or ‘low’. The thresholds for each classification were as follows. For heterozygosity <1: high ≥5, 4 ≥ medium ≥ −4 and low ≤ −5 for N50; high ≥5, 4 ≥ medium ≥1 and low ≤0 for BUSCO. For heterozygosity ≥1: high ≥3, 2 ≥ medium ≥ −2, and low ≤ −3 for N50; high ≥4, 3 ≥ medium ≥0 and low ≤ −1 for BUSCO. These classifications of the assemblers by computational resource usage, N50 and BUSCO completeness were used to select the assemblers in the guideline adapted to the degree of heterozygosity. Among the assemblers with similar evaluation, the assembler with the most stable performance for N50 and BUSCO completeness was adopted.\n\nTo measure computational usage under the same conditions, all de novo assemblers were utilized on the National Institute of Genetics supercomputer system medium nodes with 10 CPU cores (CPU: Intel Xeon Gold 6148 × 4, 80 core/node). To measure wall-clock time and memory usage, ‘ru_wallclock’ and ‘maxvmem’ reported by the qacct command of the Univa Grid Engine were used. The assemblers were classified as ‘Light’, ‘Medium’ or ‘Heavy’ according to their maximum values of the wall-clock time and memory usage. The classification thresholds for the maximum value of wall-clock time for each tool were Light <50 h, 50 h ≤ Medium <250 h and Heavy ≥250 h. The classification thresholds for the maximum value of memory usage for each tool were Light <50 GB, 50 GB ≤ Medium <400 GB and Heavy ≥400 GB. The assemblers were classified by computational resource usage, including the wall-clock time and memory usage, as a comprehensive evaluation. The thresholds for ‘Light’ and ‘Heavy’ were the maximum value of wall-clock time < 50 h with the maximum value of memory usage <300 GB, and the maximum value of wall-clock time ≥ 250 h or the maximum value of memory usage ≥400 GB, respectively. ‘Medium’ was defined as other than Light or Heavy in this case.\n\nFurthermore, the contigs of Arabidopsis F1 hybrids were evaluated by comparing with the parental haploid sequences, Col-0 (TAIR10) and Cvi-0 [10], using QUAST v5.0.2 [50]. Genome fraction is the percentage of the total number of bases aligned with contigs divided by the reference genome size. NGA50 is similar to N50 but uses the alignment block length and reference genome length instead of the contig length and total contig size for N50. The alignment block lengths are calculated by splitting contigs at misassembly breakpoints. Thus, NGA50 is the alignment block length at 50% of the total reference genome size.\n\nGenomeScope was used for all six genomes, and the estimated genome sizes were compared to the original research (Supplementary Table S2). The differences of estimated genome sizes from those of original research ranged from 6 to 27% in GenomeScope. The genome statistics estimated by GenomeScope were 0.055–3.00% heterozygosity, 32–906 Mb genome size and 2.8–43.9% repeats (Table 1).\n\nContig statistics of assembly contigs and polished contigs are described in Supplementary Table S3 and Supplementary Table S4, respectively. Even after polishing, the total lengths obtained by almost all the assemblers did not change much from those before polishing (Supplementary Table S5). However, the largest contig length of miniasm for L. calcarifer decreased by 1.84% after polishing. Similarly, after polishing, the total contig length of miniasm for N. putrida NIES-4239 decreased by 1.59% after polishing and that of N50 of miniasm for L. calcarifer decreased by 2.58%. Therefore, polishing has a certain impact on contig length assembly, and we compared the assembly ploidy as well as continuity among the assemblers with polished contigs.\n\nAs heterozygosity increased, the assembly ploidy of each assembler also increased (Figure 2). The graphs of the cumulative length for contigs are shown in Supplementary Figure S2 and the concrete values of assembly size in Supplementary Table S4. As for the genomes with heterozygosity <0.5, including those of A. thaliana C24, N. putrida NIES-4239, and L. calcarifer, most assemblers show that the assembly ploidies are approximately 1, implying that the total contig sizes almost equal the estimated genome sizes. However, in the S. sitiens genome (heterozygosity, 0.847), the assembly ploidy in miniasm, Platanus-allee and Canu exceeded 1.5. In the A. thaliana F1 genome (heterozygosity, 1.04), miniasm and Canu showed an assembly ploidy of approximately 2. Furthermore, in the C. gigas genome (heterozygosity, 3.00), the assembly ploidy in miniasm, Flye, Canu, MaSuRCA_F and MaSuRCA_C exceeded 2, whereas that in Redbean, NextDenovo, and Platanus-allee exceeded 1.5. In the N. putrida NIES-4239 genome (heterozygosity, <0.5), SPAdes showed a much larger assembly ploidy of 12.90, whereas that of HASLR and WENGAN-M tended to be smaller overall.\n\nThe values of N50 are indicated in Figure 2; the graphs of the Nx statistics for contigs, in Supplementary Figure S3; and the concrete statistical values, including number of contigs, largest contig length, total length, N50, number of Ns and number of gaps, are listed in Supplementary Table S4. The continuity score, which is obtained by ranking N50 of each genome and summing the values of each rank, calculated for genomes with a heterozygosity <1 and ≥ 1, are indicated in Tables 2A and B, respectively. Consequently, for genomes with a heterozygosity <1, Flye, NextDenovo and MaSuRCA_C were classified as ‘high’; MaSuRCA_F, Platanus-allee, Redbean, miniasm, WENGAN_M and HASLR as ‘medium’; and Canu and SPAdes as ‘low’. For genomes with a heterozygosity ≥1, MaSuRCA_C, NextDenovo and Redbean were classified as ‘high’; Canu, MaSuRCA_F, Flye, Platanus-allee and WENGAN_M as ‘medium’; and HASLR, miniasm and SPAdes as ‘low’. In general, MaSuRCA_C and NextDenovo provided better continuity regardless of heterozygosity in the given genomes, while Flye provided better continuity for genomes with a heterozygosity <1.\n\nBUSCO completeness of assembly contigs and polished contigs are shown in Figures 3 and 4, respectively. The concrete BUSCO scores are indicated in Supplementary Table S6 and Supplementary Table S7. In particular for miniasm and Redbean, BUSCO completeness tended to improve significantly after polishing (Supplementary Table S8). For SPAdes, MaSuRCA_F and MaSuRCA_C, there was little change. Platanus-allee remained unchanged in all genomes. For HASLR, WENGAN-M, Flye, NextDenovo and Canu, the BUSCO completeness moderately improved after polishing. Since these results indicated that BUSCO completeness improved significantly before and after polishing the assembled contigs, especially with miniasm and Redbean, BUSCO completeness among the assemblers was compared based on the results after polishing in the following section.\n\nThe ‘complete score’ (C) column and ‘complete and duplicate score’ (D) column of BUSCO for after polishing are indicated in Figure 2. BUSCO completeness after polishing is shown in Figure 4 and Supplementary Table S7. The genomes of A. thaliana C24 (heterozygosity, 0.055) and N. putrida NIES-4239 (heterozygosity, 0.336), which have lower heterozygosity, showed that their BUSCO completeness was not noticeably different among the assemblers, except for miniasm. For the other genomes, such as L. calcarifer (heterozygosity, 0.479), the BUSCO completeness was explicitly different among the assemblers, from 7.9 in miniasm to 98.8 in MaSuRCA_F. The completeness score, which is obtained by ranking the scores for complete BUSCO genes of each genome and summing the values of each rank, for each tool was separately calculated for genomes with heterozygosity <1 and ≥ 1 (Tables 2A and B). Consequently, for genomes with a heterozygosity <1, Flye, MaSuRCA_F and MaSuRCA_C were classified as ‘high’; SPAdes, Platanus-allee, Canu, Redbean and NextDenovo as ‘medium’; and WENGAN-M, HASLR and miniasm as ‘low’. For genomes with a heterozygosity ≥1, Canu and MaSuRCA_C were classified as ‘high’; NextDenovo, MaSuRCA_F, Redbean, Flye and Platanus-allee as ‘medium’; and SPAdes, WENGAN-M, miniasm and HASLR as ‘low’. Across the various levels of heterozygosity, MaSuRCA_F, MaSuRCA_C, Flye and Canu provided stable and high BUSCO completeness. The complete and duplicated BUSCO scores were higher in most assemblers for genomes with a heterozygosity >0.5 (e.g. S. sitiens, A. thaliana F1, and C. gigas) than for the other genomes; that of Canu was particularly large.\n\nComputational resource usages in the de novo assembly process are represented in Figure 5 and the concrete numeric values, in Supplementary Table S9. The maximum wall-clock time values exceeded 250 h for NextDenovo, SPAdes, Platanus-allee, Canu, MaSuRCA_F and MaSuRCA_C, whereas that of HASLR, WENGAN-M, Redbean and miniasm did not exceed 50 h. Flye had a maximum wall-clock time of 90 h. The wall-clock time of Canu of L. calcarifer exceeded 500 h and became a ‘timeout’. Subsequently, to evaluate the wall-clock times, HASLR, WENGAN-M, Redbean and miniasm were classified as ‘Light’; Flye, as ‘Medium’; and NextDenovo, SPAdes, Platanus-allee, Canu, MaSuRCA_F and MaSuRCA_C, as ‘Heavy’ (Table 2C).\n\nThe maximum value of memory usage exceeded 400 GB for SPAdes, Platanus-allee, Canu, MaSuRCA_F and MaSuRCA_C. Neither HASLR nor WENGAN-M used >50 GB of memory in any condition. The maximum memory usages of Redbean and miniasm were both <300 GB but >50 GB. The maximum memory usages for Flye and NextDenovo were 343 and 364 GB, respectively. Consequently, to evaluate memory usage, HASLR and WENGAN-M were classified as ‘Light’; Redbean, miniasm, Flye and NextDenovo, as ‘Medium’; and SPAdes, Platanus-allee, Canu, MaSuRCA_F and MaSuRCA_C, as ‘Heavy’ (Table 2C).\n\nWe then classified the de novo assemblers based on the computational resource usage comprising the wall-clock time and the maximum memory usage. HASLR, WENGAN-M, Redbean and miniasm were classified as ‘Light’; Flye, as ‘Medium’; and NextDenovo, SPAdes, Platanus-allee, Canu, MaSuRCA_F and MaSuRCA_C, as ‘Heavy’. Hereinafter, assemblers are referred to as ‘light-weight tool’, ‘medium-weight tool’ or ‘heavy-weight tool’ according to the categories above based on their computational resource usage.\n\nAfter the execution of Purge Haplotigs, assembly ploidies in most of the assemblers were closer to 1 across the genomes (Figure 2 and Supplementary Figure S2). Although there was no assembly with ploidy over 2, only miniasm, Flye, Canu and MaSuRCA_C provided an assembly ploidy >1.5 for genomes with higher heterozygosities, such as for C. gigas (heterozygosity, 3.00), suggesting difficulty in haplotype removal for these genomes. The concrete statistical values for primary contigs, haplotigs, and artefacts are indicated in Supplementary Table S10.\n\nThe output files from Purge Haplotigs, primary contigs, haplotigs, and artefacts were examined for BUSCO completeness (Supplementary Figures S4–S6) to evaluate the validity of the purging process. The contig sets with higher complete and duplicated BUSCO scores were purged well (Figure 2). The highest BUSCO duplicated score before purging was 79.2% in Canu of C. gigas; after purging, this score drastically decreased to 12.1%. The BUSCO duplicated scores of C. gigas, which were higher in any of the assemblers than the other genomes, decreased to 14.9% after purging for MaSuRCA_C at the highest. In contrast, the BUSCO completeness of primary contigs was similar to that before the removal of haplotigs from across the genomes (Supplementary Table S11). The completeness scores that decreased by >1% were those of miniasm for N. putrida NIES-4239 (heterozygosity, 0.336); miniasm for S. sitiens (heterozygosity, 0.847), miniasm and Platanus-allee for A. thaliana F1 (heterozygosity, 1.04); and Redbean, miniasm, SPAdes and Canu for C. gigas (heterozygosity, 3.00). The maximum decreased score was 10.6% of miniasm for C. gigas. That is, decrease in BUSCO completeness >1% was not observed even after Purge Haplotigs for any genomes in HASLR, WENGEN-M, Flye, NextDenovo, MaSuRCA_F or MaSuRCA_C.\n\nThese results suggest that some might have been over-purged. To survey the over-purge, we examined (i) the number of BUSCO genes that were detected on haplotigs but not on primary contigs and (ii) the number of BUSCO genes that were detected on artefacts but not on primary contigs (Supplementary Table S12). Consequently, an over-purge >1% of the total BUSCO genes was not observed in any of the assembled genomes with HASLR, WENGEN-M, Flye, NextDenovo, MaSuRCA_F or MaSuRCA_C. Conversely, for genomes with a heterozygosity >0.847 (such as that for S. sitiens), an over-purge was observed in some assemblers. The maximum number of (A) was 160 in miniasm for A. thaliana F1 (9.9% of the overall BUSCO genes), and that of (B) was 10 in Platanus-allee for A. thaliana F1 (0.6% of the overall BUSCO genes).\n\nTo evaluate how effectively the above processes (assembly, polishing and purging) reconstructed the haploid sequences, the assembled A. thaliana F1 primary contigs were compared with the haploid sequences of Col-0 and Cvi-0, which are parent accessions (Supplementary Table S13). The genome fractions, NGA50 values, or the number of misassemblies between F1 and Col-0 and between F1 and Cvi-0 were comparable. For Redbean, Flye, SPAdes, Platanus-allee, Canu and MaSuRCA_C, the genome fractions were > 90%, while they were 88% for MaSuRCA_F and approximately 80% for NextDenovo and WENGAN-M. However, in miniasm and HASLR, the genome fractions were 52% and < 50%, respectively. The NGA50 values in Redbean, Flye, Canu, MaSuRCA_F and MaSuRCA_C were > 130 Kb; miniasm had the lowest NGA50 value (approximately 3000 b). The NGA50 values of the remaining assemblers were almost 8000–12 000 b. HASLR has no NGA50 value because the total aligned length is <50% of the length of the parent reference genome. While the number of misassemblies was the smallest in HASLR (approximately 450), that in the others was 1750–4722, with that in Platanus-allee being the highest.\n\nWe devised a guideline to construct a haploid representation with PacBio long reads and Illumina short reads for diploid genomes with various levels of heterozygosity (Figure 6). This was based on the evaluation of the continuities and BUSCO scores for each process of assembling, polishing and purging haplotigs using various assemblers. First, to understand the sample properties such as genome size, the heterozygosity and repeat rate are estimated using tools such as GenomeScope. For evaluating de novo assemblers, it is recommended to use only polished contigs after assembly. For genomes with any heterozygosities, the first recommended assembler is Redbean, a light-weight tool (Table 2C) with a stable performance regarding continuity and BUSCO completeness, regardless of heterozygosity (Table 2A and B). Redbean can provide a rough indication of computational resource usage, continuity and BUSCO completeness when using other additional assemblers. For genomes with a heterozygosity <1, Flye can be used as the second trial assembler because it is a medium-weight tool (Table 2C) classified as ‘High’ for both continuity and BUSCO completeness when heterozygosity is <1 (Table 2A). If memory and execution time are more available than usage for Flye, MaSuRCA_C should be used because it is a heavy-weight tool (Table 2C) classified as ‘High’ for both continuity and BUSCO completeness when heterozygosity is <1 (Table 2A). For genomes with a heterozygosity ≥1, MaSuRCA_C should be used as the alternative second trial assembler because it is a heavy-weight tool (Table 2C) classified as ‘High’ both for continuity and BUSCO completeness and has a stable performance across the genomes with any heterozygosity (Table 2A and B). If MaSuRCA_C does not terminate successfully or the execution time is too long, it is better to use Flye as a medium-weight tool even for genomes with a heterozygosity ≥1. Flye is inferior to MaSuRCA_C in both continuity and BUSCO completeness (Table 2B), but it provides stable results with lower computational resources than MaSuRCA_C does (Table 2C). If two or more assemblers are used, their continuity and BUSCO completeness must be compared. Finally, removal of haploid duplication is performed using tools such as Purge Haplotigs. This process is more important for genomes with a higher heterozygosity. After purging, the results need to be verified, and manual curation is required.\n\nHerein, we evaluated the procedures for constructing a haploid representation from PacBio long reads and Illumina short reads by focusing on heterozygosity and suggested an analytical guideline adapted to the degree of heterozygosity. The guideline includes: (i) estimation of genome information, including genome size and heterozygosity, (ii) de novo assembly, (iii) polishing and (iv) removal of duplicated haploid sequences. The five long-read-only and hybrid assemblers, respectively, were assessed for computer resource usage, contig continuity and BUSCO completeness (Table 2). Contig continuity and BUSCO completeness were separately evaluated for genomes with heterozygosities <1 and for those with heterozygosities ≥1. Subsequently, we selected three high-performance assemblers: Redbean from light-weight tools, Flye from medium-weight tools and MaSuRCA_C from heavy-weight tools. These assemblers were incorporated into the analytical guideline (Figure 6).\n\nWe focused on heterozygosity and proposed recommended assemblers. Nevertheless, the best genome–assembler combination would be affected by heterozygosity, repeats, genome size and other factors that cannot be determined without practically testing them. Thus, it is essential to understand the features of assemblers and perform a selective trial. We recommend checking the following items to choose the best assembly contigs: (i) BUSCO completeness, (ii) continuity and (iii) comparison between contig size after purging haplotigs and estimated genome size. For (i), this study conducted polishing once. However, polishing iteration may be effective if time permits, particularly for Redbean and miniasm, which showed significant polishing effects. A comparison of assemblers sometimes shows a similar BUSCO completeness value, indicating the limitation of the evaluations using BUSCO completeness. Thus, not only BUSCO but also the mapping rate of RNA-seq and/or Iso-seq might help select the best assembler. For (ii), we evaluated the continuity with N50 metrics. Depending on the purpose of the individual analysis, other metrics such as the number of contigs and largest contig length would be useful. For (iii), if BUSCO completeness, continuity and genome size are less than the expected values even after using multiple tools, a review of the methods, such as DNA quality, may be required prior to assembly.\n\nWhile purging haplotigs was assessed by focusing on over-purging with the N50 metrics, BUSCO completeness, and assembly polyploidy, under-purging is still a possibility. Therefore, another study [51] evaluated haplotig purging with other metrics along with N50 and BUSCO completeness and a combination of some purging tools. However, evaluation using metrics has limitations. Therefore, to construct an ideal haploid representation, manual curation is needed. Manual curation would be better for genes predicted on the primary contigs, haplotigs and artefacts and to examine whether the predicted genes on the primary contigs cover the predicted genes on the haplotigs or artefacts. Moreover, the removal of organelle genome sequences from the primary and haplotig contigs is required. It is also necessary to remove organelle genomic sequences from the primary and haplotig contigs by homology searches against organelle sequences available at NCBI RefSeq [52].\n\nIn summary, this strategy provides a more efficient and improved quality analysis of the diploid genome. Furthermore, this guideline will be useful for beginners in bioinformatics and bioinformaticians involved with challenging genomes.\n\nComputations were partially performed on the National Institute of Genetics supercomputer at the ROIS National Institute of Genetics. The authors would like to thank Editage (www.editage.com) for English language editing.\n\nThis work was supported by JSPS grant-in-aid for Scientific Research on Innovative Areas, Platform for Advanced Genome Science [16H06279] and KAKENHI [15H05606 and 19H03274] to R.K., [20H03305] to T.N. and [17H03723] to G.T.\n\nTakako Mochizuki is a research scientist at the Genome Informatics Laboratory, National Institute of Genetics, Japan. Her research interest in bioinformatics includes genome assembly, gene annotation, and structural variation.\n\nMika Sakamoto is a research scientist at the Genome Informatics Laboratory, National Institute of Genetics, Japan. Her research interests in bioinformatics include gene annotation, genetic variation, and protein folding prediction.\n\nYasuhiro Tanizawa is an assistant professor at the Genome Informatics Laboratory, National Institute of Genetics, Japan. His research interests include genome bioinformatics and web services and database development.\n\nTakuro Nakayama is an assistant professor at the Division of Life Sciences, Center for Computational Sciences, University of Tsukuba, Japan. His research interests include genome evolutions of single-celled organisms.\n\nGoro Tanifuji is a senior curator at the Department of Zoology, National Museum of Nature and Science, Japan. His research interests include genome evolution related to symbiogenesis.\n\nRyoma Kamikawa is an associate professor at Graduate School of Agriculture, Kyoto University, Japan. His research interests include the diversity of microbial eukaryotes.\n\nYasukazu Nakamura is a professor at the Genome Informatics laboratory, National Institute of Genetics, Japan. His research interests include genome informatics."
    },
    {
        "link": "https://reddit.com/r/bioinformatics/comments/ume2b7/nanopore_long_read_assembly_help",
        "document": "I’m trying to sequence a de novo fungal genome. I already have illumina short reads which I’ve assembled using SPAdes. I now have the long read data from nanopore but my DNA is a little degraded and only contains strands from .5 - 3kb . I’ve tried Flye but this won’t work with strands shorter than 1k so I loose a lot. Having trouble with canu and I’ve also tried spades with —nanopore option.\n\nDoes anyone have any suggestions for things to try ?"
    }
]