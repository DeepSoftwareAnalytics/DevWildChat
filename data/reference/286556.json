[
    {
        "link": "https://docs.python.org/3/library/os.html",
        "document": "This module provides a portable way of using operating system dependent functionality. If you just want to read or write a file see , if you want to manipulate paths, see the module, and if you want to read all the lines in all the files on the command line see the module. For creating temporary files and directories see the module, and for high-level file and directory handling see the module.\n\nNotes on the availability of these functions:\n• None The design of all built-in operating system dependent modules of Python is such that as long as the same functionality is available, it uses the same interface; for example, the function returns stat information about path in the same format (which happens to have originated with the POSIX interface).\n• None Extensions peculiar to a particular operating system are also available through the module, but using them is of course a threat to portability.\n• None All functions accepting path or file names accept both bytes and string objects, and result in an object of the same type, if a path or file name is returned.\n• None On VxWorks, os.popen, os.fork, os.execv and os.spawn*p* are not supported.\n• None On WebAssembly platforms, Android and iOS, large parts of the module are not available or behave differently. APIs related to processes (e.g. , ) and resources (e.g. ) are not available. Others like and are emulated or stubs. WebAssembly platforms also lack support for signals (e.g. , )."
    },
    {
        "link": "https://stackoverflow.com/questions/47642683/how-to-see-documentation-of-inbuilt-functions-on-jupyter-notebook",
        "document": "The documentation comes from the docstring in the Python code.\n\nYou can see it by calling help, and the attribute returns the string.\n\nTaking the built-in as example:"
    },
    {
        "link": "https://jupyterlab.readthedocs.io/en/stable/user/files.html",
        "document": "The file browser and File menu enable you to work with files and folders on your system. This includes opening, creating, deleting, renaming, downloading, copying, and sharing files and folders.\n\nThe file browser is in the left sidebar Files tab:\n\nMany actions on files can also be carried out in the File menu:\n\nTo open any file, double-click on its name in the file browser:\n\nYou can also drag a file into the main work area to create a new tab:\n\nMany files types have multiple viewers/editors. For example, you can open a Markdown file in a text editor or as rendered HTML. A JupyterLab extension can also add new viewers/editors for files. To open a file in a non-default viewer/editor, right-click on its name in the file browser and use the “Open With…” submenu to select the viewer/editor:\n\nA single file can be open simultaneously in multiple viewer/editors and they will remain in sync:\n\nThe file system can be navigated by double-clicking on folders in the listing or clicking on the folders at the top of the folder listing:\n\nRight-click on a file or folder and select “Copy Shareable Link” to copy a URL that can be used to open JupyterLab with that file or folder open.\n\nRight-click on a file or folder and select “Copy Path” to copy the filesystem relative path. This can be used for passing arguments to open files in functions called in various kernels. If you want to copy the filesystem absolute path, you must add the jupyterlab server config and ."
    },
    {
        "link": "https://docs.jupyter.org/en/stable/use/jupyter-directories.html",
        "document": "Jupyter stores different files (i.e. configuration, data, runtime) in a number of different locations. Environment variables may be set to customize for the location of each file type.\n\nConfig files are stored by default in the directory. Set this environment variable to use a particular directory, other than the default, for Jupyter config files. Besides the , additional directories to search can be specified through . Set this environment variable to provide extra directories for the config search path. should contain a series of directories, separated by `` os.pathsep`` ( on Windows, on Unix). An example of where the can be set is if notebook or server extensions are installed in a custom prefix. Since notebook and server extensions are automatically enabled through configuration files, automatic enabling will only work if the custom prefix’s directory is added to the Jupyter config search path. Besides the user config directory mentioned above, Jupyter has a search path of additional locations from which a config file will be loaded. Here’s a table of the locations to be searched, in order of preference: To list the config directories currently being used you can run the below command from the command line: The following command shows the config directory specifically:\n\nJupyter uses a search path to find installable data files, such as kernelspecs and notebook extensions. When searching for a resource, the code will search the search path starting at the first directory until it finds where the resource is contained. Each category of file is in a subdirectory of each directory of the search path. For example, kernel specs are in subdirectories. Set this environment variable to provide extra directories for the data search path. should contain a series of directories, separated by ( on Windows, on Unix). Directories given in are searched before other locations. This is used in addition to other entries, rather than replacing any. or (if not set) (respects ) or (if not set) or (if not set) The config directory for Jupyter data files, which contain non-transient, non-configuration files. Examples include kernelspecs, nbextensions, or voila templates. Set this environment variable to use a particular directory, other than the default, as the user data directory. As mentioned above, to list the config directories currently being used you can run the below command from the command line: The following command shows the data directory specifically:\n\nThings like connection files, which are only useful for the lifetime of a particular process, have a runtime directory. These runtime files are stored in a subdirectory of the user’s data directory (second row of the table above). An environment variable may also be used to set the runtime directory. Set this to override where Jupyter stores runtime files. As mentioned above, to list the config directories currently being used you can run the below command from the command line: The following command shows the runtime directory specifically:"
    },
    {
        "link": "https://geeksforgeeks.org/file-handling-python",
        "document": "File handling refers to the process of performing operations on a file such as creating, opening, reading, writing and closing it, through a programming interface. It involves managing the data flow between the program and the file system on the storage device, ensuring that data is handled safely and efficiently.\n\nTo open a file we can use function, which requires file path and mode as arguments:\n\nWhen opening a file, we must specify the mode we want to which specifies what we want to do with the file. Here’s a table of the different modes available:\n\nOpens the file for reading. File must exist; otherwise, it raises an error. Opens the file for reading binary data. File must exist; otherwise, it raises an error. Opens the file for both reading and writing. File must exist; otherwise, it raises an error. Opens the file for both reading and writing binary data. File must exist; otherwise, it raises an error. Opens the file for writing. Creates a new file or truncates the existing file. Opens the file for writing binary data. Creates a new file or truncates the existing file. Opens the file for both writing and reading. Creates a new file or truncates the existing file. Opens the file for both writing and reading binary data. Creates a new file or truncates the existing file. Opens the file for appending data. Creates a new file if it doesn’t exist. Opens the file for appending binary data. Creates a new file if it doesn’t exist. Opens the file for appending and reading. Creates a new file if it doesn’t exist. Opens the file for appending and reading binary data. Creates a new file if it doesn’t exist. Creates a new file. Raises an error if the file already exists. Creates a new binary file. Raises an error if the file already exists. Creates a new file for reading and writing. Raises an error if the file exists. Exclusive creation with read and write in binary mode. Creates a new binary file for reading and writing. Raises an error if the file exists.\n\nFor this article we are using text file with text:\n\nReading a file can be achieved by file.read() which reads the entire content of the file. After reading the file we can close the file using file.close() which closes the file after reading it, which is necessary to free up system resources.\n\nWriting to a file is done using file.write() which writes the specified string to the file. If the file exists, its content is erased. If it doesn’t exist, a new file is created.\n\nExample: Writing to a File in Write Mode (w)\n\nIt is done using adds the specified string to the end of the file without erasing its existing content.\n\nExample: For this example, we will use the Python file created in the previous example.\n\nClosing a file is essential to ensure that all resources used by the file are properly released. loses the file and ensures that any changes made to the file are saved.\n\nstatement is used for resource management. It ensures that file is properly closed after its suite finishes, even if an exception is raised. with open() as method automatically handles closing the file once the block of code is exited, even if an error occurs. This reduces the risk of file corruption and resource leakage.\n\nIt’s important to handle exceptions to ensure that files are closed properly, even if an error occurs during file operations.\n• None Versatility : File handling in Python allows us to perform a wide range of operations, such as creating, reading, writing, appending, renaming and deleting files.\n• Flexibility : File handling in Python is highly flexible, as it allows us to work with different file types (e.g. text files, binary files, CSV files , etc.) and to perform different operations on files (e.g. read, write, append, etc.).\n• User – friendly : Python provides a user-friendly interface for file handling, making it easy to create, read and manipulate files.\n• Cross-platform : Python file-handling functions work across different platforms (e.g. Windows, Mac, Linux), allowing for seamless integration and compatibility.\n• Error-prone: File handling operations in Python can be prone to errors, especially if the code is not carefully written or if there are issues with the file system (e.g. file permissions, file locks, etc.).\n• Security risks : File handling in Python can also pose security risks, especially if the program accepts user input that can be used to access or modify sensitive files on the system.\n• Complexity : File handling in Python can be complex, especially when working with more advanced file formats or operations. Careful attention must be paid to the code to ensure that files are handled properly and securely.\n• Performance : File handling operations in Python can be slower than other programming languages, especially when dealing with large files or performing complex operations.\n\nWhat are the types of files in Python?\n\nWhat are the 4 file handling functions?\n\nWhy is file handling useful?\n\nIn Python file handling, is a method of file objects that returns the current position of the file pointer (cursor) within the file. It returns an integer representing the byte offset from the beginning of the file where the next read or write operation will occur. # Open a file in read mode file = open('example.txt', 'r') # Read the first 10 characters content = file.read(10) print(content) # Check the current position of the file pointer position = file.tell() print(\"Current position:\", position) # Close the file file.close()\n• None reads the first 10 characters from the file.\n• None returns the current position of the file pointer after reading."
    },
    {
        "link": "https://stackoverflow.com/questions/12451431/loading-and-parsing-a-json-file-with-multiple-json-objects",
        "document": "Just like Martijn Pieters' answer but maybe a bit more pythonic, and most of all, which enables streaming of data (see second part of the answer):\n\nThe function returns an iterator that applies to every item of , yielding the results (cf map() python doc).\n\n And the transforms this iterator into... a list :)\n\n But you can imagine to directly use the iterator returned by map instead: it iterates over each of your json lines. Note that in that case you need to do it in the context: that is the strength of this approach, the json lines are not fully loaded in a list, they are streamed: the map function read each line of the file when is called by the . \n\n It would give:\n\nAnd I have nothing to add to Martijn's answer for explanations about what is a jsonl (json line by line file) and why use it!"
    },
    {
        "link": "https://saturncloud.io/blog/how-to-load-a-json-file-in-jupyter-notebook-using-pandas",
        "document": "How to Load a JSON File in Jupyter Notebook Using Pandas\n\nAs a data scientist, you will often find yourself working with JSON files. These files are widely used for data exchange between web services, and they have become a popular format for storing data. If you work with Jupyter Notebook, you can easily load JSON files using the pandas library. In this post, we will go over the steps to load a JSON file in Jupyter Notebook using pandas.\n\nAs a data scientist, you will often find yourself working with JSON files. These files are widely used for data exchange between web services, and they have become a popular format for storing data. If you work with Jupyter Notebook, you can easily load JSON files using the pandas library. In this post, we will go over the steps to load a JSON file in Jupyter Notebook using pandas.\n• Why Use Pandas to Load JSON Files?\n\nWhy Use Pandas to Load JSON Files?\n\nPandas is a powerful library for data manipulation and analysis. It provides a convenient way to work with tabular data, and it is widely used in the data science community. When it comes to loading JSON files, pandas offers several benefits:\n• Pandas can convert JSON data into a tabular format, which makes it easier to work with.\n\nTo load a JSON file in Jupyter Notebook using pandas, follow these steps:\n\nBefore you can use pandas to load a JSON file, you need to import the library. You can do this by running the following code:\n\nOnce you have imported the pandas library, you can load the JSON file using the function. This function takes the path of the JSON file as a parameter. Here’s an example:\n\nIn this example, we are loading a JSON file called . The function returns a pandas DataFrame object.\n\nNow that you have loaded the JSON file into a pandas DataFrame object, you can explore the data using pandas functions. For example, you can use the function to view the first few rows of the DataFrame:\n\nIf your JSON file contains nested structures, such as dictionaries within columns, you may want to use the function to flatten the data. This is particularly useful for handling nested JSON structures.\n\nOnce you have loaded the JSON file into a pandas DataFrame object, you can manipulate the data using pandas functions. For example, you can use the function to group the data by a specific column:\n\nIn this example, we are grouping the data by the column called . The variable contains a pandas GroupBy object.\n\nFinally, you can visualize the data using pandas functions or other visualization libraries like Matplotlib or Seaborn. For example, you can use the function to create a simple plot:\n\nThis will create a simple line plot of the data.\n\nIn this post, we have gone over the steps to load a JSON file in Jupyter Notebook using pandas. Pandas is a powerful library for data manipulation and analysis, and it provides a convenient way to work with JSON files. By following the steps outlined in this post, you can easily load and manipulate JSON data in Jupyter Notebook using pandas.\n\nSaturn Cloud is your all-in-one solution for data science & ML development, deployment, and data pipelines in the cloud. Spin up a notebook with 4TB of RAM, add a GPU, connect to a distributed cluster of workers, and more. Request a demo today to learn more."
    },
    {
        "link": "https://docs.python.org/3/library/json.html",
        "document": "JSON (JavaScript Object Notation), specified by RFC 7159 (which obsoletes RFC 4627) and by ECMA-404, is a lightweight data interchange format inspired by JavaScript object literal syntax (although it is not a strict subset of JavaScript ).\n\nexposes an API familiar to users of the standard library and modules.\n\nUsing from the shell to validate and pretty-print:\n\nSerialize obj as a JSON formatted stream to fp (a -supporting file-like object) using this Python-to-JSON conversion table. Unlike and , JSON is not a framed protocol, so trying to serialize multiple objects with repeated calls to using the same fp will result in an invalid JSON file.\n• None obj (object) – The Python object to be serialized.\n• None fp (file-like object) – The file-like object obj will be serialized to. The module always produces objects, not objects, therefore must support input.\n• None skipkeys (bool) – If , keys that are not of a basic type ( , , , , ) will be skipped instead of raising a . Default .\n• None ensure_ascii (bool) – If (the default), the output is guaranteed to have all incoming non-ASCII characters escaped. If , these characters will be outputted as-is.\n• None check_circular (bool) – If , the circular reference check for container types is skipped and a circular reference will result in a (or worse). Default .\n• None allow_nan (bool) – If , serialization of out-of-range values ( , , ) will result in a , in strict compliance with the JSON specification. If (the default), their JavaScript equivalents ( , , ) are used.\n• None cls (a subclass) – If set, a custom JSON encoder with the method overridden, for serializing into custom datatypes. If (the default), is used.\n• None indent (int | str | None) – If a positive integer or string, JSON array elements and object members will be pretty-printed with that indent level. A positive integer indents that many spaces per level; a string (such as ) is used to indent each level. If zero, negative, or (the empty string), only newlines are inserted. If (the default), the most compact representation is used.\n• None separators (tuple | None) – A two-tuple: . If (the default), separators defaults to if indent is , and otherwise. For the most compact JSON, specify to eliminate whitespace.\n• None default (callable | None) – A function that is called for objects that can’t otherwise be serialized. It should return a JSON encodable version of the object or raise a . If (the default), is raised.\n• None sort_keys (bool) – If , dictionaries will be outputted sorted by key. Default . Changed in version 3.2: Allow strings for indent in addition to integers. Changed in version 3.4: Use as default if indent is not . Changed in version 3.6: All optional parameters are now keyword-only. Serialize obj to a JSON formatted using this conversion table. The arguments have the same meaning as in . Keys in key/value pairs of JSON are always of the type . When a dictionary is converted into JSON, all the keys of the dictionary are coerced to strings. As a result of this, if a dictionary is converted into JSON and then back into a dictionary, the dictionary may not equal the original one. That is, if x has non-string keys. Deserialize fp to a Python object using the JSON-to-Python conversion table.\n• None fp (file-like object) – A -supporting text file or binary file containing the JSON document to be deserialized.\n• None cls (a subclass) – If set, a custom JSON decoder. Additional keyword arguments to will be passed to the constructor of cls. If (the default), is used.\n• None object_hook (callable | None) – If set, a function that is called with the result of any object literal decoded (a ). The return value of this function will be used instead of the . This feature can be used to implement custom decoders, for example JSON-RPC class hinting. Default .\n• None object_pairs_hook (callable | None) – If set, a function that is called with the result of any object literal decoded with an ordered list of pairs. The return value of this function will be used instead of the . This feature can be used to implement custom decoders. If object_hook is also set, object_pairs_hook takes priority. Default .\n• None parse_float (callable | None) – If set, a function that is called with the string of every JSON float to be decoded. If (the default), it is equivalent to . This can be used to parse JSON floats into custom datatypes, for example .\n• None parse_int (callable | None) – If set, a function that is called with the string of every JSON int to be decoded. If (the default), it is equivalent to . This can be used to parse JSON integers into custom datatypes, for example .\n• None parse_constant (callable | None) – If set, a function that is called with one of the following strings: , , or . This can be used to raise an exception if invalid JSON numbers are encountered. Default .\n• None JSONDecodeError – When the data being deserialized is not a valid JSON document.\n• None UnicodeDecodeError – When the data being deserialized does not contain UTF-8, UTF-16 or UTF-32 encoded data.\n• None All optional parameters are now keyword-only.\n• None fp can now be a binary file. The input encoding should be UTF-8, UTF-16 or UTF-32. Changed in version 3.11: The default parse_int of now limits the maximum length of the integer string via the interpreter’s integer string conversion length limitation to help avoid denial of service attacks. Identical to , but instead of a file-like object, deserialize s (a , or instance containing a JSON document) to a Python object using this conversion table. Changed in version 3.6: s can now be of type or . The input encoding should be UTF-8, UTF-16 or UTF-32. Changed in version 3.9: The keyword argument encoding has been removed.\n\nPerforms the following translations in decoding by default: It also understands , , and as their corresponding values, which is outside the JSON spec. object_hook is an optional function that will be called with the result of every JSON object decoded and its return value will be used in place of the given . This can be used to provide custom deserializations (e.g. to support JSON-RPC class hinting). object_pairs_hook is an optional function that will be called with the result of every JSON object decoded with an ordered list of pairs. The return value of object_pairs_hook will be used instead of the . This feature can be used to implement custom decoders. If object_hook is also defined, the object_pairs_hook takes priority. parse_float is an optional function that will be called with the string of every JSON float to be decoded. By default, this is equivalent to . This can be used to use another datatype or parser for JSON floats (e.g. ). parse_int is an optional function that will be called with the string of every JSON int to be decoded. By default, this is equivalent to . This can be used to use another datatype or parser for JSON integers (e.g. ). parse_constant is an optional function that will be called with one of the following strings: , , . This can be used to raise an exception if invalid JSON numbers are encountered. If strict is false ( is the default), then control characters will be allowed inside strings. Control characters in this context are those with character codes in the 0–31 range, including (tab), , and . If the data being deserialized is not a valid JSON document, a will be raised. Changed in version 3.6: All parameters are now keyword-only. Return the Python representation of s (a instance containing a JSON document). will be raised if the given JSON document is not valid. Decode a JSON document from s (a beginning with a JSON document) and return a 2-tuple of the Python representation and the index in s where the document ended. This can be used to decode a JSON document from a string that may have extraneous data at the end. Supports the following objects and types by default: Changed in version 3.4: Added support for int- and float-derived Enum classes. To extend this to recognize other objects, subclass and implement a method with another method that returns a serializable object for if possible, otherwise it should call the superclass implementation (to raise ). If skipkeys is false (the default), a will be raised when trying to encode keys that are not , , or . If skipkeys is true, such items are simply skipped. If ensure_ascii is true (the default), the output is guaranteed to have all incoming non-ASCII characters escaped. If ensure_ascii is false, these characters will be output as-is. If check_circular is true (the default), then lists, dicts, and custom encoded objects will be checked for circular references during encoding to prevent an infinite recursion (which would cause a ). Otherwise, no such check takes place. If allow_nan is true (the default), then , , and will be encoded as such. This behavior is not JSON specification compliant, but is consistent with most JavaScript based encoders and decoders. Otherwise, it will be a to encode such floats. If sort_keys is true (default: ), then the output of dictionaries will be sorted by key; this is useful for regression tests to ensure that JSON serializations can be compared on a day-to-day basis. If indent is a non-negative integer or string, then JSON array elements and object members will be pretty-printed with that indent level. An indent level of 0, negative, or will only insert newlines. (the default) selects the most compact representation. Using a positive integer indent indents that many spaces per level. If indent is a string (such as ), that string is used to indent each level. Changed in version 3.2: Allow strings for indent in addition to integers. If specified, separators should be an tuple. The default is if indent is and otherwise. To get the most compact JSON representation, you should specify to eliminate whitespace. Changed in version 3.4: Use as default if indent is not . If specified, default should be a function that gets called for objects that can’t otherwise be serialized. It should return a JSON encodable version of the object or raise a . If not specified, is raised. Changed in version 3.6: All parameters are now keyword-only. Implement this method in a subclass such that it returns a serializable object for o, or calls the base implementation (to raise a ). For example, to support arbitrary iterators, you could implement like this: # Let the base class default method raise the TypeError Return a JSON string representation of a Python data structure, o. For example: Encode the given object, o, and yield each string representation as available. For example:\n\nThe JSON format is specified by RFC 7159 and by ECMA-404. This section details this module’s level of compliance with the RFC. For simplicity, and subclasses, and parameters other than those explicitly mentioned, are not considered. This module does not comply with the RFC in a strict fashion, implementing some extensions that are valid JavaScript but not valid JSON. In particular:\n• None Infinite and NaN number values are accepted and output;\n• None Repeated names within an object are accepted, and only the value of the last name-value pair is used. Since the RFC permits RFC-compliant parsers to accept input texts that are not RFC-compliant, this module’s deserializer is technically RFC-compliant under default settings. The RFC requires that JSON be represented using either UTF-8, UTF-16, or UTF-32, with UTF-8 being the recommended default for maximum interoperability. As permitted, though not required, by the RFC, this module’s serializer sets ensure_ascii=True by default, thus escaping the output so that the resulting strings only contain ASCII characters. Other than the ensure_ascii parameter, this module is defined strictly in terms of conversion between Python objects and , and thus does not otherwise directly address the issue of character encodings. The RFC prohibits adding a byte order mark (BOM) to the start of a JSON text, and this module’s serializer does not add a BOM to its output. The RFC permits, but does not require, JSON deserializers to ignore an initial BOM in their input. This module’s deserializer raises a when an initial BOM is present. The RFC does not explicitly forbid JSON strings which contain byte sequences that don’t correspond to valid Unicode characters (e.g. unpaired UTF-16 surrogates), but it does note that they may cause interoperability problems. By default, this module accepts and outputs (when present in the original ) code points for such sequences. The RFC does not permit the representation of infinite or NaN number values. Despite that, by default, this module accepts and outputs , , and as if they were valid JSON number literal values: # Neither of these calls raises an exception, but the results are not valid JSON In the serializer, the allow_nan parameter can be used to alter this behavior. In the deserializer, the parse_constant parameter can be used to alter this behavior. The RFC specifies that the names within a JSON object should be unique, but does not mandate how repeated names in JSON objects should be handled. By default, this module does not raise an exception; instead, it ignores all but the last name-value pair for a given name: The object_pairs_hook parameter can be used to alter this behavior. The old version of JSON specified by the obsolete RFC 4627 required that the top-level value of a JSON text must be either a JSON object or array (Python or ), and could not be a JSON null, boolean, number, or string value. RFC 7159 removed that restriction, and this module does not and has never implemented that restriction in either its serializer or its deserializer. Regardless, for maximum interoperability, you may wish to voluntarily adhere to the restriction yourself. Some JSON deserializer implementations may set limits on:\n• None the maximum level of nesting of JSON objects and arrays\n• None the range and precision of JSON numbers\n• None the content and maximum length of JSON strings This module does not impose any such limits beyond those of the relevant Python datatypes themselves or the Python interpreter itself. When serializing to JSON, beware any such limitations in applications that may consume your JSON. In particular, it is common for JSON numbers to be deserialized into IEEE 754 double precision numbers and thus subject to that representation’s range and precision limitations. This is especially relevant when serializing Python values of extremely large magnitude, or when serializing instances of “exotic” numerical types such as .\n\nThe module provides a simple command line interface to validate and pretty-print JSON objects. If the optional and arguments are not specified, and will be used respectively: Changed in version 3.5: The output is now in the same order as the input. Use the option to sort the output of dictionaries alphabetically by key. The JSON file to be validated or pretty-printed: python -m json.tool mp_films.json \"title\": \"And Now for Something Completely Different\", If infile is not specified, read from . Write the output of the infile to the given outfile. Otherwise, write it to . Sort the output of dictionaries alphabetically by key. Disable escaping of non-ascii characters, see for more information."
    },
    {
        "link": "https://stackoverflow.com/questions/74595487/how-to-open-and-display-json-files-inside-jupyter-notebook",
        "document": "I have an issue within Jupyter that I cannot find online anywhere and was hoping I could get some help.\n\nEssentially, I want to open .JSON files from multiple folders with different names. For example.\n\nI want to be able to output the info inside the data.JSON onto my Jupyter Notebook, but how do I do that as the folder names are all different.\n\nThank you in advance.\n\nWhat I tried so far\n\nfor path,dirs,files in os.walk('data/weather'): for file in files: if fnmatch.fnmatch(file,'*.json'): data = os.path.join(path,file) print(data)\n\nBut i dont want it to output the directory, I want to actually open the .JSON and display its content"
    },
    {
        "link": "https://geeksforgeeks.org/reading-and-writing-json-to-a-file-in-python",
        "document": "The full form of JSON is Javascript Object Notation. It means that a script (executable) file which is made of text in a programming language, is used to store and transfer the data. Python supports JSON through a built-in package called JSON. To use this feature, we import the JSON package in Python script. The text in JSON is done through quoted-string which contains the value in key-value mapping within { }. It is similar to the dictionary in Python.\n\nSerializing JSON refers to the transformation of data into a series of bytes (hence serial) to be stored or transmitted across a network. To handle the data flow in a file, the JSON library in Python uses dump() or dumps() function to convert the Python objects into their respective JSON object, so it makes it easy to write data to files. See the following table given below.\n\nMethod 1: Writing JSON to a file in Python using json.dumps()\n\nThe JSON package in Python has a function called json.dumps() that helps in converting a dictionary to a JSON object. It takes two parameters:\n• dictionary – the name of a dictionary which should be converted to a JSON object.\n• indent – defines the number of units for indentation\n\nAfter converting the dictionary to a JSON object, simply write it to a file using the “write” function.\n\nMethod 2: Writing JSON to a file in Python using json.dump()\n\nAnother way of writing JSON to a file is by using json.dump() method The JSON package has the “dump” function which directly writes the dictionary to a file in the form of JSON, without needing to convert it into an actual JSON object. It takes 2 parameters:\n• dictionary – the name of a dictionary which should be converted to a JSON object.\n• file pointer – pointer of the file opened in write or append mode.\n\nDeserialization is the opposite of Serialization, i.e. conversion of JSON objects into their respective Python objects. The load() method is used for it. If you have used JSON data from another program or obtained it as a string format of JSON, then it can easily be deserialized with load(), which is usually used to load from a string, otherwise, the root object is in a list or Dict.\n\nThe JSON package has json.load() function that loads the JSON content from a JSON file into a dictionary. It takes one parameter:"
    }
]