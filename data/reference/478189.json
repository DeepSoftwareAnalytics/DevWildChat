[
    {
        "link": "https://docs.kernel.org/kbuild/modules.html",
        "document": "This document describes how to build an out-of-tree kernel module.\n\n“kbuild” is the build system used by the Linux kernel. Modules must use kbuild to stay compatible with changes in the build infrastructure and to pick up the right flags to the compiler. Functionality for building modules both in-tree and out-of-tree is provided. The method for building either is similar, and all modules are initially developed and built out-of-tree. Covered in this document is information aimed at developers interested in building out-of-tree (or “external”) modules. The author of an external module should supply a makefile that hides most of the complexity, so one only has to type “make” to build the module. This is easily accomplished, and a complete example will be presented in section Creating a Kbuild File for an External Module.\n\nTo build external modules, you must have a prebuilt kernel available that contains the configuration and header files used in the build. Also, the kernel must have been built with modules enabled. If you are using a distribution kernel, there will be a package for the kernel you are running provided by your distribution. An alternative is to use the “make” target “modules_prepare.” This will make sure the kernel contains the information required. The target exists solely as a simple way to prepare a kernel source tree for building external modules. NOTE: “modules_prepare” will not build Module.symvers even if CONFIG_MODVERSIONS is set; therefore, a full kernel build needs to be executed to make module versioning work. The command to build an external module is: The kbuild system knows that an external module is being built due to the “M=<dir>” option given in the command. To build against the running kernel use: Then to install the module(s) just built, add the target “modules_install” to the command: Starting from Linux 6.13, you can use the -f option instead of -C. This will avoid unnecessary change of the working directory. The external module will be output to the directory where you invoke make. You can optionally pass MO= option if you want to build the modules in a separate directory. The directory that contains the kernel and relevant build artifacts used for building an external module. “make” will actually change to the specified directory when executing and will change back when finished. Informs kbuild that an external module is being built. The value given to “M” is the absolute path of the directory where the external module (kbuild file) is located. When building an external module, only a subset of the “make” targets are available. The default will build the module(s) located in the current directory, so a target does not need to be specified. All output files will also be generated in this directory. No attempts are made to update the kernel source, and it is a precondition that a successful “make” has been executed for the kernel. The default target for external modules. It has the same functionality as if no target was specified. See description above. Install the external module(s). The default location is /lib/modules/<kernel_release>/updates/, but a prefix may be added with INSTALL_MOD_PATH (discussed in section Module Installation). Remove all generated files in the module directory only. List the available targets for external modules. It is possible to build single files that are part of a module. This works equally well for the kernel, a module, and even for external modules. Example (The module foo.ko, consist of bar.o and baz.o): make -C $KDIR M=$PWD bar.lst make -C $KDIR M=$PWD baz.o make -C $KDIR M=$PWD foo.ko make -C $KDIR M=$PWD ./\n\nIn the last section we saw the command to build a module for the running kernel. The module is not actually built, however, because a build file is required. Contained in this file will be the name of the module(s) being built, along with the list of requisite source files. The file may be as simple as a single line: The kbuild system will build <module_name>.o from <module_name>.c, and, after linking, will result in the kernel module <module_name>.ko. The above line can be put in either a “Kbuild” file or a “Makefile.” When the module is built from multiple sources, an additional line is needed listing the files: NOTE: Further documentation describing the syntax used by kbuild is located in Linux Kernel Makefiles. The examples below demonstrate how to create a build file for the module 8123.ko, which is built from the following files: An external module always includes a wrapper makefile that supports building the module using “make” with no arguments. This target is not used by kbuild; it is only for convenience. Additional functionality, such as test targets, can be included but should be filtered out from kbuild due to possible name clashes. --> filename: Makefile ifneq ($(KERNELRELEASE),) # kbuild part of makefile obj-m := 8123.o 8123-y := 8123_if.o 8123_pci.o else # normal makefile KDIR ?= /lib/modules/`uname -r`/build default: $(MAKE) -C $(KDIR) M=$$PWD endif The check for KERNELRELEASE is used to separate the two parts of the makefile. In the example, kbuild will only see the two assignments, whereas “make” will see everything except these two assignments. This is due to two passes made on the file: the first pass is by the “make” instance run on the command line; the second pass is by the kbuild system, which is initiated by the parameterized “make” in the default target. Kbuild will first look for a file named “Kbuild”, and if it is not found, it will then look for “Makefile”. Utilizing a “Kbuild” file allows us to split up the “Makefile” from example 1 into two files: The split in example 2 is questionable due to the simplicity of each file; however, some external modules use makefiles consisting of several hundred lines, and here it really pays off to separate the kbuild part from the rest. Linux 6.13 and later support another way. The external module Makefile can include the kernel Makefile directly, rather than invoking sub Make. kbuild supports building multiple modules with a single build file. For example, if you wanted to build two modules, foo.ko and bar.ko, the kbuild lines would be:\n\nWithin the kernel, header files are kept in standard locations according to the following rule:\n• None If the header file only describes the internal interface of a module, then the file is placed in the same directory as the source files.\n• None If the header file describes an interface used by other parts of the kernel that are located in different directories, then the file is placed in include/linux/. There are two notable exceptions to this rule: larger subsystems have their own directory under include/, such as include/scsi; and architecture specific headers are located under arch/$(SRCARCH)/include/. To include a header file located under include/linux/, simply use: kbuild will add options to the compiler so the relevant directories are searched. External modules tend to place header files in a separate include/ directory where their source is located, although this is not the usual kernel style. To inform kbuild of the directory, use either ccflags-y or CFLAGS_<filename>.o. Using the example from section 3, if we moved 8123_if.h to a subdirectory named include, the resulting kbuild file would look like: kbuild can handle files that are spread over several directories. Consider the following example: To build the module complex.ko, we then need the following kbuild file: As you can see, kbuild knows how to handle object files located in other directories. The trick is to specify the directory relative to the kbuild file’s location. That being said, this is NOT recommended practice. For the header files, kbuild must be explicitly told where to look. When kbuild executes, the current directory is always the root of the kernel tree (the argument to “-C”) and therefore an absolute path is needed. $(src) provides the absolute path by pointing to the directory where the currently executing kbuild file is located.\n\nModule versioning is enabled by the CONFIG_MODVERSIONS tag, and is used as a simple ABI consistency check. A CRC value of the full prototype for an exported symbol is created. When a module is loaded/used, the CRC values contained in the kernel are compared with similar values in the module; if they are not equal, the kernel refuses to load the module. Module.symvers contains a list of all exported symbols from a kernel build. During a kernel build, a file named Module.symvers will be generated. Module.symvers contains all exported symbols from the kernel and compiled modules. For each symbol, the corresponding CRC value is also stored. The syntax of the Module.symvers file is: The fields are separated by tabs and values may be empty (e.g. if no namespace is defined for an exported symbol). For a kernel build without CONFIG_MODVERSIONS enabled, the CRC would read 0x00000000.\n• None It lists all exported symbols from vmlinux and all modules.\n• None It lists the CRC if CONFIG_MODVERSIONS is enabled. Exported symbols have information stored in __ksymtab or __ksymtab_gpl sections. Symbol names and namespaces are stored in __ksymtab_strings, using a format similar to the string table used for ELF. If CONFIG_MODVERSIONS is enabled, the CRCs corresponding to exported symbols will be added to the __kcrctab or __kcrctab_gpl. If CONFIG_BASIC_MODVERSIONS is enabled (default with CONFIG_MODVERSIONS), imported symbols will have their symbol name and CRC stored in the __versions section of the importing module. This mode only supports symbols of length up to 64 bytes. If CONFIG_EXTENDED_MODVERSIONS is enabled (required to enable both CONFIG_MODVERSIONS and CONFIG_RUST at the same time), imported symbols will have their symbol name recorded in the __version_ext_names section as a series of concatenated, null-terminated strings. CRCs for these symbols will be recorded in the __version_ext_crcs section. When building an external module, the build system needs access to the symbols from the kernel to check if all external symbols are defined. This is done in the MODPOST step. modpost obtains the symbols by reading Module.symvers from the kernel source tree. During the MODPOST step, a new Module.symvers file will be written containing all exported symbols from that external module. Sometimes, an external module uses exported symbols from another external module. Kbuild needs to have full knowledge of all symbols to avoid spitting out warnings about undefined symbols. Two solutions exist for this situation. NOTE: The method with a top-level kbuild file is recommended but may be impractical in certain situations. If you have two modules, foo.ko and bar.ko, where foo.ko needs symbols from bar.ko, you can use a common top-level kbuild file so both modules are compiled in the same build. Consider the following directory layout: The top-level kbuild file would then look like: will then do the expected and compile both modules with full knowledge of symbols from either module. If it is impractical to add a top-level kbuild file, you can assign a space separated list of files to KBUILD_EXTRA_SYMBOLS in your build file. These files will be loaded by modpost during the initialization of its symbol tables."
    },
    {
        "link": "https://kernel.org/doc/html/next/kbuild/modules.html",
        "document": "This document describes how to build an out-of-tree kernel module.\n\n“kbuild” is the build system used by the Linux kernel. Modules must use kbuild to stay compatible with changes in the build infrastructure and to pick up the right flags to the compiler. Functionality for building modules both in-tree and out-of-tree is provided. The method for building either is similar, and all modules are initially developed and built out-of-tree. Covered in this document is information aimed at developers interested in building out-of-tree (or “external”) modules. The author of an external module should supply a makefile that hides most of the complexity, so one only has to type “make” to build the module. This is easily accomplished, and a complete example will be presented in section Creating a Kbuild File for an External Module.\n\nTo build external modules, you must have a prebuilt kernel available that contains the configuration and header files used in the build. Also, the kernel must have been built with modules enabled. If you are using a distribution kernel, there will be a package for the kernel you are running provided by your distribution. An alternative is to use the “make” target “modules_prepare.” This will make sure the kernel contains the information required. The target exists solely as a simple way to prepare a kernel source tree for building external modules. NOTE: “modules_prepare” will not build Module.symvers even if CONFIG_MODVERSIONS is set; therefore, a full kernel build needs to be executed to make module versioning work. The command to build an external module is: The kbuild system knows that an external module is being built due to the “M=<dir>” option given in the command. To build against the running kernel use: Then to install the module(s) just built, add the target “modules_install” to the command: Starting from Linux 6.13, you can use the -f option instead of -C. This will avoid unnecessary change of the working directory. The external module will be output to the directory where you invoke make. You can optionally pass MO= option if you want to build the modules in a separate directory. The directory that contains the kernel and relevant build artifacts used for building an external module. “make” will actually change to the specified directory when executing and will change back when finished. Informs kbuild that an external module is being built. The value given to “M” is the absolute path of the directory where the external module (kbuild file) is located. When building an external module, only a subset of the “make” targets are available. The default will build the module(s) located in the current directory, so a target does not need to be specified. All output files will also be generated in this directory. No attempts are made to update the kernel source, and it is a precondition that a successful “make” has been executed for the kernel. The default target for external modules. It has the same functionality as if no target was specified. See description above. Install the external module(s). The default location is /lib/modules/<kernel_release>/updates/, but a prefix may be added with INSTALL_MOD_PATH (discussed in section Module Installation). Remove all generated files in the module directory only. List the available targets for external modules. It is possible to build single files that are part of a module. This works equally well for the kernel, a module, and even for external modules. Example (The module foo.ko, consist of bar.o and baz.o): make -C $KDIR M=$PWD bar.lst make -C $KDIR M=$PWD baz.o make -C $KDIR M=$PWD foo.ko make -C $KDIR M=$PWD ./\n\nIn the last section we saw the command to build a module for the running kernel. The module is not actually built, however, because a build file is required. Contained in this file will be the name of the module(s) being built, along with the list of requisite source files. The file may be as simple as a single line: The kbuild system will build <module_name>.o from <module_name>.c, and, after linking, will result in the kernel module <module_name>.ko. The above line can be put in either a “Kbuild” file or a “Makefile.” When the module is built from multiple sources, an additional line is needed listing the files: NOTE: Further documentation describing the syntax used by kbuild is located in Linux Kernel Makefiles. The examples below demonstrate how to create a build file for the module 8123.ko, which is built from the following files: An external module always includes a wrapper makefile that supports building the module using “make” with no arguments. This target is not used by kbuild; it is only for convenience. Additional functionality, such as test targets, can be included but should be filtered out from kbuild due to possible name clashes. --> filename: Makefile ifneq ($(KERNELRELEASE),) # kbuild part of makefile obj-m := 8123.o 8123-y := 8123_if.o 8123_pci.o else # normal makefile KDIR ?= /lib/modules/`uname -r`/build default: $(MAKE) -C $(KDIR) M=$$PWD endif The check for KERNELRELEASE is used to separate the two parts of the makefile. In the example, kbuild will only see the two assignments, whereas “make” will see everything except these two assignments. This is due to two passes made on the file: the first pass is by the “make” instance run on the command line; the second pass is by the kbuild system, which is initiated by the parameterized “make” in the default target. Kbuild will first look for a file named “Kbuild”, and if it is not found, it will then look for “Makefile”. Utilizing a “Kbuild” file allows us to split up the “Makefile” from example 1 into two files: The split in example 2 is questionable due to the simplicity of each file; however, some external modules use makefiles consisting of several hundred lines, and here it really pays off to separate the kbuild part from the rest. Linux 6.13 and later support another way. The external module Makefile can include the kernel Makefile directly, rather than invoking sub Make. kbuild supports building multiple modules with a single build file. For example, if you wanted to build two modules, foo.ko and bar.ko, the kbuild lines would be:\n\nWithin the kernel, header files are kept in standard locations according to the following rule:\n• None If the header file only describes the internal interface of a module, then the file is placed in the same directory as the source files.\n• None If the header file describes an interface used by other parts of the kernel that are located in different directories, then the file is placed in include/linux/. There are two notable exceptions to this rule: larger subsystems have their own directory under include/, such as include/scsi; and architecture specific headers are located under arch/$(SRCARCH)/include/. To include a header file located under include/linux/, simply use: kbuild will add options to the compiler so the relevant directories are searched. External modules tend to place header files in a separate include/ directory where their source is located, although this is not the usual kernel style. To inform kbuild of the directory, use either ccflags-y or CFLAGS_<filename>.o. Using the example from section 3, if we moved 8123_if.h to a subdirectory named include, the resulting kbuild file would look like: kbuild can handle files that are spread over several directories. Consider the following example: To build the module complex.ko, we then need the following kbuild file: As you can see, kbuild knows how to handle object files located in other directories. The trick is to specify the directory relative to the kbuild file’s location. That being said, this is NOT recommended practice. For the header files, kbuild must be explicitly told where to look. When kbuild executes, the current directory is always the root of the kernel tree (the argument to “-C”) and therefore an absolute path is needed. $(src) provides the absolute path by pointing to the directory where the currently executing kbuild file is located.\n\nModule versioning is enabled by the CONFIG_MODVERSIONS tag, and is used as a simple ABI consistency check. A CRC value of the full prototype for an exported symbol is created. When a module is loaded/used, the CRC values contained in the kernel are compared with similar values in the module; if they are not equal, the kernel refuses to load the module. Module.symvers contains a list of all exported symbols from a kernel build. During a kernel build, a file named Module.symvers will be generated. Module.symvers contains all exported symbols from the kernel and compiled modules. For each symbol, the corresponding CRC value is also stored. The syntax of the Module.symvers file is: The fields are separated by tabs and values may be empty (e.g. if no namespace is defined for an exported symbol). For a kernel build without CONFIG_MODVERSIONS enabled, the CRC would read 0x00000000.\n• None It lists all exported symbols from vmlinux and all modules.\n• None It lists the CRC if CONFIG_MODVERSIONS is enabled. Exported symbols have information stored in __ksymtab or __ksymtab_gpl sections. Symbol names and namespaces are stored in __ksymtab_strings, using a format similar to the string table used for ELF. If CONFIG_MODVERSIONS is enabled, the CRCs corresponding to exported symbols will be added to the __kcrctab or __kcrctab_gpl. If CONFIG_BASIC_MODVERSIONS is enabled (default with CONFIG_MODVERSIONS), imported symbols will have their symbol name and CRC stored in the __versions section of the importing module. This mode only supports symbols of length up to 64 bytes. If CONFIG_EXTENDED_MODVERSIONS is enabled (required to enable both CONFIG_MODVERSIONS and CONFIG_RUST at the same time), imported symbols will have their symbol name recorded in the __version_ext_names section as a series of concatenated, null-terminated strings. CRCs for these symbols will be recorded in the __version_ext_crcs section. When building an external module, the build system needs access to the symbols from the kernel to check if all external symbols are defined. This is done in the MODPOST step. modpost obtains the symbols by reading Module.symvers from the kernel source tree. During the MODPOST step, a new Module.symvers file will be written containing all exported symbols from that external module. Sometimes, an external module uses exported symbols from another external module. Kbuild needs to have full knowledge of all symbols to avoid spitting out warnings about undefined symbols. Two solutions exist for this situation. NOTE: The method with a top-level kbuild file is recommended but may be impractical in certain situations. If you have two modules, foo.ko and bar.ko, where foo.ko needs symbols from bar.ko, you can use a common top-level kbuild file so both modules are compiled in the same build. Consider the following directory layout: The top-level kbuild file would then look like: will then do the expected and compile both modules with full knowledge of symbols from either module. If it is impractical to add a top-level kbuild file, you can assign a space separated list of files to KBUILD_EXTRA_SYMBOLS in your build file. These files will be loaded by modpost during the initialization of its symbol tables."
    },
    {
        "link": "https://stackoverflow.com/questions/39810464/in-which-folder-should-i-add-the-ko-file-i-compiled",
        "document": "So I have compiled the driver for my USB wireless adapter.\n\n Now I need to know where I should copy the .ko file to. The question is really the following:\n\nWhat is the difference between\n\n /lib/modules/(uname -r)/build\n\n and\n\n /lib/module/(uname -r)/kernel/drivers\n\n ?\n\nThank you already"
    },
    {
        "link": "https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/7/html/kernel_administration_guide/chap-documentation-kernel_administration_guide-working_with_kernel_modules",
        "document": "The Linux kernel is monolithic by design. However, it is compiled with optional or additional modules as required by each use case. This means that you can extend the kernel’s capabilities through the use of dynamically-loaded kernel modules. A kernel module can provide:\n• A device driver which adds support for new hardware.\n• Support for a file system such as or . Like the kernel itself, modules can take parameters that customize their behavior. Though the default parameters work well in most cases. In relation to kernel modules, user-space tools can do the following operations:\n• Querying all available modules for available parameters and module-specific information.\n• Loading or unloading (removing) modules dynamically into or from a running kernel. Many of these utilities, which are provided by the package, take module dependencies into account when performing operations. As a result, manual dependency-tracking is rarely necessary. On modern systems, kernel modules are automatically loaded by various mechanisms when needed. However, there are occasions when it is necessary to load or unload modules manually. For example, when one module is preferred over another although either is able to provide basic functionality, or when a module performs unexpectedly.\n\nCertain kernel modules sometimes depend on one or more other kernel modules. The file contains a complete list of kernel module dependencies for the respective kernel version. The dependency file is generated by the program, which is a part of the package. Many of the utilities provided by take module dependencies into account when performing operations so that manual dependency-tracking is rarely necessary. The code of kernel modules is executed in kernel-space in the unrestricted mode. Because of this, you should be mindful of what modules you are loading.\n• For more information about , refer to the manual page.\n• For further details including the synopsis and options of , see the manual page.\n\nAt times, you find that you need to unload certain kernel modules from the running kernel. The following procedure describes how to use the command to find and unload a kernel module at system runtime from the currently loaded kernel.\n• None Execute the command and select a kernel module you want to unload. If a kernel module has dependencies, unload those prior to unloading the kernel module. For details on identifying modules with dependencies, see Listing Currently Loaded Modules and Kernel module dependencies.\n• None When entering the name of a kernel module, do not append the extension to the end of the name. Kernel module names do not have extensions; their corresponding files do. Do not unload kernel modules when they are used by the running system. Doing so can lead to an unstable or non-operational system.\n• None If the module was unloaded successfully, this command does not display any output. After finishing this procedure, the kernel modules that are defined to be automatically loaded on boot, will not stay unloaded after rebooting the system. For information on how to counter this outcome, see Preventing kernel modules from being automatically loaded at system boot time.\n• For further details about , see the manual page.\n\n1.8. Preventing kernel modules from being automatically loaded at system boot time The following procedure describes how to add a kernel module to a denylist so that it will not be automatically loaded during the boot process.\n• Ensure that a kernel module in a denylist is not vital for your current system configuration.\n• None Select a kernel module that you want to put in a denylist: The command displays a list of modules loaded to the currently running kernel.\n• None Alternatively, identify an unloaded kernel module you want to prevent from potentially loading. All kernel modules are located in the directory.\n• None The example shows the contents of the file, edited by the editor. The line ensures that the relevant kernel module will not be automatically loaded during the boot process. The command, however, does not prevent the module from being loaded as a dependency for another kernel module that is not in a denylist. Therefore the line causes the to run instead of installing a module. The lines starting with a hash sign are comments to make the file more readable. When entering the name of a kernel module, do not append the extension to the end of the name. Kernel module names do not have extensions; their corresponding files do.\n• None Create a backup copy of the current initial ramdisk image before rebuilding: The command above creates a backup image in case the new version has an unexpected problem.\n• None Alternatively, create a backup copy of other initial ramdisk image which corresponds to the kernel version for which you want to put kernel modules in a denylist:\n• None Generate a new initial ramdisk image to reflect the changes:\n• None If you are building an initial ramdisk image for a different kernel version than you are currently booted into, specify both target and kernel version: The changes described in this procedure will take effect and persist after rebooting the system. If you improperly put a key kernel module in a denylist, you can face an unstable or non-operational system.\n• For further details concerning the utility, refer to the manual page.\n• For more information on preventing automatic loading of kernel modules at system boot time on Red Hat Enterprise Linux 8 and earlier versions, see How do I prevent a kernel module from loading automatically?\n\nRed Hat Enterprise Linux 7 includes support for the UEFI Secure Boot feature, which means that Red Hat Enterprise Linux 7 can be installed and run on systems where UEFI Secure Boot is enabled. Note that Red Hat Enterprise Linux 7 does not require the use of Secure Boot on UEFI systems. If Secure Boot is enabled, the UEFI operating system boot loaders, the Red Hat Enterprise Linux kernel, and all kernel modules must be signed with a private key and authenticated with the corresponding public key. If they are not signed and authenticated, the system will not be allowed to finish the booting process. In addition, the signed first-stage boot loader and the signed kernel include embedded Red Hat public keys. These signed executable binaries and embedded keys enable Red Hat Enterprise Linux 7 to install, boot, and run with the Microsoft UEFI Secure Boot Certification Authority keys that are provided by the UEFI firmware on systems that support UEFI Secure Boot. Not all UEFI-based systems include support for Secure Boot. The information provided in the following sections describes the steps to self-sign privately built kernel modules for use with Red Hat Enterprise Linux 7 on UEFI-based build systems where Secure Boot is enabled. These sections also provide an overview of available options for importing your public key into a target system where you want to deploy your kernel modules. To sign and load kernel modules, you need to:\n• Have the relevant utilities installed on your system.\n• Import the public key on the target system.\n• Sign the kernel module with the private key. To be able to sign externally built kernel modules, install the utilities listed in the following table on the build system. The build system, where you build and sign your kernel module, does not need to have UEFI Secure Boot enabled and does not even need to be a UEFI-based system. In Red Hat Enterprise Linux 7, when a kernel module is loaded, the module’s signature is checked using the public X.509 keys on the kernel’s system key ring, excluding keys on the kernel’s system black-list key ring. The following sections provide an overview of sources of keys/keyrings, examples of loaded keys from different sources in the system. Also, the user can see what it takes to authenticate a kernel module. 1.9.2.1. Sources for public keys used to authenticate kernel modules During boot, the kernel loads X.509 keys into the system key ring or the system black-list key ring from a set of persistent key stores as shown in the table below. If the system is not UEFI-based or if UEFI Secure Boot is not enabled, then only the keys that are embedded in the kernel are loaded onto the system key ring. In that case you have no ability to augment that set of keys without rebuilding the kernel. The system black list key ring is a list of X.509 keys which have been revoked. If your module is signed by a key on the black list then it will fail authentication even if your public key is in the system key ring. You can display information about the keys on the system key rings using the utility. The following is a shortened example output from a Red Hat Enterprise Linux 7 system where UEFI Secure Boot is not enabled. The following is a shortened example output from a Red Hat Enterprise Linux 7 system where UEFI Secure Boot is enabled. The above output shows the addition of two keys from the UEFI Secure Boot \"db\" keys as well as the , which is embedded in the boot loader. You can also look for the kernel console messages that identify the keys with an UEFI Secure Boot related source. These include UEFI Secure Boot db, embedded shim, and MOK list. This section explains what conditions have to be met for loading kernel modules on systems with enabled UEFI Secure Boot functionality. If UEFI Secure Boot is enabled or if the kernel parameter has been specified, you can only load signed kernel modules that are authenticated using a key on the system key ring. In addition, the public key must not be on the system black list key ring. If UEFI Secure Boot is disabled and if the kernel parameter has not been specified, you can load unsigned kernel modules and signed kernel modules without a public key. This is summarized in the table below. You need to generate a public and private X.509 key pair to succeed in your efforts of using kernel modules on a Secure Boot-enabled system. You will later use the private key to sign the kernel module. You will also have to add the corresponding public key to the Machine Owner Key (MOK) for Secure Boot to validate the signed module. For instructions to do so, see Section 1.9.4.2, “System administrator manually adding public key to the MOK list”. Some of the parameters for this key pair generation are best specified with a configuration file.\n• None Create a configuration file with parameters for the key pair generation:\n• None Create an X.509 public and private key pair as shown in the following example: The public key will be written to the file and the private key will be written to the file.\n• None Enroll your public key on all systems where you want to authenticate and load your kernel module. For details, see Section 1.9.4, “Enrolling public key on target system”. Apply strong security measures and access policies to guard the contents of your private key. In the wrong hands, the key could be used to compromise any system which is authenticated by the corresponding public key. When Red Hat Enterprise Linux 7 boots on a UEFI-based system with Secure Boot enabled, the kernel loads onto the system key ring all public keys that are in the Secure Boot db key database, but not in the dbx database of revoked keys. The sections below describe different ways of importing a public key on a target system so that the system key ring is able to use the public key to authenticate a kernel module. To facilitate authentication of your kernel module on your systems, consider requesting your system vendor to incorporate your public key into the UEFI Secure Boot key database in their factory firmware image. The Machine Owner Key (MOK) facility feature can be used to expand the UEFI Secure Boot key database. When Red Hat Enterprise Linux 7 boots on a UEFI-enabled system with Secure Boot enabled, the keys on the MOK list are also added to the system key ring in addition to the keys from the key database. The MOK list keys are also stored persistently and securely in the same fashion as the Secure Boot database keys, but these are two separate facilities. The MOK facility is supported by , , , and the Red Hat Enterprise Linux 7 utility. Enrolling a MOK key requires manual interaction by a user at the UEFI system console on each target system. Nevertheless, the MOK facility provides a convenient method for testing newly generated key pairs and testing kernel modules signed with them. To add your public key to the MOK list:\n• None Request the addition of your public key to the MOK list: You will be asked to enter and confirm a password for this MOK enrollment request.\n• None The pending MOK key enrollment request will be noticed by and it will launch to allow you to complete the enrollment from the UEFI console.\n• None Enter the password you previously associated with this request and confirm the enrollment. Your public key is added to the MOK list, which is persistent. Once a key is on the MOK list, it will be automatically propagated to the system key ring on this and subsequent boots when UEFI Secure Boot is enabled. Assuming you have your kernel module ready:\n• None Use a Perl script to sign your kernel module with your private key: The Perl script requires that you provide both the files that contain your private and the public key as well as the kernel module file that you want to sign. Your kernel module is in ELF image format and the Perl script computes and appends the signature directly to the ELF image in your kernel module file. The utility can be used to display information about the kernel module’s signature, if it is present. For information on using , see Section 1.4, “Displaying information about a module”. The appended signature is not contained in an ELF image section and is not a formal part of the ELF image. Therefore, utilities such as will not be able to display the signature on your kernel module. Your kernel module is now ready for loading. Note that your signed kernel module is also loadable on systems where UEFI Secure Boot is disabled or on a non-UEFI system. That means you do not need to provide both a signed and unsigned version of your kernel module. Once your public key is enrolled and is in the system key ring, use to add your public key to the MOK list. Then manually load your kernel module with the command.\n• None Optionally, verify that your kernel module will not load before you have enrolled your public key. For details on how to list currently loaded kernel modules, see Section 1.3, “Listing currently-loaded modules”.\n• None Verify what keys have been added to the system key ring on the current boot: Since your public key has not been enrolled yet, it should not be displayed in the output of the command.\n• None Reboot, and complete the enrollment at the UEFI console:\n• None Verify the keys on the system key ring again:\n• None Copy the module into the directory of the kernel you want:\n• None Load the kernel module and verify that it was successfully loaded:\n• None Optionally, to load the module on boot, add it to the file:"
    },
    {
        "link": "https://sysprog21.github.io/lkmpg",
        "document": "The Linux Kernel Module Programming Guide is a free book; you may reproduce and/or modify it under the terms of the Open Software License, version 3.0.\n\nThis book is distributed in the hope that it would be useful, but without any warranty, without even the implied warranty of merchantability or fitness for a particular purpose.\n\nThe author encourages wide distribution of this book for personal or commercial use, provided the above copyright notice remains intact and the method adheres to the provisions of the Open Software License. In summary, you may copy and distribute this book free of charge or for a profit. No explicit permission is required from the author for reproduction of this book in any medium, physical or electronic.\n\nDerivative works and translations of this document must be placed under the Open Software License, and the original copyright notice must remain intact. If you have contributed new material to this book, you must make the material and source code available for your revisions. Please make revisions and updates available directly to the document maintainer, Jim Huang <jserv@ccns.ncku.edu.tw>. This will allow for the merging of updates and provide consistent revisions to the Linux community.\n\nIf you publish or distribute this book commercially, donations, royalties, and/or printed copies are greatly appreciated by the author and the Linux Documentation Project (LDP). Contributing in this way shows your support for free software and the LDP. If you have questions or comments, please contact the address above.\n\nThe Linux Kernel Module Programming Guide was initially authored by Ori Pomerantz for Linux v2.2. As the Linux kernel evolved, Ori’s availability to maintain the document diminished. Consequently, Peter Jay Salzman assumed the role of maintainer and updated the guide for Linux v2.4. Similar constraints arose for Peter when tracking developments in Linux v2.6, leading to Michael Burian joining as a co-maintainer to bring the guide up to speed with Linux v2.6. Bob Mottram contributed to the guide by updating examples for Linux v3.8 and later. Jim Huang then undertook the task of updating the guide for recent Linux versions (v5.0 and beyond), along with revising the LaTeX document.\n\nThe following people have contributed corrections or good suggestions:\n\nInvolvement in the development of Linux kernel modules requires a foundation in the C programming language and a track record of creating conventional programs intended for process execution. This pursuit delves into a domain where an unregulated pointer, if disregarded, may potentially trigger the total elimination of an entire file system, resulting in a scenario that necessitates a complete system reboot.\n\nA Linux kernel module is precisely defined as a code segment capable of dynamic loading and unloading within the kernel as needed. These modules enhance kernel capabilities without necessitating a system reboot. A notable example is seen in the device driver module, which facilitates kernel interaction with hardware components linked to the system. In the absence of modules, the prevailing approach leans toward monolithic kernels, requiring direct integration of new functionalities into the kernel image. This approach leads to larger kernels and necessitates kernel rebuilding and subsequent system rebooting when new functionalities are desired.\n\nLinux distributions provide the commands , and within a package.\n\nWhat Modules are in my Kernel?\n\nTo discover what modules are already loaded within your current kernel use the command .\n\nModules are stored within the file /proc/modules, so you can also see them with:\n\nThis can be a long list, and you might prefer to search for something particular. To search for the fat module:\n\nIs there a need to download and compile the kernel?\n\nTo effectively follow this guide, there is no obligatory requirement for performing such actions. Nonetheless, a prudent approach involves executing the examples within a test distribution on a virtual machine, thus mitigating any potential risk of disrupting the system.\n\nBefore delving into code, certain matters require attention. Variances exist among individuals’ systems, and distinct personal approaches are evident. The achievement of successful compilation and loading of the inaugural “hello world” program may, at times, present challenges. It is reassuring to note that overcoming the initial obstacle in the first attempt paves the way for subsequent endeavors to proceed seamlessly.\n• Modversioning. A module compiled for one kernel will not load if a different kernel is booted, unless is enabled in the kernel. Module versioning will be discussed later in this guide. Until module versioning is covered, the examples in this guide may not work correctly if running a kernel with modversioning turned on. However, most stock Linux distribution kernels come with modversioning enabled. If difficulties arise when loading the modules due to versioning errors, consider compiling a kernel with modversioning turned off.\n• Using X Window System. It is highly recommended to extract, compile, and load all the examples discussed in this guide from a console. Working on these tasks within the X Window System is discouraged. Modules cannot directly print to the screen like can, but they can log information and warnings that are eventually displayed on the screen, specifically within a console. If a module is loaded from an , the information and warnings will be logged, but solely within the systemd journal. These logs will not be visible unless consulting the . Refer to 4 for more information. For instant access to this information, it is advisable to perform all tasks from the console.\n• SecureBoot. Numerous modern computers arrive pre-configured with UEFI SecureBoot enabled—an essential security standard ensuring booting exclusively through trusted software endorsed by the original equipment manufacturer. Certain Linux distributions even ship with the default Linux kernel configured to support SecureBoot. In these cases, the kernel module necessitates a signed security key. Failing this, an attempt to insert your first “hello world” module would result in the message: “ERROR: could not insert module”. If this message Lockdown: insmod: unsigned module loading is restricted; see man kernel lockdown.7 appears in the output, the simplest approach involves disabling UEFI SecureBoot from the boot menu of your PC or laptop, allowing the successful insertion of “hello world” module. Naturally, an alternative involves undergoing intricate procedures such as generating keys, system key installation, and module signing to achieve functionality. However, this intricate process is less appropriate for beginners. If interested, more detailed steps for SecureBoot can be explored and followed.\n\nBefore building anything, it is necessary to install the header files for the kernel.\n\nThe following command provides information on the available kernel header files. Then for example:\n\nAll the examples from this document are available within the examples subdirectory.\n\nShould compile errors occur, it may be due to a more recent kernel version being in use, or there might be a need to install the corresponding kernel header files.\n\nMost individuals beginning their programming journey typically start with some variant of a hello world example. It is unclear what the outcomes are for those who deviate from this tradition, but it seems prudent to adhere to it. The learning process will begin with a series of hello world programs that illustrate various fundamental aspects of writing a kernel module.\n\nPresented next is the simplest possible module.\n\nPaste this into your favorite editor and save it as hello-1.c:\n\nNow you will need a Makefile. If you copy and paste this, change the indentation to use tabs, not spaces.\n\nIn Makefile, $(CURDIR) can set to the absolute pathname of the current working directory(after all -C options are processed, if any). See more about CURDIR in GNU make manual.\n\nIf there is no PWD := $(CURDIR) statement in Makefile, then it may not compile correctly with sudo make. Because some environment variables are specified by the security policy, they can’t be inherited. The default security policy is sudoers. In the sudoers security policy, env_reset is enabled by default, which restricts environment variables. Specifically, path variables are not retained from the user environment, they are set to default values (For more information see: sudoers manual). You can see the environment variable settings by:\n\nHere is a simple Makefile as an example to demonstrate the problem mentioned above.\n\nThen, we can use -p flag to print out the environment variable values from the Makefile.\n\nThe PWD variable won’t be inherited with sudo.\n\nHowever, there are three ways to solve this problem.\n• You can use the -E flag to temporarily preserve them.\n• You can set the env_reset disabled by editing the /etc/sudoers with root and visudo. ## Change env_reset to !env_reset in previous line to keep all environment variables You can view and compare these logs to find differences between env_reset and !env_reset.\n• You can preserve environment variables by appending them to env_keep in /etc/sudoers. After applying the above change, you can check the environment variable settings by:\n\nIf all goes smoothly you should then find that you have a compiled hello-1.ko module. You can find info on it with the command:\n\nAt this point the command:\n\nshould return nothing. You can try loading your shiny new module with:\n\nThe dash character will get converted to an underscore, so when you again try:\n\nYou should now see your loaded module. It can be removed again with:\n\nNotice that the dash was replaced by an underscore. To see what just happened in the logs:\n\nYou now know the basics of creating, compiling, installing and removing modules. Now for more of a description of how this module works.\n\nKernel modules must have at least two functions: a \"start\" (initialization) function called which is called when the module is ed into the kernel, and an \"end\" (cleanup) function called which is called just before it is removed from the kernel. Actually, things have changed starting with kernel 2.3.13. You can now use whatever name you like for the start and end functions of a module, and you will learn how to do this in Section 4.2. In fact, the new method is the preferred method. However, many people still use and for their start and end functions.\n\nTypically, either registers a handler for something with the kernel, or it replaces one of the kernel functions with its own code (usually code to do something and then call the original function). The function is supposed to undo whatever did, so the module can be unloaded safely.\n\nLastly, every kernel module needs to include <linux/module.h>. We needed to include <linux/printk.h> only for the macro expansion for the log level, which you’ll learn about in Section 2.\n• A point about coding style. Another thing which may not be immediately obvious to anyone getting started with kernel programming is that indentation within your code should be using and . It is one of the coding conventions of the kernel. You may not like it, but you’ll need to get used to it if you ever submit a patch upstream.\n• Introducing print macros. In the beginning there was , usually followed by a priority such as or . More recently this can also be expressed in abbreviated form using a set of print macros, such as and . This just saves some mindless keyboard bashing and looks a bit neater. They can be found within include/linux/printk.h. Take time to read through the available priority macros.\n• About Compiling. Kernel modules need to be compiled a bit differently from regular userspace apps. Former kernel versions required us to care much about these settings, which are usually stored in Makefiles. Although hierarchically organized, many redundant settings accumulated in sublevel Makefiles and made them large and rather difficult to maintain. Fortunately, there is a new way of doing these things, called kbuild, and the build process for external loadable modules is now fully integrated into the standard kernel build mechanism. To learn more on how to compile modules which are not part of the official kernel (such as all the examples you will find in this guide), see file Documentation/kbuild/modules.rst. Additional details about Makefiles for kernel modules are available in Documentation/kbuild/makefiles.rst. Be sure to read this and the related files before starting to hack Makefiles. It will probably save you lots of work. Here is another exercise for the reader. See that comment above the return statement in ? Change the return value to something negative, recompile and load the module again. What happens?\n\nIn early kernel versions you had to use the and functions, as in the first hello world example, but these days you can name those anything you want by using the and macros. These macros are defined in include/linux/module.h. The only requirement is that your init and cleanup functions must be defined before calling those macros, otherwise you’ll get compilation errors. Here is an example of this technique:\n\nSo now we have two real kernel modules under our belt. Adding another module is as simple as this:\n\nNow have a look at drivers/char/Makefile for a real world example. As you can see, some things got hardwired into the kernel (obj-y) but where have all those obj-m gone? Those familiar with shell scripts will easily be able to spot them. For those who are not, the obj-$(CONFIG_FOO) entries you see everywhere expand into obj-y or obj-m, depending on whether the CONFIG_FOO variable has been set to y or m. While we are at it, those were exactly the kind of variables that you have set in the .config file in the top-level directory of Linux kernel source tree, the last time when you said or something like that.\n\nThe macro causes the init function to be discarded and its memory freed once the init function finishes for built-in drivers, but not loadable modules. If you think about when the init function is invoked, this makes perfect sense.\n\nThere is also an which works similarly to but for init variables rather than functions.\n\nThe macro causes the omission of the function when the module is built into the kernel, and like , has no effect for loadable modules. Again, if you consider when the cleanup function runs, this makes complete sense; built-in drivers do not need a cleanup function, while loadable modules do.\n\nThese macros are defined in include/linux/init.h and serve to free up kernel memory. When you boot your kernel and see something like Freeing unused kernel memory: 236k freed, this is precisely what the kernel is freeing.\n\nHonestly, who loads or even cares about proprietary modules? If you do then you might have seen something like this:\n\nYou can use a few macros to indicate the license for your module. Some examples are \"GPL\", \"GPL v2\", \"GPL and additional rights\", \"Dual BSD/GPL\", \"Dual MIT/GPL\", \"Dual MPL/GPL\" and \"Proprietary\". They are defined within include/linux/module.h.\n\nTo reference what license you’re using a macro is available called . This and a few other macros describing the module are illustrated in the below example.\n\nModules can take command line arguments, but not with the argc/argv you might be used to.\n\nTo allow arguments to be passed to your module, declare the variables that will take the values of the command line arguments as global and then use the macro, (defined in include/linux/moduleparam.h) to set the mechanism up. At runtime, will fill the variables with any command line arguments that are given, like . The variable declarations and macros should be placed at the beginning of the module for clarity. The example code should clear up my admittedly lousy explanation.\n\nThe macro takes 3 arguments: the name of the variable, its type and permissions for the corresponding file in sysfs. Integer types can be signed as usual or unsigned. If you’d like to use arrays of integers or strings see and .\n\nArrays are supported too, but things are a bit different now than they were in the olden days. To keep track of the number of parameters you need to pass a pointer to a count variable as third parameter. At your option, you could also ignore the count and pass instead. We show both possibilities here:\n\nA good use for this is to have the module variable’s default values set, like a port or IO address. If the variables contain the default values, then perform autodetection (explained elsewhere). Otherwise, keep the current value. This will be made clear later on.\n\nLastly, there is a macro function, , that is used to document arguments that the module can take. It takes two parameters: a variable name and a free form string describing that variable.\n\nIt is recommended to experiment with the following code:\n\nSometimes it makes sense to divide a kernel module between several source files.\n\nHere is an example of such a kernel module.\n\nThis is the complete makefile for all the examples we have seen so far. The first five lines are nothing special, but for the last example we will need two lines. First we invent an object name for our combined module, second we tell what object files are part of that module.\n\nObviously, we strongly suggest you to recompile your kernel, so that you can enable a number of useful debugging features, such as forced module unloading ( ): when this option is enabled, you can force the kernel to unload a module even when it believes it is unsafe, via a command. This option can save you a lot of time and a number of reboots during the development of a module. If you do not want to recompile your kernel then you should consider running the examples within a test distribution on a virtual machine. If you mess anything up then you can easily reboot or restore the virtual machine (VM).\n\nThere are a number of cases in which you may want to load your module into a precompiled running kernel, such as the ones shipped with common Linux distributions, or a kernel you have compiled in the past. In certain circumstances you could require to compile and insert a module into a running kernel which you are not allowed to recompile, or on a machine that you prefer not to reboot. If you can’t think of a case that will force you to use modules for a precompiled kernel you might want to skip this and treat the rest of this chapter as a big footnote.\n\nNow, if you just install a kernel source tree, use it to compile your kernel module and you try to insert your module into the kernel, in most cases you would obtain an error as follows:\n\nLess cryptic information is logged to the systemd journal:\n\nIn other words, your kernel refuses to accept your module because version strings (more precisely, version magic, see include/linux/vermagic.h) do not match. Incidentally, version magic strings are stored in the module object in the form of a static string, starting with . Version data are inserted in your module when it is linked against the kernel/module.o file. To inspect version magics and other strings stored in a given module, issue the command :\n\nTo overcome this problem we could resort to the --force-vermagic option, but this solution is potentially unsafe, and unquestionably unacceptable in production modules. Consequently, we want to compile our module in an environment which was identical to the one in which our precompiled kernel was built. How to do this, is the subject of the remainder of this chapter.\n\nFirst of all, make sure that a kernel source tree is available, having exactly the same version as your current kernel. Then, find the configuration file which was used to compile your precompiled kernel. Usually, this is available in your current boot directory, under a name like config-5.14.x. You may just want to copy it to your kernel source tree: .\n\nLet’s focus again on the previous error message: a closer look at the version magic strings suggests that, even with two configuration files which are exactly the same, a slight difference in the version magic could be possible, and it is sufficient to prevent insertion of the module into the kernel. That slight difference, namely the custom string which appears in the module’s version magic and not in the kernel’s one, is due to a modification with respect to the original, in the makefile that some distributions include. Then, examine your Makefile, and make sure that the specified version information matches exactly the one used for your current kernel. For example, your makefile could start as follows:\n\nIn this case, you need to restore the value of symbol EXTRAVERSION to -rc2. We suggest keeping a backup copy of the makefile used to compile your kernel available in /lib/modules/5.14.0-rc2/build. A simple command as following should suffice.\n\nHere is the Linux kernel source you are attempting to build.\n\nNow, please run to update configuration and version headers and objects:\n\nIf you do not desire to actually compile the kernel, you can interrupt the build process (CTRL-C) just after the SPLIT line, because at that time, the files you need are ready. Now you can turn back to the directory of your module and compile it: It will be built exactly according to your current kernel settings, and it will load into it without any errors.\n\nA typical program starts with a function, executes a series of instructions, and terminates after completing these instructions. Kernel modules, however, follow a different pattern. A module always begins with either the function or a function designated by the call. This function acts as the module’s entry point, informing the kernel of the module’s functionalities and preparing the kernel to utilize the module’s functions when necessary. After performing these tasks, the entry function returns, and the module remains inactive until the kernel requires its code.\n\nAll modules conclude by invoking either or a function specified through the call. This serves as the module’s exit function, reversing the actions of the entry function by unregistering the previously registered functionalities.\n\nIt is mandatory for every module to have both an entry and an exit function. While there are multiple methods to define these functions, the terms “entry function” and “exit function” are generally used. However, they may occasionally be referred to as and , which are understood to mean the same.\n\nProgrammers use functions they do not define all the time. A prime example of this is . You use these library functions which are provided by the standard C library, libc. The definitions for these functions do not actually enter your program until the linking stage, which ensures that the code (for for example) is available, and fixes the call instruction to point to that code.\n\nKernel modules are different here, too. In the hello world example, you might have noticed that we used a function, but did not include a standard I/O library. That is because modules are object files whose symbols get resolved upon running or . The definition for the symbols comes from the kernel itself; the only external functions you can use are the ones provided by the kernel. If you’re curious about what symbols have been exported by your kernel, take a look at /proc/kallsyms.\n\nOne point to keep in mind is the difference between library functions and system calls. Library functions are higher level, run completely in user space and provide a more convenient interface for the programmer to the functions that do the real work — system calls. System calls run in kernel mode on the user’s behalf and are provided by the kernel itself. The library function may look like a very general printing function, but all it really does is format the data into strings and write the string data using the low-level system call , which then sends the data to standard output.\n\nWould you like to see what system calls are made by ? It is easy! Compile the following program:\n\nwith . Run the executable with . Are you impressed? Every line you see corresponds to a system call. strace is a handy program that gives you details about what system calls a program is making, including which call is made, what its arguments are and what it returns. It is an invaluable tool for figuring out things like what files a program is trying to access. Towards the end, you will see a line which looks like . There it is. The face behind the mask. You may not be familiar with write, since most people use library functions for file I/O (like , , ). If that is the case, try looking at man 2 write. The 2nd man section is devoted to system calls (like and ). The 3rd man section is devoted to library calls, which you would probably be more familiar with (like and ).\n\nYou can even write modules to replace the kernel’s system calls, which we will do shortly. Crackers often make use of this sort of thing for backdoors or trojans, but you can write your own modules to do more benign things, like have the kernel write Tee hee, that tickles! every time someone tries to delete a file on your system.\n\nThe kernel primarily manages access to resources, be it a video card, hard drive, or memory. Programs frequently vie for the same resources. For instance, as a document is saved, updatedb might commence updating the locate database. Sessions in editors like vim and processes like updatedb can simultaneously utilize the hard drive. The kernel’s role is to maintain order, ensuring that users do not access resources indiscriminately.\n\nTo manage this, CPUs operate in different modes, each offering varying levels of system control. The Intel 80386 architecture, for example, featured four such modes, known as rings. Unix, however, utilizes only two of these rings: the highest ring (ring 0, also known as “supervisor mode”, where all actions are permissible) and the lowest ring, referred to as “user mode”.\n\nRecall the discussion about library functions vs system calls. Typically, you use a library function in user mode. The library function calls one or more system calls, and these system calls execute on the library function’s behalf, but do so in supervisor mode since they are part of the kernel itself. Once the system call completes its task, it returns and execution gets transferred back to user mode.\n\nWhen you write a small C program, you use variables which are convenient and make sense to the reader. If, on the other hand, you are writing routines which will be part of a bigger problem, any global variables you have are part of a community of other peoples’ global variables; some of the variable names can clash. When a program has lots of global variables which aren’t meaningful enough to be distinguished, you get namespace pollution. In large projects, effort must be made to remember reserved names, and to find ways to develop a scheme for naming unique variable names and symbols.\n\nWhen writing kernel code, even the smallest module will be linked against the entire kernel, so this is definitely an issue. The best way to deal with this is to declare all your variables as static and to use a well-defined prefix for your symbols. By convention, all kernel prefixes are lowercase. If you do not want to declare everything as static, another option is to declare a symbol table and register it with the kernel. We will get to this later.\n\nThe file /proc/kallsyms holds all the symbols that the kernel knows about and which are therefore accessible to your modules since they share the kernel’s codespace.\n\nMemory management is a very complicated subject and the majority of O’Reilly’s Understanding The Linux Kernel exclusively covers memory management! We are not setting out to be experts on memory managements, but we do need to know a couple of facts to even begin worrying about writing real modules.\n\nIf you have not thought about what a segfault really means, you may be surprised to hear that pointers do not actually point to memory locations. Not real ones, anyway. When a process is created, the kernel sets aside a portion of real physical memory and hands it to the process to use for its executing code, variables, stack, heap and other things which a computer scientist would know about. This memory begins with 0x00000000 and extends up to whatever it needs to be. Since the memory space for any two processes do not overlap, every process that can access a memory address, say 0xbffff978, would be accessing a different location in real physical memory! The processes would be accessing an index named 0xbffff978 which points to some kind of offset into the region of memory set aside for that particular process. For the most part, a process like our Hello, World program can’t access the space of another process, although there are ways which we will talk about later.\n\nThe kernel has its own space of memory as well. Since a module is code which can be dynamically inserted and removed in the kernel (as opposed to a semi-autonomous object), it shares the kernel’s codespace rather than having its own. Therefore, if your module segfaults, the kernel segfaults. And if you start writing over data because of an off-by-one error, then you’re trampling on kernel data (or code). This is even worse than it sounds, so try your best to be careful.\n\nIt should be noted that the aforementioned discussion applies to any operating system utilizing a monolithic kernel. This concept differs slightly from “building all your modules into the kernel”, although the underlying principle is similar. In contrast, there are microkernels, where modules are allocated their own code space. Two notable examples of microkernels include the GNU Hurd and the Zircon kernel of Google’s Fuchsia.\n\nOne class of module is the device driver, which provides functionality for hardware like a serial port. On Unix, each piece of hardware is represented by a file located in /dev named a device file which provides the means to communicate with the hardware. The device driver provides the communication on behalf of a user program. So the es1370.ko sound card device driver might connect the /dev/sound device file to the Ensoniq IS1370 sound card. A userspace program like mp3blaster can use /dev/sound without ever knowing what kind of sound card is installed.\n\nLet’s look at some device files. Here are device files which represent the first three partitions on the primary master IDE hard drive:\n\nNotice the column of numbers separated by a comma. The first number is called the device’s major number. The second number is the minor number. The major number tells you which driver is used to access the hardware. Each driver is assigned a unique major number; all device files with the same major number are controlled by the same driver. All the above major numbers are 3, because they’re all controlled by the same driver.\n\nThe minor number is used by the driver to distinguish between the various hardware it controls. Returning to the example above, although all three devices are handled by the same driver they have unique minor numbers because the driver sees them as being different pieces of hardware.\n\nDevices are divided into two types: character devices and block devices. The difference is that block devices have a buffer for requests, so they can choose the best order in which to respond to the requests. This is important in the case of storage devices, where it is faster to read or write sectors which are close to each other, rather than those which are further apart. Another difference is that block devices can only accept input and return output in blocks (whose size can vary according to the device), whereas character devices are allowed to use as many or as few bytes as they like. Most devices in the world are character, because they don’t need this type of buffering, and they don’t operate with a fixed block size. You can tell whether a device file is for a block device or a character device by looking at the first character in the output of . If it is ‘b’ then it is a block device, and if it is ‘c’ then it is a character device. The devices you see above are block devices. Here are some character devices (the serial ports):\n\nIf you want to see which major numbers have been assigned, you can look at Documentation/admin-guide/devices.txt.\n\nWhen the system was installed, all of those device files were created by the command. To create a new char device named coffee with major/minor number 12 and 2, simply do . You do not have to put your device files into /dev, but it is done by convention. Linus put his device files in /dev, and so should you. However, when creating a device file for testing purposes, it is probably OK to place it in your working directory where you compile the kernel module. Just be sure to put it in the right place when you’re done writing the device driver.\n\nA few final points, although implicit in the previous discussion, are worth stating explicitly for clarity. When a device file is accessed, the kernel utilizes the file’s major number to identify the appropriate driver for handling the access. This indicates that the kernel does not necessarily rely on or need to be aware of the minor number. It is the driver that concerns itself with the minor number, using it to differentiate between various pieces of hardware.\n\nIt is important to note that when referring to “hardware”, the term is used in a slightly more abstract sense than just a physical PCI card that can be held in hand. Consider the following two device files:\n\nBy now you can look at these two device files and know instantly that they are block devices and are handled by same driver (block major 8). Sometimes two device files with the same major but different minor number can actually represent the same piece of physical hardware. So just be aware that the word “hardware” in our discussion can mean something very abstract.\n\nThe structure is defined in include/linux/fs.h, and holds pointers to functions defined by the driver that perform various operations on the device. Each field of the structure corresponds to the address of some function defined by the driver to handle a requested operation.\n\nFor example, every character driver needs to define a function that reads from the device. The structure holds the address of the module’s function that performs that operation. Here is what the definition looks like for kernel 5.4:\n\nSome operations are not implemented by a driver. For example, a driver that handles a video card will not need to read from a directory structure. The corresponding entries in the structure should be set to .\n\nThere is a gcc extension that makes assigning to this structure more convenient. You will see it in modern drivers, and may catch you by surprise. This is what the new way of assigning to the structure looks like:\n\nHowever, there is also a C99 way of assigning to elements of a structure, designated initializers, and this is definitely preferred over using the GNU extension. You should use this syntax in case someone wants to port your driver. It will help with compatibility:\n\nThe meaning is clear, and you should be aware that any member of the structure which you do not explicitly assign will be initialized to by gcc.\n\nAn instance of containing pointers to functions that are used to implement , , , … system calls is commonly named .\n\nSince Linux v3.14, the read, write and seek operations are guaranteed for thread-safe by using the specific lock, which makes the file position update to become the mutual exclusion. So, we can safely implement those operations without unnecessary locking.\n\nAdditionally, since Linux v5.6, the structure was introduced to replace the use of the structure when registering proc handlers. See more information in the 7.1 section.\n\nEach device is represented in the kernel by a file structure, which is defined in include/linux/fs.h. Be aware that a file is a kernel level structure and never appears in a user space program. It is not the same thing as a , which is defined by glibc and would never appear in a kernel space function. Also, its name is a bit misleading; it represents an abstract open ‘file’, not a file on a disk, which is represented by a structure named .\n\nAn instance of struct file is commonly named . You’ll also see it referred to as a struct file object. Resist the temptation.\n\nGo ahead and look at the definition of file. Most of the entries you see, like struct dentry are not used by device drivers, and you can ignore them. This is because drivers do not fill file directly; they only use structures contained in file which are created elsewhere.\n\nAs discussed earlier, char devices are accessed through device files, usually located in /dev. This is by convention. When writing a driver, it is OK to put the device file in your current directory. Just make sure you place it in /dev for a production driver. The major number tells you which driver handles which device file. The minor number is used only by the driver itself to differentiate which device it is operating on, just in case the driver handles more than one device.\n\nAdding a driver to your system means registering it with the kernel. This is synonymous with assigning it a major number during the module’s initialization. You do this by using the function, defined by include/linux/fs.h.\n\nWhere unsigned int major is the major number you want to request, is the name of the device as it will appear in /proc/devices and is a pointer to the table for your driver. A negative return value means the registration failed. Note that we didn’t pass the minor number to . That is because the kernel doesn’t care about the minor number; only our driver uses it.\n\nNow the question is, how do you get a major number without hijacking one that’s already in use? The easiest way would be to look through Documentation/admin-guide/devices.txt and pick an unused one. That is a bad way of doing things because you will never be sure if the number you picked will be assigned later. The answer is that you can ask the kernel to assign you a dynamic major number.\n\nIf you pass a major number of 0 to , the return value will be the dynamically allocated major number. The downside is that you can not make a device file in advance, since you do not know what the major number will be. There are a couple of ways to do this. First, the driver itself can print the newly assigned number and we can make the device file by hand. Second, the newly registered device will have an entry in /proc/devices, and we can either make the device file by hand or write a shell script to read the file in and make the device file. The third method is that we can have our driver make the device file using the function after a successful registration and during the call to .\n\nHowever, would occupy a range of minor numbers associated with the given major. The recommended way to reduce waste for char device registration is using cdev interface.\n\nThe newer interface completes the char device registration in two distinct steps. First, we should register a range of device numbers, which can be completed with or .\n\nThe choice between two different functions depends on whether you know the major numbers for your device. Using if you know the device major number and if you would like to allocate a dynamically-allocated major number.\n\nSecond, we should initialize the data structure for our char device and associate it with the device numbers. To initialize the , we can achieve by the similar sequence of the following codes.\n\nHowever, the common usage pattern will embed the within a device-specific structure of your own. In this case, we’ll need for the initialization.\n\nOnce we finish the initialization, we can add the char device to the system by using the .\n\nTo find an example using the interface, you can see ioctl.c described in section 9.\n\nWe can not allow the kernel module to be ’ed whenever root feels like it. If the device file is opened by a process and then we remove the kernel module, using the file would cause a call to the memory location where the appropriate function (read/write) used to be. If we are lucky, no other code was loaded there, and we’ll get an ugly error message. If we are unlucky, another kernel module was loaded into the same location, which means a jump into the middle of another function within the kernel. The results of this would be impossible to predict, but they can not be very positive.\n\nNormally, when you do not want to allow something, you return an error code (a negative number) from the function which is supposed to do it. With that’s impossible because it is a void function. However, there is a counter which keeps track of how many processes are using your module. You can see what its value is by looking at the 3rd field with the command or . If this number isn’t zero, will fail. Note that you do not have to check the counter within because the check will be performed for you by the system call , defined in include/linux/syscalls.h. You should not use this counter directly, but there are functions defined in include/linux/module.h which let you increase, decrease and display this counter:\n• : Return the value of reference count of current module.\n\nIt is important to keep the counter accurate; if you ever do lose track of the correct usage count, you will never be able to unload the module; it’s now reboot time, boys and girls. This is bound to happen to you sooner or later during a module’s development.\n\nThe next code sample creates a char driver named chardev. You can dump its device file.\n\n(or open the file with a program) and the driver will put the number of times the device file has been read from into the file. We do not support writing to the file (like ), but catch these attempts and tell the user that the operation is not supported. Don’t worry if you don’t see what we do with the data we read into the buffer; we don’t do much with it. We simply read in the data and print a message acknowledging that we received it.\n\nIn the multiple-threaded environment, without any protection, concurrent access to the same memory may lead to the race condition, and will not preserve the performance. In the kernel module, this problem may happen due to multiple instances accessing the shared resources. Therefore, a solution is to enforce the exclusive access. We use atomic Compare-And-Swap (CAS) to maintain the states, and , to determine whether the file is currently opened by someone or not. CAS compares the contents of a memory location with the expected value and, only if they are the same, modifies the contents of that memory location to the desired value. See more concurrency details in the 12 section.\n\nThe system calls, which are the major interface the kernel shows to the processes, generally stay the same across versions. A new system call may be added, but usually the old ones will behave exactly like they used to. This is necessary for backward compatibility – a new kernel version is not supposed to break regular processes. In most cases, the device files will also remain the same. On the other hand, the internal interfaces within the kernel can and do change between versions.\n\nThere are differences between different kernel versions, and if you want to support multiple kernel versions, you will find yourself having to code conditional compilation directives. The way to do this to compare the macro to the macro . In version a.b.c of the kernel, the value of this macro would be .\n\nIn Linux, there is an additional mechanism for the kernel and kernel modules to send information to processes — the /proc file system. Originally designed to allow easy access to information about processes (hence the name), it is now used by every bit of the kernel which has something interesting to report, such as /proc/modules which provides the list of modules and /proc/meminfo which gathers memory usage statistics.\n\nThe method to use the proc file system is very similar to the one used with device drivers — a structure is created with all the information needed for the /proc file, including pointers to any handler functions (in our case there is only one, the one called when somebody attempts to read from the /proc file). Then, registers the structure with the kernel and unregisters it.\n\nNormal file systems are located on a disk, rather than just in memory (which is where /proc is), and in that case the index-node (inode for short) number is a pointer to a disk location where the file’s inode is located. The inode contains information about the file, for example the file’s permissions, together with a pointer to the disk location or locations where the file’s data can be found.\n\nBecause we don’t get called when the file is opened or closed, there’s nowhere for us to put and in this module, and if the file is opened and then the module is removed, there’s no way to avoid the consequences.\n\nHere a simple example showing how to use a /proc file. This is the HelloWorld for the /proc filesystem. There are three parts: create the file /proc/helloworld in the function , return a value (and a buffer) when the file /proc/helloworld is read in the callback function , and delete the file /proc/helloworld in the function .\n\nThe /proc/helloworld is created when the module is loaded with the function . The return value is a pointer to , and it will be used to configure the file /proc/helloworld (for example, the owner of this file). A null return value means that the creation has failed.\n\nEvery time the file /proc/helloworld is read, the function is called. Two parameters of this function are very important: the buffer (the second parameter) and the offset (the fourth one). The content of the buffer will be returned to the application which read it (for example the command). The offset is the current position in the file. If the return value of the function is not null, then this function is called again. So be careful with this function, if it never returns zero, the read function is called endlessly.\n\nThe structure is defined in include/linux/proc_fs.h in Linux v5.6+. In older kernels, it used for custom hooks in /proc file system, but it contains some members that are unnecessary in VFS, and every time VFS expands set, /proc code comes bloated. On the other hand, not only the space, but also some operations were saved by this structure to improve its performance. For example, the file which never disappears in /proc can set the as to save 2 atomic ops, 1 allocation, 1 free in per open/read/close sequence.\n\nWe have seen a very simple example for a /proc file where we only read the file /proc/helloworld. It is also possible to write in a /proc file. It works the same way as read, a function is called when the /proc file is written. But there is a little difference with read, data comes from user, so you have to import data from user space to kernel space (with or )\n\nThe reason for or is that Linux memory (on Intel architecture, it may be different under some other processors) is segmented. This means that a pointer, by itself, does not reference a unique location in memory, only a location in a memory segment, and you need to know which memory segment it is to be able to use it. There is one memory segment for the kernel, and one for each of the processes.\n\nThe only memory segment accessible to a process is its own, so when writing regular programs to run as processes, there is no need to worry about segments. When you write a kernel module, normally you want to access the kernel memory segment, which is handled automatically by the system. However, when the content of a memory buffer needs to be passed between the currently running process and the kernel, the kernel function receives a pointer to the memory buffer which is in the process segment. The and macros allow you to access that memory. These functions handle only one character, you can handle several characters with and . As the buffer (in read or write function) is in kernel space, for write function you need to import data because it comes from user space, but not for the read function because data is already in kernel space.\n\nWe have seen how to read and write a /proc file with the /proc interface. But it is also possible to manage /proc file with inodes. The main concern is to use advanced functions, like permissions.\n\nIn Linux, there is a standard mechanism for file system registration. Since every file system has to have its own functions to handle inode and file operations, there is a special structure to hold pointers to all those functions, , which includes a pointer to .\n\nThe difference between file and inode operations is that file operations deal with the file itself whereas inode operations deal with ways of referencing the file, such as creating links to it.\n\nIn /proc, whenever we register a new file, we’re allowed to specify which will be used to access to it. This is the mechanism we use, a which includes a pointer to a which includes pointers to our and functions.\n\nAnother interesting point here is the function. This function is called whenever a process tries to do something with the /proc file, and it can decide whether to allow access or not. Right now it is only based on the operation and the uid of the current user (as available in current, a pointer to a structure which includes information on the currently running process), but it could be based on anything we like, such as what other processes are doing with the same file, the time of day, or the last input we received.\n\nIt is important to note that the standard roles of read and write are reversed in the kernel. Read functions are used for output, whereas write functions are used for input. The reason for that is that read and write refer to the user’s point of view — if a process reads something from the kernel, then the kernel needs to output it, and if a process writes something to the kernel, then the kernel receives it as input.\n\nStill hungry for procfs examples? Well, first of all keep in mind, there are rumors around, claiming that procfs is on its way out, consider using sysfs instead. Consider using this mechanism, in case you want to document something kernel related yourself.\n\nAs we have seen, writing a /proc file may be quite “complex”. So to help people writing /proc file, there is an API named that helps formatting a /proc file for output. It is based on sequence, which is composed of 3 functions: , , and . The API starts a sequence when a user read the /proc file.\n\nA sequence begins with the call of the function . If the return is a non value, the function is called; otherwise, the function is called directly. This function is an iterator, the goal is to go through all the data. Each time is called, the function is also called. It writes data values in the buffer read by the user. The function is called until it returns . The sequence ends when returns , then the function is called.\n\nBE CAREFUL: when a sequence is finished, another one starts. That means that at the end of function , the function is called again. This loop finishes when the function returns . You can see a scheme of this in the Figure 1.\n\nThe provides basic functions for , such as , , and some others. But nothing to write in the /proc file. Of course, you can still use the same way as in the previous example.\n\nIf you want more information, you can read this web page:\n\nYou can also read the code of fs/seq_file.c in the linux kernel.\n\nsysfs allows you to interact with the running kernel from userspace by reading or setting variables inside of modules. This can be useful for debugging purposes, or just as an interface for applications or scripts. You can find sysfs directories and files under the /sys directory on your system.\n\nAttributes can be exported for kobjects in the form of regular files in the filesystem. Sysfs forwards file I/O operations to methods defined for the attributes, providing a means to read and write kernel attributes.\n\nFor example, the driver model defines like:\n\nTo read or write attributes, or method must be specified when declaring the attribute. For the common cases include/linux/sysfs.h provides convenience macros ( , , , etc.) to make defining attributes easier as well as making code more concise and readable.\n\nAn example of a hello world module which includes the creation of a variable accessible via sysfs is given below.\n\nWhat is the current value of ?\n\nSet the value of and check that it changed.\n\nIn the above case, we use a simple kobject to create a directory under sysfs, and communicate with its attributes. Since Linux v2.6.0, the structure made its appearance. It was initially meant as a simple way of unifying kernel code which manages reference counted objects. After a bit of mission creep, it is now the glue that holds much of the device model and its sysfs interface together. For more information about kobject and sysfs, see Documentation/driver-api/driver-model/driver.rst and https://lwn.net/Articles/51437/.\n\nDevice files are supposed to represent physical devices. Most physical devices are used for output as well as input, so there has to be some mechanism for device drivers in the kernel to get the output to send to the device from processes. This is done by opening the device file for output and writing to it, just like writing to a file. In the following example, this is implemented by .\n\nThis is not always enough. Imagine you had a serial port connected to a modem (even if you have an internal modem, it is still implemented from the CPU’s perspective as a serial port connected to a modem, so you don’t have to tax your imagination too hard). The natural thing to do would be to use the device file to write things to the modem (either modem commands or data to be sent through the phone line) and read things from the modem (either responses for commands or the data received through the phone line). However, this leaves open the question of what to do when you need to talk to the serial port itself, for example to configure the rate at which data is sent and received.\n\nThe answer in Unix is to use a special function called (short for Input Output ConTroL). Every device can have its own commands, which can be read ioctl’s (to send information from a process to the kernel), write ioctl’s (to return information to a process), both or neither. Notice here the roles of read and write are reversed again, so in ioctl’s read is to send information to the kernel and write is to receive information from the kernel.\n\nThe ioctl function is called with three parameters: the file descriptor of the appropriate device file, the ioctl number, and a parameter, which is of type long so you can use a cast to use it to pass anything. You will not be able to pass a structure this way, but you will be able to pass a pointer to the structure. Here is an example:\n\nYou can see there is an argument called in function. It is the ioctl number. The ioctl number encodes the major device number, the type of the ioctl, the command, and the type of the parameter. This ioctl number is usually created by a macro call ( , , or — depending on the type) in a header file. This header file should then be included both by the programs which will use ioctl (so they can generate the appropriate ioctl’s) and by the kernel module (so it can understand it). In the example below, the header file is chardev.h and the program which uses it is userspace_ioctl.c.\n\nIf you want to use ioctls in your own kernel modules, it is best to receive an official ioctl assignment, so if you accidentally get somebody else’s ioctls, or if they get yours, you’ll know something is wrong. For more information, consult the kernel source tree at Documentation/userspace-api/ioctl/ioctl-number.rst.\n\nAlso, we need to be careful that concurrent access to the shared resources will lead to the race condition. The solution is using atomic Compare-And-Swap (CAS), which we mentioned at 6.5 section, to enforce the exclusive access.\n\nSo far, the only thing we’ve done was to use well defined kernel mechanisms to register /proc files and device handlers. This is fine if you want to do something the kernel programmers thought you’d want, such as write a device driver. But what if you want to do something unusual, to change the behavior of the system in some way? Then, you are mostly on your own.\n\nShould one choose not to use a virtual machine, kernel programming can become risky. For example, while writing the code below, the system call was inadvertently disrupted. This resulted in an inability to open any files, run programs, or shut down the system, necessitating a restart of the virtual machine. Fortunately, no critical files were lost in this instance. However, if such modifications were made on a live, mission-critical system, the consequences could be severe. To mitigate the risk of file loss, even in a test environment, it is advised to execute right before using and .\n\nForget about /proc files, forget about device files. They are just minor details. Minutiae in the vast expanse of the universe. The real process to kernel communication mechanism, the one used by all processes, is system calls. When a process requests a service from the kernel (such as opening a file, forking to a new process, or requesting more memory), this is the mechanism used. If you want to change the behaviour of the kernel in interesting ways, this is the place to do it. By the way, if you want to see which system calls a program uses, run .\n\nIn general, a process is not supposed to be able to access the kernel. It can not access kernel memory and it can’t call kernel functions. The hardware of the CPU enforces this (that is the reason why it is called “protected mode” or “page protection”).\n\nSystem calls are an exception to this general rule. What happens is that the process fills the registers with the appropriate values and then calls a special instruction which jumps to a previously defined location in the kernel (of course, that location is readable by user processes, it is not writable by them). Under Intel CPUs, this is done by means of interrupt 0x80. The hardware knows that once you jump to this location, you are no longer running in restricted user mode, but as the operating system kernel — and therefore you’re allowed to do whatever you want.\n\nThe location in the kernel a process can jump to is called system_call. The procedure at that location checks the system call number, which tells the kernel what service the process requested. Then, it looks at the table of system calls ( ) to see the address of the kernel function to call. Then it calls the function, and after it returns, does a few system checks and then return back to the process (or to a different process, if the process time ran out). If you want to read this code, it is at the source file arch/$(architecture)/kernel/entry.S, after the line .\n\nSo, if we want to change the way a certain system call works, what we need to do is to write our own function to implement it (usually by adding a bit of our own code, and then calling the original function) and then change the pointer at to point to our function. Because we might be removed later and we don’t want to leave the system in an unstable state, it’s important for to restore the table to its original state.\n\nTo modify the content of , we need to consider the control register. A control register is a processor register that changes or controls the general behavior of the CPU. For x86 architecture, the cr0 register has various control flags that modify the basic operation of the processor. The WP flag in cr0 stands for write protection. Once the WP flag is set, the processor disallows further write attempts to the read-only sections Therefore, we must disable the WP flag before modifying . Since Linux v5.3, the function cannot be used because of the sensitive cr0 bits pinned by the security issue, the attacker may write into CPU control registers to disable CPU protections like write protection. As a result, we have to provide the custom assembly routine to bypass it.\n\nHowever, symbol is unexported to prevent misuse. But there have few ways to get the symbol, manual symbol lookup and . Here we use both depend on the kernel version.\n\nBecause of the control-flow integrity, which is a technique to prevent the redirect execution code from the attacker, for making sure that the indirect calls go to the expected addresses and the return addresses are not changed. Since Linux v5.7, the kernel patched the series of control-flow enforcement (CET) for x86, and some configurations of GCC, like GCC versions 9 and 10 in Ubuntu Linux, will add with CET (the -fcf-protection option) in the kernel by default. Using that GCC to compile the kernel with retpoline off may result in CET being enabled in the kernel. You can use the following command to check out the -fcf-protection option is enabled or not:\n\nBut CET should not be enabled in the kernel, it may break the Kprobes and bpf. Consequently, CET is disabled since v5.11. To guarantee the manual symbol lookup worked, we only use up to v5.4.\n\nUnfortunately, since Linux v5.7 is also unexported, it needs certain trick to get the address of . If is enabled, we can facilitate the retrieval of function addresses by means of Kprobes to dynamically break into the specific kernel routine. Kprobes inserts a breakpoint at the entry of function by replacing the first bytes of the probed instruction. When a CPU hits the breakpoint, registers are stored, and the control will pass to Kprobes. It passes the addresses of the saved registers and the Kprobe struct to the handler you defined, then executes it. Kprobes can be registered by symbol name or address. Within the symbol name, the address will be handled by the kernel.\n\nOtherwise, specify the address of from /proc/kallsyms and /boot/System.map into parameter. Following is the sample usage for /proc/kallsyms:\n\nUsing the address from /boot/System.map, be careful about KASLR (Kernel Address Space Layout Randomization). KASLR may randomize the address of kernel code and data at every boot time, such as the static address listed in /boot/System.map will offset by some entropy. The purpose of KASLR is to protect the kernel space from the attacker. Without KASLR, the attacker may find the target address in the fixed address easily. Then the attacker can use return-oriented programming to insert some malicious codes to execute or receive the target data by a tampered pointer. KASLR mitigates these kinds of attacks because the attacker cannot immediately know the target address, but a brute-force attack can still work. If the address of a symbol in /proc/kallsyms is different from the address in /boot/System.map, KASLR is enabled with the kernel, which your system running on.\n\nIf KASLR is enabled, we have to take care of the address from /proc/kallsyms each time we reboot the machine. In order to use the address from /boot/System.map, make sure that KASLR is disabled. You can add the nokaslr for disabling KASLR in next booting time:\n\nFor more information, check out the following:\n\nThe source code here is an example of such a kernel module. We want to “spy” on a certain user, and to a message whenever that user opens a file. Towards this end, we replace the system call to open a file with our own function, called . This function checks the uid (user’s id) of the current process, and if it is equal to the uid we spy on, it calls to display the name of the file to be opened. Then, either way, it calls the original function with the same parameters, to actually open the file.\n\nThe function replaces the appropriate location in and keeps the original pointer in a variable. The function uses that variable to restore everything back to normal. This approach is dangerous, because of the possibility of two kernel modules changing the same system call. Imagine we have two kernel modules, A and B. A’s openat system call will be and B’s will be . Now, when A is inserted into the kernel, the system call is replaced with , which will call the original when it is done. Next, B is inserted into the kernel, which replaces the system call with , which will call what it thinks is the original system call, , when it’s done.\n\nNow, if B is removed first, everything will be well — it will simply restore the system call to , which calls the original. However, if A is removed and then B is removed, the system will crash. A’s removal will restore the system call to the original, , cutting B out of the loop. Then, when B is removed, it will restore the system call to what it thinks is the original, , which is no longer in memory. At first glance, it appears we could solve this particular problem by checking if the system call is equal to our open function and if so not changing it at all (so that B won’t change the system call when it is removed), but that will cause an even worse problem. When A is removed, it sees that the system call was changed to so that it is no longer pointing to , so it will not restore it to before it is removed from memory. Unfortunately, will still try to call which is no longer there, so that even without removing B the system would crash.\n\nFor x86 architecture, the system call table cannot be used to invoke a system call after commit 1e3ad78 since v6.9. This commit has been backported to long term stable kernels, like v5.15.154+, v6.1.85+, v6.6.26+ and v6.8.5+, see this answer for more details. In this case, thanks to Kprobes, a hook can be used instead on the system call entry to intercept the system call.\n\nNote that all the related problems make syscall stealing unfeasible for production use. In order to keep people from doing potential harmful things is no longer exported. This means, if you want to do something more than a mere dry run of this example, you will have to patch your current kernel in order to have exported.\n\nWhat do you do when somebody asks you for something you can not do right away? If you are a human being and you are bothered by a human being, the only thing you can say is: \"Not right now, I’m busy. Go away!\". But if you are a kernel module and you are bothered by a process, you have another possibility. You can put the process to sleep until you can service it. After all, processes are being put to sleep by the kernel and woken up all the time (that is the way multiple processes appear to run on the same time on a single CPU).\n\nThis kernel module is an example of this. The file (called /proc/sleep) can only be opened by a single process at a time. If the file is already open, the kernel module calls . The easiest way to keep a file open is to open it with:\n\nThis function changes the status of the task (a task is the kernel data structure which holds information about a process and the system call it is in, if any) to , which means that the task will not run until it is woken up somehow, and adds it to WaitQ, the queue of tasks waiting to access the file. Then, the function calls the scheduler to context switch to a different process, one which has some use for the CPU.\n\nWhen a process is done with the file, it closes it, and is called. That function wakes up all the processes in the queue (there’s no mechanism to only wake up one of them). It then returns and the process which just closed the file can continue to run. In time, the scheduler decides that that process has had enough and gives control of the CPU to another process. Eventually, one of the processes which was in the queue will be given control of the CPU by the scheduler. It starts at the point right after the call to .\n\nThis means that the process is still in kernel mode - as far as the process is concerned, it issued the open system call and the system call has not returned yet. The process does not know somebody else used the CPU for most of the time between the moment it issued the call and the moment it returned.\n\nIt can then proceed to set a global variable to tell all the other processes that the file is still open and go on with its life. When the other processes get a piece of the CPU, they’ll see that global variable and go back to sleep.\n\nSo we will use to keep the file open in the background, while trying to access it with another process (again in the background, so that we need not switch to a different vt). As soon as the first background process is killed with kill %1 , the second is woken up, is able to access the file and finally terminates.\n\nTo make our life more interesting, does not have a monopoly on waking up the processes which wait to access the file. A signal, such as Ctrl +c (SIGINT) can also wake up a process. This is because we used . We could have used instead, but that would have resulted in extremely angry users whose Ctrl+c’s are ignored.\n\nIn that case, we want to return with immediately. This is important so users can, for example, kill the process before it receives the file.\n\nThere is one more point to remember. Some times processes don’t want to sleep, they want either to get what they want immediately, or to be told it cannot be done. Such processes use the flag when opening the file. The kernel is supposed to respond by returning with the error code from operations which would otherwise block, such as opening the file in this example. The program , available in the examples/other directory, can be used to open a file with .\n\nSometimes one thing should happen before another within a module having multiple threads. Rather than using commands, the kernel has another way to do this which allows timeouts or interrupts to also happen.\n\nCompletions as code synchronization mechanism have three main parts, initialization of struct completion synchronization object, the waiting or barrier part through , and the signalling side through a call to .\n\nIn the subsequent example, two threads are initiated: crank and flywheel. It is imperative that the crank thread starts before the flywheel thread. A completion state is established for each of these threads, with a distinct completion defined for both the crank and flywheel threads. At the exit point of each thread the respective completion state is updated, and is used by the flywheel thread to ensure that it does not begin prematurely. The crank thread uses the function to update the completion, which lets the flywheel thread continue.\n\nSo even though is started first you should notice when you load this module and run , that turning the crank always happens first because the flywheel thread waits for the crank thread to complete.\n\nThere are other variations of the function, which include timeouts or being interrupted, but this basic mechanism is enough for many common situations without adding a lot of complexity.\n\nIf processes running on different CPUs or in different threads try to access the same memory, then it is possible that strange things can happen or your system can lock up. To avoid this, various types of mutual exclusion kernel functions are available. These indicate if a section of code is \"locked\" or \"unlocked\" so that simultaneous attempts to run it can not happen.\n\nYou can use kernel mutexes (mutual exclusions) in much the same manner that you might deploy them in userland. This may be all that is needed to avoid collisions in most cases.\n\nAs the name suggests, spinlocks lock up the CPU that the code is running on, taking 100% of its resources. Because of this you should only use the spinlock mechanism around code which is likely to take no more than a few milliseconds to run and so will not noticeably slow anything down from the user’s point of view.\n\nThe example here is \"irq safe\" in that if interrupts happen during the lock then they will not be forgotten and will activate when the unlock happens, using the variable to retain their state.\n\nTaking 100% of a CPU’s resources comes with greater responsibility. Situations where the kernel code monopolizes a CPU are called atomic contexts. Holding a spinlock is one of those situations. Sleeping in atomic contexts may leave the system hanging, as the occupied CPU devotes 100% of its resources doing nothing but sleeping. In some worse cases the system may crash. Thus, sleeping in atomic contexts is considered a bug in the kernel. They are sometimes called “sleep-in-atomic-context” in some materials.\n\nNote that sleeping here is not limited to calling the sleep functions explicitly. If subsequent function calls eventually invoke a function that sleeps, it is also considered sleeping. Thus, it is important to pay attention to functions being used in atomic context. There’s no documentation recording all such functions, but code comments may help. Sometimes you may find comments in kernel source code stating that a function “may sleep”, “might sleep”, or more explicitly “the caller should not hold a spinlock”. Those comments are hints that a function may implicitly sleep and must not be called in atomic contexts.\n\nRead and write locks are specialised kinds of spinlocks so that you can exclusively read from something or write to something. Like the earlier spinlocks example, the one below shows an \"irq safe\" situation in which if other functions were triggered from irqs which might also read and write to whatever you are concerned with then they would not disrupt the logic. As before it is a good idea to keep anything done within the lock as short as possible so that it does not hang up the system and cause users to start revolting against the tyranny of your module.\n\nOf course, if you know for sure that there are no functions triggered by irqs which could possibly interfere with your logic then you can use the simpler and or the corresponding write functions.\n\nIf you are doing simple arithmetic: adding, subtracting or bitwise operations, then there is another way in the multi-CPU and multi-hyperthreaded world to stop other parts of the system from messing with your mojo. By using atomic operations you can be confident that your addition, subtraction or bit flip did actually happen and was not overwritten by some other shenanigans. An example is shown below.\n\nBefore the C11 standard adopts the built-in atomic types, the kernel already provided a small set of atomic types by using a bunch of tricky architecture-specific codes. Implementing the atomic types by C11 atomics may allow the kernel to throw away the architecture-specific codes and letting the kernel code be more friendly to the people who understand the standard. But there are some problems, such as the memory model of the kernel doesn’t match the model formed by the C11 atomics. For further details, see:\n\nIn Section 1.7, it was noted that the X Window System and kernel module programming are not conducive to integration. This remains valid during the development of kernel modules. However, in practical scenarios, the necessity emerges to relay messages to the tty (teletype) originating the module load command.\n\nThe term “tty” originates from teletype, which initially referred to a combined keyboard-printer for Unix system communication. Today, it signifies a text stream abstraction employed by Unix programs, encompassing physical terminals, xterms in X displays, and network connections like SSH.\n\nTo achieve this, the “current” pointer is leveraged to access the active task’s tty structure. Within this structure lies a pointer to a string write function, facilitating the string’s transmission to the tty.\n\nIn certain conditions, you may desire a simpler and more direct way to communicate to the external world. Flashing keyboard LEDs can be such a solution: It is an immediate way to attract attention or to display a status condition. Keyboard LEDs are present on every hardware, they are always visible, they do not need any setup, and their use is rather simple and non-intrusive, compared to writing to a tty or a file.\n\nFrom v4.14 to v4.15, the timer API made a series of changes to improve memory safety. A buffer overflow in the area of a structure may be able to overwrite the and fields, providing the attacker with a way to use return-oriented programming (ROP) to call arbitrary functions within the kernel. Also, the function prototype of the callback, containing a argument, will prevent work from any type checking. Furthermore, the function prototype with argument may be an obstacle to the forward-edge protection of control-flow integrity. Thus, it is better to use a unique prototype to separate from the cluster that takes an argument. The timer callback should be passed a pointer to the structure rather than an argument. Then, it wraps all the information the callback needs, including the structure, into a larger structure, and it can use the macro instead of the value. For more information see: Improving the kernel timers API.\n\nBefore Linux v4.14, was used to initialize the timer and the structure looked like:\n\nSince Linux v4.14, is adopted and the kernel step by step converting to from . One of the reasons why API was changed is it need to coexist with the old version interface. Moreover, the was implemented by at first.\n\nThe was then removed since v4.15. As a result, the structure had changed to the following.\n\nThe following source code illustrates a minimal kernel module which, when loaded, starts blinking the keyboard LEDs until it is unloaded.\n\nIf none of the examples in this chapter fit your debugging needs, there might yet be some other tricks to try. Ever wondered what in is good for? If you activate that you get low level access to the serial port. While this might not sound very powerful by itself, you can patch kernel/printk.c or any other essential syscall to print ASCII characters, thus making it possible to trace virtually everything what your code does over a serial line. If you find yourself porting the kernel to some new and former unsupported architecture, this is usually amongst the first things that should be implemented. Logging over a netconsole might also be worth a try.\n\nWhile you have seen lots of stuff that can be used to aid debugging here, there are some things to be aware of. Debugging is almost always intrusive. Adding debug code can change the situation enough to make the bug seem to disappear. Thus, you should keep debug code to a minimum and make sure it does not show up in production code.\n\nGeneral Purpose Input/Output (GPIO) appears on the development board as pins. It acts as a bridge for communication between the development board and external devices. You can think of it like a switch: users can turn it on or off (Input), and the development board can also turn it on or off (Output).\n\nTo implement a GPIO device driver, you use the function to enable a specific GPIO pin. After successfully enabling it, you can check that the pin is being used by looking at /sys/kernel/debug/gpio.\n\nThere are other ways to register GPIOs. For example, you can use to register a GPIO while setting its direction (input or output) and initial state at the same time. You can also use to register multiple GPIOs at once. However, note that has been removed since Linux v6.10+.\n\nWhen using GPIO, you must set it as either output with or input with .\n• when the GPIO is set as output, you can use to choose to set it to high voltage or low voltage.\n• when the GPIO is set as input, you can use to read whether the voltage is high or low.\n\nIn Section 9, we learned how to communicate with device files. Therefore, we will further use device files to control the LED on and off.\n\nIn the implementation, a pull-down resistor is used. The anode of the LED is connected to GPIO4, and the cathode is connected to GND. For more details about the Raspberry Pi pin assignments, refer to Raspberry Pi Pinout. The materials used include a Raspberry Pi 5, an LED, jumper wires, and a 220Ω resistor.\n\nThere are two main ways of running tasks: tasklets and work queues. Tasklets are a quick and easy way of scheduling a single function to be run. For example, when triggered from an interrupt, whereas work queues are more complicated but also better suited to running multiple things in a sequence.\n\nIt is possible that in future tasklets may be replaced by threaded irqs. However, discussion about that has been ongoing since 2007 (Eliminating tasklets), so do not hold your breath. See the section 16.1 if you wish to avoid the tasklet debate.\n\nHere is an example tasklet module. The function runs for a few seconds. In the meantime, execution of the function may continue to the exit point, depending on whether it is interrupted by softirq.\n\nSo with this example loaded should show:\n\nAlthough tasklet is easy to use, it comes with several drawbacks, and developers are discussing about getting rid of tasklet in linux kernel. The tasklet callback runs in atomic context, inside a software interrupt, meaning that it cannot sleep or access user-space data, so not all work can be done in a tasklet handler. Also, the kernel only allows one instance of any given tasklet to be running at any given time; multiple different tasklet callbacks can run in parallel.\n\nIn recent kernels, tasklets can be replaced by workqueues, timers, or threaded interrupts. While the removal of tasklets remains a longer-term goal, the current kernel contains more than a hundred uses of tasklets. Now developers are proceeding with the API changes and the macro exists for compatibility. For further information, see https://lwn.net/Articles/830964/.\n\nTo add a task to the scheduler we can use a workqueue. The kernel then uses the Completely Fair Scheduler (CFS) to execute work within the queue.\n\nExcept for the last chapter, everything we did in the kernel so far we have done as a response to a process asking for it, either by dealing with a special file, sending an , or issuing a system call. But the job of the kernel is not just to respond to process requests. Another job, which is every bit as important, is to speak to the hardware connected to the machine.\n\nThere are two types of interaction between the CPU and the rest of the computer’s hardware. The first type is when the CPU gives orders to the hardware, the other is when the hardware needs to tell the CPU something. The second, called interrupts, is much harder to implement because it has to be dealt with when convenient for the hardware, not the CPU. Hardware devices typically have a very small amount of RAM, and if you do not read their information when available, it is lost.\n\nUnder Linux, hardware interrupts are called IRQ’s (Interrupt ReQuests). There are two types of IRQ’s, short and long. A short IRQ is one which is expected to take a very short period of time, during which the rest of the machine will be blocked and no other interrupts will be handled. A long IRQ is one which can take longer, and during which other interrupts may occur (but not interrupts from the same device). If at all possible, it is better to declare an interrupt handler to be long.\n\nWhen the CPU receives an interrupt, it stops whatever it is doing (unless it is processing a more important interrupt, in which case it will deal with this one only when the more important one is done), saves certain parameters on the stack and calls the interrupt handler. This means that certain things are not allowed in the interrupt handler itself, because the system is in an unknown state. Linux kernel solves the problem by splitting interrupt handling into two parts. The first part executes right away and masks the interrupt line. Hardware interrupts must be handled quickly, and that is why we need the second part to handle the heavy work deferred from an interrupt handler. Historically, BH (Linux naming for Bottom Halves) statistically book-keeps the deferred functions. Softirq and its higher level abstraction, Tasklet, replace BH since Linux 2.3.\n\nThe way to implement this is to call to get your interrupt handler called when the relevant IRQ is received.\n\nIn practice IRQ handling can be a bit more complex. Hardware is often designed in a way that chains two interrupt controllers, so that all the IRQs from interrupt controller B are cascaded to a certain IRQ from interrupt controller A. Of course, that requires that the kernel finds out which IRQ it really was afterwards and that adds overhead. Other architectures offer some special, very low overhead, so called \"fast IRQ\" or FIQs. To take advantage of them requires handlers to be written in assembly language, so they do not really fit into the kernel. They can be made to work similar to the others, but after that procedure, they are no longer any faster than \"common\" IRQs. SMP enabled kernels running on systems with more than one processor need to solve another truckload of problems. It is not enough to know if a certain IRQs has happened, it’s also important to know what CPU(s) it was for. People still interested in more details, might want to refer to \"APIC\" now.\n\nThis function receives the IRQ number, the name of the function, flags, a name for /proc/interrupts and a parameter to be passed to the interrupt handler. Usually there is a certain number of IRQs available. How many IRQs there are is hardware-dependent.\n\nThe flags can be used for specify behaviors of the IRQ. For example, use to indicate you are willing to share the IRQ with other interrupt handlers (usually because a number of hardware devices sit on the same IRQ); use the to indicate that the IRQ is not reenabled after the handler finished. It should be noted that in some materials, you may encounter another set of IRQ flags named with the prefix. For example, the and the . Those are the the IRQ flags in the older kernels. They have been removed completely. Today only the flags are in use. This function will only succeed if there is not already a handler on this IRQ, or if you are both willing to share.\n\nMany popular single board computers, such as Raspberry Pi or Beagleboards, have a bunch of GPIO pins. Attaching buttons to those and then having a button press do something is a classic case in which you might need to use interrupts, so that instead of having the CPU waste time and battery power polling for a change in input state, it is better for the input to trigger the CPU to then run a particular handling function.\n\nHere is an example where buttons are connected to GPIO numbers 17 and 18 and an LED is connected to GPIO 4. You can change those numbers to whatever is appropriate for your board.\n\nSuppose you want to do a bunch of stuff inside of an interrupt routine. A common way to do that without rendering the interrupt unavailable for a significant duration is to combine it with a tasklet. This pushes the bulk of the work off into the scheduler.\n\nThe example below modifies the previous example to also run an additional task when an interrupt is triggered.\n\nThreaded IRQ is a mechanism to organize both top-half and bottom-half of an IRQ at once. A threaded IRQ splits the one handler in into two: one for the top-half, the other for the bottom-half. The is the function for using threaded IRQs. Two handlers are registered at once in the .\n\nThose two handlers run in different context. The top-half handler runs in interrupt context. It’s the equivalence of the handler passed to the . The bottom-half handler on the other hand runs in its own thread. This thread is created on registration of a threaded IRQ. Its sole purpose is to run this bottom-half handler. This is where a threaded IRQ is “threaded”. If is returned by the top-half handler, that bottom-half serving thread will wake up. The thread then runs the bottom-half handler.\n\nHere is an example of how to do the same thing as before, with top and bottom halves, but using threads.\n\nA threaded IRQ is registered using . This function only takes one additional parameter than the – the bottom-half handling function that runs in its own thread. In this example it is the . Usage of other parameters are the same as .\n\nPresence of both handlers is not mandatory. If either of them is not needed, pass the instead. A top-half handler implies that no action is taken except to wake up the bottom-half serving thread, which runs the bottom-half handler. Similarly, a bottom-half handler effectively acts as if were used. In fact, this is how is implemented.\n\nNote that passing to both handlers is considered an error and will make registration fail.\n\nThe input device driver is a module that provides a way to communicate with the interaction device via the event. For example, the keyboard can send the press or release event to tell the kernel what we want to do. The input device driver will allocate a new input structure with and sets up input bitfields, device id, version, etc. After that, registers it by calling .\n\nHere is an example, vinput, It is an API to allow easy development of virtual input drivers. The drivers needs to export a that contains the virtual device name and structure that describes:\n\nThen using and will add a new device to the list of support virtual input devices.\n\nThis function is passed a already initialized with an allocated . The function is responsible for initializing the capabilities of the input device and register it.\n\nThis function will receive a user string to interpret and inject the event using the or call. The string is already copied from user.\n\nThis function is used for debugging and should fill the buffer parameter with the last event sent in the virtual input device format. The buffer will then be copied to user.\n\nvinput devices are created and destroyed using sysfs. And, event injection is done through a /dev node. The device name will be used by the userland to export a new virtual input device.\n\nThe structure is similar to other attribute types we talked about in section 8:\n\nIn vinput.c, the macro defined in include/linux/device.h (in this case, device.h is included in include/linux/input.h) will generate the structures which are named class_attr_export/unexport. Then, put them into array and the macro will generate the that should be assigned in . Finally, call to create attributes in sysfs.\n\nTo unexport the device, just echo its id in unexport:\n\nHere the virtual keyboard is one of example to use vinput. It supports all keycodes. The injection format is the such as defined in include/linux/input.h. A positive value means while a negative value is a . The keyboard supports repetition when the key stays pressed for too long. The following demonstrates how simulation work.\n\nUp to this point we have seen all kinds of modules doing all kinds of things, but there was no consistency in their interfaces with the rest of the kernel. To impose some consistency such that there is at minimum a standardized way to start, suspend and resume a device model was added. An example is shown below, and you can use this as a template to add your own suspend, resume or other interface functions.\n\nLikely and Unlikely conditions\n\nSometimes you might want your code to run as quickly as possible, especially if it is handling an interrupt or doing something which might cause noticeable latency. If your code contains boolean conditions and if you know that the conditions are almost always likely to evaluate as either or , then you can allow the compiler to optimize for this using the and macros. For example, when allocating memory you are almost always expecting this to succeed.\n\nWhen the macro is used, the compiler alters its machine instruction output, so that it continues along the false branch and only jumps if the condition is true. That avoids flushing the processor pipeline. The opposite happens if you use the macro.\n\nStatic keys allow us to enable or disable kernel code paths based on the runtime state of key. Its APIs have been available since 2010 (most architectures are already supported), use self-modifying code to eliminate the overhead of cache and branch prediction. The most typical use case of static keys is for performance-sensitive kernel code, such as tracepoints, context switching, networking, etc. These hot paths of the kernel often contain branches and can be optimized easily using this technique. Before we can use static keys in the kernel, we need to make sure that gcc supports inline assembly, and the following kernel configurations are set:\n\nTo declare a static key, we need to define a global variable using the or macro defined in include/linux/jump_label.h. This macro initializes the key with the given initial value, which is either false or true, respectively. For example, to declare a static key with an initial value of false, we can use the following code:\n\nOnce the static key has been declared, we need to add branching code to the module that uses the static key. For example, the code includes a fastpath, where a no-op instruction will be generated at compile time as the key is initialized to false and the branch is unlikely to be taken.\n\nIf the key is enabled at runtime by calling , the fastpath will be patched with an unconditional jump instruction to the slowpath code , so the branch will always be taken until the key is disabled again.\n\nThe following kernel module derived from chardev.c, demonstrates how the static key works.\n\nTo check the state of the static key, we can use the /dev/key_state interface.\n\nThis will display the current state of the key, which is disabled by default.\n\nTo change the state of the static key, we can perform a write operation on the file:\n\nThis will enable the static key, causing the code path to switch from the fastpath to the slowpath.\n\nIn some cases, the key is enabled or disabled at initialization and never changed, we can declare a static key as read-only, which means that it can only be toggled in the module init function. To declare a read-only static key, we can use the or macro instead. Attempts to change the key at runtime will result in a page fault. For more information, see Static keys\n\nYou can not do that. In a kernel module, you can only use kernel functions which are the functions you can see in /proc/kallsyms.\n\nYou might need to do this for a short time and that is OK, but if you do not enable them afterwards, your system will be stuck and you will have to power it off.\n\nWhere To Go From Here?\n\nFor those deeply interested in kernel programming, kernelnewbies.org and the Documentation subdirectory within the kernel source code are highly recommended. Although the latter may not always be straightforward, it serves as a valuable initial step for further exploration. Echoing Linus Torvalds’ perspective, the most effective method to understand the kernel is through personal examination of the source code.\n\nContributions to this guide are welcome, especially if there are any significant inaccuracies identified. To contribute or report an issue, please initiate an issue at https://github.com/sysprog21/lkmpg. Pull requests are greatly appreciated."
    },
    {
        "link": "https://docs.kernel.org/kbuild/modules.html",
        "document": "This document describes how to build an out-of-tree kernel module.\n\n“kbuild” is the build system used by the Linux kernel. Modules must use kbuild to stay compatible with changes in the build infrastructure and to pick up the right flags to the compiler. Functionality for building modules both in-tree and out-of-tree is provided. The method for building either is similar, and all modules are initially developed and built out-of-tree. Covered in this document is information aimed at developers interested in building out-of-tree (or “external”) modules. The author of an external module should supply a makefile that hides most of the complexity, so one only has to type “make” to build the module. This is easily accomplished, and a complete example will be presented in section Creating a Kbuild File for an External Module.\n\nTo build external modules, you must have a prebuilt kernel available that contains the configuration and header files used in the build. Also, the kernel must have been built with modules enabled. If you are using a distribution kernel, there will be a package for the kernel you are running provided by your distribution. An alternative is to use the “make” target “modules_prepare.” This will make sure the kernel contains the information required. The target exists solely as a simple way to prepare a kernel source tree for building external modules. NOTE: “modules_prepare” will not build Module.symvers even if CONFIG_MODVERSIONS is set; therefore, a full kernel build needs to be executed to make module versioning work. The command to build an external module is: The kbuild system knows that an external module is being built due to the “M=<dir>” option given in the command. To build against the running kernel use: Then to install the module(s) just built, add the target “modules_install” to the command: Starting from Linux 6.13, you can use the -f option instead of -C. This will avoid unnecessary change of the working directory. The external module will be output to the directory where you invoke make. You can optionally pass MO= option if you want to build the modules in a separate directory. The directory that contains the kernel and relevant build artifacts used for building an external module. “make” will actually change to the specified directory when executing and will change back when finished. Informs kbuild that an external module is being built. The value given to “M” is the absolute path of the directory where the external module (kbuild file) is located. When building an external module, only a subset of the “make” targets are available. The default will build the module(s) located in the current directory, so a target does not need to be specified. All output files will also be generated in this directory. No attempts are made to update the kernel source, and it is a precondition that a successful “make” has been executed for the kernel. The default target for external modules. It has the same functionality as if no target was specified. See description above. Install the external module(s). The default location is /lib/modules/<kernel_release>/updates/, but a prefix may be added with INSTALL_MOD_PATH (discussed in section Module Installation). Remove all generated files in the module directory only. List the available targets for external modules. It is possible to build single files that are part of a module. This works equally well for the kernel, a module, and even for external modules. Example (The module foo.ko, consist of bar.o and baz.o): make -C $KDIR M=$PWD bar.lst make -C $KDIR M=$PWD baz.o make -C $KDIR M=$PWD foo.ko make -C $KDIR M=$PWD ./\n\nIn the last section we saw the command to build a module for the running kernel. The module is not actually built, however, because a build file is required. Contained in this file will be the name of the module(s) being built, along with the list of requisite source files. The file may be as simple as a single line: The kbuild system will build <module_name>.o from <module_name>.c, and, after linking, will result in the kernel module <module_name>.ko. The above line can be put in either a “Kbuild” file or a “Makefile.” When the module is built from multiple sources, an additional line is needed listing the files: NOTE: Further documentation describing the syntax used by kbuild is located in Linux Kernel Makefiles. The examples below demonstrate how to create a build file for the module 8123.ko, which is built from the following files: An external module always includes a wrapper makefile that supports building the module using “make” with no arguments. This target is not used by kbuild; it is only for convenience. Additional functionality, such as test targets, can be included but should be filtered out from kbuild due to possible name clashes. --> filename: Makefile ifneq ($(KERNELRELEASE),) # kbuild part of makefile obj-m := 8123.o 8123-y := 8123_if.o 8123_pci.o else # normal makefile KDIR ?= /lib/modules/`uname -r`/build default: $(MAKE) -C $(KDIR) M=$$PWD endif The check for KERNELRELEASE is used to separate the two parts of the makefile. In the example, kbuild will only see the two assignments, whereas “make” will see everything except these two assignments. This is due to two passes made on the file: the first pass is by the “make” instance run on the command line; the second pass is by the kbuild system, which is initiated by the parameterized “make” in the default target. Kbuild will first look for a file named “Kbuild”, and if it is not found, it will then look for “Makefile”. Utilizing a “Kbuild” file allows us to split up the “Makefile” from example 1 into two files: The split in example 2 is questionable due to the simplicity of each file; however, some external modules use makefiles consisting of several hundred lines, and here it really pays off to separate the kbuild part from the rest. Linux 6.13 and later support another way. The external module Makefile can include the kernel Makefile directly, rather than invoking sub Make. kbuild supports building multiple modules with a single build file. For example, if you wanted to build two modules, foo.ko and bar.ko, the kbuild lines would be:\n\nWithin the kernel, header files are kept in standard locations according to the following rule:\n• None If the header file only describes the internal interface of a module, then the file is placed in the same directory as the source files.\n• None If the header file describes an interface used by other parts of the kernel that are located in different directories, then the file is placed in include/linux/. There are two notable exceptions to this rule: larger subsystems have their own directory under include/, such as include/scsi; and architecture specific headers are located under arch/$(SRCARCH)/include/. To include a header file located under include/linux/, simply use: kbuild will add options to the compiler so the relevant directories are searched. External modules tend to place header files in a separate include/ directory where their source is located, although this is not the usual kernel style. To inform kbuild of the directory, use either ccflags-y or CFLAGS_<filename>.o. Using the example from section 3, if we moved 8123_if.h to a subdirectory named include, the resulting kbuild file would look like: kbuild can handle files that are spread over several directories. Consider the following example: To build the module complex.ko, we then need the following kbuild file: As you can see, kbuild knows how to handle object files located in other directories. The trick is to specify the directory relative to the kbuild file’s location. That being said, this is NOT recommended practice. For the header files, kbuild must be explicitly told where to look. When kbuild executes, the current directory is always the root of the kernel tree (the argument to “-C”) and therefore an absolute path is needed. $(src) provides the absolute path by pointing to the directory where the currently executing kbuild file is located.\n\nModule versioning is enabled by the CONFIG_MODVERSIONS tag, and is used as a simple ABI consistency check. A CRC value of the full prototype for an exported symbol is created. When a module is loaded/used, the CRC values contained in the kernel are compared with similar values in the module; if they are not equal, the kernel refuses to load the module. Module.symvers contains a list of all exported symbols from a kernel build. During a kernel build, a file named Module.symvers will be generated. Module.symvers contains all exported symbols from the kernel and compiled modules. For each symbol, the corresponding CRC value is also stored. The syntax of the Module.symvers file is: The fields are separated by tabs and values may be empty (e.g. if no namespace is defined for an exported symbol). For a kernel build without CONFIG_MODVERSIONS enabled, the CRC would read 0x00000000.\n• None It lists all exported symbols from vmlinux and all modules.\n• None It lists the CRC if CONFIG_MODVERSIONS is enabled. Exported symbols have information stored in __ksymtab or __ksymtab_gpl sections. Symbol names and namespaces are stored in __ksymtab_strings, using a format similar to the string table used for ELF. If CONFIG_MODVERSIONS is enabled, the CRCs corresponding to exported symbols will be added to the __kcrctab or __kcrctab_gpl. If CONFIG_BASIC_MODVERSIONS is enabled (default with CONFIG_MODVERSIONS), imported symbols will have their symbol name and CRC stored in the __versions section of the importing module. This mode only supports symbols of length up to 64 bytes. If CONFIG_EXTENDED_MODVERSIONS is enabled (required to enable both CONFIG_MODVERSIONS and CONFIG_RUST at the same time), imported symbols will have their symbol name recorded in the __version_ext_names section as a series of concatenated, null-terminated strings. CRCs for these symbols will be recorded in the __version_ext_crcs section. When building an external module, the build system needs access to the symbols from the kernel to check if all external symbols are defined. This is done in the MODPOST step. modpost obtains the symbols by reading Module.symvers from the kernel source tree. During the MODPOST step, a new Module.symvers file will be written containing all exported symbols from that external module. Sometimes, an external module uses exported symbols from another external module. Kbuild needs to have full knowledge of all symbols to avoid spitting out warnings about undefined symbols. Two solutions exist for this situation. NOTE: The method with a top-level kbuild file is recommended but may be impractical in certain situations. If you have two modules, foo.ko and bar.ko, where foo.ko needs symbols from bar.ko, you can use a common top-level kbuild file so both modules are compiled in the same build. Consider the following directory layout: The top-level kbuild file would then look like: will then do the expected and compile both modules with full knowledge of symbols from either module. If it is impractical to add a top-level kbuild file, you can assign a space separated list of files to KBUILD_EXTRA_SYMBOLS in your build file. These files will be loaded by modpost during the initialization of its symbol tables."
    },
    {
        "link": "https://kernel.org/doc/html/next/kbuild/modules.html",
        "document": "This document describes how to build an out-of-tree kernel module.\n\n“kbuild” is the build system used by the Linux kernel. Modules must use kbuild to stay compatible with changes in the build infrastructure and to pick up the right flags to the compiler. Functionality for building modules both in-tree and out-of-tree is provided. The method for building either is similar, and all modules are initially developed and built out-of-tree. Covered in this document is information aimed at developers interested in building out-of-tree (or “external”) modules. The author of an external module should supply a makefile that hides most of the complexity, so one only has to type “make” to build the module. This is easily accomplished, and a complete example will be presented in section Creating a Kbuild File for an External Module.\n\nTo build external modules, you must have a prebuilt kernel available that contains the configuration and header files used in the build. Also, the kernel must have been built with modules enabled. If you are using a distribution kernel, there will be a package for the kernel you are running provided by your distribution. An alternative is to use the “make” target “modules_prepare.” This will make sure the kernel contains the information required. The target exists solely as a simple way to prepare a kernel source tree for building external modules. NOTE: “modules_prepare” will not build Module.symvers even if CONFIG_MODVERSIONS is set; therefore, a full kernel build needs to be executed to make module versioning work. The command to build an external module is: The kbuild system knows that an external module is being built due to the “M=<dir>” option given in the command. To build against the running kernel use: Then to install the module(s) just built, add the target “modules_install” to the command: Starting from Linux 6.13, you can use the -f option instead of -C. This will avoid unnecessary change of the working directory. The external module will be output to the directory where you invoke make. You can optionally pass MO= option if you want to build the modules in a separate directory. The directory that contains the kernel and relevant build artifacts used for building an external module. “make” will actually change to the specified directory when executing and will change back when finished. Informs kbuild that an external module is being built. The value given to “M” is the absolute path of the directory where the external module (kbuild file) is located. When building an external module, only a subset of the “make” targets are available. The default will build the module(s) located in the current directory, so a target does not need to be specified. All output files will also be generated in this directory. No attempts are made to update the kernel source, and it is a precondition that a successful “make” has been executed for the kernel. The default target for external modules. It has the same functionality as if no target was specified. See description above. Install the external module(s). The default location is /lib/modules/<kernel_release>/updates/, but a prefix may be added with INSTALL_MOD_PATH (discussed in section Module Installation). Remove all generated files in the module directory only. List the available targets for external modules. It is possible to build single files that are part of a module. This works equally well for the kernel, a module, and even for external modules. Example (The module foo.ko, consist of bar.o and baz.o): make -C $KDIR M=$PWD bar.lst make -C $KDIR M=$PWD baz.o make -C $KDIR M=$PWD foo.ko make -C $KDIR M=$PWD ./\n\nIn the last section we saw the command to build a module for the running kernel. The module is not actually built, however, because a build file is required. Contained in this file will be the name of the module(s) being built, along with the list of requisite source files. The file may be as simple as a single line: The kbuild system will build <module_name>.o from <module_name>.c, and, after linking, will result in the kernel module <module_name>.ko. The above line can be put in either a “Kbuild” file or a “Makefile.” When the module is built from multiple sources, an additional line is needed listing the files: NOTE: Further documentation describing the syntax used by kbuild is located in Linux Kernel Makefiles. The examples below demonstrate how to create a build file for the module 8123.ko, which is built from the following files: An external module always includes a wrapper makefile that supports building the module using “make” with no arguments. This target is not used by kbuild; it is only for convenience. Additional functionality, such as test targets, can be included but should be filtered out from kbuild due to possible name clashes. --> filename: Makefile ifneq ($(KERNELRELEASE),) # kbuild part of makefile obj-m := 8123.o 8123-y := 8123_if.o 8123_pci.o else # normal makefile KDIR ?= /lib/modules/`uname -r`/build default: $(MAKE) -C $(KDIR) M=$$PWD endif The check for KERNELRELEASE is used to separate the two parts of the makefile. In the example, kbuild will only see the two assignments, whereas “make” will see everything except these two assignments. This is due to two passes made on the file: the first pass is by the “make” instance run on the command line; the second pass is by the kbuild system, which is initiated by the parameterized “make” in the default target. Kbuild will first look for a file named “Kbuild”, and if it is not found, it will then look for “Makefile”. Utilizing a “Kbuild” file allows us to split up the “Makefile” from example 1 into two files: The split in example 2 is questionable due to the simplicity of each file; however, some external modules use makefiles consisting of several hundred lines, and here it really pays off to separate the kbuild part from the rest. Linux 6.13 and later support another way. The external module Makefile can include the kernel Makefile directly, rather than invoking sub Make. kbuild supports building multiple modules with a single build file. For example, if you wanted to build two modules, foo.ko and bar.ko, the kbuild lines would be:\n\nWithin the kernel, header files are kept in standard locations according to the following rule:\n• None If the header file only describes the internal interface of a module, then the file is placed in the same directory as the source files.\n• None If the header file describes an interface used by other parts of the kernel that are located in different directories, then the file is placed in include/linux/. There are two notable exceptions to this rule: larger subsystems have their own directory under include/, such as include/scsi; and architecture specific headers are located under arch/$(SRCARCH)/include/. To include a header file located under include/linux/, simply use: kbuild will add options to the compiler so the relevant directories are searched. External modules tend to place header files in a separate include/ directory where their source is located, although this is not the usual kernel style. To inform kbuild of the directory, use either ccflags-y or CFLAGS_<filename>.o. Using the example from section 3, if we moved 8123_if.h to a subdirectory named include, the resulting kbuild file would look like: kbuild can handle files that are spread over several directories. Consider the following example: To build the module complex.ko, we then need the following kbuild file: As you can see, kbuild knows how to handle object files located in other directories. The trick is to specify the directory relative to the kbuild file’s location. That being said, this is NOT recommended practice. For the header files, kbuild must be explicitly told where to look. When kbuild executes, the current directory is always the root of the kernel tree (the argument to “-C”) and therefore an absolute path is needed. $(src) provides the absolute path by pointing to the directory where the currently executing kbuild file is located.\n\nModule versioning is enabled by the CONFIG_MODVERSIONS tag, and is used as a simple ABI consistency check. A CRC value of the full prototype for an exported symbol is created. When a module is loaded/used, the CRC values contained in the kernel are compared with similar values in the module; if they are not equal, the kernel refuses to load the module. Module.symvers contains a list of all exported symbols from a kernel build. During a kernel build, a file named Module.symvers will be generated. Module.symvers contains all exported symbols from the kernel and compiled modules. For each symbol, the corresponding CRC value is also stored. The syntax of the Module.symvers file is: The fields are separated by tabs and values may be empty (e.g. if no namespace is defined for an exported symbol). For a kernel build without CONFIG_MODVERSIONS enabled, the CRC would read 0x00000000.\n• None It lists all exported symbols from vmlinux and all modules.\n• None It lists the CRC if CONFIG_MODVERSIONS is enabled. Exported symbols have information stored in __ksymtab or __ksymtab_gpl sections. Symbol names and namespaces are stored in __ksymtab_strings, using a format similar to the string table used for ELF. If CONFIG_MODVERSIONS is enabled, the CRCs corresponding to exported symbols will be added to the __kcrctab or __kcrctab_gpl. If CONFIG_BASIC_MODVERSIONS is enabled (default with CONFIG_MODVERSIONS), imported symbols will have their symbol name and CRC stored in the __versions section of the importing module. This mode only supports symbols of length up to 64 bytes. If CONFIG_EXTENDED_MODVERSIONS is enabled (required to enable both CONFIG_MODVERSIONS and CONFIG_RUST at the same time), imported symbols will have their symbol name recorded in the __version_ext_names section as a series of concatenated, null-terminated strings. CRCs for these symbols will be recorded in the __version_ext_crcs section. When building an external module, the build system needs access to the symbols from the kernel to check if all external symbols are defined. This is done in the MODPOST step. modpost obtains the symbols by reading Module.symvers from the kernel source tree. During the MODPOST step, a new Module.symvers file will be written containing all exported symbols from that external module. Sometimes, an external module uses exported symbols from another external module. Kbuild needs to have full knowledge of all symbols to avoid spitting out warnings about undefined symbols. Two solutions exist for this situation. NOTE: The method with a top-level kbuild file is recommended but may be impractical in certain situations. If you have two modules, foo.ko and bar.ko, where foo.ko needs symbols from bar.ko, you can use a common top-level kbuild file so both modules are compiled in the same build. Consider the following directory layout: The top-level kbuild file would then look like: will then do the expected and compile both modules with full knowledge of symbols from either module. If it is impractical to add a top-level kbuild file, you can assign a space separated list of files to KBUILD_EXTRA_SYMBOLS in your build file. These files will be loaded by modpost during the initialization of its symbol tables."
    },
    {
        "link": "https://stackoverflow.com/questions/12244979/how-to-build-kernel-module-into-a-specific-directory",
        "document": "Is there a way to set an output-directory for making kernel modules inside my makefile?\n\nI want to keep my source-directory clean from the build files."
    },
    {
        "link": "https://stackoverflow.com/questions/56241063/how-to-place-c-compilation-output-files-linux-kernel-modules-in-a-different-di",
        "document": "I have looked at these and at these other solutions but cannot write the correct Makefile to produce my wanted result.\n\nSo, I have this file. It simulates linux kernel module loading and removing. Location:\n\nI also have this in the same directory as , location: .\n\neverything is compiled correctly (in the same directory .\n\nWhat I want is this:\n\nLocation of the outputs in or/and .\n\nWhen is run, the outputs should end in the , directories.\n\nThere are some complications in the Makefile ( ) which I don't quite understand. All the various changes to the in order to reach the desired result, ended without success in errors.\n\nHow do I do it?\n\nThe in has the following code:"
    },
    {
        "link": "https://linux-kernel-labs.github.io/refs/heads/master/labs/kernel_modules.html",
        "document": "A monolithic kernel, though faster than a microkernel, has the disadvantage of lack of modularity and extensibility. On modern monolithic kernels, this has been solved by using kernel modules. A kernel module (or loadable kernel mode) is an object file that contains code that can extend the kernel functionality at runtime (it is loaded as needed); When a kernel module is no longer needed, it can be unloaded. Most of the device drivers are used in the form of kernel modules. For the development of Linux device drivers, it is recommended to download the kernel sources, configure and compile them and then install the compiled version on the test /development tool machine.\n\nCompiling a kernel module differs from compiling an user program. First, other headers should be used. Also, the module should not be linked to libraries. And, last but not least, the module must be compiled with the same options as the kernel in which we load the module. For these reasons, there is a standard compilation method ( ). This method requires the use of two files: a and a file. Below is an example of a : And the example of a file used to compile a module: As you can see, calling make on the file in the example shown will result in the make invocation in the kernel source directory ( ) and referring to the current directory ( ). This process ultimately leads to reading the file from the current directory and compiling the module as instructed in this file. For labs we will configure different KDIR, according to the virtual machine specifications: A file contains one or more directives for compiling a kernel module. The easiest example of such a directive is . Following this directive, a kernel module ( - kernel object) will be created, starting from the file. will be created starting from or . All of these files can be found in the 's directory. An example of a file that uses several sub-modules is shown below: For the example above, the steps to compile are:\n• compile the and sources, resulting in module-a.o and module-b.o objects\n• and will then be linked in\n• from will be created module The suffix of targets in determines how they are used, as follows:\n• Y (yes) represents a target for object files to be compiled and then linked to a module ( ) or within the kernel ( )\n• any other target suffix will be ignored by and will not be compiled These suffixes are used to easily configure the kernel by running the make menuconfig command or directly editing the file. This file sets a series of variables that are used to determine which features are added to the kernel at build time. For example, when adding BTRFS support with make menuconfig, add the line to the file. The BTRFS kbuild contains the line , which becomes . This will compile the object and will be linked to the kernel. Before the variable was set, the line became and so it was ignored, and the kernel was build without BTRFS support. For more details, see the and files within the kernel sources.\n\nTroubleshooting a kernel module is much more complicated than debugging a regular program. First, a mistake in a kernel module can lead to blocking the entire system. Troubleshooting is therefore much slowed down. To avoid reboot, it is recommended to use a virtual machine (qemu, virtualbox, vmware). When a module containing bugs is inserted into the kernel, it will eventually generate a kernel oops. A kernel oops is an invalid operation detected by the kernel and can only be generated by the kernel. For a stable kernel version, it almost certainly means that the module contains a bug. After the oops appears, the kernel will continue to work. Very important to the appearance of a kernel oops is saving the generated message. As noted above, messages generated by the kernel are saved in logs and can be displayed with the dmesg command. To make sure that no kernel message is lost, it is recommended to insert/test the kernel directly from the console, or periodically check the kernel messages. Noteworthy is that an oops can occur because of a programming error, but also a because of hardware error. If a fatal error occurs, after which the system can not return to a stable state, a kernel panic is generated. Look at the kernel module below that contains a bug that generates an oops: Inserting this module into the kernel will generate an oops: faust:~/lab-01/modul-oops# insmod oops.ko ... faust:~/lab-01/modul-oops# dmesg tail -32 BUG: unable to handle kernel paging request at IP: <c89d4005> my_oops_init+0x5/0x20 oops *de Oops: last sysfs file: /sys/devices/virtual/net/lo/operstate Modules linked in: oops + netconsole ide_cd_mod pcnet32 crc32 cdrom last unloaded: modul Pid: , comm: insmod Not tainted .6.28.4 EIP: : <c89d4005> EFLAGS: CPU: EIP is at my_oops_init+0x5/0x20 oops EAX: EBX: fffffffc ECX: c89d4300 EDX: ESI: c89d4000 EDI: EBP: c5799e24 ESP: c5799e24 DS: 007b ES: 007b FS: GS: SS: Process insmod pid: , c5799000 c665c780 task.ti c5799000 Stack: c5799f8c c010102d c72b51d8 0000000c c5799e58 c01708e4 c89d4300 c5799e58 c724f448 c89d4300 c5799e60 c0170981 c5799f8c c014b698 c5799f78 c5799f20 c665cb00 c89d4300 Call Trace: <c010102d> ? _stext+0x2d/0x170 <c01708e4> ? __vunmap+0xa4/0xf0 <c0170981> ? vfree+0x21/0x30 <c014b698> ? load_module+0x19b8/0x1a40 <c035e965> ? __mutex_unlock_slowpath+0xd5/0x140 <c0140da6> ? trace_hardirqs_on_caller+0x106/0x150 <c014b7aa> ? sys_init_module+0x8a/0x1b0 <c0140da6> ? trace_hardirqs_on_caller+0x106/0x150 <c0240a08> ? trace_hardirqs_on_thunk+0xc/0x10 <c0103407> ? sysenter_do_call+0x12/0x43 Code: <c7> 5d c3 eb 0d EIP: <c89d4005> my_oops_init+0x5/0x20 oops SS:ESP :c5799e24 --- end trace 2981ce73ae801363 --- Although relatively cryptic, the message provided by the kernel to the appearance of an oops provides valuable information about the error. First line: Tells us the cause and the address of the instruction that generated the error. In our case this is an invalid access to memory. Tells us that it's the first oops (#1). This is important in the context that an oops can lead to other oopses. Usually only the first oops is relevant. Furthermore, the oops code ( ) provides information about the error type (see ): In this case, we have a write access that generated the oops (bit 1 is 1). Below is a dump of the registers. It decodes the instruction pointer ( ) value and notes that the bug appeared in the function with a 5-byte offset ( ). The message also shows the stack content and a backtrace of calls until then. If an invalid read call is generated ( ), the message will be the same, but the oops code will differ, which would now be : faust:~/lab-01/modul-oops# dmesg tail -33 BUG: unable to handle kernel paging request at IP: <c89c3016> my_oops_init+0x6/0x20 oops *de Oops: last sysfs file: /sys/devices/virtual/net/lo/operstate Modules linked in: oops + netconsole pcnet32 crc32 ide_cd_mod cdrom Pid: , comm: insmod Not tainted .6.28.4 EIP: : <c89c3016> EFLAGS: CPU: EIP is at my_oops_init+0x6/0x20 oops EAX: EBX: fffffffc ECX: c89c3380 EDX: ESI: c89c3010 EDI: EBP: c57cbe24 ESP: c57cbe1c DS: 007b ES: 007b FS: GS: SS: Process insmod pid: , c57cb000 c66ec780 task.ti c57cb000 Stack: c57cbe34 c57cbf8c c010102d c57b9280 0000000c c57cbe58 c01708e4 c89c3380 c57cbe58 c5db1d38 c89c3380 c57cbe60 c0170981 c57cbf8c c014b698 c57cbf78 c57cbf20 Call Trace: <c010102d> ? _stext+0x2d/0x170 <c01708e4> ? __vunmap+0xa4/0xf0 <c0170981> ? vfree+0x21/0x30 <c014b698> ? load_module+0x19b8/0x1a40 <c035d083> ? printk+0x0/0x1a <c035e965> ? __mutex_unlock_slowpath+0xd5/0x140 <c0140da6> ? trace_hardirqs_on_caller+0x106/0x150 <c014b7aa> ? sys_init_module+0x8a/0x1b0 <c0140da6> ? trace_hardirqs_on_caller+0x106/0x150 <c0240a08> ? trace_hardirqs_on_thunk+0xc/0x10 <c0103407> ? sysenter_do_call+0x12/0x43 Code: <a1> c7 9c c8 e8 a0 f7 EIP: <c89c3016> my_oops_init+0x6/0x20 oops SS:ESP :c57cbe1c --- end trace 45eeb3d6ea8ff1ed --- Detailed information about the instruction that generated the oops can be found using the objdump utility. Useful options to use are -d to disassemble the code and -S for interleaving C code in assembly language code. For efficient decoding, however, we need the address where the kernel module was loaded. This can be found in . Here's an example of using objdump on the above module to identify the instruction that generated the oops: Note that the instruction that generated the oops ( identified earlier) is: That is exactly what was expected - storing value 3 at 0x0001234. The is used to find the address where a kernel module is loaded. The --adjust-vma option allows you to display instructions relative to . The -l option displays the number of each line in the source code interleaved with the assembly language code. A more simplistic way to find the code that generated an oops is to use the addr2line utility: Where is the value of the program counter ( ) that generated the oops, minus the base address of the module ( ) according to Minicom (or other equivalent utilities, eg picocom, screen) is a utility that can be used to connect and interact with a serial port. The serial port is the basic method for analyzing kernel messages or interacting with an embedded system in the development phase. There are two more common ways to connect:\n• a serial port where the device we are going to use is\n• a serial USB port (FTDI) in which case the device we are going to use is . For the virtual machine used in the lab, the device that we need to use is displayed after the virtual machine starts: #for connecting via COM1 and using a speed of 115,200 characters per second minicom -b -D /dev/ttyS0 minicom -D /dev/ttyUSB0 #To connect to the serial port of the virtual machine minicom -D /dev/pts/20 Netconsole is a utility that allows logging of kernel debugging messages over the network. This is useful when the disk logging system does not work or when serial ports are not available or when the terminal does not respond to commands. Netconsole comes in the form of a kernel module. To work, it needs the following parameters:\n• port, IP address, and the source interface name of the debug station\n• port, MAC address, and IP address of the machine to which the debug messages will be sent These parameters can be configured when the module is inserted into the kernel, or even while the module is inserted if it has been compiled with the option. An example configuration when inserting netconsole kernel module is as follows: Thus, the debug messages on the station that has the address will be sent to the interface, having source port . The messages will be sent to with the MAC address , on port . Messages can be played on the destination station using netcat: Alternatively, the destination station can configure syslogd to intercept these messages. More information can be found in . The two oldest and most useful debugging aids are Your Brain and Printf . For debugging, a primitive way is often used, but it is quite effective: debugging. Although a debugger can also be used, it is generally not very useful: simple bugs (uninitialized variables, memory management problems, etc.) can be easily localized by control messages and the kernel-decoded oop message. For more complex bugs, even a debugger can not help us too much unless the operating system structure is very well understood. When debugging a kernel module, there are a lot of unknowns in the equation: multiple contexts (we have multiple processes and threads running at a time), interruptions, virtual memory, etc. You can use to display kernel messages to user space. It is similar to 's functionality; the only difference is that the transmitted message can be prefixed with a string of , where indicates the error level (loglevel) and has values between and . Instead of , the levels can also be coded by symbolic constants: The definitions of all log levels are found in . Basically, these log levels are used by the system to route messages sent to various outputs: console, log files in etc. To display messages in user space, the log level must be of higher priority than variable. The default console log level can be configured from . will enable all the kernel log messages to be displayed in the console. That is, the logging level has to be strictly less than the variable. For example, if the has a value of (specific to ), only messages with loglevel stricter than (i.e , , , , ) will be shown. Console-redirected messages can be useful for quickly viewing the effect of executing the kernel code, but they are no longer so useful if the kernel encounters an irreparable error and the system freezes. In this case, the logs of the system must be consulted, as they keep the information between system restarts. These are found in and are text files, populated by and during the kernel run. and take the information from the virtual file system mounted in . In principle, with and turned on, all messages coming from the kernel will go to . A simpler version for debugging is using the file. It is populated only with the messages from the kernel with the log level. Given that a production kernel (similar to the one we're probably running with) contains only release code, our module is among the few that send messages prefixed with KERN_DEBUG . In this way, we can easily navigate through the information by finding the messages corresponding to a debugging session for our module. Such an example would be the following: # Clear the debug file of previous information (or possibly a backup) $ > /var/log/debug # If there is no critical error causing a panic kernel, check the output # if a critical error occurs and the machine only responds to a restart, restart the system and check /var/log/debug. The format of the messages must obviously contain all the information of interest in order to detect the error, but inserting in the code to provide detailed information can be as time-consuming as writing the code to solve the problem. This is usually a trade-off between the completeness of the debugging messages displayed using and the time it takes to insert these messages into the text. A very simple way, less time-consuming for inserting and providing the possibility to analyze the flow of instructions for tests is the use of the predefined constants , and :\n• is replaced by the compiler with the name of the source file it is currently being compiled.\n• is replaced by the compiler with the line number on which the current instruction is found in the current source file.\n• / is replaced by the compiler with the name of the function in which the current instruction is found. and are part of the ANSI C specifications: is part of specification C99; is a GNU extension and is not portable; However, since we write code for the kernel, we can use it without any problems. The following macro definition can be used in this case: Then, at each point where we want to see if it is \"reached\" in execution, insert PRINT_DEBUG; This is a simple and quick way, and can yield by carefully analyzing the output. The dmesg command is used to view the messages printed with but not appearing on the console. To delete all previous messages from a log file, run: To delete messages displayed by the dmesg command, run: Dynamic dyndbg debugging enables dynamic debugging activation/deactivation. Unlike , it offers more advanced options for the messages we want to display; it is very useful for complex modules or troubleshooting subsystems. This significantly reduces the amount of messages displayed, leaving only those relevant for the debug context. To enable , the kernel must be compiled with the option. Once configured, , and , can be dynamically enabled per call. The file from the debugfs (where is the path to which debugfs was mounted) is used to filter messages or to view existing filters. Debugfs is a simple file system, used as a kernel-space interface and user-space interface to configure different debug options. Any debug utility can create and use its own files /folders in debugfs. For example, to display existing filters in , you will use: And to enable the debug message from line in the file: The file is not a regular file. It shows the settings on the filters. Writing in it with an echo will change these settings (it will not actually make a write). Be aware that the file contains settings for debugging messages. Do not log in this file.\n• None - just the debug messages from the functions that have the same name as the one defined in the filter.\n• None - the name of the file(s) for which we want to display the debug messages. It can be just the source name, but also the absolute path or kernel-tree path.\n• None - only messages whose display format contains the specified string.\n• None - the line or lines for which we want to enable debug calls. # Triggers debug messages between lines 1603 and 1605 in the svcsock.c file $ > /sys/kernel/debug/dynamic_debug/control # Enables debug messages from the beginning of the file to line 1605 $ > /sys/kernel/debug/dynamic_debug/control In addition to the above options, a series of flags can be added, removed, or set with operators , or :\n• includes the name of the function in the printed message.\n• includes the line number in the printed message.\n• includes the module name in the printed message.\n• includes the thread id if it is not called from interrupt context The kernel debugger has proven to be very useful to facilitate the development and debugging process. One of its main advantages is the possibility to perform live debugging. This allows us to monitor, in real time, the accesses to memory or even modify the memory while debugging. The debugger has been integrated in the mainline kernel starting with version 2.6.26-rci. KDB is not a source debugger, but for a complete analysis it can be used in parallel with gdb and symbol files -- see the GDB debugging section To use KDB, you have the following options: For the lab, we will use a serial interface connected to the host. The following command will activate GDB over the serial port: KDB is a stop mode debugger, which means that, while it is active, all the other processes are stopped. The kernel can be forced to enter KDB during execution using the following SysRq command or by using the key combination in a terminal connected to the serial port (for example using minicom). KDB has various commands to control and define the context of the debugged system: For a better description of the available commands you can use the command in the KDB shell. In the next example, you can notice a simple KDB usage example which sets a hardware breakpoint to monitor the changes of the variable. g > /proc/sysrq-trigger # or if we are connected to the serial port issue Ctrl-O g # breakpoint on write access to the mVar variable kdb> bph mVar dataw kdb> go\n\nWe strongly encourage you to use the setup from this repository. To solve exercises, you need to perform these steps:\n• start the VM and test the module in the VM. The current lab name is kernel_modules. See the exercises for the task name. The skeleton code is generated from full source examples located in . To solve the tasks, start by generating the skeleton code for a complete lab: You can also generate the skeleton for a single task, using Once the skeleton drivers are generated, build the source: The modules are placed in /home/root/skels/kernel_modules/<task_name>. You DO NOT need to STOP the VM when rebuilding modules! The local directory is shared with the VM. Review the Exercises section for more detailed information. Before starting the exercises or generating the skeletons, please run git pull inside the Linux repo, to make sure you have the latest version of the exercises. If you have local changes, the pull command will fail. Check for local changes using . If you want to keep them, run before and after. To discard the changes, run . If you already generated the skeleton before you will need to generate it again. Using cscope or LXR find the definitions of the following symbols in the Linux kernel source code:\n• and\n• what do the two macros do? What is and ?\n• None\n• What is this variable used for? If you have problems using cscope, it is possible that the database is not generated. To generate it, use the following command in the kernel directory: When searching for a structure using cscope, use only the structure name (without ). So, to search for the structure , you will use the command For more info on using cscope, read the cscope section in the previous lab. To work with the kernel modules, we will follow the steps described above. Generate the skeleton for the task named 1-2-test-mod then build the module, by running the following command in . These command will build all the modules in the current lab skeleton. Until after solving exercise 3, you will get a compilation error for . To avoid this issue, remove the directory and remove the corresponding line from . Start the VM using make console, and perform the following tasks:\n• list the kernel modules and check if current module is present\n• view the messages displayed at loading/unloading the kernel module using dmesg command Read Loading/unloading a kernel module section. When unloading a kernel module, you can specify only the module name (without extension). Watch the virtual machine console. Why were the messages displayed directly to the virtual machine console? Configure the system such that the messages are not displayed directly on the serial console, and they can only be inspected using . One option is to set the console log level by writting the desired level to . Use a value smaller than the level used for the prints in the source code of the module. Load/unload the module again. The messages should not be printed to the virtual machine console, but they should be visible when running . Generate the skeleton for the task named 3-error-mod. Compile the sources and get the corresponding kernel module. Why have compilation errors occurred? Hint: How does this module differ from the previous module? Modify the module to solve the cause of those errors, then compile and test the module. Inspect the C source files and in . Module 2 contains only the definition of a function used by module 1. Change the file to create the module from the two C source files. Read the Compiling kernel modules section of the lab. Compile, copy, boot the VM, load and unload the kernel module. Make sure messages are properly displayed on the console. Enter the directory for the task 5-oops-mod and inspect the C source file. Notice where the problem will occur. Add the compilation flag in the Kbuild file. Compile the corresponding module and load it into the kernel. Identify the memory address at which the oops appeared. Read `Debugging`_ section of the lab. To identify the address, follow the oops message and extract the value of the instructions pointer ( ) register. Determine which instruction has triggered the oops. Use the information to get the load address of the kernel module. Use, on the physical machine, objdump and/or addr2line . Objdump needs debugging support for compilation! Read the lab's objdump and addr2line sections. Try to unload the kernel module. Notice that the operation does not work because there are references from the kernel module within the kernel since the oops; Until the release of those references (which is almost impossible in the case of an oops), the module can not be unloaded. Enter the directory for the task 6-cmd-mod and inspect the C source file. Compile and copy the associated module and load the kernel module to see the printk message. Then unload the module from the kernel. Without modifying the sources, load the kernel module so that the message shown is . The str variable can be changed by passing a parameter to the module. Find more information here. Check the skeleton for the task named 7-list-proc. Add code to display the Process ID ( ) and the executable name for the current process. Follow the commands marked with . The information must be displayed both when loading and unloading the module.\n• In the Linux kernel, a process is described by the . Use LXR or to find the definition of .\n• To find the structure field that contains the name of the executable, look for the \"executable\" comment.\n• The pointer to the structure of the current process running at a given time in the kernel is given by the variable (of the type ). To use you'll need to include the header in which the is defined, i.e . Compile, copy, boot the VM and load the module. Unload the kernel module. Repeat the loading/unloading operation. Note that the PIDs of the displayed processes differ. This is because a process is created from the executable when the module is loaded and when the module is unloaded a process is created from the executable .\n\nGo to the 8-kdb directory. Activate KDB over the serial port and enter KDB mode using SysRq. Connect to the pseudo-terminal linked to virtiocon0 using minicom, configure KDB to use the hvc0 serial port: and enable it using SysRq (Ctrl + O g). Review the current system status (help to see the available KDB commands). Continue the kernel execution using the go command. Load the module. The module will simulate a bug when writing to the file. To simulate a bug, use the below command: After running the above command, at every oops/panic the kernel stops the execution and enters debug mode. Analyze the stacktrace and determine the code that generated the bug. How can we find out from KDB the address where the module was loaded? In parallel, use GDB in a new window to view the code based on KDB information. When writing to , the module will increment the variable. Enter KDB and set a breakpoint for each write access of the variable. Return to kernel to trigger a write using: Update the created kernel module at proc-info in order to display information about all the processes in the system, when inserting the kernel module, not just about the current process. Afterwards, compare the obtained result with the output of the ps command.\n• Processes in the system are structured in a circular list.\n• macros (such as ) are useful when you want to navigate the items in a list.\n• To understand how to use a feature or a macro, use LXR or Vim and cscope and search for usage scenarios. Create a kernel module that displays the virtual memory areas of the current process; for each memory area it will display the start address and the end address.\n• Investigate the structures , and . A memory area is indicated by a structure of type .\n• Don't forget to include the headers where the necessary structures are defined. Go to the 9-dyndbg directory and compile the module. Familiarize yourself with the file system mounted in and analyze the contents of the file . Insert the module and notice the new content of the file. What appears extra in the respective file? Run the following command: Configure dyndbg so that only messages marked as \"Important\" in function are displayed when the module is unloaded. The exercise will only filter out the calls; calls being always displayed. Specify two ways to filter. Read the Dynamic debugging section and look at the dyndbg options (for example, line, format). Perform the filtering and revise the file. What has changed? How do you know which calls are activated? Check the dyndbg flags. Unload the kernel module and observe the log messages. As you have noticed, calls can only be activated /filtered after module insertion. In some situations, it might be helpful to view the messages from the initialization of the module. This can be done by using a default (fake) parameter called dyndbg that can be passed as an argument to initialize the module. With this parameter you can add /delete dyndbg flags. Read the last part of the Dynamic debugging section and see the available flags (e.g.: +/- p). Read the Debug Messages section at Module Initialization Time and insert the module so that the messages in (called ) are also displayed during initialization. In the VM from the lab, you will need to use insmod instead of modprobe. You can delete the set flags. Unload the kernel module."
    }
]