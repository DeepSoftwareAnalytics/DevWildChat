[
    {
        "link": "https://github.com/alphacep/vosk-api",
        "document": "Vosk is an offline open source speech recognition toolkit. It enables speech recognition for 20+ languages and dialects - English, Indian English, German, French, Spanish, Portuguese, Chinese, Russian, Turkish, Vietnamese, Italian, Dutch, Catalan, Arabic, Greek, Farsi, Filipino, Ukrainian, Kazakh, Swedish, Japanese, Esperanto, Hindi, Czech, Polish. More to come.\n\nVosk models are small (50 Mb) but provide continuous large vocabulary transcription, zero-latency response with streaming API, reconfigurable vocabulary and speaker identification.\n\nSpeech recognition bindings implemented for various programming languages like Python, Java, Node.JS, C#, C++, Rust, Go and others.\n\nVosk supplies speech recognition for chatbots, smart home appliances, virtual assistants. It can also create subtitles for movies, transcription for lectures and interviews.\n\nVosk scales from small devices like Raspberry Pi or Android smartphone to big clusters."
    },
    {
        "link": "https://alphacephei.com/vosk",
        "document": "Vosk is a speech recognition toolkit. The best things in Vosk are:\n• Supports 20+ languages and dialects - English, Indian English, German, French, Spanish, Portuguese, Chinese, Russian, Turkish, Vietnamese, Italian, Dutch, Catalan, Arabic, Greek, Farsi, Filipino, Ukrainian, Kazakh, Swedish, Japanese, Esperanto, Hindi, Czech, Polish, Uzbek, Korean, Breton, Gujarati, Tajik, Telugu. More to come.\n• Portable per-language models are only 50Mb each, but there are much bigger server models available.\n• Provides streaming API for the best user experience (unlike popular speech-recognition python packages)\n• There are bindings for different programming languages, too - java/csharp/javascript etc.\n• Allows quick reconfiguration of vocabulary for best accuracy.\n\nSee the following sections for more information:\n\nIf you have any questions, feel free to\n• Send us an e-mail at contact@alphacephei.com\n• Join our group dedicated to speech recognition on Telegram @speech_recognition\n• We have a Wechat group which is pretty big, so it is invitation-only. Mail us to join the group and provide some information about yourself."
    },
    {
        "link": "https://github.com/alphacep/vosk-api/issues/986",
        "document": "Hi! I have a problem with speech recognition through a microphone using NAudio.\n\nWhen I write to a file, you can listen to it, but Vosk does not recognize speech"
    },
    {
        "link": "https://alphacephei.com/vosk/models",
        "document": "We have two types of models - big and small, small models are ideal for some limited task on mobile applications. They can run on smartphones, Raspberry Pi’s. They are also recommended for desktop applications. Small model typically is around 50Mb in size and requires about 300Mb of memory in runtime. Big models are for the high-accuracy transcription on the server. Big models require up to 16Gb in memory since they apply advanced AI algorithms. Ideally you run them on some high-end servers like i7 or latest AMD Ryzen. On AWS you can take a look on c5a machines and similar machines in other clouds.\n\nMost small model allow dynamic vocabulary reconfiguration. Big models are static the vocabulary can not be modified in runtime.\n\nThis is the list of models compatible with Vosk-API.\n\nTo add a new model here create an issue on Github.\n\nFor punctuation and case restoration we recommend the models trained with https://github.com/benob/recasepunc\n\nOther places where you can check for models which might be compatible:\n• https://montreal-forced-aligner.readthedocs.io/en/latest/pretrained_models.html (GMM models, not compatible but might be still useful)\n• https://github.com/goodatlas/zeroth - Korean Kaldi (just a recipe and data to train)\n• https://doc.linto.ai/#/services/linstt - LINTO project by Linagora with French, English and Arabic models\n• https://community.rhasspy.org/ - Rhasspy (some Kaldi models for Czech, probably even more)\n• https://github.com/Appen/UHV-OTS-Speech - repository from Appen for Scalable Data Annotation Pipeline for High-Quality Large Speech Datasets Development\n\nYou can train your model with Kaldi toolkit. The training is pretty standard - you need tdnn nnet3 model with i-vectors. You can check Vosk recipe for details:\n• For smaller mobile models watch the number of parameters\n• Train the model without pitch. It might be helpful for small amount of data, but for large database it doesn’t give the advantage but complicates the processing and increases response time.\n• Train ivector of dim 40 instead of standard 100 to save memory of mobile models.\n• Many Kaldi recipes are overcomplicated and do many unnecessary steps\n• PLEASE NOTE THAT THE SIMPLE GMM MODEL YOU TRAIN WITH “KALDI FOR DUMMIES” TUTORIAL DOES NOT WORK WITH VOSK. YOU NEED TO RUN VOSK RECIPE FROM START TO END, INCLUDING CHAIN MODEL TRAINING. You also need CUDA GPU to train. If you do not have a GPU, try to run Kaldi on Google Colab.\n\nOnce you trained the model arrange the files according to the following layout (see en-us-aspire for details):\n• - required for online-cmvn models, if present enables online cmvn on features.\n• - mfcc config file. Make sure you take mfcc_hires.conf version if you are using hires model (most external ones)\n• - provide default decoding beams and silence phones. you have to create this file yourself, it is not present in kaldi model\n• - optional file to create feature pipeline with pitch features. Might be missing if model doesn’t use pitch\n• - take ivector files from ivector extractor (optional folder if the model is trained with ivectors)\n• - this is the decoding graph, if you are not using lookahead\n• - use Gr.fst and HCLr.fst instead of one big HCLG.fst if you want to run rescoring\n• - carpa rescoring is optional but helpful in big models. Usually located inside data/lang_test_rescore\n• - also optional if you want to use rescoring, also used for interpolation with RNNLM\n• - RNNLM embedding for rescoring. Optional if you have it."
    },
    {
        "link": "https://forums.raspberrypi.com/viewtopic.php?t=298045",
        "document": "I tried to follow the instructions and ran into error - No module named 'sounddevice' when I tried to run the microphone example. It seems like one needs to install that module like so pip3 install sounddevice before trying microphone example.Also it was confusing which release do we need to install on Pi - I thought it would be vosk-0.3.17-cp38-cp38-linux_armv7l.whl as I'm running on Raspberry Pi 3B which as arm7l processor - 32 bit but that gave an error :pip3 install https://github.com/alphacep/vosk-api/re ... armv7l.whl vosk-0.3.17-cp38-cp38-linux_armv7l.whl is not a supported wheel on this platform.So had to install the default one - pip3 install vosk.@OP - It would help if you have better instructions and demo for Raspberry Pi as I can't wait to try this library for my personal voice assistant project. Thanks!"
    },
    {
        "link": "https://markheath.net/post/30-days-naudio-docs",
        "document": "One of the criticisms I often get about NAudio is that the documentation isn’t good enough. And although I have written numerous tutorials and articles about it (as well as two Pluralsight courses), I do accept that there is a lot of scope for improvement.\n\nSo I decided in November to see if I could write a short article or tutorial a day and use it to form the basis for a new set of documentation, especially now that CodePlex (which was the old home for NAudio documentation) is being shut down.\n\nI’ve implemented the documentation as markdown files in the GitHub repo, to make it as simple as possible to allow community contributions and improvements. I’ve just about managed to keep up with the goal of a document a day.\n\nHopefully I’ll be able to add to and expand on this list in the future, to make NAudio much more accessible to new beginners. I’m also planning to insist on contributors of new features adding a tutorial of their own to ensure the the documentation remains comprehensive going forwards.\n\nHere’s the full list of new articles I’ve written this month:\n• Encode to MP3 and other formats using MediaFoundationEncoder\n• Skip and Take Using OffsetSampleProvider\n• Adjust the pitch of audio using SmbPitchShiftingSampleProvider"
    },
    {
        "link": "https://github.com/naudio/NAudio",
        "document": "NAudio is an open source .NET audio library written by Mark Heath\n• Read audio from many standard file formats\n• WMA, AAC, MP4 and more others with Media Foundation\n• Convert between various forms of uncompressed audio\n• Change the number of channels - Mono to stereo, stereo to mono\n• Encode audio using any ACM or Media Foundation codec installed on your computer\n• Mix and manipulate audio streams using a 32-bit floating mixing engine\n• examine sample levels for the purposes of metering or waveform rendering\n• pass blocks of samples through an FFT for metering or DSP\n• delay, loop, or fade audio in and out\n• An extensible programming model\n• All base classes easily inherited from for you to add your custom components\n\nThe easiest way to install NAudio into your project is to install the latest NAudio NuGet package. Prerelease versions of NAudio are also often made available on NuGet.\n\nNAudio comes with several demo applications which are the quickest way to see how to use the various features of NAudio. You can explore the source code here.\n• Encode to MP3 and other formats using MediaFoundationEncoder\n• Understand how to convert between any audio formats you have codecs for\n• Skip and Take Using OffsetSampleProvider\n• Adjust the pitch of audio using SmbPitchShiftingSampleProvider\n• Fade audio in and out\n• Play and Record audio at the same time\n\nAdditional sources of documentation for NAudio are:\n\nIf you want to get up to speed as quickly as possible with NAudio programming, I recommend you watch these two Pluralsight courses. You will need to be a subscriber to access the content, but there is 10 hours of training material on NAudio, and it also will give you access to their vast training library on other programming topics.\n\nTo be successful developing applications that process digital audio, there are some key concepts that you need to understand. To help developers quickly get up to speed with what they need to know before trying to use NAudio, I have created the Digital Audio Fundamentals course, which covers sample rates, bit depths, file formats, codecs, decibels, clipping, aliasing, synthesis, visualisations, effects and much more. In particular, the fourth module on signal chains is vital background information if you are to be effective with NAudio.\n\nAudio Programming with NAudio is a follow-on course which contains seven hours of training material covering all the major features of NAudio. It is highly recommended that you take this course if you intend to create an application with NAudio.\n\nThe best way to learn how to use NAudio is to download the source code and look at the two demo applications - NAudioDemo and NAudioWpfDemo. These demonstrate several of the key capabilities of the NAudio framework. They also have the advantage of being kept up to date, whilst some of the tutorials you will find on the internet refer to old versions of NAudio.\n\nNAudio is an open source audio API for .NET written in C# by Mark Heath, with contributions from many other developers. It is intended to provide a comprehensive set of useful utility classes from which you can construct your own audio application.\n\nNAudio was created because the Framework Class Library that shipped with .NET 1.0 had no support for playing audio. The System.Media namespace introduced in .NET 2.0 provided a small amount of support, and the MediaElement in WPF and Silverlight took that a bit further. The vision behind NAudio is to provide a comprehensive set of audio related classes allowing easy development of utilities that play or record audio, or manipulate audio files in some way.\n\nCan I Use NAudio in my Project?\n\nNAudio is licensed under the MIT license which means that you can use it in whatever project you like including commercial projects. Of course we would love it if you share any bug-fixes or enhancements you made to the original NAudio project files.\n\nIs .NET Performance Good Enough for Audio?\n\nWhile .NET cannot compete with unmanaged languages for very low latency audio work, it still performs better than many people would expect. On a fairly modest PC, you can quite easily mix multiple WAV files together, including pass them through various effects and codecs, play back glitch free with a latency of around 50ms.\n\nHow can I get help?\n\nThere are three main ways to get help. First, you can raise an issue here on GitHub. This is the best option when you've written some code and want to ask why it's not working as you expect. I attempt to answer all questions, but since this is a spare time project, occasionally I get behind.\n\nYou can also ask on StackOverflow and tag your question with naudio, if your question is a \"how do I...\" sort of question. This gives you a better chance of getting a quick answer. Please try to search first to see if your question has already been answered elsewhere.\n\nFinally, I am occasionally able to offer paid support for situations where you need quick advice, bugfixes or new features. Please contact Mark Heath directly if you wish to pursue this option.\n\nI welcome contributions to NAudio and have accepted many patches, but if you want your code to be included, please familiarise yourself with the following guidelines:\n• Your submission must be your own work, and able to be released under the MIT license.\n• You will need to make sure your code conforms to the layout and naming conventions used elsewhere in NAudio.\n• Remember that there are many existing users of NAudio. A patch that changes the public interface is not likely to be accepted.\n• Try to write \"clean code\" - avoid long functions and long classes. Try to add a new feature by creating a new class rather than putting loads of extra code inside an existing one.\n• I don't usually accept contributions I can't test, so please write unit tests (using NUnit) if at all possible. If not, give a clear explanation of how your feature can be unit tested and provide test data if appropriate. Tell me what you did to test it yourself, including what operating systems and soundcards you used.\n• If you are adding a new feature, please consider writing a short tutorial on how to use it.\n• Unless your patch is a small bugfix, I will code review it and give you feedback. You will need to be willing to make the recommended changes before it can be integrated into the main code.\n• Patches should be provided using the Pull Request feature of GitHub.\n• Please also bear in mind that when you add a feature to NAudio, that feature will generate future support requests and bug reports. Are you willing to stick around on the forums and help out people using it?"
    },
    {
        "link": "https://stackoverflow.com/questions/6985512/recording-with-naudio-using-c-sharp",
        "document": "I am trying to record audio in C# using NAudio. After looking at the NAudio Chat Demo, I used some code from there to record.\n\nHere is the code:\n\nHowever, the eventHandler is not being called. I am using .NET version 'v2.0.50727' and compiling it as:"
    },
    {
        "link": "https://luisllamas.es/en/naudio",
        "document": "NAudio is an open-source library for .NET that allows us to work with audio files and recording on Windows. With NAudio, we can play, record, and process audio in a variety of formats.\n\nAmong the supported audio file formats, we find WAV, MP3, AAC, WMA, among others. It also provides the ability to encode and decode audio, as well as add filters or transformations.\n\nNAudio also provides a variety of audio effects and processors, such as reverb, chorus, delay, and EQ, that you can use to manipulate audio in real time.\n\nHow to use NAudio\n\nTo use NAudio in your project, you must first download and add the library to your Visual Studio solution. You can download the library from the NAudio GitHub page or through the NuGet package manager.\n\nNAudio is built around the WaveStream class, which represents a real-time audio stream or an audio file. We can use WaveStream to play or record audio, or process audio in real time.\n\nTo play audio, NAudio provides the WaveOut class, which sends the audio stream to the system’s audio output. To record audio, you can use the WaveIn class, which captures the audio stream from the system’s audio input.\n\nHere is a simple example that plays an audio file in WAV format:\n\nThis code creates an instance of the WaveOut class and an AudioFileReader object that reads the specified audio file. Then, WaveOut is initialized with AudioFileReader and the audio is played.\n\nHow can I process audio in real time with NAudio? To process audio in real time with NAudio, you can use the WaveStream class and the audio effects and processors classes provided by the library. Here is an example that adds a reverb effect to a real-time audio stream:\n\nThis code creates an instance of the WaveIn class, which captures the audio stream from the system’s audio input, and an instance of the WaveOut class, which sends the audio stream to the system’s audio output. Then, a WaveInProvider object is created that sends the captured audio stream to an EffectStream object that contains the reverb effect.\n\nNAudio is an open-source library, and all the code is available on the project’s website at https://github.com/naudio/NAudio"
    },
    {
        "link": "https://stackoverflow.com/questions/39789869/get-audio-input-channel-from-mixer-naudio-c-sharp",
        "document": "I am very New to Library and as well as develop audio type of file I have question that how do we get audio input from each channel of Mixer use USB Audio Interface connect to PC (Support ASIO), So this mixer support 8 Channel of Audio Input.\n\nThe idea of application is like this\n• None when user press on channel 1 button it will get channel 1 input to capture the voice of speaker on that particular channel\n• None when user press channel 2 button it will get voice from channel 2 (as separate Channel)\n\nSo I just wondering which library class I should use and is there any source code example or best practice for this kind of scenario (I am using C# to Develop)"
    }
]