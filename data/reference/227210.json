[
    {
        "link": "https://docs.python.org/3/library/asyncio-task.html",
        "document": "This section outlines high-level asyncio APIs to work with coroutines and Tasks.\n\nCoroutines declared with the async/await syntax is the preferred way of writing asyncio applications. For example, the following snippet of code prints “hello”, waits 1 second, and then prints “world”: Note that simply calling a coroutine will not schedule it to be executed: To actually run a coroutine, asyncio provides the following mechanisms:\n• None The function to run the top-level entry point “main()” function (see the above example.)\n• None Awaiting on a coroutine. The following snippet of code will print “hello” after waiting for 1 second, and then print “world” after waiting for another 2 seconds:\n• None The function to run coroutines concurrently as asyncio . Let’s modify the above example and run two coroutines concurrently: # Wait until both tasks are completed (should take Note that expected output now shows that the snippet runs 1 second faster than before:\n• None The class provides a more modern alternative to . Using this API, the last example becomes: # The await is implicit when the context manager exits. The timing and output should be the same as for the previous version.\n\nWe say that an object is an awaitable object if it can be used in an expression. Many asyncio APIs are designed to accept awaitables. There are three main types of awaitable objects: coroutines, Tasks, and Futures. Python coroutines are awaitables and therefore can be awaited from other coroutines: # Nothing happens if we just call \"nested()\". # A coroutine object is created but not awaited, # so it *won't run at all*. # Let's do it differently now and await it: In this documentation the term “coroutine” can be used for two closely related concepts: Tasks are used to schedule coroutines concurrently. When a coroutine is wrapped into a Task with functions like the coroutine is automatically scheduled to run soon: # \"task\" can now be used to cancel \"nested()\", or # can simply be awaited to wait until it is complete: A is a special low-level awaitable object that represents an eventual result of an asynchronous operation. When a Future object is awaited it means that the coroutine will wait until the Future is resolved in some other place. Future objects in asyncio are needed to allow callback-based code to be used with async/await. Normally there is no need to create Future objects at the application level code. Future objects, sometimes exposed by libraries and some asyncio APIs, can be awaited: A good example of a low-level function that returns a Future object is .\n\nTask groups combine a task creation API with a convenient and reliable way to wait for all tasks in the group to finish. An asynchronous context manager holding a group of tasks. Tasks can be added to the group using . All tasks are awaited when the context manager exits. Create a task in this task group. The signature matches that of . If the task group is inactive (e.g. not yet entered, already finished, or in the process of shutting down), we will close the given . Changed in version 3.13: Close the given coroutine if the task group is not active. \"Both tasks have completed now: The statement will wait for all tasks in the group to finish. While waiting, new tasks may still be added to the group (for example, by passing into one of the coroutines and calling in that coroutine). Once the last task has finished and the block is exited, no new tasks may be added to the group. The first time any of the tasks belonging to the group fails with an exception other than , the remaining tasks in the group are cancelled. No further tasks can then be added to the group. At this point, if the body of the statement is still active (i.e., hasn’t been called yet), the task directly containing the statement is also cancelled. The resulting will interrupt an , but it will not bubble out of the containing statement. Once all tasks have finished, if any tasks have failed with an exception other than , those exceptions are combined in an or (as appropriate; see their documentation) which is then raised. Two base exceptions are treated specially: If any task fails with or , the task group still cancels the remaining tasks and waits for them, but then the initial or is re-raised instead of or . If the body of the statement exits with an exception (so is called with an exception set), this is treated the same as if one of the tasks failed: the remaining tasks are cancelled and then waited for, and non-cancellation exceptions are grouped into an exception group and raised. The exception passed into , unless it is , is also included in the exception group. The same special case is made for and as in the previous paragraph. Task groups are careful not to mix up the internal cancellation used to “wake up” their with cancellation requests for the task in which they are running made by other parties. In particular, when one task group is syntactically nested in another, and both experience an exception in one of their child tasks simultaneously, the inner task group will process its exceptions, and then the outer task group will receive another cancellation and process its own exceptions. In the case where a task group is cancelled externally and also must raise an , it will call the parent task’s method. This ensures that a will be raised at the next , so the cancellation is not lost. Changed in version 3.13: Improved handling of simultaneous internal and external cancellations and correct preservation of cancellation counts. While terminating a task group is not natively supported by the standard library, termination can be achieved by adding an exception-raising task to the task group and ignoring the raised exception: \"\"\"Used to force termination of a task group.\"\"\" # add an exception-raising task to force the group to terminate\n\nReturn an asynchronous context manager that can be used to limit the amount of time spent waiting on something. delay can either be , or a float/int number of seconds to wait. If delay is , no time limit will be applied; this can be useful if the delay is unknown when the context manager is created. In either case, the context manager can be rescheduled after creation using . If takes more than 10 seconds to complete, the context manager will cancel the current task and handle the resulting internally, transforming it into a which can be caught and handled. The context manager is what transforms the into a , which means the can only be caught outside of the context manager. \"The long operation timed out, but we've handled it.\" \"This statement will run regardless.\" The context manager produced by can be rescheduled to a different deadline and inspected. should be an absolute time at which the context should time out, as measured by the event loop’s clock:\n• None If is , the timeout will never trigger.\n• None If , the timeout will trigger on the next iteration of the event loop. Return the current deadline, or if the current deadline is not set. Return whether the context manager has exceeded its deadline (expired). # We do not know the timeout when starting, so we pass ``None``. # We know the timeout now, so we reschedule it. \"Looks like we haven't finished on time.\" Similar to , except when is the absolute time to stop waiting, or . \"The long operation timed out, but we've handled it.\" \"This statement will run regardless.\" Wait for the aw awaitable to complete with a timeout. If aw is a coroutine it is automatically scheduled as a Task. timeout can either be or a float or int number of seconds to wait for. If timeout is , block until the future completes. If a timeout occurs, it cancels the task and raises . To avoid the task , wrap it in . The function will wait until the future is actually cancelled, so the total wait time may exceed the timeout. If an exception happens during cancellation, it is propagated. If the wait is cancelled, the future aw is also cancelled. # Wait for at most 1 second Changed in version 3.7: When aw is cancelled due to a timeout, waits for aw to be cancelled. Previously, it raised immediately. Changed in version 3.11: Raises instead of .\n\nRun and instances in the aws iterable concurrently and block until the condition specified by return_when. The aws iterable must not be empty. timeout (a float or int), if specified, can be used to control the maximum number of seconds to wait before returning. Note that this function does not raise . Futures or Tasks that aren’t done when the timeout occurs are simply returned in the second set. return_when indicates when this function should return. It must be one of the following constants: The function will return when any future finishes or is cancelled. The function will return when any future finishes by raising an exception. If no future raises an exception then it is equivalent to . The function will return when all futures finish or are cancelled. Unlike , does not cancel the futures when a timeout occurs. Changed in version 3.11: Passing coroutine objects to directly is forbidden. Run awaitable objects in the aws iterable concurrently. The returned object can be iterated to obtain the results of the awaitables as they finish. The object returned by can be iterated as an asynchronous iterator or a plain iterator. When asynchronous iteration is used, the originally-supplied awaitables are yielded if they are tasks or futures. This makes it easy to correlate previously-scheduled tasks with their results. Example: # earliest_connect is done. The result can be obtained by During asynchronous iteration, implicitly-created tasks will be yielded for supplied awaitables that aren’t tasks or futures. When used as a plain iterator, each iteration yields a new coroutine that returns the result or raises the exception of the next completed awaitable. This pattern is compatible with Python versions older than 3.13: # next_connect is not one of the original task objects. It must be # awaited to obtain the result value or raise the exception of the A is raised if the timeout occurs before all awaitables are done. This is raised by the loop during asynchronous iteration or by the coroutines yielded during plain iteration. Deprecated since version 3.10: Deprecation warning is emitted if not all awaitable objects in the aws iterable are Future-like objects and there is no running event loop. Changed in version 3.13: The result can now be used as either an asynchronous iterator or as a plain iterator (previously it was only a plain iterator).\n\nTasks are used to run coroutines in event loops. If a coroutine awaits on a Future, the Task suspends the execution of the coroutine and waits for the completion of the Future. When the Future is done, the execution of the wrapped coroutine resumes. Event loops use cooperative scheduling: an event loop runs one Task at a time. While a Task awaits for the completion of a Future, the event loop runs other Tasks, callbacks, or performs IO operations. Use the high-level function to create Tasks, or the low-level or functions. Manual instantiation of Tasks is discouraged. To cancel a running Task use the method. Calling it will cause the Task to throw a exception into the wrapped coroutine. If a coroutine is awaiting on a Future object during cancellation, the Future object will be cancelled. can be used to check if the Task was cancelled. The method returns if the wrapped coroutine did not suppress the exception and was actually cancelled. inherits from all of its APIs except and . An optional keyword-only context argument allows specifying a custom for the coro to run in. If no context is provided, the Task copies the current context and later runs its coroutine in the copied context. An optional keyword-only eager_start argument allows eagerly starting the execution of the at task creation time. If set to and the event loop is running, the task will start executing the coroutine immediately, until the first time the coroutine blocks. If the coroutine returns or raises without blocking, the task will be finished eagerly and will skip scheduling to the event loop. Changed in version 3.7: Added support for the module. Changed in version 3.8: Added the name parameter. Deprecated since version 3.10: Deprecation warning is emitted if loop is not specified and there is no running event loop. Return if the Task is done. A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled. Return the result of the Task. If the Task is done, the result of the wrapped coroutine is returned (or if the coroutine raised an exception, that exception is re-raised.) If the Task has been cancelled, this method raises a exception. If the Task’s result isn’t yet available, this method raises an exception. Return the exception of the Task. If the wrapped coroutine raised an exception that exception is returned. If the wrapped coroutine returned normally this method returns . If the Task has been cancelled, this method raises a exception. If the Task isn’t done yet, this method raises an exception. Add a callback to be run when the Task is done. This method should only be used in low-level callback-based code. See the documentation of for more details. This method should only be used in low-level callback-based code. See the documentation of for more details. Return the list of stack frames for this Task. If the wrapped coroutine is not done, this returns the stack where it is suspended. If the coroutine has completed successfully or was cancelled, this returns an empty list. If the coroutine was terminated by an exception, this returns the list of traceback frames. The frames are always ordered from oldest to newest. Only one stack frame is returned for a suspended coroutine. The optional limit argument sets the maximum number of frames to return; by default all available frames are returned. The ordering of the returned list differs depending on whether a stack or a traceback is returned: the newest frames of a stack are returned, but the oldest frames of a traceback are returned. (This matches the behavior of the traceback module.) Print the stack or traceback for this Task. This produces output similar to that of the traceback module for the frames retrieved by . The limit argument is passed to directly. The file argument is an I/O stream to which the output is written; by default output is written to . Return the coroutine object wrapped by the . This will return for Tasks which have already completed eagerly. See the Eager Task Factory. Changed in version 3.12: Newly added eager task execution means result may be . Return the object associated with the task. Return the name of the Task. If no name has been explicitly assigned to the Task, the default asyncio Task implementation generates a default name during instantiation. Set the name of the Task. The value argument can be any object, which is then converted to a string. In the default Task implementation, the name will be visible in the output of a task object. Request the Task to be cancelled. This arranges for a exception to be thrown into the wrapped coroutine on the next cycle of the event loop. The coroutine then has a chance to clean up or even deny the request by suppressing the exception with a … … … block. Therefore, unlike , does not guarantee that the Task will be cancelled, although suppressing cancellation completely is not common and is actively discouraged. Should the coroutine nevertheless decide to suppress the cancellation, it needs to call in addition to catching the exception. Changed in version 3.11: The parameter is propagated from cancelled task to its awaiter. The following example illustrates how coroutines can intercept the cancellation request: Return if the Task is cancelled. The Task is cancelled when the cancellation was requested with and the wrapped coroutine propagated the exception thrown into it. Decrement the count of cancellation requests to this Task. Note that once execution of a cancelled task completed, further calls to are ineffective. This method is used by asyncio’s internals and isn’t expected to be used by end-user code. In particular, if a Task gets successfully uncancelled, this allows for elements of structured concurrency like Task Groups and to continue running, isolating cancellation to the respective structured block. For example: # Outer code not affected by the timeout: While the block with and might get cancelled due to the timeout, should continue running even in case of the timeout. This is implemented with . context managers use in a similar fashion. If end-user code is, for some reason, suppressing cancellation by catching , it needs to call this method to remove the cancellation state. When this method decrements the cancellation count to zero, the method checks if a previous call had arranged for to be thrown into the task. If it hasn’t been thrown yet, that arrangement will be rescinded (by resetting the internal flag). Changed in version 3.13: Changed to rescind pending cancellation requests upon reaching zero. Return the number of pending cancellation requests to this Task, i.e., the number of calls to less the number of calls. Note that if this number is greater than zero but the Task is still executing, will still return . This is because this number can be lowered by calling , which can lead to the task not being cancelled after all if the cancellation requests go down to zero. This method is used by asyncio’s internals and isn’t expected to be used by end-user code. See for more details."
    },
    {
        "link": "https://medium.com/@obaff/asyncio-in-python-a-comprehensive-guide-with-examples-3a3f854017f9",
        "document": "Python’s asyncio library is designed for writing concurrent code using the async/await syntax. It provides a foundation for writing asynchronous programs, handling I/O-bound operations efficiently by non-blocking calls, making it suitable for programs that deal with network connections, file I/O, or databases. This tutorial will introduce the basics of asyncio, gradually moving from simple to more complex examples.\n\nAsynchronous programming allows a program to handle tasks concurrently without waiting for one task to finish before starting another. In synchronous programming, if a function call takes time (like downloading a file), the whole program waits. In contrast, asynchronous code can perform multiple tasks seemingly simultaneously by switching between them while waiting for responses (e.g., I/O operations).\n\nAsyncio is Python’s built-in library for writing asynchronous code. It allows the execution of non-blocking functions that can yield control to the event loop when waiting for I/O operations, freeing up the program to execute other tasks.\n\nThe key components of asyncio are:\n• Event Loop: The core of asyncio that executes coroutines, scheduling them as needed.\n• Futures/Tasks: Objects representing coroutines or other computations that are yet to be completed.\n\nThe basic structure of asyncio is built around coroutines. A coroutine is a function that can suspend execution and allows other coroutines to run. Coroutines are defined with the async def keyword, and execution is paused with await.\n\nExplanation:\n\n- `say_hello` is a coroutine, denoted by async def.\n\n- await pauses the coroutine, allowing the event loop to execute other tasks while waiting for asyncio.sleep(1) to finish.\n\nCoroutines are at the heart of asyncio. These functions return a coroutine object and need to be awaited or scheduled to run.\n\nHere, task1 and task2 run sequentially. The main() coroutine orchestrates the flow by awaiting each task in sequence.\n\nTo run multiple coroutines concurrently, we can use:\n\n- asyncio.gather(): Runs a collection of coroutines concurrently.\n\n- asyncio.create_task(): Schedules a coroutine as a separate task.\n\nExplanation:\n\n- Both task1 and task2 run concurrently, so the total execution time is determined by the longest-running task (2 seconds in this case).\n\nasyncio.create_task() creates separate tasks that run in the background, allowing you to start a coroutine without blocking the main flow.\n\nExplanation:\n\nHere, both tasks are scheduled and run concurrently. The await statements ensure that the main function waits for both tasks to complete.\n\nasyncio.wait_for() allows you to set a timeout for coroutines. If the coroutine doesn’t complete within the specified time, a `TimeoutError` is raised.\n\nExplanation:\n\nThe task takes 5 seconds to complete, but the wait_for timeout is set to 3 seconds, causing a TimeoutError.\n\nasyncio provides several synchronization primitives to coordinate multiple coroutines, including:\n\n- Locks: To prevent multiple coroutines from accessing shared resources simultaneously.\n\n- Event: Signals one or more coroutines when some event has occurred.\n\nThe asyncio.Lock() ensures that only one task at a time accesses the shared resource.\n\nWhen dealing with asynchronous code, exceptions can still occur. You can handle them using try/except blocks within the coroutine.\n\nLet’s put everything together and build a simple web scraper using aiohttp for asynchronous HTTP requests.\n\nExample: Web Scraper with asyncio and aiohttp\n\nExplanation:\n\n- This example uses aiohttp to fetch multiple URLs asynchronously.\n\n- asyncio.gather() is used to collect the responses concurrently, which significantly speeds up the process compared to making sequential requests.\n\nAsyncio is a powerful library in Python for writing concurrent code. It’s well-suited for I/O-bound and high-level structured network code. We’ve covered the basic and advanced features of asyncio, from simple coroutines and task scheduling to handling timeouts and synchronization. By understanding these concepts, you can build highly efficient applications that handle multiple tasks concurrently.\n\nNote: There are affiliate links in the article and if you buy something, I’ll get a commission at no extra cost to you.\n\nThis content is free, and by using the links, You’ll be supporting my work & that means a whole lot to me."
    },
    {
        "link": "https://stackoverflow.com/questions/42231161/asyncio-gather-vs-asyncio-wait-vs-asyncio-taskgroup",
        "document": "In addition to all the previous answers, I would like to tell about the different behavior of and in case they are cancelled.\n\nIf is cancelled, all submitted awaitables (that have not completed yet) are also cancelled.\n\nIf the ing task is cancelled, it simply throws an and the waited tasks remain intact.\n\nSometimes it becomes necessary to combine and functionality. For example, we want to wait for the completion of at least one task and cancel the rest pending tasks after that, and if the itself was canceled, then also cancel all pending tasks.\n\nAs real examples, let's say we have a disconnect event and a work task. And we want to wait for the results of the work task, but if the connection was lost, then cancel it. Or we will make several parallel requests, but upon completion of at least one response, cancel all others.\n\nIt could be done this way:"
    },
    {
        "link": "https://docs.python.org/3/library/asyncio.html",
        "document": "asyncio is a library to write concurrent code using the async/await syntax.\n\nasyncio is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc.\n\nasyncio is often a perfect fit for IO-bound and high-level structured network code.\n\nasyncio provides a set of high-level APIs to:\n• None run Python coroutines concurrently and have full control over their execution;\n\nAdditionally, there are low-level APIs for library and framework developers to:\n• None create and manage event loops, which provide asynchronous APIs for networking, running subprocesses, handling OS signals, etc;\n\nYou can experiment with an concurrent context in the REPL:\n\nRaises an auditing event with no arguments."
    },
    {
        "link": "https://stackoverflow.com/questions/40558484/how-to-use-asyncio-for-parallel-tasks",
        "document": "The asyncio documentation says below so asyncio tasks run concurrently but not parallelly.\n\nAnd, @asyncio.coroutine is deprecated since Python 3.7.14 and removed since Python 3.11.0 so instead, you should use as shown below:\n\nAnd for example, with this code below:\n\n, and are run serially as shown below:\n\nAnd, if using in them as shown below:\n\nThey are run alternately sleeping 1 second each time as shown below:\n\nAnd, if using in them as shown below:\n\nThey are run alternately without sleeping as shown below:"
    },
    {
        "link": "https://docs.python.org/3/library/asyncio-task.html",
        "document": "This section outlines high-level asyncio APIs to work with coroutines and Tasks.\n\nCoroutines declared with the async/await syntax is the preferred way of writing asyncio applications. For example, the following snippet of code prints “hello”, waits 1 second, and then prints “world”: Note that simply calling a coroutine will not schedule it to be executed: To actually run a coroutine, asyncio provides the following mechanisms:\n• None The function to run the top-level entry point “main()” function (see the above example.)\n• None Awaiting on a coroutine. The following snippet of code will print “hello” after waiting for 1 second, and then print “world” after waiting for another 2 seconds:\n• None The function to run coroutines concurrently as asyncio . Let’s modify the above example and run two coroutines concurrently: # Wait until both tasks are completed (should take Note that expected output now shows that the snippet runs 1 second faster than before:\n• None The class provides a more modern alternative to . Using this API, the last example becomes: # The await is implicit when the context manager exits. The timing and output should be the same as for the previous version.\n\nWe say that an object is an awaitable object if it can be used in an expression. Many asyncio APIs are designed to accept awaitables. There are three main types of awaitable objects: coroutines, Tasks, and Futures. Python coroutines are awaitables and therefore can be awaited from other coroutines: # Nothing happens if we just call \"nested()\". # A coroutine object is created but not awaited, # so it *won't run at all*. # Let's do it differently now and await it: In this documentation the term “coroutine” can be used for two closely related concepts: Tasks are used to schedule coroutines concurrently. When a coroutine is wrapped into a Task with functions like the coroutine is automatically scheduled to run soon: # \"task\" can now be used to cancel \"nested()\", or # can simply be awaited to wait until it is complete: A is a special low-level awaitable object that represents an eventual result of an asynchronous operation. When a Future object is awaited it means that the coroutine will wait until the Future is resolved in some other place. Future objects in asyncio are needed to allow callback-based code to be used with async/await. Normally there is no need to create Future objects at the application level code. Future objects, sometimes exposed by libraries and some asyncio APIs, can be awaited: A good example of a low-level function that returns a Future object is .\n\nTask groups combine a task creation API with a convenient and reliable way to wait for all tasks in the group to finish. An asynchronous context manager holding a group of tasks. Tasks can be added to the group using . All tasks are awaited when the context manager exits. Create a task in this task group. The signature matches that of . If the task group is inactive (e.g. not yet entered, already finished, or in the process of shutting down), we will close the given . Changed in version 3.13: Close the given coroutine if the task group is not active. \"Both tasks have completed now: The statement will wait for all tasks in the group to finish. While waiting, new tasks may still be added to the group (for example, by passing into one of the coroutines and calling in that coroutine). Once the last task has finished and the block is exited, no new tasks may be added to the group. The first time any of the tasks belonging to the group fails with an exception other than , the remaining tasks in the group are cancelled. No further tasks can then be added to the group. At this point, if the body of the statement is still active (i.e., hasn’t been called yet), the task directly containing the statement is also cancelled. The resulting will interrupt an , but it will not bubble out of the containing statement. Once all tasks have finished, if any tasks have failed with an exception other than , those exceptions are combined in an or (as appropriate; see their documentation) which is then raised. Two base exceptions are treated specially: If any task fails with or , the task group still cancels the remaining tasks and waits for them, but then the initial or is re-raised instead of or . If the body of the statement exits with an exception (so is called with an exception set), this is treated the same as if one of the tasks failed: the remaining tasks are cancelled and then waited for, and non-cancellation exceptions are grouped into an exception group and raised. The exception passed into , unless it is , is also included in the exception group. The same special case is made for and as in the previous paragraph. Task groups are careful not to mix up the internal cancellation used to “wake up” their with cancellation requests for the task in which they are running made by other parties. In particular, when one task group is syntactically nested in another, and both experience an exception in one of their child tasks simultaneously, the inner task group will process its exceptions, and then the outer task group will receive another cancellation and process its own exceptions. In the case where a task group is cancelled externally and also must raise an , it will call the parent task’s method. This ensures that a will be raised at the next , so the cancellation is not lost. Changed in version 3.13: Improved handling of simultaneous internal and external cancellations and correct preservation of cancellation counts. While terminating a task group is not natively supported by the standard library, termination can be achieved by adding an exception-raising task to the task group and ignoring the raised exception: \"\"\"Used to force termination of a task group.\"\"\" # add an exception-raising task to force the group to terminate\n\nReturn an asynchronous context manager that can be used to limit the amount of time spent waiting on something. delay can either be , or a float/int number of seconds to wait. If delay is , no time limit will be applied; this can be useful if the delay is unknown when the context manager is created. In either case, the context manager can be rescheduled after creation using . If takes more than 10 seconds to complete, the context manager will cancel the current task and handle the resulting internally, transforming it into a which can be caught and handled. The context manager is what transforms the into a , which means the can only be caught outside of the context manager. \"The long operation timed out, but we've handled it.\" \"This statement will run regardless.\" The context manager produced by can be rescheduled to a different deadline and inspected. should be an absolute time at which the context should time out, as measured by the event loop’s clock:\n• None If is , the timeout will never trigger.\n• None If , the timeout will trigger on the next iteration of the event loop. Return the current deadline, or if the current deadline is not set. Return whether the context manager has exceeded its deadline (expired). # We do not know the timeout when starting, so we pass ``None``. # We know the timeout now, so we reschedule it. \"Looks like we haven't finished on time.\" Similar to , except when is the absolute time to stop waiting, or . \"The long operation timed out, but we've handled it.\" \"This statement will run regardless.\" Wait for the aw awaitable to complete with a timeout. If aw is a coroutine it is automatically scheduled as a Task. timeout can either be or a float or int number of seconds to wait for. If timeout is , block until the future completes. If a timeout occurs, it cancels the task and raises . To avoid the task , wrap it in . The function will wait until the future is actually cancelled, so the total wait time may exceed the timeout. If an exception happens during cancellation, it is propagated. If the wait is cancelled, the future aw is also cancelled. # Wait for at most 1 second Changed in version 3.7: When aw is cancelled due to a timeout, waits for aw to be cancelled. Previously, it raised immediately. Changed in version 3.11: Raises instead of .\n\nRun and instances in the aws iterable concurrently and block until the condition specified by return_when. The aws iterable must not be empty. timeout (a float or int), if specified, can be used to control the maximum number of seconds to wait before returning. Note that this function does not raise . Futures or Tasks that aren’t done when the timeout occurs are simply returned in the second set. return_when indicates when this function should return. It must be one of the following constants: The function will return when any future finishes or is cancelled. The function will return when any future finishes by raising an exception. If no future raises an exception then it is equivalent to . The function will return when all futures finish or are cancelled. Unlike , does not cancel the futures when a timeout occurs. Changed in version 3.11: Passing coroutine objects to directly is forbidden. Run awaitable objects in the aws iterable concurrently. The returned object can be iterated to obtain the results of the awaitables as they finish. The object returned by can be iterated as an asynchronous iterator or a plain iterator. When asynchronous iteration is used, the originally-supplied awaitables are yielded if they are tasks or futures. This makes it easy to correlate previously-scheduled tasks with their results. Example: # earliest_connect is done. The result can be obtained by During asynchronous iteration, implicitly-created tasks will be yielded for supplied awaitables that aren’t tasks or futures. When used as a plain iterator, each iteration yields a new coroutine that returns the result or raises the exception of the next completed awaitable. This pattern is compatible with Python versions older than 3.13: # next_connect is not one of the original task objects. It must be # awaited to obtain the result value or raise the exception of the A is raised if the timeout occurs before all awaitables are done. This is raised by the loop during asynchronous iteration or by the coroutines yielded during plain iteration. Deprecated since version 3.10: Deprecation warning is emitted if not all awaitable objects in the aws iterable are Future-like objects and there is no running event loop. Changed in version 3.13: The result can now be used as either an asynchronous iterator or as a plain iterator (previously it was only a plain iterator).\n\nTasks are used to run coroutines in event loops. If a coroutine awaits on a Future, the Task suspends the execution of the coroutine and waits for the completion of the Future. When the Future is done, the execution of the wrapped coroutine resumes. Event loops use cooperative scheduling: an event loop runs one Task at a time. While a Task awaits for the completion of a Future, the event loop runs other Tasks, callbacks, or performs IO operations. Use the high-level function to create Tasks, or the low-level or functions. Manual instantiation of Tasks is discouraged. To cancel a running Task use the method. Calling it will cause the Task to throw a exception into the wrapped coroutine. If a coroutine is awaiting on a Future object during cancellation, the Future object will be cancelled. can be used to check if the Task was cancelled. The method returns if the wrapped coroutine did not suppress the exception and was actually cancelled. inherits from all of its APIs except and . An optional keyword-only context argument allows specifying a custom for the coro to run in. If no context is provided, the Task copies the current context and later runs its coroutine in the copied context. An optional keyword-only eager_start argument allows eagerly starting the execution of the at task creation time. If set to and the event loop is running, the task will start executing the coroutine immediately, until the first time the coroutine blocks. If the coroutine returns or raises without blocking, the task will be finished eagerly and will skip scheduling to the event loop. Changed in version 3.7: Added support for the module. Changed in version 3.8: Added the name parameter. Deprecated since version 3.10: Deprecation warning is emitted if loop is not specified and there is no running event loop. Return if the Task is done. A Task is done when the wrapped coroutine either returned a value, raised an exception, or the Task was cancelled. Return the result of the Task. If the Task is done, the result of the wrapped coroutine is returned (or if the coroutine raised an exception, that exception is re-raised.) If the Task has been cancelled, this method raises a exception. If the Task’s result isn’t yet available, this method raises an exception. Return the exception of the Task. If the wrapped coroutine raised an exception that exception is returned. If the wrapped coroutine returned normally this method returns . If the Task has been cancelled, this method raises a exception. If the Task isn’t done yet, this method raises an exception. Add a callback to be run when the Task is done. This method should only be used in low-level callback-based code. See the documentation of for more details. This method should only be used in low-level callback-based code. See the documentation of for more details. Return the list of stack frames for this Task. If the wrapped coroutine is not done, this returns the stack where it is suspended. If the coroutine has completed successfully or was cancelled, this returns an empty list. If the coroutine was terminated by an exception, this returns the list of traceback frames. The frames are always ordered from oldest to newest. Only one stack frame is returned for a suspended coroutine. The optional limit argument sets the maximum number of frames to return; by default all available frames are returned. The ordering of the returned list differs depending on whether a stack or a traceback is returned: the newest frames of a stack are returned, but the oldest frames of a traceback are returned. (This matches the behavior of the traceback module.) Print the stack or traceback for this Task. This produces output similar to that of the traceback module for the frames retrieved by . The limit argument is passed to directly. The file argument is an I/O stream to which the output is written; by default output is written to . Return the coroutine object wrapped by the . This will return for Tasks which have already completed eagerly. See the Eager Task Factory. Changed in version 3.12: Newly added eager task execution means result may be . Return the object associated with the task. Return the name of the Task. If no name has been explicitly assigned to the Task, the default asyncio Task implementation generates a default name during instantiation. Set the name of the Task. The value argument can be any object, which is then converted to a string. In the default Task implementation, the name will be visible in the output of a task object. Request the Task to be cancelled. This arranges for a exception to be thrown into the wrapped coroutine on the next cycle of the event loop. The coroutine then has a chance to clean up or even deny the request by suppressing the exception with a … … … block. Therefore, unlike , does not guarantee that the Task will be cancelled, although suppressing cancellation completely is not common and is actively discouraged. Should the coroutine nevertheless decide to suppress the cancellation, it needs to call in addition to catching the exception. Changed in version 3.11: The parameter is propagated from cancelled task to its awaiter. The following example illustrates how coroutines can intercept the cancellation request: Return if the Task is cancelled. The Task is cancelled when the cancellation was requested with and the wrapped coroutine propagated the exception thrown into it. Decrement the count of cancellation requests to this Task. Note that once execution of a cancelled task completed, further calls to are ineffective. This method is used by asyncio’s internals and isn’t expected to be used by end-user code. In particular, if a Task gets successfully uncancelled, this allows for elements of structured concurrency like Task Groups and to continue running, isolating cancellation to the respective structured block. For example: # Outer code not affected by the timeout: While the block with and might get cancelled due to the timeout, should continue running even in case of the timeout. This is implemented with . context managers use in a similar fashion. If end-user code is, for some reason, suppressing cancellation by catching , it needs to call this method to remove the cancellation state. When this method decrements the cancellation count to zero, the method checks if a previous call had arranged for to be thrown into the task. If it hasn’t been thrown yet, that arrangement will be rescinded (by resetting the internal flag). Changed in version 3.13: Changed to rescind pending cancellation requests upon reaching zero. Return the number of pending cancellation requests to this Task, i.e., the number of calls to less the number of calls. Note that if this number is greater than zero but the Task is still executing, will still return . This is because this number can be lowered by calling , which can lead to the task not being cancelled after all if the cancellation requests go down to zero. This method is used by asyncio’s internals and isn’t expected to be used by end-user code. See for more details."
    },
    {
        "link": "https://news.ycombinator.com/item?id=35073136",
        "document": ""
    },
    {
        "link": "https://stackoverflow.com/questions/29269370/how-to-properly-create-and-run-concurrent-tasks-using-pythons-asyncio-module",
        "document": "I am trying to properly understand and implement two concurrently running objects using Python 3's relatively new module.\n\nIn a nutshell, asyncio seems designed to handle asynchronous processes and concurrent execution over an event loop. It promotes the use of (applied in async functions) as a callback-free way to wait for and use a result, without blocking the event loop. (Futures and callbacks are still a viable alternative.)\n\nIt also provides the class, a specialized subclass of designed to wrap coroutines. Preferably invoked by using the method. The intended use of asyncio tasks is to allow independently running tasks to run 'concurrently' with other tasks within the same event loop. My understanding is that are connected to the event loop which then automatically keeps driving the coroutine between statements.\n\nI like the idea of being able to use concurrent Tasks without needing to use one of the classes, but I haven't found much elaboration on implementation.\n\nThis is how I'm currently doing it:\n\nIn the case of trying to concurrently run two looping Tasks, I've noticed that unless the Task has an internal expression, it will get stuck in the loop, effectively blocking other tasks from running (much like a normal loop). However, as soon the Tasks have to (a)wait, they seem to run concurrently without an issue.\n\nThus, the statements seem to provide the event loop with a foothold for switching back and forth between the tasks, giving the effect of concurrency.\n\nDoes this implementation pass for a 'proper' example of concurrent looping Tasks in ?\n\nIs it correct that the only way this works is for a to provide a blocking point ( expression) in order for the event loop to juggle multiple tasks?\n\n2022 UPDATE: Please note that the API has changed fairly substantially since this question was asked. See the newly marked as correct answer which now shows the correct use of the API given Python 3.10. I still recommend the answer from @dano for broader knowledge of how this works under the hood."
    },
    {
        "link": "https://medium.com/@moraneus/mastering-pythons-asyncio-a-practical-guide-0a673265cf04",
        "document": "When you dive into Python’s world, one gem that truly shines for handling modern web and network tasks is . This toolkit is Python's answer to writing clean, efficient, and scalable code for concurrent I/O operations. It might sound a bit intimidating at first, with its event loops, coroutines, and futures. But once you get the hang of it, you'll wonder how you ever lived without it. So, let's break it down, step by step, with examples and a peek at how things look on the other side of the async fence.\n\nBefore jumping into examples, it’s crucial to grasp the core concepts of :\n• Event Loop: The central execution device provided by . It manages and distributes the execution of different tasks. It's responsible for handling events and scheduling asynchronous routines.\n• Coroutines: Asynchronous functions declared with . These functions can be paused and resumed at await points, allowing I/O operations to run in the background.\n• Futures: Objects that represent the result of work that has not yet been completed. They are returned from tasks scheduled by the event loop.\n• Tasks: Scheduled coroutines that are wrapped into a Future object by the event loop, allowing their execution.\n\nFirst off, is all about writing code that can do multiple things at once, without actually doing them at the same time. It’s like having a chef in a kitchen who starts cooking a stew, and knows it’ll take time to simmer, so they begin prepping a salad instead of just standing around. This is the essence of async programming — keep moving efficiently without unnecessary waiting.\n\nThe keyword in Python is an essential part of asynchronous programming, introduced in Python 3.5. It is used to pause the execution of an function until an awaitable object (like coroutines, Tasks, Futures, or I/O) completes, allowing other tasks to run in the meantime. This key feature enables efficient handling of I/O-bound and high-level structured network code.\n• Context: can only be used inside functions. Attempting to use it outside such a context results in a syntax error.\n• Purpose: Its primary purpose is to yield control back to the event loop, suspending the execution of the enclosing coroutine until the awaited object is resolved. This non-blocking behavior is what makes asynchronous programming efficient, especially for I/O-bound tasks.\n• Awaitables: The objects that can be used with must be awaitable. The most common awaitables are coroutines declared with , but others include asyncio Tasks, Futures, or any object with an method.\n\nImagine you’re tasked with printing “Hello, World!” after a 2-second pause. The synchronous approach is straightforward:\n\nIt does the job, but everything comes to a halt while waiting for those 2 seconds.\n\nNow, let’s switch gears to , showing the asynchronous way:\n\nWith , while we wait, the event loop can do other tasks, like checking emails or playing a tune, making our code non-blocking and more efficient:\n\nIn this modified version, the function uses to run and concurrently. This means that while the program is waiting for the function to complete its 2-second sleep, it starts and potentially completes the function, effectively doing another task during the wait time.\n\nFetching web pages is a classic example to demonstrate the power of async programming. Let’s compare fetching URLs synchronously vs. asynchronously.\n\nSynchronous HTTP requests is mostly made by the library, fetching two web pages in a row looks something like this:\n\nThis code is as simple as it gets, but it waits idly for each request to complete before moving to the next.\n\nLet’s amp up the efficiency with and which can be used for asynchronous HTTP requests:\n\nThis async version doesn’t wait around. While one page is being fetched, it starts on the next, drastically cutting down total wait time.\n\nLet’s explore a different use case for concurrent execution with , moving away from web requests. This time, we'll focus on reading multiple files asynchronously. This can be particularly useful when dealing with large files or I/O-bound tasks that do not involve network communication.\n\nIn a synchronous setup, reading multiple files one after the other can significantly increase execution time, especially with large files:\n\nFor the asynchronous version, we’ll use , a library that provides support for asynchronous file operations. If you haven't installed yet, you can do so using pip:\n\nWith , we can perform file I/O operations without blocking the event loop, allowing us to read multiple files concurrently.\n\nThe asynchronous version, by leveraging and , allows for concurrent reading of multiple files. This approach significantly reduces the total execution time compared to the synchronous version, which reads each file one after the other. By performing I/O operations concurrently, we can improve the efficiency of programs that need to handle multiple file operations.\n\nSometimes, you can’t escape synchronous functions but still want to enjoy the async ride. Here’s how you can mix them:\n\nThe provided code snippet demonstrates how to integrate synchronous functions within an asynchronous environment using Python’s library.\n• This async function demonstrates how to run the synchronous in a way that does not block the event loop. It achieves this by utilizing .\n• schedules to run in a separate thread or process, depending on the executor used. The default executor ( specified as the first argument) runs tasks in a thread pool.\n• is used to wait for the completion of without blocking the event loop, allowing other asynchronous operations to progress in the meantime.\n• The async function showcases how to run both synchronous and asynchronous tasks together without blocking.\n• is used to schedule concurrent execution of the and potentially other asynchronous tasks. By using , you ensure that the event loop can manage multiple tasks, running them concurrently where possible.\n• Finally, is called to run the coroutine, which effectively starts the event loop and executes the tasks scheduled within .\n\nWhy Is This Approach Needed?\n• Integration of Legacy Code: In real-world applications, you often encounter legacy code that is synchronous in nature. Rewriting large codebases for async compatibility is not always feasible. This approach allows you to integrate such code into your async applications seamlessly.\n• Working with Blocking I/O: Some operations, especially those involving blocking I/O, don’t have asynchronous equivalents, or you might be working with third-party libraries that only offer synchronous functions. This technique allows those operations to be offloaded to a thread, freeing the event loop to handle other async tasks.\n• CPU-bound Tasks: Although CPU-bound tasks are usually better handled by multiprocessing due to Python’s Global Interpreter Lock (GIL), you might sometimes choose to run them in threads for simplicity or because the computational overhead is not excessively high. Using allows these tasks to coexist with I/O-bound asynchronous tasks.\n\nIn Python’s asynchronous programming model, a is a low-level awaitable object that represents an eventual result of an asynchronous operation. When you create a Future, you're essentially declaring a placeholder for a result that will be available at some point in the future. Futures are a crucial part of the library, allowing for fine-grained control over asynchronous operations.\n• Role: Futures are used to bridge low-level asynchronous operations with high-level asyncio applications. They provide a way to manage the state of an asynchronous operation: pending, finished (with a result), or failed (with an exception).\n• Usage: Typically, you don’t need to create Futures yourself when using high-level functions and constructs (like Tasks, which are a subclass of Future). However, understanding Futures is essential for interfacing with lower-level async APIs or when building complex asynchronous systems.\n\nA Future object has several key methods and properties:\n• : Sets the result of the Future. This will mark it as done and notify all awaiting coroutines.\n• : Sets an exception as the result of the Future. This also marks it as done but will raise the exception when awaited.\n• : Adds a callback function to be called when the Future is done (either completed with a result or an exception).\n• : Returns the result of the Future. If the Future is not done, it will raise an . If the Future is completed with an exception, this method will re-raise the exception.\n• : Returns if the Future is done. A Future is considered done if it has a result or an exception.\n• is an async function simulating an asynchronous task that takes a object and some as arguments. It waits for 1 second to mimic some async work. Based on the value, it either sets a result on the using or raises an exception using .\n• is a callback function that prints the result of the Future once it's done. It checks if the operation succeeded or failed by calling , which either returns the result or re-raises the exception set in the Future.\n• In the coroutine, a Future object is created, and is added as its callback using . The is then awaited with the Future and sample data (\"success\" or any other value to simulate failure).\n• After completed, check if the Future is done using . It then attempts to print the result directly, handling any potential exceptions.\n\nThis example succinctly demonstrates the basic mechanisms of managing asynchronous operations with Futures in Python’s asyncio, including setting results, handling exceptions, using callbacks, and retrieving operation outcomes.\n\nAdopting in Python applications can significantly improve the performance and scalability of I/O-bound and network-driven programs. By understanding and applying the concepts of event loops, coroutines, futures, and tasks, developers can write efficient, non-blocking code that can handle thousands of simultaneous connections with ease. The examples provided in this article, are few, but yet, showcase the versatility of and demonstrate how it can be used to achieve concurrency in Python applications, offering a clear advantage over traditional synchronous code for certain types of tasks.\n\nIf you enjoyed this article and found it valuable, please consider giving it a clap to show your support. Feel free to explore my other articles, where I cover a wide range of topics related to Python programming and others. By following me, you’ll stay updated on my latest content and insights. I look forward to sharing more knowledge and connecting with you through future articles. Until then, keep coding, keep learning, and most importantly, enjoy the journey!"
    },
    {
        "link": "https://linkedin.com/pulse/mastering-performance-optimization-strategies-python-guide-mohyudin-n0q3f",
        "document": "Python, with its simplicity and versatility, has become one of the most popular programming languages across various domains. However, as projects scale up or deal with resource-intensive tasks, performance optimization becomes crucial. In this comprehensive guide, we'll explore a variety of strategies to optimize Python code for enhanced speed and efficiency.\n\n- Profiling tools such as cProfile and line_profiler help identify bottlenecks in your code by analyzing its execution time and resource usage.\n\n- Benchmarking libraries like timeit enable you to compare the performance of different implementations and make informed decisions.\n\n- Choosing the right algorithms and data structures can significantly impact performance. Dive into resources like \"Algorithm Design Manual\" by Steven S. Skiena to understand algorithmic efficiency.\n\n- Learn about Python's multiprocessing and threading modules, as well as libraries like concurrent.futures and asyncio for concurrent programming paradigms.\n\n- Delve into the world of JIT compilation with tools like PyPy and Numba, which dynamically compile Python code to machine code for improved performance.\n\n- Understand the trade-offs between interpreted and compiled code and choose the right approach based on your project requirements.\n\n- Watch educational videos and talks on platforms like YouTube, with channels such as PyCon and Tech with Tim offering valuable insights into Python optimization techniques.\n\nOptimizing the performance of your Python code is essential for delivering efficient and scalable software solutions. By employing profiling tools, algorithmic efficiency, code optimization techniques, parallelism, JIT compilation, and leveraging external resources, you can enhance the speed and efficiency of your Python applications. Keep exploring and experimenting with these strategies to continually improve the performance of your Python projects."
    }
]