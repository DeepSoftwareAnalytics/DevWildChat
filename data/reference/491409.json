[
    {
        "link": "https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html",
        "document": "Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, ftp, s3, and file. For file URLs, a host is expected. A local file could be: . If you want to pass in a path object, pandas accepts any . By file-like object, we refer to objects with a method, such as a file handle (e.g. via builtin function) or . Deprecated since version 2.1.0: Passing byte strings is deprecated. To read from a byte string, wrap it in a object.\n\nColumn (0-indexed) to use as the row labels of the DataFrame. Pass None if there is no such column. If a list is passed, those columns will be combined into a . If a subset of data is selected with , index_col is based on the subset. Missing values will be forward filled to allow roundtripping with for . To avoid forward filling the missing values use after reading the data instead of .\n\nData type for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32} Use to preserve data as stored in Excel and not interpret dtype, which will necessarily result in dtype. If converters are specified, they will be applied INSTEAD of dtype conversion. If you use , it will infer the dtype of each column based on the data.\n\nIf io is not a buffer or path, this must be set to identify io. Engine compatibility : When , the following logic will be used to determine the engine:\n• None If is an OpenDocument format (.odf, .ods, .odt), then odf will be used.\n• None Otherwise if is an xls format, will be used.\n• None Otherwise if is in xlsb format, will be used.\n• None Otherwise will be used.\n\nWhether or not to include the default NaN values when parsing the data. Depending on whether is passed in, the behavior is as follows:\n• None If is True, and are specified, is appended to the default NaN values used for parsing.\n• None If is True, and are not specified, only the default NaN values are used for parsing.\n• None If is False, and are specified, only the NaN values specified are used for parsing.\n• None If is False, and are not specified, no strings will be parsed as NaN. Note that if is passed in as False, the and parameters will be ignored.\n\nThe behavior is as follows:\n• None . If True -> try parsing the index.\n• None of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.\n• None of lists. e.g. If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column. If a column or index contains an unparsable date, the entire column or index will be returned unaltered as an object data type. If you don`t want to parse some cells as date just change their type in Excel to “Text”. For non-standard datetime parsing, use after .\n\nFunction to use for converting a sequence of string columns to an array of datetime instances. The default uses to do the conversion. Pandas will try to call in three different ways, advancing to the next if an exception occurs: 1) Pass one or more arrays (as defined by ) as arguments; 2) concatenate (row-wise) the string values from the columns defined by into a single array and pass that; and 3) call once for each row using one or more strings (corresponding to the columns defined by ) as arguments. Deprecated since version 2.0.0: Use instead, or read in as and then apply as-needed."
    },
    {
        "link": "https://geeksforgeeks.org/working-with-excel-files-using-pandas",
        "document": "Excel sheets are very instinctive and user-friendly, which makes them ideal for manipulating large datasets even for less technical folks. If you are looking for places to learn to manipulate and automate stuff in Excel files using Python, look no further. You are at the right place.\n\nIn this article, you will learn how to use Pandas to work with Excel spreadsheets. In this article we will learn about:\n\nTo install Pandas in Python, we can use the following command in the command prompt:\n\nTo install Pandas in Anaconda, we can use the following command in Anaconda Terminal:\n\nFirst of all, we need to import the Pandas module which can be done by running the command:\n\nInput File: Let’s suppose the Excel file looks like this\n\nNow we can import the Excel file using the read_excel function in Pandas to read Excel file using Pandas in Python. The second statement reads the data from Excel and stores it into a pandas Data Frame which is represented by the variable newData.\n\nIf there are multiple sheets in the Excel workbook, the command will import data from the first sheet. To make a data frame with all the sheets in the workbook, the easiest method is to create different data frames separately and then concatenate them. The read_excel method takes argument sheet_name and index_col where we can specify the sheet of which the frame should be made of and index_col specifies the title column, as is shown below:\n\nThe third statement concatenates both sheets. Now to check the whole data frame, we can simply run the following command:\n\nTo view 5 columns from the top and from the bottom of the data frame, we can run the command. This head() and tail() method also take arguments as numbers for the number of columns to show.\n\nThe shape() method can be used to view the number of rows and columns in the data frame as follows:\n\nIf any column contains numerical data, we can sort that column using the sort_values() method in pandas as follows:\n\nNow, let’s suppose we want the top 5 values of the sorted column, we can use the head() method here:\n\nWe can do that with any numerical column of the data frame as shown below:\n\nNow, suppose our data is mostly numerical. We can get the statistical information like mean, max, min, etc. about the data frame using the describe() method as shown below:\n\nThis can also be done separately for all the numerical columns using the following command:\n\nOther statistical information can also be calculated using the respective methods. Like in Excel, formulas can also be applied, and calculated columns can be created as follows:\n\nAfter operating on the data in the data frame, we can export the data back to an Excel file using the method to_excel. For this, we need to specify an output Excel file where the transformed data is to be written, as shown below:\n\nHow to Use Pandas with Excel Files?\n\nPandas provides powerful tools to read from and write to Excel files, making it easy to integrate Excel data with your Python scripts. You can read Excel files using the function. It requires the or library for files or the library for files. To write a DataFrame to an Excel file, you can use the method of the DataFrame class. It requires the library to write to files. # Write the DataFrame to an Excel file \n\n\n\nHow to Extract Data from Excel Using Pandas?\n\nCan We Read XLSX File in Pandas?\n\nIs Pandas Better Than SQL?\n\nDo I Need openpyxl for Pandas?"
    },
    {
        "link": "https://dataquest.io/blog/excel-and-pandas",
        "document": "Using Excel with Python and Pandas\n\nWhy learn to work with Excel with Python? Excel is one of the most popular and widely-used data tools; it's hard to find an organization that doesn't work with it in some way. From analysts, to sales VPs, to CEOs, various professionals use Excel for both quick stats and serious data crunching.\n\nWith Excel being so pervasive, data professionals must be familiar with it. Working with data in Python or R offers serious advantages over Excel's UI, so finding a way to work with Excel using code is critical. Thankfully, there's a great tool already out there for using Excel with Python called .\n\nPandas has excellent methods for reading all kinds of data from Excel files. You can also export your results from pandas back to Excel, if that's preferred by your intended audience. Pandas is great for other routine data analysis tasks, such as:\n• taking cleaned and processed data to any number of data tools\n\nPandas is better at automating data processing tasks than Excel, including processing Excel files.\n\nIn this tutorial, we are going to show you how to work with Excel files in pandas. We will cover the following concepts.\n• setting up your computer with the necessary software\n• reading in data from Excel files into pandas\n• visualizing data in pandas using the matplotlib visualization library\n\nNote that this tutorial does not provide a deep dive into pandas. To explore pandas more, check out our course.\n\nWe will use Python 3 and Jupyter Notebook to demonstrate the code in this tutorial.In addition to Python and Jupyter Notebook, you will need the following Python modules:\n\nThere are multiple ways to get set up with all the modules. We cover three of the most common scenarios below.\n• If you have Python installed via Anaconda package manager, you can install the required modules using the command . For example, to install pandas, you would execute the command - .\n• If you already have a regular, non-Anaconda Python installed on the computer, you can install the required modules using . Open your command line program and execute command to install a module. You should replace with the actual name of the module you are trying to install. For example, to install pandas, you would execute command - .\n• If you don't have Python already installed, you should get it through the Anaconda package manager. Anaconda provides installers for Windows, Mac, and Linux Computers. If you choose the full installer, you will get all the modules you need, along with Python and pandas within a single package. This is the easiest and fastest way to get started.\n\nIn this tutorial, we will use a multi-sheet Excel file we created from Kaggle's IMDB Scores data. You can download the file here.\n\nOur Excel file has three sheets: '1900s,' '2000s,' and '2010s.' Each sheet has data for movies from those years.\n\nWe will use this data set to find the ratings distribution for the movies, visualize movies with highest ratings and net earnings and calculate statistical information about the movies. We will be analyzing and exploring this data using Python and pandas, thus demonstrating pandas capabilities for working with Excel data in Python.\n\nWe need to first import the data from the Excel file into pandas. To do that, we start by importing the pandas module.\n\nWe then use the pandas' read_excel method to read in data from the Excel file. The easiest way to call this method is to pass the file name. If no sheet name is specified then it will read the first sheet in the index (as shown below).\n\nHere, the method read the data from the Excel file into a pandas DataFrame object. Pandas defaults to storing data in DataFrames. We then stored this DataFrame into a variable called .\n\nPandas has a built-in method that we can use to easily display the first few rows of our DataFrame. If no argument is passed, it will display first five rows. If a number is passed, it will display the equal number of rows from the top.\n\nExcel files quite often have multiple sheets and the ability to read a specific sheet or all of them is very important. To make this easy, the pandas method takes an argument called that tells pandas which sheet to read in the data from. For this, you can either use the sheet name or the sheet number. Sheet numbers start with zero. If the argument is not given, it defaults to zero and pandas will import the first sheet.\n\nBy default, pandas will automatically assign a numeric index or row label starting with zero. You may want to leave the default index as such if your data doesn't have a column with unique values that can serve as a better index. In case there is a column that you feel would serve as a better index, you can override the default behavior by setting property to a column. It takes a numeric value for setting a single column as index or a list of numeric values for creating a multi-index.\n\nIn the below code, we are choosing the first column, 'Title', as index (index=0) by passing zero to the argument.\n\nAs you noticed above, our Excel data file has three sheets. We already read the first sheet in a DataFrame above. Now, using the same syntax, we will read in rest of the two sheets too.\n\nSince all the three sheets have similar data but for different recordsmovies, we will create a single DataFrame from all the three DataFrames we created above. We will use the pandas method for this and pass in the names of the three DataFrames we just created and assign the results to a new DataFrame object, . By keeping the DataFrame name same as before, we are over-writing the previously created DataFrame.\n\nWe can check if this concatenation by checking the number of rows in the combined DataFrame by calling the method on it that will give us the number of rows and columns.\n\nUsing the ExcelFile class to read multiple sheets\n\nWe can also use the ExcelFile class to work with multiple sheets from the same Excel file. We first wrap the Excel file using and then pass it to method.\n\nIf you are reading an Excel file with a lot of sheets and are creating a lot of DataFrames, is more convenient and efficient in comparison to . With ExcelFile, you only need to pass the Excel file once, and then you can use it to get the DataFrames. When using , you pass the Excel file every time and hence the file is loaded again for every sheet. This can be a huge performance drag if the Excel file has many sheets with a large number of rows.\n\nNow that we have read in the movies data set from our Excel file, we can start exploring it using pandas. A pandas DataFrame stores the data in a tabular format, just like the way Excel displays the data in a sheet. Pandas has a lot of built-in methods to explore the DataFrame we created from the Excel file we just read in.\n\nWe already introduced the method in the previous section that displays few rows from the top from the DataFrame. Let's look at few more methods that come in handy while exploring the data set.\n\nWe can use the method to find out the number of rows and columns for the DataFrame.\n\nThis tells us our Excel file has 5042 records and 25 columns or observations. This can be useful in reporting the number of records and columns and comparing that with the source data set.\n\nWe can use the method to view the bottom rows. If no parameter is passed, only the bottom five rows are returned.\n\nIn Excel, you're able to sort a sheet based on the values in one or more columns. In pandas, you can do the same thing with the method. For example, let's sort our movies DataFrame based on the Gross Earnings column.\n\nSince we have the data sorted by values in a column, we can do few interesting things with it. For example, we can display the top 10 movies by Gross Earnings.\n\nWe can also create a plot for the top 10 movies by Gross Earnings. Pandas makes it easy to visualize your data with plots and charts through matplotlib, a popular data visualization library. With a couple lines of code, you can start plotting. Moreover, matplotlib plots work well inside Jupyter Notebooks since you can displace the plots right under the code.\n\nFirst, we import the matplotlib module and set matplotlib to display the plots right in the Jupyter Notebook.\n\nWe will draw a bar plot where each bar will represent one of the top 10 movies. We can do this by calling the plot method and setting the argument to . This tells to draw a horizontal bar plot.\n\nLet's create a histogram of IMDB Scores to check the distribution of IMDB Scores across all movies. Histograms are a good way to visualize the distribution of a data set. We use the method on the IMDB Scores series from our movies DataFrame and pass it the argument.\n\nThis data visualization suggests that most of the IMDB Scores fall between six and eight.\n\nGetting statistical information about the data\n\nPandas has some very handy methods to look at the statistical data about our data set. For example, we can use the method to get a statistical summary of the data set.\n\nThe method displays below information for each of the columns.\n• the count or number of values\n\nPlease note that this information will be calculated only for the numeric values.\n\nWe can also use the corresponding method to access this information one at a time. For example, to get the mean of a particular column, you can use the method on that column.\n\nJust like mean, there are methods available for each of the statistical information we want to access. You can read about these methods in our free pandas cheat sheet.\n\nReading files with no header and skipping records\n\nEarlier in this tutorial, we saw some ways to read a particular kind of Excel file that had headers and no rows that needed skipping. Sometimes, the Excel sheet doesn't have any header row. For such instances, you can tell pandas not to consider the first row as header or columns names. And If the Excel sheet's first few rows contain data that should not be read in, you can ask the method to skip a certain number of rows, starting from the top.\n\nFor example, look at the top few rows of this Excel file.\n\nThis file obviously has no header and first four rows are not actual records and hence should not be read in. We can tell read_excel there is no header by setting argument to and we can skip first four rows by setting argument to four.\n\nWe skipped four rows from the sheet and used none of the rows as the header. Also, notice that one can combine different options in a single read statement. To skip rows at the bottom of the sheet, you can use option , which works just like , the only difference being the rows are counted from the bottom upwards.\n\nThe column names in the previous DataFrame are numeric and were allotted as default by the pandas. We can rename the column names to descriptive ones by calling the method on the DataFrame and passing the column names as a list.\n\nNow that we have seen how to read a subset of rows from the Excel file, we can learn how to read a subset of columns.\n\nAlthough read_excel defaults to reading and importing all columns, you can choose to import only certain columns. By passing parse_cols=6, we are telling the method to read only the first columns till index six or first seven columns (the first column being indexed zero).\n\nAlternatively, you can pass in a list of numbers, which will let you import columns at particular indexes.\n\nOne of the much-used features of Excel is to apply formulas to create new columns from existing column values. In our Excel file, we have Gross Earnings and Budget columns. We can get Net earnings by subtracting Budget from Gross earnings. We could then apply this formula in the Excel file to all the rows. We can do this in pandas also as shown below.\n\nAbove, we used pandas to create a new column called Net Earnings, and populated it with the difference of Gross Earnings and Budget. It's worth noting the difference here in how formulas are treated in Excel versus pandas. In Excel, a formula lives in the cell and updates when the data changes - with Python, the calculations happen and the values are stored - if Gross Earnings for one movie was manually changed, Net Earnings won't be updated.\n\nLet's use the method to sort the data by the new column we created and visualize the top 10 movies by Net Earnings.\n\nAdvanced Excel users also often use pivot tables. A pivot table summarizes the data of another table by grouping the data on an index and applying operations such as sorting, summing, or averaging. You can use this feature in pandas too.\n\nWe need to first identify the column or columns that will serve as the index, and the column(s) on which the summarizing formula will be applied. Let's start small, by choosing Year as the index column and Gross Earnings as the summarization column and creating a separate DataFrame from this data.\n\nWe now call on this subset of data. The method takes a parameter . As mentioned, we want to use Year as the index.\n\nThis gave us a pivot table with grouping on Year and summarization on the sum of Gross Earnings. Notice, we didn't need to specify Gross Earnings column explicitly as pandas automatically identified it the values on which summarization should be applied.\n\nWe can use this pivot table to create some data visualizations. We can call the method on the DataFrame to create a line plot and call the method to display the plot in the notebook.\n\nWe saw how to pivot with a single column as the index. Things will get more interesting if we can use multiple columns. Let's create another DataFrame subset but this time we will choose the columns, Country, Language and Gross Earnings.\n\nWe will use columns Country and Language as the index for the pivot table. We will use Gross Earnings as summarization table, however, we do not need to specify this explicitly as we saw earlier.\n\nLet's visualize this pivot table with a bar plot. Since there are still few hundred records in this pivot table, we will plot just a few of them.\n\nIf you're going to be working with colleagues who use Excel, saving Excel files out of pandas is important. You can export or write a pandas DataFrame to an Excel file using pandas method. Pandas uses the Python module internally for writing to Excel files. The method is called on the DataFrame we want to export.We also need to pass a filename to which this DataFrame will be written.\n\nBy default, the index is also saved to the output file. However, sometimes the index doesn't provide any useful information. For example, the DataFrame has a numeric auto-increment index, that was not part of the original Excel data.\n\nYou can choose to skip the index by passing along index-False.\n\nWe need to be able to make our output files look nice before we can send it out to our co-workers. We can use pandas class along with the Python module to apply the formatting.\n\nWe can do use these advanced output options by creating a object and use this object to write to the EXcel file.\n\nWe can apply customizations by calling on the workbook we are writing to. Here we are setting header format as bold.\n\nFinally, we save the output file by calling the method on the writer object.\n\nAs an example, we saved the data with column headers set as bold. And the saved file looks like the image below.\n\nLike this, one can use to apply various formatting to the output Excel file.\n\nPandas is not a replacement for Excel. Both tools have their place in the data analysis workflow and can be very great companion tools. As we demonstrated, pandas can do a lot of complex data analysis and manipulations, which depending on your need and expertise, can go beyond what you can achieve if you are just using Excel. One of the major benefits of using Python and pandas over Excel is that it helps you automate Excel file processing by writing scripts and integrating with your automated data workflow. Pandas also has excellent methods for reading all kinds of data from Excel files. You can export your results from pandas back to Excel too if that's preferred by your intended audience.\n\nOn the other hand, Excel is a such a widely used data tool, it's not a wise to ignore it. Acquiring expertise in both pandas and Excel and making them work together gives you skills that can help you stand out in your organization.\n\nIf you’d like to learn more about this topic, check out Dataquest's interactive Pandas and NumPy Fundamentals course, and our Data Analyst in Python, and Data Scientist in Python paths that will help you become job-ready in around 6 months."
    },
    {
        "link": "https://pandas.pydata.org/docs/getting_started/intro_tutorials/02_read_write.html",
        "document": "Pclass: One out of the 3 ticket classes: Class 1 , Class 2 and Class 3 .\n\nSurvived: Indication whether passenger survived. 0 for yes and 1 for no.\n\nThis tutorial uses the Titanic data set, stored as CSV. The data consists of the following data columns:\n\nHow do I read and write tabular data?#\n• None I want to analyze the Titanic passenger data, available as a CSV file. pandas provides the function to read data stored as a csv file into a pandas . pandas supports many different file formats or data sources out of the box (csv, excel, sql, json, parquet, …), each of them with the prefix .\n\nMake sure to always have a check on the data after reading in the data. When displaying a , the first and last 5 rows will be shown by default:\n• None I want to see the first 8 rows of a pandas DataFrame. To see the first N rows of a , use the method with the required number of rows (in this case 8) as argument.\n\nA check on how pandas interpreted each of the column data types can be done by requesting the pandas attribute:\n\nFor each of the columns, the used data type is enlisted. The data types in this are integers ( ), floats ( ) and strings ( ).\n\nWhen asking for the , no brackets are used! is an attribute of a and . Attributes of a or do not need brackets. Attributes represent a characteristic of a / , whereas methods (which require brackets) do something with the / as introduced in the first tutorial.\n• None My colleague requested the Titanic data as a spreadsheet. Whereas functions are used to read data to pandas, the methods are used to store data. The method stores the data as an excel file. In the example here, the is named passengers instead of the default Sheet1. By setting the row index labels are not saved in the spreadsheet.\n\nThe equivalent read function will reload the data to a :\n• The method provides technical information about a , so let’s explain the output in more detail:\n• None It is indeed a .\n• None Each row has a row label (aka the ) with values ranging from 0 to 890.\n• None The table has 12 columns. Most columns have a value for each of the rows (all 891 values are ). Some columns do have missing values and less than 891 values.\n• None The columns , , and consists of textual data (strings, aka ). The other columns are numerical data with some of them whole numbers (aka ) and others are real numbers (aka ).\n• None The kind of data (characters, integers,…) in the different columns are summarized by listing the .\n• None The approximate amount of RAM used to hold the DataFrame is provided as well."
    },
    {
        "link": "https://proclusacademy.com/blog/practical/pandas-read-write-excel-files",
        "document": "We recently covered the basics of Pandas. Today, we’ll learn how to work with Excel spreadsheets using Pandas.\n\nHere’s what this article will cover:\n\nWe’ll use below Excel files in today’s tutorial:\n• largest_cities.xlsx: contains data on the 200 largest US cities.\n• ranked_cities.xlsx: has three worksheets with rankings of US cities.\n\nPlease download them from here and copy them to the same directory as your Jupyter Notebook (or Python script).\n\nPandas uses the OpenPyXL library to interact with Excel files. You can install it using the below pip command:\n\nNow you can launch your notebook and import Pandas as usual:\n\nWe’ll load the Excel file largest_cities.xlsx in this section. Here’s a screenshot of this file:\n• The first row contains the column headers (City, State, 2023 Population, etc.).\n• The first column, City, has the names of the largest US cities. We can use it as an index to identify rows uniquely.\n• Excel also assigns alphabetical labels (A, B, C, etc.) to each column ( ). These labels will come in handy, as we’ll see later.\n\nLet’s use Pandas’ method read_excel() to load this file. The method will return a Pandas DataFrame:\n\nPandas used the first row for column labels (in blue). However, it automatically created an index (in red). Let’s print the index to confirm:\n\nIt is a numerical index ranging from 0 to 199. We don’t want that. Instead, we should use the City column (in green) as the index.\n\n .\n\nWe can set a column (or a list of columns) as the index using the DataFrame method set_index():\n\nThe DataFrame looks better. The list of cities is set as the index. Let’s print the index again to confirm:\n\nYou can now use Pandas loc[] to access specific cities using the index labels:\n\nRecall that Excel assigns letter labels (A, B, C, etc.) to each column. The read_excel() method allows you to use these labels to load a specific set of columns.\n\nThe parameter takes in a list of comma-separated letter labels to load specific columns.\n\nSuppose you only want to load four specific columns - City, State, 2020 Census, and Density mi2. You can do that by passing their corresponding letter labels to the parameter :\n\nSuppose you want to load a range of columns (e.g., from A to D). We can use to specify the start and end columns using a colon (:) like below.\n\nWe can also read specific columns and a range of columns together. The below code loads a range (C:E) and two individual columns using :\n\nSo far, we’ve worked with a perfect Excel file. However, it’s common to get messy excel files with empty rows and columns.\n\nLet’s see an example - largest_cities_blank_rc.xlsx has the same data we saw in the last section, but the first column (A) and the top four rows are blank:\n\nIf you load this file using read_excel(), you’ll see a DataFrame with lots of missing values (NaN / Unnamed):\n\nYou can skip these empty rows and columns using the below parameters:\n• usecols: We already covered this parameter in the last section. Since columns B to H contain the data, we’ll set it to ‘B:H’.\n• skiprows: The number of lines to skip at the start of the file. We know the file begins with four blank rows, so let’s set it to 4.\n\nThe DataFrame looks good now. It doesn’t have any empty rows or columns, and it uses City as the index.\n\nExcel files can have multiple worksheets, each containing a different data set.\n\nPandas works great with such files. It helps you to:\n• Get the names of all the sheets in a file.\n\nLet’s see these operations in action. We’ll use the excel file ranked_cities.xlsx, which contains three sheets:\n\nHow can you get the names of all the sheets in an Excel file?\n\nEasy! Load the file using the ExcelFile class. Then get the value of the attribute:\n\nBy default, the read_excel() method loads the first sheet from the Excel file:\n\nYou can load a specific sheet by using the parameter :\n\nThe parameter is quite versatile. It also accepts a list of sheet names (or their integer positions).\n\nFor example, if you want to load the first and third sheets, you can set to , or . You’ll get back a dictionary containing data from both sheets.\n\nLet’s print the dictionary keys. It’ll be the same sheet names (or integer positions) that you passed in the above read_excel() call.\n\nThe dictionary values will be the DataFrame objects for each sheet. Let’s retrieve them:\n\nAnd print the top 5 entries from both DataFrames:\n\nLos Angeles in the fifth position? That’s shocking 😲. It’s impossible to imagine a city with worse traffic than LA!\n\nYou can read all the sheets at once by setting to :\n\nThe returned dictionary will now have all three sheets:\n\nLet’s print the DataFrame containing the fastest growing cities:\n\nPandas makes it easy to save DataFrames as Excel files. Let’s see it in action.\n\nSuppose you’ve compiled a list of your favorite restaurants in Seattle. And you prepare a DataFrame with their cuisine type and customer ratings:\n\nYou can write this DataFrame to an Excel file using the method to_excel().\n\nIt’ll persist the index as well. We don’t want to save the numeric index. So we’ll set the parameter to :\n\nYou should now see the file seattle_restaurants.xlsx in the same directory where you’re running this notebook.\n\nSuppose you also compile a list of restaurants in New York City:\n\nNow you have two DataFrames with lists of your favorite restaurants in two cities - and .\n\nYou would like to create an Excel file with two sheets - it should have one sheet for each DataFrame. You can do that by using Pandas’ class ExcelWriter and DataFrame method to_excel():\n\nYou’ll now see a new Excel file, cities_restaurants.xlsx, in your current directory. And it’ll have two sheets - Seattle and New York City.\n\nLet’s say you gather a list of restaurants in New Orleans and create a DataFrame for them:\n\nAnd you want to save this DataFrame as a new sheet in the file we created in the last section - cities_restaurants.xlsx.\n\nTo do that, you’ll need to use the class ExcelWriter in append mode (parameter ):\n\nLet’s confirm that the Excel file has the new sheet by printing the sheet names:\n\nAs expected, the file now contains the new sheet - New Orleans.\n\nThis article showed you how to interact with Excel files using Python and Pandas. Here’s a quick recap of what we learned today:\n• Read Excel files using read_excel().\n• Skip blank columns and rows with the parameters and .\n• Create an Excel file with multiple sheets with the class ExcelWriter."
    },
    {
        "link": "https://pandas.pydata.org/docs/user_guide/indexing.html",
        "document": "The axis labeling information in pandas objects serves many purposes:\n• None Identifies data (i.e. provides metadata) using known indicators, important for analysis, visualization, and interactive console display.\n• None Allows intuitive getting and setting of subsets of the data set.\n\nIn this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.\n\nSee the MultiIndex / Advanced Indexing for and more advanced indexing documentation.\n\nSee the cookbook for some advanced strategies.\n\nWhether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called and should be avoided. See Returning a View versus Copy. is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a . These will raise a . Traceback (most recent call last) in in in in # GH#33146 if start and end are combinations of str and None and Index is not # monotonic, we can not use Index.slice_indexer because it does not honor the # actual elements, is only searching for start and end in Compute the slice indexer for input labels and step. in in # For datetime indices label may be a string that has to be converted # to datetime boundary according to its resolution. # we need to look up the label in # Pandas supports slicing with dates, treated as datetimes at midnight. in in : cannot do slice indexing on DatetimeIndex with these indexers [2] of type int String likes in slicing can be convertible to the type of the index and lead to natural slicing. pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position. The attribute is the primary access method. The following are valid inputs:\n• None A single label, e.g. or (Note that is interpreted as a label of the index. This use is not an integer position along the index.).\n• None A slice object with labels (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels. Note that setting works as well: For getting a cross section using a label (equivalent to ): For getting values with a boolean array: For getting a value explicitly: # this is also equivalent to ``df1.at['a','A']`` When using with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned: If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two: However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, would raise . For the rationale behind this behavior, see Endpoints are inclusive. Also, if the index has duplicate labels and either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, would raise a . For more information about duplicate labels, see Duplicate Labels.\n\nA random selection of rows or columns from a Series or DataFrame with the method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows. # When no arguments are passed, returns 1 row. # One may specify either a number of rows: # Or a fraction of the rows: By default, will return each row at most once, but one can also sample with replacement using the option: By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the function sampling weights as . These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example: When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string. also allows users to sample columns instead of rows using the argument. Finally, one can also set a seed for ’s random number generator using the argument, which will accept either an integer (as a seed) or a NumPy RandomState object. # With a given seed, the sample will always draw the same rows.\n\nSelecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the method in and . To return only the selected rows: To return a Series of the same shape as the original: Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. is used under the hood as the implementation. The code below is equivalent to . In addition, takes an optional argument for replacement of values where the condition is False, in the returned copy. You may wish to set values based on some boolean criteria. This can be done intuitively like so: The signature for differs from . Roughly is equivalent to . Furthermore, aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via (but on the contents rather than the axis labels). Where can also accept and parameters to align the input when performing the . This is equivalent to (but faster than) the following. can accept a callable as condition and arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and argument. is the inverse boolean operation of .\n\nobjects have a method that allows selection using an expression. You can get the value of the frame where column has values between the values of columns and . For example: Do the same thing but fall back on a named index if there is no column with the name . If instead you don’t want to or cannot name your index, you can use the name in your query expression: If the name of your index overlaps with a column name, the column name is given precedence. For example, # uses the column 'a', not the index You can still use the index in a query expression by using the special identifier ‘index’: If for some reason you have a column named , then you can refer to the index as as well, but at this point you should consider renaming your columns to something less ambiguous. You can also use the levels of a with a as if they were columns in the frame: If the levels of the are unnamed, you can refer to them using special names: The convention is , which means “index level 0” for the 0th level of the . A use case for is when you have a collection of objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you’re interested in querying Slightly nicer by removing the parentheses (comparison operators bind tighter than and ): Use English instead of symbols: Pretty close to how you might write it on paper: also supports special use of Python’s and comparison operators, providing a succinct syntax for calling the method of a or . # get all rows where columns \"a\" and \"b\" have overlapping values # How you'd do it in pure Python You can combine this with other expressions for very succinct queries: # rows where cols a and b have overlapping values # and col c's values are less than col d's Note that and are evaluated in Python, since has no equivalent of this operation. However, only the / expression itself is evaluated in vanilla Python. For example, in the expression is evaluated by and then the operation is evaluated in plain Python. In general, any operations that can be evaluated using will be. Special use of the operator with objects# Comparing a of values to a column using / works similarly to / . You can negate boolean expressions with the word or the operator. Of course, expressions can be arbitrarily complex too: 'a < b < c and (not bools) or bools > 2' using is slightly faster than Python for large frames. You will only see the performance benefits of using the engine with if your frame has more than approximately 100,000 rows. This plot was created using a with 3 columns each containing floating point values generated using .\n\nThe pandas class and its subclasses can be viewed as implementing an ordered multiset. Duplicates are allowed. also provides the infrastructure necessary for lookups, data alignment, and reindexing. The easiest way to create an directly is to pass a or other sequence to : If no dtype is given, tries to infer the dtype from the data. It is also possible to give an explicit dtype when instantiating an : You can also pass a to be stored in the index: The name, if set, will be shown in the console display: Indexes are “mostly immutable”, but it is possible to set and change their attribute. You can use the , to set these attributes directly, and they default to returning a copy. See Advanced Indexing for usage of MultiIndexes. , , and also take an optional argument The two main operations are and . Difference is provided via the method. Also available is the operation, which returns elements that appear in either or , but not in both. This is equivalent to the Index created by , with duplicates dropped. The resulting index from a set operation will be sorted in ascending order. When performing between indexes with different dtypes, the indexes must be cast to a common dtype. Typically, though not always, this is object dtype. The exception is when performing a union between integer and float data. In this case, the integer values are converted to float Even though can hold missing values ( ), it should be avoided if you do not want any unexpected results. For example, some operations exclude missing values implicitly. fills missing values with specified scalar value.\n\nCopy-on-Write will become the new default in pandas 3.0. This means that chained indexing will never work. As a consequence, the won’t be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with even before pandas 3.0 is available. When setting values in a pandas object, care must be taken to avoid what is called . Here is an example. first second first second Name: (one, second), dtype: object These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 ( ) is much preferred over method 1 (chained ). selects the first level of the columns and returns a DataFrame that is singly-indexed. Then another Python operation selects the series indexed by . This is indicated by the variable because pandas sees these operations as separate events. e.g. separate calls to , so it has to treat them as linear operations, they happen one after another. Contrast this to which passes a nested tuple of to a single call to . This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired. Why does assignment fail when using chained indexing?# Copy-on-Write will become the new default in pandas 3.0. This means than chained indexing will never work. As a consequence, the won’t be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with even before pandas 3.0 is available. The problem in the previous section is just a performance issue. What’s up with the warning? We don’t usually throw warnings around when you do something that might cost a few extra milliseconds! But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code: But this code is handled differently: See that in there? Outside of simple cases, it’s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the will modify or a temporary object that gets thrown out immediately afterward. That’s what is warning you about! You may be wondering whether we should be concerned about the property in the first example. But is guaranteed to be itself with modified indexing behavior, so / operate on directly. Of course, may be a view or a copy of . Sometimes a warning will arise at times when there’s no obvious chained indexing going on. These are the bugs that is designed to catch! pandas is probably trying to warn you that you’ve done this: # Is foo a view? A copy? Nobody knows! # We don't know whether this will modify df or not! Copy-on-Write will become the new default in pandas 3.0. This means than chained indexing will never work. As a consequence, the won’t be necessary anymore. See this section for more context. We recommend turning Copy-on-Write on to leverage the improvements with even before pandas 3.0 is available. When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice. pandas has the because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected. If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option to one of these values:\n• None means pandas will raise a you have to deal with.\n• None will suppress the warnings entirely. # This will show the SettingWithCopyWarning # but the frame values will be set This however is operating on a copy and will not work. A chained assignment can also crop up in setting in a mixed dtype frame. These setting rules apply to all of . The following is the recommended access method using for multiple items (using ) and a single item using a fixed index: The following can work at times, but it is not guaranteed to, and therefore should be avoided: Last, the subsequent example will not work at all, and so should be avoided: Traceback (most recent call last) in in in in in : A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid assignment. There may be false positives; situations where a chained assignment is inadvertently reported."
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.5/user_guide/indexing.html",
        "document": "The axis labeling information in pandas objects serves many purposes:\n• None Identifies data (i.e. provides metadata) using known indicators, important for analysis, visualization, and interactive console display.\n• None Allows intuitive getting and setting of subsets of the data set.\n\nIn this section, we will focus on the final point: namely, how to slice, dice, and generally get and set subsets of pandas objects. The primary focus will be on Series and DataFrame as they have received more development attention in this area.\n\nSee the MultiIndex / Advanced Indexing for and more advanced indexing documentation.\n\nSee the cookbook for some advanced strategies.\n\nWhether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called and should be avoided. See Returning a View versus Copy. is strict when you present slicers that are not compatible (or convertible) with the index type. For example using integers in a . These will raise a . TypeError: cannot do slice indexing on <class 'pandas.tseries.index.DatetimeIndex'> with these indexers [2] of <type 'int'> String likes in slicing can be convertible to the type of the index and lead to natural slicing. pandas will raise a if indexing with a list with missing labels. See list-like Using loc with missing keys in a list is Deprecated. pandas provides a suite of methods in order to have purely label based indexing. This is a strict inclusion based protocol. Every label asked for must be in the index, or a will be raised. When slicing, both the start bound AND the stop bound are included, if present in the index. Integers are valid labels, but they refer to the label and not the position. The attribute is the primary access method. The following are valid inputs:\n• None A single label, e.g. or (Note that is interpreted as a label of the index. This use is not an integer position along the index.).\n• None A slice object with labels (Note that contrary to usual Python slices, both the start and the stop are included, when present in the index! See Slicing with labels. Note that setting works as well: For getting a cross section using a label (equivalent to ): For getting values with a boolean array: For getting a value explicitly: # this is also equivalent to ``df1.at['a','A']`` When using with slices, if both the start and the stop labels are present in the index, then elements located between the two (including them) are returned: If at least one of the two is absent, but the index is sorted, and can be compared against start and stop labels, then slicing will still work as expected, by selecting labels which rank between the two: However, if at least one of the two is absent and the index is not sorted, an error will be raised (since doing otherwise would be computationally expensive, as well as potentially ambiguous for mixed type indexes). For instance, in the above example, would raise . For the rationale behind this behavior, see Endpoints are inclusive. Also, if the index has duplicate labels and either the start or the stop label is duplicated, an error will be raised. For instance, in the above example, would raise a . For more information about duplicate labels, see Duplicate Labels.\n\nA random selection of rows or columns from a Series or DataFrame with the method. The method will sample rows by default, and accepts a specific number of rows/columns to return, or a fraction of rows. # When no arguments are passed, returns 1 row. # One may specify either a number of rows: # Or a fraction of the rows: By default, will return each row at most once, but one can also sample with replacement using the option: By default, each row has an equal probability of being selected, but if you want rows to have different probabilities, you can pass the function sampling weights as . These weights can be a list, a NumPy array, or a Series, but they must be of the same length as the object you are sampling. Missing values will be treated as a weight of zero, and inf values are not allowed. If weights do not sum to 1, they will be re-normalized by dividing all weights by the sum of the weights. For example: When applied to a DataFrame, you can use a column of the DataFrame as sampling weights (provided you are sampling rows and not columns) by simply passing the name of the column as a string. also allows users to sample columns instead of rows using the argument. Finally, one can also set a seed for ’s random number generator using the argument, which will accept either an integer (as a seed) or a NumPy RandomState object. # With a given seed, the sample will always draw the same rows.\n\nConsider the method of , which returns a boolean vector that is true wherever the elements exist in the passed list. This allows you to select rows where one or more columns have values you want: The same method is available for objects and is useful for the cases when you don’t know which of the sought labels are in fact present: # compare it to the following In addition to that, allows selecting a separate level to use in the membership check: DataFrame also has an method. When calling , pass a set of values as either an array or dict. If values is an array, returns a DataFrame of booleans that is the same shape as the original DataFrame, with True wherever the element is in the sequence of values. Oftentimes you’ll want to match certain values with certain columns. Just make values a where the key is the column, and the value is a list of items you want to check for. To return the DataFrame of booleans where the values are not in the original DataFrame, use the operator: Combine DataFrame’s with the and methods to quickly select subsets of your data that meet a given criteria. To select a row where each column meets its own criterion:\n\nSelecting values from a Series with a boolean vector generally returns a subset of the data. To guarantee that selection output has the same shape as the original data, you can use the method in and . To return only the selected rows: To return a Series of the same shape as the original: Selecting values from a DataFrame with a boolean criterion now also preserves input data shape. is used under the hood as the implementation. The code below is equivalent to . In addition, takes an optional argument for replacement of values where the condition is False, in the returned copy. You may wish to set values based on some boolean criteria. This can be done intuitively like so: By default, returns a modified copy of the data. There is an optional parameter so that the original data can be modified without creating a copy: The signature for differs from . Roughly is equivalent to . Furthermore, aligns the input boolean condition (ndarray or DataFrame), such that partial selection with setting is possible. This is analogous to partial setting via (but on the contents rather than the axis labels). Where can also accept and parameters to align the input when performing the . This is equivalent to (but faster than) the following. can accept a callable as condition and arguments. The function must be with one argument (the calling Series or DataFrame) and that returns valid output as condition and argument. is the inverse boolean operation of .\n\nobjects have a method that allows selection using an expression. You can get the value of the frame where column has values between the values of columns and . For example: Do the same thing but fall back on a named index if there is no column with the name . If instead you don’t want to or cannot name your index, you can use the name in your query expression: If the name of your index overlaps with a column name, the column name is given precedence. For example, # uses the column 'a', not the index You can still use the index in a query expression by using the special identifier ‘index’: If for some reason you have a column named , then you can refer to the index as as well, but at this point you should consider renaming your columns to something less ambiguous. You can also use the levels of a with a as if they were columns in the frame: If the levels of the are unnamed, you can refer to them using special names: The convention is , which means “index level 0” for the 0th level of the . A use case for is when you have a collection of objects that have a subset of column names (or index levels/names) in common. You can pass the same query to both frames without having to specify which frame you’re interested in querying Slightly nicer by removing the parentheses (comparison operators bind tighter than and ): Use English instead of symbols: Pretty close to how you might write it on paper: also supports special use of Python’s and comparison operators, providing a succinct syntax for calling the method of a or . # get all rows where columns \"a\" and \"b\" have overlapping values # How you'd do it in pure Python You can combine this with other expressions for very succinct queries: # rows where cols a and b have overlapping values # and col c's values are less than col d's Note that and are evaluated in Python, since has no equivalent of this operation. However, only the / expression itself is evaluated in vanilla Python. For example, in the expression is evaluated by and then the operation is evaluated in plain Python. In general, any operations that can be evaluated using will be. Special use of the operator with objects# Comparing a of values to a column using / works similarly to / . You can negate boolean expressions with the word or the operator. Of course, expressions can be arbitrarily complex too: 'a < b < c and (not bools) or bools > 2' using is slightly faster than Python for large frames. You will only see the performance benefits of using the engine with if your frame has more than approximately 200,000 rows. This plot was created using a with 3 columns each containing floating point values generated using .\n\nWhen setting values in a pandas object, care must be taken to avoid what is called . Here is an example. first second first second Name: (one, second), dtype: object These both yield the same results, so which should you use? It is instructive to understand the order of operations on these and why method 2 ( ) is much preferred over method 1 (chained ). selects the first level of the columns and returns a DataFrame that is singly-indexed. Then another Python operation selects the series indexed by . This is indicated by the variable because pandas sees these operations as separate events. e.g. separate calls to , so it has to treat them as linear operations, they happen one after another. Contrast this to which passes a nested tuple of to a single call to . This allows pandas to deal with this as a single entity. Furthermore this order of operations can be significantly faster, and allows one to index both axes if so desired. Why does assignment fail when using chained indexing?# The problem in the previous section is just a performance issue. What’s up with the warning? We don’t usually throw warnings around when you do something that might cost a few extra milliseconds! But it turns out that assigning to the product of chained indexing has inherently unpredictable results. To see this, think about how the Python interpreter executes this code: But this code is handled differently: See that in there? Outside of simple cases, it’s very hard to predict whether it will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the will modify or a temporary object that gets thrown out immediately afterward. That’s what is warning you about! You may be wondering whether we should be concerned about the property in the first example. But is guaranteed to be itself with modified indexing behavior, so / operate on directly. Of course, may be a view or a copy of . Sometimes a warning will arise at times when there’s no obvious chained indexing going on. These are the bugs that is designed to catch! pandas is probably trying to warn you that you’ve done this: # Is foo a view? A copy? Nobody knows! # We don't know whether this will modify df or not! When you use chained indexing, the order and type of the indexing operation partially determine whether the result is a slice into the original object, or a copy of the slice. pandas has the because assigning to a copy of a slice is frequently not intentional, but a mistake caused by chained indexing returning a copy where a slice was expected. If you would like pandas to be more or less trusting about assignment to a chained indexing expression, you can set the option to one of these values:\n• None means pandas will raise a you have to deal with.\n• None will suppress the warnings entirely. # This will show the SettingWithCopyWarning # but the frame values will be set This however is operating on a copy and will not work. A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_index,col_indexer] = value instead A chained assignment can also crop up in setting in a mixed dtype frame. These setting rules apply to all of . The following is the recommended access method using for multiple items (using ) and a single item using a fixed index: The following can work at times, but it is not guaranteed to, and therefore should be avoided: Last, the subsequent example will not work at all, and so should be avoided: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_index,col_indexer] = value instead The chained assignment warnings / exceptions are aiming to inform the user of a possibly invalid assignment. There may be false positives; situations where a chained assignment is inadvertently reported."
    },
    {
        "link": "https://stackoverflow.com/questions/61589780/how-do-i-index-an-pandas-dataframe-using-boolean-indexing",
        "document": "I am starting a new practice module in pandas where we deal with indexing and filtering of data. I have come across a format of method chaining that was not explained in the course and I was wondering if anyone could help me make sense of this. The dataset is from the fortune 500 company listings.\n\nThe issue is that we have been taught to use boolean indexing by passing the bool condition to the dataframe like so;\n\nThe above code was to find the countries that have \"Motor Vehicles and Parts\" as their industries. The last exercise in the module asks us to\n\n\" Create a series, industry_usa, containing counts of the two most common values in the industry column for companies headquartered in the USA.\"\n\nAnd the answer code is\n\nI don't understand how we can suddenly use df[col]df[col] back to back? Am I not supposed pass the bool condition first then specify which column i want to assign it to using .loc? The method chaining the used is very different to what we have practiced.\n\nPlease help. I am truly confused.\n\nAs always, thanks you, stack community."
    },
    {
        "link": "https://pandas.pydata.org/pandas-docs/version/1.5.0/reference/frame.html",
        "document": "Access a single value for a row/column pair by integer position. Access a group of rows and columns by label(s) or a boolean array. Insert column into DataFrame at specified location. Get the 'info axis' (see Indexing for more). Get item from object for given key (ex: DataFrame column). Whether each element in the DataFrame is contained in values. Replace values where the condition is False. Replace values where the condition is True. Query the columns of a DataFrame with a boolean expression. For more information on , , , and , see the indexing documentation.\n\nGet Addition of dataframe and other, element-wise (binary operator ). Get Subtraction of dataframe and other, element-wise (binary operator ). Get Multiplication of dataframe and other, element-wise (binary operator ). Get Floating division of dataframe and other, element-wise (binary operator ). Get Floating division of dataframe and other, element-wise (binary operator ). Get Integer division of dataframe and other, element-wise (binary operator ). Get Modulo of dataframe and other, element-wise (binary operator ). Get Exponential power of dataframe and other, element-wise (binary operator ). Compute the matrix multiplication between the DataFrame and other. Get Addition of dataframe and other, element-wise (binary operator ). Get Subtraction of dataframe and other, element-wise (binary operator ). Get Multiplication of dataframe and other, element-wise (binary operator ). Get Floating division of dataframe and other, element-wise (binary operator ). Get Floating division of dataframe and other, element-wise (binary operator ). Get Integer division of dataframe and other, element-wise (binary operator ). Get Modulo of dataframe and other, element-wise (binary operator ). Get Exponential power of dataframe and other, element-wise (binary operator ). Get Less than of dataframe and other, element-wise (binary operator ). Get Greater than of dataframe and other, element-wise (binary operator ). Get Less than or equal to of dataframe and other, element-wise (binary operator ). Get Greater than or equal to of dataframe and other, element-wise (binary operator ). Get Not equal to of dataframe and other, element-wise (binary operator ). Get Equal to of dataframe and other, element-wise (binary operator ). Update null elements with value in the same location in .\n\nReturn a Series/DataFrame with absolute numeric value of each element. Return whether all elements are True, potentially over an axis. Return whether any element is True, potentially over an axis. Count non-NA cells for each column or row. (DEPRECATED) Return the mean absolute deviation of the values over the requested axis. Return the maximum of the values over the requested axis. Return the mean of the values over the requested axis. Return the median of the values over the requested axis. Return the minimum of the values over the requested axis. Get the mode(s) of each element along the selected axis. Percentage change between the current and a prior element. Return the product of the values over the requested axis. Return the product of the values over the requested axis. Return values at the given quantile over requested axis. Return unbiased standard error of the mean over requested axis. Return the sum of the values over the requested axis. Count number of distinct elements in specified axis. Return a Series containing counts of unique rows in the DataFrame.\n\nAlign two objects on their axes with the specified join method. Select values at particular time of day (e.g., 9:30AM). Select values between particular times of the day (e.g., 9:00-9:30 AM). Drop specified labels from rows or columns. Test whether two objects contain the same elements. Subset the dataframe rows or columns according to the specified index labels. Return index of first occurrence of maximum over requested axis. Return index of first occurrence of minimum over requested axis. Conform Series/DataFrame to new index with optional filling logic. Return an object with matching indices as other object. Set the name of the axis for the index or columns. Reset the index, or a level of it. Return a random sample of items from an axis of object. Return the elements in the given positional indices along an axis. Truncate a Series or DataFrame before and after some index value.\n\nSort by the values along either axis. Sort object by labels (along an axis). Return the first rows ordered by in descending order. Return the first rows ordered by in ascending order. Stack the prescribed level(s) from columns to index. Transform each element of a list-like to a row, replicating index values. Return an xarray object from the pandas object.\n\nReturn the last row(s) without any NaNs before . Shift index by desired number of periods with an optional time . (DEPRECATED) Shift the time index, using the index's frequency if available. Return index for first non-NA value or None, if no non-NA value is found. Return index for last non-NA value or None, if no non-NA value is found. Cast to DatetimeIndex of timestamps, at beginning of period. Localize tz-naive index of a Series or DataFrame to target time zone.\n\nFlags refer to attributes of the pandas object. Properties of the dataset (like the date is was recorded, the URL it was accessed from, etc.) should be stored in ."
    },
    {
        "link": "https://stackoverflow.com/questions/63848502/boolean-indexing-in-pandas-combining-a-variable-number-of-columns",
        "document": "Is there a (pythonic) way to loop over columns in dataframe for boolean-indexing in pandas ?\n\nI'm not sure you can do this using list comprehension...\n\nThe only way I've found would be :\n\nThis seems to do the job... But I'm not sure this is memory efficient and (IMHO) this is neither straightforward nor easy to understand.\n\nIs there a way in pandas to make some kind of serialisation of the \"|\" or \"&\" operators ?"
    }
]