[
    {
        "link": "https://joblib.readthedocs.io/en/latest/generated/joblib.load.html",
        "document": "Read more in the User Guide.\n\nWARNING: joblib.load relies on the pickle module and can therefore execute arbitrary Python code. It should therefore never be used to load files from untrusted sources.\n\nThe file object or path of the file from which to load the object If not None, the arrays are memory-mapped from the disk. This mode has no effect for compressed files. Note that in this case the reconstructed object might no longer match exactly the originally pickled object. The object stored in the file.\n\nThis function can load numpy array files saved separately during the dump. If the mmap_mode argument is given, it is passed to np.load and arrays are loaded as memmaps. As a consequence, the reconstructed object might not match the original pickled object. Note that if the file was saved with compression, the arrays cannot be memmapped."
    },
    {
        "link": "https://joblib.readthedocs.io",
        "document": "Joblib is a set of tools to provide lightweight pipelining in Python. In particular:\n\nJoblib is optimized to be fast and robust on large data in particular and has specific optimizations for arrays. It is BSD-licensed.\n\nThe vision is to provide tools to easily achieve better performance and reproducibility when working with long running jobs.\n• None Avoid computing the same thing twice: code is often rerun again and again, for instance when prototyping computational-heavy jobs (as in scientific development), but hand-crafted solutions to alleviate this issue are error-prone and often lead to unreproducible results.\n• None Persist to disk transparently: efficiently persisting arbitrary objects containing large data is hard. Using joblib’s caching mechanism avoids hand-written persistence and implicitly links the file on disk to the execution context of the original Python object. As a result, joblib’s persistence is good for resuming an application status or computational job, eg after a crash. Joblib addresses these problems while leaving your code and your flow control as unmodified as possible (no framework, no new paradigms).\n• None Transparent and fast disk-caching of output value: a memoize or make-like functionality for Python functions that works well for arbitrary Python objects, including very large numpy arrays. Separate persistence and flow-execution logic from domain logic or algorithmic code by writing the operations as a set of steps with well-defined inputs and outputs: Python functions. Joblib can save their computation to disk and rerun it only if necessary: # The above call did not trigger an evaluation\n• None Embarrassingly parallel helper: to make it easy to write readable parallel code and debug it quickly:\n• None Fast compressed Persistence: a replacement for pickle to work efficiently on Python objects containing large data ( joblib.dump & joblib.load )."
    },
    {
        "link": "https://pypi.org/project/joblib",
        "document": "A required part of this site couldn’t load. This may be due to a browser extension, network issues, or browser settings. Please check your connection, disable any ad blockers, or try using a different browser."
    },
    {
        "link": "https://analyticsvidhya.com/blog/2023/02/how-to-save-and-load-machine-learning-models-in-python-using-joblib-library",
        "document": "How to Save and Load Machine Learning Models in Python Using Joblib Library?\n\nMachine Learning models require large datasets to get high accuracy, so in order to train a machine learning model with a large-size dataset, we also need a reasonable amount of time. So we use the joblib library to get rid of training the model again and again, instead, what we do is just train the model once and then save it using the joblib library, and then we use the same model.\n\nThis post will look at using Python’s joblib package to save and load machine learning models. For this project, Google Colab is used.\n\nJoblib is a Python library for running computationally intensive tasks in parallel. It provides a set of functions for performing operations in parallel on large data sets and for caching the results of computationally expensive functions. Joblib is especially useful for machine learning models because it allows you to save the state of your computation and resume your work later or on a different machine.\n• Understanding the importance of the Joblib library and why saving our machine learning models is useful.\n• How to use the joblib library for saving and loading our trained machine learning model?\n• Understanding the different functions that save and load models, including functions like “save” and “load.”\n\nThis article was published as a part of the Data Science Blogathon.\n\nWhy Should you Use Joblib?\n\nCompared to other techniques of storing and loading machine learning models, using Joblib has a number of benefits. Since data is stored as byte strings rather than objects, it may be stored quickly and easily in a smaller amount of space than traditional pickling. Moreover, it automatically corrects errors when reading or writing files, making it more dependable than manual pickling. Last but not least, using joblib enables you to save numerous iterations of the same model, making it simpler to contrast them and identify the most accurate one.\n\nJoblib enables multiprocessing across several machines or cores on a single machine, which enables programmers to parallelize jobs across numerous machines. This makes it simple for programmers to utilize distributed computing resources like clusters or GPUs to accelerate their model training process.\n\nImport joblib using the following code:\n\nIf the above code gives an error, you don’t have joblib installed in your environment.\n\ninstall joblib using the following code:\n\nWe will make a logistic regression model for this purpose and use the iris dataset present in sklearn. datasets.\n\nThe Iris dataset is a well-known dataset in the field of machine learning and statistics. It contains 150 observations of iris flowers and the measurements of their sepals and petals. The dataset includes 50 observations for each of three species of iris flowers (Iris setosa, Iris virginica, and Iris versicolor). The measurements included in the dataset are sepal length, sepal width, petal length, and petal width. The Iris dataset is commonly used as a benchmark for classification algorithms as it is small, well-understood, and multi-class.\n\nLogistic Regression is a type of statistical method used for binary classification problems. It is used to model the relationship between a dependent variable and one or more independent variables. Logistic regression aims to estimate the probability of an event occurring based on the values of the independent variables. The output of logistic regression is a probability between 0 and 1, which can then be thresholded to make a binary decision about the class of the event. Logistic regression is widely used in various fields, including medicine, marketing, and finance, due to its simplicity, interpretability, and ability to handle various data types and distributions. Despite its simplicity, logistic regression is a powerful tool for solving many binary classification problems and is often a good starting point for more complex machine learning models.\n\nSaving our trained machine learning model using the dump function of the joblib library.\n\nBelow given image show the current working directory before saving the model using the joblib library.\n\nBelow is the screenshot after saving the model using the joblib dump method.\n\nYou can clearly notice that after running the code: joblib.dump(reg, ‘regression_model.joblib’), a new file has been saved in the current directory as ‘regression_model.joblib’.\n\nLoading the regression_model.joblib for using it for making predictions.\n\nMaking predictions for the test dataset using our trained ML model.\n\nJoblib library is very useful when we want to use machine learning models in applications and websites.\n\nJoblib can be useful in development in several ways:\n\n1. Debugging and Profiling: It might be challenging to identify which sections of code are taking the longest to execute when creating a large application with several functions. Joblib offers simple tools for profiling the performance of your code, allowing you to locate and speed up the areas of your application that are the slowest.\n\n2. Reproducibility: Doing the same calculations several times might be time-consuming when working with huge datasets. In order to reuse the results of time-consuming computations without having to run the code again, Joblib offers a means to cache the results. By doing this, you can save time and guarantee the reproducibility of your results.\n\n3. Testing: Writing tests is crucial when creating a complex program since they ensure that the code performs as intended. Joblib offers a means to run tests concurrently so you can learn more quickly about the state of your code. This can speed up your development process and enable you to write and execute more tests in less time.\n\n4. Experimentation: Running several iterations of the code simultaneously can be useful when creating a new algorithm or testing out various strategies. Joblib offers a straightforward method for running various iterations of your code concurrently so you can rapidly compare their outcomes and determine which strategy works best.\n\nIn conclusion, Joblib can be helpful in development by offering instruments for debugging and profiling, ensuring repeatability, accelerating the testing procedure, and enabling experimentation. With the aid of these features, you can create more substantial and intricate apps with greater productivity and efficiency.\n\nThe key takeaways of this article are as follows:\n• It offers Python-based machine learning frameworks like scikit-learn and TensorFlow developers an effective approach to instantly save and load their learned models without having to redo the time-consuming and expensive process of training from scratch each time they require them.\n• It also allows developers to take advantage of parallelization techniques such as multiprocessing across multiple machines or cores on a single machine making higher performance levels achievable at lower costs.\n• So if you’re looking for an easy way to optimize your model creation and storage processes in Python, look no further than JobLib!\n\nThe media shown in this article is not owned by Analytics Vidhya and is used at the Author’s discretion."
    },
    {
        "link": "https://github.com/joblib/joblib-hadoop",
        "document": "This repository has been unmaintained for several years. Making the tests pass again requires significant work to update the configuration to use more versions of the base docker images and dependencies.\n\nFurthermore, it depends on https://github.com/dask/hdfs3 which is also unmaintained and would therefore require significant code rewrite to switch to pyarrow.\n\nSince there was very little adoption of this project. It was decided to mark it as officially archived on 2025-02-26.\n\nThis package provides parallel and store backends for joblib that can be use on a Hadoop cluster.\n\nIf you don't know joblib already, user documentation is located on https://pythonhosted.org/joblib\n\nTo get the latest code use git:\n\nWe recommend using Python Anaconda 3 distribution for full support of the HDFS store backends.\n• Create an Anaconda environment (use python 2.7, 3.4 or 3.5) and activate it:\n\nWe recommend using anaconda because it provides a pre-built version of libhdfs3. See build_libhdfs3 if you want to install it using pip.\n• From the joblibhadoop-env environment, perform installation using pip:\n\n1. Use a HDFS storage backend with Joblib memory to cache results (replace 'namenode' with the name of the HDFS namenode):\n• Use a YARN backend with Joblib parallel to parallelize computations:\n\nThe YARN parallel backend example only works on a host where Hadoop is installed and correctly configured.\n\nAll examples are available in the examples directory.\n\nIn order to run the test suite, you need to setup a local hadoop cluster inside Docker containers. This can be achieved very easily using the recipes available in the docker directory and with the provided Makefile targets.\n\nTo avoid problems when accessing an Hadoop cluster using localhost, joblib-hadoop provides the joblib-hadoop-client container. This container has Hadoop 2.7.0 installed and is thus fully functionnal for playing locally with the hadoop cluster.\n\nAnother important point is that the root directory of this project is shared with the /shared directory inside the Hadoop client container. Thanks to this trick, one can code on the host and test in the container without having to rebuild it.\n\nThere are some prerequisites to check before going further.\n\nYou have to be able to run the hello-world container:\n\nThe test suite has to be launched from the joblib-hadoop-client container of the docker-compose configuration. This is achieved very easily with docker-test Makefile target.\n• First, ensure your hadoop cluster is already started:\n\nYour containers should all be in the state Up except joblib-hadoop-client that should have exited with code 0.\n• You can now start the test suite with:\n\nIf you want to access the container directly and test some customizations or run examples. We provided the other following targets to be run from your host:\n• make run-container: start an interactive shell in the joblib-hadoop-client container\n• make run-examples: start a new container, install joblib-hadoop and run the examples\n\nHere we list the helpers to be run from the container:\n• make install: install joblib-hadoop in the container once logged in (you need to be in the container with make run-container first)\n• make run-hdfs-example: run the HDFS Memory multiply example with the cluster.\n• make run-yarb-example: run the YARN parallel backend example on the cluster.\n\nFor the moment hdfs3 cannot be directly installed using pip : the reason is because hdfs3 depends on a C++ based library that is not available in the Linux distros and that one needs to build by hand first.\n\nThe following notes are specific to Ubuntu 16.04 but can also be adapted to Fedora (packages names are slightly different).\n• Use CMake to configure and build\n• Add the following to your ~/.bashrc environment file:\n• Use pip to install hdfs3 (use sudo if needed):"
    },
    {
        "link": "https://scikit-learn.org/stable/api/index.html",
        "document": ""
    },
    {
        "link": "https://scikit-learn.org/stable/inspection.html",
        "document": "Predictive performance is often the main goal of developing machine learning models. Yet summarizing performance with an evaluation metric is often insufficient: it assumes that the evaluation metric and test dataset perfectly reflect the target domain, which is rarely true. In certain domains, a model needs a certain level of interpretability before it can be deployed. A model that is exhibiting performance issues needs to be debugged for one to understand the model’s underlying issue. The module provides tools to help understand the predictions from a model and what affects them. This can be used to evaluate assumptions and biases of a model, design a better model, or to diagnose issues with model performance.\n• None Common pitfalls in the interpretation of coefficients of linear models"
    },
    {
        "link": "https://scikit-learn.org/stable/glossary.html",
        "document": "This glossary hopes to definitively represent the tacit and explicit conventions applied in Scikit-learn and its API, while providing a reference for users and contributors. It aims to describe the concepts and either detail their corresponding API or link to other relevant parts of the documentation which do so. By linking to glossary entries from the API Reference and User Guide, we may minimize redundancy and inconsistency.\n\nWe begin by listing general concepts (and any that didn’t fit elsewhere), but more specific sets of related terms are listed below: Class APIs and Estimator Types, Target Types, Methods, Parameters, Attributes, Data and sample properties.\n\nIn a fitted classifier or outlier detector, predicts a “soft” score for each sample in relation to each class, rather than the “hard” categorical prediction produced by predict. Its input is usually only some observed data, X. If the estimator was not already fitted, calling this method should raise a . A 1-dimensional array, where values strictly greater than zero indicate the positive class (i.e. the last class in classes_). A 2-dimensional array, where the row-wise arg-maximum is the predicted class. Columns are ordered according to classes_. Scikit-learn is inconsistent in its representation of multilabel decision functions. It may be represented one of two ways:\n• None List of 2d arrays, each array of shape: ( , 2), like in multiclass multioutput. List is of length .\n• None Single 2d array of shape ( , ), with each ‘column’ in the array corresponding to the individual binary classification decisions. This is identical to the multiclass classification format, though its semantics differ: it should be interpreted, like in the binary case, by thresholding at 0. A list of 2d arrays, corresponding to each multiclass decision function. A 1-dimensional array, where a value greater than or equal to zero indicates an inlier. The method is provided on every estimator. It usually takes some samples , targets if the model is supervised, and potentially other sample properties such as sample_weight. It should:\n• None clear any prior attributes stored on the estimator, unless warm_start is used;\n• None validate and interpret any parameters, ideally raising an error if invalid;\n• None estimate and store model attributes from the estimated parameters and provided data; and\n• None return the now fitted estimator to facilitate method chaining. Used especially for unsupervised, transductive estimators, this fits the model and returns the predictions (similar to predict) on the training data. In clusterers, these predictions are also stored in the labels_ attribute, and the output of is usually equivalent to . The parameters to are the same as those to . A method on transformers which fits the estimator and returns the transformed training data. It takes parameters as in fit and its output should have the same shape as calling . There are nonetheless rare cases where and do not return the same value, wherein training data needs to be handled differently (due to model blending in stacked ensembles, for instance; such cases should be clearly documented). Transductive transformers may also provide but not transform. One reason to implement is that performing and separately would be less efficient than together. provides a default implementation, providing a consistent interface across transformers where is or is not specialized. In inductive learning – where the goal is to learn a generalized model that can be applied to new data – users should be careful not to apply to the entirety of a dataset (i.e. training and test data together) before further modelling, as this results in data leakage. Primarily for feature extractors, but also used for other transformers to provide string names for each column in the output of the estimator’s transform method. It outputs an array of strings and may take an array-like of strings as input, corresponding to the names of input columns from which output column names can be generated. If is not passed in, then the attribute will be used. If the attribute is not defined, then the input names are named . On a CV splitter (not an estimator), returns the number of elements one would get if iterating through the return value of split given the same parameters. Takes the same parameters as split. Gets all parameters, and their values, that can be set using set_params. A parameter can be used, when set to False to only return those parameters not including , i.e. not due to indirection via contained estimators. Most estimators adopt the definition from , which simply adopts the parameters defined for . , among others, reimplements to declare the estimators named in its parameters as themselves being parameters. Facilitates fitting an estimator in an online fashion. Unlike , repeatedly calling does not clear the model, but updates it with the data provided. The portion of data provided to may be called a mini-batch. Each mini-batch must be of consistent shape, etc. In iterative estimators, often only performs a single iteration. may also be used for out-of-core learning, although usually limited to the case where learning can be performed online, i.e. the model is usable after each and there is no separate processing needed to finalize the model. introduces the convention that calling will produce a model that is not finalized, but the model can be finalized by calling i.e. without passing a further mini-batch. Generally, estimator parameters should not be modified between calls to , although should validate them as well as the new mini-batch of data. In contrast, is used to repeatedly fit the same estimator with the same data but varying parameters. Like , should return the estimator object. To clear the model, a new estimator should be constructed, for instance with . NOTE: Using after results in undefined behavior. Makes a prediction for each sample, usually only taking X as input (but see under regressor output conventions below). In a classifier or regressor, this prediction is in the same target space used in fitting (e.g. one of {‘red’, ‘amber’, ‘green’} if the in fitting consisted of these strings). Despite this, even when passed to fit is a list or other array-like, the output of should always be an array or sparse matrix. In a clusterer or outlier detector the prediction is an integer. If the estimator was not already fitted, calling this method should raise a . An array of shape . Multilabel data may be represented as a sparse matrix if a sparse matrix was used in fitting. Each element should be one of the values in the classifier’s classes_ attribute. An array of shape where each value is from 0 to if the corresponding sample is clustered, and -1 if the sample is not clustered, as in . An array of shape where each value is -1 for an outlier and 1 otherwise. A numeric array of shape , usually float64. Some regressors have extra options in their method, allowing them to return standard deviation ( ) or covariance ( ) relative to the predicted value. In this case, the return value is a tuple of arrays corresponding to (prediction mean, std, cov) as required. The natural logarithm of the output of predict_proba, provided to facilitate numerical stability. A method in classifiers and clusterers that can return probability estimates for each class/cluster. Its input is usually only some observed data, X. If the estimator was not already fitted, calling this method should raise a . Output conventions are like those for decision_function except in the binary classification case, where one column is output for each class (while outputs a 1d array). For binary and multiclass predictions, each row should add to 1. Like other methods, should only be present when the estimator can make probabilistic predictions (see duck typing). This means that the presence of the method may depend on estimator parameters (e.g. in ) or training data (e.g. in ) and may only appear after fitting. A method on an estimator, usually a predictor, which evaluates its predictions on a given dataset, and returns a single numerical score. A greater return value should indicate better predictions; accuracy is used for classifiers and R^2 for regressors by default. If the estimator was not already fitted, calling this method should raise a . Some estimators implement a custom, estimator-specific score function, often the likelihood of the data under the model. A method that returns a score for each given sample. The exact definition of score varies from one class to another. In the case of density estimation, it can be the log density model on the data, and in the case of outlier detection, it can be the opposite of the outlier factor of the data. If the estimator was not already fitted, calling this method should raise a . Available in any estimator, takes keyword arguments corresponding to keys in get_params. Each is provided a new value to assign such that calling after will reflect the changed parameters. Most estimators use the implementation in , which handles nested parameters and otherwise sets the parameter as an attribute on the estimator. The method is overridden in and related estimators. On a CV splitter (not an estimator), this method accepts parameters (X, y, groups), where all may be optional, and returns an iterator over pairs. Each of {train,test}_idx is a 1d integer array, with values from 0 from of any length, such that no values appear in both some and its corresponding . In a transformer, transforms the input, usually only X, into some transformed space (conventionally notated as Xt). Output is an array or sparse matrix of length n_samples and with the number of columns fixed after fitting. If the estimator was not already fitted, calling this method should raise a .\n\nThese common parameter names, specifically used in estimator construction (see concept parameter), sometimes also appear as parameters of functions or non-estimator constructors. Used to specify sample weights when fitting classifiers as a function of the target class. Where sample_weight is also supported and given, it is multiplied by the contribution. Similarly, where is used in a multioutput (including multilabel) tasks, the weights are multiplied across outputs (i.e. columns of ). By default, all samples have equal weight such that classes are effectively weighted by their prevalence in the training data. This could be achieved explicitly with for all class labels. More generally, is specified as a dict mapping class labels to weights ( ), such that each sample of the named class is given that weight. can be used to give all classes equal weight by giving each sample a weight inversely related to its class’s prevalence in the training data: . Class weights will be used differently depending on the algorithm: for linear models (such as linear SVM or logistic regression), the class weights will alter the loss function by weighting the loss of each sample by its class weight. For tree-based algorithms, the class weights will be used for reweighting the splitting criterion. Note however that this rebalancing does not take the weight of samples in each class into account. For multioutput classification, a list of dicts is used to specify weights for each output. For example, for four-class multilabel classification weights should be instead of . The parameter is validated and interpreted with . Determines a cross validation splitting strategy, as used in cross-validation based routines. is also available in estimators such as or which use the predictions of one estimator as training data for another, to not overfit the training supervision. Possible inputs for are usually:\n• None An integer, specifying the number of folds in K-fold cross validation. K-fold will be stratified over classes if the estimator is a classifier (determined by ) and the targets may represent a binary or multiclass (but not multioutput) classification problem (determined by ).\n• None A cross-validation splitter instance. Refer to the User Guide for splitters available within Scikit-learn. With some exceptions (especially where not using cross validation at all is an option), the default is 5-fold. values are validated and interpreted with . Specifies the kernel function to be used by Kernel Method algorithms. For example, the estimators and both have a parameter that takes the name of the kernel to use as string or a callable kernel function used to compute the kernel matrix. For more reference, see the Kernel Approximation and the Gaussian Processes user guides. For estimators involving iterative optimization, this determines the maximum number of iterations to be performed in fit. If iterations are run without convergence, a should be raised. Note that the interpretation of “a single iteration” is inconsistent across estimators: some, but not all, use it to mean a single epoch (i.e. a pass over every sample in the data). FIXME perhaps we should have some common tests about the relationship between ConvergenceWarning and max_iter. Some estimators make use of to store partial solutions during fitting. Thus when is called again, those partial solutions have been memoized and can be reused. A parameter can be specified as a string with a path to a directory, or a instance (or an object with a similar interface, i.e. a method) can be used. values are validated and interpreted with . As a parameter, this is the scheme for determining the distance between two data points. See . In practice, for some algorithms, an improper distance metric (one that does not obey the triangle inequality, such as Cosine Distance) may be used. XXX: hierarchical clustering uses with this meaning. We also use metric to refer to evaluation metrics, but avoid using this sense as a parameter name. The number of features which a transformer should transform the input into. See components_ for the special case of affine projection. Number of iterations with no improvement to wait before stopping the iterative procedure. This is also known as a patience parameter. It is typically used with early stopping to avoid stopping too early. This parameter is used to specify how many concurrent processes or threads should be used for routines that are parallelized with joblib. is an integer, specifying the maximum number of concurrently running workers. If 1 is given, no joblib parallelism is used at all, which is useful for debugging. If set to -1, all CPUs are used. For below -1, (n_cpus + 1 + n_jobs) are used. For example with , all CPUs but one are used. is by default, which means unset; it will generally be interpreted as , unless the current backend context specifies otherwise. Note that even if , low-level parallelism (via Numpy and OpenMP) might be used in some configuration. For more details on the use of and its interactions with scikit-learn, please refer to our parallelism notes. Value with which positive labels must be encoded in binary classification problems in which the positive class is not assumed. This value is typically required to compute asymmetric evaluation metrics such as precision and recall. Whenever randomization is part of a Scikit-learn algorithm, a parameter may be provided to control the random number generator used. Note that the mere presence of doesn’t mean that randomization is always used, as it may be dependent on another parameter, e.g. , being set. The passed value will have an effect on the reproducibility of the results returned by the function (fit, split, or any other function like ). ’s value may be: Use the global random state instance from . Calling the function multiple times will reuse the same instance, and will produce different results. Use a new random number generator seeded by the given integer. Using an int will produce the same results across different calls. However, it may be worthwhile checking that your results are stable across a number of different distinct random seeds. Popular integer random seeds are 0 and 42. Integer values must be in the range . Use the provided random state, only affecting other users of that same random state instance. Calling the function multiple times will reuse the same instance, and will produce different results. is used internally to validate the input and return a instance. For more details on how to control the randomness of scikit-learn objects and avoid common pitfalls, you may refer to Controlling randomness. Depending on the object, can specify:\n• None the score function to be maximized (usually by cross validation),\n• None the multiple score functions to be reported,\n• None the score function to be used to check early stopping, or\n• None for visualization related objects, the score function to output or plot The score function can be a string accepted by or a callable scorer, not to be confused with an evaluation metric, as the latter have a more diverse API. may also be set to None, in which case the estimator’s score method is used. See The scoring parameter: defining model evaluation rules in the User Guide. Where multiple metrics can be evaluated, may be given either as a list of unique strings, a dictionary with names as keys and callables as values or a callable that returns a dictionary. Note that this does not specify which score function is to be maximized, and another parameter such as maybe used for this purpose. The parameter is validated and interpreted using . Logging is not handled very consistently in Scikit-learn at present, but when it is provided as an option, the parameter is usually available to choose no logging (set to False). Any True value should enable some logging, but larger integers (e.g. above 10) may be needed for full verbosity. Verbose logs are usually printed to Standard Output. Estimators should not produce any output on Standard Output with the default setting. When fitting an estimator repeatedly on the same dataset, but for multiple parameter values (such as to find the value maximizing performance as in grid search), it may be possible to reuse aspects of the model learned from the previous parameter value, saving time. When is true, the existing fitted model attributes are used to initialize the new model in a subsequent call to fit. Note that this is only applicable for some models and some parameters, and even some orders of parameter values. In general, there is an interaction between and the parameter controlling the number of iterations of the estimator. For estimators imported from , will interact with or . For these models, the number of iterations, reported via or , corresponds the total number of estimators/iterations learnt since the initialization of the model. Thus, if a model was already initialized with estimators, and is called with or set to , the model will train new estimators. Other models, usually using gradient-based solvers, have a different behavior. They all expose a parameter. The reported corresponds to the number of iteration done during the last call to and will be at most . Thus, we do not consider the state of the estimator since the initialization. partial_fit also retains the model between calls, but differs: with the parameters change and the data is (more-or-less) constant across calls to ; with , the mini-batch of data changes and model parameters stay fixed. There are cases where you want to use to fit on different, but closely related data. For example, one may initially fit to a subset of the data, then fine-tune the parameter search on the full dataset. For classification, all data in a sequence of calls to must include samples from each class."
    },
    {
        "link": "https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html",
        "document": "There are several Python libraries which provide solid implementations of a range of machine learning algorithms. One of the best known is Scikit-Learn, a package that provides efficient versions of a large number of common algorithms. Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well as by very useful and complete online documentation. A benefit of this uniformity is that once you understand the basic use and syntax of Scikit-Learn for one type of model, switching to a new model or algorithm is very straightforward. This section provides an overview of the Scikit-Learn API; a solid understanding of these API elements will form the foundation for understanding the deeper practical discussion of machine learning algorithms and approaches in the following chapters. We will start by covering data representation in Scikit-Learn, followed by covering the Estimator API, and finally go through a more interesting example of using these tools for exploring a set of images of hand-written digits.\n\nIn addition to the feature matrix , we also generally work with a label or target array, which by convention we will usually call . The target array is usually one dimensional, with length , and is generally contained in a NumPy array or Pandas . The target array may have continuous numerical values, or discrete classes/labels. While some Scikit-Learn estimators do handle multiple target values in the form of a two-dimensional, target array, we will primarily be working with the common case of a one-dimensional target array. Often one point of confusion is how the target array differs from the other features columns. The distinguishing feature of the target array is that it is usually the quantity we want to predict from the data: in statistical terms, it is the dependent variable. For example, in the preceding data we may wish to construct a model that can predict the species of flower based on the other measurements; in this case, the column would be considered the target array. With this target array in mind, we can use Seaborn (see Visualization With Seaborn) to conveniently visualize the data:"
    },
    {
        "link": "https://pm.jh.edu/cookbook/PythonDataScienceHandbook/notebooks/05.02-Introducing-Scikit-Learn.html",
        "document": ""
    }
]