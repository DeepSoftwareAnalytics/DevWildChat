[
    {
        "link": "https://analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming",
        "document": "Dynamic Programming is a problem-solving method used to break complex problems into smaller, simpler subproblems. Instead of solving the same subproblem multiple times, it stores the results of these subproblems and reuses them when needed. This saves time and makes the solution more efficient. It is often used in optimization problems, where you need to find the best solution among many possibilities. Examples include finding the shortest path, solving knapsack problems, or calculating Fibonacci numbers. In this article, we will cover all about dynamic programming, from basics to advanced concepts, in a simple and easy-to-understand way.\n\nApart from being a good starting point for grasping reinforcement learning, dynamic programming can help find optimal solutions to planning problems faced in the industry, with an important assumption that the specifics of the environment are known. DP presents a good starting point to understand RL algorithms that can solve more complex problems.Also, with artificial intelligence it can help more to these algorithms.\n\nSunny manages a motorbike rental company in Ladakh. Being near the highest motorable road in the world, there is a lot of demand for motorbikes on rent from tourists. Within the town he has 2 locations where tourists can come and get a bike on rent. If he is out of bikes at one location, then he loses business.\n• Bikes are rented out for Rs 1200 per day and are available for renting the day after they are returned.\n• Sunny can move the bikes from 1 location to another and incurs a cost of Rs 100\n• With experience Sunny has figured out the approximate probability distributions of demand and return rates.\n• Number of bikes returned and requested at each location are given by functions g(n) and h(n) respectively. In exact terms the probability that the number of bikes rented at both locations is n is given by g(n) and probability that the number of bikes returned at both locations is n is given by h(n)\n\nThe problem that Sunny is trying to solve is to find out how many bikes he should move each day from 1 location to another so that he can maximise his earnings.\n\nHere, we exactly know the environment (g(n) & h(n)) and this is the kind of problem in which dynamic programming can come in handy. Similarly, if you can properly model the environment of your problem where you can take discrete actions, then DP can help you find the optimal solution. In this article, we will use DP to train an agent using Python to traverse a simple environment, while touching upon key concepts in RL such as policy, reward, value function and more.\n• Breaks Problems into Smaller Parts: Dynamic Programming simplifies complex problems by dividing them into smaller, easier-to-solve subproblems. This makes it easier to handle and understand.\n• Avoids Repeated Work: It saves time by storing the results of subproblems and reusing them when needed, instead of recalculating the same thing multiple times.\n• Efficient for Certain Problems: For problems like finding the shortest path or optimizing resources, DP often provides a faster and more efficient solution compared to other methods.\n• Clear Structure: DP solutions follow a step-by-step approach, making it easier to plan and implement the solution in a logical way.\n• Works for Many Real-Life Problems: It can be applied to a wide range of practical issues, such as scheduling, budgeting, and inventory management, making it a versatile tool.\n\nMost of you must have played the tic-tac-toe game in your childhood. If not, you can grasp the rules of this simple game from its wiki page. Suppose tic-tac-toe is your favourite game, but you have nobody to play it with. So you decide to design a bot that can play this game with you. Some key questions are:\n\nCan you define a rule-based framework to design an efficient bot?\n\nYou sure can, but you will have to hardcode a lot of rules for each of the possible situations that might arise in a game. However, an even more interesting question to answer is:\n\nCan you train the bot to learn by playing against you several times? And that too without being explicitly programmed to play tic-tac-toe efficiently?\n\nA few considerations for this are:\n\nFirst, the bot needs to understand the situation it is in. A tic-tac-toe has 9 spots to fill with an X or O. Each different possible combination in the game will be a different situation for the bot, based on which it will make the next move. Each of these scenarios as shown in the below image is a different state.\n• Once the state is known, the bot must take an action in a way it considers to be optimum to win the game (policy)\n• This move will result in a new scenario with new combinations of O’s and X’s which is a new state and a numerical reward will be given based on the quality of move with the goal of winning the game (cumulative reward)\n\nFor more clarity on the aforementioned reward, let us consider a match between bots O and X:\n\nConsider the following situation encountered in tic-tac-toe:\n\nIf bot X puts X in the bottom right position for example, it results in the following situation:\n\nBot O would be rejoicing (Yes! They are programmed to show emotions) as it can win the match with just one move. Now, we need to teach X not to do this again. So we give a negative reward or punishment to reinforce the correct behaviour in the next trial. We say that this action in the given state would correspond to a negative reward and should not be considered as an optimal action in this situation.\n\nSimilarly, a positive reward would be conferred to X if it stops O from winning in the next move:\n\nCheckout this article about Reinforcement Learning via Markov Decision Process\n\nNow that we understand the basic terminology, let’s talk about formalising this whole process using a concept called a Markov Decision Process or MDP.\n• A description T of each action’s effects in each state\n\nNow, let us understand the markov or ‘memoryless’ property.\n\nAny random process in which the probability of being in a given state depends only on the previous state, is a markov process.\n\nIn other words, in the markov decision process setup, the environment’s response at time t+1 depends only on the state and action representations at time t, and is independent of whatever happened in the past.\n\nThe above diagram clearly illustrates the iteration at each time step wherein the agent receives a reward R and ends up in state S based on its action A at a particular state S . The overall goal for the agent is to maximise the cumulative reward it receives in the long run. Total reward at any time instant t is given by:\n\nwhere T is the final time step of the episode. In the above equation, we see that all future rewards have equal weight which might not be desirable. That’s where an additional concept of discounting comes into the picture. Basically, we define γ as a discounting factor and each reward after the immediate reward is discounted by this factor as follows:\n\nFor discount factor < 1, the rewards further in the future are getting diminished. This can be understood as a tuning parameter which can be changed based on how much one wants to consider the long term (γ close to 1) or short term (γ close to 0).\n\nLearn about the Logistic Regression in Machine Learning\n\nState Value Function:How good it is to be in a given state?\n\nCan we use the reward function defined at each time step to define how good it is, to be in a given state for a given policy? The value function denoted as v(s) under a policy π represents how good a state is for an agent to be in. In other words, what is the average reward that the agent will get starting from the current state under policy π?\n\nE in the above equation represents the expected reward at each state if the agent follows policy π and S represents the set of all possible states.\n\nPolicy, as discussed earlier, is the mapping of probabilities of taking each possible action at each state (π(a/s)). The policy might also be deterministic when it tells you exactly what to do at each state and does not give probabilities.\n\nNow, it’s only intuitive that ‘the optimum policy’ can be reached if the value function is maximised for each state. This optimal policy is then given by:\n\nState-Action Value Function: How good an action is at a particular state?\n\nThe above value function only characterizes a state. Can we also know how good an action is at a particular state? A state-action value function, which is also called the q-value, does exactly that. We define the value of action a, in state s, under a policy π, as:\n\nThis is the expected return the agent will get if it takes action At at time t, given state St, and thereafter follows policy π.\n\nBellman Expectation Equation: The value information from successor states is being transferred back to the current state\n\nBellman was an applied mathematician who derived equations that help to solve an Markov Decision Process.\n\nLet’s go back to the state value function v and state-action value function q. Unroll the value function equation to get:\n\nIn this equation, we have the value function for a given policy π represented in terms of the value function of the next state.\n• Choose an action a, with probability π(a/s) at the state s, which leads to state s’ with prob p(s’/s,a). This gives a reward [r + γ*v (s)] as given in the square bracket above.\n• This is called the Bellman Expectation Equation. The value information from successor states is being transferred back to the current state, and this can be represented efficiently by something called a backup diagram as shown below.\n\nThe Bellman expectation equation averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way.\n\nWe have n (number of states) linear equations with unique solution to solve for each state s.\n\nThe goal here is to find the optimal policy, which when followed by the agent gets the maximum cumulative reward. In other words, find a policy π, such that for no other π can the agent get a better expected return. We want to find a policy which achieves maximum value for each state.\n\nNote that we might not get a unique policy, as under any situation there can be 2 or more paths that have the same return and are still optimal.\n\nOptimal value function can be obtained by finding the action a which will lead to the maximum of q*. This is called the bellman optimality equation for v*.\n\nIntuitively, the Bellman optimality equation says that the value of each state under an optimal policy must be the return the agent gets when it follows the best action as given by the optimal policy. For optimal policy π*, the optimal value function is given by:\n\nGiven a value function q*, we can recover an optimum policy as follows:\n\nThe value function for optimal policy can be solved through a non-linear system of equations. We can can solve these efficiently using iterative methods that fall under the umbrella of dynamic programming.\n\nDynamic programming algorithms solve a category of problems called planning problems. Herein given the complete model and specifications of the environment (MDP), we can successfully find an optimal policy for the agent to follow. It contains two main steps\n\nBreak the problem into subproblems and solve it. Solutions to subproblems are cached or stored for reuse to find an overall optimal solution to the problem at hand.\n\nTo solve a given MDP, the solution must have the components to:\n• Find out how good an arbitrary policy isFind out the optimal policy for the given MDP\n\nPolicy Evaluation: Find out how good a policy is?\n\nPolicy evaluation answers the question of how good a policy is. Given an MDP and an arbitrary policy π, we will compute the state-value function. This is called policy evaluation in the DP literature. The idea is to turn bellman expectation equation discussed earlier to an update.\n\nTo produce each successive approximation vk+1 from vk, iterative policy evaluation applies the same operation to each state s. It replaces the old value of s with a new value obtained from the old values of the successor states of s, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated, until it converges to the true value function of a given policy π.\n\nLet us understand policy evaluation using the very popular example of Gridworld.\n\nA bot is required to traverse a grid of 4×4 dimensions to reach its goal (1 or 16). Each step is associated with a reward of -1. There are 2 terminal states here: 1 and 16 and 14 non-terminal states given by [2,3,….,15]. Consider a random policy for which, at every state, the probability of every action {up, down, left, right} is equal to 0.25. We will start with initialising v for the random policy to all 0s.\n\nThis is definitely not very useful. Let’s calculate v for all the states of 6:\n\nFor terminal states p(s’/s,a) = 0 and hence v (1) = v (16) = 0 for all k. So v for the random policy is given by:\n\nNow, for v (s) we are assuming γ or the discounting factor to be 1:\n\nAs you can see, all the states marked in red in the above diagram are identical to 6 for the purpose of calculating the value function. Hence, for all these states, v (s) = -2.\n\nFor all the remaining states, i.e., 2, 5, 12 and 15, v can be calculated as follows:\n\nIf we repeat this step several times, we get v\n\nRead this article about the Machine Learning Algorithms\n\nUsing policy evaluation we have determined the value function v for an arbitrary policy π. We know how good our current policy is. Now for some state s, we want to understand what is the impact of taking an action a that does not pertain to policy π. Let’s say we select a in s, and after that we follow the original policy π. The value of this way of behaving is represented as:\n• If the new value is greater than the current value function vπ(s)vπ(s), it means the new policy π′π′ is better.\n• We repeat this process for all states to find the best policy.\n• In this process, the agent follows a greedy policy, meaning it only looks one step ahead to make decisions.\n• Using the value function vπvπ from the random policy ππ, we can improve the policy by choosing the path with the highest value.\n• We start with any policy and, for each state, look one step ahead to find the action that leads to the state with the highest value.\n• This step-by-step improvement is done for every state to refine the policy.\n\nAs shown below for state 2, the optimal action is left which leads to the terminal state having a value . This is the highest among all the next states (0,-18,-20). This is repeated for all states to find the new policy.\n\nOverall, after the policy improvement step using v , we get the new policy π’:\n\nLooking at the new policy, it is clear that it’s much better than the random policy. However, we should calculate v ’ using the policy evaluation technique we discussed earlier to verify this point and for better understanding.\n\nOnce the policy has been improved using v to yield a better policy π’, we can then compute v ’ to improve it further to π’’. Repeated iterations are done to converge approximately to the true value function for a given policy π (policy evaluation). Improving the policy as described in the policy improvement section is called policy iteration.\n\nIn this way, the new policy is sure to be an improvement over the previous one and given enough iterations, it will return the optimal policy. This sounds amazing but there is a drawback – each iteration in policy iteration itself includes another iteration of policy evaluation that may require multiple sweeps through all the states. Value iteration technique discussed in the next section provides a possible solution to this.\n\nWe saw in the gridworld example that at around k = 10, we were already in a position to find the optimal policy. So, instead of waiting for the policy evaluation step to converge exactly to the value function v , we could stop earlier.\n\nWe can also get the optimal policy with just 1 step of policy evaluation followed by updating the value function repeatedly (but this time with the updates derived from bellman optimality equation). Let’s see how this is done as a simple backup operation:\n\nThis is identical to the bellman update in policy evaluation, with the difference being that we are taking the maximum over all actions. Once the updates are small enough, we can take the value function obtained as final and estimate the optimal policy corresponding to that.\n• DP can only be used if the model of the environment is known.\n• Has a very high computational expense, i.e., it does not scale well as the number of states increase to a large number. An alternative called asynchronous dynamic programming helps to resolve this issue to some extent.\n\nDP in Action: Finding optimal policy for Frozen Lake environment using Python\n\nIt is of utmost importance to first have a defined environment in order to test any kind of policy for solving an MDP efficiently. Thankfully, OpenAI, a non profit research organization provides a large number of environments to test and play with various reinforcement learning algorithms. To illustrate dynamic programming here, we will use it to navigate the Frozen Lake environment.\n\nThe agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n\nThe surface is described using a grid like the following:\n\nThe idea is to reach the goal from the starting point by walking only on frozen surface and avoiding all the holes. Installation details and documentation is available at this link.\n\nOnce gym library is installed, you can just open a jupyter notebook to get started.\n\nNow, the env variable contains all the information regarding the frozen lake environment. Before we move on, we need to understand what an episode is. An episode represents a trial by the agent in its pursuit to reach the goal. An episode ends once the agent reaches a terminal state which in this case is either a hole or the goal.\n• Policy: 2D array of a size n(S) x n(A), each cell represents a probability of taking action a in state s.\n• Theta: A threshold of a value function change. Once the update to value function is below this number\n• Max_iterations: Maximum number of iterations to avoid letting the program run indefinitely\n\nThis function will return a vector of size nS, which represent a value function for each state.\n\nLet’s start with the policy evaluation step. The objective is to converge to the true value function for a given policy π. We will define a function that returns the required value function.\n\nNow coming to the policy improvement part of the policy iteration algorithm. We need a helper function that does one step lookahead to calculate the state-value function. This will return an array of length nA containing expected value of each action\n\nNow, the overall policy iteration would be as described below. This will return a tuple (policy,V) which is the optimal policy matrix and value function for each state.\n\nThe parameters are defined in the same manner for value iteration. The value iteration algorithm can be similarly coded:\n\nFinally, let’s compare both methods to look at which of them works better in a practical setting. To do this, we will try to learn the optimal policy for the frozen lake environment using both techniques described above. Later, we will check which technique performed better based on the average return after 10,000 episodes.\n\nWe observe that value iteration has a better average reward and higher number of wins when it is run for 10,000 episodes.\n\nWhat is the difference between dynamic programming and reinforcement learning?\n\nDynamic programming (DP) and reinforcement learning (RL) are both powerful tools in computer science for solving problems involving decision-making and sequential actions, but they approach the problem in fundamentally different ways. Temporal difference learning is a key concept within reinforcement learning. Here’s a breakdown of the key differences:\n• Dynamic Programming (DP): Considers the environment to be a perfect model. This implies that DP is aware of the precise ramifications (rewards and transitions) of every decision made in every state space.\n• Reinforcement Learning (RL): Doesn’t need an ideal model. Through interacting with its surroundings and getting rewarded for its efforts, RL learns by making mistakes and learning from them.\n\nChoosing the Best Course of Action:\n• DP: Finds the optimal control or best course of action to maximize the overall reward using a deterministic methodology.\n• RL: Adopts a more investigative strategy. It gains experience by making mistakes, and while the best answer it comes up with might not be optimal, it’s still good enough.\n\nIn this article, we became familiar with model based planning using dynamic programming, which given all specifications of an environment, can find the best policy to take. I want to particularly mention the brilliant book on RL by Sutton and Barto which is a bible for this technique and encourage people to refer it. More importantly, you have taken the first step towards mastering reinforcement learning. Stay tuned for more articles covering different algorithms within this exciting domain.This article will help you understand full concepts of approximate dynamic programming and having transition probabilities to make these programming a dynamic programming methods."
    },
    {
        "link": "https://geeksforgeeks.org/dynamic-programming-in-reinforcement-learning",
        "document": "Dynamic Programming (DP) in Reinforcement Learning (RL) deals with solving complex decision-making problems where an agent learns to make optimal choices through experience. It is an algorithmic technique that relies on breaking down a problem into simpler subproblems, solving them independently, and combining their solutions.\n\nIn Reinforcement Learning, dynamic programming is often used for policy evaluation, policy improvement, and value iteration. The main goal is to optimize an agent's behavior over time based on a reward signal received from the environment.\n\nIn Reinforcement Learning, the problem of learning an optimal policy involves an agent that interacts with an environment modeled as a Markov Decision Process (MDP).\n• States ): The different situations in which the agent can be.\n• Actions ): The choices the agent can make in each state.\n• Transition Model ): The probability of transitioning from one state sss to another state\n• Reward Function ): The reward the agent receives after taking action\n• Discount Factor ): A value between 0 and 1 that determines the importance of future rewards.\n\nIn Dynamic Programming, we assume that the agent has access to a model of the environment (i.e., transition probabilities and reward functions). Using this model, DP algorithms iteratively compute the value function or Q-function that estimates the expected return for each state or state-action pair.\n\nThe main Dynamic Programming algorithms used in Reinforcement Learning are:\n\nPolicy evaluation is the process of determining the value function for a given policy . The value function represents the expected cumulative reward the agent will receive if it follows policy from state .\n\nThe Bellman equation for policy evaluation is:\n\nHere, is updated iteratively until it converges to the true value function for the policy.\n\nPolicy iteration is an iterative process of improving the policy based on the value function. It alternates between two steps:\n• Policy Evaluation : Evaluate the current policy by calculating the value function.\n• Policy Improvement : Improve the policy by choosing the action that maximizes the expected return, given the current value function.\n\nThe process repeats until the policy converges to the optimal policy, where no further improvements can be made.\n\nValue iteration combines both the policy evaluation and policy improvement steps into a single update. It iteratively updates the value function using the Bellman optimality equation:\n\nThis update is applied to all states, and the algorithm converges to the optimal value function and the optimal policy.\n\nDynamic Programming is particularly useful in Reinforcement Learning when the agent has a complete model of the environment, which is often not the case in real-world applications. However, it serves as a valuable tool for:\n• Solving Deterministic MDPs : In problems where the transition probabilities are known, DP can compute the optimal policy with high efficiency.\n• Policy Improvement : DP algorithms like policy iteration can systematically improve a policy by refining the value function and updating the agent’s behavior.\n• Robust Evaluation : DP provides an effective way to evaluate policies in environments where transition models are complex but known.\n\nWhile Dynamic Programming provides a theoretical foundation for solving RL problems, it has several limitations:\n• Model Dependency : DP assumes that the agent has a perfect model of the environment, including transition probabilities and rewards. In real-world scenarios, this is often not the case.\n• Computational Complexity : The state and action spaces in real-world problems can be very large, making DP algorithms computationally expensive and time-consuming.\n• Exponential Growth : In high-dimensional state and action spaces, the number of computations grows exponentially, which may lead to infeasible solutions.\n\nIn this implementation, we are going to create a simple Grid World environment and apply Dynamic Programming methods such as Policy Evaluation and Value Iteration.\n\nGrid World is a grid where an agent moves and receives rewards based on its state. The agent takes actions (up, down, left, right) to navigate through the grid. In this step, we create a class to define the environment, including the grid size, terminal states, and rewards.\n\nPolicy Evaluation involves calculating the value function for a given policy. The value function indicates the expected return from each state when following the policy. This is done iteratively until convergence. The Bellman equation is used to update the value function for each state.\n\nValue Iteration is a method to find the optimal policy by iteratively updating the value function for each state. In each iteration, it computes the value of each state considering all possible actions and then updates the value function by choosing the action that maximizes the expected reward.\n\nAfter the value function converges, the optimal policy is derived by selecting the action that gives the maximum expected value for each state.\n\nWe will create several visualization functions to plot the grid world, value function, and policy. This helps to visually understand how the agent is navigating the grid and making decisions.\n\nIn this step, we initialize the grid world environment, define a policy, and then apply Policy Evaluation and Value Iteration. The results are visualized to help better understand the learned value function and policy.\n\nThis output represent the original grid-world environment and terminal states are marked with red 'T' and non-terminal states are not marked.\n\nThe terminal states states have a value of 0 because no further rewards are earned after reaching them. Non-terminal states have negative values because the agent incurs a cost (-1) for each step taken.\n\nThe values are the same as in Policy Evaluation because the default policy is already optimal for this simple grid world. In more complex environments, the optimal value function will differ from the policy evaluation results.\n\nHere, the arrow point Arrows point towards the terminal states and he policy guides the agent to reach the terminal states in the fewest steps."
    },
    {
        "link": "https://medium.com/data-science/dynamic-programming-in-rl-52b44b3d4965",
        "document": "Dynamic Programming is a mathematical optimization approach typically used to improvise recursive algorithms. It basically involves simplifying a large problem into smaller sub-problems. There are two properties that a problem must exhibit to be solved using dynamic programming:\n\nWe’ll be discussing ‘Planning in RL’ using dynamic programming. Planning mainly requires the complete environment’s knowledge (usually an MDP) or a model of the environment in advance. And using this knowledge, we can solve for the optimal policy.\n\nIn my previous article on Reinforcement Learning, I have covered the formulation of RL problems as a Markov Decision Process (MDP). You can consider giving it a read if you’re unaware of what an MDP is.\n\nIn this article, we will take the example of a Grid World Game and try to win it using ‘planning’. But first things first, let’s talk about how are we going to approach this problem:\n• Prediction: In this step, we evaluate our policy, i.e., we evaluate the value function for all the states.\n• Control: In this step, we use the computed value function to improve our policy."
    },
    {
        "link": "https://blog.mlq.ai/guide-to-deep-reinforcement-learning",
        "document": ""
    },
    {
        "link": "https://ecal.berkeley.edu/tbsi/References/Lewis09%20-%20UTA%20-%20RL%20Feedback%20Control.pdf",
        "document": ""
    },
    {
        "link": "https://analyticsvidhya.com/blog/2021/02/understanding-the-bellman-optimality-equation-in-reinforcement-learning",
        "document": "In this article, first, we will discuss some of the basic terminologies of Reinforcement Learning, then we will further understand the crux behind the most commonly used equations in Reinforcement Learning, and then we will dive deep into understanding the Bellman Optimality Equation.\n\nThe aim of this article is to give an intuition about Reinforcement Learning as well as what the q learning Bellman equation Optimality Equation is and how it is used in solving Reinforcement Learning problems. This article does not contain complex mathematical explanations for the equations. So don’t worry.\n\nThis article was published as a part of the Data Science Blogathon.\n\nReinforcement Learning (RL)in Dynamic Programming is a paradigm in machine learning that encompasses the concept of agents interacting with environments to learn optimal actions through trial and error. Analogous to a child learning to walk or ride a bicycle, RL entails navigating a dynamic environment with partial supervision, akin to a child’s experience of receiving occasional guidance but largely learning through self-correction. Here are the points\n• In RL, agents function within environments, moving between states and taking actions to maximize cumulative rewards.\n• The process occurs over an infinite horizon, where each action affects subsequent states, creating a successor state based on the chosen action.\n• Agents use policies to guide decision-making, which dictate strategies for action selection.\n• Policy iteration is a core technique in RL, involving iterative refinement of policies to achieve optimal solutions.\n• This process includes evaluating policies using backup diagrams, recursively assessing state-action values to update policies.\n• Dynamic programming is a key method in RL, assisting in policy evaluation and improvement.\n• It enables agents to make informed decisions by leveraging learned value functions.\n\nMarkov process underpins RL frameworks, characterized by state transitions governed by probabilities, facilitating decision-making in uncertain environments. We often use dynamic programming techniques to efficiently solve Markov decision processes, allowing agents to make optimal decisions even in complex environments.\n\nAgents seek optimal actions through iterative policy refinement, aiming to maximize cumulative rewards over time. Neural networks play a crucial role in RL, enabling agents to approximate value functions or policies, enhancing decision-making capabilities.\n\nAt its core, RL embodies a recursive relationship between actions, states, and rewards, where agents iteratively learn from experience to optimize decision-making processes. By navigating environments and adapting to feedback, RL agents emulate the trial-and-error learning observed in natural systems, fostering autonomous and adaptive behavior.\n\nThe agent in RL is an entity that tries to learn the best way to perform a specific task. In our example, the child is the agent who learns to ride a bicycle.\n\nThe action in RL is what the agent does at each time step. In the example of a child learning to walk, the action would be “walking”.\n\nThe state in RL describes the current situation of the agent. After doing performing an action, the agent can move to different states. In the example of a child learning to walk, the child can take the action of taking a step and move to the next state (position).\n\nIn reinforcement learning, we give feedback to the agent based on its actions. We reward the agent positively when its action is good and can lead to winning or a positive outcome, and we give a negative reward otherwise. This is the same as if after successfully riding a bicycle for more amount of time, keeping balance the elder ones of child applauds him/her indicating the positive feedback.\n\nThe environment in RL refers to the outside world of an agent or physical world in which the agent operates.\n\nNow, the above definition taken from Wikipedia should make sense to some extent. Let us now understand the most common equations used in RL.\n\nIn RL, the agent selects an action from a given state, which results in transitioning to a new state, and the agent receives rewards based on the chosen action. This process happens sequentially which creates a trajectory that represents the chain/sequence of states, actions, and rewards. The goal of an RL agent is to maximize the total amount of rewards or cumulative rewards by taking actions in given states.\n\nFor easy understanding of Dynamic Programming equations, we will define some notations. We denote a set of states as S, a set of actions as A, and a set of rewards as R. At each time step t = 0, 1, 2, …, some representation of the environment’s state St ∈ S is received by the agent. According to this state, the agent selects an action At ∈ A which gives us the state-action pair (St, At). In the next time step t+1, the transition of the environment happens and the new state St+1 ∈ S is achieved. At this time step t+1, a reward Rt+1 ∈ R is received by the agent for the action At taken from state St.\n\nAs we mentioned above that the goal of the agent is to maximize the cumulative rewards, we need to represent this cumulative reward in a formal way to use it in the calculations. We can call it as Expected Return and can be represented as,\n\nThe above Dynamic Programming equation works for episodic tasks. For continuing tasks, we need to update this equation as we don’t have a limit of time step T in continuing tasks. Discount factor γ is introduced here which forces the agent to focus on immediate rewards instead of future rewards. The value of γ remains between 0 and 1.\n\nPolicy in RL decides which action will the agent take in the current state. In other words, It tells the probability that an agent will select a specific action from a specific state.\n\nIf at time t, an agent follows policy π, then π(a|s) becomes the probability that the action at time step t is At=a if the state at time step t is St=s. The meaning of this is, the probability that an agent will take an action a in state s is π(a|s) at time t with policy π.\n\nThere are two kinds of value functions which are described below:\n\nThe state-value function for policy π denoted as vπ determines the goodness of any given state for an agent who is following policy π. This function gives us the value which is the expected return starting from state s at time step t and following policy π afterward.\n\nCheckout this article about Model Based Planning using Dynamic Programming\n\nThe action-value function mentioned above determines the goodness of the action taken by the agent from a given state for policy π. This function gives the value which is the expected return starting from state s at time step t, with action a, and following policy π afterward. The output of this function is also called as Q-value where q stands for Quality. Note that in the state-value function, we did not consider the action taken by the agent.\n\nNow that we have defined the basic equations for solving an RL problem, our focus now should be to find an optimal solution (optimal policy in the case of RL). Therefore let us go through the optimality domain of RL and along the way, we will encounter the q learning Bellman equation Optimality Equation also.\n\nThe optimal policy should have an associated optimal state-value function or action-value function.\n\nThe optimal state-value function v∗ yields the largest expected return feasible for each of the state s with any policy π. And the optimal action-value function q∗ yields the largest expected return feasible for each state-action pair (s, a) with any policy π.\n\nThe optimal action-value function q∗ satisfies the below-given equation. This is a fundamental property of the optimal action-value function.\n\nThis equation is called Bellman Optimality Equation. Now, what this equation has to do with optimality in RL? The answer is in the next paragraph.\n\nAt time ( t ), for any state-action pair ( (s, a) ), this equation asserts that the expected return starts with the immediate reward ( R_{t+1} ) obtained by choosing action ( a ) in state ( s ), plus the maximum expected discounted return from any potential next state-action pair ( (s’, a’) ).\n\nConsidering the agent following an optimal policy, the latter state s′ will be the state from we can take the best possible next action a′ at time t+1. This seems somewhat difficult to understand.\n\nIn the equation, we observe the above property by finding ( q^*(s’, a’) ), which represents the expected return after choosing action ( a ) in state ( s ), and then maximizing it to achieve the optimal Q-value.\n\nDuring the training of an RL agent in order to determine the optimal policy, we need to update the Q-value (Output of the Action-Value Function) iteratively.\n\nTo converge the Q-value to an optimal q∗, we adjust it towards the right-hand side of the Bellman Optimality Equation for each state-action pair. Iteratively, we minimize the loss between the Q-value and q∗ by updating the Q-value whenever encountering the same state-action pair. The loss can be given as q∗(s, a)-q(s, a).\n\nWe can not overwrite the newly computed Q-value with the older value. Instead, we use a parameter called learning rate denoted by α, to determine how much information of the previously computed Q-value for the given state-action pair we retain over the newly computed Q-value calculated for the same state-action pair at a later time step.\n\nThe higher the learning rate, the more quickly the agent will adopt the newly computed Q-value. Therefore we need to take care of the trade-off between new and old Q-value using the appropriate learning rate.\n\nAt this point, you should have grasped why and how the Q-learning Bellman equation is utilized in RL. Furthermore, you should have strengthened your understanding of Reinforcement Learning. Thank you for reading this article.\n\nIn the realm of reinforcement learning, understanding the Bellman Optimality Equation is paramount. It delineates the optimal value function, guiding agents towards decision-making that maximizes cumulative rewards. Subsequently, through Q-learning and iterative updates, agents converge to the optimal policy, unraveling the intricate dynamics of decision-making in complex environments.\n\nThe Bellman Optimality Equation guides the quest for optimal solutions in intelligent agent-based systems. It helps orchestrate actions to navigate the uncertainties and complexities of real-world scenarios.\n• Reinforcement learning is a paradigm where agents learn to take optimal actions in an environment to maximize cumulative rewards through trial and error.\n• The goal is to find an optimal policy that maximizes the expected return for any given state.\n• The Bellman Optimality Equation relates the optimal action-value (Q-value) of a state-action pair to the expected return of the best action in the subsequent state.\n• The Q-learning algorithm iteratively updates the Q-values using the Bellman Optimality Equation until convergence to the optimal Q-function.\n\n[1] NPTEL Course on RL [2] RL Course by David Silver [3] DeepLizard RL"
    },
    {
        "link": "https://datacamp.com/tutorial/bellman-equation-reinforcement-learning",
        "document": "Learn the art of Machine Learning and come away as a boss at prediction, pattern recognition, and the beginnings of Deep and Reinforcement Learning."
    },
    {
        "link": "https://suzyahyah.github.io/reinforcement%20learning/2023/02/01/DP-RL-Bellman.html",
        "document": "The Bellman optimality equation is the necessary condition of finding the optimal policy in MDPs via dynamic Programming. DP is appropriate when we know the dynamics of the environment, i.e., when we know the transition probability over the next state and rewards: $p(s’ r \\mid s, a)$.\n\nThe full notebook demonstration of Policy Iteration including how to set up the simulation on Google Colab is available HERE.\n\nTo guide us on how to act in the current time step, we need Value Functions of either being in a state, or of taking a particular action in a state (state-action). A key assumption to writing this is the Reward Hypothesis: All that is goals and plans can be described as the cumulative sum of future rewards.\n\nThe sum of future rewards from time $t$, can be expressed as the reward, $R_{t+1}$, and a discounted future sum of rewards $G_{t+1}$ which itself is recursively defined in terms of future rewards. $G_{t} = R_{t+1} + \\gamma G_{t+1}$\n\nThe value function of a state $v_{\\pi}(s)$ and the action-value function of taking action $a$ in state $s$ is $q_{\\pi}(s, a)$ are the expected future rewards, where the expectation is taken with respect to future actions according to the policy $\\pi$.\n\nDeconstructing $\\mathbb{E}_{\\pi}$ into expectation of returns under the policy, we have a weighted sum over the action probabilities $\\pi(a \\mid s)$ and a weighted sum over all possible next states $s’$ and rewards $r$.\n\nThe Bellman equation for $q_{\\pi}$ follows a similar form, except in order to write this recursively in terms of $q_{\\pi}$, we need to expand the expectation in eq(8).\n\nNote that because of the Markov Assumption in MDPs, we can use a recursive definition from Eq(4) $\\rightarrow$ Eq(5) and Eq(9) $\\rightarrow$ Eq(10), since the policy and the value function does not depend on time.\n\nThe centrality of the Bellman equations in RL Optimization\n• In mathematical optimization, writing things in terms of other variables that we want to solve for often makes optimization more “compact”. With the Bellman equations, we can solve for the value function of a very small state space directly with a system of $\\mid S \\mid$ linear equations.\n• Writing things recursively allows us to break a larger problem into a series of much smaller problems. Recursion gives rise to update equations that reuse previous computation allowing us to iteratively compute the final value. The key insight is that DP algorithms turn Bellman equations which contain recursive definitions, into update rules. This structures the search for good policies, allowing us to compute the value function iteratively for a given fixed policy (Policy evaluation).\n\nPolicy iteration consists of cycling between two phases; policy evaluation phase (computing $v_{\\pi}$), and policy improvement phase once we know $v_{\\pi}$; i.e. finding $\\pi’ \\geq \\pi$. We essentially perform the iterative computation $v_{\\pi0} \\rightarrow \\pi_1 \\rightarrow v_{\\pi1} \\cdots v_* \\rightarrow \\pi_{*}$.\n\nPolicy Evaluation: Finding the value function for a given policy\n\nThe policy evaluation phase gets us iteratively closer to the estimate of the actual value function $v_{\\pi}$. The recursive definition in the Bellman equation of eq(5) indicates an updating procedure that converges to the true value function. Let $\\tilde{v_{\\pi}}$ be an approximate value function for the policy $\\pi$. Then, we can get closer to the true value function $v_{\\pi}$ with the update:\n\nThe iterative procedure converges when the LHS and RHS of Eq(5) reaches equality or the changes in values are less than some very small $\\delta$.\n\nValue functions $v_{\\pi}$ allow us to estimate the value of a state which incorporates future rewards. If we had the optimal value function which expresses the best action that achieves the most value from that state, then we essentially have the optimal policy.\n\nHowever we don’t have the optimal policy or the optimal Value function to begin with. A value function is described with respect to a policy on downstream timesteps. If the policy changes, the value function changes. How can we compute the “optimal value function” if we don’t have the optimal policy to begin with?\n\nThe policy improvement theorem states that if we have $v_{\\pi}$, we can act greedily (which implies a different policy $\\pi’$) to get better or equal rewards, since $q_{\\pi}(s, \\pi’(s)) \\geq v_{\\pi}(s)$.\n\nWe implement the Policy Iteration algorithm via Dynamic Programming on a 8x8 frozen lake map which looks like the following:\n\nThe full notebook demonstration of Policy Iteration including how to set up the simulation on Google Colab is available here.\n\nCan there be more than one optimal policy?\n\nThe vanilla policy iteration algorithm solves the Frozen Lake environment in 15 iterations.\n\nStarting from a random policy at iteration 0, we eventually arrive at the optimal policy which is not unique. At iteration 15, going left or down in the first two rows of the map are equally optimal. This is consistent with the known result that the optimal value function is unique, but there can be more than one optimal policy.\n\nWhat is the effect of the environment on policies?\n\nWe can see that applying the exact same DP algorithm on the same map, results in rather different policies if the model of the world is different. In the deterministic case:\n\nis_slippery=True: If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions.\n\nThe policy learnt is one that makes use of the non-deterministic condition; the agent tries very hard to stay away from the lakes and also to move along the map in the far right by facing the border.\n\nHow can we be greedy if we have two equally good actions?\n\nIf we have two equally good actions and apply a greedy policy with , this will always select the first occurrence of highest action. Instead we want to break ties randomly between top scoring actions (still considered greedy).\n\nDo we need to run policy evaluation or policy improvement to convergence each time?\n\nThere is some flexibility in the general form of policy iteration (which cycles between evaluation and improvement). Both of these phases can be evaluated partially or fully to convergence, or can involved synchronous or asynchronous updates to the states $v(s), s \\in S$. Generalised policy iteration, just involves two interacting processes revolving around approximate policy and approximate value function.\n\nThe greedy approach to finding the policy seems redundant if all it’s doing is taking greedy actions. Value Iteration is a special case of generalised Policy Iteration where we run a single step updating the value function, and then greedify immediately to say that the new $v(s)$ is the value of taking the greedy action. Since there is no reference to a specific policy, we call this simply value iteration. This will still converge to $v*$.\n\nWhich “Bellman equation” should I use? Why is “Q-learning” more of a thing than V-learning?\n\nThere are two Bellman equations in RL; one for the Q-function (state-action value), and one for the V-function (state value). These are closely related; the Q-function additional incorporates taking an action in a state\n\nwhereas the v-function would average across actions in the (stochastic) policy: \\begin{equation} v_{\\pi}(s) = \\sum_a \\pi(a \\mid s) \\sum_{s’, r}p(s’, r \\mid s,a)[r + \\gamma G_{t+1}(s’)] \\end{equation}\n\nFunction approximation methods which allow us to estimate $q$ already incorporate the choice of action. As compared to $v$, we do not have to extract the optimal action and explicitly compute the $argmax_a \\sum_{s’, r} p(s’ r \\mid s, a)$, which requires knowing a model of the environment (transition probability p). This makes conversion to policy more straightforward and therefore Q-learning (function approximation) is known as a model-free RL method.\n\nReinforcement Learning, an Introduction. Chapter 3, 4. Sutton and Barto\n\n Fundamentals of RL; University of Alberta"
    },
    {
        "link": "https://reddit.com/r/reinforcementlearning/comments/owbglx/what_is_the_benefit_of_using_bellman_equations_in",
        "document": "Reinforcement learning is a subfield of AI/statistics focused on exploring/understanding complicated environments and learning how to optimally acquire rewards. Examples are AlphaGo, clinical trials & A/B tests, and Atari game playing."
    },
    {
        "link": "https://shivang-ahd.medium.com/bellman-equation-and-value-iteration-in-dynamic-programming-609219f5d3e1",
        "document": "\n• Let us get our hands dirty and make few calculations\n\n- Calculating V(s) using Bellman Equation\n• Snippet of Original Code used to do value iteration\n\n- Let us break this code to understand Value iteration part\n\n 1. Iterating through all 4 actions for every state to calculate V(s)\n\n 2. Updating the values of States and optimal action taken in each state\n\n 3. Condition for convergence\n\nThe bellman equation is given as:\n• V is the value function, which represents the maximum expected return (or value) when starting in state s.\n• max indicates that we’re looking for the maximum value among all possible actions a.\n• a represents the action taken in state s.\n• The goal is to find the best action that maximizes the expected return.\n• R is the reward function, which gives the immediate reward obtained when taking action a in state s.\n• R(s, a) represents the reward for taking action a in state s.\n• γ (gamma) is the discount factor, which determines the importance of future rewards.\n• γ ∈ [0, 1] — a value close to 0 means future rewards are less important, while a value close to 1 means they’re more important.\n• P(s’|s, a) is the transition probability, representing the probability of moving from state s to state s’ when taking action a.\n• ∑ indicates a sum over all possible next states s’.\n• V(s’) is the value function for the next state s’.\n• This term represents the expected value of the next state, weighted by the transition probability.\n• Imagine you’re in state s, and you take action a. There are multiple possible next states s’, each with a probability P(s’|s, a). For each next state, you calculate the value function V(s’).\n• Sigma adds up these values, weighted by their probabilities, to give you the expected value of the next state.\n\nTo find the maximum expected return (V(s)) when starting in state s, consider all possible actions a. For each action, calculate the immediate reward (R(s, a)) and the expected value of the next state (∑P(s’|s, a)V(s’)). Take the maximum of these values over all actions a, and that’s the value of V(s).\n\nIn a Markov Decision Process (MDP), when you take an action a in state s, the next state is not always deterministic. Instead, there’s a probability distribution over possible next states.\n\nThink of it like this:\n\n- You’re in state s and take action a.\n\n- The environment responds with a probability distribution over next states, e.g., P(s1'|s, a) = 0.4, P(s2'|s, a) = 0.3, P(s3'|s, a) = 0.3.\n\n- The actual next state is randomly selected according to this probability distribution.\n\nThe sigma (Σ) in the Bellman equation:\n\nimplies a sum over all possible next states s’.\n\nΣ is the summation operator, which adds up the values of the expression P(s’|s, a)V(s’) for all possible s’.\n\nIn this context, the summation is over the probability distribution of next states s’, given the current state s and action a.\n\nThe expression calculates the expected value of the next state, weighted by the transition probabilities.\n\nThink of it as:\n\n- For each possible next state s’, calculate the product of:\n\n — The transition probability P(s’|s, a) (how likely we are to end up in s’)\n\n — The value of the next state V(s’) (the expected return from s’)\n\n- Add up these products for all possible s’\n\nThe result is the expected value of the next state, considering all possible transitions and their probabilities.\n\nIn mathematical terms, the summation is:\n\nIn contrast, a deterministic transition model would mean that taking action a in state s always leads to a single, specific next state.\n\nIn Deterministic transition model, there is no need for sigma, as when we take actin a from state s, we end up in state s’ with 100% probability and hence the bellman equation in that case is:\n\nIn Deterministic transition Model P(s’|s,a)=1,because there is no probability distribution. When we take action a from State s to move to state s’, we end up in state s’ and we don’t move to any other state.\n\nHence, for Deterministic Transition Model, the equation for V(s) becomes\n\nAs you can see here, sigma is not there, as the possible state is only s’.\n\nValue Iteration is an iterative algorithm that computes the optimal value function, V*(s), by iteratively improving an estimate of the value function, V(s).\n\nThe algorithm starts with an arbitrary initial value function and repeatedly updates it using the following steps:\n\n1. Initialization: Initialize the value function, V(s), arbitrarily for all states s.\n\n2. Evaluation: For each state s, compute the action-value function, Q(s, a), using the current value function V(s):\n\n — Q(s, a) = R(s, a, s’) + γ * P(s’ | s, a) * V(s’)\n\n3. Improvement: Update the value function, V(s), using the maximum action-value function, Q(s, a):\n\n — V(s) = max_a Q(s, a)\n\n4. Repeat: Repeat steps 2–3 until convergence or a stopping criterion is reached.\n\nValue Iteration converges to the optimal value function, V*(s), under certain conditions:\n\n1. Finite State Space: The state space is finite.\n\n2. Finite Action Space: The action space is finite.\n\n3. Discount Factor: The discount factor, γ, is less than 1.\n\nOnce the optimal value function, V*(s), is computed, the optimal policy, π*(s), can be derived:\n\n1. For each state s, select the action a that maximizes the action-value function, Q(s, a):\n\n — π*(s) = argmax_a Q(s, a)\n\nUsing Value Iteration, we can compute the optimal value function, V*(s), and policy, π*(s).\n\nValue Iteration has several advantages:\n\n1. Guaranteed Convergence: Value Iteration is guaranteed to converge to the optimal value function under certain conditions.\n\n2. Easy to Implement: Value Iteration is a simple algorithm to implement.\n\n3. Flexible: Value Iteration can be used with any reward function and transition model.\n\nValue Iteration also has some disadvantages:\n\n1. Computational Complexity: Value Iteration can be computationally expensive for large state and action spaces.\n\n2. Slow Convergence: Value Iteration can converge slowly for certain problems.\n\nLet us get our hands dirty and make few calculations\n\nLet us assume a 4X4 grid world with states from S1,….S16. We need to move from State S1 to S16. We need to find the shortest path, given that there are many road blocks where we cannot proceed and given that few paths will be longer than than others.\n\nAs per the procedure stated above, we need to initialize the value of all the states with zero.\n\nThen we have to start the iteration process of finding the value of each state. Below is the value of states after certain number(assume 10) of iterations are performed.\n\nNow, let us try to understand how the value of each state is calculated.\n\nTo understand this, we pick any one state, here we pick S6 as the state of interest, whose value we want to calculate.\n\nTo calculate the value of State S6, we need to calculate the value of actions one can take from State S6.\n\nHere, we can take 4 actions UP, DOWN, RIGHT, LEFT. Taking these 4 actions will lead us to 4 different states S2,S10,S7 and S5 respectively\n\nEach of these states have different value, and hence when we calculate value of State S6, while taking each of these actions, we will get 4 different values.\n\nFor the sake of simplicity, let us assume that the reward R(s,a) we get for all the 4 actions is same and it is -1.\n\nAs shown in the first diagram of the blog, the snapshot of which is above, when we assume the deterministic transition model, our Bellman Equation changes slightly. We remove sigma, as we don’t need to add other states. When we take any one action, we end up in that particular state and hence we need not to use sigma.\n\nAlso, important thing to note here is that P(s’|s,a) becomes 1, since we have assumed Deterministic Transition model.\n\nWe will calculate the value of state S6 for iteration 11.\n\nOur new Bellman Equation, on the assumption of Deterministic Transition Model is:\n\nV(s) for Action UP\n\nhence, V(s) for taking action UP is -8.92\n\nV(s) for Action DOWN\n\nhence, V(s) for taking action Down is -3.97\n\nV(s) for Action RIGHT\n\nhence, V(s) for taking action RIGHT is -5.95\n\nhence, V(s) for taking action LEFT is -7.93\n\nSnippet of Original Code used to do value iteration\n\nLet us break this code to understand Value iteration part\n• Iterating through all 4 actions for every state to calculate V(s)\n\nHere, we have created two matrices named state_values, policy_probs of the dimension row X col.\n\nWe initialized the value of these matrices as zeros.\n\nThen we iterate through each cell of state_values matrix, as seen in the for loop.\n\nIn each cell 4 actions can be taken, UP, DOWN , RIGHT, LEFT which are represented by 0,1,2,3. Hence, we have created a for-loop for range(4), which will give us four values viz: 0,1,2,3.\n\nSo for each of the iteration of four actions we have two important values, viz: action and current state. From that we get reward for a particular action taken & next state reached for that action.\n\nWe get all the ingredients to calculate V(s) and we do calculate it. ‘qsa’ is nothing but calculation of V(s). Notice, here since we have assumed Deterministic Transition Model, the probability value is 1 and hence not shown in the calculation.\n\nmax_qsa is the variable which is initialized at the beginning of ‘while loop’ to minus infinity. max_qsa is then used to compare all 4 qsa values and store the max of all 4 qsa values and action_probs is used to capture the particular action which got us the max_qsa.\n\n2. Updating the values of States and optimal action taken in each state\n\nAs you can see, after every for-loop of iterating through all 4 actions and identifying max_qsa (max V(s) out of all 4 V(s)) and aciton a taken to get max_qsa, we update the matrices with these values.\n\nFor convergence condition we primarily make use of two variables viz: theta and delta.\n\ntheta we have initialized to the value of 1e-6 i.e 10^-6.\n\ndelta is a variable which is initialized to minus infinity.\n\nAt the start of while loop, before we start iterating through all the cells/states of state_values matrix, we set delta=0. delta is update after each cell of the state_values matrix is visited and max_qsa i.e max V(s) value is identified for that state.\n\nAs you can see above we take max of two values:-\n\nHere max_qsa is the new value of a particular state and old_value is the old value of that state. So, basically we update value of delta to the difference of the values of new V(s) and old V(s) from previous iteration.\n\nAfter many iterations, there will come a point, where this difference will be less than 1e-6, and at that point we would have made the convergence. This is because new values of the state will not be much different from old state values of previous iteration.\n\nAt this point the while loop condition of delta>theta will break and we will come out of while loop.\n\nThis is how the convergence condition is checked.\n• It was a pleasure exploring the Bellman Equation and Value Iteration with you!\n• We’ve covered the fundamentals, delved into the details, and even rewritten some code for better clarity.\n• If any points still seem fuzzy, please don’t hesitate to ask.\n• Remember, reinforcement learning is a journey, and mastering these concepts takes time and practice.\n• Keep pushing forward, and you’ll become a pro at solving complex problems using Bellman’s Equation and Value Iteration.\n• Stay curious, keep learning, and remember: every small step counts!\n\nFeel free to ask me any questions or explore other topics anytime!"
    }
]