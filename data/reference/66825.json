[
    {
        "link": "https://stackoverflow.com/questions/14760920/passing-colors-through-a-pixel-shader-in-hlsl",
        "document": "I have have a pixel shader that should simply pass the input color through, but instead I am getting a constant result. I think my syntax might be the problem. Here is the shader:\n\nFor testing, I have the vertex shader that precedes this in the pipleline passing a COLOR parameter of 0.5, 0.5, 0.5. Stepping through the pixel shader in VisualStudio, input.color has the correct values, and these are being assinged to output.color correctly. However when rendered, the vertices that use this shader are all black.\n\nHere is the vertex shader element description:\n\nI'm not sure if it's important that the vertex shader takes colors as RGB outputs the same, but the pixel shader outputs RGBA. The alpha layer is working correctly at least.\n\nIf I comment out that first assignment, the one using input.color, and uncomment the other assignment, with the explicit values, then the rendered pixels are gray (as expected).\n\nAny ideas on what I'm doing wrong here?\n\nI'm using shader model 4 level 9_1, with optimizations disabled and debug info enabled."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-writing-shaders-9",
        "document": "When in operation, a programmable vertex shader replaces the vertex processing done by the Microsoft Direct3D graphics pipeline. While using a vertex shader, state information regarding transformation and lighting operations is ignored by the fixed function pipeline. When the vertex shader is disabled and fixed function processing is returned, all current state settings apply.\n\nTessellation of high-order primitives should be done before the vertex shader executes. Implementations that perform surface tessellation after the shader processing must do so in a way that is not apparent to the application and shader code.\n\nAs a minimum, a vertex shader must output vertex position in homogeneous clip space. Optionally, the vertex shader can output texture coordinates, vertex color, vertex lighting, fog factors, and so on.\n\nPixel processing is performed by pixel shaders on individual pixels. Pixel shaders work in concert with vertex shaders; the output of a vertex shader provides the inputs for a pixel shader. Other pixel operations (fog blending, stencil operations, and render-target blending) occur after execution of the shader.\n\nA pixel shader completely replaces the pixel-blending functionality specified by the multi-texture blender including operations previously defined by the texture stage states. Texture sampling and filtering operations which were controlled by the standard texture stage states for minification, magnification, mip filtering, and the wrap addressing modes, can be initialized in shaders. The application is free to change these states without requiring the regeneration of the currently bound shader. Setting state can be made even easier if your shaders are designed within an effect.\n\nFor pixel shader versions ps_1_1 - ps_2_0, diffuse and specular colors are saturated (clamped) in the range 0 to 1 before use by the shader.\n\nColor values input to the pixel shader are assumed to be perspective correct, but this is not guaranteed (for all hardware). Colors sampled from texture coordinates are iterated in a perspective correct manner, and are clamped to the 0 to 1 range during iteration.\n\nFor pixel shader versions ps_1_1 - ps_1_4, the result emitted by the pixel shader is the contents of register r0. Whatever it contains when the shader completes processing is sent to the fog stage and render-target blender.\n\nFor pixel shader versions ps_2_0 and above, output color is emitted from oC0 - oC4.\n\nThe simplest variable declaration includes a type and a variable name, such as this floating-point declaration:\n\nYou can initialize a variable in the same statement.\n\nAn array of variables can be declared,\n\nor declared and initialized in the same statement.\n\nHere are a few declarations that demonstrate many of the characteristics of high-level shader language (HLSL) variables:\n\nData declarations can use any valid type including:\n\nA shader can have top-level variables, arguments, and functions.\n\nTop-level variables are declared outside of all functions. Top-level arguments are parameters to a top-level function. A top-level function is any function called by the application (as opposed to a function that is called by another function).\n\nVertex and pixel shaders accept two kinds of input data: varying and uniform. The varying input is the data that is unique to each execution of the shader. For a vertex shader, the varying data (for example: position, normal, etc.) comes from the vertex streams. The uniform data (for example: material color, world transform, etc.) is constant for multiple executions of a shader. For those familiar with the assembly shader models, uniform data is specified by constant registers and varying data by the v and t registers.\n\nUniform data can be specified by two methods. The most common method is to declare global variables and use them within a shader. Any use of global variables within a shader will result in adding that variable to the list of uniform variables required by that shader. The second method is to mark an input parameter of the top-level shader function as uniform. This marking specifies that the given variable should be added to the list of uniform variables.\n\nUniform variables used by a shader are communicated back to the application via the constant table. The constant table is the name for the symbol table that defines how the uniform variables used by a shader fit into the constant registers. The uniform function parameters appear in the constant table prepended with a dollar sign ($), unlike the global variables. The dollar sign is required to avoid name collisions between local uniform inputs and global variables of the same name.\n\nThe constant table contains the constant register locations of all uniform variables used by the shader. The table also includes the type information and the default value, if specified.\n\nVarying input parameters (of a top-level shader function) must be marked either with a semantic or uniform keyword indicating the value is constant for the execution of the shader. If a top-level shader input is not marked with a semantic or uniform keyword, then the shader will fail to compile.\n\nThe input semantic is a name used to link the given input to an output of the previous part of the graphics pipeline. For example, the input semantic POSITION0 is used by the vertex shaders to specify where the position data from the vertex buffer should be linked.\n\nPixel and vertex shaders have different sets of input semantics due to the different parts of the graphics pipeline that feed into each shader unit. Vertex shader input semantics describe the per-vertex information (for example: position, normal, texture coordinates, color, tangent, binormal, etc.) to be loaded from a vertex buffer into a form that can be consumed by the vertex shader. The input semantics directly map to the vertex declaration usage and the usage index.\n\nPixel shader input semantics describe the information that is provided per pixel by the rasterization unit. The data is generated by interpolating between outputs of the vertex shader for each vertex of the current primitive. The basic pixel shader input semantics link the output color and texture coordinate information to input parameters.\n\nInput semantics can be assigned to shader input by two methods:\n• Appending a colon and the semantic name to the parameter declaration.\n• Defining an input structure with input semantics assigned to each structure member.\n\nVertex and pixel shaders provide output data to the subsequent graphics pipeline stage. Output semantics are used to specify how data generated by the shader should be linked to the inputs of the next stage. For example, the output semantics for a vertex shader are used to link the outputs of the interpolators in the rasterizer to generate the input data for the pixel shader. The pixel shader outputs are the values provided to the alpha blending unit for each of the render targets or the depth value written to the depth buffer.\n\nVertex shader output semantics are used to link the shader both to the pixel shader and to the rasterizer stage. A vertex shader that is consumed by the rasterizer and not exposed to the pixel shader must generate position data as a minimum. Vertex shaders that generate texture coordinate and color data provide that data to a pixel shader after interpolation is done.\n\nPixel shader output semantics bind the output colors of a pixel shader with the correct render target. The pixel shader output color is linked to the alpha blend stage, which determines how the destination render targets are modified. The pixel shader depth output can be used to change the destination depth values at the current raster location. The depth output and multiple render targets are only supported with some shader models.\n\nThe syntax for output semantics is identical to the syntax for specifying input semantics. The semantics can be either specified directly on parameters declared as \"out\" parameters or assigned during the definition of a structure that either returned as an \"out\" parameter or the return value of a function.\n\nSemantics identify where data comes from. Semantics are optional identifiers that identify shader inputs and outputs. Semantics appear in one of three places:\n• After an argument in a function's input argument list.\n\nThis example uses a structure to provide one or more vertex shader inputs, and another structure to provide one or more vertex shader outputs. Each of the structure members uses a semantic.\n\nThe input structure identifies the data from the vertex buffer that will provide the shader inputs. This shader maps the data from the position, normal, and blendweight elements of the vertex buffer into vertex shader registers. The input data type does not have to exactly match the vertex declaration data type. If it doesn't exactly match, the vertex data will automatically be converted into the HLSL's data type when it is written into the shader registers. For instance, if the normal data were defined to be of type UINT by the application, it would be converted into a float3 when read by the shader.\n\nIf the data in the vertex stream contains fewer components than the corresponding shader data type, the missing components will be initialized to 0 (except for w, which is initialized to 1).\n\nInput semantics are similar to the values in the D3DDECLUSAGE.\n\nThe output structure identifies the vertex shader output parameters of position and color. These outputs will be used by the pipeline for triangle rasterization (in primitive processing). The output marked as position data denotes the position of a vertex in homogeneous space. As a minimum, a vertex shader must generate position data. The screen space position is computed after the vertex shader completes by dividing the (x, y, z) coordinate by w. In screen space, -1 and 1 are the minimum and maximum x and y values of the boundaries of the viewport, while z is used for z-buffer testing.\n\nOutput semantics are also similar to the values in D3DDECLUSAGE. In general, an output structure for a vertex shader can also be used as the input structure for a pixel shader, provided the pixel shader does not read from any variable marked with the position, point size, or fog semantics. These semantics are associated with per-vertex scalar values that are not used by a pixel shader. If these values are needed for the pixel shader, they can be copied into another output variable that uses a pixel shader semantic.\n\nGlobal variables are assigned to registers automatically by the compiler. Global variables are also called uniform parameters because the contents of the variable is the same for all pixels processed each time the shader is called. The registers are contained in the constant table, which can be read using the ID3DXConstantTable interface.\n\nInput semantics for pixel shaders map values into specific hardware registers for transport between vertex shaders and pixel shaders. Each register type has specific properties. Because there are currently only two semantics for color and texture coordinates, it is common for most data to be marked as a texture coordinate even when it is not.\n\nNotice that the vertex shader output structure used an input with position data, which is not used by the pixel shader. HLSL allows valid output data of a vertex shader that is not valid input data for a pixel shader, provided that it is not referenced in the pixel shader.\n\nInput arguments can also be arrays. Semantics are automatically incremented by the compiler for each element of the array. For instance, consider the following explicit declaration:\n\nThe explicit declaration given above is equivalent to the following declaration that will have semantics automatically incremented by the compiler:\n\nJust like input semantics, output semantics identify data usage for pixel shader output data. Many pixel shaders write to only one output color. Pixel shaders can also write out a depth value into one or more multiple render targets at the same time (up to four). Like vertex shaders, pixel shaders use a structure to return more than one output. This shader writes 0 to the color components, as well as to the depth component.\n\nPixel shader output colors must be of type float4. When writing multiple colors, all output colors must be used contiguously. In other words, COLOR1 cannot be an output unless COLOR0 has already been written. Pixel shader depth output must be of type float1.\n\nA sampler contains sampler state. Sampler state specifies the texture to be sampled, and controls the filtering that is done during sampling. Three things are required to sample a texture:\n\nSamplers can be initialized with textures and sampler state as shown here:\n\nHere's an example of the code to sample a 2D texture:\n\nThe texture is declared with a texture variable tex0.\n\nIn this example, a sampler variable named s_2D is declared. The sampler contains the sampler state inside of curly braces. This includes the texture that will be sampled and, optionally, the filter state (that is, wrap modes, filter modes, etc.). If the sampler state is omitted, a default sampler state is applied specifying linear filtering and a wrap mode for the texture coordinates. The sampler function takes a two-component floating-point texture coordinate, and returns a two-component color. This is represented with the float2 return type and represents data in the red and green components.\n\nFour types of samplers are defined (see Keywords) and texture lookups are performed by the intrinsic functions: tex1D(s, t) (DirectX HLSL), tex2D(s, t) (DirectX HLSL), tex3D(s, t) (DirectX HLSL), texCUBE(s, t) (DirectX HLSL). Here is an example of 3D sampling:\n\nThis sampler declaration uses default sampler state for the filter settings and address mode.\n\nHere is the corresponding cube sampling example:\n\nAnd finally, here is the 1D sampling example:\n\nBecause the runtime does not support 1D textures, the compiler will use a 2D texture with the knowledge that the y-coordinate is unimportant. Since tex1D(s, t) (DirectX HLSL) is implemented as a 2D texture lookup, the compiler is free to choose the y-component in an efficient manner. In some rare scenarios, the compiler cannot choose an efficient y-component, in which case it will issue a warning.\n\nThis particular example is inefficient because the compiler must move the input coordinate into another register (because a 1D lookup is implemented as a 2D lookup and the texture coordinate is declared as a float1). If the code is rewritten using a float2 input instead of a float1, the compiler can use the input texture coordinate because it knows that y is initialized to something.\n\nAll texture lookups can be appended with \"bias\" or \"proj\" (that is, tex2Dbias (DirectX HLSL), texCUBEproj (DirectX HLSL)). With the \"proj\" suffix, the texture coordinate is divided by the w-component. With \"bias,\" the mip level is shifted by the w-component. Thus, all texture lookups with a suffix always take a float4 input. tex1D(s, t) (DirectX HLSL) and tex2D(s, t) (DirectX HLSL) ignore the yz- and z-components respectively.\n\nSamplers may also be used in array, although no back end currently supports dynamic array access of samplers. Therefore, the following is valid because it can be resolved at compile time:\n\nHowever, this example is not valid.\n\nDynamic access of samplers is primarily useful for writing programs with literal loops. The following code illustrates sampler array accessing:\n\nFunctions break large tasks into smaller ones. Small tasks are easier to debug and can be reused, once proven. Functions can be used to hide details of other functions, which makes a program composed of functions easier to follow.\n\nHLSL functions are similar to C functions in several ways: They both contain a definition and a function body and they both declare return types and argument lists. Like C functions, HLSL validation does type checking on the arguments, argument types, and the return value during shader compilation.\n\nUnlike C functions, HLSL entry point functions use semantics to bind function arguments to shader inputs and outputs (HLSL functions called internally ignore semantics). This makes it easier to bind buffer data to a shader, and bind shader outputs to shader inputs.\n\nA function contains a declaration and a body, and the declaration must precede the body.\n\nThe function declaration includes everything in front of the curly braces:\n\nThe return type can be any of the HLSL basic data types such as a float4:\n\nThe return type can be a structure that has already been defined:\n\nIf the function does not return a value, void can be used as the return type.\n\nThe return type always appears first in a function declaration.\n\nAn argument list declares the input arguments to a function. It may also declare values that will be returned. Some arguments are both input and output arguments. Here is an example of a shader that takes four input arguments.\n\nThis function returns a final color, that is a blend of a texture sample and the light color. The function takes four inputs. Two inputs have semantics: LightDir has the TEXCOORD1 semantic, and texcrd has the TEXCOORD0 semantic. The semantics mean that the data for these variables will come from the vertex buffer. Even though the LightDir variable has a TEXCOORD1 semantic, the parameter is probably not a texture coordinate. The TEXCOORDn semantic type is often used to supply a semantic for a type that is not predefined (there is no vertex shader input semantic for a light direction).\n\nThe other two inputs LightColor and samp are labeled with the uniform keyword. These are uniform constants that will not change between draw calls. The values for these parameters come from shader global variables.\n\nArguments can be labeled as inputs with the in keyword, and output arguments with the out keyword. Arguments cannot be passed by reference; however, an argument can be both an input and an output if it is declared with the inout keyword. Arguments passed to a function that are marked with the inout keyword are considered copies of the original until the function returns, and they are copied back. Here's an example using inout:\n\nThis function increments the values in A and B and returns them.\n\nThe function body is all of the code after the function declaration.\n\nThe body consists of statements which are surrounded by curly braces. The function body implements all of the functionality using variables, literals, expressions, and statements.\n\nThe shader body does two things: it performs a matrix multiply and returns a float4 result. The matrix multiply is accomplished with the mul (DirectX HLSL) function, which performs a 4x4 matrix multiply. mul (DirectX HLSL) is called an intrinsic function because it is already built into the HLSL library of functions. Intrinsic functions will be covered in more detail in the next section.\n\nThe matrix multiply combines an input vector Pos and a composite matrix WorldViewProj. The result is position data transformed into screen space. This is the minimum vertex shader processing we can do. If we were using the fixed function pipeline instead of a vertex shader, the vertex data could be drawn after doing this transform.\n\nThe last statement in a function body is a return statement. Just like C, this statement returns control from the function to the statement that called the function.\n\nFunction return types can be any of the simple data types defined in HLSL, including bool, int half, float, and double. Return types can be one of the complex data types such as vectors and matrices. HLSL types that refer to objects cannot be used as return types. This includes pixelshader, vertexshader, texture, and sampler.\n\nHere is an example of a function that uses a structure for a return type.\n\nThe float4 return type has been replaced with the structure VS_OUTPUT, which now contains a single float4 member.\n\nA return statement signals the end of a function. This is the simplest return statement. It returns control from the function to the calling program. It returns no value.\n\nA return statement can return one or more values. This example returns a literal value:\n\nThis example returns the scalar result of an expression:\n\nThis example returns a float4 constructed from a local variable and a literal:\n\nThis example returns a float4 that is constructed from the result returned from an intrinsic function, and a few literal values:\n\nThis example returns a structure that contains one or more members:\n\nMost current vertex and pixel shader hardware is designed to run a shader line by line, executing each instruction once. HLSL supports flow control, which includes static branching, predicated instructions, static looping, dynamic branching, and dynamic looping.\n\nPreviously, using an if statement resulted in assembly-language shader code that implements both the if side and the else side of the code flow. Here is an example of the in HLSL code that was compiled for vs_1_1:\n\nAnd here is the resulting assembly code:\n\nSome hardware allows for either static or dynamic looping, but most require linear execution. On the models that do not support looping, all loops must be unrolled. An example is the DepthOfField Sample sample that uses unrolled loops even for ps_1_1 shaders.\n\nHLSL now includes support for each of these types of flow control:\n\nStatic branching allows blocks of shader code to be switched on or off based on a Boolean shader constant. This is a convenient method for enabling or disabling code paths based on the type of object currently being rendered. Between draw calls, you can decide which features you want to support with the current shader and then set the Boolean flags required to get that behavior. Any statements that are disabled by a Boolean constant are skipped during shader execution.\n\nThe most familiar branching support is dynamic branching. With dynamic branching, the comparison condition resides in a variable, which means that the comparison is done for each vertex or each pixel at run time (as opposed to the comparison occurring at compile time, or between two draw calls). The performance hit is the cost of the branch plus the cost of the instructions on the side of the branch taken. Dynamic branching is implemented in shader model 3 or higher. Optimizing shaders that work with these models is similar to optimizing code that runs on a CPU."
    },
    {
        "link": "https://stackoverflow.com/questions/12894920/hlsl-pixel-shader-global-variables",
        "document": "I am new to HLSL and shaders. I can't seem to replace the color I retrieve. It's for use in 2D text, i.e. subtitles. The problem is if I set osd_color outside main() it doesn't display anything. I am using Shazzam Shader Editor 1.4 to quickly see the effect, however same thing happens in the program..\n\nHope you can help.\n\nWhile I'm at it, if I'd want to add a shadow/outline and returns its color as well, how would I do that? Let's say every variable works. And osd_color is white and a float4 outline is black. I've tried:\n\nWith this all I get is a white color (osd_color).."
    },
    {
        "link": "https://reddit.com/r/monogame/comments/98g5ir/hlsl_how_to_pass_a_color_to_effectparameters",
        "document": "I'm trying to pass in a color object's values to a shader variable:\n\nfloat4 bgColor;\n\nBoth of these lines compile but crash while applying the shader:\n\nWhat is the correct way to pass a color to a shader?\n\nEDIT:\n\nNevermind. For some reason the shader is not taking ANY additional parameters, regardless of data type, variable name, or placement in the file. I'll investigate that issue instead."
    },
    {
        "link": "https://gamedev.net/forums/topic/561291-hlsl-pulsing-colour-any-advice-on-the-following/4599077",
        "document": "where I would modify the .r property using a sinusoidal value. I'm just not too certain as to how it should be modified. Has anyone got any advice or know of a HLSL tutorials on the subject? Thank you. Hi folks, If I want to have an object whose colour pulses when highlighted how would I go about implementing it? For example if I want the object to pulse from its original texture colour to red how would I do it. So far I figured that I would have the original texture colour:where I would modify the .r property using a sinusoidal value. I'm just not too certain as to how it should be modified. Has anyone got any advice or know of a HLSL tutorials on the subject? Thank you.\n\nInside the shader you can have your rgba values outside the range [0.0,1.0]. However, when leaving the shader the values are clamped to that range which is then finally mapped to the rgba range [0,255] for an 8 bit per color channel render target.\n\n\n\nIf you want to rescue values greater than 1.0 to the output render target you have to use a high dynamic range (HDR) render target. That means your render target has to be a floating point format. \n\n\n\nAnd that's the point. To get the pulsing right you only need to have a glow effect that increases and decreases over time. And to have a decent glow effect you have to work with HDR rendering to make it look good. Otherwise your colors will get wrong results when all color channels sum up and get clamped to RGBA(1,1,1,1) which is just white. \n\n\n\nGoogle for glow effect and you will find what you are looking for or just look at the \"HDR pipeline\" sample of the DirectX SDK. That has code with quite good comment on how glow effect works. ------------------------------------I always enjoy being rated up by you ...\n\n\n\n \n\nJust for clarification: amount is just in the color range by coincident, since you (normally) use a lerp amount in that range. The color channels stay in the range [0, 1] since you feed it with values in that range, too (the color from your texture and red).\n\n\n\nYou can (ab)use lerp with an amount < 0 or > 1, but then it would be rather a extrapolation than a interpolation (lexp ? [smile]). You're welcome.Just for clarification: amount is just in the color range by coincident, since you (normally) use a lerp amount in that range. The color channels stay in the range [0, 1] since you feed it with values in that range, too (the color from your texture and red).You can (ab)use lerp with an amount < 0 or > 1, but then it would be rather a extrapolation than a interpolation (lexp ? [smile]).\n\nI don't know if you've already settled on your solution yet, but you might consider trying this this in the HSV color space rather than RGB. \n\n\n\nI think if you do RGB interpolation you're going to end up with some weird colors during the transition. If you start with a bright blue-green color of (0,1,1) and lerp it to bright-red (1,0,0), then you're going to end up with a dull gray color (0.5,0.5,0.5) during transition, which I think might look pretty ugly.\n\n\n\nIf you use HSV you can just dial the hue towards zero degrees without any decrease in intensity or saturation. Interpolate the hue towards zero, and the intensity and saturation towards 1, and that should give a nicer looking result.\n\n\n\nhttp://en.wikipedia.org/wiki/HSL_and_HSV"
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-to-sample",
        "document": "The texture's template type, which may be a single- or multi-component vector. The format is based on the texture's DXGI_FORMAT.\n\nThis function is supported in the following shader models.\n• TextureCubeArray is available in Shader Model 4.1 or higher.\n• Shader Model 4.1 is available in Direct3D 10.1 or higher.\n\nThis partial code example is based on the BasicHLSL11.fx file in the BasicHLSL11 Sample.\n\nTexture sampling uses the texel position to look up a texel value. An offset can be applied to the position before lookup. The sampler state contains the sampling and filtering options. This method can be invoked within a pixel shader, but it is not supported in a vertex shader or a geometry shader.\n\nUse an offset only at an integer miplevel; otherwise, you may get different results depending on hardware implementation or driver settings.\n\nTexture coordinates are floating-point values that reference texture data, which is also known as normalized texture space. Address wrapping modes are applied in this order (texture coordinates + offsets + wrap mode) to modify texture coordinates outside the [0...1] range.\n\nFor texture arrays, an additional value in the location parameter specifies an index into a texture array. This index is treated as a scaled float value (instead of the normalized space for standard texture coordinates). The conversion to an integer index is done in the following order (float + round-to-nearest-even integer + clamp to the array range).\n\nThe offset parameter modifies the texture coordinates, in texel space. Even though texture coordinates are normalized floating-point numbers, the offset applies an integer offset. Also note that the texture offsets need to be static.\n\nThe data format returned is determined by the texture format. For example, if the texture resource was defined with the DXGI_FORMAT_A8B8G8R8_UNORM_SRGB format, the sampling operation converts sampled texels from gamma 2.0 to 1.0, filter, and writes the result as a floating-point value in the range [0..1]."
    },
    {
        "link": "https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-sm2",
        "document": "This browser is no longer supported.\n\nUpgrade to Microsoft Edge to take advantage of the latest features, security updates, and technical support."
    },
    {
        "link": "https://microsoft.github.io/hlsl-specs/specs/hlsl.pdf",
        "document": ""
    },
    {
        "link": "https://developer.download.nvidia.com/shaderlibrary/webpages/hlsl_shaders.html",
        "document": "With a wide range of shaders including skin, natural effects, metals, post processing effects, and much more, the NVIDIA Shader Library exists to help developers easily find and integrate great shaders into their projects. All the shaders in this library are provided free of charge for use in derivative works, whether academic, commercial, or personal (Full License). If you would like to submit a shader, please read our Shader Submission Guidelines. Perceptually-correct soft shadows. For more information, please see the \"Percentage-Closer Soft Shadows\" talk from GDC 2005. The talk is available at heres (1 technique/s) Color space conversion -- takes the existing scene, and polarizes the colors along the color wheel -- colors that are close to the \"Guide Color\" become more like the guide, while colors closer to its complementary color in that direction. Parameters allow control of how tightly colors \"bunch up\" and permits the user to turn the effect so that desaturated colors are less affected. In the sample image, an orange tone was chosen -- skin tones are reddish and those have migrated towards the orange guide, while colors near blue (complement of orange) have become bluer. (1 technique/s) A simple defered-rendering example -- an initial pass renders color, surface normals, and view vectors into multiple render targets (textures). A second pass combines the data in these textures with lighting info to create a final shaded image. (1 technique/s) This postprocess uses the values rendered by the previous part of the scene, and re-interprets their values as a 2D displacement map, which it applies to another image. The standard way to use this would be to have all objects export their normals. (1 technique/s) A \"godrays\" effect done as a single pass. Make sure the input texture has alpha! It works best on a simple planar card, but go ahead and experiment with all sorts of geometry. The size of the glow is kept constant in screen space by adjustng the rays according to the partial derivates of UV in screenspace x and y -- that is, using ddx(UV) and ddy(UV) (1 technique/s) Glow/bloom post processing effect -- for \"bloom,\" only the brightest areas are affected. (1 technique/s) Based on a radal blur effect, then with the origin image re-composited. (1 technique/s) Persistence of blurry vision -- this effect uses \"ping pong\" render targets so that its state persists from frame to frame. Uses FP16 buffers (1 technique/s) This material shows and compares results from four popular and advanced schemes for emulating displaement mapping. They are: Relief Mapping, Parallax Mapping, Normal Mapping, and Relief Mapping with Shadows. Original File by Fabio Policarpo. (4 technique/s) \"Toksvig-factor\" anti-aliased bump mapping -- eliminate \"buzzy\" hilights along bump edges. Note use of 16-bit textures (g16r16) for precision with performance A description of the technique can be found at heres (2 technique/s) Create a transparent \"envelope\" around any existing model. While implemented as a \"post process\" effect, this effect is just a second pass on the geometry -- no render-to-texture is needed. Great cheap effect for glows (or deep-sea egg pods) (1 technique/s) Amazing inflating teapots! A plastic \"balloon\" surface (lit from either a point or directional source). Twiddle the \"inflate\" parameter to change the shape. (1 technique/s) Application of \"Image Enhancement by Unsharp Masking the Depth Buffer\" from Siggraph 2006. This version applies the depth enhancement to an existing scene. The scene can be rendered very simply -- in fact it looks great on \"flat\" render effects like \"FlatTexture\" but can work with any sort of rendering. The user should choose Near and Far depth values to cover the ranges of depth found in the scene. See the original paper at heres (1 technique/s) A simple defered-rendering example. Some channels are currently un-used, while the \"ViewSampler\" could be considered redundant (you COULD calculate it on the fly, if texture-bandwidth limited) (1 technique/s) Blast from the past (Cg 1 Maya sample shader). Analytic anti-aliasing against an arbitrary function -- in this case pulsing 3D sine waves. (1 technique/s) A simplified UV-space-diffusion effect for use on character skin. See the chapter in GPU Gems 3 for the full nine yards! This shader further-extends the techniques from \"scene_uvd_skin\" and \"scene_uv_diffusion\" by adding shadows. The light and shadow here are unwrapped into a UV-space texture, diffused in surface coordinates and re-applied to the geometry in 3D, mixed with yet more 3D lighting to give both the crisp \"immediacy\" of the skin surface along with the soft, subsurface-diffused tones of skin's natural translucence. This effect is easy to apply to existing models without requiring any new art assets. (1 technique/s) Simple shadow map example using HW shadow textures and render ports. Plastic-style shading with quadratic light falloff. Both shadowed full-scene shadow and unshadowed materials provided. (2 technique/s) An .FX Paint Program. Scene geometry is ignored. Draw with the left mouse button. To clear screen, just make the brush big and paint everything. Resizing the window will mess up your drawing. Brush strokes will change in size and opacity over time, set \"FadeTime\" to a high value for more even (though less expressive) strokes. (1 technique/s) This shader assumes the input model is a multi-segment unit square in XY with center at the origin. It distorts this unit square into a disk-like \"ring\" for use in sword battles etc. (1 technique/s) UV-space lighting diffusion, as pioneered by George Borshukov in the \"Matrix\" films. We also use a specal \"TexBlender\" value, as used in the NVIDIA \"Human Head\" demo, to control the mix of surface detail in tandem with textured subsurface scattering. Be sure that your object UV coordinates fit within the range 0-1 and have no repeats or overlaps. (1 technique/s) Phong-shaded, metal- and plastic-style surfaces with a mirror term. Plastic or dielectic surfaces have varying reflectivity according to the angle at which a surface is viewed -- this variation is often called \"fresnel\" reflectance. (2 technique/s) Clip low and high colors so that the resulting image is within a narrower range, e.g. for TV signals. Two Techniques are provided -- one clips the colors that go outside the specified range between Min and Max, while the other stretches or compresses the total color space to conform to the indicated range. (2 technique/s) A phong-shaded plastic-style surface lit from either a point or directional source. Textured, untextured, quadratic falloff or not (4 technique/s) A phong-shaded metallic surface lit from either a point or directional source. Textured, untextured, quadratic falloff or not (4 technique/s) This surface is DULL. A matte, lambert surface lit from either a point or directional source (You can attach either a point light OR a directional source to it). Four techniques are provided: permutations of textured/untextured and quadratic-falloff/constant lighting (4 technique/s) A phong-shaded metallic surface lit from a point source. Textured, untextured, quadratic falloff or not (4 technique/s) A phong-shaded plastic-style surface lit from a point source. Textured, untextured, quadratic falloff or not (4 technique/s) This .fx file uses 3d checker patterns to illustrate a number of important coordinate systems and shading vectors. #define USER_COLORS if you want to use parameters instead of the fixed macro colors. (8 technique/s) Simple edge lighting effect -- the color of the surface shifts when viewed on-edge. Optionally, the light color can also be effected -- the result of both is to create an illusion of the surface being covered by some sort of smooth barely-visible fuzz. (1 technique/s) Shading via multitexture. Two textures are interpolated over the surface, and their product results in the final specular BDRF. The initial textures supplied approximate a Cook-Torrance model using one set of possible parameters, but different textures can be used to emulate a wide variety of isotropic BRDF models. Try painting some of your own! (2 technique/s) Radial blur effect. This version allows you to specifiy an arbitrary number of samples. (1 technique/s) A simple combination of vertex and pixel shaders with velvety edge effects. Great for any model that needs that \"feeling of softness.\" (4 technique/s) Shading via multitexture. In this case, a texture is used to vary the underlying surface color based upon both the view angle and the angle at which ligh strikes the surface. The initial data driving this shading model came from Ford Motor Company, which directly measured \"Mystique\" and other lustrous car paints in their lab. The associated default texture is a hand-enhanced variant on the original Ford paint -- try painting your own! (2 technique/s) \"Grisaille\" is a style of drawing based on a style of sculture relief where the figures are \"flattened\" against a larger flat surface. This effect allows the user to tweak the \"flatness\" of the shading against the surface of the screen, as if the 3D scene were carved in (animating) relief. (2 technique/s) Uses a texture map as a high-speed lookup, so that complex anisotropic highlights can be displayed in real time. This new version of the effect generates its own anisotropy map, and is compatible with both FX Composer and EffectEdit. (1 technique/s) A sort of defered toon shading, which renders the light-dark transition as a soft, rounded line. This technique inspired by a method used in Studio Ghibli's \"Howl's Moving Castle.\" Typically, you should render objects using an un-shaded effect (that is, pure color and/or texture -- no lighting, or lighting with a strong ambient light) (1 technique/s) Render-to-Texture (RTT) *animated* glow example. Blur is done in two separable passes. (1 technique/s) A lambertian-like surface with light \"bleed-through\" -- appropriate for soft translucent materials like skin. The \"subColor\" represents the tinting acquired by light diffused below the surface. Set the \"Rolloff\" angle to the cosine of the angle used for additional lighting \"wraparound\" -- the diffuse effect propogates based on the angle of LightDirection versus SurfaceNormal. Versions are provided for shading in pixel or vertex shaders, textured or untextured. (4 technique/s) Just pass image through as monochrome -- multiply it by the relative intensities defined by the parameters \"Red,\" Green,\" and \"Blue.\" (1 technique/s) This is much like a Photoshop(tm) \"color mixer\" layer -- the intensities of each input red, green, and blue channels are graded against the colors indicated in the paramter list, then remixed. (1 technique/s) Same as \"scene_edgeDetect,\" but with the kernel values \"hand-cooked\" for efficiency (1 technique/s) Same as \"scene_edgeDetect,\" but with separated R, G, and B (1 technique/s) Animatable overlay titling - uses the ROP rather than render-to-texture (2 technique/s) Create a negative image of the underlying scene. Un-inverted alpha is preserved. (1 technique/s) Toony stuff. Two kinds of edge detection combined: normals and depth edge detection, resulting in clean predictable lines. (1 technique/s) A look typical of 1960s \"op-art\" optical printing. Add slight shifts to an image (1 technique/s) Makes a texture appear as if its \"beheath\" the surface. Useful for kinds of ceramic glazing (1 technique/s) Radial-dot B&W halftones applied to the underlying scene. The dots are pre-calculated and are fetched, according to the desired intensity, from a small volume texture. (1 technique/s) HLSL noise implementation for an animated vertex program. This is based on Ken Perlins original code: heres (1 technique/s) Similar to the classic \"RenderMan Companion\" wood shader, though for realtime performance we use a noise texture rather than calls to a numeric noise() function. This texture can be loaded from disk, or dynamically created on the spot by DirectX9 using the HLSL virtual machine and setting the macro \"PROCEDURAL_TEXTURE\". This new version is updated to support varying shininess for light and dark bands in the wood. (1 technique/s) A noisy halftoning pattern, based on noisy pre-calculated an indexed out of a 3D volume texture. (1 technique/s) A helpful tool for artists to optimize their texture sizes, so that textures created by artists wont be too small (looking bad) or too big (wasting artist time on texture detail that will never be seen). HOW TO USE UVDETECTIVE: (1) Look for regions where desired texture reso is dominant. (2) Set desired size in the \"Reso\" parameter. (3) In the \"TexRez\" techniques (6 technique/s) Render-to-Texture (RTT) Halo example. Blur is done in two separable passes. (1 technique/s) Render-to-Texture (RTT) glow example. Blurs is done in two separable passes. (2 technique/s) Simple tone mapping shader with exposure and gamma controls. This is an HDR example, so it requires a GPU capable of supporting the FP16 formats used in typical HDR formats such as OpenEXR. heres (1 technique/s) Place a gradient background into the scene. The colors are animatable and the interpolation of the gradient occurs in HSV color space, rather than RB, to provide more-consistent luminance changes (1 technique/s) 2-pass blurring directionally -- the two passes are completely separated, resulting in a \"star\" pattern (1 technique/s) 2D \"lighting\" effects -- with or without bump. This is somewhat similar to the Adobe Photoshop (tm) \"Lighting\" effect (2 technique/s) Reduce color space - each RGB channel will be reduced to no more than \"nColors\" tones (1 technique/s) Renders the scene to an offscreen texture then re-renders it to the screen, with pulsing, changing, on-screen texture coordinates. Clicking the mouse in the screen will also change the effect slightly. (1 technique/s) Combines two different methods of edge detection to make a more-robust line drawing. (4 technique/s) Shadow-map for all geometry thats overlaid on white and composited. This trick provides simple shadowing across multiple materials without editing their shaders. The downsides are incorrect shadowing of with transparency and objects that are lit by multiple lights (1 technique/s) Use the indicated map to distort the current scene. (1 technique/s) Convert the current scene to monochrome with \"sepia\" toning (1 technique/s) 3D meshcage effect, created by procedural texturing. Texture is pre-calculated by HLSL. Wires are aligned to world coordinates in this sample. $Date: 2008/06/25 $ (1 technique/s) 3D Checkerboard effect, created by procedural texturing. Texture is pre-calculated, using the HLSL virtual machine (VM). To see a purely analytic alternative that gives good anti-aliasing at all scales, see \"checker3d_math.fx\" As an \"extra,\" the check pattern is also applied to the specular value, to make the variation between materials stronger. (1 technique/s) 3D Checker showing anti-aliasing using ddx/ddy. This result is PURELY numeric, so slower than using texture-based AA. It is, however, able to anti-alias regardless of the view scale. For a fast texture-based version, see \"checker3d.fx\" (1 technique/s) A surface using \"blinn\" shading, which is especially appropriate for some metal finishes and sometimes even for materials like skin. (1 technique/s) A surface using \"blinn\" shading, which is especially appropriate for some true finishes and sometimes even for materials like skin. (1 technique/s) Brick pattern, with controls, using texture-based patterning. The lighting here is PURELY lambert and from a directional source, so it's done in the vertex shader. (1 technique/s) A wet-glossy surface, a little smoke & mirrors to make things ultra-shiny. Glossiness is controlled not only by the usual power function, but also by applying a set of gloss controls that cause a sharp falloff across a specified range. The falloff will occur in the highlight range [glossBot-glossTop] and the amount of falloff is specified by \"glossDrop.\" Setting \"glossDrop\" to 1.0 nullifies the effect. (2 technique/s) Align as a quad to screen according to the original tex coords -- as a sprite, best applied to simple quads with 0-1 tex coords. Then apply \"Numbers.dds\" as a stream of number textures to create an apparent numeric ocunter, driven by the vertex shader. (1 technique/s) This effect is intended to look like pen crosshatching -- it was inspired by the British Museums Durer exhibit of 2003. Some of Durer's most famous drawings were made in two colors of ink on medium-colored paper. The diffuse shape rendering was drawn in cross-hatches in a dark ink (2 technique/s) Creates the illusion that the surface is covered with a thin film of transparent material, as in oily water, thin shellacs, dirty layered ice, etc. (1 technique/s) Just draw a shadow map ONLY -- dont display or otherwise use it! This shader is intended for use with COLLADA-Cg TO USE: add this effect in FXComposer (1 technique/s) For use in backgrounds, or to cut \"holes\" through other objects. The surface of any model will be replaced by the colors of the background environment map. (1 technique/s) Just Show Me The Texture - No Lighting! Simplistic, but useful as an input render style for many scene-level render (and defered-render) techniques. (3 technique/s) Depth as color - the values Hither' and 'Yon' (Near and Far) must be set explicitly. The result will be an image where the depth will be coded as a blend between foreground and background colors. The \"rolloff\" parameter can be used to bias values toward the front or back. (1 technique/s) Day/Night Earth Shader -- Apply to a Sphere! This example uses two textures for the same surface and modulates between them for the light/dark lighting transitions, rather than ramping-off to black. (1 technique/s) Simple ocean shader with animated bump map and geometric waves Based partly on \"Effective Water Simulation From Physical Models\", GPU Gems (1 technique/s) A simple diffuse example that shows some texture positioning capabilities. (1 technique/s) A simple way to view 1D Textures (e.g., color ramps) as a 2D graph This is an imaging shader, all geometry is ignored (1 technique/s) Simple sinusoidal vertex animation on a phong-shaded plastic surface. The highlight is done in VERTEX shading -- not as a texture. Textured/Untextured versions are supplied Do not let your kids play with this shader, you will not get your computer back for a while. (2 technique/s) Make a 3D volume of color by intersecting a projected rgb texture by its own alpha, where the alpha is projected at right angles. To see the effect. move an object around in XYZ space and it will move in and out of the \"nebula\" colors. (1 technique/s) Simple color correction controls using a color matrix, as seen in the NVIDIA \"Toys\" demo. Controls are much like those on your TV: Brightness, Contrast, etc. See heres (1 technique/s) Ignore selection geometry, but use its orientation to rotate the colors of a texture mapped to a full-screen quad. In FX Composer, assign this effect to any node, and then spin the node to rotate the color matrix of the overall image. (1 technique/s) Use a noise texture to distort the render target, creating an appearance not unlike seeing through rippled glass (1 technique/s) Desaturate the color in the current scene (1 technique/s) Apply grayscale values to color curve texture -- the color ramp texture can be from read from a file or be created procedurally, just #define CURVE_FILE to use a file. Different techniques provide gradients against each of the R, G, B channels or against an overall grayscale. (4 technique/s) Key based on the RGB-space distance from a specified color (1 technique/s) Render-to-Texture (RTT) glow example - glow is overlaid on top of the current scene. The render target is a fixed size. Blur is done in two separable passes: a horizontal pass and then a vertical pass. (2 technique/s) Render-to-Texture (RTT) glow example. Blurs are done in two separable passes. (1 technique/s) Full-screen render-to-texture (RTT) example, adding a 2D dropshadow to the (possibly 3D) scene. Blur is done in two separable passes. (1 technique/s) An imaging effect that looked like viewing through ice-frosted glass. (1 technique/s) Very similar to \"post_frost\" but faster at the expense of some sampling choices. (1 technique/s) Texture-based remap of color space. Color ramp textures can be easily generated by Photoshop and the PS \"Curves\" command. The technique is described in \"GPU Gems\" in the section on color control. (1 technique/s) Slice an object along any arbitrary plane. Gooch shading -- but the SLICE portion is important here. Slicing is across the Z axis of an attachable (spot)light xform. (1 technique/s) Gooch shading w/glossy hilight in HLSL ps_2 pixel shader. Textured and non-textued versions are supplied. (2 technique/s) Emulates CMYK printing -- where the print passes are misaligned!!! Note that you might not see much effect on any channel except K' if you apply this effect to a gray object (1 technique/s) An image effect that's intended to look like the movie-film printing effect called \"bleach bypass,\" where a normal step of processing is skipped to cause unique color- and contrast effects. The \"Blend Opacity\" slider lets you dial-in the strength of this effect. (1 technique/s) Typical set of blend modes -- overlay a file texture. By setting the appropriate #define values and recompiling, these shaders also support \"Advanced\" blend modes like those found in the layers of Adobe Photoshop (TM). Advanced \"blend\" ranges are available, based on VM-generated textures. For best results,use a card capable of FP-pixel texture support. (38 technique/s)"
    },
    {
        "link": "https://docs.unity3d.com/6000.0/Documentation/Manual/built-in-shader-examples.html",
        "document": "This section contains example source code for hand-coded custom shadersA program that runs on the GPU. More info\n\nSee in Glossary that are compatible with the Built-in Render PipelineA series of operations that take the contents of a Scene, and displays them on a screen. Unity lets you choose from pre-built render pipelines, or write your own. More info\n\nSee in Glossary."
    }
]