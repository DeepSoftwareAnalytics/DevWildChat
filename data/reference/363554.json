[
    {
        "link": "https://realpython.com/python-json",
        "document": "Python’s module provides you with the tools you need to effectively handle JSON data. You can convert Python data types to a JSON-formatted string with or write them to files using . Similarly, you can read JSON data from files with and parse JSON strings with .\n\nJSON, or JavaScript Object Notation, is a widely-used text-based format for data interchange. Its syntax resembles Python dictionaries but with some differences, such as using only double quotes for strings and lowercase for Boolean values. With built-in tools for validating syntax and manipulating JSON files, Python makes it straightforward to work with JSON data.\n\nBy the end of this tutorial, you’ll understand that:\n• JSON in Python is handled using the standard-library module, which allows for data interchange between JSON and Python data types.\n• JSON is a good data format to use with Python as it’s human-readable and straightforward to serialize and deserialize, which makes it ideal for use in APIs and data storage.\n• You write JSON with Python using to serialize data to a file.\n• You can minify and prettify JSON using Python’s module.\n\nSince its introduction, JSON has rapidly emerged as the predominant standard for the exchange of information. Whether you want to transfer data with an API or store information in a document database, it’s likely you’ll encounter JSON. Fortunately, Python provides robust tools to facilitate this process and help you manage JSON data efficiently.\n\nWhile JSON is the most common format for data distribution, it’s not the only option for such tasks. Both XML and YAML serve similar purposes. If you’re interested in how the formats differ, then you can check out the tutorial on how to serialize your data with Python.\n\nThe acronym JSON stands for JavaScript Object Notation. As the name suggests, JSON originated from JavaScript. However, JSON has transcended its origins to become language-agnostic and is now recognized as the standard for data interchange. The popularity of JSON can be attributed to native support by the JavaScript language, resulting in excellent parsing performance in web browsers. On top of that, JSON’s straightforward syntax allows both humans and computers to read and write JSON data effortlessly. To get a first impression of JSON, have a look at this example code: You’ll learn more about the JSON syntax later in this tutorial. For now, recognize that the JSON format is text-based. In other words, you can create JSON files using the code editor of your choice. Once you set the file extension to , most code editors display your JSON data with syntax highlighting out of the box: The screenshot above shows how VS Code displays JSON data using the Bearded color theme. You’ll have a closer look at the syntax of the JSON format next! In the previous section, you got a first impression of how JSON data looks. And as a Python developer, the JSON structure probably reminds you of common Python data structures, like a dictionary that contains a string as a key and a value. If you understand the syntax of a dictionary in Python, you already know the general syntax of a JSON object. Note: Later in this tutorial, you’ll learn that you’re free to use lists and other data types at the top level of a JSON document. The similarity between Python dictionaries and JSON objects is no surprise. One idea behind establishing JSON as the go-to data interchange format was to make working with JSON as convenient as possible, independently of which programming language you use: [A collection of key-value pairs and arrays] are universal data structures. Virtually all modern programming languages support them in one form or another. It makes sense that a data format that is interchangeable with programming languages is also based on these structures. (Source) To explore the JSON syntax further, create a new file named and add a more complex JSON structure as the content of the file: In the code above, you see data about a dog named Frieda, which is formatted as JSON. The top-level value is a JSON object. Just like Python dictionaries, you wrap JSON objects inside curly braces ( ). In line 1, you start the JSON object with an opening curly brace ( ), and then you close the object at the end of line 20 with a closing curly brace ( ). Note: Although whitespace doesn’t matter in JSON, it’s customary for JSON documents to be formatted with two or four spaces to indicate indentation. If the file size of the JSON document is important, then you may consider minifying the JSON file by removing the whitespace. You’ll learn more about minifying JSON data later in the tutorial. Inside the JSON object, you can define zero, one, or more key-value pairs. If you add multiple key-value pairs, then you must separate them with a comma ( ). A key-value pair in a JSON object is separated by a colon ( ). On the left side of the colon, you define a key. A key is a string you must wrap in double quotes ( ). Unlike Python, JSON strings don’t support single quotes ( ). The values in a JSON document are limited to the following data types: Either or without quotes Just like in dictionaries and lists, you’re able to nest data in JSON objects and arrays. For example, you can include an object as the value of an object. Also, you’re free to use any other allowed value as an item in a JSON array. As a Python developer, you may need to pay extra attention to the Boolean values. Instead of using or in title case, you must use the lowercase JavaScript-style Booleans or . Unfortunately, there are some other details in the JSON syntax that you may stumble over as a developer. You’ll have a look at them next. The JSON standard doesn’t allow any comments, trailing commas, or single quotes for strings. This can be confusing to developers who are used to Python dictionaries or JavaScript objects. Here’s a smaller version of the JSON file from before with invalid syntax:\n• Line 5 has a trailing comma after the final key-value pair.\n• Line 10 contains a trailing comma in the array. Using double quotes is something you can get used to as a Python developer. Comments can be helpful in explaining your code, and trailing commas can make moving lines around in your code less fragile. This is why some developers like to use Human JSON (Hjson) or JSON with comments (JSONC). Hjson gives you the freedom to use comments, ditch commas between properties, or create quoteless strings. Apart from the curly braces ( ), the Hjson syntax look like a mix of YAML and JSON. JSONC is a bit stricter than Hjson. Compared to regular JSON, JSONC allows you to use comments and trailing commas. You may have encountered JSONC when editing the file of VS Code. Inside its configuration files, VS Code works in a JSONC mode. For common JSON files, VS Code is more strict and points out JSON syntax errors. If you want to make sure you write valid JSON, then your coding editor can be of great help. The invalid JSON document above contains marks for each occurrence of incorrect JSON syntax: When you don’t want to rely on your code editor, you can also use online tools to verify that the JSON syntax you write is correct. Popular online tools for validating JSON are JSON Lint and JSON Formatter. Later in the tutorial, you’ll learn how to validate JSON documents from the comfort of your terminal. But before that, it’s time to find out how you can work with JSON data in Python.\n\nPython supports the JSON format through the built-in module named . The module is specifically designed for reading and writing strings formatted as JSON. That means you can conveniently convert Python data types into JSON data and the other way around. The act of converting data into the JSON format is referred to as serialization. This process involves transforming data into a series of bytes for storage or transmission over a network. The opposite process, deserialization, involves decoding data from the JSON format back into a usable form within Python. You’ll start with the serialization of Python code into JSON data with the help of the module. One of the most common actions when working with JSON in Python is to convert a Python dictionary into a JSON object. To get an impression of how this works, hop over to your Python REPL and follow along with the code below: After importing the module, you can use to convert a Python dictionary to a JSON-formatted string, which represents a JSON object. It’s important to understand that when you use , you get a Python string in return. In other words, you don’t create any kind of JSON data type. The result is similar to what you’d get if you used Python’s built-in function: Using gets more interesting when your Python dictionary doesn’t contain strings as keys or when values don’t directly translate to a JSON format: In the dictionary, the keys , , and are numbers. Once you use , the dictionary keys become strings in the JSON-formatted string. Note: When you convert a dictionary to JSON, the dictionary keys will always be strings in JSON. The Boolean Python values of your dictionary become JSON Booleans. As mentioned before, the tiny but significant difference between JSON Booleans and Python Booleans is that JSON Booleans are lowercase. The cool thing about Python’s module is that it takes care of the conversion for you. This can come in handy when you’re using variables as dictionary keys: When converting Python data types into JSON, the module receives the evaluated values. While doing so, sticks tightly to the JSON standard. For example, when converting integer keys like to the string . The module allows you to convert common Python data types to JSON. Here’s an overview of all Python data types and values that you can convert to JSON values: Note that different Python data types like lists and tuples serialize to the same JSON data type. This can cause problems when you convert JSON data back to Python, as the data type may not be the same as before. You’ll explore this pitfall later in this tutorial when you learn how to read JSON. Dictionaries are probably the most common Python data type that you’ll use as a top-level value in JSON. But you can convert the data types listed above just as smoothly as dictionaries using . Take a Boolean or a list, for example: A JSON document may contain a single scalar value, like a number, at the top level. That’s still valid JSON. But more often than not, you want to work with a collection of key-value pairs. Similar to how not every data type can be used as a dictionary key in Python, not all keys can be converted into JSON key strings: You can’t use dictionaries, lists, or tuples as JSON keys. For dictionaries and lists, this rule makes sense as they’re not hashable. But even when a tuple is hashable and allowed as a key in a dictionary, you’ll get a when you try to use a tuple as a JSON key: : keys must be str, int, float, bool or None, not tuple By providing the argument, you can prevent getting a when creating JSON data with unsupported Python keys: When you set in to , then Python skips the keys that are not supported and would otherwise raise a . The result is a JSON-formatted string that only contains a subset of the input dictionary. In practice, you usually want your JSON data to resemble the input object as close as possible. So, you must use with caution to not lose information when calling . Note: If you’re ever in a situation where you need to convert an unsupported object into JSON, then you can consider creating a subclass of the and implementing a method. When you use , you can use additional arguments to control the look of the resulting JSON-formatted string. For example, you can sort the dictionary keys by setting the parameter to : When you set to , then Python sorts the keys alphabetically for you when serializing a dictionary. Sorting the keys of a JSON object can come in handy when your dictionary keys formerly represented the column names of a database, and you want to display them in an organized fashion to the user. Another notable parameter of is , which you’ll probably use the most when serializing JSON data. You’ll explore later in this tutorial in the prettify JSON section. When you convert Python data types into the JSON format, you usually have a goal in mind. Most commonly, you’ll use JSON to persist and exchange data. To do so, you need to save your JSON data outside of your running Python program. Conveniently, you’ll explore saving JSON data to a file next. The JSON format can come in handy when you want to save data outside of your Python program. Instead of spinning up a database, you may decide to use a JSON file to store data for your workflows. Again, Python has got you covered. To write Python data into an external JSON file, you use . This is a similar function to the one you saw earlier, but without the s at the end of its name: In lines 3 to 22, you define a dictionary that you write to a JSON file in line 25 using a context manager. To properly indicate that the file contains JSON data, you set the file extension to . When you use , then it’s good practice to define the encoding. For JSON, you commonly want to use as the encoding when reading and writing files: The RFC requires that JSON be represented using either UTF-8, UTF-16, or UTF-32, with UTF-8 being the recommended default for maximum interoperability. (Source) The function has two required arguments:\n• The object you want to write\n• The file you want to write into Other than that, there are a bunch of optional parameters for . The optional parameters of are the same as for . You’ll investigate some of them later in this tutorial when you prettify and minify JSON files.\n\nIn the former sections, you learned how to serialize Python data into JSON-formatted strings and JSON files. Now, you’ll see what happens when you load JSON data back into your Python program. In parallel to and , the library provides two functions to deserialize JSON data into a Python object: As a rule of thumb, you work with when your data is already present in your Python program. You use with external files that are saved on your disk. The conversion from JSON data types and values to Python follows a similar mapping as before when you converted Python objects into the JSON format: When you compare this table to the one in the previous section, you may recognize that Python offers a matching data type for all JSON types. That’s very convenient because this way, you can be sure you won’t lose any information when deserializing JSON data to Python. Note: Deserialization is not the exact reverse of the serialization process. The reason for this is that JSON keys are always strings, and not all Python data types can be converted to JSON data types. This discrepancy means that certain Python objects may not retain their original type when serialized and then deserialized. To get a better feeling for the conversion of data types, you’ll start with serializing a Python object to JSON and then convert the JSON data back to Python. That way, you can spot differences between the Python object you serialize and the Python object you end up with after deserializing the JSON data. To investigate how to load a Python dictionary from a JSON object, revisit the example from before. Start by creating a dictionary and then serialize the Python dictionary to a JSON string using : By passing into , you’re creating a string with a JSON object that you save in . If you want to convert back to a Python dictionary, then you can use : By using , you can convert JSON data back into Python objects. With the knowledge about JSON that you’ve gained so far, you may already suspect that the content of the dictionary is not identical to the content of : The difference between and is subtle but can be impactful in your Python programs. In JSON, the keys must always be strings. When you converted to using , the integer key became the string . When you used , there was no way for Python to know that the string key should be an integer again. That’s why your dictionary key remained a string after deserialization. You’ll investigate a similar behavior by doing another conversion roundtrip with other Python data types! To explore how different data types behave in a roundtrip from Python to JSON and back, take a portion of the dictionary from a former section. Note how the dictionary contains different data types as values: The dictionary contains a bunch of common Python data types as values. For example, a string in line 2, a Boolean in line 3, a in line 7, and a tuple in line 8, just to name a few. Next, convert to a JSON-formatted string and back to Python again. Afterward, have a look at the newly created dictionary: You can convert every JSON data type perfectly into a matching Python data type. The JSON Boolean deserializes into , converts back into , and objects and arrays become dictionaries and lists. Still, there’s one exception that you may encounter in roundtrips: When you serialize a Python tuple, it becomes a JSON array. When you load JSON, a JSON array correctly deserializes into a list because Python has no way of knowing that you want the array to be a tuple. Problems like the one described above can always be an issue when you’re doing data roundtrips. When the roundtrip happens in the same program, you may be more aware of the expected data types. Data type conversions may be even more obfuscated when you’re dealing with external JSON files that originated in another program. You’ll investigate a situation like this next! In a previous section, you created a file that saved a file. If you need to refresh your memory, you can expand the collapsible section below that shows the code again: Take a look at the data types of the dictionary. Is there a data type in a value that the JSON format doesn’t support? When you want to write content to a JSON file, you use . The counterpart to is . As the name suggests, you can use to load a JSON file into your Python program. Jump back into the Python REPL and load the JSON file from before: Just like when writing files, it’s a good idea to use a context manager when reading a file in Python. That way, you don’t need to bother with closing the file again. When you want to read a JSON file, then you use inside the statement’s block. The argument for the function must be either a text file or a binary file. The Python object that you get from depends on the top-level data type of your JSON file. In this case, the JSON file contains an object at the top level, which deserializes into a dictionary. When you deserialize a JSON file as a Python object, then you can interact with it natively—for example, by accessing the value of the key with square bracket notation ( ). Still, there’s a word of caution here. Import the original dictionary from before and compare it to : When you load a JSON file as a Python object, then any JSON data type happily deserializes into Python. That’s because Python knows about all data types that the JSON format supports. Unfortunately, it’s not the same the other way around. As you learned before, there are Python data types like that you can convert into JSON, but you’ll end up with an data type in the JSON file. Once you convert the JSON data back to Python, then an array deserializes into the Python data type. Generally, being cautious about data type conversions should be the concern of the Python program that writes the JSON. With the knowledge you have about JSON files, you can always anticipate which Python data types you’ll end up with as long as the JSON file is valid. If you use , then the content of the file you load must contain valid JSON syntax. Otherwise, you’ll receive a . Luckily, Python caters to you with more tools you can use to interact with JSON. For example, it allows you to check a JSON file’s validity from the convenience of the terminal.\n\nSo far, you’ve explored the JSON syntax and have already spotted some common JSON pitfalls like trailing commas and single quotes for strings. When writing JSON, you may have also spotted some annoying details. For example, neatly indented Python dictionaries end up being a blob of JSON data. In the last section of this tutorial, you’ll try out some techniques to make your life easier as you work with JSON data in Python. To start, you’ll give your JSON object a well-deserved glow-up. One huge advantage of the JSON format is that JSON data is human-readable. Even more so, JSON data is human-writable. This means you can open a JSON file in your favorite text editor and change the content to your liking. Well, that’s the idea, at least! Editing JSON data by hand is not particularly easy when your JSON data looks like this in the text editor: Even with word wrapping and syntax highlighting turned on, JSON data is hard to read when it’s a single line of code. And as a Python developer, you probably miss some whitespace. But worry not, Python has got you covered! When you call or to serialize a Python object, then you can provide the argument. Start by trying out with different indentation levels: The default value for is . When you call without or with as a value, you’ll end up with one line of a compact JSON-formatted string. If you want linebreaks in your JSON string, then you can set to or provide an empty string. Although probably less useful, you can even provide a negative number as the indentation or any other string. More commonly, you’ll provide values like or for : When you use positive integers as the value for when calling , then you’ll indent every level of the JSON object with the given count as spaces. Also, you’ll have newlines for each key-value pair. Note: To actually see the whitespace in the REPL, you can wrap the calls in function calls. The parameter works exactly the same for as it does for . Go ahead and write the dictionary into a JSON file with an indentation of spaces: When you set the indentation level when serializing JSON data, then you end up with prettified JSON data. Have a look at how the file looks in your editor: Python can work with JSON files no matter how they’re indented. As a human, you probably prefer a JSON file that contains newlines and is neatly indented. A JSON file that looks like this is way more convenient to edit. The convenience of being able to edit JSON data in the editor comes with a risk. When you move key-value pairs around or add strings with one quote instead of two, you end up with an invalid JSON. To swiftly check if a JSON file is valid, you can leverage Python’s . You can run the module as an executable in the terminal using the switch. To see in action, also provide as the positional argument: When you run only with an option, then Python validates the JSON file and outputs the JSON file’s content in the terminal if the JSON is valid. Running in the example above means that contains valid JSON syntax. Note: The prints the JSON data with an indentation of 4 by default. You’ll explore this behavior in the next section. To make complain, you need to invalidate your JSON document. You can make the JSON data of invalid by removing the comma ( ) between the key-value pairs: After saving , run again to validate the file: The module successfully stumbles over the missing comma in . Python notices that there’s a delimiter missing once the property name enclosed in double quotes starts in line 3 at position 5. Go ahead and try fixing the JSON file again. You can also be creative with invalidating and check how reports your error. But keep in mind that only reports the first error. So you may need to go back and forth between fixing a JSON file and running . Once is valid, you may notice that the output always looks the same. Of course, like any well-made command-line interface, offers you some options to control the program. In the previous section, you used to validate a JSON file. When the JSON syntax was valid, showed the content with newlines and an indentation of four spaces. To control how prints the JSON, you can set the option. If you followed along with the tutorial, then you’ve got a file that doesn’t contain newlines or indentation. Alternatively, you can download in the materials by clicking the link below: Free Bonus: Click here to download the free sample code that shows you how to work with JSON data in Python. When you pass in to , then you can pretty print the content of the JSON file in your terminal. When you set , then you can control which indentation level uses to display the code: Seeing the prettified JSON data in the terminal is nifty. But you can step up your game even more by providing another option to the run! By default, writes the output to , just like you commonly do when calling the function. But you can also redirect the output of into a file by providing a positional argument: With as the value of the option, you write the output into the JSON file instead of showing the content in the terminal. If the file doesn’t exist yet, then Python creates the file on the way. If the target file already exists, then you overwrite the file with the new content. Note: You can prettify a JSON file in place by using the same file as and arguments. You can verify that the file exists by running the terminal command: The whitespace you added to comes with a price. Compared to the original, unindented file, the file size of is now around double that. Here, the 308-byte increase may not be significant. But when you’re dealing with big JSON data, then a good-looking JSON file will take up quite a bit of space. Having a small data footprint is especially useful when serving data over the web. Since the JSON format is the de facto standard for exchanging data over the web, it’s worth keeping the file size as small as possible. And again, Python’s has got your back! As you know by now, Python is a great helper when working with JSON. You can minify JSON data with Python in two ways:\n• Use the module in your Python code Before, you used with the option to add whitespace. Instead of using here, you can use provide to do the opposite and remove any whitespace between the key-value pairs of your JSON: After calling the module, you provide a JSON file as the and another JSON file as the . If the target JSON file exists, then you overwrite its contents. Otherwise, you create a new file with the filename you provide. Just like with , you provide the same file as a source and target file to minify the file in-place. In the example above, you minify into . Run the command to see how many bytes you squeezed out of the original JSON file: Compared to , the file size of is 337 bytes smaller. That’s even 29 bytes less than the original file that didn’t contain any indentation. To investigate where Python managed to remove even more whitespace from the original JSON, open the Python REPL again and minify the content of the original file with Python’s module: In the code above, you use Python’s to get the content of as text. Then, you use to deserialize to , which is a Python dictionary. You could use to get a Python dictionary right away, but you need the JSON data as a string first to compare it properly. That’s also why you use to create and then use instead of leveraging directly to save the minified JSON data in . As you learned before, needs JSON data as the first argument and then accepts a value for the indentation. The default value for is , so you could skip setting the argument explicitly like you do above. But with , you’re making your intention clear that you don’t want any indentation, which will be a good thing for others who read your code later. The parameter for allows you to define a tuple with two values:\n• The separator between the key-value pairs or list items. By default, this separator is a comma followed by a space ( ).\n• The separator between the key and the value. By default, this separator is a colon followed by a space ( ). By setting to , you continue to use valid JSON separators. But you tell Python not to add any spaces after the comma ( ) and the colon ( ). That means that the only whitespace left in your JSON data can be whitespace appearing in key names and values. That’s pretty tight! With both and containing your JSON strings, it’s time to compare them: You can already spot the difference between and when you look at the output. You then use the function to verify that the size of is indeed smaller. If you’re curious about why the length of the JSON strings almost exactly matches the file size of the written files, then looking into Unicode & character encodings in Python is a great idea. Both and are excellent helpers when you want to make JSON data look prettier, or if you want to minify JSON data to save some bytes. With the module, you can conveniently interact with JSON data in your Python programs. That’s great when you need to have more control over the way you interact with JSON. The module comes in handy when you want to work with JSON data directly in your terminal."
    },
    {
        "link": "https://datacamp.com/tutorial/json-data-python",
        "document": "In this course, you'll learn the basics of relational databases and how to interact with them."
    },
    {
        "link": "https://stackoverflow.com/questions/67331398/how-to-write-json-files-with-a-good-structure",
        "document": "I want to make a logging system with discord.py that captures all messages, writes them in a json file, and when a command is executed, retrieve the data, filter with the user parameters and return the filtered data.\n\nThe problem is that i can't write json files with a good structure, currently my structure is looking like this currently :\n\nBut i want to do something like this :\n\nCurrently my code is this :\n\nWhat code should i write to get the structure as i shown ?"
    },
    {
        "link": "https://dev.to/rishabdugar/crafting-structured-json-responses-ensuring-consistent-output-from-any-llm-l9h",
        "document": "Large Language Models (LLMs) are revolutionizing how we interact with data, but getting these models to generate well-formatted & usable responses consistently can feel like herding digital cats. You ask for structured data and get a jumbled mess interspersed with friendly commentary. Frustrating, right?\n\n A reliable output is crucial, whether you're categorizing customer feedback, extracting structured data from unstructured text, or automating data pipelines. This article aims to provide a comprehensive, generalized approach to ensure you get perfectly formatted JSON from any LLM, every time.\n\n\n\n LLMs are trained on massive text datasets, making them adept at generating human-like text. However, this strength becomes a weakness when seeking precise, structured output like JSON or Python Dictionary.\n\n Common issues include:\n• Extraneous Text: LLMs often add conversational fluff before or after the JSON, making extraction difficult.\n• Hallucinations: LLMs might invent data points or misinterpret instructions, leading to invalid or inaccurate JSON.\n\nThese issues can disrupt downstream processes and lead to significant inefficiencies. Let's explore some proven techniques to overcome these challenges.\n• Explicitly Request JSON: Clearly state that you expect the output in JSON format. Explicitly stating the intended use of the JSON output in the prompt can significantly improve its validity. Giving explicit instructions to provide a structured response in \"system_prompt\" can also prove helpful.\n• Provide a JSON Schema: Define the exact structure of the desired JSON, including keys and data types.\n• Use Examples: Show the LLM examples of correctly formatted JSON output for your specific use case.\n\nAs suggested in Anthropic Documentation, one more effective method is to guide the LLM by pre-filling the assistant's response with the beginning of the JSON structure. This technique leverages the model's ability to continue from a given starting point.\n\n# Create a Bedrock Runtime client in the AWS Region of your choice. # Define the JSON schema and example prefill response with stop sequences. Ensure the output is valid JSON as it will be parsed using `json.loads()` in Python. It should be in the schema: <output> { # Define the prompt for the model. Provide an example of 5 cars with their color and models in JSON format enclosed in <output></output> XML tags. You are an AI language model that provides structured JSON outputs. # Format the request payload using the model's native structure. # Invoke the model with the request.\n\nThe salient features of this method are :\n• Prefilling the Response: \"Put words in the LLM's mouth\" by starting the assistant's response with the opening bracket or other relevant beginning sequences as we have used above . This encourages the model to follow the expected format.\n• Strategic Stop Sequences: Define stop sequences ( like or specific keywords, for example : . ) to prevent the LLM from adding extraneous text after the JSON.\n• Leveraging Tags for Complex Outputs: For multiple JSON objects, ask the output to be enclosed within unique tags ( e.g., XML tags ). This allows for easy extraction using regular expressions.\n\nWhen working with APIs or systems that return responses wrapped in XML tags, it becomes crucial to extract and utilize the JSON data embedded within those tags. Below, we'll explore methods to extract JSON data from XML tags both with and without the use of regular expressions (regex), followed by saving the extracted data to a JSON file.\n\nRegex can be a powerful tool for pattern matching and extraction. In this case, we can use regex to locate the JSON content within the specified XML tags.\n\n\n\nIn this function, is used to find the first occurrence of the pattern in the response. If found, it extracts the content between these tags and attempts to parse it as . If parsing fails, it returns .\n\nFor scenarios where you prefer not to use regex, a more manual approach can be employed to achieve the same goal.\n\n\n\nThis function locates the starting and ending positions of the tags manually, extracts the content between them and attempts to parse it as JSON. Like the regex approach, it returns None if parsing fails or the tags are not found.\n\nSaving Extracted JSON to a File\n\n After extracting the JSON data, the next step is to save it to a file for further processing or record-keeping. The function below handles this task.\n\n\n\nThis utility function opens a file in write mode and uses json.dump() to write the JSON data to it, ensuring the output is formatted with an indentation of 4 spaces for better readability.\n\nDespite employing the earlier techniques, minor syntax errors can occasionally disrupt the JSON structure. These errors can be addressed using the following methods:\n\nWe can fix these minor errors using some simple methods :\n• Requesting the LLM to Correct the JSON: Feed the malformed JSON back to the LLM and prompt it to correct the errors.\n• Utilizing JSON Repair Tools: Using tools like or can help correct these errors quickly.\n\nThe second method is generally more economical, faster, and reliable for straightforward cleanup tasks. In contrast, the first method may be more effective for addressing complex issues, albeit at the cost of additional time and an extra LLM call.\n\nYou can also use this library to completely replace :\n\n\n\nExample (Asking LLM to fix broken JSON) :\n\n\n\n# AWS Bedrock setup with credentials and region from environment variables # Create a Bedrock Runtime client in the AWS Region of your choice (hardcoded to 'us-east-1') : Toyota Corolla, # Missing quotes around the value }, { model: Chevrolet Camaro, # Missing quotes around the key and value White # Missing closing quote and closing brace for the object } ] } # Define the prompt for the model ### Instruction Your task is to act as an expert JSON fixer and repairer. You are responsible for correcting any broken JSON and ensuring there are no syntax errors. The resulting JSON should be validated and easily parsed using `json.loads()` in Python. ### Context JSON is built on two primary structures: 1. A collection of name/value pairs, realized in various languages as an object, record, struct, dictionary, hash table, keyed list, or associative array. 2. An ordered list of values, realized in most languages as an array, vector, list, or sequence. These structures are supported by virtually all modern programming languages, making JSON a widely used data interchange format. In JSON, the structures take the following forms: - An **object** is an unordered set of name/value pairs. An object begins with a `{` (left brace) and ends with a `}` (right brace). Each name is followed by a `:` (colon) and the name/value pairs are separated by `,` (comma). - An **array** is an ordered collection of values. An array begins with a `[` (left bracket) and ends with a `]` (right bracket). Values are separated by `,` (comma). ### Requirements 1. Repair only the JSON structure without changing or modifying any data or values of the keys. 2. Ensure that the data is accurately represented and properly formatted within the JSON structure. 3. The resulting JSON should be validated and able to be parsed using `json.loads()` in Python. ### Example #### Broken JSON { } } ### Notes - Pay close attention to missing commas, unmatched braces or brackets, and any other structural issues. - Maintain the integrity of the data without making assumptions or altering the content. - Ensure the output is clean, precise, and ready for parsing in Python. # Format the request payload using the model's native structure # Invoke the model with the request\n\nWhile these techniques can significantly improve the consistency of JSON output from LLMs, they are not foolproof. Potential challenges include:\n\nMoreover, ethical considerations such as data privacy and model biases should always be taken into account when deploying LLMs in production environments.\n• Start with a Clear JSON Template: Define the JSON structure and use it as a guide for the LLM with few-shot prompting examples.\n• Leverage Post-Processing Tools: Use tools like to correct minor syntax errors in the JSON output.\n• Iterate and Improve: Continuously refining our prompts and validation rules based on the output and feedback. By following these steps, we can ensure that our LLM consistently generates well-formatted JSON, making our AI-driven applications more reliable and efficient.\n\nGenerating perfectly formatted JSON from LLMs is a common yet challenging task. By guiding the JSON syntax, communicating its usage, and using validation tools like json-fixer, we can significantly improve the consistency and reliability of the output. By combining clear instructions, strategic prompting, and robust validation, we can transform our LLM interactions from a gamble into a reliable pipeline for structured data.\n\n That's all for the day folks, Stay informed, iterate, and refine your approach to master the art of JSON generation from any LLM."
    },
    {
        "link": "https://stackoverflow.com/questions/53175422/formatting-json-in-python",
        "document": "I think for a true JSON object print, it's probably as good as it gets. for the following took about :\n\nI tried with , but it actually wouldn't print the pure JSON string unless it's converted to a Python , which loses your , and etc valid JSON as mentioned in the other answer. As well it doesn't retain the order in which the items appeared, so it's not great if order is important for readability.\n\nJust for fun I whipped up the following function:\n\nIt prints pretty alright, and unsurprisingly it took a whooping to run in , which is 3 times as much as a would get you. It's probably not worthwhile to build your own function to achieve this unless you spend some time to optimize it, and with the lack of a for the same, it's probably best you stick with your gun since your efforts will most likely give diminishing returns."
    },
    {
        "link": "https://stackoverflow.com/questions/1071720/sql-like-selects-in-imperative-languages",
        "document": "I'm doing some coding at work in C++, and a lot of the things that I work on involve analyzing sets of data. Very often I need to select some elements from a STL container, and very frequently I wrote code like this:\n\nOver time this for loop and the logic contained within it gets a lot more complicated and unreadable. Code like this is less satisfying than the analogous SELECT statement in SQL, assuming that I have a table called numbers with a column named \"num\" rather than a std::vector< int > :\n\nThat's a lot more readable to me, and also scales better, over time a lot of the if-statement logic that's in our codebase has become complicated, order-dependent and unmaintainable. If we could do SQL-like statements in C++ without having to go to a database I think that the state of the code might be better.\n\nIs there a simpler way that I can implement something like a SELECT statement in C++ where I can create a new container of objects by only describing the characteristics of the objects that I want? I'm still relatively new to C++, so I'm hoping that there's something magic with either template metaprogramming or clever iterators that would solve this. Thanks!\n\nEdit based on first two answers. Thanks, I had no idea that's what LINQ actually was. I program on Linux and OSX systems primarily, and am interested in something cross-platform across OSX, Linux and Windows. So a more educated version of this question would be - is there a cross-platform implementation of something like LINQ for C++?"
    },
    {
        "link": "https://geeksforgeeks.org/sql-like",
        "document": "The SQL LIKE operator is used for performing pattern-based searches in a database. It is used in combination with the WHERE clause to filter records based on specified patterns, making it essential for any database-driven application that requires flexible search functionality.\n\nIn this article, we will explain the SQL LIKE operator, its syntax, uses, and practical examples. It also dives into advanced concepts like case sensitivity and wildcard characters, helping you optimize your queries for better performance and relevance.\n\nWhat is the SQL LIKE Operator?\n\nSQL LIKE operator is used with the WHERE clause to search for a specified pattern in a column. LIKE operator finds and returns the rows that fit in the given pattern.\n\nLIKE operator is case-insensitive by default in most database systems. This means that if you search for “apple” using the LIKE operator, it will return results that include “Apple”, “APPLE”, “aPpLe”, and so on.\n• None column_name: The column to be searched.\n• None pattern: The pattern to search for, which can include wildcard characters.\n\nFor making the LIKE operator case-sensitive, you can use the “BINARY” keyword in MySQL or the “COLLATE” keyword in other database systems.\n\nThis following query will only return products whose name starts with “apple” and is spelled exactly like that, without capital letters.\n\nWildcard Characters with the SQL LIKE Operator\n\nWildcards are used with the LIKE operator to search for specific patterns in strings. Wildcard characters substitute one or more characters in the string. There are four wildcard characters in SQL:\n• % (Percent): Represents zero or more characters.\n\nThe below table shows some examples on how wild card can be written and what do they mean:\n\nIn this tutorial on SQL LIKE Operator, we will use the following table in the examples.\n\nRetrieve SupplierID, Name, and Address from suppliers table, where supplier name starts form k.\n\nExample 3: Match Names Where ‘ango’ Appears in the Second Position\n\nRetrieve SupplierID, Name and Address of supplier whose name contains “ango” in second substring.\n\nExample 4: Using LIKE with AND for Complex Conditions\n\nRetrieve suppliers from Delhi with names starting with “C”:\n\nExample 5: Using NOT LIKE for Exclusion\n\nTo retrieve all suppliers whose name does not contain “Mango”\n\nThe LIKE operator is extremely resourceful in situations such as address filtering wherein we know only a segment or a portion of the entire address (such as locality or city) and would like to retrieve results based on that. The wildcards can be resourcefully exploited to yield even better and more filtered tuples based on the requirement.\n\nThe SQL LIKE operator is a powerful tool for pattern matching, allowing you to perform flexible searches within columns. By combining wildcards and logical operators, you can craft complex queries to find the data you need with precision. Understanding how to optimize the use of LIKE with indexes and case sensitivity will help improve query performance."
    },
    {
        "link": "https://freecodecamp.org/news/gql-design-and-implementation",
        "document": "Hello everyone! I'm a Software engineer who's interested in low-level programming, compilers, and tool development.\n\nThree months ago I decided to learn the Rust programming language and build a Git client that focuses on simplicity and productivity.\n\n‌I started to think about how I could build the Git client to provide some unique and useful features.\n\nFor example, I like the analysis page on GitHub that tells you how many commits each developer has made and how many lines they've inserted or deleted. But what if I want to get this analysis for some period of time, or order everything by inserted lines and not number of commits? Or order them by how many commits were made by week or month?\n\nYou can add a custom sorting option for the client, right? But I started thinking about how I could make it more dynamic. This motivated me to wonder if I could run SQL-like queries on the local .git files so I could query any information I wanted.\n\nSo imagine if you could run a query like this on your local git repositories:\n\nI have implemented this idea with a project I made called GQL (Git Query Language). And in this article, I'm going to show you how I designed and implemented the functionality.\n\nHow Can You Take a SQL-like Query and Run it on .git Files?\n\nThe first idea I had was to use SQLite. But there were some problems I couldn't resolve.\n\nFor example, I couldn't customize the syntax, and I didn't want to read .git files and store them on a SQLite database and then perform the query. I wanted everything to run on the fly.\n\nI also wanted to be able to use not only the SELECT, DELETE, and UPDATE commands but also provide commands related to Git like , , and so on.\n\nI've created different tools like compilers before, so why not create a SQL-like language from scratch and make it perform queries on the fly and see if it works?\n\nHow I Designed and Implemented a Query Language from Scratch\n\nI wanted to start small by only supporting the command without advanced features such as aggregations, grouping, joining, and so on.\n\nSo I planned to parse the query into a data structure that would make it easy to perform validation and evaluation on it (like type checking and displaying helpful error messages if anything went wrong). After that, I would pass this data structure to the evaluator that would apply the query on my .git files.\n\nThe best data structure for this case is to represent the query using an Abstract Syntax Tree (AST). This is a very common data structure used in compilers because it's fixable and make it easy to traverse and compose nodes inside others.\n\nAlso in this case, I didn't need to keep all the information about the query, only the information that needed for the next steps (this is why it's called Abstract).\n\nThe most important validation in this case would be type checking to make sure each value is valid and used in the correct place.\n\nFor example, what if the query wanted to multiply text by other text – would this be valid?\n\nThe multiplication operator expects both sides to be a number. So in this case, I wanted to inform the user that their query is invalid and try to help them understand the problem as much as possible.\n\nSo how would that work? When I see an operator like , you need to check both sides to see if the values are valid types for this operator or not. If not then, report a message like this:\n\nBeside operators, I knew that I needed to check whether each identifier was a table, field, alias of a function name, or if it should be undefined. I also needed to report an error if, for example, a branches table contained only 2 fields like the example below:\n\nSo I created a table that contained representations for all tables and fields so I could easily perform type checking. If the user tried to select a field which was undefined in this schema, then it reported an error:\n\nI had to make sure the same checks would be performed on conditions, function names, and arguments. Then, if everything was properly defined and had the correct types, the AST would be valid and we could go to the next step.\n\nWhat happens after validating the Abstract Syntax Tree?\n\nAfter making sure everything was valid, it was time to evaluate the query and how it fetched the result.\n\nTo do that, I just traversed the syntax tree and evaluated each node. After finishing, I should have the correct result in a list.\n\nLet's go through that process step by step to see how it works.\n\nFor example, in a query like this:\n\nThe AST representation will look like this:\n\nNow we need to traverse and evaluate each node but in a specific order. We don't just go start to end or end to start because we need to do this in the same order that SQL would do it to get the same result.\n\nFor example in SQL, the statement must be executed before , and must be executed after.\n\nIn the above example, everything is in the correct order to execute, so let's see what each statement will do.\n\nThis will select all the fields from the table with the name and push them to a list – let's call it . But how can I select them from the local repository?\n\nAll information about commits, branches, tags, and so on is stored by Git on files inside a folder called in each repository. One option is to write a full parser from scratch to extract the needed information. But using a library to do this instead worked for me.\n\nI decided to use the libgit2 library to perform this task. It's a pure C implementation of the Git core methods, so you can read all the information you need and to use it from Rust. There is a crate (Rust Library) created by the Rust official team called , so you can get the branch information easily like this:\n\nand then iterate over each branch to get its information and store it like this:\n\nNow we end up with list of all branches that we'll use in the next steps.\n\nThis will filter the objects list and remove all items that do not match the conditions – in our case, those ending with \"/main\".\n\nThis sorts the objects list by the value of the field .\n\nThis takes only the first five items and removes the rest from the objects list.\n\nThat's it! And now we end up with a valid result, which you can see below:\n\nThe examples below are valid and run correctly:\n\nHow to support running on multiple repositories at the same time\n\nAfter I published GQL, I got amazing feedback from people. I also got some feature requests, like wanting support for multiple repositories and filtering by repository path.\n\nI thought this was a great idea, because I could get analysis for multiple projects and also because I could do it on multiple threads. It didn't seem like it would be very hard to implement, either.\n\nSo after finishing the validation step for the AST, it's time for the evaluation step but instead of evaluating it once, it will be evaluated once for each repository and then merging all results back in one list.\n\nBut what about supporting the ability to filter by repository path?\n\nThat was pretty easy. Do you remember the branches table schema? All I needed to do was introduce a new field with name to represent the repository local path for this branch and introduce it to other tables too.\n\nSo the final schema will look like this:\n\nNow we can run a query that uses this field:\n\nAnd that's it! 😉\n\nIf you liked the project, you can give it a star ⭐ on github.com/AmrDeveloper/GQL.\n\nYou can check the website github.io/GQL for how to download and use the project on different operating systems.\n\nThe project is not done yet – this is just the start. Everyone is welcome to join and contribute to the project and suggest ideas or report bugs."
    },
    {
        "link": "https://medium.com/@techsuneel99/20-advanced-sql-techniques-with-practical-examples-b47490d9896d",
        "document": "SQL (Structured Query Language) is a powerful language for managing and manipulating relational databases. While most developers are familiar with the basic SQL commands like SELECT, INSERT, UPDATE, and DELETE, there are many advanced techniques that can greatly enhance your SQL skills and enable you to write more efficient and effective queries.\n\nIn this article, we’ll explore 20 advanced SQL techniques, each accompanied by practical examples to help you understand and apply them in real-world scenarios. Whether you’re a seasoned SQL developer or just starting out, these techniques will help you take your SQL skills to the next level.\n\nCommon Table Expressions (CTEs) are temporary named result sets that can be used within a larger SQL statement. CTEs are useful for breaking down complex queries into more manageable parts, improving code readability and maintainability.\n\nIn this example, we create a CTE named that selects specific columns from the table. We can then reference this CTE in the main query to filter the results based on the .\n\nWindow functions perform calculations across sets of rows, allowing you to perform complex data analysis and transformation tasks within a single SQL statement. Some common window functions include , , , , and .\n\nHere, we use the window function to assign a rank to each product within its category, based on the sales column. The clause groups the data by , and the clause sorts the data within each partition by in descending order.\n\nRecursive queries allow you to traverse hierarchical or tree-like data structures, such as employee-manager relationships or nested categories. They work by running a query repeatedly until a specified condition is met.\n\nIn this example, we use a recursive CTE to retrieve all employees and their management levels within an organization. The base query selects the top-level managers (where is ), and the recursive part joins the table with the CTE to retrieve the employees at each subsequent level.\n\nPivoting and unpivoting are techniques for transforming data between row and column formats. Pivoting rotates rows into columns, while unpivoting rotates columns into rows.\n\nIn the pivoting example, we take the , , and columns and pivot them into separate columns for each category, with the sales values aggregated using . The unpivoting example does the opposite, transforming the category columns into rows with a column and corresponding values.\n\nAnalytic functions are a powerful feature in SQL that allows you to perform complex calculations and data analyses within a single query. These functions include , , , , and many others.\n\nIn this query, we use the function to retrieve the next higher sales value for each product, and the function to assign each product to a sales quartile based on its sales value. The clause specifies the window over which the functions are applied, in this case ordering the results by in descending order.\n\nSubquery factoring is a technique for refactoring complex SQL queries with repeated subqueries into simpler, more readable forms. It involves defining a subquery once and referencing it multiple times within the main query.\n\nIn this example, we define a CTE that calculates the total sales for each product within a specific date range. We then reference this CTE in the main query, joining it with the table to retrieve the product names and corresponding total sales. The function is used to handle cases where a product has no sales, returning 0 instead of .\n\nPartitioning and windowing are techniques for logically dividing a result set into partitions and applying calculations or functions to each partition. They are often used in conjunction with window functions.\n\nIn this query, we partition the employee data by using the clause. Within each department partition, we rank the employees based on their salary in descending order using the window function and the clause. The result is a ranking of employees by salary within each department.\n\nLateral joins, also known as lateral subqueries or lateral views, allow you to reference columns from preceding tables in a subquery. This powerful technique enables you to perform complex calculations and data transformations within a single query.\n\nIn this example, we use lateral joins to retrieve the category name and associated tags for each product. The first lateral join retrieves the based on the of the current product. The second lateral join uses to construct an array of tag names for the current product, ordered alphabetically.\n\nGrouping sets, , and are advanced grouping techniques in SQL that allow you to generate multiple groupings and subtotals in a single query. These techniques are particularly useful for generating reports and performing data analysis.\n\nIn this query, we use the clause to generate subtotals for each product within a category, as well as grand totals for each category and across all products. The function is used to handle values and provide meaningful labels for the subtotals and grand totals.\n\nProper indexing is crucial for optimizing SQL query performance, especially for large datasets. By creating appropriate indexes on the relevant columns, you can significantly improve the speed of data retrieval and reduce execution times.\n\nIn this example, we create an index on the column of the table. This index will be used to quickly retrieve orders within a specific date range, as demonstrated in the second query.\n\nUser-Defined Functions (UDFs) allow you to extend SQL’s built-in functionality by creating custom functions. UDFs can be written in various languages, such as SQL, Python, or Java, depending on your database system.\n\nIn this example, we create a UDF called that takes a parameter and returns the calculated age as an integer. We can then use this UDF in the statement to retrieve the first name, last name, and calculated age for each employee.\n\nTemporal tables and data auditing techniques enable you to track and manage changes to data over time, providing a historical record of data modifications. This is particularly useful in scenarios where you need to maintain data integrity, comply with regulatory requirements, or perform data analysis on historical data.\n\nIn this example, we create a temporal table called with a system-versioned temporal period ( and ) that automatically tracks changes to the data. We can then query the table for a specific point in time using the FOR SYSTEM_TIME AS OF clause, retrieving the data as it existed on January 1, 2022.\n\nWindow frame clauses allow you to define the set of rows (the window frame) to be included in window function calculations. By specifying the frame boundaries, you can control the range of rows that a window function operates on.\n\nIn this query, we calculate a running total of quantities for each product, ordered by the order date. The clause specifies that the window frame should include all rows from the first row up to the current row, effectively calculating a running total over time.\n\nThe statement is a powerful SQL feature that combines insert, update, and delete operations into a single statement. It allows you to synchronize data between two tables or data sources, making it easier to manage data integrity and reduce coding complexity.\n\nIn this example, we use a statement to synchronize data between a and a . The clause specifies the join condition based on the . The clause updates existing rows in the with new values from the , while the clause inserts new rows from the into the .\n\nFull text search is a powerful feature that allows you to perform text-based searches on large amounts of unstructured data, such as product descriptions, articles, or documents. It enables you to find relevant data based on keywords, phrases, or complex search patterns.\n\nIn this example, we first create a full text catalog ( ) and a full text index on the column of the table. We can then perform a full text search using the function, which retrieves products whose descriptions contain the phrase \"laptop computer\".\n\nSQL injection is a common security vulnerability that occurs when user input is not properly sanitized and validated before being included in SQL queries. Failing to prevent SQL injection can lead to data breaches, unauthorized access, and other security risks.\n\nIn the first example, we use parameterized queries to prevent SQL injection by separating user input from the SQL statement itself. The input values are bound to the query parameters, ensuring that they are treated as data and not as part of the SQL code.\n\nIn the second example, we create a user-defined function that sanitizes user input by removing potential SQL injection characters ( and ). This function can be used to clean user input before including it in SQL queries.\n\nDatabase sharding and partitioning are techniques used to divide large datasets across multiple databases or table partitions. This can improve query performance, scalability, and manageability for databases with massive amounts of data.\n\nIn this example, we create a partitioned table `orders` that is partitioned by year based on the `order_date` column. We define three partitions: `orders_2022` for orders in 2022, `orders_2023` for orders in 2023, and `orders_future` for orders in future years.\n\nWe can then insert data into the partitioned table, and the data will be automatically distributed across the appropriate partitions based on the `order_date` value. To query a specific partition, we can use the `PARTITION` clause, as shown in the final query that retrieves data from the `orders_2022` partition.\n\n18. Database Mirroring and Replication\n\nDatabase mirroring and replication are techniques used to maintain multiple copies of a database, either for high availability, disaster recovery, or load balancing purposes. These techniques ensure that data is synchronized across different servers or locations, providing fault tolerance and improving overall system reliability.\n\nIn the first example, we configure database mirroring for the database, specifying the partner server ( ) and port ( ) for the mirror instance.\n\nIn the second example, we set up database replication using SQL Server’s built-in stored procedures. We first create a publication named on the data source. We then configure a snapshot publication for the same publication, specifying the frequency at which the snapshot should be generated (in this case, every day).\n\nMany database systems provide built-in support for geospatial data types and spatial queries, allowing you to store and analyze geographic data, such as points, lines, and polygons.\n\nIn this example, we create a table with a spatial column of type . We insert two locations, New York and London, using the function to create a spatial point from well-known text (WKT) representation and the SRID (Spatial Reference System Identifier) 4326 for WGS84 geographic coordinates.\n\nWe then perform a spatial query to retrieve location names that are within 100 kilometers of Washington, DC, using the function to calculate the distance between the stored coordinates and a specified point (Washington, DC).\n\nIn addition to these advanced techniques, there are several general tips and best practices that can help you write better, more efficient SQL code:\n• Use proper indexing: Indexes can significantly improve query performance by allowing the database to quickly locate relevant data.\n• Avoid wildcard searches: Wildcard searches (e.g., ) can be inefficient, especially for large datasets. Use them sparingly or consider alternative solutions like full-text search.\n• Write simple, readable queries: Complex queries can be difficult to understand and maintain. Break them down into smaller, more readable parts using CTEs or subqueries.\n• Use or execution plans: Most database systems provide tools like or execution plans that allow you to analyze and optimize query performance.\n• Minimize data manipulation in SQL: While SQL can perform data manipulation tasks, it’s generally more efficient to handle complex data transformations in the application layer or using dedicated tools like ETL (Extract, Transform, Load) processes.\n• Regularly maintain and optimize databases: Perform regular database maintenance tasks, such as indexing, statistics updates, and partitioning, to ensure optimal performance.\n• Keep up-to-date with database features and best practices: Database systems are constantly evolving, with new features and optimizations introduced in each release. Stay informed and take advantage of these advancements to write more efficient SQL code.\n\nBy mastering these advanced SQL techniques and following best practices, you can significantly improve the performance, scalability, and maintainability of your database applications, enabling you to handle complex data processing tasks with ease."
    },
    {
        "link": "https://machinelearningplus.com/sql/sql-like",
        "document": "You’re going to delve into an essential yet often overlooked operator in the SQL toolkit – the ‘LIKE’ operator\n\nWhat Is the SQL ‘LIKE’ Operator?\n\nThe ‘LIKE’ operator in SQL is a logical operator that you can use in the WHERE clause to search for specific patterns in a column. This operator becomes particularly useful when you’re searching for partial information in the database fields, and you don’t know the exact data.\n\nSo, let’s break down the ‘LIKE’ operator, including its usage, examples, and useful tips. I will make sure this is understandable, even for beginners.\n\nThe basic syntax for ‘LIKE’ is\n\nHere, ‘pattern’ can be a combination of ordinary characters and wildcard characters.\n\nTo fully utilize the ‘LIKE’ operator, it’s crucial to understand wildcards. These are special characters that help you match other characters in a string. There are two common wildcards used with ‘LIKE’:\n\n1) %: Represents zero, one, or multiple characters\n\n 2) _: Represents a single character\n\nImagine you have a table named Books with the following data\n\n1) Find All Books With “SQL” in the Title\n\nTo find all the books containing “SQL” in their titles, you can use the ‘%’ wildcard on both sides of “SQL”. Here’s the SQL statement:\n\nRunning this command returns all books with “SQL” anywhere in their title\n\nTo find all books starting with “Advanced”, place the ‘%’ wildcard after “Advanced”. Here’s the SQL statement\n\nRunning this command returns any book with a title starting with “Advanced”\n\nTo find books whose title ends with “Basics”, place the ‘%’ wildcard before “Basics”. Here’s the SQL statement\n\nRunning this command returns any book with a title ending with “Basics”\n\nRemember, SQL is case-insensitive. This means that ‘LIKE’ ‘A%’ and ‘LIKE’ ‘a%’ will return the same results.\n\nThe ‘LIKE’ operator, though simple, can be incredibly powerful when combined with wildcards. It’s a key tool in your SQL kit, enabling you to perform pattern matching and partial searches in your database.\n\nThe secret to mastering SQL, as with any other skill, is practice. So, start experimenting with ‘LIKE’ and see what you can discover!"
    }
]