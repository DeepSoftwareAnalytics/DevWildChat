[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html",
        "document": "Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.\n\nWith sparse inputs, the ARPACK implementation of the truncated SVD can be used (i.e. through ). Alternatively, one may consider where the data are not centered.\n\nNotice that this class only supports sparse inputs for some solvers such as “arpack” and “covariance_eigh”. See for an alternative with sparse data.\n\nFor a usage example, see Principal Component Analysis (PCA) on Iris Dataset\n\nRead more in the User Guide.\n\nNumber of components to keep. if n_components is not set all components are kept: If and , Minka’s MLE is used to guess the dimension. Use of will interpret as . If and , select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components. If , the number of components must be strictly less than the minimum of n_features and n_samples. Hence, the None case results in: If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead. When True (False by default) the vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions. The solver is selected by a default ‘auto’ policy is based on and : if the input data has fewer than 1000 features and more than 10 times as many samples, then the “covariance_eigh” solver is used. Otherwise, if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient “randomized” method is selected. Otherwise the exact “full” SVD is computed and optionally truncated afterwards. Run exact full SVD calling the standard LAPACK solver via and select the components by postprocessing Precompute the covariance matrix (on centered data), run a classical eigenvalue decomposition on the covariance matrix typically using LAPACK and select the components by postprocessing. This solver is very efficient for n_samples >> n_features and small n_features. It is, however, not tractable otherwise for large n_features (large memory footprint required to materialize the covariance matrix). Also note that compared to the “full” solver, this solver effectively doubles the condition number and is therefore less numerical stable (e.g. on input data with a large range of singular values). Run SVD truncated to calling ARPACK solver via . It requires strictly Run randomized SVD by the method of Halko et al. Tolerance for singular values computed by svd_solver == ‘arpack’. Must be of range [0.0, infinity). Number of iterations for the power method computed by svd_solver == ‘randomized’. Must be of range [0, infinity). This parameter is only relevant when . It corresponds to the additional number of random vectors to sample the range of so as to ensure proper conditioning. See for more details. Power iteration normalizer for randomized SVD solver. Not used by ARPACK. See for more details. Used when the ‘arpack’ or ‘randomized’ solvers are used. Pass an int for reproducible results across multiple function calls. See Glossary. Principal axes in feature space, representing the directions of maximum variance in the data. Equivalently, the right singular vectors of the centered input data, parallel to its eigenvectors. The components are sorted by decreasing . The amount of variance explained by each of the selected components. The variance estimation uses degrees of freedom. Equal to n_components largest eigenvalues of the covariance matrix of X. Percentage of variance explained by each of the selected components. If is not set then all components are stored and the sum of the ratios is equal to 1.0. The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the variables in the lower-dimensional space. Per-feature empirical mean, estimated from the training set. The estimated number of components. When n_components is set to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None. Number of samples in the training data. The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See “Pattern Recognition and Machine Learning” by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to compute the estimated data covariance and score samples. Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X. Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nFor n_components == ‘mle’, this class uses the method from: Minka, T. P.. “Automatic choice of dimensionality for PCA”. In NIPS, pp. 598-604\n\nImplements the probabilistic PCA model from: Tipping, M. E., and Bishop, C. M. (1999). “Probabilistic principal component analysis”. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3), 611-622. via the score and score_samples methods.\n\nFor svd_solver == ‘randomized’, see: Halko, N., Martinsson, P. G., and Tropp, J. A. (2011). “Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions”. SIAM review, 53(2), 217-288. and also Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011). “A randomized algorithm for the decomposition of matrices”. Applied and Computational Harmonic Analysis, 30(1), 47-68."
    },
    {
        "link": "https://scikit-learn.org/stable/api/index.html",
        "document": ""
    },
    {
        "link": "https://scikit-learn.org/1.0",
        "document": "\"We use scikit-learn to support leading-edge basic research [...]\" \"We use scikit-learn to support leading-edge basic research [...]\"\n\n\"I think it's the most well-designed ML package I've seen so far.\" \"I think it's the most well-designed ML package I've seen so far.\"\n\n\"scikit-learn's ease-of-use, performance and overall variety of algorithms implemented has proved invaluable [...].\" \"scikit-learn's ease-of-use, performance and overall variety of algorithms implemented has proved invaluable [...].\"\n\n\"The great benefit of scikit-learn is its fast learning curve [...]\" \"The great benefit of scikit-learn is its fast learning curve [...]\"\n\n\"It allows us to do AWesome stuff we would not otherwise accomplish\" \"It allows us to do AWesome stuff we would not otherwise accomplish\"\n\n\"scikit-learn makes doing advanced analysis in Python accessible to anyone.\" \"scikit-learn makes doing advanced analysis in Python accessible to anyone.\""
    },
    {
        "link": "https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html",
        "document": "Go to the end to download the full example code. or to run this example in your browser via JupyterLite or Binder\n\nThis example shows a well known decomposition technique known as Principal Component Analysis (PCA) on the Iris dataset.\n\nThis dataset is made of 4 features: sepal length, sepal width, petal length, petal width. We use PCA to project this 4 feature space into a 3-dimensional space.\n\nThe Iris dataset is directly available as part of scikit-learn. It can be loaded using the function. With the default parameters, a object is returned, containing the data, the target values, the feature names, and the target names.\n\nPlot of pairs of features of the Iris dataset# Let’s first plot the pairs of features of the Iris dataset. Each data point on each scatter plot refers to one of the 150 iris flowers in the dataset, with the color indicating their respective type (Setosa, Versicolor, and Virginica). You can already see a pattern regarding the Setosa type, which is easily identifiable based on its short and wide sepal. Only considering these two dimensions, sepal width and length, there’s still overlap between the Versicolor and Virginica types. The diagonal of the plot shows the distribution of each feature. We observe that the petal width and the petal length are the most discriminant features for the three types."
    },
    {
        "link": "https://geeksforgeeks.org/implementing-pca-in-python-with-scikit-learn",
        "document": "In this article, we will learn about PCA (Principal Component Analysis) in Python with scikit-learn. Let’s start our learning step by step.\n• When there are many input attributes, it is difficult to visualize the data. There is a very famous term ‘Curse of dimensionality in the machine learning domain.\n• Basically, it refers to the fact that a higher number of attributes in a dataset adversely affects the accuracy and training time of the machine learning model.\n• Principal Component Analysis (PCA) is a way to address this issue and is used for better data visualization and improving accuracy.\n• PCA is an unsupervised pre-processing task that is carried out before applying any ML algorithm. PCA is based on “orthogonal linear transformation” which is a mathematical technique to project the attributes of a data set onto a new coordinate system. The attribute which describes the most variance is called the first principal component and is placed at the first coordinate.\n• Similarly, the attribute which stands second in describing variance is called a second principal component and so on. In short, the complete dataset can be expressed in terms of principal components. Usually, more than 90% of the variance is explained by two/three principal components.\n• Principal component analysis, or PCA, thus converts data from high dimensional space to low dimensional space by selecting the most important attributes that capture maximum information about the dataset.\n• To implement PCA in Scikit learn, it is essential to standardize/normalize the data before applying PCA.\n• PCA is imported from sklearn.decomposition. We need to select the required number of principal components.\n• Usually, n_components is chosen to be 2 for better visualization but it matters and depends on data.\n• By the fit and transform method, the attributes are passed.\n• The values of principal components can be checked using components_ while the variance explained by each principal component can be calculated using explained_variance_ratio.\n\nLoad the breast_cancer dataset from sklearn.datasets. It is clear that the dataset has 569 data items with 30 input attributes. There are two output classes-benign and malignant. Due to 30 input features, it is impossible to visualize this data\n\nLet us select it to 3. After executing this code, we get to know that the dimensions of x are (569,3) while the dimension of actual data is (569,30). Thus, it is clear that with PCA, the number of dimensions has reduced to 3 from 30. If we choose n_components=2, the dimensions would be reduced to 2.\n\nThe principal.components_ provide an array in which the number of rows tells the number of principal components while the number of columns is equal to the number of features in actual data. We can easily see that there are three rows as n_components was chosen to be 3. However, each row has 30 columns as in actual data.\n\nPlot the principal components for better data visualization. Though we had taken n_components =3, here we are plotting a 2d graph as well as 3d using first two principal components and 3 principal components respectively. For three principal components, we need to plot a 3d graph. The colors show the 2 output classes of the original dataset-benign and malignant. It is clear that principal components show clear separation between two output classes.\n\nFor three principal components, we need to plot a 3d graph. x[:,0] signifies the first principal component. Similarly, x[:,1] and x[:,2] represent the second and the third principal component.\n\nExplained_variance_ratio provides an idea of how much variation is explained by principal components."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/preprocessing.html",
        "document": "The package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n\nIn general, many learning algorithms such as linear models benefit from standardization of the data set (see Importance of Feature Scaling). If some outliers are present in the set, robust scalers or other transformers can be more appropriate. The behaviors of the different scalers, transformers, and normalizers on a dataset containing marginal outliers is highlighted in Compare the effect of different scalers on data with outliers.\n\nStandardization, or mean removal and variance scaling# Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation. For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) may assume that all features are centered around zero or have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected. The module provides the utility class, which is a quick and easy way to perform the following operation on an array-like dataset: Scaled data has zero mean and unit variance: This class implements the API to compute the mean and standard deviation on a training set so as to be able to later re-apply the same transformation on the testing set. This class is hence suitable for use in the early steps of a : It is possible to disable either centering or scaling by either passing or to the constructor of . An alternative standardization is scaling features to lie between a given minimum and maximum value, often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. This can be achieved using or , respectively. The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data. Here is an example to scale a toy data matrix to the range: The same instance of the transformer can then be applied to some new test data unseen during the fit call: the same scaling and shifting operations will be applied to be consistent with the transformation performed on the train data: It is possible to introspect the scaler attributes to find about the exact nature of the transformation learned on the training data: If is given an explicit the full formula is: works in a very similar fashion, but scales in a way that the training data lies within the range by dividing through the largest maximum value in each feature. It is meant for data that is already centered at zero or sparse data. Here is how to use the toy data from the previous example with this scaler: Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a sensible thing to do. However, it can make sense to scale sparse inputs, especially if features are on different scales. was specifically designed for scaling sparse data, and is the recommended way to go about this. However, can accept matrices as input, as long as is explicitly passed to the constructor. Otherwise a will be raised as silently centering would break the sparsity and would often crash the execution by allocating excessive amounts of memory unintentionally. cannot be fitted to sparse inputs, but you can use the method on sparse inputs. Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns format (see and ). Any other sparse input will be converted to the Compressed Sparse Rows representation. To avoid unnecessary memory copies, it is recommended to choose the CSR or CSC representation upstream. Finally, if the centered data is expected to be small enough, explicitly converting the input to an array using the method of sparse matrices is another option. If your data contains many outliers, scaling using the mean and variance of the data is likely to not work very well. In these cases, you can use as a drop-in replacement instead. It uses more robust estimates for the center and range of your data. Further discussion on the importance of centering and scaling data is available on this FAQ: Should I normalize/standardize/rescale the data? It is sometimes not enough to center and scale the features independently, since a downstream model can further make some assumption on the linear independence of the features. To address this issue you can use with to further remove the linear correlation across features. If you have a kernel matrix of a kernel \\(K\\) that computes a dot product in a feature space (possibly implicitly) defined by a function \\(\\phi(\\cdot)\\), a can transform the kernel matrix so that it contains inner products in the feature space defined by \\(\\phi\\) followed by the removal of the mean in that space. In other words, computes the centered Gram matrix associated to a positive semidefinite kernel \\(K\\). We can have a look at the mathematical formulation now that we have the intuition. Let \\(K\\) be a kernel matrix of shape computed from \\(X\\), a data matrix of shape , during the step. \\(K\\) is defined by \\(\\phi(X)\\) is a function mapping of \\(X\\) to a Hilbert space. A centered kernel \\(\\tilde{K}\\) is defined as: where \\(\\tilde{\\phi}(X)\\) results from centering \\(\\phi(X)\\) in the Hilbert space. Thus, one could compute \\(\\tilde{K}\\) by mapping \\(X\\) using the function \\(\\phi(\\cdot)\\) and center the data in this new space. However, kernels are often used because they allows some algebra calculations that avoid computing explicitly this mapping using \\(\\phi(\\cdot)\\). Indeed, one can implicitly center as shown in Appendix B in [Scholkopf1998]: \\(1_{\\text{n}_{samples}}\\) is a matrix of where all entries are equal to \\(\\frac{1}{\\text{n}_{samples}}\\). In the step, the kernel becomes \\(K_{test}(X, Y)\\) defined as: \\(Y\\) is the test dataset of shape and thus \\(K_{test}\\) is of shape . In this case, centering \\(K_{test}\\) is done as: \\(1'_{\\text{n}_{samples}}\\) is a matrix of shape where all entries are equal to \\(\\frac{1}{\\text{n}_{samples}}\\).\n\nTwo types of transformations are available: quantile transforms and power transforms. Both quantile and power transforms are based on monotonic transformations of the features and thus preserve the rank of the values along each feature. Quantile transforms put all features into the same desired distribution based on the formula \\(G^{-1}(F(X))\\) where \\(F\\) is the cumulative distribution function of the feature and \\(G^{-1}\\) the quantile function of the desired output distribution \\(G\\). This formula is using the two following facts: (i) if \\(X\\) is a random variable with a continuous cumulative distribution function \\(F\\) then \\(F(X)\\) is uniformly distributed on \\([0,1]\\); (ii) if \\(U\\) is a random variable with uniform distribution on \\([0,1]\\) then \\(G^{-1}(U)\\) has distribution \\(G\\). By performing a rank transformation, a quantile transform smooths out unusual distributions and is less influenced by outliers than scaling methods. It does, however, distort correlations and distances within and across features. Power transforms are a family of parametric transformations that aim to map data from any distribution to as close to a Gaussian distribution. provides a non-parametric transformation to map the data to a uniform distribution with values between 0 and 1: This feature corresponds to the sepal length in cm. Once the quantile transformation applied, those landmarks approach closely the percentiles previously defined: This can be confirmed on a independent testing set with similar remarks: In many modeling scenarios, normality of the features in a dataset is desirable. Power transforms are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible in order to stabilize variance and minimize skewness. currently provides two such power transformations, the Yeo-Johnson transform and the Box-Cox transform. Box-Cox can only be applied to strictly positive data. In both methods, the transformation is parameterized by \\(\\lambda\\), which is determined through maximum likelihood estimation. Here is an example of using Box-Cox to map samples drawn from a lognormal distribution to a normal distribution: While the above example sets the option to , will apply zero-mean, unit-variance normalization to the transformed output by default. Below are examples of Box-Cox and Yeo-Johnson applied to various probability distributions. Note that when applied to certain distributions, the power transforms achieve very Gaussian-like results, but with others, they are ineffective. This highlights the importance of visualizing the data before and after transformation. It is also possible to map data to a normal distribution using by setting . Using the earlier example with the iris dataset: Thus the median of the input becomes the mean of the output, centered at 0. The normal output is clipped so that the input’s minimum and maximum — corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively — do not become infinite under the transformation.\n\nNormalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples. This assumption is the base of the Vector Space Model often used in text classification and clustering contexts. The function provides a quick and easy way to perform this operation on a single array-like dataset, either using the , , or norms: The module further provides a utility class that implements the same operation using the API (even though the method is useless in this case: the class is stateless as this operation treats samples independently). This class is hence suitable for use in the early steps of a : The normalizer instance can then be used on sample vectors as any transformer: Note: L2 normalization is also known as spatial sign preprocessing. and accept both dense array-like and sparse matrices from scipy.sparse as input. For sparse input the data is converted to the Compressed Sparse Rows representation (see ) before being fed to efficient Cython routines. To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.\n\nDiscretization (otherwise known as quantization or binning) provides a way to partition continuous features into discrete values. Certain datasets with continuous features may benefit from discretization, because discretization can transform the dataset of continuous attributes to one with only nominal attributes. One-hot encoded discretized features can make a model more expressive, while maintaining interpretability. For instance, pre-processing with a discretizer can introduce nonlinearity to linear models. For more advanced possibilities, in particular smooth ones, see Generating polynomial features further below. By default the output is one-hot encoded into a sparse matrix (See Encoding categorical features) and this can be configured with the parameter. For each feature, the bin edges are computed during and together with the number of bins, they will define the intervals. Therefore, for the current example, these intervals are defined as: Based on these bin intervals, is transformed as follows: The resulting dataset contains ordinal attributes which can be further used in a . Discretization is similar to constructing histograms for continuous data. However, histograms focus on counting features which fall into particular bins, whereas discretization focuses on assigning feature values to these bins. implements different binning strategies, which can be selected with the parameter. The ‘uniform’ strategy uses constant-width bins. The ‘quantile’ strategy uses the quantiles values to have equally populated bins in each feature. The ‘kmeans’ strategy defines bins based on a k-means clustering procedure performed on each feature independently. Be aware that one can specify custom bins by passing a callable defining the discretization strategy to . For instance, we can use the Pandas function :\n• None Demonstrating the different strategies of KBinsDiscretizer Feature binarization is the process of thresholding numerical features to get boolean values. This can be useful for downstream probabilistic estimators that make assumption that the input data is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for the . It is also common among the text processing community to use binary feature values (probably to simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or TF-IDF valued features often perform slightly better in practice. As for the , the utility class is meant to be used in the early stages of . The method does nothing as each sample is treated independently of others: It is possible to adjust the threshold of the binarizer: As for the class, the preprocessing module provides a companion function to be used when the transformer API is not necessary. Note that the is similar to the when , and when the bin edge is at the value . and accept both dense array-like and sparse matrices from scipy.sparse as input. For sparse input the data is converted to the Compressed Sparse Rows representation (see ). To avoid unnecessary memory copies, it is recommended to choose the CSR representation upstream.\n\nOften it’s useful to add complexity to a model by considering nonlinear features of the input data. We show two possibilities that are both based on polynomials: The first one uses pure polynomials, the second one uses splines, i.e. piecewise polynomials. A simple and common method to use is polynomial features, which can get features’ high-order and interaction terms. It is implemented in : The features of X have been transformed from \\((X_1, X_2)\\) to \\((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\\). In some cases, only interaction terms among features are required, and it can be gotten with the setting : The features of X have been transformed from \\((X_1, X_2, X_3)\\) to \\((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\\). Note that polynomial features are used implicitly in kernel methods (e.g., , ) when using polynomial Kernel functions. See Polynomial and Spline interpolation for Ridge regression using created polynomial features. Another way to add nonlinear terms instead of pure polynomials of features is to generate spline basis functions for each feature with the . Splines are piecewise polynomials, parametrized by their polynomial degree and the positions of the knots. The implements a B-spline basis, cf. the references below. The treats each feature separately, i.e. it won’t give you interaction terms. Some of the advantages of splines over polynomials are:\n• None B-splines are very flexible and robust if you keep a fixed low degree, usually 3, and parsimoniously adapt the number of knots. Polynomials would need a higher degree, which leads to the next point.\n• None B-splines do not have oscillatory behaviour at the boundaries as have polynomials (the higher the degree, the worse). This is known as Runge’s phenomenon.\n• None B-splines provide good options for extrapolation beyond the boundaries, i.e. beyond the range of fitted values. Have a look at the option .\n• None B-splines generate a feature matrix with a banded structure. For a single feature, every row contains only non-zero elements, which occur consecutively and are even positive. This results in a matrix with good numerical properties, e.g. a low condition number, in sharp contrast to a matrix of polynomials, which goes under the name Vandermonde matrix. A low condition number is important for stable algorithms of linear models. The following code snippet shows splines in action: As the is sorted, one can easily see the banded matrix output. Only the three middle diagonals are non-zero for . The higher the degree, the more overlapping of the splines. Interestingly, a of is the same as with and if .\n• None Eilers, P., & Marx, B. (1996). Flexible Smoothing with B-splines and Penalties. Statist. Sci. 11 (1996), no. 2, 89–121.\n• None Perperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. A review of spline function procedures in R. BMC Med Res Methodol 19, 46 (2019)."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html",
        "document": "Standardize features by removing the mean and scaling to unit variance.\n\nThe standard score of a sample is calculated as:\n\nwhere is the mean of the training samples or zero if , and is the standard deviation of the training samples or one if .\n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using .\n\nStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n\nis sensitive to outliers, and the features may scale differently from each other in the presence of outliers. For an example visualization, refer to Compare StandardScaler with other scalers.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing to avoid breaking the sparsity structure of the data.\n\nRead more in the User Guide.\n\nIf False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. If True, scale the data to unit variance (or equivalently, unit standard deviation). scale_ ndarray of shape (n_features,) or None Per feature relative scaling of the data to achieve zero mean and unit variance. Generally this is calculated using . If a variance is zero, we can’t achieve unit variance, and the data is left as-is, giving a scaling factor of 1. is equal to when . mean_ ndarray of shape (n_features,) or None The mean value for each feature in the training set. Equal to when and . var_ ndarray of shape (n_features,) or None The variance for each feature in the training set. Used to compute . Equal to when and . Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings. The number of samples processed by the estimator for each feature. If there are no missing samples, the will be an integer, otherwise it will be an array of dtype int. If are used it will be a float (if no missing data) or an array of dtype float that sums the weights seen so far. Will be reset on new calls to fit, but increments across calls.\n\nNaNs are treated as missing values: disregarded in fit, and maintained in transform.\n\nWe use a biased estimator for the standard deviation, equivalent to . Note that the choice of is unlikely to affect model performance.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://digitalocean.com/community/tutorials/standardscaler-function-in-python",
        "document": "Hello, readers! In this article, we will be focusing on one of the most important pre-processing techniques in Python - Standardization using StandardScaler() function.\n\nSo, let us begin!!\n\nBefore getting into Standardization, let us first understand the concept of Scaling.\n\nScaling of Features is an essential step in modeling the algorithms with the datasets. The data that is usually used for the purpose of modeling is derived through various means such as:\n\nSo, the data obtained contains features of various dimensions and scales altogether. Different scales of the data features affect the modeling of a dataset adversely.\n\nIt leads to a biased outcome of predictions in terms of misclassification error and accuracy rates. Thus, it is necessary to Scale the data prior to modeling.\n\nThis is when standardization comes into picture.\n\nStandardization is a scaling technique wherein it makes the data scale-free by converting the statistical distribution of the data into the below format:\n\nBy this, the entire data set scales with a zero mean and unit variance, altogether.\n\nLet us now try to implement the concept of Standardization in the upcoming sections.\n\nPython sklearn library offers us with StandardScaler() function to standardize the data values into a standard format.\n\nAccording to the above syntax, we initially create an object of the function. Further, we use along with the assigned object to transform the data and standardize it.\n\nNote: Standardization is only applicable on the data values that follows Normal Distribution.\n\nHave a look at the below example!\n• Import the necessary libraries required. We have imported sklearn library to use the StandardScaler function.\n• Load the dataset. Here we have used the IRIS dataset from sklearn.datasets library. You can find the dataset here.\n• Set an object to the StandardScaler() function.\n• Segregate the independent and the target variables as shown above.\n• Apply the function onto the dataset using the fit_transform() function.\n\nBy this, we have come to the end of this topic. Feel free to comment below, in case you come across any question.\n\nFor more posts related to Python, Stay tuned @ Python with JournalDev and till then, Happy Learning!! :)"
    },
    {
        "link": "https://stackoverflow.com/questions/52642940/feature-scaling-for-a-big-dataset",
        "document": "I am trying to use a deep learning model for time series prediction, and before passing the data to the model I want to scale the different variables as they have widely different ranges.\n\nI have normally done this \"on the fly\": load the training subset of the data set, obtain the scaler from the whole subset, store it and then load it when I want to use it for testing.\n\nNow the data is pretty big and I will not load all the training data at once for training.\n\nHow could I go to obtain the scaler? A priori I thought of doing a one-time operation of loading all the data just to calculate the scaler (normally I use the sklearn scalers, like StandardScaler), and then load it when I do my training process.\n\nIs this a common practice? If it is, how would you do if you add data to the training dataset? can scalers be combined to avoid that one-time operation and just \"update\" the scaler?"
    },
    {
        "link": "https://machinelearningmastery.com/standardscaler-and-minmaxscaler-transforms-in-python",
        "document": "Many machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n\nThis includes algorithms that use a weighted sum of the input, like linear regression, and algorithms that use distance measures, like k-nearest neighbors.\n\nThe two most popular techniques for scaling numerical data prior to modeling are normalization and standardization. Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision. Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.\n\nIn this tutorial, you will discover how to use scaler transforms to standardize and normalize numerical input variables for classification and regression.\n\nAfter completing this tutorial, you will know:\n• Data scaling is a recommended pre-processing step when working with many machine learning algorithms.\n• Data scaling can be achieved by normalizing or standardizing real-valued input and output variables.\n• How to apply standardization and normalization to improve the performance of predictive modeling algorithms.\n\nKick-start your project with my new book Data Preparation for Machine Learning, including step-by-step tutorials and the Python source code files for all examples.\n\nThis tutorial is divided into six parts; they are:\n• The Scale of Your Data Matters\n\nThe Scale of Your Data Matters\n\nMachine learning models learn a mapping from input variables to an output variable.\n\nAs such, the scale and distribution of the data drawn from the domain may be different for each variable.\n\nInput variables may have different units (e.g. feet, kilometers, and hours) that, in turn, may mean the variables have different scales.\n\nDifferences in the scales across input variables may increase the difficulty of the problem being modeled. An example of this is that large input values (e.g. a spread of hundreds or thousands of units) can result in a model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher generalization error.\n\nThis difference in scale for input variables does not affect all machine learning algorithms.\n\nFor example, algorithms that fit a model that use a weighted sum of input variables are affected, such as linear regression, logistic regression, and artificial neural networks (deep learning).\n\nAlso, algorithms that use distance measures between examples or exemplars are affected, such as k-nearest neighbors and support vector machines. There are also algorithms that are unaffected by the scale of numerical input variables, most notably decision trees and ensembles of trees, like random forest.\n\nIt can also be a good idea to scale the target variable for regression predictive modeling problems to make the problem easier to learn, most notably in the case of neural network models. A target variable with a large spread of values, in turn, may result in large error gradient values causing weight values to change dramatically, making the learning process unstable.\n\nScaling input and output variables is a critical step in using neural network models.\n\nBoth normalization and standardization can be achieved using the scikit-learn library.\n\nLet’s take a closer look at each in turn.\n\nNormalization is a rescaling of the data from the original range so that all values are within the new range of 0 and 1.\n\nNormalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data.\n\nA value is normalized as follows:\n\nWhere the minimum and maximum values pertain to the value x being normalized.\n\nFor example, for a dataset, we could guesstimate the min and max observable values as 30 and -10. We can then normalize any value, like 18.8, as follows:\n\nYou can see that if an x value is provided that is outside the bounds of the minimum and maximum values, the resulting value will not be in the range of 0 and 1. You could check for these observations prior to making predictions and either remove them from the dataset or limit them to the pre-defined maximum or minimum values.\n\nYou can normalize your dataset using the scikit-learn object MinMaxScaler.\n\nGood practice usage with the MinMaxScaler and other scaling techniques is as follows:\n• Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the fit() function.\n• Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling the transform() function.\n• Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions.\n\nThe default scale for the MinMaxScaler is to rescale variables into the range [0,1], although a preferred scale can be specified via the “feature_range” argument and specify a tuple, including the min and the max for all variables.\n\nWe can demonstrate the usage of this class by converting two variables to a range 0-to-1, the default range for normalization. The first variable has values between about 4 and 100, the second has values between about 0.1 and 0.001.\n\nThe complete example is listed below.\n\nRunning the example first reports the raw dataset, showing 2 columns with 4 rows. The values are in scientific notation which can be hard to read if you’re not used to it.\n\nNext, the scaler is defined, fit on the whole dataset and then used to create a transformed version of the dataset with each column normalized independently. We can see that the largest raw value for each column now has the value 1.0 and the smallest value for each column now has the value 0.0.\n\nNow that we are familiar with normalization, let’s take a closer look at standardization.\n\nStandardizing a dataset involves rescaling the distribution of values so that the mean of observed values is 0 and the standard deviation is 1.\n\nThis can be thought of as subtracting the mean value or centering the data.\n\nLike normalization, standardization can be useful, and even required in some machine learning algorithms when your data has input values with differing scales.\n\nStandardization assumes that your observations fit a Gaussian distribution (bell curve) with a well-behaved mean and standard deviation. You can still standardize your data if this expectation is not met, but you may not get reliable results.\n\nStandardization requires that you know or are able to accurately estimate the mean and standard deviation of observable values. You may be able to estimate these values from your training data, not the entire dataset.\n\nSubtracting the mean from the data is called centering, whereas dividing by the standard deviation is called scaling. As such, the method is sometime called “center scaling“.\n\nA value is standardized as follows:\n\nWhere the mean is calculated as:\n\nAnd the standard_deviation is calculated as:\n\nWe can guesstimate a mean of 10.0 and a standard deviation of about 5.0. Using these values, we can standardize the first value of 20.7 as follows:\n\nThe mean and standard deviation estimates of a dataset can be more robust to new data than the minimum and maximum.\n\nYou can standardize your dataset using the scikit-learn object StandardScaler.\n\nWe can demonstrate the usage of this class by converting two variables to a range 0-to-1 defined in the previous section. We will use the default configuration that will both center and scale the values in each column, e.g. full standardization.\n\nThe complete example is listed below.\n\nRunning the example first reports the raw dataset, showing 2 columns with 4 rows as before.\n\nNext, the scaler is defined, fit on the whole dataset and then used to create a transformed version of the dataset with each column standardized independently. We can see that the mean value in each column is assigned a value of 0.0 if present and the values are centered around 0.0 with values both positive and negative.\n\nNext, we can introduce a real dataset that provides the basis for applying normalization and standardization transforms as a part of modeling.\n\nThe sonar dataset is a standard machine learning dataset for binary classification.\n\nIt involves 60 real-valued inputs and a two-class target variable. There are 208 examples in the dataset and the classes are reasonably balanced.\n\nA baseline classification algorithm can achieve a classification accuracy of about 53.4 percent using repeated stratified 10-fold cross-validation. Top performance on this dataset is about 88 percent using repeated stratified 10-fold cross-validation.\n\nThe dataset describes radar returns of rocks or simulated mines.\n\nYou can learn more about the dataset from here:\n\nNo need to download the dataset; we will download it automatically from our worked examples.\n\nFirst, let’s load and summarize the dataset. The complete example is listed below.\n\nRunning the example first summarizes the shape of the loaded dataset.\n\nThis confirms the 60 input variables, one output variable, and 208 rows of data.\n\nA statistical summary of the input variables is provided showing that values are numeric and range approximately from 0 to 1.\n\nFinally, a histogram is created for each input variable.\n\nIf we ignore the clutter of the plots and focus on the histograms themselves, we can see that many variables have a skewed distribution.\n\nThe dataset provides a good candidate for using scaler transforms as the variables have differing minimum and maximum values, as well as different data distributions.\n\nNext, let’s fit and evaluate a machine learning model on the raw dataset.\n\nWe will use a k-nearest neighbor algorithm with default hyperparameters and evaluate it using repeated stratified k-fold cross-validation. The complete example is listed below.\n\nRunning the example evaluates a KNN model on the raw sonar dataset.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nWe can see that the model achieved a mean classification accuracy of about 79.7 percent, showing that it has skill (better than 53.4 percent) and is in the ball-park of good performance (88 percent).\n\nNext, let’s explore a scaling transform of the dataset.\n\nWe can apply the MinMaxScaler to the Sonar dataset directly to normalize the input variables.\n\nWe will use the default configuration and scale values to the range 0 and 1. First, a MinMaxScaler instance is defined with default hyperparameters. Once defined, we can call the fit_transform() function and pass it to our dataset to create a transformed version of our dataset.\n\nLet’s try it on our sonar dataset.\n\nThe complete example of creating a MinMaxScaler transform of the sonar dataset and plotting histograms of the result is listed below.\n\nRunning the example first reports a summary of each input variable.\n\nWe can see that the distributions have been adjusted and that the minimum and maximum values for each variable are now a crisp 0.0 and 1.0 respectively.\n\nHistogram plots of the variables are created, although the distributions don’t look much different from their original distributions seen in the previous section.\n\nNext, let’s evaluate the same KNN model as the previous section, but in this case, on a MinMaxScaler transform of the dataset.\n\nThe complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example, we can see that the MinMaxScaler transform results in a lift in performance from 79.7 percent accuracy without the transform to about 81.3 percent with the transform.\n\nNext, let’s explore the effect of standardizing the input variables.\n\nWe can apply the StandardScaler to the Sonar dataset directly to standardize the input variables.\n\nWe will use the default configuration and scale values to subtract the mean to center them on 0.0 and divide by the standard deviation to give the standard deviation of 1.0. First, a StandardScaler instance is defined with default hyperparameters.\n\nOnce defined, we can call the fit_transform() function and pass it to our dataset to create a transformed version of our dataset.\n\nLet’s try it on our sonar dataset.\n\nThe complete example of creating a StandardScaler transform of the sonar dataset and plotting histograms of the results is listed below.\n\nRunning the example first reports a summary of each input variable.\n\nWe can see that the distributions have been adjusted and that the mean is a very small number close to zero and the standard deviation is very close to 1.0 for each variable.\n\nHistogram plots of the variables are created, although the distributions don’t look much different from their original distributions seen in the previous section other than their scale on the x-axis.\n\nNext, let’s evaluate the same KNN model as the previous section, but in this case, on a StandardScaler transform of the dataset.\n\nThe complete example is listed below.\n\nNote: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n\nRunning the example, we can see that the StandardScaler transform results in a lift in performance from 79.7 percent accuracy without the transform to about 81.0 percent with the transform, although slightly lower than the result using the MinMaxScaler.\n\nThis section lists some common questions and answers when scaling numerical data.\n\nWhether input variables require scaling depends on the specifics of your problem and of each variable.\n\nYou may have a sequence of quantities as inputs, such as prices or temperatures.\n\nIf the distribution of the quantity is normal, then it should be standardized, otherwise, the data should be normalized. This applies if the range of quantity values is large (10s, 100s, etc.) or small (0.01, 0.0001).\n\nIf the quantity values are small (near 0-1) and the distribution is limited (e.g. standard deviation near 1), then perhaps you can get away with no scaling of the data.\n\nPredictive modeling problems can be complex, and it may not be clear how to best scale input data.\n\nIf in doubt, normalize the input sequence. If you have the resources, explore modeling with the raw data, standardized data, and normalized data and see if there is a beneficial difference in the performance of the resulting model.\n\nStandardization can give values that are both positive and negative centered around zero.\n\nIt may be desirable to normalize data after it has been standardized.\n\nThis might be a good idea of you have a mixture of standardized and normalized variables and wish all input variables to have the same minimum and maximum values as input for a given algorithm, such as an algorithm that calculates distance measures.\n\nQ. But Which is Best?\n\nEvaluate models on data prepared with each transform and use the transform or combination of transforms that result in the best performance for your data set on your model.\n\nYou may normalize your data by calculating the minimum and maximum on the training data.\n\nLater, you may have new data with values smaller or larger than the minimum or maximum respectively.\n\nOne simple approach to handling this may be to check for such out-of-bound values and change their values to the known minimum or maximum prior to scaling. Alternately, you may want to estimate the minimum and maximum values used in the normalization manually based on domain knowledge.\n\nThis section provides more resources on the topic if you are looking to go deeper.\n• How to use Data Scaling Improve Deep Learning Model Stability and Performance\n• Rescaling Data for Machine Learning in Python with Scikit-Learn\n• How to Scale Data for Long Short-Term Memory Networks in Python\n• How to Normalize and Standardize Time Series Data in Python\n\nIn this tutorial, you discovered how to use scaler transforms to standardize and normalize numerical input variables for classification and regression.\n• Data scaling is a recommended pre-processing step when working with many machine learning algorithms.\n• Data scaling can be achieved by normalizing or standardizing real-valued input and output variables.\n• How to apply standardization and normalization to improve the performance of predictive modeling algorithms.\n\nDo you have any questions?\n\n Ask your questions in the comments below and I will do my best to answer."
    }
]