[
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html",
        "document": "LinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n\nWhether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered). If True, X will be copied; else, it may be overwritten. The number of jobs to use for the computation. This will only provide speedup in case of sufficiently large problems, that is if firstly and secondly is sparse or if is set to . means 1 unless in a context. means using all processors. See Glossary for more details. When set to , forces the coefficients to be positive. This option is only supported for dense arrays. Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features. Rank of matrix . Only available when is dense. Singular values of . Only available when is dense. Independent term in the linear model. Set to 0.0 if . Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nFrom the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares (scipy.optimize.nnls) wrapped as a predictor object.\n\nReturn the coefficient of determination of the prediction. The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares and \\(v\\) is the total sum of squares . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of , disregarding the input features, would get a \\(R^2\\) score of 0.0. Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape , where is the number of samples used in the fitting for the estimator. The \\(R^2\\) score used when calling on a regressor uses from version 0.23 to keep consistent with default value of . This influences the method of all the multioutput regressors (except for ).\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect.\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/linear_model.html",
        "document": "The following are a set of methods intended for regression in which the target value is expected to be a linear combination of the features. In mathematical notation, if \\(\\hat{y}\\) is the predicted value.\n\nAcross the module, we designate the vector \\(w = (w_1, ..., w_p)\\) as and \\(w_0\\) as .\n\nTo perform classification with generalized linear models, see Logistic regression.\n\nBayesian regression techniques can be used to include regularization parameters in the estimation procedure: the regularization parameter is not set in a hard sense but tuned to the data at hand. This can be done by introducing uninformative priors over the hyper parameters of the model. The \\(\\ell_{2}\\) regularization used in Ridge regression and classification is equivalent to finding a maximum a posteriori estimation under a Gaussian prior over the coefficients \\(w\\) with precision \\(\\lambda^{-1}\\). Instead of setting manually, it is possible to treat it as a random variable to be estimated from the data. To obtain a fully probabilistic model, the output \\(y\\) is assumed to be Gaussian distributed around \\(X w\\): where \\(\\alpha\\) is again treated as a random variable that is to be estimated from the data. The advantages of Bayesian Regression are:\n• None It adapts to the data at hand.\n• None It can be used to include regularization parameters in the estimation procedure.\n• None Inference of the model can be time consuming.\n• None A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning\n• None Original Algorithm is detailed in the book by Radford M. Neal estimates a probabilistic model of the regression problem as described above. The prior for the coefficient \\(w\\) is given by a spherical Gaussian: The priors over \\(\\alpha\\) and \\(\\lambda\\) are chosen to be gamma distributions, the conjugate prior for the precision of the Gaussian. The resulting model is called Bayesian Ridge Regression, and is similar to the classical . The parameters \\(w\\), \\(\\alpha\\) and \\(\\lambda\\) are estimated jointly during the fit of the model, the regularization parameters \\(\\alpha\\) and \\(\\lambda\\) being estimated by maximizing the log marginal likelihood. The scikit-learn implementation is based on the algorithm described in Appendix A of (Tipping, 2001) where the update of the parameters \\(\\alpha\\) and \\(\\lambda\\) is done as suggested in (MacKay, 1992). The initial value of the maximization procedure can be set with the hyperparameters and . There are four more hyperparameters, \\(\\alpha_1\\), \\(\\alpha_2\\), \\(\\lambda_1\\) and \\(\\lambda_2\\) of the gamma prior distributions over \\(\\alpha\\) and \\(\\lambda\\). These are usually chosen to be non-informative. By default \\(\\alpha_1 = \\alpha_2 = \\lambda_1 = \\lambda_2 = 10^{-6}\\). Bayesian Ridge Regression is used for regression: After being fitted, the model can then be used to predict new values: The coefficients \\(w\\) of the model can be accessed: Due to the Bayesian framework, the weights found are slightly different to the ones found by Ordinary Least Squares. However, Bayesian Ridge Regression is more robust to ill-posed problems. The Automatic Relevance Determination (as being implemented in ) is a kind of linear model which is very similar to the Bayesian Ridge Regression, but that leads to sparser coefficients \\(w\\) . poses a different prior over \\(w\\): it drops the spherical Gaussian distribution for a centered elliptic Gaussian distribution. This means each coefficient \\(w_{i}\\) can itself be drawn from a Gaussian distribution, centered on zero and with a precision \\(\\lambda_{i}\\): with \\(A\\) being a positive definite diagonal matrix and \\(\\text{diag}(A) = \\lambda = \\{\\lambda_{1},...,\\lambda_{p}\\}\\). In contrast to the Bayesian Ridge Regression, each coordinate of \\(w_{i}\\) has its own standard deviation \\(\\frac{1}{\\lambda_i}\\). The prior over all \\(\\lambda_i\\) is chosen to be the same gamma distribution given by the hyperparameters \\(\\lambda_1\\) and \\(\\lambda_2\\). ARD is also known in the literature as Sparse Bayesian Learning and Relevance Vector Machine . For a worked-out comparison between ARD and Bayesian Ridge Regression, see the example below.\n\nThe logistic regression is implemented in . Despite its name, it is implemented as a linear model for classification rather than regression in terms of the scikit-learn/ML nomenclature. The logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function. This implementation can fit binary, One-vs-Rest, or multinomial logistic regression with optional \\(\\ell_1\\), \\(\\ell_2\\) or Elastic-Net regularization. Regularization is applied by default, which is common in machine learning but not in statistics. Another advantage of regularization is that it improves numerical stability. No regularization amounts to setting C to a very high value. Logistic Regression as a special case of the Generalized Linear Models (GLM) Logistic regression is a special case of Generalized Linear Models with a Binomial / Bernoulli conditional distribution and a Logit link. The numerical output of the logistic regression, which is the predicted probability, can be used as a classifier by applying a threshold (by default 0.5) to it. This is how it is implemented in scikit-learn, so it expects a categorical target, making the Logistic Regression a classifier. For notational ease, we assume that the target \\(y_i\\) takes values in the set \\(\\{0, 1\\}\\) for data point \\(i\\). Once fitted, the method of predicts the probability of the positive class \\(P(y_i=1|X_i)\\) as As an optimization problem, binary class logistic regression with regularization term \\(r(w)\\) minimizes the following cost function: where \\({s_i}\\) corresponds to the weights assigned by the user to a specific training sample (the vector \\(s\\) is formed by element-wise multiplication of the class weights and sample weights), and the sum \\(S = \\sum_{i=1}^n s_i\\). We currently provide four choices for the regularization term \\(r(w)\\) via the argument: For ElasticNet, \\(\\rho\\) (which corresponds to the parameter) controls the strength of \\(\\ell_1\\) regularization vs. \\(\\ell_2\\) regularization. Elastic-Net is equivalent to \\(\\ell_1\\) when \\(\\rho = 1\\) and equivalent to \\(\\ell_2\\) when \\(\\rho=0\\). Note that the scale of the class weights and the sample weights will influence the optimization problem. For instance, multiplying the sample weights by a constant \\(b>0\\) is equivalent to multiplying the (inverse) regularization strength by \\(b\\). The binary case can be extended to \\(K\\) classes leading to the multinomial logistic regression, see also log-linear model. It is possible to parameterize a \\(K\\)-class classification model using only \\(K-1\\) weight vectors, leaving one class probability fully determined by the other class probabilities by leveraging the fact that all class probabilities must sum to one. We deliberately choose to overparameterize the model using \\(K\\) weight vectors for ease of implementation and to preserve the symmetrical inductive bias regarding ordering of classes, see . This effect becomes especially important when using regularization. The choice of overparameterization can be detrimental for unpenalized models since then the solution may not be unique, as shown in . Let \\(y_i \\in {1, \\ldots, K}\\) be the label (ordinal) encoded target variable for observation \\(i\\). Instead of a single coefficient vector, we now have a matrix of coefficients \\(W\\) where each row vector \\(W_k\\) corresponds to class \\(k\\). We aim at predicting the class probabilities \\(P(y_i=k|X_i)\\) via as: The objective for the optimization becomes where \\([P]\\) represents the Iverson bracket which evaluates to \\(0\\) if \\(P\\) is false, otherwise it evaluates to \\(1\\). Again, \\(s_{ik}\\) are the weights assigned by the user (multiplication of sample weights and class weights) with their sum \\(S = \\sum_{i=1}^n \\sum_{k=0}^{K-1} s_{ik}\\). We currently provide four choices for the regularization term \\(r(W)\\) via the argument, where \\(m\\) is the number of features: The solvers implemented in the class are “lbfgs”, “liblinear”, “newton-cg”, “newton-cholesky”, “sag” and “saga”: The following table summarizes the penalties and multinomial multiclass supported by each solver: The “lbfgs” solver is used by default for its robustness. For large datasets the “saga” solver is usually faster. For large dataset, you may also consider using with , which might be even faster but requires more tuning. There might be a difference in the scores obtained between with or and the external liblinear library directly, when and the fit (or) the data to be predicted are zeroes. This is because for the sample(s) with zero, and predict the negative class, while liblinear predicts the positive class. Note that a model with and having many samples with zero, is likely to be a underfit, bad model and you are advised to set and increase the .\n• None The solver “liblinear” uses a coordinate descent (CD) algorithm, and relies on the excellent C++ LIBLINEAR library, which is shipped with scikit-learn. However, the CD algorithm implemented in liblinear cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a “one-vs-rest” fashion so separate binary classifiers are trained for all classes. This happens under the hood, so instances using this solver behave as multiclass classifiers. For \\(\\ell_1\\) regularization allows to calculate the lower bound for C in order to get a non “null” (all feature weights to zero) model.\n• None The “lbfgs”, “newton-cg” and “sag” solvers only support \\(\\ell_2\\) regularization or no regularization, and are found to converge faster for some high-dimensional data. Setting to “multinomial” with these solvers learns a true multinomial logistic regression model , which means that its probability estimates should be better calibrated than the default “one-vs-rest” setting.\n• None The “sag” solver uses Stochastic Average Gradient descent . It is faster than other solvers for large datasets, when both the number of samples and the number of features are large.\n• None The “saga” solver is a variant of “sag” that also supports the non-smooth . This is therefore the solver of choice for sparse multinomial logistic regression. It is also the only solver that supports .\n• None The “lbfgs” is an optimization algorithm that approximates the Broyden–Fletcher–Goldfarb–Shanno algorithm , which belongs to quasi-Newton methods. As such, it can deal with a wide range of different training data and is therefore the default solver. Its performance, however, suffers on poorly scaled datasets and on datasets with one-hot encoded categorical features with rare categories.\n• None The “newton-cholesky” solver is an exact Newton solver that calculates the hessian matrix and solves the resulting linear system. It is a very good choice for >> , but has a few shortcomings: Only \\(\\ell_2\\) regularization is supported. Furthermore, because the hessian matrix is explicitly computed, the memory usage has a quadratic dependency on as well as on . As a consequence, only the one-vs-rest scheme is implemented for the multiclass case. For a comparison of some of these solvers, see . A logistic regression with \\(\\ell_1\\) penalty yields sparse models, and can thus be used to perform feature selection, as detailed in L1-based feature selection. It is possible to obtain the p-values and confidence intervals for coefficients in cases of regression without penalization. The statsmodels package natively supports this. Within sklearn, one could use bootstrapping instead as well. implements Logistic Regression with built-in cross-validation support, to find the optimal and parameters according to the attribute. The “newton-cg”, “sag”, “saga” and “lbfgs” solvers are found to be faster for high-dimensional dense data, due to warm-starting (see Glossary).\n\nGeneralized Linear Models (GLM) extend linear models in two ways . First, the predicted values \\(\\hat{y}\\) are linked to a linear combination of the input variables \\(X\\) via an inverse link function \\(h\\) as Secondly, the squared loss function is replaced by the unit deviance \\(d\\) of a distribution in the exponential family (or more precisely, a reproductive exponential dispersion model (EDM) ). where \\(\\alpha\\) is the L2 regularization penalty. When sample weights are provided, the average becomes a weighted average. The following table lists some specific EDMs and their unit deviance : The Probability Density Functions (PDF) of these distributions are illustrated in the following figure, PDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma distributions with different mean values (\\(\\mu\\)). Observe the point mass at \\(Y=0\\) for the Poisson distribution and the Tweedie (power=1.5) distribution, but not for the Gamma distribution which has a strictly positive target domain.# The Bernoulli distribution is a discrete probability distribution modelling a Bernoulli trial - an event that has only two mutually exclusive outcomes. The Categorical distribution is a generalization of the Bernoulli distribution for a categorical random variable. While a random variable in a Bernoulli distribution has two possible outcomes, a Categorical random variable can take on one of K possible categories, with the probability of each category specified separately. The choice of the distribution depends on the problem at hand:\n• None If the target values \\(y\\) are counts (non-negative integer valued) or relative frequencies (non-negative), you might use a Poisson distribution with a log-link.\n• None If the target values are positive valued and skewed, you might try a Gamma distribution with a log-link.\n• None If the target values seem to be heavier tailed than a Gamma distribution, you might try an Inverse Gaussian distribution (or even higher variance powers of the Tweedie family).\n• None If the target values \\(y\\) are probabilities, you can use the Bernoulli distribution. The Bernoulli distribution with a logit link can be used for binary classification. The Categorical distribution with a softmax link can be used for multiclass classification.\n• None Agriculture / weather modeling: number of rain events per year (Poisson), amount of rainfall per event (Gamma), total rainfall per year (Tweedie / Compound Poisson Gamma).\n• None Risk modeling / insurance policy pricing: number of claim events / policyholder per year (Poisson), cost per event (Gamma), total cost per policyholder per year (Tweedie / Compound Poisson Gamma).\n• None Fraud Detection: probability that a financial transaction like a cash transfer is a fraudulent transaction (Bernoulli).\n• None Predictive maintenance: number of production interruption events per year (Poisson), duration of interruption (Gamma), total interruption time per year (Tweedie / Compound Poisson Gamma).\n• None Medical Drug Testing: probability of curing a patient in a set of trials or probability that a patient will experience side effects (Bernoulli).\n• None News Classification: classification of news articles into three categories namely Business News, Politics and Entertainment news (Categorical). implements a generalized linear model for the Tweedie distribution, that allows to model any of the above mentioned distributions using the appropriate parameter. In particular:\n• None : Normal distribution. Specific estimators such as , are generally more appropriate in this case.\n• None : Poisson distribution. is exposed for convenience. However, it is strictly equivalent to .\n• None : Gamma distribution. is exposed for convenience. However, it is strictly equivalent to . The link function is determined by the parameter. The feature matrix should be standardized before fitting. This ensures that the penalty treats features equally. Since the linear predictor \\(Xw\\) can be negative and Poisson, Gamma and Inverse Gaussian distributions don’t support negative values, it is necessary to apply an inverse link function that guarantees the non-negativeness. For example with , the inverse link function becomes \\(h(Xw)=\\exp(Xw)\\). If you want to model a relative frequency, i.e. counts per exposure (time, volume, …) you can do so by using a Poisson distribution and passing \\(y=\\frac{\\mathrm{counts}}{\\mathrm{exposure}}\\) as target values together with \\(\\mathrm{exposure}\\) as sample weights. For a concrete example see e.g. Tweedie regression on insurance claims. When performing cross-validation for the parameter of , it is advisable to specify an explicit function, because the default scorer is a function of itself.\n\nOne common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data. For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data: If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this: The (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new set of features With this re-labeling of the data, our problem can be written We see that the resulting polynomial regression is in the same class of linear models we considered above (i.e. the model is linear in \\(w\\)) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data. Here is an example of applying this idea to one-dimensional data, using polynomial features of varying degrees: This figure is created using the transformer, which transforms an input data matrix into a new data matrix of a given degree. It can be used as follows: The features of have been transformed from \\([x_1, x_2]\\) to \\([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\\), and can now be used within any linear model. This sort of preprocessing can be streamlined with the Pipeline tools. A single object representing a simple polynomial regression can be created and used as follows: The linear model trained on polynomial features is able to exactly recover the input polynomial coefficients. In some cases it’s not necessary to include higher powers of any single feature, but only the so-called interaction features that multiply together at most \\(d\\) distinct features. These can be gotten from with the setting . For example, when dealing with boolean features, \\(x_i^n = x_i\\) for all \\(n\\) and is therefore useless; but \\(x_i x_j\\) represents the conjunction of two booleans. This way, we can solve the XOR problem with a linear classifier: And the classifier “predictions” are perfect:"
    },
    {
        "link": "https://scikit-learn.org/0.15/modules/generated/sklearn.linear_model.LinearRegression.html",
        "document": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). If True, the regressors X will be normalized before regression. Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features.\n\nFrom the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n\nn_jobs : The number of jobs to use for the computation. If -1 all CPUs are used. This will only provide speedup for n_targets > 1 and sufficient large problems self : returns an instance of self.\n\nReturns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0, lower values are worse.\n\nSet the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The former have parameters of the form so that it’s possible to update each component of a nested object."
    },
    {
        "link": "https://simplilearn.com/tutorials/scikit-learn-tutorial/sklearn-linear-regression-with-examples",
        "document": ""
    },
    {
        "link": "https://activestate.com/resources/quick-reads/how-to-run-linear-regressions-in-python-scikit-learn",
        "document": "Scikit-learn is a Python package that simplifies the implementation of a wide range of Machine Learning (ML) methods for predictive data analysis, including linear regression.\n\nLinear regression can be thought of as finding the straight line that best fits a set of scattered data points:\n\nYou can then project that line to predict new data points. Linear regression is a fundamental ML algorithm due to its comparatively simple and core properties.\n\nA basic understanding of statistical math is key to comprehending linear regression, as is a good grounding in ML concepts.\n\nFor more information on ML concepts and terminology, refer to: What is Scikit-Learn In Python?\n\nThe following are some key concepts you will come across when you work with scikit-learn’s linear regression method:\n• Best Fit – the straight line in a plot that minimizes the deviation between related scattered data points.\n• Coefficient – also known as a parameter, is the factor a variable is multiplied by. In linear regression, a coefficient represents changes in a Response Variable (see below).\n• Coefficient of Determination – the correlation coefficient denoted as 𝑅². Used to describe the precision or degree of fit in a regression.\n• Correlation – the relationship between two variables in terms of quantifiable strength and degree, often referred to as the ‘degree of correlation’. Values range between -1.0 and 1.0.\n• Dependent Feature – a variable denoted as y in the slope equation y=ax+b. Also known as an Output, or a Response.\n• Estimated Regression Line – the straight line that best fits a set of scattered data points.\n• Independent Feature – a variable denoted as x in the slope equation y=ax+b. Also known as an Input, or a predictor.\n• Intercept – the location where the Slope intercepts the Y-axis denoted b in the slope equation y=ax+b.\n• Least Squares – a method of estimating a Best Fit to data, by minimizing the sum of the squares of the differences between observed and estimated values.\n• Mean – an average of a set of numbers, but in linear regression, Mean is modeled by a linear function.\n• Ordinary Least Squares Regression (OLS) – more commonly known as Linear Regression.\n• Residual – vertical distance between a data point and the line of regression (see Residual in Figure 1 below).\n• Regression – estimate of predictive change in a variable in relation to changes in other variables (see Predicted Response in Figure 1 below).\n• Response Variables – includes both the Predicted Response (the value predicted by the regression) and the Actual Response, which is the actual value of the data point (see Figure 1 below).\n• Slope – the steepness of a line of regression. Slope and Intercept can be used to define the linear relationship between two variables: y=ax+b.\n\nFigure 1. Illustration of some of the concepts and terminology defined in the above section, and used in linear regression:\n\nAlthough the class is not visible in the script, it contains default parameters that do the heavy lifting for simple least squares linear regression:\n\nCalculate the intercept for the model. If set to False, no intercept will be used in the calculation.\n\nConverts an input value to a boolean.\n\nCopies the X value. If True, X will be copied; else it may be overwritten.\n\nIn this example, a linear regression model is created based on data in a numpy array. The coefficients are formulated and then printed in the console:\n\nWatch how to create a Linear Regression and then print the Coefficients\n\nHow to Create a Linear Regression and Display it\n\nIn this example, random data is displayed in a plot. A linear regression model is then created against the data, and an estimated regression line is finally displayed.\n\nWatch how to create a Linear Regression and display it in a Plot\n\nThe main difference between regression and classification is that the output variable in regression is continuous, while the output for classification is discrete. Regression predicts quantity; classification predicts labels.\n\nFor information about classification, refer to: How to Classify Data in Python"
    },
    {
        "link": "https://medium.com/@karen.mossoyan/building-an-autoregression-model-for-time-series-analysis-in-python-49402bdd6d08",
        "document": "As a student trying to learn Machine Learning by myself I almost never saw mentions of Time Series analysis.\n\nYou go online to find some Machine Learning resources — you get classification or linear regression. You go to a coding bootcamp — you’re taught classification and regression. You attend a Deep Learning course — you’re taught (best case scenario) some unsupervised learning.\n\nBecause of this I have found myself unable to answer the question “How would you model this Time Series?” with anything more coherent than “Uhh… regression maybe idk”.\n\nEven after learning about all these algorithms like AR, MA, ARMA, ARIMA and so on, I just couldn’t for the life of me find any in-depth guide on how to implement them or actually use them on real world data. Thus, that’s what we will be doing today.\n\nUnderstanding what makes up a Time Series that is ready to be analyzed.\n\nAt first you might think that this is a really stupid question. A time series is just data that is dependent on time.\n\nWhile this is completely true, not every time series is created equal. Just look at the following three series —\n\nHere we have three different Time Series. You can probably easily tell that there is something fundamentally different between these three datasets.\n\nWhile the first one is just noise, the second one has very strong periodicity (cuz it’s a sin wave).\n\nWhile the second one is oscillating nicely around zero, the third one is just going crazy.\n\nAs you can tell, these descriptions are correct, but aren’t what an abstract mathematician would call rigorous. To give this a bit more rigor, we can frame our analysis in more statistical terms.\n\nThe third signal has both a changing mean and standard deviation. This kind of series is called Non Stationary.\n\nWhile this particular series would be easy to model with some mathematical guesswork, we usually like to model Stationary time series. That is, time series where the mean and standard deviation don’t change with time and we have little periodicity.\n\nIf this is a bit confusing — don’t worry! We will be examining this issue later, when testing our model on a real world dataset.\n\nThe AutoRegression Model is essentially a fancy way of saying that the value of the signal at a given time can be determined using its value at previous times.\n\nMore mathematically put, we can say that an AutoRegression Model of order p would model a Time Series X the following way:\n\nHere the Ψ-s are coefficients for previous Signal values. These coefficients ensure that the model can learn how much the previous values impact the current value. The η-s is just white noise.\n\nIf I were a statistician I’d tell you that this assumes that the η-s are not correlated with each other or with the previous values of the signal. However I am not a statistician and I don’t care about such details :)\n\nWhat I do care about (and so should you) is where this model is applicable. This model can be applied to any Stationary time series centered around 0.\n\nThe other condition is pretty soft — this model doesn’t like seasonality too much.\n\nAs an example, let’s take a AR(2) model. (AutoRegression model with order 2)\n\nObserve that this is equivalent to writing everything out in Matrix form like this (I’m so sorry)\n\nYou can verify that this is the case, or you can blindly trust my word.\n\nYou can also probably see that this looks a bit familiar, if you’ve ever done linear regression with Matrices. Namely, this is an equation of the form\n\nThe conventional X in the Linear Regression equation has been substituted with A to avoid further confusion.\n\nHere Y corresponds to the column starting with X_3, A corresponds to the two-column matrix starting with [X_2, X_1] etc.\n\nAs anyone with access to wikipedia can tell you, the best approximation using Least Square method here is\n\nNow this is precisely what we will use to train our model.\n\nNumPy — just move things around till there are no more dimension errors\n\nWe will be implementing this model as a class.\n\nFirstly, we will use SciKitLearn’s Linear Regression. This is because we have established that this model is just a fancy linear regression and SKLearn is lot faster and more numerically stable than anything we can write ourselves.\n\nNow we need to decide what variables will be stored in our class. For this particular model we only need 3 things — a variable p for storing the order of the AutoRegression model, a variable model to store the linear regression model working behind the scenes, and a variable sigma to use in generating noise later. All this can be summarized in this __init__ function\n\nNow we need to actually generate the data on which the underlying linear regression model will be trained.\n\nThis is, when stripped to its core, a linear regression model. However, the data on which it is trained needs to be engineered from a time series given as a 1-D array. For this, we will write a separate function\n\nIn this function we first make the starting column from the equation above (the matrix form of the AR Model). Afterwards, we just stack the next columns next to this one.\n\nThis was done through columns because the order of the AR Model will generally be smaller than the amount of data fed into it, so processing more data through numpy is significantly faster.\n\nAn analogous process is done for the target variable\n\nFinally, this is assembled in the .fit() function\n\nFinally, we are on to the most interesting part of this model — the predict function.\n\nThis function must incorporate in it two things — the ability to predict an arbitrary amount of steps into the future and the ability to perform a Monte Carlo simulation on its predictions\n\nHere the input parameters include num_predictions and mc_depth. Num_predictions is a variable which stands for the amount of steps which we will be predicting based on the given data. Mc_depth is the amount of Monte Carlo simulations of the model we will be averaging.\n\nThe way this function works is, it takes the last p values of the given signal (variable a in the function), and predicts the next value using the trained model and noise. Then the set of values a drops the oldest value and puts the newest value at its front.\n\nWe continue this way until we predict the desired number of steps.\n\nThis process defines one cycle of the Monte Carlo simulation. So we will repeat it over and over until we get the desired amount of simulations.\n\nIn the end we average all the outcomes of the Monte Carlo simulation and output that average.\n\nSo, assembling everything we have the AutoRegression model in all its glory!\n\nTo test out this model we will be using the Minimum Daily Temperatures Dataset. This dataset contains the minimum daily temperature recorded over 10 years (1981–1990) in the great Australian city of Melbourne, as well as the date when any given temperature was recorded.\n\nBelow is the plot of the data.\n\nAs we can see there is very strong seasonality in the data. This really is what we’d expect, given temperatures literally change with seasons. So it would be a good idea to see if the seasonality can be described well using the month.\n\nFrom now on, we will only be using the first 2000 data points. This way we can later gauge how well our model can extrapolate what it has already seen.\n\nThus, we will plot a box-plot chart grouped by the month to see how well the data’s month describes its seasonality.\n\nAs you can see the month is a pretty good descriptor of the seasonality of this data.\n\nTo do this, we will subtract the mean values of every month’s temperatures to get rid of seasonality. After this we will subtract the global average to reset the data’s average at zero.\n\nIn the end, we will get this data.\n\nIt is pretty obvious from this graph that the data has little to no seasonality. You can also see that the average of the data is around zero and that the data doesn’t undergo any qualitative changes over time. That is, the data is stationary.\n\nA very important step which has pretty visualizations for Time Series Data\n\nIf you look back to the definition of the AR model, you will see that we use previous values of the Series to model its future values. Now to perform a sanity check on this idea, we will plot the data against itself, with varying offsets.\n\nJust like we expected, the series is perfectly correlated with itself — it’s just a diagonal line.\n\nWe can also see that it is decently correlated with itself given an offset of 1 and 2. However, starting from an offset of 3 (Lag-3 Temperature) the series has very little correlation with itself.\n\nTo make this analysis a bit more rigorous, we will plot the AutoCorrelation and Partial AutoCorrelation functions for this series. To do this we will use the statsmodels.graphics.tsaplots library.\n\nFor the AutoRegression model we are generally interested in the Partial AutoCorrelation.\n\nThis graph should be interpreted this way — its value at a given point shows how much the series correlates with itself, given a lag corresponding to the numbers on the x axis. The amount of correlation is quantified using the correlation coefficient in the case of AutoCorrelation. In the case of Partial AutoCorrelation things are a bit more complicated and outside of the scope of this article.\n\nThe blue regions show the correlation levels which don’t imply any meaningful connection between the “real-time” data and the offset data.\n\nIn this particular graph we can see that the Partial AutoCorrelation has significant values for only the first 3 offsets (you can see that the 3-rd one is just barely above the blue region).\n\nThis means that we will be picking our model to have the order of 3. In other words, we will be predicting the value of our Series at a give point in time using the previous 3 values of the Series.\n\nTo recap, we will be building an AR(3) model on the first 2000 data points.\n\nThe end is finally in sight\n\nWe have looked at the data, pre-processed it for our model and decided on the model’s hyperparameters. Finally, all that is left is to see if the model actually works or not.\n\nIn order to do this we predict 3600 data points based on the first 3. This will give us this kind of result, which looks a lot like random noise.\n\nThis serves as a kind of sanity check to see if our predicted data will look anything like the real data. As we can seem the results don’t look so bad.\n\nFinally, we will bring the data back to its original state by bringing back the seasonality we subtracted earlier and adding the old mean.\n\nAs we can see the model often undershoots or overshoots at the turning-points of the seasons. Even though we can’t really see any other clear abnormalities with our prediction, we can’t say with great precision how much our model contributed to the prediction.\n\nThis is mostly due to the fact that this data is highly seasonal. So a lot of the information we could extract from this time series was taken from the seasonality analysis we did, and not obtained by the model.\n\nTo somehow gauge this part, we can visualize only the seasonality we extracted from the data. That’s to say, we will visualize the average of the given month for every month overtop the actual observations —\n\nWe can clearly see that the seasonality captures a good bit of the signal, while leaving out a lot of the noise (which isn’t necessarily what we want).\n\nThus, it is reasonable to suggest that, while our model has captured a good amount of information from the data, it would be wrong to assume that all the information about the signal is contained within our model.\n\nWe have defined what is a Time Series, and what kind of Time Series can be modeled effectively;\n\nWe have defined an AutoRegression model mathematically and identified an easy way of approaching it involving Linear Regression;\n\nWe have implemented AutoRegression in Python using NumPy and SKLearn;\n\nWe have introduced, pre-processed and analyzed Real World Data;\n\nWe have modeled Real World Data and assessed our results.\n\nAnd just like that, you’ve learned how to do all this in less than 10 minutes! Good for you!\n\nYou will find all the code used to produce this article in this github repository."
    },
    {
        "link": "https://ethanrosenthal.com/2018/03/22/time-series-for-scikit-learn-people-part2",
        "document": "What is an ARIMA model? Last post, we built an autoregressive model. Recall that we had a function $y$ which dependended on time ($t$), and we wanted to build a model, $\\hat{y}$, to predict $y$. In that post, we created “features” (aka a design matrix) which consisted of previous values of $y$. We then learned the weights of these features via linear regression. If we considered the previous two values of $y$ as our features, then we can write this mathematically as where $a_{j}$ is linear regression coefficient for the $j$-th previous value of $y$. These are the AutoRegressive terms (the AR in ARIMA). Next are the Integrated terms. These are for if you want to predict the difference between pairs of $y$ values as opposed to predicting $y$ directly. We can think of this like a preprocessing step to our data. As an example, the first order integrated model would transform our $y$ into $y^{*}$ as follows: I read some anecdote that these are called the Integrated terms because we’re dealing with the deltas in our data which is somehow related to integrals. I mean, I know what an integral is and what it means to add up the little $dx$’s in a function, but I don’t fully get the analogy. The last term in ARIMA is the Moving Average. This actually ruins my grand plan of scikit-learning all time series analysis by translating everything into the form of $\\hat{y_{t}} = f(\\mathbf{X}_{t})$. The reason is that the moving average terms are the difference between the true values of $y$ at previous points in time and our prediction, $\\hat{y}$. For example, 2 moving average terms would look like When the moving average terms are combined with the AR and I terms, then you end up with a gnarly equation that cannot be simplified into a nice, design matrix form because it is, in fact, a nonlinear partial differential equation. Okay, that’s an ARIMA model: 2 different types of “features” that we can build out of previous values of $y$ and a preprocessing step.\n\nHow do I make and use an ARIMA model? With , of course :) But before we get there, remember how I said that we should be careful to make sure our data is stationary? Let me detail this point, first. What’s Stationarity Got To Do With It? The Wikipedia definition of a stationary process is “a stochastic process whose unconditional joint probability distribution does not change when shifted in time”. I’m not sure of the general usefulness of that definition, but their second comment is more accessible: “…parameters such as mean and variance, if they are present, also do not change over time.” That’s more palatable. Now here’s the thing about stationarity: I’ve found it incredibly confusing to research this online (Author’s note: This is a sign that I should probably just read a damn textbook). Here’s the mess of assertions that I have come across:\n• The data must be stationary before it is fed into an ARIMA model.\n• The residuals must be stationary after they are calculated from the ARIMA model.\n• If the data is not stationary then difference it until it becomes stationary.\n• If the data is not stationary then try adding in a Trend term. While much of the above feels contradictory, it’s all fairly correct. The following walks through how I understand this, but I would be happy for feedback on where I’m wrong: One would like to feed stationary data into a linear model because this ensures that the model will not suffer from multicollinearity such that individual predictions worsen and interpretability is reduced. One way to (try to) stationarize the data is to difference it. One typically differences the data by hand in order to determine how many times one should difference the data to make it stationary. With this knowledge in hand, one then passes the undifferenced data into the ARIMA model. The ARIMA model containes a differencing step. Differencing by hand is performed to determine the differencing order paramater (like an ML hyperparameter!) for the ARIMA model. This is all part of the Box-Jenkins method for building ARIMA models. Another way to stationarize data is to add a trend term to a model, and we decide on differencing vs. trend terms depending on whether there is a stochastic or deterministic (pdf) trend in the data, respectively. If the residuals are stationary after being fed through a linear model, then the Gauss-Markov theorem guarantees us that we have found the best unbiased linear estimator (BLUE) of the data. Another way to think about this is that, if we see that the residuals are not stationary, then there is probably some pattern in the data that we should be able to incorporate into our model such that the residuals become stationary. There’s also a ton of bonus goodies that we get if we can follow Gauss-Markov, such as accurate estimates of uncertainty. This leads to a big danger in that we may underestimate the uncertainty of our model (and consequently overestimate correlations) if we use Gauss-Markov theorems on non-stationary data. I got very hung up on determining whether the data entering the model must be stationary or the residuals that result from the model must be stationary because lots of literature focuses on stationarizing the data but the theory relies on stationary residuals. Stationarizing the data is a unique aspect of ARIMA models - it helps to determine the parameters of the ARIMA model due to some theoretical things that pop out, but it is perfectly fine to feed nonstationary data into a model and hope that the model produces stationary residuals (or not even produce stationary residuals if you’re happy to give up Gauss-Markov). One could make the argument Who cares about the antiquated methods for determining the parameters of my ARIMA model? I’ll just figure them out with grid search and cross validation. The devil on my shoulder cajoles Yes! Ain’t nobody got time for learning old school statistics. Compute is cheap - go for it. Well, you can do that, but it may be hard to find suitable parameters and you should be aware of what you’re sacrificing in the process. See, a lot of the time series theory was developed and used in econometrics. In that field, it is particularly important to understand the “data generating process”, and one would like to be able to make decisions in the face of new data, new patterns, and new forecasts. There are ARIMA models that represent data generating processes that would be unrealistic in economic scenarios, and one would want to know this when developing a model! What I would like to do is trade some of the stability and interpretability of classical time series modeling for the potentially higher accuracy and easier implementation (AKA I don’t have to know the stats or solve nonlinear PDEs) of supervised machine learning. After all, from my original most, I just want to know how many Citi Bikes will be occupying a station in the next 15 minutes. This is not mission critical! So actually we can’t build ARIMA models with skits :( But, we can build parts of them! Recall that the moving average terms make the problem such that we cannot write it in our nice design matrix form of $\\hat{y_{t}} = f(\\mathbf{X}_{t})$. So, we’ll stick with the integrated and autoregressive terms, for now. To start, let’s import some standard libraries and load the old Citi Bike data. Recall that the data consists of the number of bikes available at a station as a function of time. Just by eye, we can see that there is some daily and weekly periodicity.\n\nFor posterity, let’s walk through how to look at the stationarity of the data (like, in the proper way to estimate ARIMA parameters). The common method of checking to see if data is stationary is to plot its autocorrelation function. The autocorrelation involves “sliding” or “shifting” a function and multiplying it with its unshifted self. This allows one to measure the overlap of the function with itself at different points in time. This process ends up being useful for discovering periodicity in ones data. For example, a sine wave repeats itself every $2\\pi$, so one would see peaks in the autocorrelation function every $2\\pi$. If that’s confusing, please check out this great post which contains an explanatory animation. We can use statsmodels to plot our autocorrelation function. We can use the autocorrelation function to quantify this. Below, I plot three autocorrelations at different numbers of “lags” (the terminology uses). Our residual samples are 5 minutes apart. Each plot below looks at the autocorrelation going out to some time window or “number of lags”. Autocorrelations always start at 1 (a function perfectly overlaps with itself), but we would like to see that it drops down close to zero. These plots show that it certaintly does not! Notice that the autocorrelation rises as we hit 1-day of periodicity in the second plot. In the third plot, we get daily peaks but an even bigger peak out at the seventh day which corresponds to the weekly periodicity or “seasonality”.\n\nSo we should probably try differencing our data. That is, we should subtract the adjacent point from each point. This seems like a simple concept. Lots of posts reference the DataFrame.shift() function in pandas as an easy way to subtract shifted data. My neurotic issue was that, once one does this, it is nontrivial to reconstruct the original time series. Consequently, how does one then feed the differenced data through a model and plot the prediction alongside the original data? Additionally, one performs regression on the difference time series data in an ARIMA model. This is a unique problem that shows where time series diverge from conventional machine learning via scikit-learn. While one may build and manipulate a bunch of features for an ML prediction, the target values (i.e. $y$) are typically left alone. I, for one, am always scared too touch them lest I leak target information into my features. With time series, though, one must actually transform the variable that is fed into the eventual model method. Given the needs that I had - reversible transformations and the ability to modify both and - I ended up building a library that is inspired by scikit-learn (all classes inherit from scikit-learn classes), but there are definitely untoward actions deviating from the scikit-learn paradigm. As mentioned at the top, this library is called , and can be found on my github. I should warn that it is definitely a work in progress with a non-stationary API (har har har).\n\nWhy not use an existing library? statsmodels has a lot of time series models along with plotting and pandas integrations. However, I had difficulty figuring out what was going on under the hood. Also, I did not understand how one would use a trained model on new data. And even if I did figure this out, who the hell knows how you serialize these models? Finally, I like the composability of scikit-learn. If I want to throw a neural network at the end of a pipeline, why shouldn’t I be able to? After I started this post, I found this library tsfresh which does appear to allow one to build features out of time series and throw them in a scikit-learn pipeline. However, this library does so many things and builds so many features, and I could not figure out how to do anything simple with it. It felt like a sledgehammer, and I just needed a tapper. Nonetheless, the library seems extremely powerful, and I would love to play with it given a suitable dataset. I would like to call out Facebook’s prophet which seems excellent for understanding uncertainty in time series predictions. Uncertainty in time series is typically quite large (it’s hard to predict the future!), so quantifying this is often of paramount importance (think: risk management). I would like to write about this library in a future post. Lastly, I will talk later about forecasting, but I will say now that I wanted the ability to build models that directly optimize for “multi-step ahead” forecasting which I have not seen widely implemented.\n\nWe would like to difference our data and plot the autocorrelation. We can use the class in to difference the data. When we difference the data, we will necessarily create some Null ( ) values in the data (we cannot subtract the earlier point from the first point in the array). does not like Null values, so we will use a to fill in these values. The other option would be to remove the Null values, but the number of rows in your data never changes in scikit-learn pipelines, and I did not want to deviate from that behavior. Both of these steps occur before we end up building machine learning features with the data, so they are in the module. All classes in contain methods such that we reconstruct the original time series. We would like to chain their transformations together, so we will use the in . Due to me not figuring out a better way to do this, all preprocessing transformers must prefix their name with .\n\nThe above is my attempt to illuminate why simple ARIMA models can be helpful - I knew what the forecast would look like before I even plotted it. Knowing how a deterministic process will play out is, like, the foundation of lots of science. However, when I think of these models from the machine learning side, they seem silly. If you want your model to forecast well, then why not bake that into your loss function and actually optimize for this? That, my friends, will have to wait for the next post. I set out to learn about time series so that I could know if there would be docks available at a future point in time at a Citi Bike station. After all my waxing and waning above, let’s get back to the real world and apply our knowledge. For simplicity, let’s solve the problem of predicting whether there will be any bikes available at the station. I don’t care exactly how many bikes are at the station - I just want to know if there will be any. We can thus cast this as a classification problem. I will keep my original matrix containing the number of bikes, but I will turn the array into a binary indicator of whether or not there are any bikes. We have a reasonable amount of class imbalance, which is good - there are often bikes!"
    },
    {
        "link": "https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python",
        "document": "Autoregression is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step.\n\nIt is a very simple idea that can result in accurate forecasts on a range of time series problems.\n\nIn this tutorial, you will discover how to implement an autoregressive model for time series forecasting with Python.\n\nAfter completing this tutorial, you will know:\n• How to explore your time series data for autocorrelation.\n• How to develop an autocorrelation model and use it to make predictions.\n• How to use a developed autocorrelation model to make rolling predictions.\n\nKick-start your project with my new book Time Series Forecasting With Python, including step-by-step tutorials and the Python source code files for all examples.\n• Updated Aug/2019: Updated data loading to use new API.\n\nA regression model, such as linear regression, models an output value based on a linear combination of input values.\n\nWhere yhat is the prediction, b0 and b1 are coefficients found by optimizing the model on training data, and X is an input value.\n\nThis technique can be used on time series where input variables are taken as observations at previous time steps, called lag variables.\n\nFor example, we can predict the value for the next time step (t+1) given the observations at the last two time steps (t-1 and t-2). As a regression model, this would look as follows:\n\nBecause the regression model uses data from the same input variable at previous time steps, it is referred to as an autoregression (regression of self).\n\nAn autoregression model makes an assumption that the observations at previous time steps are useful to predict the value at the next time step.\n\nThis relationship between variables is called correlation.\n\nIf both variables change in the same direction (e.g. go up together or down together), this is called a positive correlation. If the variables move in opposite directions as values change (e.g. one goes up and one goes down), then this is called negative correlation.\n\nWe can use statistical measures to calculate the correlation between the output variable and values at previous time steps at various different lags. The stronger the correlation between the output variable and a specific lagged variable, the more weight that autoregression model can put on that variable when modeling.\n\nAgain, because the correlation is calculated between the variable and itself at previous time steps, it is called an autocorrelation. It is also called serial correlation because of the sequenced structure of time series data.\n\nThe correlation statistics can also help to choose which lag variables will be useful in a model and which will not.\n\nInterestingly, if all lag variables show low or no correlation with the output variable, then it suggests that the time series problem may not be predictable. This can be very useful when getting started on a new dataset.\n\nIn this tutorial, we will investigate the autocorrelation of a univariate time series then develop an autoregression model and use it to make predictions.\n\nBefore we do that, let’s first review the Minimum Daily Temperatures data that will be used in the examples.\n\nThis dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.\n\nThe units are in degrees Celsius and there are 3,650 observations. The source of the data is credited as the Australian Bureau of Meteorology.\n\nDownload the dataset into your current working directory with the filename “daily-min-temperatures.csv“.\n\nThe code below will load the dataset as a Pandas Series.\n\nRunning the example prints the first 5 rows from the loaded dataset.\n\nA line plot of the dataset is then created.\n\nThere is a quick, visual check that we can do to see if there is an autocorrelation in our time series dataset.\n\nWe can plot the observation at the previous time step (t-1) with the observation at the next time step (t+1) as a scatter plot.\n\nThis could be done manually by first creating a lag version of the time series dataset and using a built-in scatter plot function in the Pandas library.\n\nBut there is an easier way.\n\nPandas provides a built-in plot to do exactly this, called the lag_plot() function.\n\nBelow is an example of creating a lag plot of the Minimum Daily Temperatures dataset.\n\nRunning the example plots the temperature data (t) on the x-axis against the temperature on the previous day (t-1) on the y-axis.\n\nWe can see a large ball of observations along a diagonal line of the plot. It clearly shows a relationship or some correlation.\n\nThis process could be repeated for any other lagged observation, such as if we wanted to review the relationship with the last 7 days or with the same day last month or last year.\n\nAnother quick check that we can do is to directly calculate the correlation between the observation and the lag variable.\n\nWe can use a statistical test like the Pearson correlation coefficient. This produces a number to summarize how correlated two variables are between -1 (negatively correlated) and +1 (positively correlated) with small values close to zero indicating low correlation and high values above 0.5 or below -0.5 showing high correlation.\n\nCorrelation can be calculated easily using the corr() function on the DataFrame of the lagged dataset.\n\nThe example below creates a lagged version of the Minimum Daily Temperatures dataset and calculates a correlation matrix of each column with other columns, including itself.\n\nThis is a good confirmation for the plot above.\n\nIt shows a strong positive correlation (0.77) between the observation and the lag=1 value.\n\nThis is good for one-off checks, but tedious if we want to check a large number of lag variables in our time series.\n\nNext, we will look at a scaled-up version of this approach.\n\nWe can plot the correlation coefficient for each lag variable.\n\nThis can very quickly give an idea of which lag variables may be good candidates for use in a predictive model and how the relationship between the observation and its historic values changes over time.\n\nWe could manually calculate the correlation values for each lag variable and plot the result. Thankfully, Pandas provides a built-in plot called the autocorrelation_plot() function.\n\nThe plot provides the lag number along the x-axis and the correlation coefficient value between -1 and 1 on the y-axis. The plot also includes solid and dashed lines that indicate the 95% and 99% confidence interval for the correlation values. Correlation values above these lines are more significant than those below the line, providing a threshold or cutoff for selecting more relevant lag values.\n\nRunning the example shows the swing in positive and negative correlation as the temperature values change across summer and winter seasons each previous year.\n\nThe statsmodels library also provides a version of the plot in the plot_acf() function as a line plot.\n\nIn this example, we limit the lag variables evaluated to 31 for readability.\n\nNow that we know how to review the autocorrelation in our time series, let’s look at modeling it with an autoregression.\n\nBefore we do that, let’s establish a baseline performance.\n\nLet’s say that we want to develop a model to predict the last 7 days of minimum temperatures in the dataset given all prior observations.\n\nThe simplest model that we could use to make predictions would be to persist the last observation. We can call this a persistence model and it provides a baseline of performance for the problem that we can use for comparison with an autoregression model.\n\nWe can develop a test harness for the problem by splitting the observations into training and test sets, with only the last 7 observations in the dataset assigned to the test set as “unseen” data that we wish to predict.\n\nThe predictions are made using a walk-forward validation model so that we can persist the most recent observations for the next day. This means that we are not making a 7-day forecast, but 7 1-day forecasts.\n\nRunning the example prints the mean squared error (MSE).\n\nThe value provides a baseline performance for the problem.\n\nThe expected values for the next 7 days are plotted (blue) compared to the predictions from the model (red).\n\nAn autoregression model is a linear regression model that uses lagged variables as input variables.\n\nWe could calculate the linear regression model manually using the LinearRegession class in scikit-learn and manually specify the lag input variables to use.\n\nAlternately, the statsmodels library provides an autoregression model where you must specify an appropriate lag value and trains a linear regression model. It is provided in the AutoReg class.\n\nWe can use this model by first creating the model AutoReg() and then calling fit() to train it on our dataset. This returns an AutoRegResults object.\n\nOnce fit, we can use the model to make a prediction by calling the predict() function for a number of observations in the future. This creates 1 7-day forecast, which is different from the persistence example above.\n\nThe complete example is listed below.\n\nRunning the example the list of coefficients in the trained linear regression model.\n\nThe 7 day forecast is then printed and the mean squared error of the forecast is summarized.\n\nA plot of the expected (blue) vs the predicted values (red) is made.\n\nThe forecast does look pretty good (about 1 degree Celsius out each day), with big deviation on day 5.\n\nThe statsmodels API does not make it easy to update the model as new observations become available.\n\nOne way would be to re-train the AutoReg model each day as new observations become available, and that may be a valid approach, if not computationally expensive.\n\nAn alternative would be to use the learned coefficients and manually make predictions. This requires that the history of 29 prior observations be kept and that the coefficients be retrieved from the model and used in the regression equation to come up with new forecasts.\n\nThe coefficients are provided in an array with the intercept term followed by the coefficients for each lag variable starting at t-1 to t-n. We simply need to use them in the right order on the history of observations, as follows:\n\nBelow is the complete example.\n\nAgain, running the example prints the forecast and the mean squared error.\n\nWe can see a small improvement in the forecast when comparing the error scores.\n\nThis section provides some resources if you are looking to dig deeper into autocorrelation and autoregression.\n\nIn this tutorial, you discovered how to make autoregression forecasts for time series data using Python.\n• About autocorrelation and autoregression and how they can be used to better understand time series data.\n• How to explore the autocorrelation in a time series using plots and statistical tests.\n• How to train an autoregression model in Python and use it to make short-term and rolling forecasts.\n\nDo you have any questions about autoregression, or about this tutorial?\n\n Ask your questions in the comments below and I will do my best to answer."
    },
    {
        "link": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html",
        "document": "Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)\n\nRead more in the User Guide.\n\nStop the algorithm if w has converged. Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. If True, compute the objective function at each step of the model. Threshold for removing (pruning) weights with high precision from the computation. Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered). If True, X will be copied; else, it may be overwritten. Coefficients of the regression model (mean of distribution) if computed, value of the objective function (to be maximized) The actual number of iterations to reach the stopping criterion. Independent term in decision function. Set to 0.0 if . If , offset subtracted for centering data to a zero mean. Set to np.zeros(n_features) otherwise. Number of features seen during fit. Names of features seen during fit. Defined only when has feature names that are all strings.\n\nFor an example, see examples/linear_model/plot_ard.py.\n\nD. J. C. MacKay, Bayesian nonlinear modeling for the prediction competition, ASHRAE Transactions, 1994.\n\nR. Salakhutdinov, Lecture notes on Statistical Machine Learning, http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15 Their beta is our Their alpha is our ARD is a little different than the slide: only dimensions/features for which are kept and the rest are discarded.\n\nReturn the coefficient of determination of the prediction. The coefficient of determination \\(R^2\\) is defined as \\((1 - \\frac{u}{v})\\), where \\(u\\) is the residual sum of squares and \\(v\\) is the total sum of squares . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of , disregarding the input features, would get a \\(R^2\\) score of 0.0. Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape , where is the number of samples used in the fitting for the estimator. The \\(R^2\\) score used when calling on a regressor uses from version 0.23 to keep consistent with default value of . This influences the method of all the multioutput regressors (except for ).\n\nNote that this method is only relevant if (see ). Please see User Guide on how the routing mechanism works. The options for each parameter are:\n• None : metadata is requested, and passed to if provided. The request is ignored if metadata is not provided.\n• None : metadata is not requested and the meta-estimator will not pass it to .\n• None : metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n• None : metadata should be passed to the meta-estimator with this given alias instead of the original name. The default ( ) retains the existing request. This allows you to change the request for some parameters and not others. This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a . Otherwise it has no effect."
    },
    {
        "link": "https://hcrystalball.readthedocs.io/en/latest/examples/tutorial/wrappers/02_ar_modelling_in_sklearn.html",
        "document": "In a lot of cases, traditional time series models work well, but there are many traditional machine learning algorithms that work very well on tabular datasets and it would be waste not to leverage their power for time-series forecast. To enable its use we developed SklearnWrapper\n\nAllows you use Sklearn-API regressors as autoregressive models for time-series predictions. In terms of usage, there is one difference between the rest of the wrappers and SklearnWrapper. Since the model is provided by package user and we don’t know parameters of the model ahead - usage of factory function get_sklearn_wrapper is needed. You can put any sklearn-compatible regressor to the function and it will return SklearnWrapper class You can provide any parameter for original regressor model. It doesn’t need to be just Sklearn model, only Sklearn API is required. You define as first positional argument the wrapping class and than you can mix wrappers parameters ( ) with wrapped class parameters( )"
    }
]